<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1908.07873] Federated Learning: Challenges, Methods, and Future Directions</title><meta property="og:description" content="Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive ne…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning: Challenges, Methods, and Future Directions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning: Challenges, Methods, and Future Directions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1908.07873">

<!--Generated on Wed Mar  6 18:03:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning: 
<br class="ltx_break">Challenges, Methods, and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id1.1.id1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="id1.1.id1.1.1" class="ltx_tr">
<span id="id1.1.id1.1.1.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.1.1.1.1" class="ltx_text" style="font-size:90%;">Tian Li</span></span>
<span id="id1.1.id1.1.1.2" class="ltx_td ltx_align_center"><span id="id1.1.id1.1.1.2.1" class="ltx_text" style="font-size:90%;">Anit Kumar Sahu</span></span></span>
<span id="id1.1.id1.2.2" class="ltx_tr">
<span id="id1.1.id1.2.2.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.2.2.1.1" class="ltx_text" style="font-size:90%;">Carnegie Mellon University</span></span>
<span id="id1.1.id1.2.2.2" class="ltx_td ltx_align_center"><span id="id1.1.id1.2.2.2.1" class="ltx_text" style="font-size:90%;">Bosch Center for Artificial Intelligence</span></span></span>
<span id="id1.1.id1.3.3" class="ltx_tr">
<span id="id1.1.id1.3.3.1" class="ltx_td ltx_align_center"><a href="mailto:tianli@cmu.edu" title="" class="ltx_ref ltx_href ltx_font_typewriter" style="font-size:90%;">tianli@cmu.edu</a></span>
<span id="id1.1.id1.3.3.2" class="ltx_td ltx_align_center"><a href="mailto:anit.sahu@gmail.com" title="" class="ltx_ref ltx_href ltx_font_typewriter" style="font-size:90%;">anit.sahu@gmail.com</a></span></span>
<span id="id1.1.id1.4.4" class="ltx_tr">
<span id="id1.1.id1.4.4.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.4.4.1.1" class="ltx_text" style="font-size:90%;">Ameet Talwalkar</span></span>
<span id="id1.1.id1.4.4.2" class="ltx_td ltx_align_center"><span id="id1.1.id1.4.4.2.1" class="ltx_text" style="font-size:90%;">Virginia Smith</span></span></span>
<span id="id1.1.id1.5.5" class="ltx_tr">
<span id="id1.1.id1.5.5.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.5.5.1.1" class="ltx_text" style="font-size:90%;">Carnegie Mellon University &amp; Determined AI</span></span>
<span id="id1.1.id1.5.5.2" class="ltx_td ltx_align_center"><span id="id1.1.id1.5.5.2.1" class="ltx_text" style="font-size:90%;">Carnegie Mellon University</span></span></span>
<span id="id1.1.id1.6.6" class="ltx_tr">
<span id="id1.1.id1.6.6.1" class="ltx_td ltx_align_center"><a href="mailto:talwalkar@cmu.edu" title="" class="ltx_ref ltx_href ltx_font_typewriter" style="font-size:90%;">talwalkar@cmu.edu</a></span>
<span id="id1.1.id1.6.6.2" class="ltx_td ltx_align_center"><a href="mailto:smithv@cmu.edu" title="" class="ltx_ref ltx_href ltx_font_typewriter" style="font-size:90%;">smithv@cmu.edu</a></span></span>
</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Mobile phones, wearable devices, and autonomous vehicles are just a few of the modern distributed networks generating a wealth of data each day.
Due to the growing computational power of these devices—coupled with concerns over transmitting private information—it is increasingly attractive to store data <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">locally</em> and push network computation to the edge.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">The concept of edge computing is not a new one. Indeed, computing simple queries across distributed, low-powered
devices is a decades-long area of research that has been explored under the purview of query processing
in sensor networks, computing at the edge, and fog computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">74</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>. Recent works have also considered training machine learning models centrally but serving and storing them locally; for example, this is a common approach in mobile user modeling and personalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>, <a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1908.07873/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">An example application of federated learning for the task of next-word prediction on mobile phones. To preserve the privacy of the text data and to reduce strain on the network, we seek to train a predictor in a distributed fashion, rather than sending the raw data to a central server. In this setup, remote devices communicate with a central server periodically to learn a global model. At each communication round, a subset of selected phones performs local training on their non-identically-distributed user data, and sends these local updates to the server. After incorporating the updates, the server then sends back the new global model to another subset of devices. This iterative training process continues across the network until convergence is reached or some stopping criterion is met.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">However, as the storage and computational capabilities of the devices within distributed networks grow, it is possible to leverage enhanced local resources on each device. This has led to a growing interest in <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">federated learning</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite>, which
explores <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">training</em> statistical models directly on remote devices<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We use the term ‘device’ throughout the article to describe entities in the network, such as nodes, clients, sensors, or organizations.</span></span></span>.
As we discuss in this article, learning in such a setting differs significantly from traditional distributed environments—requiring fundamental advances in areas such as privacy, large-scale machine learning, and distributed optimization, and raising new questions at the intersection of diverse fields, such as machine learning and systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">91</span></a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Federated learning methods have been deployed by major service providers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib124" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">124</span></a>]</cite>, and play a critical role in supporting privacy-sensitive applications where the training data are distributed at the edge <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib139" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">139</span></a>, <a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">89</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib105" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">105</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib127" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">127</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>.
Examples of potential applications include: learning sentiment, semantic location, or activities of mobile phone users; adapting to pedestrian behavior in autonomous vehicles; and predicting health events like heart attack risk from wearable devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">84</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>.
We discuss several canonical applications of federated learning below:</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Smart phones.</em> By jointly learning user behavior across a large pool of mobile phones, statistical models can power applications such as next-word prediction, face detection, and voice recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">89</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>.
However, users may not be willing to share their data in order to protect their personal privacy or to save the limited bandwidth/battery power of their phone.
Federated learning has the potential to enable predictive features on smart phones without diminishing the user experience or leaking private information. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts one such application in which we aim to learn a next-word predictor in a large-scale mobile phone network based on users’ historical text data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i2.p1.1" class="ltx_p"><em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Organizations.</em> Organizations or institutions can also be viewed as ‘devices’ in the context of federated learning.
For example, hospitals are organizations that contain a multitude of patient data for predictive healthcare. However, hospitals operate under strict privacy practices, and may face legal, administrative, or ethical
constraints that require data to remain local. Federated learning is a promising solution for these applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>, as it can reduce strain on the network and enable private learning between various devices/organizations.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p"><em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Internet of things.</em>
Modern IoT networks, such as wearable devices, autonomous vehicles, or smart homes, may contain numerous sensors that allow them to collect, react, and adapt to incoming data in real-time. For example, a fleet of autonomous vehicles may require an up-to-date model of traffic, construction, or pedestrian behavior to safely operate. However, building aggregate models in these scenarios may be difficult due to the private nature of the data and the limited connectivity of each device. Federated learning methods can help to train models that efficiently adapt to changes in these systems while maintaining user privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">98</span></a>, <a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">84</span></a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Problem Formulation</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS1.p1.12" class="ltx_p">The canonical federated learning problem involves learning a <em id="S1.SS1.p1.12.1" class="ltx_emph ltx_font_italic">single, global</em> statistical model from data stored on tens to potentially millions of remote devices. We aim to learn this model under the constraint that
device-generated data is stored and processed locally, with only intermediate updates being communicated periodically with a central server. In particular, the goal is typically to minimize the following objective function:</p>
<table id="S4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S1.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S1.E1.m1.4" class="ltx_Math" alttext="\displaystyle\min_{w}\,F(w)\,,\,\,\,\text{where}\,\,\,F(w):=\sum_{k=1}^{m}p_{k}F_{k}(w)\,." display="inline"><semantics id="S1.E1.m1.4a"><mrow id="S1.E1.m1.4.4.1" xref="S1.E1.m1.4.4.1.1.cmml"><mrow id="S1.E1.m1.4.4.1.1" xref="S1.E1.m1.4.4.1.1.cmml"><mrow id="S1.E1.m1.4.4.1.1.2.2" xref="S1.E1.m1.4.4.1.1.2.3.cmml"><mrow id="S1.E1.m1.4.4.1.1.1.1.1" xref="S1.E1.m1.4.4.1.1.1.1.1.cmml"><mrow id="S1.E1.m1.4.4.1.1.1.1.1.2" xref="S1.E1.m1.4.4.1.1.1.1.1.2.cmml"><munder id="S1.E1.m1.4.4.1.1.1.1.1.2.1" xref="S1.E1.m1.4.4.1.1.1.1.1.2.1.cmml"><mi id="S1.E1.m1.4.4.1.1.1.1.1.2.1.2" xref="S1.E1.m1.4.4.1.1.1.1.1.2.1.2.cmml">min</mi><mi id="S1.E1.m1.4.4.1.1.1.1.1.2.1.3" xref="S1.E1.m1.4.4.1.1.1.1.1.2.1.3.cmml">w</mi></munder><mo lspace="0.337em" id="S1.E1.m1.4.4.1.1.1.1.1.2a" xref="S1.E1.m1.4.4.1.1.1.1.1.2.cmml">⁡</mo><mi id="S1.E1.m1.4.4.1.1.1.1.1.2.2" xref="S1.E1.m1.4.4.1.1.1.1.1.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S1.E1.m1.4.4.1.1.1.1.1.1" xref="S1.E1.m1.4.4.1.1.1.1.1.1.cmml">​</mo><mrow id="S1.E1.m1.4.4.1.1.1.1.1.3.2" xref="S1.E1.m1.4.4.1.1.1.1.1.cmml"><mo stretchy="false" id="S1.E1.m1.4.4.1.1.1.1.1.3.2.1" xref="S1.E1.m1.4.4.1.1.1.1.1.cmml">(</mo><mi id="S1.E1.m1.1.1" xref="S1.E1.m1.1.1.cmml">w</mi><mo rspace="0.170em" stretchy="false" id="S1.E1.m1.4.4.1.1.1.1.1.3.2.2" xref="S1.E1.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.667em" id="S1.E1.m1.4.4.1.1.2.2.3" xref="S1.E1.m1.4.4.1.1.2.3.cmml">,</mo><mrow id="S1.E1.m1.4.4.1.1.2.2.2" xref="S1.E1.m1.4.4.1.1.2.2.2.cmml"><mtext id="S1.E1.m1.4.4.1.1.2.2.2.2" xref="S1.E1.m1.4.4.1.1.2.2.2.2a.cmml">where</mtext><mo lspace="0.500em" rspace="0em" id="S1.E1.m1.4.4.1.1.2.2.2.1" xref="S1.E1.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mi id="S1.E1.m1.4.4.1.1.2.2.2.3" xref="S1.E1.m1.4.4.1.1.2.2.2.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S1.E1.m1.4.4.1.1.2.2.2.1a" xref="S1.E1.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mrow id="S1.E1.m1.4.4.1.1.2.2.2.4.2" xref="S1.E1.m1.4.4.1.1.2.2.2.cmml"><mo stretchy="false" id="S1.E1.m1.4.4.1.1.2.2.2.4.2.1" xref="S1.E1.m1.4.4.1.1.2.2.2.cmml">(</mo><mi id="S1.E1.m1.2.2" xref="S1.E1.m1.2.2.cmml">w</mi><mo rspace="0.278em" stretchy="false" id="S1.E1.m1.4.4.1.1.2.2.2.4.2.2" xref="S1.E1.m1.4.4.1.1.2.2.2.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.278em" id="S1.E1.m1.4.4.1.1.3" xref="S1.E1.m1.4.4.1.1.3.cmml">:=</mo><mrow id="S1.E1.m1.4.4.1.1.4" xref="S1.E1.m1.4.4.1.1.4.cmml"><mstyle displaystyle="true" id="S1.E1.m1.4.4.1.1.4.1" xref="S1.E1.m1.4.4.1.1.4.1.cmml"><munderover id="S1.E1.m1.4.4.1.1.4.1a" xref="S1.E1.m1.4.4.1.1.4.1.cmml"><mo movablelimits="false" id="S1.E1.m1.4.4.1.1.4.1.2.2" xref="S1.E1.m1.4.4.1.1.4.1.2.2.cmml">∑</mo><mrow id="S1.E1.m1.4.4.1.1.4.1.2.3" xref="S1.E1.m1.4.4.1.1.4.1.2.3.cmml"><mi id="S1.E1.m1.4.4.1.1.4.1.2.3.2" xref="S1.E1.m1.4.4.1.1.4.1.2.3.2.cmml">k</mi><mo id="S1.E1.m1.4.4.1.1.4.1.2.3.1" xref="S1.E1.m1.4.4.1.1.4.1.2.3.1.cmml">=</mo><mn id="S1.E1.m1.4.4.1.1.4.1.2.3.3" xref="S1.E1.m1.4.4.1.1.4.1.2.3.3.cmml">1</mn></mrow><mi id="S1.E1.m1.4.4.1.1.4.1.3" xref="S1.E1.m1.4.4.1.1.4.1.3.cmml">m</mi></munderover></mstyle><mrow id="S1.E1.m1.4.4.1.1.4.2" xref="S1.E1.m1.4.4.1.1.4.2.cmml"><msub id="S1.E1.m1.4.4.1.1.4.2.2" xref="S1.E1.m1.4.4.1.1.4.2.2.cmml"><mi id="S1.E1.m1.4.4.1.1.4.2.2.2" xref="S1.E1.m1.4.4.1.1.4.2.2.2.cmml">p</mi><mi id="S1.E1.m1.4.4.1.1.4.2.2.3" xref="S1.E1.m1.4.4.1.1.4.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S1.E1.m1.4.4.1.1.4.2.1" xref="S1.E1.m1.4.4.1.1.4.2.1.cmml">​</mo><msub id="S1.E1.m1.4.4.1.1.4.2.3" xref="S1.E1.m1.4.4.1.1.4.2.3.cmml"><mi id="S1.E1.m1.4.4.1.1.4.2.3.2" xref="S1.E1.m1.4.4.1.1.4.2.3.2.cmml">F</mi><mi id="S1.E1.m1.4.4.1.1.4.2.3.3" xref="S1.E1.m1.4.4.1.1.4.2.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S1.E1.m1.4.4.1.1.4.2.1a" xref="S1.E1.m1.4.4.1.1.4.2.1.cmml">​</mo><mrow id="S1.E1.m1.4.4.1.1.4.2.4.2" xref="S1.E1.m1.4.4.1.1.4.2.cmml"><mo stretchy="false" id="S1.E1.m1.4.4.1.1.4.2.4.2.1" xref="S1.E1.m1.4.4.1.1.4.2.cmml">(</mo><mi id="S1.E1.m1.3.3" xref="S1.E1.m1.3.3.cmml">w</mi><mo stretchy="false" id="S1.E1.m1.4.4.1.1.4.2.4.2.2" xref="S1.E1.m1.4.4.1.1.4.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0.170em" id="S1.E1.m1.4.4.1.2" xref="S1.E1.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.E1.m1.4b"><apply id="S1.E1.m1.4.4.1.1.cmml" xref="S1.E1.m1.4.4.1"><csymbol cd="latexml" id="S1.E1.m1.4.4.1.1.3.cmml" xref="S1.E1.m1.4.4.1.1.3">assign</csymbol><list id="S1.E1.m1.4.4.1.1.2.3.cmml" xref="S1.E1.m1.4.4.1.1.2.2"><apply id="S1.E1.m1.4.4.1.1.1.1.1.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1"><times id="S1.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.1"></times><apply id="S1.E1.m1.4.4.1.1.1.1.1.2.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.2"><apply id="S1.E1.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S1.E1.m1.4.4.1.1.1.1.1.2.1.1.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.2.1">subscript</csymbol><min id="S1.E1.m1.4.4.1.1.1.1.1.2.1.2.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.2.1.2"></min><ci id="S1.E1.m1.4.4.1.1.1.1.1.2.1.3.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.2.1.3">𝑤</ci></apply><ci id="S1.E1.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S1.E1.m1.4.4.1.1.1.1.1.2.2">𝐹</ci></apply><ci id="S1.E1.m1.1.1.cmml" xref="S1.E1.m1.1.1">𝑤</ci></apply><apply id="S1.E1.m1.4.4.1.1.2.2.2.cmml" xref="S1.E1.m1.4.4.1.1.2.2.2"><times id="S1.E1.m1.4.4.1.1.2.2.2.1.cmml" xref="S1.E1.m1.4.4.1.1.2.2.2.1"></times><ci id="S1.E1.m1.4.4.1.1.2.2.2.2a.cmml" xref="S1.E1.m1.4.4.1.1.2.2.2.2"><mtext id="S1.E1.m1.4.4.1.1.2.2.2.2.cmml" xref="S1.E1.m1.4.4.1.1.2.2.2.2">where</mtext></ci><ci id="S1.E1.m1.4.4.1.1.2.2.2.3.cmml" xref="S1.E1.m1.4.4.1.1.2.2.2.3">𝐹</ci><ci id="S1.E1.m1.2.2.cmml" xref="S1.E1.m1.2.2">𝑤</ci></apply></list><apply id="S1.E1.m1.4.4.1.1.4.cmml" xref="S1.E1.m1.4.4.1.1.4"><apply id="S1.E1.m1.4.4.1.1.4.1.cmml" xref="S1.E1.m1.4.4.1.1.4.1"><csymbol cd="ambiguous" id="S1.E1.m1.4.4.1.1.4.1.1.cmml" xref="S1.E1.m1.4.4.1.1.4.1">superscript</csymbol><apply id="S1.E1.m1.4.4.1.1.4.1.2.cmml" xref="S1.E1.m1.4.4.1.1.4.1"><csymbol cd="ambiguous" id="S1.E1.m1.4.4.1.1.4.1.2.1.cmml" xref="S1.E1.m1.4.4.1.1.4.1">subscript</csymbol><sum id="S1.E1.m1.4.4.1.1.4.1.2.2.cmml" xref="S1.E1.m1.4.4.1.1.4.1.2.2"></sum><apply id="S1.E1.m1.4.4.1.1.4.1.2.3.cmml" xref="S1.E1.m1.4.4.1.1.4.1.2.3"><eq id="S1.E1.m1.4.4.1.1.4.1.2.3.1.cmml" xref="S1.E1.m1.4.4.1.1.4.1.2.3.1"></eq><ci id="S1.E1.m1.4.4.1.1.4.1.2.3.2.cmml" xref="S1.E1.m1.4.4.1.1.4.1.2.3.2">𝑘</ci><cn type="integer" id="S1.E1.m1.4.4.1.1.4.1.2.3.3.cmml" xref="S1.E1.m1.4.4.1.1.4.1.2.3.3">1</cn></apply></apply><ci id="S1.E1.m1.4.4.1.1.4.1.3.cmml" xref="S1.E1.m1.4.4.1.1.4.1.3">𝑚</ci></apply><apply id="S1.E1.m1.4.4.1.1.4.2.cmml" xref="S1.E1.m1.4.4.1.1.4.2"><times id="S1.E1.m1.4.4.1.1.4.2.1.cmml" xref="S1.E1.m1.4.4.1.1.4.2.1"></times><apply id="S1.E1.m1.4.4.1.1.4.2.2.cmml" xref="S1.E1.m1.4.4.1.1.4.2.2"><csymbol cd="ambiguous" id="S1.E1.m1.4.4.1.1.4.2.2.1.cmml" xref="S1.E1.m1.4.4.1.1.4.2.2">subscript</csymbol><ci id="S1.E1.m1.4.4.1.1.4.2.2.2.cmml" xref="S1.E1.m1.4.4.1.1.4.2.2.2">𝑝</ci><ci id="S1.E1.m1.4.4.1.1.4.2.2.3.cmml" xref="S1.E1.m1.4.4.1.1.4.2.2.3">𝑘</ci></apply><apply id="S1.E1.m1.4.4.1.1.4.2.3.cmml" xref="S1.E1.m1.4.4.1.1.4.2.3"><csymbol cd="ambiguous" id="S1.E1.m1.4.4.1.1.4.2.3.1.cmml" xref="S1.E1.m1.4.4.1.1.4.2.3">subscript</csymbol><ci id="S1.E1.m1.4.4.1.1.4.2.3.2.cmml" xref="S1.E1.m1.4.4.1.1.4.2.3.2">𝐹</ci><ci id="S1.E1.m1.4.4.1.1.4.2.3.3.cmml" xref="S1.E1.m1.4.4.1.1.4.2.3.3">𝑘</ci></apply><ci id="S1.E1.m1.3.3.cmml" xref="S1.E1.m1.3.3">𝑤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.E1.m1.4c">\displaystyle\min_{w}\,F(w)\,,\,\,\,\text{where}\,\,\,F(w):=\sum_{k=1}^{m}p_{k}F_{k}(w)\,.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S1.SS1.p1.11" class="ltx_p">Here, <math id="S1.SS1.p1.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S1.SS1.p1.1.m1.1a"><mi id="S1.SS1.p1.1.m1.1.1" xref="S1.SS1.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.1.m1.1b"><ci id="S1.SS1.p1.1.m1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.1.m1.1c">m</annotation></semantics></math> is the total number of devices, <math id="S1.SS1.p1.2.m2.1" class="ltx_Math" alttext="p_{k}\geq 0" display="inline"><semantics id="S1.SS1.p1.2.m2.1a"><mrow id="S1.SS1.p1.2.m2.1.1" xref="S1.SS1.p1.2.m2.1.1.cmml"><msub id="S1.SS1.p1.2.m2.1.1.2" xref="S1.SS1.p1.2.m2.1.1.2.cmml"><mi id="S1.SS1.p1.2.m2.1.1.2.2" xref="S1.SS1.p1.2.m2.1.1.2.2.cmml">p</mi><mi id="S1.SS1.p1.2.m2.1.1.2.3" xref="S1.SS1.p1.2.m2.1.1.2.3.cmml">k</mi></msub><mo id="S1.SS1.p1.2.m2.1.1.1" xref="S1.SS1.p1.2.m2.1.1.1.cmml">≥</mo><mn id="S1.SS1.p1.2.m2.1.1.3" xref="S1.SS1.p1.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.2.m2.1b"><apply id="S1.SS1.p1.2.m2.1.1.cmml" xref="S1.SS1.p1.2.m2.1.1"><geq id="S1.SS1.p1.2.m2.1.1.1.cmml" xref="S1.SS1.p1.2.m2.1.1.1"></geq><apply id="S1.SS1.p1.2.m2.1.1.2.cmml" xref="S1.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S1.SS1.p1.2.m2.1.1.2.1.cmml" xref="S1.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S1.SS1.p1.2.m2.1.1.2.2.cmml" xref="S1.SS1.p1.2.m2.1.1.2.2">𝑝</ci><ci id="S1.SS1.p1.2.m2.1.1.2.3.cmml" xref="S1.SS1.p1.2.m2.1.1.2.3">𝑘</ci></apply><cn type="integer" id="S1.SS1.p1.2.m2.1.1.3.cmml" xref="S1.SS1.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.2.m2.1c">p_{k}\geq 0</annotation></semantics></math> and <math id="S1.SS1.p1.3.m3.1" class="ltx_Math" alttext="\sum_{k}p_{k}=1" display="inline"><semantics id="S1.SS1.p1.3.m3.1a"><mrow id="S1.SS1.p1.3.m3.1.1" xref="S1.SS1.p1.3.m3.1.1.cmml"><mrow id="S1.SS1.p1.3.m3.1.1.2" xref="S1.SS1.p1.3.m3.1.1.2.cmml"><msub id="S1.SS1.p1.3.m3.1.1.2.1" xref="S1.SS1.p1.3.m3.1.1.2.1.cmml"><mo id="S1.SS1.p1.3.m3.1.1.2.1.2" xref="S1.SS1.p1.3.m3.1.1.2.1.2.cmml">∑</mo><mi id="S1.SS1.p1.3.m3.1.1.2.1.3" xref="S1.SS1.p1.3.m3.1.1.2.1.3.cmml">k</mi></msub><msub id="S1.SS1.p1.3.m3.1.1.2.2" xref="S1.SS1.p1.3.m3.1.1.2.2.cmml"><mi id="S1.SS1.p1.3.m3.1.1.2.2.2" xref="S1.SS1.p1.3.m3.1.1.2.2.2.cmml">p</mi><mi id="S1.SS1.p1.3.m3.1.1.2.2.3" xref="S1.SS1.p1.3.m3.1.1.2.2.3.cmml">k</mi></msub></mrow><mo id="S1.SS1.p1.3.m3.1.1.1" xref="S1.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S1.SS1.p1.3.m3.1.1.3" xref="S1.SS1.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.3.m3.1b"><apply id="S1.SS1.p1.3.m3.1.1.cmml" xref="S1.SS1.p1.3.m3.1.1"><eq id="S1.SS1.p1.3.m3.1.1.1.cmml" xref="S1.SS1.p1.3.m3.1.1.1"></eq><apply id="S1.SS1.p1.3.m3.1.1.2.cmml" xref="S1.SS1.p1.3.m3.1.1.2"><apply id="S1.SS1.p1.3.m3.1.1.2.1.cmml" xref="S1.SS1.p1.3.m3.1.1.2.1"><csymbol cd="ambiguous" id="S1.SS1.p1.3.m3.1.1.2.1.1.cmml" xref="S1.SS1.p1.3.m3.1.1.2.1">subscript</csymbol><sum id="S1.SS1.p1.3.m3.1.1.2.1.2.cmml" xref="S1.SS1.p1.3.m3.1.1.2.1.2"></sum><ci id="S1.SS1.p1.3.m3.1.1.2.1.3.cmml" xref="S1.SS1.p1.3.m3.1.1.2.1.3">𝑘</ci></apply><apply id="S1.SS1.p1.3.m3.1.1.2.2.cmml" xref="S1.SS1.p1.3.m3.1.1.2.2"><csymbol cd="ambiguous" id="S1.SS1.p1.3.m3.1.1.2.2.1.cmml" xref="S1.SS1.p1.3.m3.1.1.2.2">subscript</csymbol><ci id="S1.SS1.p1.3.m3.1.1.2.2.2.cmml" xref="S1.SS1.p1.3.m3.1.1.2.2.2">𝑝</ci><ci id="S1.SS1.p1.3.m3.1.1.2.2.3.cmml" xref="S1.SS1.p1.3.m3.1.1.2.2.3">𝑘</ci></apply></apply><cn type="integer" id="S1.SS1.p1.3.m3.1.1.3.cmml" xref="S1.SS1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.3.m3.1c">\sum_{k}p_{k}=1</annotation></semantics></math>, and <math id="S1.SS1.p1.4.m4.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="S1.SS1.p1.4.m4.1a"><msub id="S1.SS1.p1.4.m4.1.1" xref="S1.SS1.p1.4.m4.1.1.cmml"><mi id="S1.SS1.p1.4.m4.1.1.2" xref="S1.SS1.p1.4.m4.1.1.2.cmml">F</mi><mi id="S1.SS1.p1.4.m4.1.1.3" xref="S1.SS1.p1.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.4.m4.1b"><apply id="S1.SS1.p1.4.m4.1.1.cmml" xref="S1.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S1.SS1.p1.4.m4.1.1.1.cmml" xref="S1.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S1.SS1.p1.4.m4.1.1.2.cmml" xref="S1.SS1.p1.4.m4.1.1.2">𝐹</ci><ci id="S1.SS1.p1.4.m4.1.1.3.cmml" xref="S1.SS1.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.4.m4.1c">F_{k}</annotation></semantics></math> is the local objective function for the <math id="S1.SS1.p1.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.SS1.p1.5.m5.1a"><mi id="S1.SS1.p1.5.m5.1.1" xref="S1.SS1.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.5.m5.1b"><ci id="S1.SS1.p1.5.m5.1.1.cmml" xref="S1.SS1.p1.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.5.m5.1c">k</annotation></semantics></math>th device. The local objective function is often defined as the empirical risk over local data, i.e., <math id="S1.SS1.p1.6.m6.4" class="ltx_Math" alttext="F_{k}(w)=\frac{1}{n_{k}}\sum_{j_{k}=1}^{n_{k}}f_{j_{k}}(w;x_{j_{k}},y_{j_{k}})" display="inline"><semantics id="S1.SS1.p1.6.m6.4a"><mrow id="S1.SS1.p1.6.m6.4.4" xref="S1.SS1.p1.6.m6.4.4.cmml"><mrow id="S1.SS1.p1.6.m6.4.4.4" xref="S1.SS1.p1.6.m6.4.4.4.cmml"><msub id="S1.SS1.p1.6.m6.4.4.4.2" xref="S1.SS1.p1.6.m6.4.4.4.2.cmml"><mi id="S1.SS1.p1.6.m6.4.4.4.2.2" xref="S1.SS1.p1.6.m6.4.4.4.2.2.cmml">F</mi><mi id="S1.SS1.p1.6.m6.4.4.4.2.3" xref="S1.SS1.p1.6.m6.4.4.4.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S1.SS1.p1.6.m6.4.4.4.1" xref="S1.SS1.p1.6.m6.4.4.4.1.cmml">​</mo><mrow id="S1.SS1.p1.6.m6.4.4.4.3.2" xref="S1.SS1.p1.6.m6.4.4.4.cmml"><mo stretchy="false" id="S1.SS1.p1.6.m6.4.4.4.3.2.1" xref="S1.SS1.p1.6.m6.4.4.4.cmml">(</mo><mi id="S1.SS1.p1.6.m6.1.1" xref="S1.SS1.p1.6.m6.1.1.cmml">w</mi><mo stretchy="false" id="S1.SS1.p1.6.m6.4.4.4.3.2.2" xref="S1.SS1.p1.6.m6.4.4.4.cmml">)</mo></mrow></mrow><mo id="S1.SS1.p1.6.m6.4.4.3" xref="S1.SS1.p1.6.m6.4.4.3.cmml">=</mo><mrow id="S1.SS1.p1.6.m6.4.4.2" xref="S1.SS1.p1.6.m6.4.4.2.cmml"><mfrac id="S1.SS1.p1.6.m6.4.4.2.4" xref="S1.SS1.p1.6.m6.4.4.2.4.cmml"><mn id="S1.SS1.p1.6.m6.4.4.2.4.2" xref="S1.SS1.p1.6.m6.4.4.2.4.2.cmml">1</mn><msub id="S1.SS1.p1.6.m6.4.4.2.4.3" xref="S1.SS1.p1.6.m6.4.4.2.4.3.cmml"><mi id="S1.SS1.p1.6.m6.4.4.2.4.3.2" xref="S1.SS1.p1.6.m6.4.4.2.4.3.2.cmml">n</mi><mi id="S1.SS1.p1.6.m6.4.4.2.4.3.3" xref="S1.SS1.p1.6.m6.4.4.2.4.3.3.cmml">k</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S1.SS1.p1.6.m6.4.4.2.3" xref="S1.SS1.p1.6.m6.4.4.2.3.cmml">​</mo><mrow id="S1.SS1.p1.6.m6.4.4.2.2" xref="S1.SS1.p1.6.m6.4.4.2.2.cmml"><msubsup id="S1.SS1.p1.6.m6.4.4.2.2.3" xref="S1.SS1.p1.6.m6.4.4.2.2.3.cmml"><mo id="S1.SS1.p1.6.m6.4.4.2.2.3.2.2" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.2.cmml">∑</mo><mrow id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.cmml"><msub id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.cmml"><mi id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.2" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.2.cmml">j</mi><mi id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.3" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.3.cmml">k</mi></msub><mo id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.1" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.1.cmml">=</mo><mn id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.3" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.3.cmml">1</mn></mrow><msub id="S1.SS1.p1.6.m6.4.4.2.2.3.3" xref="S1.SS1.p1.6.m6.4.4.2.2.3.3.cmml"><mi id="S1.SS1.p1.6.m6.4.4.2.2.3.3.2" xref="S1.SS1.p1.6.m6.4.4.2.2.3.3.2.cmml">n</mi><mi id="S1.SS1.p1.6.m6.4.4.2.2.3.3.3" xref="S1.SS1.p1.6.m6.4.4.2.2.3.3.3.cmml">k</mi></msub></msubsup><mrow id="S1.SS1.p1.6.m6.4.4.2.2.2" xref="S1.SS1.p1.6.m6.4.4.2.2.2.cmml"><msub id="S1.SS1.p1.6.m6.4.4.2.2.2.4" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.cmml"><mi id="S1.SS1.p1.6.m6.4.4.2.2.2.4.2" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.2.cmml">f</mi><msub id="S1.SS1.p1.6.m6.4.4.2.2.2.4.3" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.cmml"><mi id="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.2" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.2.cmml">j</mi><mi id="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.3" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.3.cmml">k</mi></msub></msub><mo lspace="0em" rspace="0em" id="S1.SS1.p1.6.m6.4.4.2.2.2.3" xref="S1.SS1.p1.6.m6.4.4.2.2.2.3.cmml">​</mo><mrow id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.3.cmml"><mo stretchy="false" id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.3" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.3.cmml">(</mo><mi id="S1.SS1.p1.6.m6.2.2" xref="S1.SS1.p1.6.m6.2.2.cmml">w</mi><mo id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.4" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.3.cmml">;</mo><msub id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.cmml"><mi id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.2" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.2.cmml">x</mi><msub id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.cmml"><mi id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.2" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.2.cmml">j</mi><mi id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.3" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.3.cmml">k</mi></msub></msub><mo id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.5" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.3.cmml">,</mo><msub id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.cmml"><mi id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.2" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.2.cmml">y</mi><msub id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.cmml"><mi id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.2" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.2.cmml">j</mi><mi id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.3" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.3.cmml">k</mi></msub></msub><mo stretchy="false" id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.6" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.6.m6.4b"><apply id="S1.SS1.p1.6.m6.4.4.cmml" xref="S1.SS1.p1.6.m6.4.4"><eq id="S1.SS1.p1.6.m6.4.4.3.cmml" xref="S1.SS1.p1.6.m6.4.4.3"></eq><apply id="S1.SS1.p1.6.m6.4.4.4.cmml" xref="S1.SS1.p1.6.m6.4.4.4"><times id="S1.SS1.p1.6.m6.4.4.4.1.cmml" xref="S1.SS1.p1.6.m6.4.4.4.1"></times><apply id="S1.SS1.p1.6.m6.4.4.4.2.cmml" xref="S1.SS1.p1.6.m6.4.4.4.2"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.4.2.1.cmml" xref="S1.SS1.p1.6.m6.4.4.4.2">subscript</csymbol><ci id="S1.SS1.p1.6.m6.4.4.4.2.2.cmml" xref="S1.SS1.p1.6.m6.4.4.4.2.2">𝐹</ci><ci id="S1.SS1.p1.6.m6.4.4.4.2.3.cmml" xref="S1.SS1.p1.6.m6.4.4.4.2.3">𝑘</ci></apply><ci id="S1.SS1.p1.6.m6.1.1.cmml" xref="S1.SS1.p1.6.m6.1.1">𝑤</ci></apply><apply id="S1.SS1.p1.6.m6.4.4.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2"><times id="S1.SS1.p1.6.m6.4.4.2.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.3"></times><apply id="S1.SS1.p1.6.m6.4.4.2.4.cmml" xref="S1.SS1.p1.6.m6.4.4.2.4"><divide id="S1.SS1.p1.6.m6.4.4.2.4.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.4"></divide><cn type="integer" id="S1.SS1.p1.6.m6.4.4.2.4.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.4.2">1</cn><apply id="S1.SS1.p1.6.m6.4.4.2.4.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.4.3"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.4.3.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.4.3">subscript</csymbol><ci id="S1.SS1.p1.6.m6.4.4.2.4.3.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.4.3.2">𝑛</ci><ci id="S1.SS1.p1.6.m6.4.4.2.4.3.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.4.3.3">𝑘</ci></apply></apply><apply id="S1.SS1.p1.6.m6.4.4.2.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2"><apply id="S1.SS1.p1.6.m6.4.4.2.2.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.2.3.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3">superscript</csymbol><apply id="S1.SS1.p1.6.m6.4.4.2.2.3.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.2.3.2.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3">subscript</csymbol><sum id="S1.SS1.p1.6.m6.4.4.2.2.3.2.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.2"></sum><apply id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3"><eq id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.1"></eq><apply id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2">subscript</csymbol><ci id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.2">𝑗</ci><ci id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.2.3">𝑘</ci></apply><cn type="integer" id="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.2.3.3">1</cn></apply></apply><apply id="S1.SS1.p1.6.m6.4.4.2.2.3.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.3"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.2.3.3.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.3">subscript</csymbol><ci id="S1.SS1.p1.6.m6.4.4.2.2.3.3.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.3.2">𝑛</ci><ci id="S1.SS1.p1.6.m6.4.4.2.2.3.3.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.3.3.3">𝑘</ci></apply></apply><apply id="S1.SS1.p1.6.m6.4.4.2.2.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2"><times id="S1.SS1.p1.6.m6.4.4.2.2.2.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.3"></times><apply id="S1.SS1.p1.6.m6.4.4.2.2.2.4.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.2.2.4.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4">subscript</csymbol><ci id="S1.SS1.p1.6.m6.4.4.2.2.2.4.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.2">𝑓</ci><apply id="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.3"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.3">subscript</csymbol><ci id="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.2">𝑗</ci><ci id="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.4.3.3">𝑘</ci></apply></apply><list id="S1.SS1.p1.6.m6.4.4.2.2.2.2.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2"><ci id="S1.SS1.p1.6.m6.2.2.cmml" xref="S1.SS1.p1.6.m6.2.2">𝑤</ci><apply id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.cmml" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.1.cmml" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.2.cmml" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.2">𝑥</ci><apply id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.cmml" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.1.cmml" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.2.cmml" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.2">𝑗</ci><ci id="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.3.cmml" xref="S1.SS1.p1.6.m6.3.3.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply><apply id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2">subscript</csymbol><ci id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.2">𝑦</ci><apply id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.1.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3">subscript</csymbol><ci id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.2.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.2">𝑗</ci><ci id="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.3.cmml" xref="S1.SS1.p1.6.m6.4.4.2.2.2.2.2.2.3.3">𝑘</ci></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.6.m6.4c">F_{k}(w)=\frac{1}{n_{k}}\sum_{j_{k}=1}^{n_{k}}f_{j_{k}}(w;x_{j_{k}},y_{j_{k}})</annotation></semantics></math>, where <math id="S1.SS1.p1.7.m7.1" class="ltx_Math" alttext="n_{k}" display="inline"><semantics id="S1.SS1.p1.7.m7.1a"><msub id="S1.SS1.p1.7.m7.1.1" xref="S1.SS1.p1.7.m7.1.1.cmml"><mi id="S1.SS1.p1.7.m7.1.1.2" xref="S1.SS1.p1.7.m7.1.1.2.cmml">n</mi><mi id="S1.SS1.p1.7.m7.1.1.3" xref="S1.SS1.p1.7.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.7.m7.1b"><apply id="S1.SS1.p1.7.m7.1.1.cmml" xref="S1.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S1.SS1.p1.7.m7.1.1.1.cmml" xref="S1.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S1.SS1.p1.7.m7.1.1.2.cmml" xref="S1.SS1.p1.7.m7.1.1.2">𝑛</ci><ci id="S1.SS1.p1.7.m7.1.1.3.cmml" xref="S1.SS1.p1.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.7.m7.1c">n_{k}</annotation></semantics></math> is the number of samples available locally. The user-defined term <math id="S1.SS1.p1.8.m8.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="S1.SS1.p1.8.m8.1a"><msub id="S1.SS1.p1.8.m8.1.1" xref="S1.SS1.p1.8.m8.1.1.cmml"><mi id="S1.SS1.p1.8.m8.1.1.2" xref="S1.SS1.p1.8.m8.1.1.2.cmml">p</mi><mi id="S1.SS1.p1.8.m8.1.1.3" xref="S1.SS1.p1.8.m8.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.8.m8.1b"><apply id="S1.SS1.p1.8.m8.1.1.cmml" xref="S1.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S1.SS1.p1.8.m8.1.1.1.cmml" xref="S1.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S1.SS1.p1.8.m8.1.1.2.cmml" xref="S1.SS1.p1.8.m8.1.1.2">𝑝</ci><ci id="S1.SS1.p1.8.m8.1.1.3.cmml" xref="S1.SS1.p1.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.8.m8.1c">p_{k}</annotation></semantics></math> specifies the relative impact of each device, with two natural settings being <math id="S1.SS1.p1.9.m9.1" class="ltx_Math" alttext="p_{k}=\frac{1}{n}" display="inline"><semantics id="S1.SS1.p1.9.m9.1a"><mrow id="S1.SS1.p1.9.m9.1.1" xref="S1.SS1.p1.9.m9.1.1.cmml"><msub id="S1.SS1.p1.9.m9.1.1.2" xref="S1.SS1.p1.9.m9.1.1.2.cmml"><mi id="S1.SS1.p1.9.m9.1.1.2.2" xref="S1.SS1.p1.9.m9.1.1.2.2.cmml">p</mi><mi id="S1.SS1.p1.9.m9.1.1.2.3" xref="S1.SS1.p1.9.m9.1.1.2.3.cmml">k</mi></msub><mo id="S1.SS1.p1.9.m9.1.1.1" xref="S1.SS1.p1.9.m9.1.1.1.cmml">=</mo><mfrac id="S1.SS1.p1.9.m9.1.1.3" xref="S1.SS1.p1.9.m9.1.1.3.cmml"><mn id="S1.SS1.p1.9.m9.1.1.3.2" xref="S1.SS1.p1.9.m9.1.1.3.2.cmml">1</mn><mi id="S1.SS1.p1.9.m9.1.1.3.3" xref="S1.SS1.p1.9.m9.1.1.3.3.cmml">n</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.9.m9.1b"><apply id="S1.SS1.p1.9.m9.1.1.cmml" xref="S1.SS1.p1.9.m9.1.1"><eq id="S1.SS1.p1.9.m9.1.1.1.cmml" xref="S1.SS1.p1.9.m9.1.1.1"></eq><apply id="S1.SS1.p1.9.m9.1.1.2.cmml" xref="S1.SS1.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S1.SS1.p1.9.m9.1.1.2.1.cmml" xref="S1.SS1.p1.9.m9.1.1.2">subscript</csymbol><ci id="S1.SS1.p1.9.m9.1.1.2.2.cmml" xref="S1.SS1.p1.9.m9.1.1.2.2">𝑝</ci><ci id="S1.SS1.p1.9.m9.1.1.2.3.cmml" xref="S1.SS1.p1.9.m9.1.1.2.3">𝑘</ci></apply><apply id="S1.SS1.p1.9.m9.1.1.3.cmml" xref="S1.SS1.p1.9.m9.1.1.3"><divide id="S1.SS1.p1.9.m9.1.1.3.1.cmml" xref="S1.SS1.p1.9.m9.1.1.3"></divide><cn type="integer" id="S1.SS1.p1.9.m9.1.1.3.2.cmml" xref="S1.SS1.p1.9.m9.1.1.3.2">1</cn><ci id="S1.SS1.p1.9.m9.1.1.3.3.cmml" xref="S1.SS1.p1.9.m9.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.9.m9.1c">p_{k}=\frac{1}{n}</annotation></semantics></math> or <math id="S1.SS1.p1.10.m10.1" class="ltx_Math" alttext="p_{k}=\frac{n_{k}}{n}" display="inline"><semantics id="S1.SS1.p1.10.m10.1a"><mrow id="S1.SS1.p1.10.m10.1.1" xref="S1.SS1.p1.10.m10.1.1.cmml"><msub id="S1.SS1.p1.10.m10.1.1.2" xref="S1.SS1.p1.10.m10.1.1.2.cmml"><mi id="S1.SS1.p1.10.m10.1.1.2.2" xref="S1.SS1.p1.10.m10.1.1.2.2.cmml">p</mi><mi id="S1.SS1.p1.10.m10.1.1.2.3" xref="S1.SS1.p1.10.m10.1.1.2.3.cmml">k</mi></msub><mo id="S1.SS1.p1.10.m10.1.1.1" xref="S1.SS1.p1.10.m10.1.1.1.cmml">=</mo><mfrac id="S1.SS1.p1.10.m10.1.1.3" xref="S1.SS1.p1.10.m10.1.1.3.cmml"><msub id="S1.SS1.p1.10.m10.1.1.3.2" xref="S1.SS1.p1.10.m10.1.1.3.2.cmml"><mi id="S1.SS1.p1.10.m10.1.1.3.2.2" xref="S1.SS1.p1.10.m10.1.1.3.2.2.cmml">n</mi><mi id="S1.SS1.p1.10.m10.1.1.3.2.3" xref="S1.SS1.p1.10.m10.1.1.3.2.3.cmml">k</mi></msub><mi id="S1.SS1.p1.10.m10.1.1.3.3" xref="S1.SS1.p1.10.m10.1.1.3.3.cmml">n</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.10.m10.1b"><apply id="S1.SS1.p1.10.m10.1.1.cmml" xref="S1.SS1.p1.10.m10.1.1"><eq id="S1.SS1.p1.10.m10.1.1.1.cmml" xref="S1.SS1.p1.10.m10.1.1.1"></eq><apply id="S1.SS1.p1.10.m10.1.1.2.cmml" xref="S1.SS1.p1.10.m10.1.1.2"><csymbol cd="ambiguous" id="S1.SS1.p1.10.m10.1.1.2.1.cmml" xref="S1.SS1.p1.10.m10.1.1.2">subscript</csymbol><ci id="S1.SS1.p1.10.m10.1.1.2.2.cmml" xref="S1.SS1.p1.10.m10.1.1.2.2">𝑝</ci><ci id="S1.SS1.p1.10.m10.1.1.2.3.cmml" xref="S1.SS1.p1.10.m10.1.1.2.3">𝑘</ci></apply><apply id="S1.SS1.p1.10.m10.1.1.3.cmml" xref="S1.SS1.p1.10.m10.1.1.3"><divide id="S1.SS1.p1.10.m10.1.1.3.1.cmml" xref="S1.SS1.p1.10.m10.1.1.3"></divide><apply id="S1.SS1.p1.10.m10.1.1.3.2.cmml" xref="S1.SS1.p1.10.m10.1.1.3.2"><csymbol cd="ambiguous" id="S1.SS1.p1.10.m10.1.1.3.2.1.cmml" xref="S1.SS1.p1.10.m10.1.1.3.2">subscript</csymbol><ci id="S1.SS1.p1.10.m10.1.1.3.2.2.cmml" xref="S1.SS1.p1.10.m10.1.1.3.2.2">𝑛</ci><ci id="S1.SS1.p1.10.m10.1.1.3.2.3.cmml" xref="S1.SS1.p1.10.m10.1.1.3.2.3">𝑘</ci></apply><ci id="S1.SS1.p1.10.m10.1.1.3.3.cmml" xref="S1.SS1.p1.10.m10.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.10.m10.1c">p_{k}=\frac{n_{k}}{n}</annotation></semantics></math>, where <math id="S1.SS1.p1.11.m11.1" class="ltx_Math" alttext="n=\sum_{k}n_{k}" display="inline"><semantics id="S1.SS1.p1.11.m11.1a"><mrow id="S1.SS1.p1.11.m11.1.1" xref="S1.SS1.p1.11.m11.1.1.cmml"><mi id="S1.SS1.p1.11.m11.1.1.2" xref="S1.SS1.p1.11.m11.1.1.2.cmml">n</mi><mo rspace="0.111em" id="S1.SS1.p1.11.m11.1.1.1" xref="S1.SS1.p1.11.m11.1.1.1.cmml">=</mo><mrow id="S1.SS1.p1.11.m11.1.1.3" xref="S1.SS1.p1.11.m11.1.1.3.cmml"><msub id="S1.SS1.p1.11.m11.1.1.3.1" xref="S1.SS1.p1.11.m11.1.1.3.1.cmml"><mo id="S1.SS1.p1.11.m11.1.1.3.1.2" xref="S1.SS1.p1.11.m11.1.1.3.1.2.cmml">∑</mo><mi id="S1.SS1.p1.11.m11.1.1.3.1.3" xref="S1.SS1.p1.11.m11.1.1.3.1.3.cmml">k</mi></msub><msub id="S1.SS1.p1.11.m11.1.1.3.2" xref="S1.SS1.p1.11.m11.1.1.3.2.cmml"><mi id="S1.SS1.p1.11.m11.1.1.3.2.2" xref="S1.SS1.p1.11.m11.1.1.3.2.2.cmml">n</mi><mi id="S1.SS1.p1.11.m11.1.1.3.2.3" xref="S1.SS1.p1.11.m11.1.1.3.2.3.cmml">k</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.11.m11.1b"><apply id="S1.SS1.p1.11.m11.1.1.cmml" xref="S1.SS1.p1.11.m11.1.1"><eq id="S1.SS1.p1.11.m11.1.1.1.cmml" xref="S1.SS1.p1.11.m11.1.1.1"></eq><ci id="S1.SS1.p1.11.m11.1.1.2.cmml" xref="S1.SS1.p1.11.m11.1.1.2">𝑛</ci><apply id="S1.SS1.p1.11.m11.1.1.3.cmml" xref="S1.SS1.p1.11.m11.1.1.3"><apply id="S1.SS1.p1.11.m11.1.1.3.1.cmml" xref="S1.SS1.p1.11.m11.1.1.3.1"><csymbol cd="ambiguous" id="S1.SS1.p1.11.m11.1.1.3.1.1.cmml" xref="S1.SS1.p1.11.m11.1.1.3.1">subscript</csymbol><sum id="S1.SS1.p1.11.m11.1.1.3.1.2.cmml" xref="S1.SS1.p1.11.m11.1.1.3.1.2"></sum><ci id="S1.SS1.p1.11.m11.1.1.3.1.3.cmml" xref="S1.SS1.p1.11.m11.1.1.3.1.3">𝑘</ci></apply><apply id="S1.SS1.p1.11.m11.1.1.3.2.cmml" xref="S1.SS1.p1.11.m11.1.1.3.2"><csymbol cd="ambiguous" id="S1.SS1.p1.11.m11.1.1.3.2.1.cmml" xref="S1.SS1.p1.11.m11.1.1.3.2">subscript</csymbol><ci id="S1.SS1.p1.11.m11.1.1.3.2.2.cmml" xref="S1.SS1.p1.11.m11.1.1.3.2.2">𝑛</ci><ci id="S1.SS1.p1.11.m11.1.1.3.2.3.cmml" xref="S1.SS1.p1.11.m11.1.1.3.2.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.11.m11.1c">n=\sum_{k}n_{k}</annotation></semantics></math> is the total number of samples. We will reference problem (<a href="#S1.E1" title="In 1.1 Problem Formulation ‣ 1 Introduction ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) throughout the article, but, as discussed below, we note that other objectives or modeling approaches may be appropriate depending on the application of interest.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Core Challenges</h3>

<div id="S1.SS2.p1" class="ltx_para ltx_noindent">
<p id="S1.SS2.p1.1" class="ltx_p">We next describe four of the core challenges associated with solving the distributed optimization problem posed in (<a href="#S1.E1" title="In 1.1 Problem Formulation ‣ 1 Introduction ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). These challenges make the federated setting distinct from other classical problems, such as distributed learning in data center settings or traditional private data analyses.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para ltx_noindent">
<p id="S1.SS2.p2.1" class="ltx_p"><span id="S1.SS2.p2.1.1" class="ltx_text ltx_font_bold">Challenge 1: Expensive Communication.</span>
Communication is a critical bottleneck in federated networks, which, coupled with privacy concerns over sending raw data, necessitates that data generated on each device remain local.
Indeed, federated networks are potentially comprised of a massive number of devices, e.g., millions of smart phones, and communication in the network can be slower than local computation by many orders of magnitude <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">115</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>.
In order to fit a model to data generated by the devices in the federated network, it is therefore necessary to develop communication-efficient methods that iteratively send small messages or <span id="S1.SS2.p2.1.2" class="ltx_text ltx_font_italic">model updates</span> as part of the training process, as opposed to sending the entire dataset over the network. To further reduce communication in such a setting, two key aspects to consider are: (i) reducing the total number of communication rounds, or (ii) reducing the size of transmitted messages at each round.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para ltx_noindent">
<p id="S1.SS2.p3.1" class="ltx_p"><span id="S1.SS2.p3.1.1" class="ltx_text ltx_font_bold">Challenge 2: Systems Heterogeneity.</span> The storage, computational, and communication capabilities of each device in federated networks may differ due to variability in hardware (CPU, memory), network connectivity (3G, 4G, 5G, wifi), and power (battery level). Additionally, the network size and systems-related constraints on each device typically result in only a small fraction of the devices being active at once, e.g., hundreds of active devices in a million-device network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>. Each device may also be unreliable, and it is not uncommon for an active device to drop out at a given iteration due to connectivity or energy constraints. These system-level characteristics dramatically exacerbate challenges such as straggler mitigation and fault tolerance. Federated learning methods that are developed and analyzed must therefore: (i) anticipate a low amount of participation, (ii) tolerate heterogeneous hardware, and (iii) be robust to dropped devices in the network.</p>
</div>
<div id="S1.SS2.p4" class="ltx_para ltx_noindent">
<p id="S1.SS2.p4.1" class="ltx_p"><span id="S1.SS2.p4.1.1" class="ltx_text ltx_font_bold">Challenge 3: Statistical Heterogeneity.</span> Devices frequently generate and collect data in a non-identically distributed manner across the network, e.g., mobile phone users have varied use of language in the context of a next word prediction task. Moreover, the number of data points across devices may vary significantly, and there may be an underlying structure present that captures the relationship amongst devices and their associated distributions.
This data generation paradigm violates frequently-used independent and identically distributed (I.I.D.) assumptions in distributed optimization, increases the likelihood of stragglers, and may add complexity in terms of modeling, analysis, and evaluation.
Indeed, although the canonical federated learning problem of (<a href="#S1.E1" title="In 1.1 Problem Formulation ‣ 1 Introduction ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) aims to learn a single global model,
there exist other alternatives such as simultaneously learning distinct local models
via multi-task learning frameworks <cite class="ltx_cite ltx_citemacro_cite">[cf. <a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">106</span></a>]</cite>. There is also a close connection in this regard between leading approaches for federated learning and meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>. Both the multi-task and meta-learning perspectives enable <em id="S1.SS2.p4.1.2" class="ltx_emph ltx_font_italic">personalized</em> or <em id="S1.SS2.p4.1.3" class="ltx_emph ltx_font_italic">device-specific</em> modeling, which is often a more natural approach to handle the statistical heterogeneity of the data.</p>
</div>
<div id="S1.SS2.p5" class="ltx_para ltx_noindent">
<p id="S1.SS2.p5.1" class="ltx_p"><span id="S1.SS2.p5.1.1" class="ltx_text ltx_font_bold">Challenge 4: Privacy Concerns.</span> Finally, privacy is often a major concern in federated learning applications. Federated learning makes a step towards protecting data generated on each device by sharing model updates, e.g., gradient information, instead of the raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>. However, communicating model updates throughout the training process can nonetheless reveal sensitive information, either to a third-party, or to the central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>. While recent methods aim to enhance the privacy of federated learning using tools such as secure multiparty computation or differential privacy, these approaches often provide privacy at the cost of reduced model performance or system efficiency. Understanding and balancing these trade-offs, both theoretically and empirically, is a considerable challenge in realizing private federated learning systems.</p>
</div>
<div id="S1.SS2.p6" class="ltx_para ltx_noindent">
<p id="S1.SS2.p6.1" class="ltx_p">The remainder of this article is organized as follows. In Section <a href="#S2" title="2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we introduce previous and current works that aim to address the four discussed challenges of federated learning. In Section <a href="#S3" title="3 Future Directions ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we outline several promising directions of future research.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Survey of Related and Current Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">The challenges in federated learning at first glance resemble classical problems in areas such as privacy, large-scale machine learning, and distributed optimization. For instance, numerous methods have been proposed to tackle expensive communication in the machine learning, optimization, and signal processing communities. However, these methods are typically unable to fully handle the scale of federated networks, much less the challenges of systems and statistical heterogeneity.
Similarly, while privacy is an important aspect for many machine learning applications, privacy-preserving methods for federated learning can be challenging to rigorously assert due to the statistical variation in the data, and may be even more difficult to implement due to systems constraints on each device and across the potentially massive network.
In this section, we explore in more detail the challenges presented in Section <a href="#S1" title="1 Introduction ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, including a discussion of classical results
as well as more recent work focused specifically on federated learning.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Communication-efficiency</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Communication is a key bottleneck to consider when developing methods for federated networks. While it is beyond the scope of this article to provide a self-contained review of communication-efficient distributed learning methods, we point out several general directions, which we group into (1) local updating methods, (2) compression schemes, and (3) decentralized training.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Local Updating</h4>

<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="148" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x3.png" id="S2.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="206" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.10.3.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.6.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Left: Distributed (mini-batch) SGD.<span id="S2.F2.6.2.2" class="ltx_text ltx_font_upright"> Each device, <math id="S2.F2.5.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.F2.5.1.1.m1.1b"><mi id="S2.F2.5.1.1.m1.1.1" xref="S2.F2.5.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.F2.5.1.1.m1.1c"><ci id="S2.F2.5.1.1.m1.1.1.cmml" xref="S2.F2.5.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.1.1.m1.1d">k</annotation></semantics></math>, locally computes gradients from a mini-batch of data points to approximate <math id="S2.F2.6.2.2.m2.1" class="ltx_Math" alttext="\nabla F_{k}(w)" display="inline"><semantics id="S2.F2.6.2.2.m2.1b"><mrow id="S2.F2.6.2.2.m2.1.2" xref="S2.F2.6.2.2.m2.1.2.cmml"><mrow id="S2.F2.6.2.2.m2.1.2.2" xref="S2.F2.6.2.2.m2.1.2.2.cmml"><mo rspace="0.167em" id="S2.F2.6.2.2.m2.1.2.2.1" xref="S2.F2.6.2.2.m2.1.2.2.1.cmml">∇</mo><msub id="S2.F2.6.2.2.m2.1.2.2.2" xref="S2.F2.6.2.2.m2.1.2.2.2.cmml"><mi id="S2.F2.6.2.2.m2.1.2.2.2.2" xref="S2.F2.6.2.2.m2.1.2.2.2.2.cmml">F</mi><mi id="S2.F2.6.2.2.m2.1.2.2.2.3" xref="S2.F2.6.2.2.m2.1.2.2.2.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.F2.6.2.2.m2.1.2.1" xref="S2.F2.6.2.2.m2.1.2.1.cmml">​</mo><mrow id="S2.F2.6.2.2.m2.1.2.3.2" xref="S2.F2.6.2.2.m2.1.2.cmml"><mo stretchy="false" id="S2.F2.6.2.2.m2.1.2.3.2.1" xref="S2.F2.6.2.2.m2.1.2.cmml">(</mo><mi id="S2.F2.6.2.2.m2.1.1" xref="S2.F2.6.2.2.m2.1.1.cmml">w</mi><mo stretchy="false" id="S2.F2.6.2.2.m2.1.2.3.2.2" xref="S2.F2.6.2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.6.2.2.m2.1c"><apply id="S2.F2.6.2.2.m2.1.2.cmml" xref="S2.F2.6.2.2.m2.1.2"><times id="S2.F2.6.2.2.m2.1.2.1.cmml" xref="S2.F2.6.2.2.m2.1.2.1"></times><apply id="S2.F2.6.2.2.m2.1.2.2.cmml" xref="S2.F2.6.2.2.m2.1.2.2"><ci id="S2.F2.6.2.2.m2.1.2.2.1.cmml" xref="S2.F2.6.2.2.m2.1.2.2.1">∇</ci><apply id="S2.F2.6.2.2.m2.1.2.2.2.cmml" xref="S2.F2.6.2.2.m2.1.2.2.2"><csymbol cd="ambiguous" id="S2.F2.6.2.2.m2.1.2.2.2.1.cmml" xref="S2.F2.6.2.2.m2.1.2.2.2">subscript</csymbol><ci id="S2.F2.6.2.2.m2.1.2.2.2.2.cmml" xref="S2.F2.6.2.2.m2.1.2.2.2.2">𝐹</ci><ci id="S2.F2.6.2.2.m2.1.2.2.2.3.cmml" xref="S2.F2.6.2.2.m2.1.2.2.2.3">𝑘</ci></apply></apply><ci id="S2.F2.6.2.2.m2.1.1.cmml" xref="S2.F2.6.2.2.m2.1.1">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.2.2.m2.1d">\nabla F_{k}(w)</annotation></semantics></math>, and the aggregated mini-batch updates are applied on the server. </span>Right: Local updating schemes.<span id="S2.F2.6.2.3" class="ltx_text ltx_font_upright"> Each device immediately applies local updates, e.g., gradients, after they are computed and a server performs a global aggregation after a variable number of local updates. Local-updating schemes can reduce communication by performing additional work locally.</span></span></figcaption>
</figure>
<div id="S2.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Mini-batch optimization methods, which involve extending classical stochastic methods to process multiple data points at a time, have emerged as a popular paradigm for distributed machine learning in data center environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib102" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">102</span></a>, <a href="#bib.bib103" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">103</span></a>, <a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">88</span></a>, <a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite>.
In practice, however, they have been shown to have limited flexibility to adapt to communication-computation trade-offs that would maximally leverage distributed data processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">107</span></a>, <a href="#bib.bib108" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">108</span></a>]</cite>.
In response, several recent methods have been proposed to improve communication-efficiency in distributed settings by allowing for a variable number of <span id="S2.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">local updates</span> to be applied on each machine in parallel at each communication round, making the amount of computation versus communication substantially more flexible.
For convex objectives, distributed local-updating <em id="S2.SS1.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">primal-dual</em> methods have emerged as a popular way to tackle such a problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">107</span></a>, <a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>, <a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib128" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">128</span></a>]</cite>. These approaches leverage duality structure to effectively decompose the global objective into subproblems that can be solved in parallel at each communication round. Several distributed local-updating <em id="S2.SS1.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">primal</em> methods have also been proposed, which have the added benefit of being applicable to non-convex objectives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">136</span></a>, <a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">93</span></a>]</cite>. These methods drastically improve performance in practice, and have been shown to achieve orders-of-magnitude speedups over traditional mini-batch methods or distributed approaches like ADMM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> in real-world data center environments. We provide an intuitive illustration of local updating methods in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1.1 Local Updating ‣ 2.1 Communication-efficiency ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">In federated settings, optimization methods that allow for flexible local updating and low client participation have become the de facto solvers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>, <a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">106</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>.
The most commonly used method for federated learning is Federated Averaging (<span id="S2.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite>, a method based on averaging local stochastic gradient descent (SGD) updates for the primal problem. <span id="S2.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span> has been shown to work well empirically, particularly for non-convex problems, but comes without convergence guarantees and can diverge in practical settings when data are heterogeneous <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>. We discuss methods to handle such statistical heterogeneity in more detail in Section <a href="#S2.SS3.SSS2" title="2.3.2 Convergence Guarantees for Non-IID Data ‣ 2.3 Statistical Heterogeneity ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.2</span></a>.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Compression Schemes</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">While local updating methods can reduce the total <span id="S2.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">number of communication rounds</span>, model compression schemes such as sparsification, subsampling, and quantization can significantly reduce the <span id="S2.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">size of messages</span> communicated at each round. These methods have been extensively studied, both empirically and theoretically, in previous literature for distributed training in data center environments; we defer the readers to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">119</span></a>, <a href="#bib.bib135" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">135</span></a>]</cite> for a more complete review. In federated environments, the low participation of devices, non-identically distributed local data, and local updating schemes pose novel challenges to these model compression approaches.
For instance,
the commonly-used error compensation techniques in classical distributed learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">101</span></a>]</cite> cannot be directly extended to federated settings as the errors accumulated locally may be stale if the devices are not frequently sampled.
Nevertheless, several works have provided practical strategies in federated settings, such as forcing the updating models to be sparse and low-rank; performing quantization with structured random rotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>; using lossy compression and dropout to reduce server-to-device communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>; and applying Golomb lossless encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">99</span></a>]</cite>.
From a theoretical perspective, while prior work has explored convergence guarantees with low-precision training in the presence of non-identically distributed data <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">111</span></a>]</cite>, the assumptions made do not take into consideration common characteristics of the federated setting, such as low device participation or locally-updating optimization methods.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x4.png" id="S2.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="202" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x5.png" id="S2.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="229" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.5.2" class="ltx_text" style="font-size:90%;">Centralized vs. decentralized topologies. In the typical federated learning setting and as a focus of this article, we assume a star network (left) where a server connects with all remote devices. Decentralized topologies (right) are a potential alternative when communication to the server becomes a bottleneck.</span></figcaption>
</figure>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Decentralized Training</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">In federated learning, a star network (where a central server is connected to a network of devices, as in the left panel of Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1.2 Compression Schemes ‣ 2.1 Communication-efficiency ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) is the predominant communication topology; we therefore focus on the star-network setting in this article. However, we briefly discuss decentralized topologies (where devices only communicate with their neighbors, e.g., the right panel of Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1.2 Compression Schemes ‣ 2.1 Communication-efficiency ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) as a potential alternative. In data center environments, decentralized training has been demonstrated to be faster than centralized training when operating on networks with low bandwidth or high latency; we defer readers to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> for a more comprehensive review.
Similarly, in federated learning, decentralized algorithms can in theory reduce the high communication cost on the central server. Some recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite> have investigated decentralized training over heterogeneous data with local updating schemes. However, they are either restricted to linear models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> or assume full device participation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite>.
Finally, hierarchical communication patterns have also been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>, <a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite> to further ease the burden on the central server, by first leveraging <em id="S2.SS1.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">edge servers</em> to aggregate the updates from edge devices and then relying on a <em id="S2.SS1.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">cloud server</em> to aggregate updates from edge servers. While this is a promising approach to reduce communication, it is not applicable to all networks, as this type of physical hierarchy may not exist or be known a priori.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Systems Heterogeneity</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">In federated settings, there is significant variability in the <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">systems</em> characteristics across the network, as devices
may differ in terms of hardware, network connectivity, and battery power. As depicted in Figure <a href="#S2.F4" title="Figure 4 ‣ 2.2 Systems Heterogeneity ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, these systems characteristics make issues such as stragglers significantly more prevalent than in typical data center environments. We roughly group several key directions to handle systems heterogeneity into: (i) asynchronous communication, (ii) active device sampling, and (ii) fault tolerance. As mentioned in Section <a href="#S2.SS1.SSS3" title="2.1.3 Decentralized Training ‣ 2.1 Communication-efficiency ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1.3</span></a>, we assume a star topology in our following discussions.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/1908.07873/assets/x6.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="428" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Systems heterogeneity in federated learning. Devices may vary in terms of network connection, power, and hardware. Moreover, some of the devices may drop at any time during training. Therefore, federated training methods must tolerate heterogeneous systems environments and low participation of devices, i.e., they must allow for only a small subset of devices to be active at each round.</span></figcaption>
</figure>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Asynchronous Communication</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p1.2" class="ltx_p">In traditional data center settings, synchronous and asynchronous schemes are both commonly used to parallelize iterative optimization algorithms, with each approach having pros and cons. Synchronous schemes are simple and guarantee a serial-equivalent computational model, but they are also more susceptible to stragglers in the face of device variability. Asynchronous schemes are an attractive approach to mitigate stragglers in heterogeneous environments, particularly in shared-memory systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">92</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>, <a href="#bib.bib141" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">141</span></a>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>. However, they typically rely on bounded-delay assumptions to control the degree of staleness, which for device <math id="S2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mi id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><ci id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">k</annotation></semantics></math> depends on the number of other devices that have updated since device <math id="S2.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mi id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">k</annotation></semantics></math> pulled from the central server. While asynchronous parameter servers have been successful in distributed data centers <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib141" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">141</span></a>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, classical bounded-delay assumptions can be unrealistic in federated settings, where the delay may be on the order of hours to days, or completely unbounded.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Active Sampling</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">In federated networks, typically only a small subset of devices participate at each round of training. However, the vast majority of federated
methods, e.g. those described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>, <a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">106</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>,
are <em id="S2.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">passive</em> in that they do not aim to influence which devices participate. An alternative approach involves <em id="S2.SS2.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">actively</em> selecting participating devices at each round.
For example, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Nishio and Yonetani</span> [<a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite> explore novel device sampling policies based on systems resources, with the aim being for the server to aggregate as many device updates as possible within a pre-defined time window.
Similarly, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Kang et al.</span> [<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> take into account systems overheads incurred on each device when designing incentive mechanisms to encourage devices with higher-quality data to participate in the learning process. However, these methods assume a static model of the systems characteristics of the network; it remains open how to extend these approaches to handle <span id="S2.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_italic">real-time</span>, device-specific fluctuations in computation and communication delays. Moreover, while these methods primarily focus on systems variability to perform active sampling, we note that it is also worth considering actively sampling a set of small but sufficiently representative devices based on the underlying <em id="S2.SS2.SSS2.p1.1.4" class="ltx_emph ltx_font_italic">statistical</em> structure.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Fault Tolerance</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Fault tolerance has been extensively studied in the systems community and is a fundamental consideration of classical distributed systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">110</span></a>, <a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. Recent works have also investigated fault tolerance specifically for machine learning workloads in data center environments <cite class="ltx_cite ltx_citemacro_cite">[e.g., <a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">112</span></a>, <a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite>.
When learning over remote devices, however, fault tolerance becomes more critical as it is common for some participating devices to drop out at some point before the completion of the given training iteration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>. One practical strategy is to simply ignore such device failure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>,
which may introduce bias into the device sampling scheme if the failed devices have specific data characteristics. For instance, devices from remote areas may be more likely to drop due to poor network connections and thus the trained federated model will be biased towards devices with favorable network conditions. Theoretically, while several recent works have investigated convergence guarantees of variants of federated learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">132</span></a>, <a href="#bib.bib123" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">123</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib131" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">131</span></a>]</cite>, few analyses allow for low participation <cite class="ltx_cite ltx_citemacro_cite">[e.g., <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">106</span></a>]</cite>, or study directly the effect of dropped devices.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p"><em id="S2.SS2.SSS3.p2.1.1" class="ltx_emph ltx_font_italic">Coded computation</em> is another option to tolerate device failures by introducing algorithmic redundancy. Recent works have explored using codes to speed up distributed machine learning training <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib109" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">109</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">94</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. For instance, in the presence of stragglers, gradient coding and its variants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">109</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> carefully replicate data blocks (as well as the gradient computation on those data blocks) across computing nodes to obtain either exact or inexact recovery of the true gradients.
While this is a seemingly promising approach for the federated setting, these methods face fundamental challenges in federated networks as sharing data/replication across devices is often infeasible due to privacy constraints and the scale of the network.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Statistical Heterogeneity</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Challenges arise when training federated models from data that is not identically distributed across devices, both in terms of modeling the data (as depicted in Figure <a href="#S2.F5" title="Figure 5 ‣ 2.3 Statistical Heterogeneity ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), and in terms of analyzing the convergence behavior of associated training procedures. We discuss related work in these directions below.</p>
</div>
<figure id="S2.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x7.png" id="S2.F5.sf1.g1" class="ltx_graphics ltx_img_square" width="461" height="454" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Learn personalized models for each device; do not learn from peers.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x8.png" id="S2.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="461" height="452" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Learn a global model; learn from peers.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x9.png" id="S2.F5.sf3.g1" class="ltx_graphics ltx_img_square" width="461" height="454" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">Learn personalized models for each device; learn from peers.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F5.3.2" class="ltx_text" style="font-size:90%;">Different modeling approaches in federated networks. Depending on properties of the data, network, and application of interest, one may choose to (a) learn separate models for each device, (b) fit a single global model to all devices, or (c) learn related but distinct models in the network.</span></figcaption>
</figure>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Modeling Heterogeneous Data</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">There exists a large body of literature in machine learning that has modeled statistical heterogeneity via methods such as meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">114</span></a>]</cite> and multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>; these ideas have been recently extended to the federated setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">106</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib138" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite>. For instance, <span id="S2.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_typewriter">MOCHA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">106</span></a>]</cite>, an optimization framework designed for the federated setting, can allow for personalization by learning <em id="S2.SS3.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">separate</em> but related models for each device while leveraging a shared representation via multi-task learning. This method has provable theoretical convergence guarantees for the considered objectives, but is limited in its ability to scale to massive networks and is restricted to convex objectives. Another approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> models the star topology as a Bayesian network and performs variational inference during learning. Although this method can handle non-convex models, it is expensive to generalize to large federated networks. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Khodak et al.</span> [<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> provably meta-learn a within-task learning rate using multi-task information (where each task corresponds to a device) and have demonstrated improved empirical performance over vanilla <span id="S2.SS3.SSS1.p1.1.3" class="ltx_text ltx_font_typewriter">FedAvg</span>.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Eichner et al.</span> [<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> investigate a pluralistic solution (adaptively choosing between a global model and device-specific models) to address the cyclic patterns in data samples during federated training. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Zhao et al.</span> [<a href="#bib.bib138" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite> explore transfer learning for personalization by running <span id="S2.SS3.SSS1.p1.1.4" class="ltx_text ltx_font_typewriter">FedAvg</span> after training a global model centrally on some shared proxy data. Despite these recent advances, key challenges still remain in making methods for heterogeneous modeling that are robust, scalable, and automated in federated settings.</p>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS1.p2.1" class="ltx_p">When modeling federated data, it may also be important to consider issues beyond accuracy, such as <span id="S2.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_italic">fairness</span>.
In particular, naively solving an aggregate loss function such as in (<a href="#S1.E1" title="In 1.1 Problem Formulation ‣ 1 Introduction ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) may implicitly advantage or disadvantage some of the devices, as the learned model may become biased towards devices with larger amounts of data, or (if weighting devices equally), to commonly occurring groups of devices. Recent works have proposed modified modeling approaches that aim to reduce the variance of the model performance across devices. Some heuristics simply perform a varied number of local updates based on local loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>. Other more principled approaches include Agnostic Federated Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>, which optimizes the centralized model for any target distribution formed by a mixture of the client distributions via a minimax optimization scheme.
Another more general approach is taken by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite>, which proposes an objective called <math id="S2.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS3.SSS1.p2.1.m1.1a"><mi id="S2.SS3.SSS1.p2.1.m1.1.1" xref="S2.SS3.SSS1.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p2.1.m1.1b"><ci id="S2.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p2.1.m1.1c">q</annotation></semantics></math>-FFL in which devices with higher loss are given higher relative weight to encourage less variance in the final accuracy distribution. Beyond issues of fairness, we note that aspects such as accountability and interpretability in federated learning are additionally worth exploring, but may be challenging due to the scale and heterogeneity of the network.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Convergence Guarantees for Non-IID Data</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">Statistical heterogeneity also presents novel challenges in terms of analyzing the convergence behavior in federated settings—even when learning a single global model.
Indeed, when data is not identically distributed across devices in the network, methods such as <span id="S2.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span> have been shown to diverge in practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite>. Parallel SGD and related variants, which make local updates similar to <span id="S2.SS3.SSS2.p1.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span>, have been analyzed in the I.I.D. setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>, <a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">93</span></a>, <a href="#bib.bib104" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">104</span></a>, <a href="#bib.bib108" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">108</span></a>, <a href="#bib.bib120" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">120</span></a>, <a href="#bib.bib125" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">125</span></a>, <a href="#bib.bib136" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">136</span></a>, <a href="#bib.bib140" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">140</span></a>, <a href="#bib.bib122" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">122</span></a>, <a href="#bib.bib121" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">121</span></a>]</cite>.
However, the results rely on the premise that each local
solver is a copy of the same stochastic process (due to the I.I.D. assumption), which is not the case in typical federated settings.
To understand the performance of <span id="S2.SS3.SSS2.p1.1.3" class="ltx_text ltx_font_typewriter">FedAvg</span> in statistically heterogeneous settings, <span id="S2.SS3.SSS2.p1.1.4" class="ltx_text ltx_font_typewriter">FedProx</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite> has recently been proposed. <span id="S2.SS3.SSS2.p1.1.5" class="ltx_text ltx_font_typewriter">FedProx</span> makes a small modification to the <span id="S2.SS3.SSS2.p1.1.6" class="ltx_text ltx_font_typewriter">FedAvg</span> method to help ensure convergence, both theoretically and in practice. <span id="S2.SS3.SSS2.p1.1.7" class="ltx_text ltx_font_typewriter">FedProx</span> can also be interpreted as a generalized, reparameterized version of <span id="S2.SS3.SSS2.p1.1.8" class="ltx_text ltx_font_typewriter">FedAvg</span> that has practical ramifications in the context of accounting for systems heterogeneity across devices.
Several other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">123</span></a>, <a href="#bib.bib132" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">132</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib131" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">131</span></a>]</cite> have also explored convergence guarantees in the presence of heterogeneous data with different assumptions, e.g., convexity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">123</span></a>]</cite> or uniformly bounded gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">131</span></a>]</cite>.
There are also heuristic approaches that aim to tackle statistical heterogeneity, either by sharing local device data or some server-side proxy data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib138" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite>. However, these methods may be unrealistic: in addition to imposing burdens on network bandwidth, sending local data to the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite> violates the key privacy assumption of federated learning, and sending globally-shared proxy data to all devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib138" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite> requires effort to carefully generate or collect such auxiliary data.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Privacy</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">Privacy concerns often motivate the need to keep raw data on each device local in federated settings. However, sharing other information such as model updates as part of the training process can also leak sensitive user information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">78</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>. For instance, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Carlini et al.</span> [<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> demonstrate that one can extract sensitive text patterns, e.g., a specific credit card number, from a recurrent neural network trained on users’ language data.
Given increasing interest in privacy-preserving learning approaches, in Section <a href="#S2.SS4.SSS1" title="2.4.1 Privacy in Machine Learning ‣ 2.4 Privacy ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4.1</span></a>, we first briefly revisit prior work on enhancing privacy in the general (distributed) machine learning setting. We then review recent privacy-preserving methods specifically designed for federated settings in Section <a href="#S2.SS4.SSS2" title="2.4.2 Privacy in Federated Learning ‣ 2.4 Privacy ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4.2</span></a>.</p>
</div>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Privacy in Machine Learning</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">Privacy-preserving learning has been extensively studied by the machine learning <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>, systems <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, and theory <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite> communities. Three main strategies, each of which we will briefly review, include differential privacy to communicate noisy data sketches, homomorphic encryption to operate on encrypted data, and secure function evaluation or multiparty computation.</p>
</div>
<div id="S2.SS4.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS1.p2.2" class="ltx_p">Among these various privacy approaches, <span id="S2.SS4.SSS1.p2.2.1" class="ltx_text ltx_font_italic">differential privacy</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> is most widely used due to its strong information theoretic guarantees, algorithmic simplicity, and relatively small systems overhead.
Simply put, a randomized mechanism is differentially private if the change of one input element will not result in too much difference in the output distribution; this means that one cannot draw any conclusions about whether or not a specific sample is used in the learning process.
Such sample-level privacy can be achieved in many learning tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">85</span></a>, <a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">86</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.
For gradient-based learning methods, a popular approach is to apply differential privacy by randomly perturbing the intermediate output at each iteration <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib126" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">126</span></a>]</cite>. Before applying the perturbation, e.g., via Gaussian noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, Laplacian noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>, or Binomial noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, it is common to clip the gradients in order to bound the influence of each example on the overall update. There exists an inherent trade-off between differential privacy and model accuracy, as adding more noise results in greater privacy, but may compromise accuracy significantly.
Despite the fact that differential privacy is the de facto metric for privacy in machine learning,
there are many other privacy definitions,
such as <math id="S2.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS4.SSS1.p2.1.m1.1a"><mi id="S2.SS4.SSS1.p2.1.m1.1.1" xref="S2.SS4.SSS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.1.m1.1b"><ci id="S2.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.1.m1.1c">k</annotation></semantics></math>-anonymity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, <math id="S2.SS4.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS4.SSS1.p2.2.m2.1a"><mi id="S2.SS4.SSS1.p2.2.m2.1.1" xref="S2.SS4.SSS1.p2.2.m2.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.2.m2.1b"><ci id="S2.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p2.2.m2.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.2.m2.1c">\delta</annotation></semantics></math>-presence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> and distance correlation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">117</span></a>]</cite>, that may be applicable to different learning problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">118</span></a>]</cite>.</p>
</div>
<div id="S2.SS4.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS1.p3.1" class="ltx_p">Beyond differential privacy, homomorphic encryption can be used to secure the learning process by computing on encrypted data, although it has currently been applied in limited settings, e.g., training linear models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite> or involving only a few entities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">133</span></a>]</cite>. When the sensitive datasets are distributed across different data owners, another natural option is to perform privacy-preserving learning via secure function evaluation (SFE) or secure multiparty computation (SMC). The resulting protocols can enable multiple parties to collaboratively compute an agreed-upon function without leaking input information from any party except for what can be inferred from the output <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">95</span></a>]</cite>. Thus, while SMC cannot guarantee protection from information leakage, it can be combined with differential privacy to achieve stronger privacy guarantees. However, approaches along these lines may not be applicable to large-scale machine learning scenarios as they incur substantial additional communication and computation costs.
Moreover, SMC protocols need to be carefully designed and implemented for each operation in the targeted learning algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>.
We defer interested readers to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib97" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">97</span></a>]</cite> for a more comprehensive review of the approaches based on homomorphic encryption and SMC.</p>
</div>
<figure id="S2.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x10.png" id="S2.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="285" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">Federated learning without additional privacy protection mechanisms.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x11.png" id="S2.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="280" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">Global privacy, where a trusted server is assumed.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1908.07873/assets/x12.png" id="S2.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="228" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F6.sf3.3.2" class="ltx_text" style="font-size:90%;">Local privacy, where the central server might be malicious.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.4.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S2.F6.2.1" class="ltx_text" style="font-size:90%;">An illustration of different privacy-enhancing mechanisms in one round of federated learning. <math id="S2.F6.2.1.m1.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S2.F6.2.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S2.F6.2.1.m1.1.1" xref="S2.F6.2.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S2.F6.2.1.m1.1c"><ci id="S2.F6.2.1.m1.1.1.cmml" xref="S2.F6.2.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F6.2.1.m1.1d">\mathcal{M}</annotation></semantics></math> denotes a randomized mechanism used to privatize the data. With global privacy (b), the model updates are private to all third parties other than a single trusted party (the central server). With local privacy (c), the individual model updates are also private to the server.</span></figcaption>
</figure>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Privacy in Federated Learning</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">The federated setting poses novel challenges to existing privacy-preserving algorithms. Beyond providing rigorous privacy guarantees, it is necessary to develop methods that are computationally cheap, communication-efficient, and tolerant to dropped devices—all without overly compromising accuracy.
Although there are a variety of privacy definitions in federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>, <a href="#bib.bib113" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">113</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, typically they can be classified into two categories: <span id="S2.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_italic">global privacy</span> and <span id="S2.SS4.SSS2.p1.1.2" class="ltx_text ltx_font_italic">local privacy</span>. As demonstrated in Figure <a href="#S2.F6" title="Figure 6 ‣ 2.4.1 Privacy in Machine Learning ‣ 2.4 Privacy ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>,
global privacy requires that the model updates generated at each round are private to all untrusted third parties other than the central server, while local privacy further requires that the updates are also private to the server.</p>
</div>
<div id="S2.SS4.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS2.p2.1" class="ltx_p">Current works that aim to improve the privacy of federated learning typically build upon previous classical cryptographic protocols such as SMC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> and differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bonawitz et al.</span> [<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> introduce an SMC protocol to protect individual model updates. The central server is not able to see any local updates, but can still observe the exact aggregated results at each round. SMC is a lossless method, and can retain the original accuracy with a very high privacy guarantee.
However, the resulting method incurs significant extra communication cost. Other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> apply differential privacy to federated learning and offer global differential privacy. These approaches have a number of hyperparameters that affect communication and accuracy that must be carefully chosen, though follow up work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">113</span></a>]</cite> proposes adaptive gradient clipping strategies to help alleviate this issue. In the case where stronger privacy guarantees are required, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bhowmick et al.</span> [<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> introduce a relaxed version of local privacy by limiting the power of potential adversaries. It affords stronger privacy guarantees than global privacy, and has better model performance than strict local privacy. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> propose locally differentially-private algorithms in the context of meta-learning, which can be applied to federated learning with personalization, while also
providing provable learning guarantees in convex settings.
In addition, differential privacy can be combined with model compression techniques to reduce communication and obtain privacy benefits simultaneously <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Future Directions</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Federated learning is an active and ongoing area of research. Although recent work has begun to address the challenges discussed in Section <a href="#S2" title="2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, there are a number of critical open directions yet to be explored.
In this section, we briefly outline a few promising research directions surrounding the previously discussed challenges (expensive communication, systems heterogeneity, statistical heterogeneity, and privacy concerns), and introduce additional challenges regarding issues such as productionizing and benchmarking in federated settings.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Extreme communication schemes.</span> It remains to be seen how much communication is necessary in federated learning. Indeed, it is well-known that optimization methods for machine learning can tolerate a lack of precision; this error can in fact help with generalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">129</span></a>]</cite>. While one-shot or divide-and-conquer communication schemes have been explored in traditional data center settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">137</span></a>, <a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite>, the behavior of these methods is not well-understood in massive or statistical heterogeneous networks. Similarly, one-shot/few-shot heuristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">134</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> have recently been proposed for the federated setting, but have yet to be theoretically analyzed or evaluated at scale.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Communication reduction and the Pareto frontier.</span> We discussed several ways to reduce communication in federated training, such as local updating and model compression. In order to create a realistic system for federated learning, it is important to understand how these techniques <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">compose</span> with one another, and to <span id="S3.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">systematically</span> analyze the trade-off between accuracy and communication for each approach. In particular, the most useful techniques will demonstrate improvements at the Pareto frontier—achieving an accuracy greater than any other approach under the same communication budget, and ideally, across a wide range of communication/accuracy profiles.
Similar comprehensive analyses have been performed for efficient neural network inference <cite class="ltx_cite ltx_citemacro_cite">[e.g., <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, and are necessary in order to compare communication-reduction techniques for federated learning in a meaningful way.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Novel models of asynchrony.</span> As discussed in Section <a href="#S2.SS2.SSS1" title="2.2.1 Asynchronous Communication ‣ 2.2 Systems Heterogeneity ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>, two communication schemes most commonly studied in distributed optimization are bulk synchronous approaches and asynchronous approaches (where it is assumed that the delay is bounded). These schemes are more realistic in data center settings—where worker nodes are typically <em id="S3.I1.i3.p1.1.2" class="ltx_emph ltx_font_italic">dedicated</em> to the workload, i.e., they are ready to ‘pull’ their next job from the central node immediately after they ‘push’ the results of their previous job. In contrast, in federated networks, each device is often <em id="S3.I1.i3.p1.1.3" class="ltx_emph ltx_font_italic">undedicated</em> to the task at hand and most devices are not active on any given iteration.
Therefore, it is worth studying the effects of this more realistic <em id="S3.I1.i3.p1.1.4" class="ltx_emph ltx_font_italic">device-centric</em> communication scheme—in which each device can decide when to ‘wake up’ and interact with the central server in an event-triggered manner.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Heterogeneity diagnostics.</span>
Recent works have aimed to quantify statistical heterogeneity through metrics such as local dissimilarity (as defined in the context of federated learning in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite> and used for other purposes in works such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">130</span></a>, <a href="#bib.bib100" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">100</span></a>, <a href="#bib.bib116" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">116</span></a>]</cite>) and earth mover’s distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite>. However, these metrics cannot be easily calculated over the federated network before training occurs.
The importance of these metrics motivates the following open questions: (i) Do simple diagnostics exist to quickly determine the level of heterogeneity in federated networks <span id="S3.I1.i4.p1.1.2" class="ltx_text ltx_font_italic">a priori</span>? (ii) Can analogous diagnostics be developed to quantify the amount of <em id="S3.I1.i4.p1.1.3" class="ltx_emph ltx_font_italic">systems-related</em> heterogeneity? (iii) Can current or new definitions of heterogeneity be exploited to further improve the convergence of federated optimization methods?</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Granular privacy constraints.</span> The definitions of privacy outlined in Section <a href="#S2.SS4.SSS2" title="2.4.2 Privacy in Federated Learning ‣ 2.4 Privacy ‣ 2 Survey of Related and Current Work ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4.2</span></a> cover privacy at a local or global level with respect to all devices in the network. However, in practice, it may be necessary to define privacy on a more granular level, as privacy constraints may differ across devices or even across data points on a single device. For instance, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> recently proposed sample-specific (as opposed to user-specific) privacy guarantees, thus providing a weaker form of privacy in exchange for more accurate models. Developing methods to handle mixed (device-specific or sample-specific) privacy restrictions is an interesting and ongoing direction of future work.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p"><span id="S3.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Beyond supervised learning.</span> It is important to note that the methods discussed thus far have been developed with the task of <span id="S3.I1.i6.p1.1.2" class="ltx_text ltx_font_italic">supervised learning</span> in mind, i.e., they assume that labels exist for all of the data in the federated network. In practice, much of the data generated in realistic federated networks may be unlabeled or weakly labeled. Furthermore, the problem at hand may not be to fit a model to data as presented in (<a href="#S1.E1" title="In 1.1 Problem Formulation ‣ 1 Introduction ‣ Federated Learning: Challenges, Methods, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), but instead to perform some exploratory data analysis, determine aggregate statistics, or run a more complex task such as reinforcement learning. Tackling problems beyond supervised learning in federated networks will likely require addressing similar challenges of scalability, heterogeneity, and privacy.</p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i7.p1" class="ltx_para">
<p id="S3.I1.i7.p1.1" class="ltx_p"><span id="S3.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Productionizing federated learning.</span> Beyond the major challenges discussed in this article, there are a number of practical concerns that arise when running federated learning in production. In particular, issues such as concept drift (when the underlying data-generation model changes over time); diurnal variations (when the devices exhibit different behavior at different times of the day or week) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>; and cold start problems (when new devices enter the network) must be handled with care. We defer the readers to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, which discusses some of the practical systems-related issues that exist in production federated learning systems.</p>
</div>
</li>
<li id="S3.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i8.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i8.p1.1" class="ltx_p"><span id="S3.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">Benchmarks.</span> Finally, as federated learning is a nascent field, we are at a pivotal time to shape the developments made in this area and ensure that they are grounded in real-world settings, assumptions, and datasets. It is critical for the broader research communities to further build upon existing implementations and benchmarking tools, such as LEAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> and TensorFlow Federated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, to facilitate both the reproducibility of empirical results and the dissemination of new solutions for federated learning.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this article, we have provided an overview of federated learning, a learning paradigm where statistical models are trained at the edge in distributed networks. We have discussed the unique properties and associated challenges of federated learning compared with traditional distributed data center computing and classical privacy-preserving learning. We provided an extensive survey on classical results
as well as more recent work specifically focused on federated settings. Finally, we have outlined out a handful of open problems worth future research effort. Providing solutions to these problems will require interdisciplinary effort from a broad set of research communities.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Acknowledgement.</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">We thank Jeffrey Li and Mikhail Khodak for helpful discussions and comments. This work was supported in part by DARPA FA875017C0141, the National Science Foundation
grants IIS1705121 and IIS1838017, an Okawa Grant, a Google Faculty Award, an Amazon Web
Services Award, a JP Morgan A.I. Research Faculty Award, a Carnegie Bosch Institute Research Award and the CONIX Research Center, one of
six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the views of DARPA, the National Science Foundation, or
any other funding agency.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.3.3.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">
Tensorflow federated: Machine learning on decentralized data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.tensorflow.org/federated</a><span id="bib.bib1.7.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Abadi et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Deep learning with differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Computer and Communications Security</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Agarwal et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
N. Agarwal, A. T. Suresh, F. X. X. Yu, S. Kumar, and B. McMahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">cpSGD: Communication-efficient and differentially-private
distributed sgd.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.4.4.1" class="ltx_text" style="font-size:90%;">Agrawal and Srikant [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">
R. Agrawal and R. Srikant.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">Privacy-preserving data mining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Management of Data</em><span id="bib.bib4.10.3" class="ltx_text" style="font-size:90%;">, 2000.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Ammad-ud din et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
M. Ammad-ud din, E. Ivannikova, S. A. Khan, W. Oyomno, Q. Fu, K. E. Tan, and
A. Flanagan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Federated collaborative filtering for privacy-preserving personalized
recommendation system.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1901.09888</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Anguita et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">A public domain dataset for human activity recognition using
smartphones.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Symposium on Artificial Neural Networks,
Computational Intelligence and Machine Learning</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Bassily et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
R. Bassily, A. Smith, and A. Thakurta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Private empirical risk minimization: Efficient algorithms and tight
error bounds.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Foundations of Computer Science</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Bhowmick et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
A. Bhowmick, J. Duchi, J. Freudiger, G. Kapoor, and R. Rogers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Protection against reconstruction and its applications in private
federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.00984</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Bolukbasi et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
T. Bolukbasi, J. Wang, O. Dekel, and V. Saligrama.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Adaptive neural networks for efficient inference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Bonawitz et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
D. Ramage, A. Segal, and K. Seth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Practical secure aggregation for privacy-preserving machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Computer and Communications Security</em><span id="bib.bib10.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Bonawitz et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan, T. V. Overveldt,
D. Petrou, D. Ramage, and J. Roselander.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Towards federated learning at scale: system design.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Systems and Machine Learning</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Bonomi et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
F. Bonomi, R. Milito, J. Zhu, and S. Addepalli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Fog computing and its role in the internet of things.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGCOMM Workshop on Mobile Cloud Computing</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Bost et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
R. Bost, R. A. Popa, S. Tu, and S. Goldwasser.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Machine learning classification over encrypted data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Network and Distributed System Security Symposium</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Boyd et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Distributed optimization and statistical learning via the alternating
direction method of multipliers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Foundations and Trends® in Machine Learning</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">,
3:1–122, 2011.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Caldas et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
S. Caldas, J. Konečny, H. B. McMahan, and A. Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Expanding the reach of federated learning by reducing client resource
requirements.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib15.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.07210</em><span id="bib.bib15.10.2" class="ltx_text" style="font-size:90%;">, 2018a.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Caldas et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
S. Caldas, P. Wu, T. Li, J. Konečnỳ, H. B. McMahan, V. Smith, and
A. Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Leaf: A benchmark for federated settings.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.01097</em><span id="bib.bib16.10.2" class="ltx_text" style="font-size:90%;">, 2018b.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Carlini et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
N. Carlini, C. Liu, J. Kos, Ú. Erlingsson, and D. Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">The secret sharer: Measuring unintended neural network memorization
&amp; extracting secrets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.08232</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.4.4.1" class="ltx_text" style="font-size:90%;">Caruana [1997]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">
R. Caruana.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">Multitask learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Machine Learning</em><span id="bib.bib18.9.2" class="ltx_text" style="font-size:90%;">, 28:41–75, 1997.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Castro et al. [1999]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
M. Castro, B. Liskov, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Practical byzantine fault tolerance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Operating Systems Design and Implementation</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, 1999.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.4.4.1" class="ltx_text" style="font-size:90%;">Charles and Papailiopoulos [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">
Z. Charles and D. Papailiopoulos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">Gradient coding using the stochastic block model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Symposium on Information Theory</em><span id="bib.bib20.10.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Charles et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Z. B. Charles, D. S. Papailiopoulos, and J. Ellenberg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Approximate gradient coding via sparse random graphs.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.0677</em><span id="bib.bib21.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Chaudhuri et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
K. Chaudhuri, C. Monteleoni, and A. D. Sarwate.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Differentially private empirical risk minimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, 12:1069–1109,
2011.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.4.4.1" class="ltx_text" style="font-size:90%;">Chaum [1988]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text" style="font-size:90%;">
D. Chaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">The dining cryptographers problem: Unconditional sender and recipient
untraceability.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Cryptology</em><span id="bib.bib23.9.2" class="ltx_text" style="font-size:90%;">, 1:65–75, 1988.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
F. Chen, Z. Dong, Z. Li, and X. He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Federated meta-learning for recommendation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.07876</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
V. Chen, V. Pastro, and M. Raykova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Secure computation for machine learning with spdz.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1901.00329</em><span id="bib.bib25.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.4.4.1" class="ltx_text" style="font-size:90%;">Corinzia and Buhmann [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text" style="font-size:90%;">
L. Corinzia and J. M. Buhmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">Variational federated multi-task learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.06268</em><span id="bib.bib26.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Dai et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
W. Dai, A. Kumar, J. Wei, Q. Ho, G. Gibson, and E. P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">High-performance distributed ML at scale through parameter server
consistency models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI Conference on Artificial Intelligence</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Dekel et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Optimal distributed online prediction using mini-batches.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib28.10.2" class="ltx_text" style="font-size:90%;">, 13:165–202,
2012.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Deshpande et al. [2005]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
A. Deshpande, C. Guestrin, S. R. Madden, J. M. Hellerstein, and W. Hong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Model-based approximate querying in sensor networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">The VLDB Journal</em><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">, 14:417–443, 2005.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Duchi et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
J. Duchi, M. I. Jordan, and B. McMahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Estimation, optimization, and parallelism when data is sparse.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Duchi et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
J. C. Duchi, M. I. Jordan, and M. J. Wainwright.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Privacy aware learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib31.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib31.11.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.4.4.1" class="ltx_text" style="font-size:90%;">Dwork [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">
C. Dwork.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">A firm foundation for private data analysis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Communications of the ACM</em><span id="bib.bib32.9.2" class="ltx_text" style="font-size:90%;">, 54:86–95, 2011.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.4.4.1" class="ltx_text" style="font-size:90%;">Dwork and Roth [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">
C. Dwork and A. Roth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">The algorithmic foundations of differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Foundations and Trends in Theoretical Computer Science</em><span id="bib.bib33.9.2" class="ltx_text" style="font-size:90%;">,
9:211–407, 2014.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Dwork et al. [2006]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
C. Dwork, F. McSherry, K. Nissim, and A. Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Calibrating noise to sensitivity in private data analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Theory of Cryptography Conference</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">, 2006.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Eichner et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
H. Eichner, T. Koren, H. B. McMahan, N. Srebro, and K. Talwar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Semi-cyclic stochastic gradient descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.4.4.1" class="ltx_text" style="font-size:90%;">El Emam and Dankar [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">
K. El Emam and F. K. Dankar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">Protecting privacy using k-anonymity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of the American Medical Informatics Association</em><span id="bib.bib36.9.2" class="ltx_text" style="font-size:90%;">,
15:627–637, 2008.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.4.4.1" class="ltx_text" style="font-size:90%;">Evgeniou and Pontil [2004]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text" style="font-size:90%;">
T. Evgeniou and M. Pontil.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">Regularized multi–task learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Knowledge Discovery and Data Mining</em><span id="bib.bib37.10.3" class="ltx_text" style="font-size:90%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Feldman et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
V. Feldman, I. Mironov, K. Talwar, and A. Thakurta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Privacy amplification by iteration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Foundations of Computer Science</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Fredrikson et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
M. Fredrikson, S. Jha, and T. Ristenpart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Model inversion attacks that exploit confidence information and basic
countermeasures.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Computer and Communications Security</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Garcia Lopez et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
P. Garcia Lopez, A. Montresor, D. Epema, A. Datta, T. Higashino, A. Iamnitchi,
M. Barcellos, P. Felber, and E. Riviere.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Edge-centric computing: Vision and challenges.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib40.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGCOMM Computer Communication Review</em><span id="bib.bib40.10.2" class="ltx_text" style="font-size:90%;">, 45:37–42,
2015.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Geyer et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
R. C. Geyer, T. Klein, and M. Nabi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Differentially private federated learning: A client level
perspective.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1712.07557</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Ghazi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
B. Ghazi, R. Pagh, and A. Velingker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Scalable and differentially private distributed aggregation in the
shuffled model.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.08320</em><span id="bib.bib42.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.4.4.1" class="ltx_text" style="font-size:90%;">Goryczka and Xiong [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.6.1" class="ltx_text" style="font-size:90%;">
S. Goryczka and L. Xiong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">A comprehensive comparison of multiparty secure additions with
differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Dependable and Secure Computing</em><span id="bib.bib43.9.2" class="ltx_text" style="font-size:90%;">,
14:463–477, 2015.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.4.4.1" class="ltx_text" style="font-size:90%;">Guha and Smith [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.6.1" class="ltx_text" style="font-size:90%;">
N. Guha and V. Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">Model aggregation via good-enough model spaces.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1805.07782</em><span id="bib.bib44.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Guha et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
N. Guha, A. Talwalkar, and V. Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">One-shot federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib45.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1902.11175</em><span id="bib.bib45.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Hard et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
A. Hard, K. Rao, R. Mathews, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon,
and D. Ramage.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Federated learning for mobile keyboard prediction.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.03604</em><span id="bib.bib46.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
L. He, A. Bian, and M. Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Cola: Decentralized linear learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib47.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib47.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Ho et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson,
G. Ganger, and E. P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">More effective distributed ML via a stale synchronous parallel
parameter server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib48.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib48.11.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Hong et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
K. Hong, D. Lillethun, U. Ramachandran, B. Ottenwälder, and B. Koldehofe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Mobile fog: A programming model for large-scale applications on the
internet of things.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib49.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGCOMM Workshop on Mobile Cloud Computing</em><span id="bib.bib49.11.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Huang et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
J. Huang, F. Qian, Y. Guo, Y. Zhou, Q. Xu, Z. M. Mao, S. Sen, and
O. Spatscheck.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">An in-depth study of lte: effect of network protocol and application
behavior on performance.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib50.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGCOMM Computer Communication Review</em><span id="bib.bib50.10.2" class="ltx_text" style="font-size:90%;">, 43:363–374,
2013.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.4.4.1" class="ltx_text" style="font-size:90%;">Huang and Liu [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.6.1" class="ltx_text" style="font-size:90%;">
L. Huang and D. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">Patient clustering improves efficiency of federated machine learning
to predict mortality and hospital stay time using distributed electronic
medical records.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1903.09296</em><span id="bib.bib51.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Huang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
L. Huang, Y. Yin, Z. Fu, S. Zhang, H. Deng, and D. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Loadaboost: Loss-based adaboost federated machine learning on medical
data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.12629</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Iyengar et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
R. Iyengar, J. P. Near, D. Song, O. Thakkar, A. Thakurta, and L. Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Towards practical differentially private convex optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Computer and Communications Security</em><span id="bib.bib53.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Jaggi et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
M. Jaggi, V. Smith, M. Takác, J. Terhorst, S. Krishnan, T. Hofmann, and
M. I. Jordan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Communication-efficient distributed dual coordinate ascent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Jeong et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S.-L. Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Communication-efficient on-device machine learning: Federated
distillation and augmentation under non-iid private data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib55.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.11479</em><span id="bib.bib55.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.4.4.1" class="ltx_text" style="font-size:90%;">Jiang and Agrawal [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text" style="font-size:90%;">
P. Jiang and G. Agrawal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">A linear speedup analysis of distributed deep learning with sparse
and quantized communication.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib56.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib56.10.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Kang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
J. Kang, Z. Xiong, D. Niyato, H. Yu, Y.-C. Liang, and D. I. Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Incentive design for efficient federated learning in mobile networks:
A contract theory approach.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.07479</em><span id="bib.bib57.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Khodak et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
M. Khodak, M.-F. Balcan, and A. Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Adaptive gradient-based meta-learning methods.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.02717</em><span id="bib.bib58.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Konečnỳ et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh,
and D. Bacon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Federated learning: strategies for improving communication
efficiency.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib59.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1610.05492</em><span id="bib.bib59.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text" style="font-size:90%;">Kuflik et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">
T. Kuflik, J. Kay, and B. Kummerfeld.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text" style="font-size:90%;">Challenges and solutions of ubiquitous user modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib60.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Ubiquitous Display Environments</em><span id="bib.bib60.11.3" class="ltx_text" style="font-size:90%;">. 2012.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Lalitha et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
A. Lalitha, X. Wang, O. Kilinc, Y. Lu, T. Javidi, and F. Koushanfar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">Decentralized bayesian learning over graphs.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib61.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.10466</em><span id="bib.bib61.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.4.4.1" class="ltx_text" style="font-size:90%;">Lee and Roth [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.6.1" class="ltx_text" style="font-size:90%;">
C.-P. Lee and D. Roth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">Distributed box-constrained quadratic optimization for dual linear
svm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib62.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib62.10.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text" style="font-size:90%;">Lee et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">
K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">Speeding up distributed machine learning using codes.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib63.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</em><span id="bib.bib63.10.2" class="ltx_text" style="font-size:90%;">, 64:1514–1529, 2017.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">
J. Li, M. Khodak, S. Caldas, and A. Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text" style="font-size:90%;">Differentially-private gradient-based meta-learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib64.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Technical Report</em><span id="bib.bib64.10.2" class="ltx_text" style="font-size:90%;">, 2019a.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text" style="font-size:90%;">
T. Li, A. K. Sahu, M. Sanjabi, M. Zaheer, A. Talwalkar, and V. Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text" style="font-size:90%;">Federated optimization for heterogeneous networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib65.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.06127</em><span id="bib.bib65.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text" style="font-size:90%;">
T. Li, M. Sanjabi, and V. Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text" style="font-size:90%;">Fair resource allocation in federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib66.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.10497</em><span id="bib.bib66.10.2" class="ltx_text" style="font-size:90%;">, 2019b.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text" style="font-size:90%;">Lian et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text" style="font-size:90%;">
X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text" style="font-size:90%;">Can decentralized algorithms outperform centralized algorithms? a
case study for decentralized parallel stochastic gradient descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib67.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib67.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text" style="font-size:90%;">
T. Lin, S. U. Stich, and M. Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text" style="font-size:90%;">Don’t use large mini-batches, use local sgd.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib68.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1808.07217</em><span id="bib.bib68.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.4.4.1" class="ltx_text" style="font-size:90%;">Lindell and Pinkas [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.6.1" class="ltx_text" style="font-size:90%;">
Y. Lindell and B. Pinkas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text" style="font-size:90%;">Privacy preserving data mining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib69.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Cryptology</em><span id="bib.bib69.10.3" class="ltx_text" style="font-size:90%;">, 2000.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text" style="font-size:90%;">
L. Liu, J. Zhang, S. Song, and K. B. Letaief.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text" style="font-size:90%;">Edge-assisted hierarchical federated learning with non-iid data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib70.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.06641</em><span id="bib.bib70.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text" style="font-size:90%;">
Y. Liu, J. K. Muppala, M. Veeraraghavan, D. Lin, and M. Hamdi.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib71.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Data center networks: Topologies, architectures and
fault-tolerance characteristics</em><span id="bib.bib71.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.10.1" class="ltx_text" style="font-size:90%;">Springer Science &amp; Business Media, 2013.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.5.5.1" class="ltx_text" style="font-size:90%;">Ma et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text" style="font-size:90%;">
C. Ma, V. Smith, M. Jaggi, M. I. Jordan, P. Richtárik, and
M. Takáč.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text" style="font-size:90%;">Adding vs. averaging in distributed primal-dual optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib72.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib72.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.5.5.1" class="ltx_text" style="font-size:90%;">Mackey et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text" style="font-size:90%;">
L. W. Mackey, M. I. Jordan, and A. Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text" style="font-size:90%;">Divide-and-conquer matrix factorization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib73.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib73.11.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text" style="font-size:90%;">Madden et al. [2005]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text" style="font-size:90%;">
S. R. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text" style="font-size:90%;">Tinydb: an acquisitional query processing system for sensor networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib74.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Transactions on Database Systems</em><span id="bib.bib74.10.2" class="ltx_text" style="font-size:90%;">, 30:122–173, 2005.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.5.5.1" class="ltx_text" style="font-size:90%;">McMahan et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.7.1" class="ltx_text" style="font-size:90%;">
H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.8.1" class="ltx_text" style="font-size:90%;">Communication-efficient learning of deep networks from decentralized
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib75.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Artificial Intelligence and Statistics</em><span id="bib.bib75.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text" style="font-size:90%;">McMahan et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text" style="font-size:90%;">
H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text" style="font-size:90%;">Learning differentially private recurrent language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib76.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib76.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.5.5.1" class="ltx_text" style="font-size:90%;">Melis et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text" style="font-size:90%;">
L. Melis, G. Danezis, and E. D. Cristofaro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text" style="font-size:90%;">Efficient private statistics with succinct sketches.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib77.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Network and Distributed System Security Symposium</em><span id="bib.bib77.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.5.5.1" class="ltx_text" style="font-size:90%;">Melis et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text" style="font-size:90%;">
L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.8.1" class="ltx_text" style="font-size:90%;">Exploiting unintended feature leakage in collaborative learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib78.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Symposium on Security &amp; Privacy</em><span id="bib.bib78.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.4.4.1" class="ltx_text" style="font-size:90%;">Mohassel and Rindal [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.6.1" class="ltx_text" style="font-size:90%;">
P. Mohassel and P. Rindal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text" style="font-size:90%;">Aby 3: a mixed protocol framework for machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib79.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Computer and Communications Security</em><span id="bib.bib79.10.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.5.5.1" class="ltx_text" style="font-size:90%;">Mohri et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.7.1" class="ltx_text" style="font-size:90%;">
M. Mohri, G. Sivek, and A. T. Suresh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.8.1" class="ltx_text" style="font-size:90%;">Agnostic federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib80.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib80.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.5.4.1" class="ltx_text" style="font-size:90%;">Nergiz and Clifton [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.7.1" class="ltx_text" style="font-size:90%;">
M. E. Nergiz and C. Clifton.
</span>
</span>
<span class="ltx_bibblock"><math id="bib.bib81.1.m1.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="bib.bib81.1.m1.1a"><mi mathsize="90%" id="bib.bib81.1.m1.1.1" xref="bib.bib81.1.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="bib.bib81.1.m1.1b"><ci id="bib.bib81.1.m1.1.1.cmml" xref="bib.bib81.1.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib81.1.m1.1c">\delta</annotation></semantics></math><span id="bib.bib81.8.1" class="ltx_text" style="font-size:90%;">-presence without complete world knowledge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib81.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Knowledge and Data Engineering</em><span id="bib.bib81.10.2" class="ltx_text" style="font-size:90%;">,
22:868–883, 2010.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.5.5.1" class="ltx_text" style="font-size:90%;">Nikolaenko et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.7.1" class="ltx_text" style="font-size:90%;">
V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye, D. Boneh, and N. Taft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.8.1" class="ltx_text" style="font-size:90%;">Privacy-preserving ridge regression on hundreds of millions of
records.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib82.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Symposium on Security and Privacy</em><span id="bib.bib82.11.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.4.4.1" class="ltx_text" style="font-size:90%;">Nishio and Yonetani [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.6.1" class="ltx_text" style="font-size:90%;">
T. Nishio and R. Yonetani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.7.1" class="ltx_text" style="font-size:90%;">Client selection for federated learning with heterogeneous resources
in mobile edge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib83.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Communications</em><span id="bib.bib83.10.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.4.4.1" class="ltx_text" style="font-size:90%;">Pantelopoulos and Bourbakis [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.6.1" class="ltx_text" style="font-size:90%;">
A. Pantelopoulos and N. G. Bourbakis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.7.1" class="ltx_text" style="font-size:90%;">A survey on wearable sensor-based systems for health monitoring and
prognosis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib84.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Systems, Man, and Cybernetics</em><span id="bib.bib84.9.2" class="ltx_text" style="font-size:90%;">,
40:1–12, 2010.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.5.5.1" class="ltx_text" style="font-size:90%;">Papernot et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.7.1" class="ltx_text" style="font-size:90%;">
N. Papernot, M. Abadi, U. Erlingsson, I. Goodfellow, and K. Talwar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.8.1" class="ltx_text" style="font-size:90%;">Semi-supervised knowledge transfer for deep learning from private
training data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib85.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib85.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.5.5.1" class="ltx_text" style="font-size:90%;">Papernot et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.7.1" class="ltx_text" style="font-size:90%;">
N. Papernot, S. Song, I. Mironov, A. Raghunathan, K. Talwar, and
Ú. Erlingsson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.8.1" class="ltx_text" style="font-size:90%;">Scalable private learning with pate.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib86.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib86.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.5.5.1" class="ltx_text" style="font-size:90%;">Qiao et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.7.1" class="ltx_text" style="font-size:90%;">
A. Qiao, B. Aragam, B. Zhang, and E. Xing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.8.1" class="ltx_text" style="font-size:90%;">Fault tolerance in iterative-convergent machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib87.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib87.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.5.5.1" class="ltx_text" style="font-size:90%;">Qu et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.7.1" class="ltx_text" style="font-size:90%;">
Z. Qu, P. Richtárik, and T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.8.1" class="ltx_text" style="font-size:90%;">Quartz: Randomized dual coordinate ascent with arbitrary sampling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib88.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib88.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.5.5.1" class="ltx_text" style="font-size:90%;">Ramaswamy et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.7.1" class="ltx_text" style="font-size:90%;">
S. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.8.1" class="ltx_text" style="font-size:90%;">Federated learning for emoji prediction in a mobile keyboard.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib89.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.04329</em><span id="bib.bib89.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.5.5.1" class="ltx_text" style="font-size:90%;">Rastegari et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.7.1" class="ltx_text" style="font-size:90%;">
M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.8.1" class="ltx_text" style="font-size:90%;">Xnor-net: Imagenet classification using binary convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib90.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</em><span id="bib.bib90.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.4.4.1" class="ltx_text" style="font-size:90%;">Ratner et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.6.1" class="ltx_text" style="font-size:90%;">
A. Ratner et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.7.1" class="ltx_text" style="font-size:90%;">SysML: The new frontier of machine learning systems.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib91.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.03257</em><span id="bib.bib91.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.5.5.1" class="ltx_text" style="font-size:90%;">Recht et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.7.1" class="ltx_text" style="font-size:90%;">
B. Recht, C. Re, S. Wright, and F. Niu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.8.1" class="ltx_text" style="font-size:90%;">Hogwild: A lock-free approach to parallelizing stochastic gradient
descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib92.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib92.11.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.5.5.1" class="ltx_text" style="font-size:90%;">Reddi et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.7.1" class="ltx_text" style="font-size:90%;">
S. J. Reddi, J. Konečnỳ, P. Richtárik, B. Póczós, and
A. Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.8.1" class="ltx_text" style="font-size:90%;">Aide: Fast and communication efficient distributed optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib93.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1608.06879</em><span id="bib.bib93.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib94.5.5.1" class="ltx_text" style="font-size:90%;">Reisizadeh et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib94.7.1" class="ltx_text" style="font-size:90%;">
A. Reisizadeh, S. Prakash, R. Pedarsani, and A. S. Avestimehr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.8.1" class="ltx_text" style="font-size:90%;">Coded computation over heterogeneous clusters.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib94.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</em><span id="bib.bib94.10.2" class="ltx_text" style="font-size:90%;">, 65:4227–4242, 2019.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib95.5.5.1" class="ltx_text" style="font-size:90%;">Riazi et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib95.7.1" class="ltx_text" style="font-size:90%;">
M. S. Riazi, C. Weinert, O. Tkachenko, E. M. Songhori, T. Schneider, and
F. Koushanfar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.8.1" class="ltx_text" style="font-size:90%;">Chameleon: A hybrid secure computation framework for machine learning
applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib95.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Asia Conference on Computer and Communications Security</em><span id="bib.bib95.11.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib96.4.4.1" class="ltx_text" style="font-size:90%;">Richtárik and Takáč [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib96.6.1" class="ltx_text" style="font-size:90%;">
P. Richtárik and M. Takáč.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.7.1" class="ltx_text" style="font-size:90%;">Distributed coordinate descent method for learning with big data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib96.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib96.9.2" class="ltx_text" style="font-size:90%;">, 17:2657–2681,
2016.
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib97.5.5.1" class="ltx_text" style="font-size:90%;">Rouhani et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib97.7.1" class="ltx_text" style="font-size:90%;">
B. D. Rouhani, M. S. Riazi, and F. Koushanfar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.8.1" class="ltx_text" style="font-size:90%;">Deepsecure: Scalable provably-secure deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib97.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Design Automation Conference</em><span id="bib.bib97.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib98.5.5.1" class="ltx_text" style="font-size:90%;">Samarakoon et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib98.7.1" class="ltx_text" style="font-size:90%;">
S. Samarakoon, M. Bennis, W. Saad, and M. Debbah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.8.1" class="ltx_text" style="font-size:90%;">Federated learning for ultra-reliable low-latency v2v communications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib98.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Global Communications Conference</em><span id="bib.bib98.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib99.5.5.1" class="ltx_text" style="font-size:90%;">Sattler et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib99.7.1" class="ltx_text" style="font-size:90%;">
F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib99.8.1" class="ltx_text" style="font-size:90%;">Robust and communication-efficient federated learning from non-iid
data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib99.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1903.02891</em><span id="bib.bib99.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib100.4.4.1" class="ltx_text" style="font-size:90%;">Schmidt and Roux [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib100.6.1" class="ltx_text" style="font-size:90%;">
M. Schmidt and N. L. Roux.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib100.7.1" class="ltx_text" style="font-size:90%;">Fast convergence of stochastic gradient descent under a strong growth
condition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib100.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1308.6370</em><span id="bib.bib100.9.2" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib101.5.5.1" class="ltx_text" style="font-size:90%;">Seide et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib101.7.1" class="ltx_text" style="font-size:90%;">
F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib101.8.1" class="ltx_text" style="font-size:90%;">1-bit stochastic gradient descent and its application to
data-parallel distributed training of speech dnns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib101.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib101.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Speech Communication Association</em><span id="bib.bib101.11.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib102.4.4.1" class="ltx_text" style="font-size:90%;">Shalev-Shwartz and Zhang [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib102.6.1" class="ltx_text" style="font-size:90%;">
S. Shalev-Shwartz and T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib102.7.1" class="ltx_text" style="font-size:90%;">Accelerated mini-batch stochastic dual coordinate ascent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib102.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib102.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib102.10.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib103.4.4.1" class="ltx_text" style="font-size:90%;">Shamir and Srebro [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib103.6.1" class="ltx_text" style="font-size:90%;">
O. Shamir and N. Srebro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib103.7.1" class="ltx_text" style="font-size:90%;">Distributed stochastic optimization and learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib103.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib103.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Allerton Conference on Communication, Control, and
Computing</em><span id="bib.bib103.10.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib104.5.5.1" class="ltx_text" style="font-size:90%;">Shamir et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib104.7.1" class="ltx_text" style="font-size:90%;">
O. Shamir, N. Srebro, and T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib104.8.1" class="ltx_text" style="font-size:90%;">Communication-efficient distributed optimization using an approximate
newton-type method.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib104.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib104.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib104.11.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib105.5.5.1" class="ltx_text" style="font-size:90%;">Silva et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib105.7.1" class="ltx_text" style="font-size:90%;">
S. Silva, B. Gutman, E. Romero, P. M. Thompson, A. Altmann, and M. Lorenzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib105.8.1" class="ltx_text" style="font-size:90%;">Federated learning in distributed medical databases: Meta-analysis of
large-scale subcortical brain data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib105.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.08553</em><span id="bib.bib105.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib106.5.5.1" class="ltx_text" style="font-size:90%;">Smith et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib106.7.1" class="ltx_text" style="font-size:90%;">
V. Smith, C.-K. Chiang, M. Sanjabi, and A. Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib106.8.1" class="ltx_text" style="font-size:90%;">Federated multi-task learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib106.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib106.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib106.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib107.5.5.1" class="ltx_text" style="font-size:90%;">Smith et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib107.7.1" class="ltx_text" style="font-size:90%;">
V. Smith, S. Forte, C. Ma, M. Takac, M. I. Jordan, and M. Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib107.8.1" class="ltx_text" style="font-size:90%;">Cocoa: a general framework for communication-efficient distributed
optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib107.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib107.10.2" class="ltx_text" style="font-size:90%;">, 18:1–47,
2018.
</span>
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib108.4.4.1" class="ltx_text" style="font-size:90%;">Stich [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib108.6.1" class="ltx_text" style="font-size:90%;">
S. U. Stich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib108.7.1" class="ltx_text" style="font-size:90%;">Local sgd converges fast and communicates little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib108.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib108.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib108.10.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib109.5.5.1" class="ltx_text" style="font-size:90%;">Tandon et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib109.7.1" class="ltx_text" style="font-size:90%;">
R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib109.8.1" class="ltx_text" style="font-size:90%;">Gradient coding: Avoiding stragglers in distributed learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib109.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib109.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib109.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib110.4.4.1" class="ltx_text" style="font-size:90%;">Tanenbaum and Van Steen [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib110.6.1" class="ltx_text" style="font-size:90%;">
A. S. Tanenbaum and M. Van Steen.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib110.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Distributed systems: principles and paradigms</em><span id="bib.bib110.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib110.9.1" class="ltx_text" style="font-size:90%;">Prentice-Hall, 2007.
</span>
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib111.5.5.1" class="ltx_text" style="font-size:90%;">Tang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib111.7.1" class="ltx_text" style="font-size:90%;">
H. Tang, S. Gan, C. Zhang, T. Zhang, and J. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib111.8.1" class="ltx_text" style="font-size:90%;">Communication compression for decentralized training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib111.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib111.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib111.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib112.5.5.1" class="ltx_text" style="font-size:90%;">Tang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib112.7.1" class="ltx_text" style="font-size:90%;">
H. Tang, C. Yu, C. Renggli, S. Kassing, A. Singla, D. Alistarh, J. Liu, and
C. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib112.8.1" class="ltx_text" style="font-size:90%;">Distributed learning over unreliable networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib112.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib112.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib112.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib113.5.5.1" class="ltx_text" style="font-size:90%;">Thakkar et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib113.7.1" class="ltx_text" style="font-size:90%;">
O. Thakkar, G. Andrew, and H. B. McMahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib113.8.1" class="ltx_text" style="font-size:90%;">Differentially private learning with adaptive clipping.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib113.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.03871</em><span id="bib.bib113.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib114.4.4.1" class="ltx_text" style="font-size:90%;">Thrun and Pratt [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib114.6.1" class="ltx_text" style="font-size:90%;">
S. Thrun and L. Pratt.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib114.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Learning to learn</em><span id="bib.bib114.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib114.9.1" class="ltx_text" style="font-size:90%;">Springer Science &amp; Business Media, 2012.
</span>
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib115.4.4.1" class="ltx_text" style="font-size:90%;">Van Berkel [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib115.6.1" class="ltx_text" style="font-size:90%;">
C. Van Berkel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib115.7.1" class="ltx_text" style="font-size:90%;">Multi-core for mobile phones.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib115.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib115.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Design, Automation and Test in Europe</em><span id="bib.bib115.10.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib116.5.5.1" class="ltx_text" style="font-size:90%;">Vaswani et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib116.7.1" class="ltx_text" style="font-size:90%;">
S. Vaswani, F. Bach, and M. Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib116.8.1" class="ltx_text" style="font-size:90%;">Fast and faster convergence of sgd for over-parameterized models (and
an accelerated perceptron).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib116.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib116.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Artificial Intelligence and Statistics</em><span id="bib.bib116.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib117.5.5.1" class="ltx_text" style="font-size:90%;">Vepakomma et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib117.7.1" class="ltx_text" style="font-size:90%;">
P. Vepakomma, O. Gupta, A. Dubey, and R. Raskar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib117.8.1" class="ltx_text" style="font-size:90%;">Reducing leakage in distributed deep learning for sensitive health
data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib117.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.00564</em><span id="bib.bib117.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib118.4.4.1" class="ltx_text" style="font-size:90%;">Wagner and Eckhoff [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib118.6.1" class="ltx_text" style="font-size:90%;">
I. Wagner and D. Eckhoff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib118.7.1" class="ltx_text" style="font-size:90%;">Technical privacy metrics: a systematic survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib118.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Computing Surveys</em><span id="bib.bib118.9.2" class="ltx_text" style="font-size:90%;">, 51:57, 2018.
</span>
</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib119.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib119.7.1" class="ltx_text" style="font-size:90%;">
H. Wang, S. Sievert, S. Liu, Z. Charles, D. Papailiopoulos, and S. Wright.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib119.8.1" class="ltx_text" style="font-size:90%;">Atomo: Communication-efficient learning via atomic sparsification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib119.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib119.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib119.11.3" class="ltx_text" style="font-size:90%;">,
2018a.
</span>
</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib120.4.4.1" class="ltx_text" style="font-size:90%;">Wang and Joshi [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib120.6.1" class="ltx_text" style="font-size:90%;">
J. Wang and G. Joshi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib120.7.1" class="ltx_text" style="font-size:90%;">Cooperative sgd: A unified framework for the design and analysis of
communication-efficient sgd algorithms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib120.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1808.07576</em><span id="bib.bib120.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib121.4.4.1" class="ltx_text" style="font-size:90%;">Wang and Joshi [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib121.6.1" class="ltx_text" style="font-size:90%;">
J. Wang and G. Joshi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib121.7.1" class="ltx_text" style="font-size:90%;">Adaptive communication strategies to achieve the best error-runtime
trade-off in local-update sgd.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib121.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib121.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Systems and Machine Learning</em><span id="bib.bib121.10.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib122.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib122.7.1" class="ltx_text" style="font-size:90%;">
S. Wang, F. Roosta-Khorasani, P. Xu, and M. W. Mahoney.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib122.8.1" class="ltx_text" style="font-size:90%;">Giant: Globally improved approximate newton method for distributed
optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib122.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib122.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib122.11.3" class="ltx_text" style="font-size:90%;">,
2018b.
</span>
</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib123.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib123.7.1" class="ltx_text" style="font-size:90%;">
S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib123.8.1" class="ltx_text" style="font-size:90%;">Adaptive federated learning in resource constrained edge computing
systems.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib123.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal on Selected Areas in Communications</em><span id="bib.bib123.10.2" class="ltx_text" style="font-size:90%;">, 37:1205–1221, 2019.
</span>
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib124.4.4.1" class="ltx_text" style="font-size:90%;">WeBank AI Group [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib124.6.1" class="ltx_text" style="font-size:90%;">
WeBank AI Group.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib124.7.1" class="ltx_text" style="font-size:90%;">Federated learning white paper v1.0.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib124.8.1" class="ltx_text" style="font-size:90%;">2018.
</span>
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib125.5.5.1" class="ltx_text" style="font-size:90%;">Woodworth et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib125.7.1" class="ltx_text" style="font-size:90%;">
B. Woodworth, J. Wang, A. Smith, B. McMahan, and N. Srebro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib125.8.1" class="ltx_text" style="font-size:90%;">Graph oracle models, lower bounds, and gaps for parallel stochastic
optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib125.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib125.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib125.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib126.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib126.7.1" class="ltx_text" style="font-size:90%;">
X. Wu, F. Li, A. Kumar, K. Chaudhuri, S. Jha, and J. Naughton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib126.8.1" class="ltx_text" style="font-size:90%;">Bolt-on differential privacy for scalable stochastic gradient
descent-based analytics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib126.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib126.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Management of Data</em><span id="bib.bib126.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib127.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib127.7.1" class="ltx_text" style="font-size:90%;">
Q. Yang, Y. Liu, T. Chen, and Y. Tong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib127.8.1" class="ltx_text" style="font-size:90%;">Federated machine learning: Concept and applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib127.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on Intelligent Systems and Technology</em><span id="bib.bib127.10.2" class="ltx_text" style="font-size:90%;">,
10:12, 2019.
</span>
</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib128.4.4.1" class="ltx_text" style="font-size:90%;">Yang [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib128.6.1" class="ltx_text" style="font-size:90%;">
T. Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib128.7.1" class="ltx_text" style="font-size:90%;">Trading computation for communication: Distributed stochastic dual
coordinate ascent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib128.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib128.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib128.10.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib129.5.5.1" class="ltx_text" style="font-size:90%;">Yao et al. [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib129.7.1" class="ltx_text" style="font-size:90%;">
Y. Yao, L. Rosasco, and A. Caponnetto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib129.8.1" class="ltx_text" style="font-size:90%;">On early stopping in gradient descent learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib129.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Constructive Approximation</em><span id="bib.bib129.10.2" class="ltx_text" style="font-size:90%;">, 26:289–315, 2007.
</span>
</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib130.5.5.1" class="ltx_text" style="font-size:90%;">Yin et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib130.7.1" class="ltx_text" style="font-size:90%;">
D. Yin, A. Pananjady, M. Lam, D. Papailiopoulos, K. Ramchandran, and
P. Bartlett.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib130.8.1" class="ltx_text" style="font-size:90%;">Gradient diversity: a key ingredient for scalable distributed
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib130.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib130.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Artificial Intelligence and Statistics</em><span id="bib.bib130.11.3" class="ltx_text" style="font-size:90%;">, pages
1998–2007, 2018.
</span>
</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib131.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib131.7.1" class="ltx_text" style="font-size:90%;">
H. Yu, S. Yang, and S. Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib131.8.1" class="ltx_text" style="font-size:90%;">Parallel restarted sgd for non-convex optimization with faster
convergence and less communication.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib131.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib131.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI Conference on Artificial Intelligence</em><span id="bib.bib131.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib132.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib132.7.1" class="ltx_text" style="font-size:90%;">
H. Yu, R. Jin, and S. Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib132.8.1" class="ltx_text" style="font-size:90%;">On the linear speedup analysis of communication efficient momentum
sgd for distributed non-convex optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib132.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib132.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib132.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib133.4.4.1" class="ltx_text" style="font-size:90%;">Yuan and Yu [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib133.6.1" class="ltx_text" style="font-size:90%;">
J. Yuan and S. Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib133.7.1" class="ltx_text" style="font-size:90%;">Privacy preserving back-propagation neural network learning made
practical with cloud computing.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib133.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Parallel and Distributed Systems</em><span id="bib.bib133.9.2" class="ltx_text" style="font-size:90%;">,
25:212–221, 2013.
</span>
</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib134.5.5.1" class="ltx_text" style="font-size:90%;">Yurochkin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib134.7.1" class="ltx_text" style="font-size:90%;">
M. Yurochkin, M. Agarwal, S. Ghosh, K. Greenewald, T. N. Hoang, and
Y. Khazaeni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib134.8.1" class="ltx_text" style="font-size:90%;">Bayesian nonparametric federated learning of neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib134.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib134.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib134.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib135.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib135.7.1" class="ltx_text" style="font-size:90%;">
H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib135.8.1" class="ltx_text" style="font-size:90%;">ZipML: Training linear models with end-to-end low precision, and a
little bit of deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib135.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib135.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib135.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib136.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2015a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib136.7.1" class="ltx_text" style="font-size:90%;">
S. Zhang, A. E. Choromanska, and Y. LeCun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib136.8.1" class="ltx_text" style="font-size:90%;">Deep learning with elastic averaging sgd.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib136.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib136.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib136.11.3" class="ltx_text" style="font-size:90%;">,
2015a.
</span>
</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib137.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2015b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib137.7.1" class="ltx_text" style="font-size:90%;">
Y. Zhang, J. Duchi, and M. Wainwright.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib137.8.1" class="ltx_text" style="font-size:90%;">Divide and conquer kernel ridge regression: A distributed algorithm
with minimax optimal rates.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib137.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib137.10.2" class="ltx_text" style="font-size:90%;">, 16:3299–3340,
2015b.
</span>
</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib138.5.5.1" class="ltx_text" style="font-size:90%;">Zhao et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib138.7.1" class="ltx_text" style="font-size:90%;">
Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib138.8.1" class="ltx_text" style="font-size:90%;">Federated learning with non-iid data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib138.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1806.00582</em><span id="bib.bib138.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib139.5.5.1" class="ltx_text" style="font-size:90%;">Zhao et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib139.7.1" class="ltx_text" style="font-size:90%;">
Y. Zhao, J. Zhao, L. Jiang, R. Tan, and D. Niyato.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib139.8.1" class="ltx_text" style="font-size:90%;">Mobile edge computing, blockchain and reputation-based crowdsourcing
iot federated learning: A secure, decentralized and privacy-preserving
system.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib139.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.10893</em><span id="bib.bib139.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib140.5.4.1" class="ltx_text" style="font-size:90%;">Zhou and Cong [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib140.7.1" class="ltx_text" style="font-size:90%;">
F. Zhou and G. Cong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib140.8.1" class="ltx_text" style="font-size:90%;">On the convergence properties of a </span><math id="bib.bib140.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="bib.bib140.1.m1.1a"><mi mathsize="90%" id="bib.bib140.1.m1.1.1" xref="bib.bib140.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="bib.bib140.1.m1.1b"><ci id="bib.bib140.1.m1.1.1.cmml" xref="bib.bib140.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib140.1.m1.1c">k</annotation></semantics></math><span id="bib.bib140.9.2" class="ltx_text" style="font-size:90%;">-step averaging stochastic
gradient descent algorithm for nonconvex optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib140.10.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib140.11.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Joint Conference on Artificial Intelligence</em><span id="bib.bib140.12.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib141.5.5.1" class="ltx_text" style="font-size:90%;">Zinkevich et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib141.7.1" class="ltx_text" style="font-size:90%;">
M. Zinkevich, M. Weimer, L. Li, and A. J. Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib141.8.1" class="ltx_text" style="font-size:90%;">Parallelized stochastic gradient descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib141.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib141.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib141.11.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1908.07872" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1908.07873" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1908.07873">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1908.07873" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1908.07874" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 18:03:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
