<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2312.01550] Using human and robot synthetic data for training smart hand tools</title><meta property="og:description" content="The future of work does not require a choice between human and robot. Aside from explicit human-robot collaboration, robotics can play an increasingly important role in helping train workers as well as the tools they m…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Using human and robot synthetic data for training smart hand tools">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Using human and robot synthetic data for training smart hand tools">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2312.01550">

<!--Generated on Tue Feb 27 15:52:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Using human and robot synthetic data for training smart hand tools
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jose Bendaña<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Sundar Sripada V. S.<sup id="id9.9.id2" class="ltx_sup"><span id="id9.9.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Carlos D. Salazar<sup id="id10.10.id3" class="ltx_sup"><span id="id10.10.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Sandeep Chinchali<sup id="id11.11.id4" class="ltx_sup"><span id="id11.11.id4.1" class="ltx_text ltx_font_italic">2</span></sup> and Raul G. Longoria<sup id="id12.12.id5" class="ltx_sup"><span id="id12.12.id5.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"><sup id="id13.13.id1" class="ltx_sup"><span id="id13.13.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Department of Mechanical Engineering, <sup id="id14.14.id2" class="ltx_sup"><span id="id14.14.id2.1" class="ltx_text ltx_font_italic">2</span></sup>Department of Electrical and Computer Engineering, The University of Texas at Austin</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">The future of work does not require a choice between human and robot. Aside from explicit human-robot collaboration, robotics can play an increasingly important role in helping train workers as well as the tools they may use, especially in complex tasks that may be difficult to automate or effectively roboticize. This paper introduces a form of smart tool for use by human workers and shows how training the tool for task recognition, one of the key requirements, can be accomplished. Machine learning (ML) with purely human-based data can be extremely laborious and time-consuming. First, we show how data synthetically-generated by a robot can be leveraged in the ML training process. Later, we demonstrate how fine-tuning ML models for individual physical tasks and workers can significantly scale up the benefits of using ML to provide this feedback. Experimental results show the effectiveness and scalability of our approach, as we test data size versus accuracy. Smart hand tools of the type introduced here can provide insights and real-time analytics on efficient and safe tool usage and operation, thereby enhancing human participation and skill in a wide range of work environments. Using robotic platforms to help train smart tools will be essential, particularly given the diverse types of applications for which smart hand tools are envisioned for human use.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Machine learning (ML) for enabling devices and systems to support humans in the future of human work is poised for significant growth in the upcoming decades. This holds true especially for collaborative robots, and recently for smart hand tools designed to monitor and enhance human user skills and safety. The initial phase of smart hand tool development entails the seamless integration of sensors and onboard intelligence. In this context, ML has the potential to harness sensor data for various purposes, including the identification of ongoing work tasks, facilitating real-time detection of issues related to tool usage, and recognizing potential safety concerns.
</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Nevertheless, ML algorithms are inherently data-hungry, which presents a formidable challenge in situations where human operators play a crucial role in data acquisition. This issue is further exacerbated when considering the myriad ways individuals manipulate, wield, and utilize tools to accomplish identical tasks. Synthetic data has emerged as a promising solution for addressing this data requirement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This approach involves using algorithms to generate data that mimics real-world scenarios, subsequently employed for training ML models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, a persisting challenge revolves around the generation of synthetic data that faithfully replicates the intricacies of human-generated data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">When examining current instances of synthetic data implementation within robotics applications, such as pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and sim-to-real transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, it becomes evident that the predominant focus centers on leveraging simulations to generate new data. The key difference in our approach lies in its departure from this trend, as we create synthetic data by utilizing a Yaskawa Robot Arm using a smart hand tool equipped with edge devices (Arduino/RaspberryPi) to collect physical sensor data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, rather than relying solely on simulation (see Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). In this context, our technical contributions to prior work are fourfold:</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2312.01550/assets/assets/1_teaser.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Robot for synthetic data generation.<span id="S1.F1.4.2.1" class="ltx_text ltx_font_medium"> An industry-grade Yaskawa robot arm equipped with the smart hand tool at its end-effector. Robot arms like this offer a potential solution to the massive data demands inherent in machine learning tasks applied to physical applications, such as mechanical engineering.</span></span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Smart Hand Tool Development:</span> We have equipped a rotary power tool (RPT) with a smart tool module (STM). This module comprises an inertial measurement unit (IMU), a current sensor, and a microphone. The amalgamation of these sensors equips us with data that empowers the discrimination of various tasks performed by the tool. These tasks encompass routing, sanding, engraving, and cutting.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Data Collection:</span> We generated synthetic data using a robot arm capable of simulating four commonplace tasks frequently encountered by a human using a rotary power tool (RPT). The dataset generated through our methodology encompasses 11 distinct physical signals, meticulously measured by the smart tool module (STM).</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Measuring Data Efficacy:</span> To quantitatively assess the efficacy of the proposed data-centric approach, we conduct a comparative analysis. This evaluation involves contrasting the performance of zero-shot training, using data collected from the smart hand tool operated by humans against the fine-tuning of a model pre-trained using synthetic data obtained from the robot arm equipped with the RPT. These experiments are discussed in detail in Section <a href="#S4" title="IV Experimental Results ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Open-Source Contribution:</span> We have made available approximately 20 hours of data that encompasses recordings collected from the rotary power tool by humans and the robot arm. In addition, pre-trained models using the synthetic data generated by the Yaskawa robot are also available at: <a target="_blank" href="https://github.com/UTAustin-SwarmLab/Smart-Tools" title="" class="ltx_ref ltx_href">https://github.com/UTAustin-SwarmLab/Smart-Tools</a></p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Humans have continuously evolved hand tool technology, and this work lends to ongoing efforts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, notably taking advantage of sensing and machine learning innovations. While the intent is to leave the tool in the hand of the human, robotic partners are essential. Past and present works have examined how robots can be trained to manipulate tools, in many cases by learning from humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Indeed, the latter is very complicated, as humans excel in manipulation tasks where tools interact with the environment. In contrast to cited works, the robot used here is focused on replicating basic work tasks in an open-loop programmatic fashion, as will be described. This is found to be sufficient for the purposes of generating synthetic data for this preliminary study. Robots have been used for data collection and synthetic data generation for over a decade. Some of these applications involve perception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, soft robotics to mimic human dexterous motion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and more. We also distinguish our work from human-robot collaborative tool use, such as by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Our work tries to harness robotic data collection capabilities by extending it towards general human tool use.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Monitoring sensors from tools and processes is not unlike the trends seen in smart manufacturing, where streaming sensor data is enabled by Internet-of-Things (IoT) devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Such data has been used to monitor machine health, detect anomalies, and even adjust process parameters to optimize yield. Smart manufacturing has partly been motivated by the demand for customized products in small batches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, such as in 3D-printing, or to improve sustainability of production schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. In addition, the need to support humans in work is essential in tasks related to construction, maintenance, and other applications where robotic solutions may not be available. Lastly, prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> examines the needs and implications through a broader socio-technical context.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">System Architecture</span>
</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2312.01550/assets/assets/2_pipeline.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="367" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">System architecture.<span id="S3.F2.4.2.1" class="ltx_text ltx_font_medium"> Data from multiple sensors is collected separately from both human subjects and a Yaskawa robot. Initially, the robot-collected data is used for pre-training a model. Later, the same model is fine-tuned on human-collected data on a per-subject basis, for better individual performance. The significance of this architecture lies in its scalability: pre-training robot-collected data can improve performance for increasing number of subjects and tasks.</span></span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Smart Tool Module</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The apparatus developed in this work serves as a platform for demonstrating real-world applications of a smart tool concept. The intent is to identify best practices in smart hand tool design for a given work application. Two prototype sensor modules were used in this work, both composed of a 9-axis inertial measurement unit (IMU), a microphone, and a current sensor. When the tool is in operation, the sensor data is recorded and analyzed, one module by a RaspberryPi microcomputer, the other by an Arduino Nano BLE sense. One prototype, a smart tool module (STM), had a small OLED screen embedded, for communicating with the operator, and a custom-made encasing to protect the sensors and computer from the environment. The IMU, microphone, OLED screen, and 5-way switch are mounted to the STM and placed in a 3D-printed box encased in aluminum. The setup can be replicated with low-effort soldering and manufacturing skills.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">A rotary power tool (RPT) was chosen as the platform for testing the smart tool concept (Model 4300AC, Dremel, Racine, WI, USA). A typical RPT is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This hand-held power tool has an electrical AC motor directly coupled to a drive-shaft that can be manually adjusted to rotate at a desired speed (5,000 to 35,000 revolutions per minute, or rpm) by a user. Different tool bits can be attached to the drive-shaft to change the tool function. A RPT is not typically used in applications requiring high levels of applied force. Rather, a user guides the tool, controlling how the tool interacts with a workpiece. A wide range of attachments enable a RPT to be used for different purposes such as polishing, grinding, cutting, and routing. This versatility and the diverse manual control actions required by a human user are key reasons for selecting a RPT for this preliminary study in smart tool development.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Yaskawa Robot</span>
</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2312.01550/assets/assets/3_ST_Clamp_DAQ.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Yaskawa robot arm with the smart hand tool attached.<span id="S3.F3.4.2.1" class="ltx_text ltx_font_medium"> The robot operates within an enclosed area to prevent the dispersion of dust generated during the tool’s operation. Different parts of the tool are labeled in the figure. In this experiment, the smart hand tool is equipped with a wood-cutting wheel, which is employed for the purpose of slicing a cylindrical wooden rod (see Section <a href="#S3.SS3" title="III-C Data Collection ‣ III System Architecture ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>). A simple setup like this is able to mimic humans performing the tasks (cutting in this case) in the real world.</span></span></figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To accomplish the tasks set out in Section <a href="#S4" title="IV Experimental Results ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and produce synthetic data, a programmable automatic machine is needed. In this case, we selected the Yaskawa SDA10D robot due to its ability to execute movements similar to those of a human using a hand tool. The Yaskawa SDA10D robot is an industrial robot that is equipped with two robotic arms that operate independently. The robot features dual 7-axis arms that enable it to exhibit human-like flexibility when performing a variety of movements. Moreover, its programming environment is highly adaptable, as it provides the ability to modify not only position values but also speed and acceleration. The robot can be controlled by manipulating each of its joints individually, or alternatively, by regulating the XYZ position of the end-effector.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Prior to programming the movement patterns and constructing the tool-fastening system to the end-effector of the robot arm, the variables involved in routing, sanding, engraving, and cutting processes that a human would undertake were examined. Human execution of these tasks is not consistent, as it is a stochastic process due to varying compliance of a user’s arm as it reacts to the resistance of the material being machined. In addition, the anisotropic material of wood, as used for these experiments, also contributes to variability. Furthermore, factors such as speed, trajectory, working depth, working and travel angles generated by humans are subject to variation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Data Collection</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Our software stack comprises three main components: data collection, data cleaning, and model building. The data collection process is facilitated by an Arduino microcontroller that communicates with the sensor module on the RPT. The Arduino is responsible for collecting dataframes in the form of CSV files containing the sensor readings. The data collection process was carried out using two methods: the first involved human subjects performing the tasks, while the second utilized the Yaskawa Robot, described as follows.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The same RPT, sensor suite, and experimental setup used during human data collection was mounted on the robotic arm for data collection purposes. To imitate human user use and operating variability, and owing to the fact that robotic movements are more uniform, a compliant robotic arm-tool attachment was developed. Passive compliance, introduced by a rubber insert as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ III-B Yaskawa Robot ‣ III System Architecture ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, introduces variations in trajectory, compliance, work angles and travel angles. Variability in working depth and speed was introduced by programming of the robot.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Movement patterns for each activity were programmed and executed using Cartesian control based on the coordinates of the robot arm’s end-effector. Specifically, XYZ coordinates were determined to simulate human-like movements of routing, sanding, engraving, and cutting with a RPT tool. The working depth was programmed based on the maximum and minimum values observed during human performance using the RPT, with the depth value being adjusted with each tool pass. A trapezoidal velocity profile was selected for the Cartesian control programming. Combining these parameters allowed for a better dispersion of data that is comparable to that produced by a human user. All of the programming was carried out using Robot Operating System (ROS1), on version Noetic Ninjemys (ROS Noetic) in C++. The movement patterns were encoded into a 6-DOF vector for each task.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Once data was collected, we performed data cleaning to remove outliers. The human-collected data was found to be relatively “dirtier” than the robot-collected data, and we removed about 30% of the human data after outlier removal. In contrast, the robot-collected data was cleaner as the robot was programmed to perform the tasks consistently for every run. Only about 10% of robot-collected data was removed after discounting outliers.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Consenting volunteers were instructed on how to use the instrumented RPT in the four activities: cutting, engraving, routing, and sanding, with the bits shown in Figure <a href="#S3.F4" title="Figure 4 ‣ III-C Data Collection ‣ III System Architecture ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. A high-level verbal description with minimal instructions were given for each activity as follows.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2312.01550/assets/x1.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Tool bits.<span id="S3.F4.4.2.1" class="ltx_text ltx_font_medium"> From left to right: A wood cutting wheel, an engraving bit, a routing bit, and a sanding drum. Each bit allows the worker to perform a different activity associated with it. These bits were attached to the RPT as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ III-B Yaskawa Robot ‣ III System Architecture ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</span></span></figcaption>
</figure>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Cutting.</span> The 1.5 inch wood cutting wheel is installed on the tool and a 0.5 inch diameter round wooden dowel rod is secured to a workbench. The user is asked to slice approximately 1/4 inch discs over a time period.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.1" class="ltx_p"><span id="S3.SS3.p7.1.1" class="ltx_text ltx_font_bold">Engraving.</span> The engraving bit is installed on the rotary tool and a piece of standard ‘2-by-4’ lumber measuring 12 inches in length was secured to a workbench. Each user was asked to engrave numbers from zero through nine continuously over a time period.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<p id="S3.SS3.p8.1" class="ltx_p"><span id="S3.SS3.p8.1.1" class="ltx_text ltx_font_bold">Routing.</span> The straight routing bit is installed on the rotary tool and a piece of standard ‘2-by-4’ lumber measuring 12 inches in length was secured to a workbench. Each user routes approximately one-eighth inch deep grooves across the width of the wood over a designated time period.</p>
</div>
<div id="S3.SS3.p9" class="ltx_para">
<p id="S3.SS3.p9.1" class="ltx_p"><span id="S3.SS3.p9.1.1" class="ltx_text ltx_font_bold">Sanding.</span> The sanding drum is installed on the rotary tool. The subject creates a one half inch <em id="S3.SS3.p9.1.2" class="ltx_emph ltx_font_italic">chamfer</em> on every edge of the ‘2-by-4’ lumber over a designated time period.</p>
</div>
<div id="S3.SS3.p10" class="ltx_para">
<p id="S3.SS3.p10.1" class="ltx_p">The participants were given instructions on how to set-up the different tool bits. Some pointers on how to use the tool for a given task were provided, but users were given freedom to achieve each task based on their own comfort with the tool, experience, and unique style. This approach was taken to allow for variability in how each person operated the tool and completed the task.</p>
</div>
<div id="S3.SS3.p11" class="ltx_para">
<p id="S3.SS3.p11.1" class="ltx_p">Data collection was conducted for about a one and a half month period. A total of seven participants (five graduate students and two faculty) used the instrumented RPT to work on wooden samples. Each participant performed nine runs of each of the 4 tasks. Each sample run was set for three minutes. A total of 204 sample runs or 612 minutes of data was collected. The nine runs were broken down into 3 runs of training data, 3 runs of validation data, and 3 runs of testing data. After each test, the ‘2-by-4’ wood samples were collected and organized for future analysis. In addition, each run was video recorded for future research work using images and raw sound.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.01550/assets/assets/Yaskawa_Example.png" id="S3.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.01550/assets/assets/Human_Example.png" id="S3.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.5.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Typical current draw and accelerations during a routing task.<span id="S3.F5.6.2.1" class="ltx_text ltx_font_medium"> The robot data (top) reflects a much more consistent motion compared to human data (bottom) due to differences in tool usage. Similar patterns were seen for similar tasks, but distinct across task types. This alludes to a pre-trained approach in the ML pipeline.</span></span></figcaption>
</figure>
<div id="S3.SS3.p12" class="ltx_para">
<p id="S3.SS3.p12.1" class="ltx_p">After collecting data from each subject performing the four different activities, three CSV files were concatenated into three different data frames containing the raw data for train, validate, and test data. These raw data frames were then preprocessed to produce offline training data for the ML models. Data pre-processing involved breaking down the data into 10 second frames with 50% overlap. The frame size was selected through an iterative process which
resulted in 10 seconds producing the best results, while the 50% overlap was chosen to feed the model more data. Ten statistical features (minimum, maximum, kurtosis, standard error of the mean, standard deviation, variance, sample skewness, median absolute deviation, and sum) where extracted from each sensor measurement (9 axis IMU, microphone, current sensor) for a total of 110 features. Subsequently, the data was normalized to values from 0 to 1.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experimental Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we present results obtained from our study that aim to answer three key hypotheses. Firstly, we compare the distributions of raw sensor data values when collected for humans versus the Yaskawa robot. Secondly, we compare the performance of zero-shot training on data collected from the tool used by humans with the fine-tuning of a pre-trained model trained on synthetic data obtained from the Yaskawa robot. Lastly, we investigate fine-tuning on individual subjects using the pre-trained model on synthetic data. The next paragraphs discuss the three motives, respectively.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_bold">Hypothesis 1: Data from the robot can be effectively used for pre-training an ML model.</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Firstly, we compare typical experimental realizations of data collected during robot and human-use in Figure <a href="#S3.F5" title="Figure 5 ‣ III-C Data Collection ‣ III System Architecture ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. It is clear that, although the Yaskawa robot’s movements were sharper and more consistent, they provide a baseline for signal features one can observe from during these activities. As the human subjects pick up the tool to perform a routing pass and proceed to move the tool laterally along the lumber repetitively, a candid periodicity is observed which the robot neatly replicates. This empirical confirmation successfully alluded to using the robot in our ML training pipeline.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2312.01550/assets/assets/6_sensor_comparisons.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.4.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Grouped boxplot comparing raw sensor values.<span id="S4.F6.2.1.1" class="ltx_text ltx_font_medium"> The x-axis is each sensor on the STM attached to the RPT. The y-axis is the range of values normalized from <math id="S4.F6.2.1.1.m1.2" class="ltx_Math" alttext="(-1,1)" display="inline"><semantics id="S4.F6.2.1.1.m1.2b"><mrow id="S4.F6.2.1.1.m1.2.2.1" xref="S4.F6.2.1.1.m1.2.2.2.cmml"><mo stretchy="false" id="S4.F6.2.1.1.m1.2.2.1.2" xref="S4.F6.2.1.1.m1.2.2.2.cmml">(</mo><mrow id="S4.F6.2.1.1.m1.2.2.1.1" xref="S4.F6.2.1.1.m1.2.2.1.1.cmml"><mo id="S4.F6.2.1.1.m1.2.2.1.1b" xref="S4.F6.2.1.1.m1.2.2.1.1.cmml">−</mo><mn id="S4.F6.2.1.1.m1.2.2.1.1.2" xref="S4.F6.2.1.1.m1.2.2.1.1.2.cmml">1</mn></mrow><mo id="S4.F6.2.1.1.m1.2.2.1.3" xref="S4.F6.2.1.1.m1.2.2.2.cmml">,</mo><mn id="S4.F6.2.1.1.m1.1.1" xref="S4.F6.2.1.1.m1.1.1.cmml">1</mn><mo stretchy="false" id="S4.F6.2.1.1.m1.2.2.1.4" xref="S4.F6.2.1.1.m1.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F6.2.1.1.m1.2c"><interval closure="open" id="S4.F6.2.1.1.m1.2.2.2.cmml" xref="S4.F6.2.1.1.m1.2.2.1"><apply id="S4.F6.2.1.1.m1.2.2.1.1.cmml" xref="S4.F6.2.1.1.m1.2.2.1.1"><minus id="S4.F6.2.1.1.m1.2.2.1.1.1.cmml" xref="S4.F6.2.1.1.m1.2.2.1.1"></minus><cn type="integer" id="S4.F6.2.1.1.m1.2.2.1.1.2.cmml" xref="S4.F6.2.1.1.m1.2.2.1.1.2">1</cn></apply><cn type="integer" id="S4.F6.2.1.1.m1.1.1.cmml" xref="S4.F6.2.1.1.m1.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.2.1.1.m1.2d">(-1,1)</annotation></semantics></math> to account for different scales in different sensors. The diamonds in each x-axis entry represent the outliers of that distribution. The similarity of data distributions warrant the potential use of robot data for pre-training models.</span></span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Secondly, we investigate the distributions of the raw sensor values collected from our smart hand tool using human subjects as well as the Yaskawa robot. It is crucial to ensure that the distributions of both datasets are similar yet possess variations to account for real-world data distribution shifts.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">To address this, we use a grouped boxplot to examine each sensor’s raw value ranges. Each sensor is represented by two boxes that compare the robot-collected and the human-collected value distribution. This is shown in Figure <a href="#S4.F6" title="Figure 6 ‣ IV-A Hypothesis 1: Data from the robot can be effectively used for pre-training an ML model. ‣ IV Experimental Results ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Additionally, the diamonds in each x-axis entry represent the outliers of that distribution. This analysis enables us to gain insights into the similarities and differences between the human and robot-collected data. It shows that the robot and human data are roughly similar in variation, accounting for stochasticity of human use and wood heterogeneity. This similarity is essential for using robot-collected data to pre-train our classification model.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_bold">Hypothesis 2: Pre-training the model will help generalize well for human-collected data.</span>
</h3>

<figure id="S4.F7" class="ltx_figure"><img src="/html/2312.01550/assets/assets/7_data_vs_accuracy.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Test accuracy versus dataset volume.<span id="S4.F7.4.2.1" class="ltx_text ltx_font_medium"> Here we see the zero-shot training on all human subject data (in </span>red<span id="S4.F7.4.2.2" class="ltx_text ltx_font_medium">), versus fine-tuning a pre-trained model on the synthetic robot-collected data (in </span>green<span id="S4.F7.4.2.3" class="ltx_text ltx_font_medium">). In the plot, we measure the accuracy on an unseen test dataset (higher is better). Clearly, these trends follow the law of diminishing returns in ML. Moreover, the fine-tuning approach outperforms the zero-shot training by a quantifiable margin.</span></span></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Now, we focus on demonstrating the practical application of our approach. Most distributed ML systems in production generally house a large model in the server, and fine-tune local models for each specific client. With the data collected by the Yaskawa robot, we can train a large model and fine-tune it on human-collected data. Note that ‘large’ in this context means lots of data; the local models for each user will only be trained on a tool user’s collected data.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In this case, we experimented on all human-collected data. Subsequently, we perform the same analysis for individual human subject-collected data. To systematically evaluate the effectiveness of our approach, we train the model on varying percentages (x%) of human data in two ways: (1) <em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">zero-shot</em> training, where we train a new model simply on human-collected data, and (2) pre-trained fine-tuning, where we use a checkpoint of a model trained on all Yaskawa robot data. We use test set accuracy for evaluation, since this is multi-class classification. Test accuracy measures the accuracy of the model on unseen test data. The results are presented in Figure <a href="#S4.F7" title="Figure 7 ‣ IV-B Hypothesis 2: Pre-training the model will help generalize well for human-collected data. ‣ IV Experimental Results ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, which compares the two approaches across different percentages of training data.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_bold">Hypothesis 3: Pre-training the model quantifiably improves performance for individual subjects.</span>
</h3>

<figure id="S4.F8" class="ltx_figure"><img src="/html/2312.01550/assets/assets/8_id_ood_barplot.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="643" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Model performance on individual subjects.<span id="S4.F8.4.2.1" class="ltx_text ltx_font_medium"> This grouped barplot describes the differences in model performance on in-distribution (ID) versus out-of-distribution (OoD) test data. The red bars for each subject denote that fine-tuning on a pre-trained model is generally beneficial and boosts individual local model performance. </span></span></figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Finally, we fine-tune models for individual subjects. Test data for fine-tuning is only drawn from human-collected data that is isolated from the training pipeline. We conducted In-Distribution (ID) and Out-of-Distribution (OoD) tests. Separating each human subject’s data at the start and creating a training pipeline for all human data except for the subject in question is Out-of-Distribution testing. Using all human data including the subject in question for training and then testing on all subjects is testing In-Distribution.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The goal of this experiment is to determine if (a) pre-training works to boost accuracy, and (b) In-Distribution is better than Out-of-Distribution training and testing. The results are presented in Figure <a href="#S4.F8" title="Figure 8 ‣ IV-C Hypothesis 3: Pre-training the model quantifiably improves performance for individual subjects. ‣ IV Experimental Results ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, which is a bar plot comparing both OoD and ID testing. These results show that the green bars denoting pre-trained fine-tuning on In-Distribution data works best for our scale of models. Our key takeaway is that, by virtue of pre-training on robot data, accuracy on unseen human test data is improved by 11.4% on average (and as high as 36%), although this isn’t consistent in some subjects (subjects 5 and 6).</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Limitations and Future Work</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Our preliminary work shows that synthetic data can be a viable solution for bridging the data requirement gap in training ML models for human work task recognition. While our experiments show promise for scalability, a truer benchmark of scale is required using many more smart tools, Another key step is to investigate the data collection and ML model deployment ‘on tool’ using edge devices. This will be critical for online feedback and active assistance to a tool user. Lastly, human work tasks require ways to track quality of work. Some preliminary ideas for assessing quality of work for the RPT tasks investigated here are described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Future researchers are encouraged to use data we collected in <a href="#S3.SS3" title="III-C Data Collection ‣ III System Architecture ‣ Using human and robot synthetic data for training smart hand tools" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>, possibly from camera data of the ‘2-by-4’ lumber to study quality of work.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have presented a way for combining human and robot generated synthetic data to train a prototype smart hand tool intended for human use. Our experimental results show the effectiveness and scalability of fine-tuning machine learning models that can identify typical work tasks. However, there are limitations to our approach, such as the need for a model zoo to perform model selection and improve individual user’s experience, testing on a larger dataset of human subjects, and testing for quality of work. Overall, the proposed approach provides a promising direction for showing how synthetic data can be collected and used for smart hand tool development.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by funding from Good Systems, a research grand challenge at the University of Texas at Austin, Microsoft Research, and The MITRE Corporation. We thank Dr. Ashish D. Deshpande for providing access to the Yaskawa SDA10D robot for our synthetic data collection process. Elena Soto’s insight and assistance in programming the Yaskawa Robot for the synthetic data collection process is greatly appreciated. Finally, thanks to Burzin Balsara, Elena Soto, Jose Bendana, Kathy Hill, Pablo Pejlatowicz, Raul G. Longoria, and Sandeep Chinchali for manually performing tasks and helping collect many hours of human data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Markus Hittmeir, Andreas Ekelhart, and Rudolf Mayer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">On the utility of synthetic data: An empirical evaluation on machine
learning tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 14th International Conference on
Availability, Reliability and Security</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 1–6, 2019.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Sergey I Nikolenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Synthetic data for deep learning</span><span id="bib.bib2.3.2" class="ltx_text" style="font-size:90%;">, volume 174.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">Springer, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Josip Josifovski, Matthias Kerzel, Christoph Pregizer, Lukas Posniak, and
Stefan Wermter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Object detection and pose estimation based on convolutional neural
networks trained with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE/RSJ international conference on intelligent robots
and systems (IROS)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 6269–6276. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Sim-to-real transfer of robotic control with dynamics randomization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE international conference on robotics and automation
(ICRA)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 3803–3810. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Wenshuai Zhao, Jorge Peña Queralta, and Tomi Westerlund.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Sim-to-real transfer in deep reinforcement learning for robotics: a
survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE symposium series on computational intelligence
(SSCI)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 737–744. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Vijay Janapa Reddi, Alexander Elium, Shawn Hymel, David Tischler, Daniel
Situnayake, Carl Ward, Louis Moreau, Jenny Plunkett, Matthew Kelcey, Mathijs
Baaijens, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Edge Impulse: An MLOps Platform for Tiny Machine Learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of Machine Learning and Systems</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 5, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Amit Zoran, Roy Shilkrot, Pragun Goyal, Pattie Maes, and Joseph A. Paradiso.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">The wise chisel: The rise of the smart handheld tool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Pervasive Computing</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 13(3):48–57, 2014.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Jose Bendana.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Analyzing power driven hand tool operation using low-cost sensors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">Master’s thesis, The University of Texas at Austin, Mechanical
Engineering Department, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Sheng Liu and Haruhiko Asada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Transferring Manipulative Skills to Robots: Representation and
Acquisition of Tool Manipulative Skills Using a Process Dynamics Model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Dynamic Systems, Measurement, and Control</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">,
114(2):220–228, 06 1992.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
S. Liu and H. Asada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Teaching and learning of deburring robots using neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">[1993] Proceedings IEEE International Conference on Robotics
and Automation</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 339–345 vol.3, 1993.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Rachel Holladay, Tomás Lozano-Pérez, and Alberto Rodriguez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Force-and-motion constrained planning for tool use.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 7409–7416, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yuki Shirai, Devesh K. Jha, Arvind U. Raghunathan, and Dennis Hong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Tactile tool manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2023 IEEE International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 12597–12603, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Yunzhu Li, Jun-Yan Zhu, Russ Tedrake, and Antonio Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Connecting touch and vision via cross-modal prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 10609–10618, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Abhinav Gupta, Pieter Abbeel, and
Lerrel Pinto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Visual imitation made easy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Robot Learning</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 1992–2005. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Haokun Wang, Xiaobo Liu, Nuofan Qiu, Ning Guo, Fang Wan, and Chaoyang Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Deepclaw 2.0: A data collection platform for learning human
manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Frontiers in Robotics and AI</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, page 38, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Rundong Tian and Eric Paulos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Adroid: Augmenting hands-on making with a collaborative robot.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The 34th Annual ACM Symposium on User Interface Software and
Technology</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, page 270–281, New York, NY, USA, 2021. Association for
Computing Machinery.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Francesco Longo, Letizia Nicoletti, and Antonio Padovano.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Smart operators in industry 4.0: A human-centered approach to enhance
operators’ capabilities and competencies within the new smart factory
context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computers &amp; industrial engineering</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 113:144–159, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Shiyong Wang, Jiafu Wan, Daqiang Zhang, Di Li, and Chunhua Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Towards smart factory for industry 4.0: a self-organized multi-agent
system with big data based feedback and coordination.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer networks</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 101:158–168, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Emrah Alkaya, Merve Bogurcu, Ferda Ulutas, and Göksel Niyazi Demirer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Adaptation to climate change in industry: Improving resource
efficiency through sustainable production applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Water Environment Research</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 87(1):14–25, 2015.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Chelsea Collier, Kenneth R. Fleischmann, Tinna Lassiter, Sherri R. Greenberg,
Raul G. Longoria, and Sandeep Chinchali.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Co-designing socio-technical interventions with skilled trade
workers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Symposium on Technology
and Society (ISTAS23), Public Interest Technology University Network</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2312.01549" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2312.01550" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2312.01550">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2312.01550" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2312.01551" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 15:52:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
