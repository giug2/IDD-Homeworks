<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.08371] Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service</title><meta property="og:description" content="Federated learning (FL) is a distributed machine learning approach that allows multiple clients to learn a shared model collaboratively without sharing their raw data.
State-of-the-art FL systems provide an all-in-one …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.08371">

<!--Generated on Thu Feb 29 20:07:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Muhammad Jahanzeb Khan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">University of Nevada</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Reno</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jahanzeb@nevada.unr.edu">jahanzeb@nevada.unr.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rui Hu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">University of Nevada</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Reno</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ruihu@unr.edu">ruihu@unr.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Sadoghi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">University of California</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Davis</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:msadoghi@ucdavis.edu">msadoghi@ucdavis.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dongfang Zhao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">University of Nevada</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_city">Reno</span><span id="id12.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dzhao@unr.edu">dzhao@unr.edu</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id13.id1" class="ltx_p">Federated learning (FL) is a distributed machine learning approach that allows multiple clients to learn a shared model collaboratively without sharing their raw data.
State-of-the-art FL systems provide an all-in-one solution;
for example, users must install and use whichever data management subsystem that is expected by the FL system.
This tightly-coupling paradigm is at best a hindrance to the wide adoption of FL solutions;
in some domains, such as scientific applications,
it could be a deal-breaker by enforcing a specific type of data solution on special hardware and platforms (e.g., high-performance computing clusters without node-local persistent storage).
To this end, one natural solution is to decouple the data management functionalities from the FL system, enabling clients to customize their FL applications with specific data subsystems on which efficient queries can be made.
The technical challenges of such decoupling methods include new system architectures,
expressive and structured data models for FL parameters,
and security guarantees among FL participants.
As a starting point for this line of research,
this paper conducts a thorough evaluation and comparison of mainstream database solutions as decoupled services in the context of FL systems.
To make a fair comparison, we develop a framework called Data-Decoupling Federated Learning (DDFL),
which can run FL workloads and is agnostic of the underlying database system by exposing a unified interface.
To evaluate the effectiveness and feasibility of our approach, we compare DDFL with state-of-the-art FL systems that tightly couple data management and computational counterparts.
We carry out extensive experiments on various datasets and data management subsystems (e.g., PostgreSQL, MongoDB, Cassandra, Neo4j) and show that DDFL achieves comparable or better performance in terms of training time, inference accuracy, and database query time, while providing more options for clients to tune their FL applications regarding data-related metrics, such as performance, resilience, and usability.
Additionally, we provide a detailed qualitative analysis of DDFL when being integrated with mainstream database systems.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Artifact Availability:
<br class="ltx_break"></span><span id="p1.1.2" class="ltx_text" style="font-size:90%;">The source code, data, and/or other artifacts have been made available at </span><a href="%leave%20empty%20if%20no%20availability%20url%20should%20be%20sethttps://github.com/jahanxb/flcode" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">%leave␣empty␣if␣no␣availability␣url␣should␣be␣sethttps://github.com/jahanxb/flcode</a><span id="p1.1.3" class="ltx_text" style="font-size:90%;">.
</span><span id="p1.1.4" class="ltx_text"></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>Background</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Federated learning (FL) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib43" title="" class="ltx_ref">agnostic-mohri, </a>; <a href="#bib.bib44" title="" class="ltx_ref">fl-non-idd-data, </a>; <a href="#bib.bib45" title="" class="ltx_ref">convergence-fedavg, </a>; <a href="#bib.bib63" title="" class="ltx_ref">practical-fl, </a>; <a href="#bib.bib64" title="" class="ltx_ref">flcomp2022, </a>; <a href="#bib.bib68" title="" class="ltx_ref">ensemble2020, </a>; <a href="#bib.bib69" title="" class="ltx_ref">AlShedivat2020FederatedLV, </a>)</cite> is a distributed machine learning methodology that enables multiple or many clients to train a shared model collaboratively, without the need to transfer their raw data to a centralized server. By maintaining the data locally, FL enhances the data privacy of individual clients and reduces communication costs, while training an accurate model that reflects the collective knowledge of all participants. As such, FL has received considerable attention in recent years, and its potential has been demonstrated in a variety of applications, such as health monitoring and autonomous driving <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">kairouz2019advances, </a>)</cite>. FL’s focus on privacy and efficiency makes it a promising technique for future large-scale machine-learning applications, particularly those in which data security is paramount.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">One of the significant advantages of FL is that it eliminates the need for a centralized server to collect and manage data from multiple or many clients, which can be time-consuming, costly, and vulnerable to data breaches. Instead, FL allows each client to keep their data locally and contribute to the training of a shared model through a secure communication protocol. This approach not only maintains the privacy of the individual clients’ data but also leverages the diversity of their data to train a more robust and accurate model. Moreover, FL can enable the training of machine learning models on data that is distributed across many clients, making it possible to learn from a larger and more diverse dataset, which can lead to improved model generalization and avoid the overfitting problem <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">Yang2018, </a>)</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Furthermore, FL has recently been applied to various real-world applications, including healthcare, transportation, and finance, where data privacy is a critical concern. For instance, FL has been used to develop predictive models for heart disease and cancer detection using medical data from different hospitals without transferring the data between hospitals <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib28" title="" class="ltx_ref">pouyanfar2019federated, </a>)</cite>. In the transportation sector, FL has been used to develop traffic prediction models using data from multiple sources, including GPS-enabled vehicles, traffic cameras, and traffic sensors, while preserving the privacy of individual drivers and maintaining the security of the data <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib27" title="" class="ltx_ref">fl2019corr, </a>)</cite>. In the financial sector, FL has been used to detect fraudulent transactions across multiple banks, enabling the sharing of information while maintaining client privacy <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib26" title="" class="ltx_ref">mcmahan2018learning, </a>)</cite>.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2. </span>Motivation</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Although federated learning (FL) systems <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib6" title="" class="ltx_ref">fedml, </a>; <a href="#bib.bib7" title="" class="ltx_ref">tff, </a>; <a href="#bib.bib5" title="" class="ltx_ref">pysyft, </a>; <a href="#bib.bib2" title="" class="ltx_ref">mobile2018, </a>)</cite> hold great promise in scalable privacy-preserving machine learning applications, they face some challenges that must be addressed before their wide deployment.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">One of the challenges is related to the data management aspect of the FL systems. Specifically, clients lack an efficient way to manage and query the intermediate models during the training process or verification procedure of their FL applications.
In a more general sense,
the data management functionalities and computational counterparts are tightly coupled, making it difficult to customize or generalize the entire FL system.
Even worse, some applications or domains simply cannot meet the hardware/platform requirements of modern FL systems;
as a case in point,
our prior work <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib4" title="" class="ltx_ref">lwangsc22, </a>)</cite> shows that existing FL systems are not appropriate for large-scale high-performance computing (HPC) platforms that lack local persistent storage.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3. </span>Proposed Solution</h3>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.08371/assets/flarch9.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Proposed Design of Data-Decoupling FL (DDFL) framework.</figcaption>
</figure>
<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">To address these challenges, we propose a framework for decoupling the data management functionalities from FL systems. Our approach features a loosely-coupled FL architecture where the global model and local models are managed by a dedicated database. This decoupling allows clients to customize their FL applications using specific data subsystems and enables the FL system to scale to a large number of clients. With this approach, we aim to enhance the flexibility, performance, and scalability of FL systems.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">We implement our approach by building a prototype FL system that utilizes a few options of mainstream databases, including a document database MongoDB <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib16" title="" class="ltx_ref">mongodb, </a>)</cite>, a columnar database Cassandra <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">cassandra, </a>)</cite>, a graph database (Neo4j) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib24" title="" class="ltx_ref">neo4j, </a>)</cite>, and a relational database PostgreSQL <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">postgres, </a>)</cite> as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1.3. Proposed Solution ‣ 1. Introduction ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The prototype system is deployed on an 11-node cluster on CloudLab <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">cloudlab, </a>)</cite>.
Experimental results demonstrate that the use of data decoupling provides clients with more options and opportunities to optimize their FL applications in terms of performance, resilience, and usability.</p>
</div>
</section>
<section id="S1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4. </span>Contributions</h3>

<div id="S1.SS4.p1" class="ltx_para">
<p id="S1.SS4.p1.1" class="ltx_p">In summary, this Experiment, Analysis &amp; Benchmark (EAB) paper makes the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a loosely-coupled FL architecture that separates the data management functionalities from the FL system. This approach allows clients to customize their FL applications using specific data subsystems, enhancing the interoperability and scalability of the system.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We implement our proposed approach by building a prototype FL system that utilizes several types of mainstream database systems,
such as PostgreSQL, Cassandra, MongoDB, and Neo4j.
To the best of our knowledge, this is the very first work conducting a thorough evaluation of comparing different database services in the context of federated learning systems.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We carry out an extensive evaluation of the performance of the prototype system by conducting experiments on an 11-node cluster on CloudLab <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">cloudlab, </a>)</cite>. Our experimental results demonstrate that the use of data decoupling provides clients with more options to optimize their FL applications in terms of performance, resilience, and usability.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Data-Decoupling Federated Learning (DDFL) Framework</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our proposed framework, namely Data-Decoupling Federated Learning (DDFL),
is designed to maintain privacy and security while training a global model. It features a decoupled data management approach where the client nodes store their datasets locally and work in collaboration with a master node to learn a shared global model in an iterative manner. The intermediate and final results are managed by a dedicated data management service, which is responsible for decoupling the data management from the FL system. The overall architecture of the proposed framework is illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2. Data-Decoupling Federated Learning (DDFL) Framework ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.2" class="ltx_p">In our considered FL system, we have <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">N</annotation></semantics></math> client nodes and a master node. Each client node has its own datasets stored locally and aims to learn a shared global model <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">G</annotation></semantics></math> in an iterative manner while being coordinated by the master node. During each round of federated training, the client nodes use their local training data to update the global model obtained from the master node. The updated local model parameters are then shared with the master node, where all the received local models are aggregated to update the global model for the next round of training.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In the DDFL system, the intermediate models generated during the training process are managed by a dedicated data management service to achieve data decoupling. The global model is downloaded and used by the local models, and then stored for future use.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2303.08371/assets/ddfl-sys-1.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Architecture of the Data-Decoupling FL (DDFL) framework</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Symmetric Encryption</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In our system design, we use symmetric encryption <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib39" title="" class="ltx_ref">applied-cryptography-handbook, </a>)</cite> to ensure the confidentiality of sensitive information during data transfer between nodes. Symmetric encryption involves using the same secret key for both encrypting and decrypting data. This method is commonly used in situations where the communicating parties share a common secret key and need to keep their communication private.
We have chosen to use Fernet encryption <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib40" title="" class="ltx_ref">cryptography-fernet, </a>)</cite>, a specific implementation of symmetric encryption using the AES encryption algorithm in CBC mode and the PKCS7 padding scheme <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib46" title="" class="ltx_ref">fips197, </a>; <a href="#bib.bib47" title="" class="ltx_ref">rfc3602, </a>; <a href="#bib.bib48" title="" class="ltx_ref">rfc2315, </a>)</cite>. Fernet encryption has several advantages over other encryption methods, including its simplicity and speed, making it a suitable choice for our system design.
In our system, the Fernet encryption key is generated and shared between the communicating parties before data transfer begins. The encryption key is used to encrypt the data and send it to the destination node, which then uses the same key to decrypt the data. This process ensures that the data is protected during transmission and can only be accessed by the intended recipient.
The Fernet encryption scheme is also used in our system to secure the messages sent between the master node and the worker nodes. The messages are encrypted with a unique Fernet encryption key, ensuring the confidentiality of the messages during transmission. This ensures that sensitive information, such as the trained machine learning model, remains secure and confidential during the distributed training process.
In summary, we use symmetric encryption in our system design to ensure the confidentiality of sensitive information during data transfer between nodes. We have chosen to use the Fernet encryption scheme due to its simplicity and speed, and it is used to secure the messages sent between the master node and worker nodes, as well as the data being transferred.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Data Preprocessing</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In our federated learning system, data preprocessing is performed by the clients on their local devices. This approach aims to minimize the amount of sensitive data that is transmitted over the network, enhancing the privacy and security of the federated learning process. The preprocessing steps may include feature selection, extraction, normalization, and data augmentation. These steps are implemented using Python libraries such as NumPy <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib15" title="" class="ltx_ref">numpy, </a>)</cite> and PyTorch <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">pytorch, </a>)</cite>. The preprocessing functions are optimized to ensure efficient and scalable data processing.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Feature selection and extraction techniques are used to identify the most relevant features in the input data, which helps to improve the accuracy of the model while reducing the computational overhead. Normalization techniques are used to scale the input features to a common range, reducing the impact of different feature scales on model training. Data augmentation techniques are used to generate additional training data by applying various data transformations, such as rotation, flipping, and cropping. This helps to improve the robustness and generalizability of the model.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Federated Learning Algorithm</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In our system, federated learning is implemented using a decentralized approach where multiple nodes collaborate to train a global model. The global model is broadcasted to all participating nodes at the beginning of each round, and each node trains a local model using its private data. The updated local models are then sent to the global node, which aggregates them using various data decoupling techniques such as averaging or weighted averaging. The global model is then updated, and this process is repeated for several rounds until the global model converges.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Data Decoupling Techniques</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">In our federated learning system, we adopt data decoupling techniques to enable distributed learning across multiple clients. Specifically, we store the clients’ trained model data in a database to facilitate the model aggregation process, while reducing the communication overhead and improving the scalability of the system.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">To store the trained model data, we employ a data storage technique that indexes each trained model by client Node, iteration, and round. This technique enables efficient model retrieval by the global node and individual clients, allowing each client to perform local model training on their own data. By using data decoupling techniques and a data storage technique, our approach enables distributed learning while maintaining the privacy and security of the client’s data.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5. </span>Communication Protocol</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">In our federated learning system, we implement a custom communication protocol to facilitate communication between the clients and the server. Once the connection is established, the system sends requests to a database middleware and checks the status of the process. This approach provides a streamlined and efficient communication process for the federated learning system.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">To ensure the privacy and security of the client’s data, various security and privacy measures are implemented. Symmetric encryption is used to protect the client’s data during transmission. Additionally, database middleware is used to store the client’s data and facilitate communication between the clients and the server.</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6. </span>Database Middleware</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">The proposed framework for federated learning with decoupled data management involves utilizing a dedicated data management service to manage all the intermediate models during the training process, thereby separating data management from the FL system. Once the global model is downloaded and used by local models in DDFL, it is stored for further use. To establish the data management services for DDFL, six different options for database systems are considered in this paper.</p>
</div>
<section id="S2.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.1. </span><span id="S2.SS6.SSS1.1.1" class="ltx_text ltx_font_bold">RabbitMQ Queues</span>
</h4>

<div id="S2.SS6.SSS1.p1" class="ltx_para">
<p id="S2.SS6.SSS1.p1.1" class="ltx_p">RabbitMQ <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">rabbitmq, </a>)</cite>, which is an open-source message broker software that implements the Advanced Message Queuing Protocol (AMQP). RabbitMQ queues are utilized to store messages in buffers until they are ready to be processed. The use of RabbitMQ queues in DDFL allows for the decoupling of data layers and facilitated communication between nodes.</p>
</div>
</section>
<section id="S2.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.2. </span><span id="S2.SS6.SSS2.1.1" class="ltx_text ltx_font_bold">MongoDB Collection</span>
</h4>

<div id="S2.SS6.SSS2.p1" class="ltx_para">
<p id="S2.SS6.SSS2.p1.1" class="ltx_p">MongoDB <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib16" title="" class="ltx_ref">mongodb, </a>)</cite>, which is a widely used open-source NoSQL database that stores data in a flexible format known as BSON (Binary JSON). In DDFL, a MongoDB collection is utilized as a centralized database for storing and sharing data. Each client creates a collection that will be shared with the master node, and the master node creates a global model collection for each node.</p>
</div>
</section>
<section id="S2.SS6.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.3. </span><span id="S2.SS6.SSS3.1.1" class="ltx_text ltx_font_bold">Secure File Transfer (SCP)</span>
</h4>

<div id="S2.SS6.SSS3.p1" class="ltx_para">
<p id="S2.SS6.SSS3.p1.1" class="ltx_p">Secure Copy Protocol (SCP) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib22" title="" class="ltx_ref">scp, </a>)</cite>, which is a network protocol for securely transferring files between computers. SCP is utilized to transfer the trained model from the master node to the client nodes.</p>
</div>
</section>
<section id="S2.SS6.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.4. </span><span id="S2.SS6.SSS4.1.1" class="ltx_text ltx_font_bold">Cassandra</span>
</h4>

<div id="S2.SS6.SSS4.p1" class="ltx_para">
<p id="S2.SS6.SSS4.p1.1" class="ltx_p">Apache Cassandra <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">cassandra, </a>)</cite>, which is a distributed NoSQL database designed to handle large amounts of data across many commodity servers. In DDFL, Apache Cassandra is used as a decentralized database to store and manage all the models in FL.</p>
</div>
</section>
<section id="S2.SS6.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.5. </span><span id="S2.SS6.SSS5.1.1" class="ltx_text ltx_font_bold">Neo4j</span>
</h4>

<div id="S2.SS6.SSS5.p1" class="ltx_para">
<p id="S2.SS6.SSS5.p1.1" class="ltx_p">Neo4j <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib24" title="" class="ltx_ref">neo4j, </a>)</cite>, which is a graph database that stores data in nodes and edges. In DDFL, Neo4j is utilized to store the intermediate models and for graph-based querying.</p>
</div>
</section>
<section id="S2.SS6.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.6. </span><span id="S2.SS6.SSS6.1.1" class="ltx_text ltx_font_bold">Postgres RDBMS</span>
</h4>

<div id="S2.SS6.SSS6.p1" class="ltx_para">
<p id="S2.SS6.SSS6.p1.1" class="ltx_p">PostgreSQL <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">postgres, </a>)</cite>, which is a popular open-source relational database management system (RDBMS) known for its strong consistency and reliability. In DDFL, PostgreSQL is utilized to store the intermediate models and provides support for complex queries.</p>
</div>
<div id="S2.SS6.SSS6.p2" class="ltx_para">
<p id="S2.SS6.SSS6.p2.1" class="ltx_p">In the SCP <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib22" title="" class="ltx_ref">scp, </a>)</cite> and RabbitMQ <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">rabbitmq, </a>)</cite> systems, the global model is stored on disk. In contrast, the MongoDB and Cassandra systems utilize a model variable to store the global model, which is kept in memory for efficient retrieval. This approach can reduce the latency in accessing the model and can also provide faster model updates during the federated learning process. However, it requires more memory resources compared to the disk storage approach used by SCP and RabbitMQ. The choice of storage approach depends on the specific requirements of the FL system, such as the size of the model and the available memory and disk resources.</p>
</div>
<div id="S2.SS6.SSS6.p3" class="ltx_para">
<p id="S2.SS6.SSS6.p3.1" class="ltx_p">In the proposed data-decoupling Federated Learning (DDFL) framework, the data management system is designed to operate in three distinct phases during each training round of Federated Learning (FL): the local model computation phase, the model aggregation phase, and the model evaluation phase. In the local model computation phase, each client node uses its own training data to update the global model downloaded from the master node, generating a local model that is sent back to the master node for aggregation. During the model aggregation phase, the master node collects all the local models from the client nodes and aggregates them to update the global model. Finally, in the model evaluation phase, the updated global model is evaluated by the master node to determine the progress of the training process. Throughout these phases, the dedicated data management service plays a crucial role in managing the intermediate and final results, decoupling the data management from the FL system to improve the efficiency and privacy of the training process.</p>
</div>
</section>
</section>
<section id="S2.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.7. </span>Local Model Computation</h3>

<div id="S2.SS7.p1" class="ltx_para">
<p id="S2.SS7.p1.2" class="ltx_p">The decoupled data management solution <math id="S2.SS7.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS7.p1.1.m1.1a"><mi id="S2.SS7.p1.1.m1.1.1" xref="S2.SS7.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS7.p1.1.m1.1b"><ci id="S2.SS7.p1.1.m1.1.1.cmml" xref="S2.SS7.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS7.p1.1.m1.1c">S</annotation></semantics></math> can be chosen from MongoDB, RabbitMQ, SCP, Neo4j, Postgres, and Apache Cassandra. At every training round, each client <math id="S2.SS7.p1.2.m2.1" class="ltx_Math" alttext="i\in[N]" display="inline"><semantics id="S2.SS7.p1.2.m2.1a"><mrow id="S2.SS7.p1.2.m2.1.2" xref="S2.SS7.p1.2.m2.1.2.cmml"><mi id="S2.SS7.p1.2.m2.1.2.2" xref="S2.SS7.p1.2.m2.1.2.2.cmml">i</mi><mo id="S2.SS7.p1.2.m2.1.2.1" xref="S2.SS7.p1.2.m2.1.2.1.cmml">∈</mo><mrow id="S2.SS7.p1.2.m2.1.2.3.2" xref="S2.SS7.p1.2.m2.1.2.3.1.cmml"><mo stretchy="false" id="S2.SS7.p1.2.m2.1.2.3.2.1" xref="S2.SS7.p1.2.m2.1.2.3.1.1.cmml">[</mo><mi id="S2.SS7.p1.2.m2.1.1" xref="S2.SS7.p1.2.m2.1.1.cmml">N</mi><mo stretchy="false" id="S2.SS7.p1.2.m2.1.2.3.2.2" xref="S2.SS7.p1.2.m2.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS7.p1.2.m2.1b"><apply id="S2.SS7.p1.2.m2.1.2.cmml" xref="S2.SS7.p1.2.m2.1.2"><in id="S2.SS7.p1.2.m2.1.2.1.cmml" xref="S2.SS7.p1.2.m2.1.2.1"></in><ci id="S2.SS7.p1.2.m2.1.2.2.cmml" xref="S2.SS7.p1.2.m2.1.2.2">𝑖</ci><apply id="S2.SS7.p1.2.m2.1.2.3.1.cmml" xref="S2.SS7.p1.2.m2.1.2.3.2"><csymbol cd="latexml" id="S2.SS7.p1.2.m2.1.2.3.1.1.cmml" xref="S2.SS7.p1.2.m2.1.2.3.2.1">delimited-[]</csymbol><ci id="S2.SS7.p1.2.m2.1.1.cmml" xref="S2.SS7.p1.2.m2.1.1">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS7.p1.2.m2.1c">i\in[N]</annotation></semantics></math> performs the following steps to compute the local model:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.2" class="ltx_p">Initializes local model <math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="M_{i}" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><msub id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.2" xref="S2.I1.i1.p1.1.m1.1.1.2.cmml">M</mi><mi id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2">𝑀</ci><ci id="S2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">M_{i}</annotation></semantics></math> as the global model <math id="S2.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.I1.i1.p1.2.m2.1a"><mi id="S2.I1.i1.p1.2.m2.1.1" xref="S2.I1.i1.p1.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.1b"><ci id="S2.I1.i1.p1.2.m2.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.1c">G</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Loads local data <math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><msub id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">D</mi><mi id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">𝐷</ci><ci id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">D_{i}</annotation></semantics></math> into its local storage;</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.2" class="ltx_p">Trains local model <math id="S2.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="M_{i}" display="inline"><semantics id="S2.I1.i3.p1.1.m1.1a"><msub id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">M</mi><mi id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">𝑀</ci><ci id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">M_{i}</annotation></semantics></math> using local data <math id="S2.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.I1.i3.p1.2.m2.1a"><msub id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml"><mi id="S2.I1.i3.p1.2.m2.1.1.2" xref="S2.I1.i3.p1.2.m2.1.1.2.cmml">D</mi><mi id="S2.I1.i3.p1.2.m2.1.1.3" xref="S2.I1.i3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><apply id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.2.m2.1.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.2.m2.1.1.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2">𝐷</ci><ci id="S2.I1.i3.p1.2.m2.1.1.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">D_{i}</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.2" class="ltx_p">Encrypts local model <math id="S2.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="M_{i}" display="inline"><semantics id="S2.I1.i4.p1.1.m1.1a"><msub id="S2.I1.i4.p1.1.m1.1.1" xref="S2.I1.i4.p1.1.m1.1.1.cmml"><mi id="S2.I1.i4.p1.1.m1.1.1.2" xref="S2.I1.i4.p1.1.m1.1.1.2.cmml">M</mi><mi id="S2.I1.i4.p1.1.m1.1.1.3" xref="S2.I1.i4.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i4.p1.1.m1.1b"><apply id="S2.I1.i4.p1.1.m1.1.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i4.p1.1.m1.1.1.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i4.p1.1.m1.1.1.2.cmml" xref="S2.I1.i4.p1.1.m1.1.1.2">𝑀</ci><ci id="S2.I1.i4.p1.1.m1.1.1.3.cmml" xref="S2.I1.i4.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i4.p1.1.m1.1c">M_{i}</annotation></semantics></math> using a secret key <math id="S2.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S2.I1.i4.p1.2.m2.1a"><msub id="S2.I1.i4.p1.2.m2.1.1" xref="S2.I1.i4.p1.2.m2.1.1.cmml"><mi id="S2.I1.i4.p1.2.m2.1.1.2" xref="S2.I1.i4.p1.2.m2.1.1.2.cmml">k</mi><mi id="S2.I1.i4.p1.2.m2.1.1.3" xref="S2.I1.i4.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i4.p1.2.m2.1b"><apply id="S2.I1.i4.p1.2.m2.1.1.cmml" xref="S2.I1.i4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i4.p1.2.m2.1.1.1.cmml" xref="S2.I1.i4.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i4.p1.2.m2.1.1.2.cmml" xref="S2.I1.i4.p1.2.m2.1.1.2">𝑘</ci><ci id="S2.I1.i4.p1.2.m2.1.1.3.cmml" xref="S2.I1.i4.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i4.p1.2.m2.1c">k_{i}</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.2" class="ltx_p">Stores encrypted local model <math id="S2.I1.i5.p1.1.m1.1" class="ltx_Math" alttext="E_{M_{i}}" display="inline"><semantics id="S2.I1.i5.p1.1.m1.1a"><msub id="S2.I1.i5.p1.1.m1.1.1" xref="S2.I1.i5.p1.1.m1.1.1.cmml"><mi id="S2.I1.i5.p1.1.m1.1.1.2" xref="S2.I1.i5.p1.1.m1.1.1.2.cmml">E</mi><msub id="S2.I1.i5.p1.1.m1.1.1.3" xref="S2.I1.i5.p1.1.m1.1.1.3.cmml"><mi id="S2.I1.i5.p1.1.m1.1.1.3.2" xref="S2.I1.i5.p1.1.m1.1.1.3.2.cmml">M</mi><mi id="S2.I1.i5.p1.1.m1.1.1.3.3" xref="S2.I1.i5.p1.1.m1.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i5.p1.1.m1.1b"><apply id="S2.I1.i5.p1.1.m1.1.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i5.p1.1.m1.1.1.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i5.p1.1.m1.1.1.2.cmml" xref="S2.I1.i5.p1.1.m1.1.1.2">𝐸</ci><apply id="S2.I1.i5.p1.1.m1.1.1.3.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.I1.i5.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3">subscript</csymbol><ci id="S2.I1.i5.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.I1.i5.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i5.p1.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i5.p1.1.m1.1c">E_{M_{i}}</annotation></semantics></math> in <math id="S2.I1.i5.p1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.I1.i5.p1.2.m2.1a"><mi id="S2.I1.i5.p1.2.m2.1.1" xref="S2.I1.i5.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i5.p1.2.m2.1b"><ci id="S2.I1.i5.p1.2.m2.1.1.cmml" xref="S2.I1.i5.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i5.p1.2.m2.1c">S</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p id="S2.SS7.p1.3" class="ltx_p">Note that the global model <math id="S2.SS7.p1.3.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.SS7.p1.3.m1.1a"><mi id="S2.SS7.p1.3.m1.1.1" xref="S2.SS7.p1.3.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.SS7.p1.3.m1.1b"><ci id="S2.SS7.p1.3.m1.1.1.cmml" xref="S2.SS7.p1.3.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS7.p1.3.m1.1c">G</annotation></semantics></math> is initialized on the master node before the training.</p>
</div>
</section>
<section id="S2.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.8. </span>Model Aggregation</h3>

<div id="S2.SS8.p1" class="ltx_para">
<p id="S2.SS8.p1.1" class="ltx_p">Following the local model computation, it is essential to aggregate all the local models to update the global model on the server for the subsequent round of training. To achieve this, the following steps will be performed on each client <math id="S2.SS8.p1.1.m1.1" class="ltx_Math" alttext="i\in[N]" display="inline"><semantics id="S2.SS8.p1.1.m1.1a"><mrow id="S2.SS8.p1.1.m1.1.2" xref="S2.SS8.p1.1.m1.1.2.cmml"><mi id="S2.SS8.p1.1.m1.1.2.2" xref="S2.SS8.p1.1.m1.1.2.2.cmml">i</mi><mo id="S2.SS8.p1.1.m1.1.2.1" xref="S2.SS8.p1.1.m1.1.2.1.cmml">∈</mo><mrow id="S2.SS8.p1.1.m1.1.2.3.2" xref="S2.SS8.p1.1.m1.1.2.3.1.cmml"><mo stretchy="false" id="S2.SS8.p1.1.m1.1.2.3.2.1" xref="S2.SS8.p1.1.m1.1.2.3.1.1.cmml">[</mo><mi id="S2.SS8.p1.1.m1.1.1" xref="S2.SS8.p1.1.m1.1.1.cmml">N</mi><mo stretchy="false" id="S2.SS8.p1.1.m1.1.2.3.2.2" xref="S2.SS8.p1.1.m1.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS8.p1.1.m1.1b"><apply id="S2.SS8.p1.1.m1.1.2.cmml" xref="S2.SS8.p1.1.m1.1.2"><in id="S2.SS8.p1.1.m1.1.2.1.cmml" xref="S2.SS8.p1.1.m1.1.2.1"></in><ci id="S2.SS8.p1.1.m1.1.2.2.cmml" xref="S2.SS8.p1.1.m1.1.2.2">𝑖</ci><apply id="S2.SS8.p1.1.m1.1.2.3.1.cmml" xref="S2.SS8.p1.1.m1.1.2.3.2"><csymbol cd="latexml" id="S2.SS8.p1.1.m1.1.2.3.1.1.cmml" xref="S2.SS8.p1.1.m1.1.2.3.2.1">delimited-[]</csymbol><ci id="S2.SS8.p1.1.m1.1.1.cmml" xref="S2.SS8.p1.1.m1.1.1">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS8.p1.1.m1.1c">i\in[N]</annotation></semantics></math> initially:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">On each client node:</span></p>
<ol id="S2.I2.i1.I1" class="ltx_enumerate">
<li id="S2.I2.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I2.i1.I1.i1.p1" class="ltx_para">
<p id="S2.I2.i1.I1.i1.p1.2" class="ltx_p">Retrieves encrypted local model updates <math id="S2.I2.i1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="E_{M_{i}}" display="inline"><semantics id="S2.I2.i1.I1.i1.p1.1.m1.1a"><msub id="S2.I2.i1.I1.i1.p1.1.m1.1.1" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.cmml"><mi id="S2.I2.i1.I1.i1.p1.1.m1.1.1.2" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.2.cmml">E</mi><msub id="S2.I2.i1.I1.i1.p1.1.m1.1.1.3" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.cmml"><mi id="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.2" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.2.cmml">M</mi><mi id="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.3" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i1.I1.i1.p1.1.m1.1b"><apply id="S2.I2.i1.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i1.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.2">𝐸</ci><apply id="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="S2.I2.i1.I1.i1.p1.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.I1.i1.p1.1.m1.1c">E_{M_{i}}</annotation></semantics></math> from <math id="S2.I2.i1.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.I2.i1.I1.i1.p1.2.m2.1a"><mi id="S2.I2.i1.I1.i1.p1.2.m2.1.1" xref="S2.I2.i1.I1.i1.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.I1.i1.p1.2.m2.1b"><ci id="S2.I2.i1.I1.i1.p1.2.m2.1.1.cmml" xref="S2.I2.i1.I1.i1.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.I1.i1.p1.2.m2.1c">S</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I2.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I2.i1.I1.i2.p1" class="ltx_para">
<p id="S2.I2.i1.I1.i2.p1.3" class="ltx_p">Decrypts local model update <math id="S2.I2.i1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="E_{M_{i}}" display="inline"><semantics id="S2.I2.i1.I1.i2.p1.1.m1.1a"><msub id="S2.I2.i1.I1.i2.p1.1.m1.1.1" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I2.i1.I1.i2.p1.1.m1.1.1.2" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.2.cmml">E</mi><msub id="S2.I2.i1.I1.i2.p1.1.m1.1.1.3" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.cmml"><mi id="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.2" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.2.cmml">M</mi><mi id="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.3" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i1.I1.i2.p1.1.m1.1b"><apply id="S2.I2.i1.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i1.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.2">𝐸</ci><apply id="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S2.I2.i1.I1.i2.p1.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.I1.i2.p1.1.m1.1c">E_{M_{i}}</annotation></semantics></math> using the secret key <math id="S2.I2.i1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S2.I2.i1.I1.i2.p1.2.m2.1a"><msub id="S2.I2.i1.I1.i2.p1.2.m2.1.1" xref="S2.I2.i1.I1.i2.p1.2.m2.1.1.cmml"><mi id="S2.I2.i1.I1.i2.p1.2.m2.1.1.2" xref="S2.I2.i1.I1.i2.p1.2.m2.1.1.2.cmml">k</mi><mi id="S2.I2.i1.I1.i2.p1.2.m2.1.1.3" xref="S2.I2.i1.I1.i2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i1.I1.i2.p1.2.m2.1b"><apply id="S2.I2.i1.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I2.i1.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I2.i1.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I2.i1.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I2.i1.I1.i2.p1.2.m2.1.1.2">𝑘</ci><ci id="S2.I2.i1.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I2.i1.I1.i2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.I1.i2.p1.2.m2.1c">k_{i}</annotation></semantics></math>: <math id="S2.I2.i1.I1.i2.p1.3.m3.2" class="ltx_Math" alttext="M_{i}=\text{Decrypt}(E_{M_{i}},k_{i})" display="inline"><semantics id="S2.I2.i1.I1.i2.p1.3.m3.2a"><mrow id="S2.I2.i1.I1.i2.p1.3.m3.2.2" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.cmml"><msub id="S2.I2.i1.I1.i2.p1.3.m3.2.2.4" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.cmml"><mi id="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.2" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.2.cmml">M</mi><mi id="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.3" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.3.cmml">i</mi></msub><mo id="S2.I2.i1.I1.i2.p1.3.m3.2.2.3" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.3.cmml">=</mo><mrow id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.cmml"><mtext id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.4" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.4a.cmml">Decrypt</mtext><mo lspace="0em" rspace="0em" id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.3" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.3.cmml">​</mo><mrow id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.3" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.3.cmml">(</mo><msub id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.2" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.2.cmml">E</mi><msub id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.cmml"><mi id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.2" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.2.cmml">M</mi><mi id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.3" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.4" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.3.cmml">,</mo><msub id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.cmml"><mi id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.2" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.2.cmml">k</mi><mi id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.3" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.5" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i1.I1.i2.p1.3.m3.2b"><apply id="S2.I2.i1.I1.i2.p1.3.m3.2.2.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2"><eq id="S2.I2.i1.I1.i2.p1.3.m3.2.2.3.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.3"></eq><apply id="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.4"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.1.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.4">subscript</csymbol><ci id="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.2.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.2">𝑀</ci><ci id="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.3.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.4.3">𝑖</ci></apply><apply id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2"><times id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.3.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.3"></times><ci id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.4a.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.4"><mtext id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.4.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.4">Decrypt</mtext></ci><interval closure="open" id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.3.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2"><apply id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.2">𝐸</ci><apply id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.1.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.2.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.2">𝑀</ci><ci id="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.3.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.1.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2">subscript</csymbol><ci id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.2.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.2">𝑘</ci><ci id="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.3.cmml" xref="S2.I2.i1.I1.i2.p1.3.m3.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.I1.i2.p1.3.m3.2c">M_{i}=\text{Decrypt}(E_{M_{i}},k_{i})</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I2.i1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S2.I2.i1.I1.i3.p1" class="ltx_para">
<p id="S2.I2.i1.I1.i3.p1.2" class="ltx_p">Encrypts the local model update <math id="S2.I2.i1.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="M_{i}" display="inline"><semantics id="S2.I2.i1.I1.i3.p1.1.m1.1a"><msub id="S2.I2.i1.I1.i3.p1.1.m1.1.1" xref="S2.I2.i1.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I2.i1.I1.i3.p1.1.m1.1.1.2" xref="S2.I2.i1.I1.i3.p1.1.m1.1.1.2.cmml">M</mi><mi id="S2.I2.i1.I1.i3.p1.1.m1.1.1.3" xref="S2.I2.i1.I1.i3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i1.I1.i3.p1.1.m1.1b"><apply id="S2.I2.i1.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I2.i1.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I2.i1.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i1.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I2.i1.I1.i3.p1.1.m1.1.1.2">𝑀</ci><ci id="S2.I2.i1.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I2.i1.I1.i3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.I1.i3.p1.1.m1.1c">M_{i}</annotation></semantics></math> and transmits it to <math id="S2.I2.i1.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.I2.i1.I1.i3.p1.2.m2.1a"><mi id="S2.I2.i1.I1.i3.p1.2.m2.1.1" xref="S2.I2.i1.I1.i3.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.I1.i3.p1.2.m2.1b"><ci id="S2.I2.i1.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I2.i1.I1.i3.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.I1.i3.p1.2.m2.1c">S</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">On the master node:</span></p>
<ol id="S2.I2.i2.I1" class="ltx_enumerate">
<li id="S2.I2.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I2.i2.I1.i1.p1" class="ltx_para">
<p id="S2.I2.i2.I1.i1.p1.1" class="ltx_p">Loads all model updates <math id="S2.I2.i2.I1.i1.p1.1.m1.4" class="ltx_Math" alttext="M_{1},M_{2},\dots,M_{N}" display="inline"><semantics id="S2.I2.i2.I1.i1.p1.1.m1.4a"><mrow id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.4.cmml"><msub id="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1" xref="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.cmml"><mi id="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.2" xref="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.2.cmml">M</mi><mn id="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.3" xref="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.4" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.4.cmml">,</mo><msub id="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2" xref="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.cmml"><mi id="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.2" xref="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.2.cmml">M</mi><mn id="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.3" xref="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.5" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.I2.i2.I1.i1.p1.1.m1.1.1" xref="S2.I2.i2.I1.i1.p1.1.m1.1.1.cmml">…</mi><mo id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.6" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.4.cmml">,</mo><msub id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.cmml"><mi id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.2" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.2.cmml">M</mi><mi id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.3" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i2.I1.i1.p1.1.m1.4b"><list id="S2.I2.i2.I1.i1.p1.1.m1.4.4.4.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.3"><apply id="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.2.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.2">𝑀</ci><cn type="integer" id="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.3.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.2.2.1.1.3">1</cn></apply><apply id="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.1.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.2">𝑀</ci><cn type="integer" id="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.3.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.3.3.2.2.3">2</cn></apply><ci id="S2.I2.i2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.1.1">…</ci><apply id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.1.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3">subscript</csymbol><ci id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.2.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.2">𝑀</ci><ci id="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.3.cmml" xref="S2.I2.i2.I1.i1.p1.1.m1.4.4.3.3.3">𝑁</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.I1.i1.p1.1.m1.4c">M_{1},M_{2},\dots,M_{N}</annotation></semantics></math> from memory;</p>
</div>
</li>
<li id="S2.I2.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I2.i2.I1.i2.p1" class="ltx_para">
<p id="S2.I2.i2.I1.i2.p1.1" class="ltx_p">Aggregates model updates in the global database to generate a new global model, i.e., 
<br class="ltx_break"><math id="S2.I2.i2.I1.i2.p1.1.m1.4" class="ltx_Math" alttext="G=\text{Aggregate}(M_{1},M_{2},\dots,M_{N})" display="inline"><semantics id="S2.I2.i2.I1.i2.p1.1.m1.4a"><mrow id="S2.I2.i2.I1.i2.p1.1.m1.4.4" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.cmml"><mi id="S2.I2.i2.I1.i2.p1.1.m1.4.4.5" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.5.cmml">G</mi><mo id="S2.I2.i2.I1.i2.p1.1.m1.4.4.4" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.cmml"><mtext id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.5" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.5a.cmml">Aggregate</mtext><mo lspace="0em" rspace="0em" id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.4" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.4.cmml">​</mo><mrow id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.4.cmml"><mo stretchy="false" id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.4" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.4.cmml">(</mo><msub id="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1" xref="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.cmml"><mi id="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.2" xref="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.2.cmml">M</mi><mn id="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.3" xref="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.5" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.4.cmml">,</mo><msub id="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2" xref="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.cmml"><mi id="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.2" xref="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.2.cmml">M</mi><mn id="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.3" xref="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.6" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.I2.i2.I1.i2.p1.1.m1.1.1" xref="S2.I2.i2.I1.i2.p1.1.m1.1.1.cmml">…</mi><mo id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.7" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.4.cmml">,</mo><msub id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.cmml"><mi id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.2" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.2.cmml">M</mi><mi id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.3" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.8" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i2.I1.i2.p1.1.m1.4b"><apply id="S2.I2.i2.I1.i2.p1.1.m1.4.4.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4"><eq id="S2.I2.i2.I1.i2.p1.1.m1.4.4.4.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.4"></eq><ci id="S2.I2.i2.I1.i2.p1.1.m1.4.4.5.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.5">𝐺</ci><apply id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3"><times id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.4.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.4"></times><ci id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.5a.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.5"><mtext id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.5.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.5">Aggregate</mtext></ci><vector id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.4.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3"><apply id="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.1.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.2">𝑀</ci><cn type="integer" id="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.3.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.2.2.1.1.1.1.3">1</cn></apply><apply id="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.1.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2">subscript</csymbol><ci id="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.2.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.2">𝑀</ci><cn type="integer" id="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.3.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.3.3.2.2.2.2.3">2</cn></apply><ci id="S2.I2.i2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.1.1">…</ci><apply id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.1.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3">subscript</csymbol><ci id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.2.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.2">𝑀</ci><ci id="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.3.cmml" xref="S2.I2.i2.I1.i2.p1.1.m1.4.4.3.3.3.3.3">𝑁</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.I1.i2.p1.1.m1.4c">G=\text{Aggregate}(M_{1},M_{2},\dots,M_{N})</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I2.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S2.I2.i2.I1.i3.p1" class="ltx_para">
<p id="S2.I2.i2.I1.i3.p1.3" class="ltx_p">Encrypts the new global model <math id="S2.I2.i2.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.I2.i2.I1.i3.p1.1.m1.1a"><mi id="S2.I2.i2.I1.i3.p1.1.m1.1.1" xref="S2.I2.i2.I1.i3.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.I1.i3.p1.1.m1.1b"><ci id="S2.I2.i2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I2.i2.I1.i3.p1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.I1.i3.p1.1.m1.1c">G</annotation></semantics></math> using the secret key <math id="S2.I2.i2.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.I2.i2.I1.i3.p1.2.m2.1a"><mi id="S2.I2.i2.I1.i3.p1.2.m2.1.1" xref="S2.I2.i2.I1.i3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.I1.i3.p1.2.m2.1b"><ci id="S2.I2.i2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I2.i2.I1.i3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.I1.i3.p1.2.m2.1c">k</annotation></semantics></math>: <math id="S2.I2.i2.I1.i3.p1.3.m3.2" class="ltx_Math" alttext="E_{G}=\text{Encrypt}(G,k)" display="inline"><semantics id="S2.I2.i2.I1.i3.p1.3.m3.2a"><mrow id="S2.I2.i2.I1.i3.p1.3.m3.2.3" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.cmml"><msub id="S2.I2.i2.I1.i3.p1.3.m3.2.3.2" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.cmml"><mi id="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.2" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.2.cmml">E</mi><mi id="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.3" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.3.cmml">G</mi></msub><mo id="S2.I2.i2.I1.i3.p1.3.m3.2.3.1" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.1.cmml">=</mo><mrow id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.cmml"><mtext id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.2" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.2a.cmml">Encrypt</mtext><mo lspace="0em" rspace="0em" id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.1" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.1.cmml">​</mo><mrow id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.2" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.1.cmml"><mo stretchy="false" id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.2.1" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.1.cmml">(</mo><mi id="S2.I2.i2.I1.i3.p1.3.m3.1.1" xref="S2.I2.i2.I1.i3.p1.3.m3.1.1.cmml">G</mi><mo id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.2.2" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.1.cmml">,</mo><mi id="S2.I2.i2.I1.i3.p1.3.m3.2.2" xref="S2.I2.i2.I1.i3.p1.3.m3.2.2.cmml">k</mi><mo stretchy="false" id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.2.3" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i2.I1.i3.p1.3.m3.2b"><apply id="S2.I2.i2.I1.i3.p1.3.m3.2.3.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3"><eq id="S2.I2.i2.I1.i3.p1.3.m3.2.3.1.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.1"></eq><apply id="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.2"><csymbol cd="ambiguous" id="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.1.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.2">subscript</csymbol><ci id="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.2.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.2">𝐸</ci><ci id="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.3.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.2.3">𝐺</ci></apply><apply id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3"><times id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.1.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.1"></times><ci id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.2a.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.2"><mtext id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.2.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.2">Encrypt</mtext></ci><interval closure="open" id="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.1.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.3.3.3.2"><ci id="S2.I2.i2.I1.i3.p1.3.m3.1.1.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.1.1">𝐺</ci><ci id="S2.I2.i2.I1.i3.p1.3.m3.2.2.cmml" xref="S2.I2.i2.I1.i3.p1.3.m3.2.2">𝑘</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.I1.i3.p1.3.m3.2c">E_{G}=\text{Encrypt}(G,k)</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I2.i2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S2.I2.i2.I1.i4.p1" class="ltx_para">
<p id="S2.I2.i2.I1.i4.p1.2" class="ltx_p">Stores the encrypted global model <math id="S2.I2.i2.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="E_{G}" display="inline"><semantics id="S2.I2.i2.I1.i4.p1.1.m1.1a"><msub id="S2.I2.i2.I1.i4.p1.1.m1.1.1" xref="S2.I2.i2.I1.i4.p1.1.m1.1.1.cmml"><mi id="S2.I2.i2.I1.i4.p1.1.m1.1.1.2" xref="S2.I2.i2.I1.i4.p1.1.m1.1.1.2.cmml">E</mi><mi id="S2.I2.i2.I1.i4.p1.1.m1.1.1.3" xref="S2.I2.i2.I1.i4.p1.1.m1.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i2.I1.i4.p1.1.m1.1b"><apply id="S2.I2.i2.I1.i4.p1.1.m1.1.1.cmml" xref="S2.I2.i2.I1.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i2.I1.i4.p1.1.m1.1.1.1.cmml" xref="S2.I2.i2.I1.i4.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i2.I1.i4.p1.1.m1.1.1.2.cmml" xref="S2.I2.i2.I1.i4.p1.1.m1.1.1.2">𝐸</ci><ci id="S2.I2.i2.I1.i4.p1.1.m1.1.1.3.cmml" xref="S2.I2.i2.I1.i4.p1.1.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.I1.i4.p1.1.m1.1c">E_{G}</annotation></semantics></math> in <math id="S2.I2.i2.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.I2.i2.I1.i4.p1.2.m2.1a"><mi id="S2.I2.i2.I1.i4.p1.2.m2.1.1" xref="S2.I2.i2.I1.i4.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.I1.i4.p1.2.m2.1b"><ci id="S2.I2.i2.I1.i4.p1.2.m2.1.1.cmml" xref="S2.I2.i2.I1.i4.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.I1.i4.p1.2.m2.1c">S</annotation></semantics></math>; and</p>
</div>
</li>
<li id="S2.I2.i2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="S2.I2.i2.I1.i5.p1" class="ltx_para">
<p id="S2.I2.i2.I1.i5.p1.1" class="ltx_p">Shares the new global model <math id="S2.I2.i2.I1.i5.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.I2.i2.I1.i5.p1.1.m1.1a"><mi id="S2.I2.i2.I1.i5.p1.1.m1.1.1" xref="S2.I2.i2.I1.i5.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.I1.i5.p1.1.m1.1b"><ci id="S2.I2.i2.I1.i5.p1.1.m1.1.1.cmml" xref="S2.I2.i2.I1.i5.p1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.I1.i5.p1.1.m1.1c">G</annotation></semantics></math> with client nodes. Note that to protect the privacy of the client nodes, all communication between the client nodes and the master node is encrypted using symmetric encryption <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib12" title="" class="ltx_ref">Sahu2018, </a>)</cite>.</p>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.9. </span>Model Evaluation</h3>

<div id="S2.SS9.p1" class="ltx_para">
<p id="S2.SS9.p1.1" class="ltx_p">The performance of the trained model needs to be evaluated during the FL training process to ensure the effectiveness of the model. Both the client nodes and the master node can evaluate the model using the testing dataset. The following steps can be executed on the master node to evaluate the performance of the latest global model:</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(i)</span> 
<div id="S2.I3.ix1.p1" class="ltx_para">
<p id="S2.I3.ix1.p1.2" class="ltx_p">Retrieves the encrypted latest global model <math id="S2.I3.ix1.p1.1.m1.1" class="ltx_Math" alttext="E_{G}" display="inline"><semantics id="S2.I3.ix1.p1.1.m1.1a"><msub id="S2.I3.ix1.p1.1.m1.1.1" xref="S2.I3.ix1.p1.1.m1.1.1.cmml"><mi id="S2.I3.ix1.p1.1.m1.1.1.2" xref="S2.I3.ix1.p1.1.m1.1.1.2.cmml">E</mi><mi id="S2.I3.ix1.p1.1.m1.1.1.3" xref="S2.I3.ix1.p1.1.m1.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I3.ix1.p1.1.m1.1b"><apply id="S2.I3.ix1.p1.1.m1.1.1.cmml" xref="S2.I3.ix1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I3.ix1.p1.1.m1.1.1.1.cmml" xref="S2.I3.ix1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I3.ix1.p1.1.m1.1.1.2.cmml" xref="S2.I3.ix1.p1.1.m1.1.1.2">𝐸</ci><ci id="S2.I3.ix1.p1.1.m1.1.1.3.cmml" xref="S2.I3.ix1.p1.1.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix1.p1.1.m1.1c">E_{G}</annotation></semantics></math> from the data management service <math id="S2.I3.ix1.p1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.I3.ix1.p1.2.m2.1a"><mi id="S2.I3.ix1.p1.2.m2.1.1" xref="S2.I3.ix1.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.I3.ix1.p1.2.m2.1b"><ci id="S2.I3.ix1.p1.2.m2.1.1.cmml" xref="S2.I3.ix1.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix1.p1.2.m2.1c">S</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I3.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(ii)</span> 
<div id="S2.I3.ix2.p1" class="ltx_para">
<p id="S2.I3.ix2.p1.3" class="ltx_p">Decrypts the global model <math id="S2.I3.ix2.p1.1.m1.1" class="ltx_Math" alttext="E_{G}" display="inline"><semantics id="S2.I3.ix2.p1.1.m1.1a"><msub id="S2.I3.ix2.p1.1.m1.1.1" xref="S2.I3.ix2.p1.1.m1.1.1.cmml"><mi id="S2.I3.ix2.p1.1.m1.1.1.2" xref="S2.I3.ix2.p1.1.m1.1.1.2.cmml">E</mi><mi id="S2.I3.ix2.p1.1.m1.1.1.3" xref="S2.I3.ix2.p1.1.m1.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I3.ix2.p1.1.m1.1b"><apply id="S2.I3.ix2.p1.1.m1.1.1.cmml" xref="S2.I3.ix2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I3.ix2.p1.1.m1.1.1.1.cmml" xref="S2.I3.ix2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I3.ix2.p1.1.m1.1.1.2.cmml" xref="S2.I3.ix2.p1.1.m1.1.1.2">𝐸</ci><ci id="S2.I3.ix2.p1.1.m1.1.1.3.cmml" xref="S2.I3.ix2.p1.1.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix2.p1.1.m1.1c">E_{G}</annotation></semantics></math> using the secret key <math id="S2.I3.ix2.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.I3.ix2.p1.2.m2.1a"><mi id="S2.I3.ix2.p1.2.m2.1.1" xref="S2.I3.ix2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.I3.ix2.p1.2.m2.1b"><ci id="S2.I3.ix2.p1.2.m2.1.1.cmml" xref="S2.I3.ix2.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix2.p1.2.m2.1c">k</annotation></semantics></math>: <math id="S2.I3.ix2.p1.3.m3.2" class="ltx_Math" alttext="G=\text{Decrypt}(E_{G},k)" display="inline"><semantics id="S2.I3.ix2.p1.3.m3.2a"><mrow id="S2.I3.ix2.p1.3.m3.2.2" xref="S2.I3.ix2.p1.3.m3.2.2.cmml"><mi id="S2.I3.ix2.p1.3.m3.2.2.3" xref="S2.I3.ix2.p1.3.m3.2.2.3.cmml">G</mi><mo id="S2.I3.ix2.p1.3.m3.2.2.2" xref="S2.I3.ix2.p1.3.m3.2.2.2.cmml">=</mo><mrow id="S2.I3.ix2.p1.3.m3.2.2.1" xref="S2.I3.ix2.p1.3.m3.2.2.1.cmml"><mtext id="S2.I3.ix2.p1.3.m3.2.2.1.3" xref="S2.I3.ix2.p1.3.m3.2.2.1.3a.cmml">Decrypt</mtext><mo lspace="0em" rspace="0em" id="S2.I3.ix2.p1.3.m3.2.2.1.2" xref="S2.I3.ix2.p1.3.m3.2.2.1.2.cmml">​</mo><mrow id="S2.I3.ix2.p1.3.m3.2.2.1.1.1" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.2" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.2.cmml">(</mo><msub id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.cmml"><mi id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.2" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.2.cmml">E</mi><mi id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.3" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.3.cmml">G</mi></msub><mo id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.3" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.2.cmml">,</mo><mi id="S2.I3.ix2.p1.3.m3.1.1" xref="S2.I3.ix2.p1.3.m3.1.1.cmml">k</mi><mo stretchy="false" id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.4" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I3.ix2.p1.3.m3.2b"><apply id="S2.I3.ix2.p1.3.m3.2.2.cmml" xref="S2.I3.ix2.p1.3.m3.2.2"><eq id="S2.I3.ix2.p1.3.m3.2.2.2.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.2"></eq><ci id="S2.I3.ix2.p1.3.m3.2.2.3.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.3">𝐺</ci><apply id="S2.I3.ix2.p1.3.m3.2.2.1.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1"><times id="S2.I3.ix2.p1.3.m3.2.2.1.2.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1.2"></times><ci id="S2.I3.ix2.p1.3.m3.2.2.1.3a.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1.3"><mtext id="S2.I3.ix2.p1.3.m3.2.2.1.3.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1.3">Decrypt</mtext></ci><interval closure="open" id="S2.I3.ix2.p1.3.m3.2.2.1.1.2.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.1"><apply id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.1.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1">subscript</csymbol><ci id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.2.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.2">𝐸</ci><ci id="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.3.cmml" xref="S2.I3.ix2.p1.3.m3.2.2.1.1.1.1.3">𝐺</ci></apply><ci id="S2.I3.ix2.p1.3.m3.1.1.cmml" xref="S2.I3.ix2.p1.3.m3.1.1">𝑘</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix2.p1.3.m3.2c">G=\text{Decrypt}(E_{G},k)</annotation></semantics></math>;</p>
</div>
</li>
<li id="S2.I3.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iii)</span> 
<div id="S2.I3.ix3.p1" class="ltx_para">
<p id="S2.I3.ix3.p1.2" class="ltx_p">Evaluates the global model <math id="S2.I3.ix3.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S2.I3.ix3.p1.1.m1.1a"><mi id="S2.I3.ix3.p1.1.m1.1.1" xref="S2.I3.ix3.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S2.I3.ix3.p1.1.m1.1b"><ci id="S2.I3.ix3.p1.1.m1.1.1.cmml" xref="S2.I3.ix3.p1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix3.p1.1.m1.1c">G</annotation></semantics></math> on the test dataset <math id="S2.I3.ix3.p1.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.I3.ix3.p1.2.m2.1a"><mi id="S2.I3.ix3.p1.2.m2.1.1" xref="S2.I3.ix3.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.I3.ix3.p1.2.m2.1b"><ci id="S2.I3.ix3.p1.2.m2.1.1.cmml" xref="S2.I3.ix3.p1.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix3.p1.2.m2.1c">T</annotation></semantics></math>; and</p>
</div>
</li>
<li id="S2.I3.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(iv)</span> 
<div id="S2.I3.ix4.p1" class="ltx_para">
<p id="S2.I3.ix4.p1.1" class="ltx_p">Obtains the testing accuracy of the trained global model <math id="S2.I3.ix4.p1.1.m1.2" class="ltx_Math" alttext="\text{Accuracy}(G,T)" display="inline"><semantics id="S2.I3.ix4.p1.1.m1.2a"><mrow id="S2.I3.ix4.p1.1.m1.2.3" xref="S2.I3.ix4.p1.1.m1.2.3.cmml"><mtext id="S2.I3.ix4.p1.1.m1.2.3.2" xref="S2.I3.ix4.p1.1.m1.2.3.2a.cmml">Accuracy</mtext><mo lspace="0em" rspace="0em" id="S2.I3.ix4.p1.1.m1.2.3.1" xref="S2.I3.ix4.p1.1.m1.2.3.1.cmml">​</mo><mrow id="S2.I3.ix4.p1.1.m1.2.3.3.2" xref="S2.I3.ix4.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.I3.ix4.p1.1.m1.2.3.3.2.1" xref="S2.I3.ix4.p1.1.m1.2.3.3.1.cmml">(</mo><mi id="S2.I3.ix4.p1.1.m1.1.1" xref="S2.I3.ix4.p1.1.m1.1.1.cmml">G</mi><mo id="S2.I3.ix4.p1.1.m1.2.3.3.2.2" xref="S2.I3.ix4.p1.1.m1.2.3.3.1.cmml">,</mo><mi id="S2.I3.ix4.p1.1.m1.2.2" xref="S2.I3.ix4.p1.1.m1.2.2.cmml">T</mi><mo stretchy="false" id="S2.I3.ix4.p1.1.m1.2.3.3.2.3" xref="S2.I3.ix4.p1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I3.ix4.p1.1.m1.2b"><apply id="S2.I3.ix4.p1.1.m1.2.3.cmml" xref="S2.I3.ix4.p1.1.m1.2.3"><times id="S2.I3.ix4.p1.1.m1.2.3.1.cmml" xref="S2.I3.ix4.p1.1.m1.2.3.1"></times><ci id="S2.I3.ix4.p1.1.m1.2.3.2a.cmml" xref="S2.I3.ix4.p1.1.m1.2.3.2"><mtext id="S2.I3.ix4.p1.1.m1.2.3.2.cmml" xref="S2.I3.ix4.p1.1.m1.2.3.2">Accuracy</mtext></ci><interval closure="open" id="S2.I3.ix4.p1.1.m1.2.3.3.1.cmml" xref="S2.I3.ix4.p1.1.m1.2.3.3.2"><ci id="S2.I3.ix4.p1.1.m1.1.1.cmml" xref="S2.I3.ix4.p1.1.m1.1.1">𝐺</ci><ci id="S2.I3.ix4.p1.1.m1.2.2.cmml" xref="S2.I3.ix4.p1.1.m1.2.2">𝑇</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix4.p1.1.m1.2c">\text{Accuracy}(G,T)</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p id="S2.SS9.p1.2" class="ltx_p">In our proposed data-decoupling Federated Learning (DDFL) framework, the management and storage of intermediate and final results of the federated learning process are important for efficient model retrieval and management. To achieve this, we store each model as a collection of its parameters and other relevant features in a database system. Specifically, we employ a schema consisting of four columns for efficient management of the model. These columns are as follows:</p>
<ol id="S2.I4" class="ltx_enumerate">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p"><span id="S2.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">Round:</span> This column stores the round number of the federated training process for each model. As the federated learning process consists of several rounds of training, this column serves as a unique identifier for each model and helps to distinguish it from others in the database.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p id="S2.I4.i2.p1.1" class="ltx_p"><span id="S2.I4.i2.p1.1.1" class="ltx_text ltx_font_bold">Model:</span> The model column stores the parameters of the model, which are essential for reusing and updating the model in future rounds of training. This column enables the retrieval of a particular model, and its parameters from the database, which can then be used to resume the training process or update the model with the new training data.</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S2.I4.i3.p1" class="ltx_para">
<p id="S2.I4.i3.p1.1" class="ltx_p"><span id="S2.I4.i3.p1.1.1" class="ltx_text ltx_font_bold">Accuracy:</span> This column stores the testing accuracy of the model, which provides an evaluation metric for the model. This metric can be used to compare the effectiveness of different models or identify the best-performing model among all the stored models in the database.</p>
</div>
</li>
<li id="S2.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S2.I4.i4.p1" class="ltx_para">
<p id="S2.I4.i4.p1.1" class="ltx_p"><span id="S2.I4.i4.p1.1.1" class="ltx_text ltx_font_bold">Time:</span> This column stores the time taken to complete the corresponding round of training. This column serves as a performance metric for the training process and can be used to identify the training rounds that took more time and optimize them to achieve better performance.</p>
</div>
</li>
</ol>
<p id="S2.SS9.p1.3" class="ltx_p">By using this schema to store and organize all the models in the database system, the client nodes and the master node can efficiently manage and retrieve the models during the federated learning process. This enables us to efficiently track the performance of the models, identify the best-performing models, and use the stored models for resuming or updating the training process.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Evaluation</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Experimental Setup</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In our study, we assess the performance of the proposed DDFL framework using three standard benchmark datasets: CIFAR10 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib14" title="" class="ltx_ref">Krizhevsky2009, </a>)</cite>, Fashion-MNIST (FMNIST) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib21" title="" class="ltx_ref">fmnist, </a>)</cite>, and Street View House Numbers (SVHN) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">svhn, </a>)</cite>. To conduct our experiments, we utilize an 11-node cluster in CloudLab <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">cloudlab, </a>)</cite>, in which one node serves as the master node and the remaining nodes function as client nodes. The hardware configuration of the master and client nodes includes an Intel Xeon CPU E5-2690 v4 @ 2.60GHz processor and 377GB of RAM and 2x Ethernet Controller X710 for 10GbE SFP+. Importantly, the nodes share a common network interface, which facilitates communication among them. Our experiments are implemented using PyTorch as the deep learning framework. By conducting these experiments, we aim to evaluate the efficiency and effectiveness of the proposed framework in federated learning scenarios using various datasets.
In order to evaluate the performance of the DDFL framework, we deployed the system using six different database systems: RabbitMQ <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">rabbitmq, </a>)</cite>, MongoDB <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib16" title="" class="ltx_ref">mongodb, </a>)</cite>, Secure File Transfer (SCP) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib22" title="" class="ltx_ref">scp, </a>)</cite>, Apache Cassandra <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">cassandra, </a>)</cite>, Neo4j <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib24" title="" class="ltx_ref">neo4j, </a>)</cite>, Postgres <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">postgres, </a>)</cite>. The objective of this experiment was to compare the performance of these systems in terms of several important metrics, such as training time, accuracy, and training loss. By testing the DDFL framework with different database systems, we aimed to identify which system works best with the framework and provides the best results in terms of the aforementioned metrics.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To evaluate the effectiveness of our proposed approach for DDFL, we conducted experiments on three different datasets, namely Fashion-MNIST, SVHN, and CIFAR-10. We report the results of both fine-tuned hyper-parameters <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib71" title="" class="ltx_ref">comprehensiveabu2022, </a>)</cite> and without hyper-parameters experiments in terms of the accuracy of the global models.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The source code is released at <a target="_blank" href="https://github.com/jahanxb/flcode" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/jahanxb/flcode</a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Usability of Different Database Layers in Federated Learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The choice of database layer in a federated learning system can have a significant impact on its development and adaptability. In this study, we investigated the integration of several popular database systems, including Apache Cassandra, MongoDB, PostgreSQL, Neo4j, SCP, and RabbitMQ Queues, into our DDFL framework.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The integration process for each database system varied in terms of difficulty, development effort, and adaptability to our system. Database systems with established connectors or plugins for our programming language and framework were easier to integrate and required less development effort. For example, MongoDB and PostgreSQL had established libraries that provided straightforward integration with our system, and their installation and configuration were relatively easy.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">However, integrating Apache Cassandra and Neo4j into our system required a more substantial development effort, as they had less established connectors and required custom development of query and data retrieval methods. These systems also required more time for installation and configuration due to their additional dependencies and configuration requirements.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">SCP and RabbitMQ Queues posed significant challenges for integration, as they required considerable custom development to integrate with our system. SCP required a custom wrapper to support its interface, which required significant development effort. RabbitMQ Queues also required additional configuration for message queuing and handling, which added complexity to the integration process.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span><span id="S3.SS2.SSS1.1.1" class="ltx_text ltx_font_bold">Ease of Adopting Database System in Federated Learning</span>
</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The ease of adopting different database systems is also an important consideration when designing a federated learning system. We evaluated popular database systems such as MongoDB, Postgres, Neo4j, SCP, Cassandra, and RabbitMQ based on their ease of learning and integration into our DDFL system. Our results showed that some systems, such as MongoDB and Postgres, were relatively easy to learn and integrate, requiring less than 200 lines of code. Other systems, such as Neo4j and RabbitMQ, were more challenging to learn and required over 500 lines of code to integrate fully.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">In addition to ease of integration, the size and activity of each system’s community can also impact its adoption. Systems such as MongoDB and Postgres have large and active communities that provide support and resources for developers. In contrast, other systems, such as Neo4j and RabbitMQ, have smaller communities, which can make it more challenging to find resources and support when integrating them into a federated learning system.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">In conclusion, the ease of integration and adaptability of database layers in federated learning systems can vary significantly. Developers should carefully consider the specific requirements of their federated learning system when choosing a database system. Systems with established connectors and plugins for the programming language and framework used in the system are easier to integrate and require less development effort. Custom development is required for systems that lack established connectors and plugins, which can add significant development time and effort. The size and activity of the community around each system are also important factors to consider when adopting database systems.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Testing Accuracy</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span><span id="S3.SS3.SSS1.1.1" class="ltx_text ltx_font_bold">Testing Accuracy without Fine-tuned Hyper-parameters</span>
</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">We present the results of testing the accuracy of our proposed Data-Decoupling Federated Learning (DDFL) system without fine-tuned hyperparameters. We evaluated our system’s performance using three different datasets: CIFAR-10, Fashion-MNIST, and SVHN. Our experiments aimed to compare the accuracy of our DDFL system with different database systems, including MongoDB, SCP, Apache Cassandra, and RabbitMQ Queues, against a centralized machine learning method. In the centralized method, the model was trained on a single node, and clients were simulated on the same machine <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib38" title="" class="ltx_ref">shaoxiongji20184321561, </a>; <a href="#bib.bib8" title="" class="ltx_ref">mcmahanaistat17, </a>; <a href="#bib.bib9" title="" class="ltx_ref">Konecny2016, </a>; <a href="#bib.bib60" title="" class="ltx_ref">practical-secure-aggr, </a>)</cite>. The experiments were conducted with baseline arguments for all distributed nodes without any fine-tuning.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p">The primary purpose of the experiments was to evaluate the effectiveness of the data-decoupling solutions in improving the system’s performance. We compared the testing accuracy of our DDFL system with the centralized machine learning method, which provided a baseline for our evaluation. The results of the experiments provided insights into the performance of our DDFL system and the data-decoupling solutions.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.1" class="ltx_p">As shown in Figures <a href="#S3.F3" title="Figure 3 ‣ 3.3.1. Testing Accuracy without Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <a href="#S3.F4" title="Figure 4 ‣ 3.3.1. Testing Accuracy without Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and  <a href="#S3.F5" title="Figure 5 ‣ 3.3.1. Testing Accuracy without Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, our DDFL system achieves a higher testing accuracy than the single node baseline after 10 rounds of training. Specifically, on the CIFAR-10 dataset, the DDFL system achieves a testing accuracy of 85% after 10 rounds, compared to the baseline accuracy of 81%. On the Fashion-MNIST dataset, the DDFL system achieves a testing accuracy of 92% after 10 rounds, compared to the baseline accuracy of 89%. On the SVHN dataset, the DDFL system achieves a testing accuracy of 95% after 10 rounds, compared to the baseline accuracy of 94%.</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para">
<p id="S3.SS3.SSS1.p4.1" class="ltx_p">We also observe that the performance of each database system varies across the datasets. On the CIFAR-10 dataset, MongoDB and SCP perform the best, achieving a testing accuracy of 85.56% and 85.86%, respectively, after 10 rounds of training. Apache Cassandra achieved a comparable testing accuracy score to the other database systems in the DDFL system, but exhibited lower performance during different iterations, but performs better on the Fashion-MNIST and SVHN datasets, achieving testing accuracy scores of 92% and 95.75%, respectively, after 10 rounds. RabbitMQ Queues shows moderate performance on all datasets, achieving testing accuracies of 83%, 90%, and 94% on CIFAR-10, Fashion-MNIST, and SVHN, respectively, after 10 rounds.</p>
</div>
<div id="S3.SS3.SSS1.p5" class="ltx_para">
<p id="S3.SS3.SSS1.p5.1" class="ltx_p">We further compare the testing accuracy of our DDFL system with different database systems against the single node baseline in the 1st, 5th, and 10th iterations. As shown in Figures <a href="#S3.F3" title="Figure 3 ‣ 3.3.1. Testing Accuracy without Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <a href="#S3.F4" title="Figure 4 ‣ 3.3.1. Testing Accuracy without Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and  <a href="#S3.F5" title="Figure 5 ‣ 3.3.1. Testing Accuracy without Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the DDFL system consistently outperforms the single node baseline in all iterations. In addition, we observe that the difference in testing accuracy between the DDFL system and the single node baseline becomes more significant as the number of iterations increases. For example, on the CIFAR-10 dataset, the DDFL system achieves a testing accuracy of 50-58% in the 1st iteration, compared to the baseline accuracy of 48%. However, in the 10th iteration, the DDFL system achieves a testing accuracy of 83-85%, compared to the baseline accuracy of 81%.</p>
</div>
<div id="S3.SS3.SSS1.p6" class="ltx_para">
<p id="S3.SS3.SSS1.p6.1" class="ltx_p">These results suggest that the choice of data-decoupling solution can have a significant impact on the performance of an FL system and that this impact may vary depending on the characteristics of the dataset being used. When choosing a data-decoupling solution, it is essential to consider the specific requirements and constraints of the FL system. For instance, the communication protocol and system scalability are crucial factors that need to be taken into account. In addition, the architecture of the system and its optimization for distributed environments can also play a crucial role in improving the performance of the system.</p>
</div>
<div id="S3.SS3.SSS1.p7" class="ltx_para">
<p id="S3.SS3.SSS1.p7.1" class="ltx_p">Regarding the differences in performance between the baseline and the different database systems, we observe that our DDFL system consistently outperforms the single node baseline after 10 rounds of training on all three datasets. For example, on CIFAR-10, the DDFL system achieves a testing accuracy of approximately 81% after 10 rounds of training, compared to approximately 74% for the baseline. On Fashion-MNIST, the DDFL system achieves a testing accuracy of approximately 90% after 10 rounds of training, compared to approximately 83% for the baseline. On SVHN, the DDFL system achieves a testing accuracy of approximately 85% after 10 rounds of training, compared to approximately 77% for the baseline.</p>
</div>
<div id="S3.SS3.SSS1.p8" class="ltx_para">
<p id="S3.SS3.SSS1.p8.1" class="ltx_p">Moreover, we observe that the performance of each database system varies across the datasets. On CIFAR-10, MongoDB and SCP perform similarly and achieve the highest testing accuracy among the four database systems, Although Apache Cassandra had lower performance in some of the iterations, the system still achieved comparable accuracy scores. On Fashion-MNIST and SVHN, MongoDB and SCP again perform the best, with Apache Cassandra showing a higher performance than the other systems. RabbitMQ Queues show moderate performance on all datasets. These differences in performance highlight the importance of selecting an appropriate data-decoupling solution for a given FL system and dataset.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2303.08371/assets/cifar.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Testing accuracy of the model trained using DDFL with different types of databases on the CIFAR-10 dataset, compared with the baseline model trained on a single node without Fine-tuned Hyper-parameters.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2303.08371/assets/fmnist.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Testing accuracy of the model trained using DDFL with different types of databases on Fashion-MNIST dataset, compared with the baseline model trained on a single node without Fine-tuned Hyper-parameters.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2303.08371/assets/svhn.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Testing accuracy of the model trained using DDFL with different types of databases on SVHN dataset, compared with the baseline model trained on a single node without Fine-tuned Hyper-parameters.</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span><span id="S3.SS3.SSS2.1.1" class="ltx_text ltx_font_bold">Testing Accuracy with Fine-tuned Hyper-parameters</span>
</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">In this section, we present the results of our fine-tuned experiments where we optimized the hyperparameters of the models to improve the accuracy of our Data-Decoupling Federated Learning (DDFL) system. We expanded our evaluation by including two additional database systems, Neo4j and PostgreSQL, to further validate our results. We conducted experiments on the CIFAR-10, Fashion-MNIST, and SVHN datasets, using the same model architectures and training procedures as in the previous section (<a href="#S3.SS3.SSS1" title="3.3.1. Testing Accuracy without Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>). We fine-tuned the models on all nodes with the same hyperparameters and compared the performance of the fine-tuned models with the baseline single-node model. The testing accuracy of the fine-tuned DDFL system was compared to that of the centralized machine learning method. Our evaluation aimed to investigate the performance of our DDFL system with different database systems when the models are fine-tuned with optimal hyperparameters.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">Figure <a href="#S3.F6" title="Figure 6 ‣ 3.3.2. Testing Accuracy with Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the testing accuracy of the models trained using DDFL with different database systems on the CIFAR-10 dataset, compared to the baseline model trained on a single node. We observed that the fine-tuned models achieved higher accuracy compared to the single node baseline, with an average accuracy improvement of 2.23%. We also observed that the performance of DDFL was consistent across all platforms, with no significant difference in accuracy scores.</p>
</div>
<div id="S3.SS3.SSS2.p3" class="ltx_para">
<p id="S3.SS3.SSS2.p3.1" class="ltx_p">Similarly, Figure <a href="#S3.F7" title="Figure 7 ‣ 3.3.2. Testing Accuracy with Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the testing accuracy of the models trained using DDFL with different database systems on the Fashion-MNIST dataset, compared to the baseline model trained on a single node. We observed an average accuracy improvement of 1.84% with the fine-tuned models, again with consistent performance across all platforms.</p>
</div>
<div id="S3.SS3.SSS2.p4" class="ltx_para">
<p id="S3.SS3.SSS2.p4.1" class="ltx_p">Finally, Figure <a href="#S3.F8" title="Figure 8 ‣ 3.3.2. Testing Accuracy with Fine-tuned Hyper-parameters ‣ 3.3. Testing Accuracy ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the testing accuracy of the models trained using DDFL with different database systems on the SVHN dataset, compared to the baseline model trained on a single node. The fine-tuned models achieved an average accuracy improvement of 1.87% over the single node baseline, again with consistent performance across all platforms.</p>
</div>
<div id="S3.SS3.SSS2.p5" class="ltx_para">
<p id="S3.SS3.SSS2.p5.1" class="ltx_p">These results demonstrate the effectiveness of the DDFL framework for collaborative deep learning with multiple database systems and the benefits of using fine-tuning to improve the performance of the models.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2303.08371/assets/cifar1.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Testing accuracy of the model trained using DDFL with different types of databases on the CIFAR-10 dataset, compared with the baseline model trained on a single node with Fine-tuned Hyper-parameters.</figcaption>
</figure>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2303.08371/assets/fmnist1.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Testing accuracy of the model trained using DDFL with different types of databases on Fashion-MNIST dataset, compared with the baseline model trained on a single node with Fine-tuned Hyper-parameters.</figcaption>
</figure>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2303.08371/assets/svhn1.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="314" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Testing accuracy of the model trained using DDFL with different types of databases on SVHN dataset, compared with the baseline model trained on a single node with Fine-tuned Hyper-parameters.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Training Time</h3>

<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span><span id="S3.SS4.SSS1.1.1" class="ltx_text ltx_font_bold">Training Time Comparison without Fine-tuned Hyper-parameters</span>
</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">The results show that the training time of the models varied depending on the database system used. For example, the SCP training system took significantly longer to train compared to other database systems such as MongoDB, RabbitMQ, and Apache Cassandra. The other systems, such as MongoDB, RabbitMQ, and Apache Cassandra, took less time to train the models.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">The results in Figures <a href="#S3.F11" title="Figure 11 ‣ 3.4.1. Training Time Comparison without Fine-tuned Hyper-parameters ‣ 3.4. Training Time ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>–<a href="#S3.F11" title="Figure 11 ‣ 3.4.1. Training Time Comparison without Fine-tuned Hyper-parameters ‣ 3.4. Training Time ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> show that the centralized database system MongoDB has the shortest training time across all three datasets (CIFAR-10, Fashion-MNIST, and SVHN). Overall, these results suggest that the centralized database system MongoDB can be effective in reducing the training time of an FL system, which is important for real-world applications.</p>
</div>
<figure id="S3.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F11.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;"><img src="/html/2303.08371/assets/training-time-cifar.png" id="S3.F11.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Training time of DDFL with different database systems on CIFAR-10 dataset without Fine-tuned Hyper-parameters.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F11.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;"><img src="/html/2303.08371/assets/training-time-fmnist.png" id="S3.F11.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Training time of DDFL with different database systems on Fashion-MNIST dataset without Fine-tuned Hyper-parameters.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F11.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;"><img src="/html/2303.08371/assets/training-time-svhn.png" id="S3.F11.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Training time of DDFL with different database systems on SVHN dataset without Fine-tuned Hyper-parameters.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span><span id="S3.SS4.SSS2.1.1" class="ltx_text ltx_font_bold">Training Time Comparison with  Fine-tuned Hyper-parameters</span>
</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">Fine-tuning a pre-trained deep learning model for a specific task is known to take longer than training from scratch due to the complex nature of the pre-trained model, the larger size of the fine-tuning dataset, and the many parameters that need to be updated. In our study, we observed a significant increase in training time for fine-tuning the deep learning models on the three datasets (CIFAR-10, Fashion-MNIST, and SVHN) when compared to the training from scratch. As shown in Figures <a href="#S3.F14" title="Figure 14 ‣ 3.4.2. Training Time Comparison with Fine-tuned Hyper-parameters ‣ 3.4. Training Time ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, <a href="#S3.F14" title="Figure 14 ‣ 3.4.2. Training Time Comparison with Fine-tuned Hyper-parameters ‣ 3.4. Training Time ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, and <a href="#S3.F14" title="Figure 14 ‣ 3.4.2. Training Time Comparison with Fine-tuned Hyper-parameters ‣ 3.4. Training Time ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> illustrate the training time of the DDFL with different database systems on the CIFAR-10, Fashion-MNIST, and SVHN datasets, respectively, the training time for fine-tuning the deep learning models ranged from 47 minutes to 55 minutes, depending on the dataset, while training from scratch took approximately 35-40 minutes. The longer training time observed in the fine-tuned experiments can be attributed to the additional complexity of the fine-tuning process, which requires additional computations to optimize the weights of the pre-trained model for the new task. Additionally, fine-tuning requires more computing resources and a larger number of training iterations, which result in more time-consuming training. Despite the longer training time, fine-tuning has been shown to lead to better performance compared to training from scratch, as it takes advantage of the knowledge learned by the pre-trained model on a larger dataset.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p">This observation is in line with previous studies that have reported a longer training time for fine-tuning compared to training from scratch <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib35" title="" class="ltx_ref">howard2018universal, </a>; <a href="#bib.bib36" title="" class="ltx_ref">chen2019big, </a>; <a href="#bib.bib37" title="" class="ltx_ref">gao2021auto, </a>)</cite>. The increase in training time is also due to the complexity of the deep learning model architecture and the amount of data used for fine-tuning. However, it is important to note that the longer training time is a trade-off for better performance achieved by fine-tuning the pre-trained models. As shown in our study, fine-tuning the pre-trained models resulted in better performance compared to training from scratch on all three datasets. Thus, the longer training time required for fine-tuning can be justified in applications where achieving optimal performance is critical.</p>
</div>
<figure id="S3.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F14.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;"><img src="/html/2303.08371/assets/ttt-cifar10.png" id="S3.F14.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>Training time of DDFL with different database systems on CIFAR-10 dataset with Fine-tuned Hyper-parameters.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F14.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;"><img src="/html/2303.08371/assets/ttt-fmnist.png" id="S3.F14.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>Training time of DDFL with different database systems on Fashion-MNIST dataset with Fine-tuned Hyper-parameters.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F14.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:433.6pt;"><img src="/html/2303.08371/assets/ttt-svhn.png" id="S3.F14.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>Training time of DDFL with different database systems on SVHN dataset with Fine-tuned Hyper-parameters.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Query Performance</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">In this study, we evaluated the query performance of different database systems in a DDFL system by comparing MongoDB, Apache Cassandra, PostgreSQL, Neo4j, and RabbitMQ Queues. We executed a sample query as shown in Figure <a href="#S3.F15" title="Figure 15 ‣ 3.5. Query Performance ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> to retrieve the testing data of a specific client on each database system and measured the execution time. Our experiments were conducted on three datasets, CIFAR-10, FMNIST, and SVHN.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Our findings suggest that centralized database systems, such as MongoDB, have better query performance in FL systems compared to distributed database systems like Neo4j and RabbitMQ Queues. MongoDB had the shortest query execution time across all datasets, with an average query execution time of 0.058 ms for CIFAR-10, 0.059 ms for FMNIST, and 0.056 ms for SVHN. In contrast, Neo4j and RabbitMQ Queues had significantly longer query execution times. For example, the average query execution time for Neo4j was 117.787 ms for CIFAR-10, 97.986 ms for FMNIST, and 191.491 ms for SVHN. It is important to note that the query performance may be influenced by several factors, such as the dataset size, query complexity, and the number of nodes in the FL system.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Query performance of different database systems on the CIFAR-10, FMNIST, and SVHN datasets in a DDFL system. The query execution time is reported in milliseconds.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Databases</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">MongoDB</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">SCP</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">RabbitMQ</span></th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Cassandra</span></th>
<th id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Postgres</span></th>
<th id="S3.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">Neo4j</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.1.1" class="ltx_text ltx_font_bold">CIFAR10</span></td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.059</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">227.160</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.098</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">35.438</td>
<td id="S3.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">279.426</td>
<td id="S3.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">117.787</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.1.1" class="ltx_text ltx_font_bold">FMNIST</span></td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center">0.059</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center">244.847</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center">0.129</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center">132.974</td>
<td id="S3.T1.1.3.2.6" class="ltx_td ltx_align_center">390.992</td>
<td id="S3.T1.1.3.2.7" class="ltx_td ltx_align_center">97.986</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.1.4.3.1.1" class="ltx_text ltx_font_bold">SVHN</span></td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.056</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">282.027</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.239</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">358.901</td>
<td id="S3.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">693.879</td>
<td id="S3.T1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">191.491</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">We designed a sample query to retrieve the testing data of a specific client, and the query execution time is reported in milliseconds in Table <a href="#S3.T1" title="Table 1 ‣ 3.5. Query Performance ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our study has practical implications for real-world FL applications where query performance is critical. However, further research is needed to investigate the scalability and robustness of different database systems in FL systems.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">Our experiments demonstrated that centralized database systems like MongoDB may have better query performance in FL systems compared to other database systems like Postgres, Apache Cassandra, Neo4j, etc. Our study contributes to the ongoing research in FL systems by evaluating the query performance of different database systems in a DDFL system.</p>
</div>
<figure id="S3.F15" class="ltx_figure"><img src="/html/2303.08371/assets/sample-query.png" id="S3.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="354" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>Sample Database Queries</figcaption>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6. </span>Communication Cost</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">The communication cost is a critical aspect of federated learning systems, as it determines the amount of data that must be transmitted between clients and the server during the model training process. To evaluate the communication cost of our federated learning system, we conducted experiments with varying numbers of clients and different batch sizes.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Communication cost, model size, number of values, and communication time for a single iteration on a single client</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Model Size (MB)</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Num. of Values</span></th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Comm. Time(ms)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">CIFAR-10</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.061</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">53,761</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.488</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_center">FMNIST</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center">0.184</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_center">166,370</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_center">1.472</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b">SVHN</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">0.382</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b">343,175</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b">3.056</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Scalability results for different database systems on different numbers of clients (in seconds)</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Num. of Clients</span></th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">SCP</span></th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Cassandra</span></th>
<th id="S3.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">RabbitMQ</span></th>
<th id="S3.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Neo4j</span></th>
<th id="S3.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">PostgreSQL</span></th>
<th id="S3.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T3.1.1.1.7.1" class="ltx_text ltx_font_bold">MongoDB</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<th id="S3.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.1.2.1.1.1" class="ltx_text ltx_font_bold">FMNIST</span></th>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.2.1.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.2.1.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.2.1.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.2.1.7" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<th id="S3.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center">409.649</td>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center">152.232</td>
<td id="S3.T3.1.3.2.4" class="ltx_td ltx_align_center">138.218</td>
<td id="S3.T3.1.3.2.5" class="ltx_td ltx_align_center">199.319</td>
<td id="S3.T3.1.3.2.6" class="ltx_td ltx_align_center">37.037</td>
<td id="S3.T3.1.3.2.7" class="ltx_td ltx_align_center">32.032</td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<th id="S3.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<td id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center">1058.1738</td>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center">201.321</td>
<td id="S3.T3.1.4.3.4" class="ltx_td ltx_align_center">249.49</td>
<td id="S3.T3.1.4.3.5" class="ltx_td ltx_align_center">527.847</td>
<td id="S3.T3.1.4.3.6" class="ltx_td ltx_align_center">65.15</td>
<td id="S3.T3.1.4.3.7" class="ltx_td ltx_align_center">40.040</td>
</tr>
<tr id="S3.T3.1.5.4" class="ltx_tr">
<th id="S3.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S3.T3.1.5.4.2" class="ltx_td ltx_align_center">1409.2329</td>
<td id="S3.T3.1.5.4.3" class="ltx_td ltx_align_center">286.446</td>
<td id="S3.T3.1.5.4.4" class="ltx_td ltx_align_center">379.619</td>
<td id="S3.T3.1.5.4.5" class="ltx_td ltx_align_center">319.519</td>
<td id="S3.T3.1.5.4.6" class="ltx_td ltx_align_center">108.148</td>
<td id="S3.T3.1.5.4.7" class="ltx_td ltx_align_center">53.053</td>
</tr>
<tr id="S3.T3.1.6.5" class="ltx_tr">
<th id="S3.T3.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">8</th>
<td id="S3.T3.1.6.5.2" class="ltx_td ltx_align_center">2303.3823</td>
<td id="S3.T3.1.6.5.3" class="ltx_td ltx_align_center">312.512</td>
<td id="S3.T3.1.6.5.4" class="ltx_td ltx_align_center">414.654</td>
<td id="S3.T3.1.6.5.5" class="ltx_td ltx_align_center">393.633</td>
<td id="S3.T3.1.6.5.6" class="ltx_td ltx_align_center">135.215</td>
<td id="S3.T3.1.6.5.7" class="ltx_td ltx_align_center">67.17</td>
</tr>
<tr id="S3.T3.1.7.6" class="ltx_tr">
<th id="S3.T3.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.1.7.6.1.1" class="ltx_text ltx_font_bold">CIFAR10</span></th>
<td id="S3.T3.1.7.6.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.7.6.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.7.6.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.7.6.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.7.6.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.7.6.7" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T3.1.8.7" class="ltx_tr">
<th id="S3.T3.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S3.T3.1.8.7.2" class="ltx_td ltx_align_center">2059.3419</td>
<td id="S3.T3.1.8.7.3" class="ltx_td ltx_align_center">493.813</td>
<td id="S3.T3.1.8.7.4" class="ltx_td ltx_align_center">537.857</td>
<td id="S3.T3.1.8.7.5" class="ltx_td ltx_align_center">71.111</td>
<td id="S3.T3.1.8.7.6" class="ltx_td ltx_align_center">29.029</td>
<td id="S3.T3.1.8.7.7" class="ltx_td ltx_align_center">25.025</td>
</tr>
<tr id="S3.T3.1.9.8" class="ltx_tr">
<th id="S3.T3.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<td id="S3.T3.1.9.8.2" class="ltx_td ltx_align_center">2328.3848</td>
<td id="S3.T3.1.9.8.3" class="ltx_td ltx_align_center">1804.304</td>
<td id="S3.T3.1.9.8.4" class="ltx_td ltx_align_center">1059.1739</td>
<td id="S3.T3.1.9.8.5" class="ltx_td ltx_align_center">140.220</td>
<td id="S3.T3.1.9.8.6" class="ltx_td ltx_align_center">68.18</td>
<td id="S3.T3.1.9.8.7" class="ltx_td ltx_align_center">52.052</td>
</tr>
<tr id="S3.T3.1.10.9" class="ltx_tr">
<th id="S3.T3.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S3.T3.1.10.9.2" class="ltx_td ltx_align_center">2743.4543</td>
<td id="S3.T3.1.10.9.3" class="ltx_td ltx_align_center">2295.3815</td>
<td id="S3.T3.1.10.9.4" class="ltx_td ltx_align_center">2044.344</td>
<td id="S3.T3.1.10.9.5" class="ltx_td ltx_align_center">278.438</td>
<td id="S3.T3.1.10.9.6" class="ltx_td ltx_align_center">100.140</td>
<td id="S3.T3.1.10.9.7" class="ltx_td ltx_align_center">65.15</td>
</tr>
<tr id="S3.T3.1.11.10" class="ltx_tr">
<th id="S3.T3.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">8</th>
<td id="S3.T3.1.11.10.2" class="ltx_td ltx_align_center">2719.4519</td>
<td id="S3.T3.1.11.10.3" class="ltx_td ltx_align_center">2535.4215</td>
<td id="S3.T3.1.11.10.4" class="ltx_td ltx_align_center">2635.4355</td>
<td id="S3.T3.1.11.10.5" class="ltx_td ltx_align_center">312.512</td>
<td id="S3.T3.1.11.10.6" class="ltx_td ltx_align_center">135.215</td>
<td id="S3.T3.1.11.10.7" class="ltx_td ltx_align_center">97.137</td>
</tr>
<tr id="S3.T3.1.12.11" class="ltx_tr">
<th id="S3.T3.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.1.12.11.1.1" class="ltx_text ltx_font_bold">SVHN</span></th>
<td id="S3.T3.1.12.11.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.12.11.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.12.11.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.12.11.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.12.11.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.12.11.7" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T3.1.13.12" class="ltx_tr">
<th id="S3.T3.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S3.T3.1.13.12.2" class="ltx_td ltx_align_center">1433.2353</td>
<td id="S3.T3.1.13.12.3" class="ltx_td ltx_align_center">123.23</td>
<td id="S3.T3.1.13.12.4" class="ltx_td ltx_align_center">1124.1844</td>
<td id="S3.T3.1.13.12.5" class="ltx_td ltx_align_center">1224.2024</td>
<td id="S3.T3.1.13.12.6" class="ltx_td ltx_align_center">823.1343</td>
<td id="S3.T3.1.13.12.7" class="ltx_td ltx_align_center">823.1343</td>
</tr>
<tr id="S3.T3.1.14.13" class="ltx_tr">
<th id="S3.T3.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<td id="S3.T3.1.14.13.2" class="ltx_td ltx_align_center">2255.3735</td>
<td id="S3.T3.1.14.13.3" class="ltx_td ltx_align_center">152.232</td>
<td id="S3.T3.1.14.13.4" class="ltx_td ltx_align_center">1522.2522</td>
<td id="S3.T3.1.14.13.5" class="ltx_td ltx_align_center">1426.2346</td>
<td id="S3.T3.1.14.13.6" class="ltx_td ltx_align_center">1089.189</td>
<td id="S3.T3.1.14.13.7" class="ltx_td ltx_align_center">1089.189</td>
</tr>
<tr id="S3.T3.1.15.14" class="ltx_tr">
<th id="S3.T3.1.15.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S3.T3.1.15.14.2" class="ltx_td ltx_align_center">2541.4221</td>
<td id="S3.T3.1.15.14.3" class="ltx_td ltx_align_center">171.251</td>
<td id="S3.T3.1.15.14.4" class="ltx_td ltx_align_center">2179.3619</td>
<td id="S3.T3.1.15.14.5" class="ltx_td ltx_align_center">1630.271</td>
<td id="S3.T3.1.15.14.6" class="ltx_td ltx_align_center">1504.254</td>
<td id="S3.T3.1.15.14.7" class="ltx_td ltx_align_center">1504.254</td>
</tr>
<tr id="S3.T3.1.16.15" class="ltx_tr">
<th id="S3.T3.1.16.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">8</th>
<td id="S3.T3.1.16.15.2" class="ltx_td ltx_align_center ltx_border_bb">2831.4711</td>
<td id="S3.T3.1.16.15.3" class="ltx_td ltx_align_center ltx_border_bb">192.312</td>
<td id="S3.T3.1.16.15.4" class="ltx_td ltx_align_center ltx_border_bb">2634.4354</td>
<td id="S3.T3.1.16.15.5" class="ltx_td ltx_align_center ltx_border_bb">1832.3032</td>
<td id="S3.T3.1.16.15.6" class="ltx_td ltx_align_center ltx_border_bb">2058.3418</td>
<td id="S3.T3.1.16.15.7" class="ltx_td ltx_align_center ltx_border_bb">2058.3418</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">Our results, presented in Table <a href="#S3.T2" title="Table 2 ‣ 3.6. Communication Cost ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, show that the communication cost increases linearly with the number of clients and the batch size. Compared to a traditional centralized machine learning system, our federated learning system incurs a higher communication cost due to the need to transmit updated model parameters from the clients to the server. However, we note that our system offers significant advantages in terms of data privacy and security since sensitive data is kept on the client’s side and not transmitted to the server.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">These results are consistent with previous studies, such as those by McMahan et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib8" title="" class="ltx_ref">mcmahanaistat17, </a>)</cite>, FedML <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib6" title="" class="ltx_ref">fedml, </a>)</cite>, and Bonawitz et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib27" title="" class="ltx_ref">fl2019corr, </a>)</cite>. Their studies also report similar trends in communication costs with varying batch sizes and numbers of clients.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p">Overall, our experiments demonstrate the importance of using data partitioning techniques to reduce communication costs in federated learning systems as we observed that the use of data partitioning techniques significantly reduces the communication cost, as smaller data chunks are transmitted during each round. This finding is critical for the development of efficient and scalable federated learning systems that can handle large numbers of clients while maintaining the privacy and security of sensitive data.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7. </span>Scalability</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">Scalability is a crucial aspect of distributed systems, especially for federated learning systems with a large number of nodes participating in the training process. To improve the scalability of our research system, we implemented various techniques, such as decoupling the data between clients and the server using database middleware. By storing the clients’ data in a database, the server can access the data for model training without direct communication with the clients, reducing communication costs and network overhead.
Moreover, we adopted a decentralized approach to federated learning, where each node trains a local model using its own private data. The local models are then aggregated by the global node to update the global model. This approach distributes the training process among nodes and scales efficiently to a large number of clients.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">We evaluated the scalability of our system by conducting experiments with different numbers of nodes and data sizes. The results presented in Table <a href="#S3.T3" title="Table 3 ‣ 3.6. Communication Cost ‣ 3. Evaluation ‣ Comparative Evaluation of Data Decoupling Techniques for Federated Machine Learning with Database as a Service" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrate that our system can scale efficiently to a large number of nodes without significant performance degradation. We observed a linear increase in training time with the number of nodes and data size, indicating that our system can efficiently handle large-scale federated learning tasks.</p>
</div>
<div id="S3.SS7.p3" class="ltx_para">
<p id="S3.SS7.p3.1" class="ltx_p">Overall, our research system demonstrates excellent scalability, which is essential for large-scale federated learning applications. The combination of decentralized federated learning with a database middleware approach provides a promising solution for scaling federated learning systems to meet the requirements of real-world applications.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Related Work</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Distributed Machine Learning</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Federated learning is a promising distributed machine learning <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib65" title="" class="ltx_ref">Konecn2016FederatedOD, </a>; <a href="#bib.bib10" title="" class="ltx_ref">Bonawitz2019, </a>; <a href="#bib.bib66" title="" class="ltx_ref">Bao2022, </a>; <a href="#bib.bib70" title="" class="ltx_ref">distibutedmlwolfe2022, </a>)</cite> technique that enables multiple clients to collaboratively train a model without sharing their private data. The technique has garnered significant attention from both the research community and industry, owing to its potential to facilitate privacy-preserving and scalable data analysis. In recent years, researchers have explored various aspects of federated learning, including privacy and security, communication efficiency, and scalability.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Numerous research works have been published on federated learning, with a focus on different aspects of the technique. For example, Bonawitz et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib10" title="" class="ltx_ref">Bonawitz2019, </a>)</cite> proposed a framework for secure and privacy-preserving federated learning, while Konecny et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib9" title="" class="ltx_ref">Konecny2016, </a>)</cite> introduced a method for federated optimization. Yang et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib29" title="" class="ltx_ref">Yang2019, </a>)</cite> studied the communication efficiency of federated learning, while Jeong et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib50" title="" class="ltx_ref">jeong2018communication, </a>)</cite> proposed a scheme to reduce communication costs in federated learning. Additionally, researchers have investigated data decoupling techniques for federated learning, including database middleware <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib51" title="" class="ltx_ref">Zhang2020HybridFL, </a>)</cite>, data partitioning <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">kairouz2019advances, </a>)</cite>, and privacy-preserving data aggregation <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib49" title="" class="ltx_ref">hercules, </a>)</cite>. These works have contributed to a better understanding of federated learning and its potential applications in various domains, including healthcare, finance, and smart cities.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Communication-efficient Federated Learning</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Federated Averaging is a communication-efficient approach for federated learning proposed by McMahan et al <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib8" title="" class="ltx_ref">mcmahanaistat17, </a>)</cite>. It involves exchanging model updates between client nodes and a server, which reduces communication overhead. This approach enables distributed model training without data sharing, making it a potential solution for privacy and security concerns in data sharing.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Konecny et al <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib9" title="" class="ltx_ref">Konecny2016, </a>)</cite> proposed sparsification, quantization, and differential privacy as strategies to reduce communication costs in federated learning. Experimental results have demonstrated their effectiveness in maintaining model accuracy, but further research is needed to determine their suitability for different data types and to optimize the trade-off between communication cost and model accuracy.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Jhun et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib53" title="" class="ltx_ref">Jhunjhunwala2021AdaptiveQO, </a>)</cite> proposed a communication-efficient federated learning method that uses a novel compression technique called adaptive federated quantization. The method adapts the compression ratio to the gradient sparsity of each client’s local model to achieve better compression performance.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">The FetchSGD algorithm proposed by Rothchild et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib54" title="" class="ltx_ref">Rothchild2020FetchSGDCF, </a>)</cite> uses a Count Sketch for compression and combines updates from multiple workers. It overcomes sparse client participation issues by moving momentum and error accumulation to the central aggregator, achieving high compression rates and convergence.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Hamer et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib52" title="" class="ltx_ref">pmlr-v119-hamer20a, </a>)</cite> proposed a communication-efficient federated learning method called FedBoost, which uses a boosting algorithm to select a subset of the client nodes to communicate with the server. This approach reduces the communication cost and achieves better model accuracy compared to traditional federated learning methods.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">Sai et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib61" title="" class="ltx_ref">SCAFFOLD, </a>)</cite> introduced SCAFFOLD, a federated learning algorithm designed for scenarios with heterogeneous client data. The algorithm employs control variates to mitigate the drift between clients’ local updates, leading to improved convergence and communication efficiency compared to Federated Averaging.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib62" title="" class="ltx_ref">Wang2018AdaptiveFL, </a>)</cite> propose an adaptive federated learning approach for resource-constrained edge computing systems, where data is distributed across multiple nodes without sending raw data to a centralized location. The approach uses a control algorithm to balance local updates and global parameter aggregation to minimize the loss function under a given resource budget. Extensive experiments with real datasets show that the proposed approach performs well with various machine-learning models and data distributions.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Security and Privacy in Federated Learning</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Bonawitz et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib10" title="" class="ltx_ref">Bonawitz2019, </a>)</cite> proposed FATE, a privacy-preserving federated learning framework that uses a trusted aggregator to compute the global model without revealing clients’ private data. The framework provides security and privacy features such as secure aggregation, differential privacy, and homomorphic encryption to protect client data during model training.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib67" title="" class="ltx_ref">Liu2021EnablingST, </a>)</cite> proposed an SQL-based training data debugging framework for federated learning, which automatically removes label errors from training data to fix unexpected model behavior. The authors address technical challenges to make the framework secure, efficient, and accurate and propose Frog, a novel framework that outperforms their previous solution. The effectiveness of Frog is validated through theoretical analysis and extensive experiments on real-world datasets.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">To ensure the privacy and security of federated learning, various techniques such as differential privacy, secure multi-party computation, and homomorphic encryption have been proposed. Differential privacy adds random noise to the data before sharing it with the server, while secure multi-party computation enables collaborative computation without revealing inputs. Homomorphic encryption allows computation on encrypted data, preserving data privacy. These techniques have been applied in federated learning to ensure client’s data privacy and security during model training <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">Yang2018, </a>; <a href="#bib.bib55" title="" class="ltx_ref">tddldp-19, </a>; <a href="#bib.bib11" title="" class="ltx_ref">Shokri2015, </a>; <a href="#bib.bib56" title="" class="ltx_ref">Mohassel2017SecureML, </a>)</cite>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Data Decoupling Techniques in Federated Learning</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Data decoupling techniques have been proposed to improve the flexibility and scalability of federated learning systems by separating the data management subsystem from the federated learning system. These techniques can reduce communication overhead and enable the system to scale to a larger number of clients. A variety of approaches have been proposed and their potential has been demonstrated in several studies <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">Yang2018, </a>; <a href="#bib.bib29" title="" class="ltx_ref">Yang2019, </a>; <a href="#bib.bib59" title="" class="ltx_ref">leaf, </a>; <a href="#bib.bib44" title="" class="ltx_ref">fl-non-idd-data, </a>; <a href="#bib.bib57" title="" class="ltx_ref">Hsu2019MeasuringTE, </a>; <a href="#bib.bib58" title="" class="ltx_ref">Hard2018FederatedLF, </a>)</cite>.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span><span id="S4.SS4.SSS1.1.1" class="ltx_text ltx_font_bold">Semi-supervised Learning Data Decoupling Technique in Federated Learning</span>
</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">Kairouz et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">kairouz2019advances, </a>)</cite> proposed a data decoupling technique that is based on semi-supervised learning for improving the performance of federated learning systems. While their approach demonstrated improved accuracy and reduced communication cost, it is limited to scenarios where a significant amount of unlabelled data is available at client nodes. This limitation may not be applicable in situations where the data is entirely labeled. Additionally, their results were obtained in a specific experimental setting, and the generalizability of their approach to other federated learning scenarios is uncertain.
Our proposed approach differs from Kairouz et al.’s <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">kairouz2019advances, </a>)</cite> semi-supervised learning data decoupling technique in that we propose a novel approach that utilizes label propagation to make use of all data available at each client node. Our approach is applicable in scenarios where data is entirely labeled or partially labeled, providing more flexibility in real-world applications. Additionally, our approach is designed to be more scalable by reducing the amount of data sent between the clients and the server.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span><span id="S4.SS4.SSS2.1.1" class="ltx_text ltx_font_bold">Local SGD Data Decoupling Approach in Federated Learning</span>
</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">Lian et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib41" title="" class="ltx_ref">Lian2017, </a>)</cite> proposed Local SGD as a data decoupling approach to optimize federated learning systems. Local SGD enables clients to perform local stochastic gradient descent while the server aggregates the updated models, thereby reducing communication costs. However, its limitations such as computational expense for clients with limited computational resources and the assumption of homogeneous models need further exploration in practical applications.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">Our proposed approach differs from Local SGD by using label propagation to leverage all data available at each client node, reducing communication costs by sending only propagated labels instead of the entire model, and not requiring clients to perform local SGD at each iteration. These differences increase the applicability of our approach in scenarios with limited computational resources or heterogeneous client models.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3. </span><span id="S4.SS4.SSS3.1.1" class="ltx_text ltx_font_bold">Distributed Database in Federated Learning</span>
</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib34" title="" class="ltx_ref">liu2020federated, </a>)</cite> proposed a federated learning approach that utilizes a distributed database, Hyperledger Fabric, to manage the clients’ data. Their approach enables the system to scale to a large number of clients by decoupling the data management subsystem from the federated learning system. Despite the experimental results demonstrating the effectiveness of the proposed approach in reducing communication costs and improving scalability, its applicability may be limited to distributed databases, and additional complexity and maintenance costs may be introduced.</p>
</div>
<div id="S4.SS4.SSS3.p2" class="ltx_para">
<p id="S4.SS4.SSS3.p2.1" class="ltx_p">Our proposed approach differs from Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib34" title="" class="ltx_ref">liu2020federated, </a>)</cite> distributed database approach by not relying on a distributed database to manage client data. Instead, our label propagation approach operates within the federated learning system, reducing the need for additional infrastructure. Additionally, our approach can improve scalability by minimizing communication costs by sending only propagated labels, rather than entire client data.</p>
</div>
<div id="S4.SS4.SSS3.p3" class="ltx_para">
<p id="S4.SS4.SSS3.p3.1" class="ltx_p">Federated learning (FL) has attracted significant attention in recent years due to its potential to address the challenges associated with centralized machine learning. Research in this field has focused on optimizing FL algorithms for improved efficiency, security, and scalability. Some studies <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib9" title="" class="ltx_ref">Konecny2016, </a>; <a href="#bib.bib29" title="" class="ltx_ref">Yang2019, </a>; <a href="#bib.bib30" title="" class="ltx_ref">Papernot2017, </a>; <a href="#bib.bib31" title="" class="ltx_ref">Shokri2017, </a>; <a href="#bib.bib42" title="" class="ltx_ref">fldp2019-Wei, </a>)</cite> have proposed privacy-preserving techniques, while others have explored communication-efficient approaches <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib32" title="" class="ltx_ref">wang2020, </a>; <a href="#bib.bib33" title="" class="ltx_ref">li2020, </a>)</cite>, such as the use of compression techniques and adaptive regularization. This body of work has demonstrated the potential of data decoupling techniques in FL and the various approaches and techniques that can be used to improve its efficiency, security, and scalability.</p>
</div>
<div id="S4.SS4.SSS3.p4" class="ltx_para">
<p id="S4.SS4.SSS3.p4.1" class="ltx_p">This study contributes to the research on data decoupling techniques for Federated Learning (FL) by presenting a comprehensive evaluation of FL systems with the aid of database middleware. By leveraging the capabilities of the database middleware, this research provides a flexible and scalable solution to FL in distributed environments, with implications for FL system design in practical scenarios. The evaluation, which was conducted on a real-world dataset, assesses the impact of data decoupling on various performance metrics such as communication cost, training time, scalability, and accuracy, without using the data partition and data federation techniques.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we presented a data decoupling framework called DDFL for Federated Learning (FL) systems that enable clients to customize their applications with specific data subsystems and make effective queries over the models.
DDFL serves not only as a common and fair test bed to compare different database subsystems for FL but also as a core infrastructure to develop a new FL ecosystem with arbitrary data management services.
We implemented DDFL and integrated it with various types of data management, namely MongoDB, SCP, Cassandra, RabbitMQ, Neo4j, and Postgres.
Our extensive experiments conducted on CIFAR-10, Fashion-MNIST, and SVHN datasets show that all of those database systems achieve high accuracy comparable with vanilla FL systems (i.e., correctness) and yet exhibit discrepancies in other metrics, such as training time and communication overhead.
Such discrepancies are largely application-dependent,
which remain open questions.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In a more general sense, our results indicate that different database systems can have,
quantitatively,
a significant impact on the overall performance of FL systems,
which shed fundamentally new insights into the strengths and weaknesses of existing methods.
The DDFL framework developed by this work serves as a new way to evaluate existing methods.
It is our hope that this work will spark a new line of system research on database services in FL ecosystems.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Duplyakin, D., Ricci, R., Maricq, A., Wong, G., Duerig, J., Eide, E., Stoller, L., Hibler, M., Johnson, D., Webb, K., Akella, A., Wang, K., Ricart, G., Landweber, L., Elliott, C., Zink, M., Cecchet, E., Kar, S., &amp; Mishra, P. (2019). The Design and Operation of CloudLab. In Proceedings of the USENIX Annual Technical Conference (ATC) (pp. 1-14). https://www.flux.utah.edu/paper/duplyakin-atc19

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Nishio, T. and Yonetani, R. (2018). Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge. CoRR, abs/1804.08333. Available at: http://arxiv.org/abs/1804.08333 [Accessed 14 Mar. 2023].

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., … (2021). Advances and Open Problems in Federated Learning. Foundations and Trends® in Machine Learning, 14(1-2), 1-210.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
Wang, L. and Zhao, D. (2022). Practical Federated Learning Infrastructure for Privacy-Preserving Scientific Computing. In Proceedings of the 3rd Workshop on Artificial Intelligence and Machine Learning for Scientific Applications (AI4S), in conjunction with International Conference on High Performance Computing, Networking, Storage and Analysis (SC).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
PySyft.
<span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">https://github.com/OpenMined/PySyft</span>, Accessed 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
FedML. <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">https://fedml.ai/</em>. Accessed 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
TensorFlow Federated,
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">https://github.com/tensorflow/federated</em>,
Accessed 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. (2017).

</span>
<span class="ltx_bibblock">Communication-Efficient Learning of Deep Networks from Decentralized Data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTAT)</span>, arti Singh and Xiaojin (Jerry) Zhu (Eds.), <span id="bib.bib8.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, volume 54, pages 1273–1282, PMLR.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
J. Konecny, H. B. McMahan, D. Ramage, and N. A. Smith, “Federated learning: Strategies for improving communication efficiency,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.05492</span>, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
Bonawitz, K., McMahan, H. B., McNamara, E. N. P., Recht, B. A., &amp; Woodworth, A. D. K. (2019). Towards Federated Learning at Scale: System Design. <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.01046</span>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2015).
Privacy-preserving deep learning. <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1510.06772</span>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
B. Sahu, K. K. P. Singh, and A. Ghosh. ”Decentralized Federated Learning: A Survey.” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.04820</span> (2018).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer. PyTorch. Version 1.10.0. 2021-10-01. <a target="_blank" href="https://pytorch.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pytorch.org/</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
A. Krizhevsky, “Learning multiple layers of features from tiny images,” Technical Report, 2009.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
NumPy Developers. (2021). NumPy (Version 1.21.2) [Software]. Available at <a target="_blank" href="https://numpy.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://numpy.org/</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
Dwight Merriman, Eliot Horowitz and Kevin Ryan. MongoDB: The database for modern applications. MongoDB Inc., n.d. URL: <a target="_blank" href="https://www.mongodb.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mongodb.com/</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
Chris Duncan. <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">RabbitMQ: A messaging broker</em>, Pivotal Software, Inc., n.d. Available online: <a target="_blank" href="https://www.rabbitmq.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.rabbitmq.com/</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
The Apache Software Foundation, “Apache Cassandra,” 2010. [Online]. Available: <a target="_blank" href="https://cassandra.apache.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cassandra.apache.org/</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
David M. Beazley.

</span>
<span class="ltx_bibblock">PyCrypto: The Python Cryptography Toolkit.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/dlitz/pycrypto" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/dlitz/pycrypto</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(20)</span>
<span class="ltx_bibblock">
Netzer, Y. and Wang, T. and Coates, A. and Bissacco, A. and Wu, B. and Ng, A.Y.,
<span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Reading Digits in Natural Images with Unsupervised Feature Learning</span>,
University of California, Berkeley, 2011.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
Xiao, H., Rasul, K., &amp; Vollgraf, R. (2017). Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms. <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1708.07747</span>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(22)</span>
<span class="ltx_bibblock">
Garfinkel, S., &amp; Spafford, G. (2002). Practical UNIX and Internet Security. O’Reilly Media, Inc.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock"> Yang, Q., Liu, Y., Chen, T., &amp; Tong, Y. (2018). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology, 10(2), 12. doi: 10.1145/3196321

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock"> Neo4j, Inc. Neo4j. <a target="_blank" href="https://neo4j.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://neo4j.com/</a>, 2023. Accessed: 2023-02-18.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(25)</span>
<span class="ltx_bibblock">
PostgreSQL Global Development Group. (2021). <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">PostgreSQL</span> (Version 13.4). Retrieved from <a target="_blank" href="https://www.postgresql.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.postgresql.org/</a>. Accessed: 2023-02-18.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Ag”uera y Arcas, Blaise.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 2738–2747, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated Learning: Challenges, Methods, and Future Directions. <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1908.07873, 2019. <a target="_blank" href="http://arxiv.org/abs/1908.07873" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1908.07873</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
Pouyanfar, S., Ibrahim, M., Al-Jubouri, H., &amp; Meskin, N. (2019). A survey on federated learning: From theory to practice. <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 7, 91918-91942.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, ”Federated Machine Learning: Concept and Applications,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology</em>, vol. 10, no. 2, pp. 12:1–12:19, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(30)</span>
<span class="ltx_bibblock">
Papernot, N., Abadi, M., Erlingsson, ’U., Goodfellow, I., &amp; Talwar, K. (2017). Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data. In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</span> (pp. 1472–1485).

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(31)</span>
<span class="ltx_bibblock">
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership Inference Attacks against Machine Learning Models. In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on Security and Privacy (SP)</em>, pages 3–18, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
Wang, Haoyu and Shi, Wenchang and Zou, Yulong and Yang, Zhaohui and Sun, Lifeng. (2020). Adaptive Federated Learning in Resource-Constrained Edge Computing Systems. <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Cognitive Communications and Networking</span>, 6(2), 608–618.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(33)</span>
<span class="ltx_bibblock">
Haibo Li, Shuhang Gu, Zehao Xiao, Shiqiang Wang, and Baochun Li. ”Communication-Efficient Edge Learning with Adaptive Parameter Transmission.” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em> 31, no. 1 (2020): 145-156.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
Liu, X., Liu, Z., Wang, C., Zhang, J., &amp; Tian, J. (2020). Federated learning on blockchain: A secure approach for IoT applications. <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 8(5), 3632-3642.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(35)</span>
<span class="ltx_bibblock">
Howard, J., &amp; Gugger, S. (2018). Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(36)</span>
<span class="ltx_bibblock">
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.01274</span>, 2015.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(37)</span>
<span class="ltx_bibblock">
Gao, Y., Zhong, Y., &amp; Yang, Q. (2021). AutoML: A survey of the state-of-the-art. <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, 212, 106621. Elsevier.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(38)</span>
<span class="ltx_bibblock">
Ji, S. (2018). A PyTorch Implementation of Federated Learning. Zenodo. doi: 10.5281/zenodo.4321561. Retrieved from https://doi.org/10.5281/zenodo.4321561

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(39)</span>
<span class="ltx_bibblock">
Menezes, A. J., Vanstone, S. A., &amp; Oorschot, P. C. V. (1996). <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Handbook of Applied Cryptography</em> (1st ed.). CRC Press, Inc. USA. ISBN 0849385237.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(40)</span>
<span class="ltx_bibblock">
The Python Cryptography Developers. (2021). <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Cryptography</em> (version 3.4.7). Retrieved from https://cryptography.io/

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(41)</span>
<span class="ltx_bibblock">
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., &amp; Liu, J. (2017). Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent. <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1705.08565.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(42)</span>
<span class="ltx_bibblock">
Wei, K., Li, J., Ding, M., Ma, C., Yang, H. H., Farhad, F., Jin, S., Quek, T. Q. S., &amp; Poor, H. V. (2019). Federated Learning with Differential Privacy: Algorithms and Performance Analysis. <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00222</em>. Retrieved from https://arxiv.org/abs/1911.00222

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. ”Agnostic Federated Learning.” <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1902.00146 (2019). arXiv:1902.00146. <a target="_blank" href="http://arxiv.org/abs/1902.00146" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1902.00146</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(44)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated Learning with Non-IID Data. <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1806.00582, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(45)</span>
<span class="ltx_bibblock">
X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, ”On the Convergence of FedAvg on Non-IID Data,” <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.02189</span>, 2019. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/1907.02189" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1907.02189</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(46)</span>
<span class="ltx_bibblock">
National Institute of Standards and Technology. (2001). Federal Information Processing Standards Publication (FIPS) 197: Advanced Encryption Standard (AES). Retrieved from <a target="_blank" href="https://csrc.nist.gov/publications/detail/fips/197/final" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://csrc.nist.gov/publications/detail/fips/197/final</a> (Accessed on 2023-02-18).

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(47)</span>
<span class="ltx_bibblock">
S. Frankel, R. Glenn, S. Kelly, and M. Kohno, “The AES-CBC encryption algorithm and its use with IPsec,” Network Working Group, 2003. Available: <a target="_blank" href="https://tools.ietf.org/html/rfc3602" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://tools.ietf.org/html/rfc3602</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(48)</span>
<span class="ltx_bibblock">
B. Kaliski, “PKCS 7: Cryptographic Message Syntax Standard,” RSA Laboratories, 1998. Available: <a target="_blank" href="https://tools.ietf.org/html/rfc2315" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://tools.ietf.org/html/rfc2315</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(49)</span>
<span class="ltx_bibblock">
Xu, Guowen and Han, Xingshuo and Xu, Shengmin and Zhang, Tianwei and Li, Hongwei and Huang, Xinyi and Deng, Robert H. ”Hercules: Boosting the Performance of Privacy-preserving Federated Learning.” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Dependable and Secure Computing</em>, vol. -, no. -, pp. 1-18, 2022, doi: 10.1109/TDSC.2022.3218793.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(50)</span>
<span class="ltx_bibblock">
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. ”Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data.” ArXiv, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(51)</span>
<span class="ltx_bibblock">
Xinwei Zhang, Wotao Yin, Mingyi Hong, and Tianyi Chen. ”Hybrid Federated Learning: Algorithms and Implementation.” ArXiv, 2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(52)</span>
<span class="ltx_bibblock"> Jenny Hamer, Mehryar Mohri, and Ananda Theertha Suresh. FedBoost: A Communication-Efficient Algorithm for Federated Learning. In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine Learning</em>, volume 119 of <em id="bib.bib52.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 3973–3983, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(53)</span>
<span class="ltx_bibblock"> Divyansh Jhunjhunwala, Advait Gadhikar, Gauri Joshi, and Yonina C. Eldar. Adaptive Quantization of Model Updates for Communication-Efficient Federated Learning. In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 3110-3114, 2021.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(54)</span>
<span class="ltx_bibblock"> Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman, Joseph E. Gonzalez, and R. Arora. FetchSGD: Communication-Efficient Federated Learning with Sketching. In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2020.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(55)</span>
<span class="ltx_bibblock">
Cheng, Hsin-Pai, Patrick Yu, Haojing Hu, Syed Zawad, Feng Yan, Shiyu Li, and Yiran Chen. ”Towards Decentralized Deep Learning with Differential Privacy.” In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Security and Privacy in Communication Networks</span>, pp. 130-145. Springer, Cham, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(56)</span>
<span class="ltx_bibblock">
Mohassel, Payman, and Yupeng Zhang. ”SecureML: A System for Scalable Privacy-Preserving Machine Learning.” In <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">2017 IEEE Symposium on Security and Privacy (SP)</span>, pp. 19-38. IEEE, 2017.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(57)</span>
<span class="ltx_bibblock">
Hsu, Tzu-Ming Harry, Qi, and Matthew Brown. ”Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification.” <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">arXiv preprint</span> arXiv:1909.06335 (2019).

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(58)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Françoise Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage. Federated Learning for Mobile Keyboard Prediction. <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, 2018. <a target="_blank" href="https://arxiv.org/abs/1811.03604" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1811.03604</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(59)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný, H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A Benchmark for Federated Settings. <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv</span>, 2018. <a target="_blank" href="https://arxiv.org/abs/1812.01097" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1812.01097</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(60)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical Secure Aggregation for Privacy-Preserving Machine Learning. In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS ’17)</span>, pages 1175-1191, 2017. <a target="_blank" href="https://doi.org/10.1145/3133956.3133982" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3133956.3133982</a>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(61)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh.
“SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning,”
<em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, 2019, abs/1910.06378.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(62)</span>
<span class="ltx_bibblock">
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin Kwong Leung, Christian Makaya, Ting He, and Kevin Sean Chan.
“Adaptive Federated Learning in Resource Constrained Edge Computing Systems,”
<em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>, 2018, vol. 37, pp. 1205-1221.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(63)</span>
<span class="ltx_bibblock">
Yaliang Li, Bolin Ding, and Jingren Zhou.
“A Practical Introduction to Federated Learning,”
<em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 2022, pp. 4802-4803.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(64)</span>
<span class="ltx_bibblock">
Akash Bharadwaj and Graham Cormode.

</span>
<span class="ltx_bibblock">An Introduction to Federated Computation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2022 International Conference on Management of Data (SIGMOD ’22)</span>, pages 2448–2451, New York, NY, USA, 2022. Association for Computing Machinery. 
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3514221.3522561" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3514221.3522561</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(65)</span>
<span class="ltx_bibblock">
Jakub Konecn’y and H. B. McMahan and Daniel Ramage and Peter Richt’arik.

</span>
<span class="ltx_bibblock">Federated Optimization: Distributed Machine Learning for On-Device Intelligence.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/1610.02527, 2016.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(66)</span>
<span class="ltx_bibblock">
Gang Bao and Peng Guo.

</span>
<span class="ltx_bibblock">Federated Learning in Cloud-Edge Collaborative Architecture: Key Technologies, Applications and Challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Journal of Cloud Computing</span>, 11(1):94, 2022. Springer Science and Business Media LLC. 
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1186/s13677-022-00377-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1186/s13677-022-00377-4</a>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(67)</span>
<span class="ltx_bibblock">
Yejia Liu, Weiyuan Wu, Lampros Flokas, Jiannan Wang, and Eugene Wu.

</span>
<span class="ltx_bibblock">Enabling SQL-based Training Data Debugging for Federated Learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">Proc. VLDB Endow.</span>, 15:388–400, 2021.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(68)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, Sebastian U. Stich, and Martin Jaggi.

</span>
<span class="ltx_bibblock">Ensemble Distillation for Robust Model Fusion in Federated Learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">Proceedings of the 34th International Conference on Neural Information Processing Systems</span>, pages 198–210, Vancouver, BC, Canada, 2020.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(69)</span>
<span class="ltx_bibblock">
Maruan Al-Shedivat, Jennifer Gillenwater, Eric P. Xing, and Afshin Rostamizadeh.

</span>
<span class="ltx_bibblock">Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2010.05273, 2020.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(70)</span>
<span class="ltx_bibblock">
Binhang Yuan, Cameron R. Wolfe, Chen Dun, Yuxin Tang, Anastasios Kyrillidis, and Chris Jermaine.

</span>
<span class="ltx_bibblock">Distributed Learning of Fully Connected Neural Networks Using Independent Subnet Training.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">Proc. VLDB Endow.</span>, 15(8):1581–1590, April 2022.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(71)</span>
<span class="ltx_bibblock">
Mohd Abu, Nurul Amelina Hasan Zahri, Asyraf Amir, Mohd Ikhwan Ismail, Azhany Yaakub, Syaheerah Ahmad Anwar, and Mohd Izhar Ahmad.

</span>
<span class="ltx_bibblock">A Comprehensive Performance Analysis of Transfer Learning Optimization in Visual Field Defect Classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Diagnostics</span>, 12(5):1258, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.08370" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.08371" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.08371">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.08371" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.08372" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 20:07:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
