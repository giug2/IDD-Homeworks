<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.17541] Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.17541">

<!--Generated on Fri Jul  5 19:36:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\svgsetup</span>
<p id="p1.2" class="ltx_p">inkscapelatex=false

 









</p>
</div>
<h1 class="ltx_title ltx_title_document">Principal Component Clustering for Semantic
Segmentation in Synthetic Data Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felix Stillger
<br class="ltx_break">Bergische Universität Wuppertal, Aptiv
<br class="ltx_break">Wuppertal, Germany
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">felix.stillger@uni-wuppertal.de</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frederik Hasecke
<br class="ltx_break">Aptiv
<br class="ltx_break">Wuppertal, Germany
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">frederik.hasecke@aptiv.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tobias Meisen
<br class="ltx_break">Bergische Universität Wuppertal
<br class="ltx_break">Wuppertal, Germany
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">meisen@uni-wuppertal.de</span>
</span></span>
</div>

<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">This technical report outlines our method for generating a synthetic dataset for semantic segmentation using a latent diffusion model, as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. Our approach eliminates the need for additional models specifically trained on segmentation data and is part of our submission to the CVPR 2024 workshop challenge, entitled <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">CVPR</span>
2024 workshop challenge <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">”SyntaGen - Harnessing Generative Models for
Synthetic Visual Datasets”</span>. In the following, we describe the development of our pipeline and the training of a <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">DeepLabv3</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> model on the generated dataset.
Our methodology is inspired by the attention interpretation techniques used in <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">Dataset Diffusion</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>. To implement the
self-attention gathering, we leverage the <span id="S1.p1.1.5" class="ltx_text ltx_font_italic">DAAM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> block. These self-attentions facilitate a novel head-wise semantic information condensation, thereby enabling the direct acquisition of class-agnostic image segmentation from the <span id="S1.p1.1.6" class="ltx_text ltx_font_italic">Stable Diffusion latents</span>.
Furthermore, we employ <span id="S1.p1.1.7" class="ltx_text ltx_font_italic">OVAM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> for
non-prompt-influencing cross-attentions from text to pixel, thus facilitating the classification of the previously generated masks. Finally, we propose a mask refinement step by using only the output image by <span id="S1.p1.1.8" class="ltx_text ltx_font_italic">Stable Diffusion</span>.
Our code is available on <span id="S1.p1.1.9" class="ltx_text ltx_font_italic">GitHub<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnotex1.1.1.1" class="ltx_text ltx_font_upright">1</span></span><a target="_blank" href="https://github.com/felixstillger/Syntagen_Submission_PCC_Segmentation" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://github.com/felixstillger/Syntagen_Submission_PCC_Segmentation</a></span></span></span></span>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our methodology employs <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">Stable Diffusion 2.1</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> as the foundational model, leveraging publicly available text prompts from <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">Dataset Diffusion</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, which are originally derived from the <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">Pascal VOC</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> training dataset. Unlike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite> and similar approaches that typically average and sum self-attentions across all heads and timesteps to manage these large tensors, our approach considers each head’s features independently in the final iteration step. This allows us to generate masks that isolate the semantic content of the output image. The overall pipeline is illustrated in <span id="S2.p1.1.4" class="ltx_text ltx_font_smallcaps">Figure</span> <a href="#S2.F1" title="Figure 1 ‣ 2 Method ‣ Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, with our contribution highlighted in the red dotted area.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<p id="S2.F1.1" class="ltx_p ltx_align_center"><span id="S2.F1.1.1" class="ltx_text"><object type="image/svg+xml" data="/html/2406.17541/assets/images/diagram.svg" id="S2.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="314" height="189"></object></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.5.2" class="ltx_text" style="font-size:90%;">Diagram of the Pipeline (Figure Adapted from <span id="S2.F1.5.2.1" class="ltx_text ltx_font_italic">OVAM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>)</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Process Self-Attentions</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.2" class="ltx_p">Each head in the denoising network targets distinct features and objectives. We hypothesize that the attention mechanism, being critical for image reconstruction, conveys semantic information. Hence, our objective is to extract semantic meanings that are interpretable by humans and correspond specifically to the <span id="S2.SS1.p1.2.1" class="ltx_text ltx_font_italic">Pascal VOC</span> classes.
To achieve this, we employ <span id="S2.SS1.p1.2.2" class="ltx_text ltx_font_italic">Principal Component Analysis</span> (<span id="S2.SS1.p1.2.3" class="ltx_text ltx_font_italic">PCA</span>) for each head, reducing
the feature dimensionality from 64 to 3. This procedure is repeated at each upsampling layer, from a resolution of <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">16</cn><cn type="integer" id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">16\times 16</annotation></semantics></math> up to <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mn id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><times id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">64</cn><cn type="integer" id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">64\times 64</annotation></semantics></math>. By performing such head-specific PCA, we condense the original features, facilitating the separation of the most distinct objectives for each head.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Clustering and Classification</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">After computing the principal components for each head, we upsample all smaller outputs to a resolution of <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><times id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">64</cn><cn type="integer" id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">64\times 64</annotation></semantics></math>. Subsequently, we concatenate the principal components of each head for every individual pixel to form a single tensor. To enhance robustness, we normalize the features and include the relative pixel positions as additional features. As, according to the challenge rules, the use of annotated masks is prohibited, we employ unsupervised <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">K-Means clustering</span> multiple times with varying numbers of clusters. In doing so, this approach generates clusters by minimizing the squared Euclidean distance of the head-wise principal components. To ensure reproducibility, we fix the random initialization of clusters.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Determining the optimal number of clusters is complex as it depends on multiple factors, which cannot be exhaustively defined and extend beyond class, scene, and semantic meaning. To address this and maintain flexibility, we cluster using [<math id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="4,7,10" display="inline"><semantics id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.1.cmml"><mn id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">4</mn><mo id="S2.SS2.p2.1.m1.3.4.2.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">,</mo><mn id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">7</mn><mo id="S2.SS2.p2.1.m1.3.4.2.2" xref="S2.SS2.p2.1.m1.3.4.1.cmml">,</mo><mn id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><list id="S2.SS2.p2.1.m1.3.4.1.cmml" xref="S2.SS2.p2.1.m1.3.4.2"><cn type="integer" id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">4</cn><cn type="integer" id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">7</cn><cn type="integer" id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">10</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">4,7,10</annotation></semantics></math>] clusters, aiming for a rough segmentation of main objects and a finer segmentation of smaller parts. We separate clusters that are not directly connected via 4-connected neighborhood pixels.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">For class assignment to the masks, we utilize cross-attention maps from <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">OVAM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, enabling class identification independent of the input prompt. We avoid optimized tokens to keep the approach generalized and free from any segmentation labels. Additionally, we observed that the <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">start-of-text</span> (<span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_italic">SoT</span>) token can serve as an indicator for the background class. Furthermore, we replace certain class names with more descriptive token names (see <span id="S2.SS2.p3.1.4" class="ltx_text ltx_font_smallcaps">Table</span> <a href="#S2.T1" title="Table 1 ‣ 2.2 Clustering and Classification ‣ 2 Method ‣ Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Renaming Class Names to More Expressive Tokens.</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<th id="S2.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S2.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Pascal VOC Name</span></th>
<th id="S2.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">Renamed for OVAM</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.4.2.1" class="ltx_tr">
<td id="S2.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">diningtable</td>
<td id="S2.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">table</td>
</tr>
<tr id="S2.T1.4.3.2" class="ltx_tr">
<td id="S2.T1.4.3.2.1" class="ltx_td ltx_align_center ltx_border_r">tvmonitor</td>
<td id="S2.T1.4.3.2.2" class="ltx_td ltx_align_center">monitor</td>
</tr>
<tr id="S2.T1.4.4.3" class="ltx_tr">
<td id="S2.T1.4.4.3.1" class="ltx_td ltx_align_center ltx_border_r">pottedplant</td>
<td id="S2.T1.4.4.3.2" class="ltx_td ltx_align_center">pot plant</td>
</tr>
<tr id="S2.T1.4.5.4" class="ltx_tr">
<td id="S2.T1.4.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">aeroplane</td>
<td id="S2.T1.4.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">airplane</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">We iterate over all clusters and apply varying confidence thresholds to the class-wise cross-attention maps from <span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_italic">OVAM</span>. Initially, we normalize all original attention values to the range [0, 1], followed by multiple binary thresholding at 0.3, 0.5, and 0.8. For the background class, derived from the <span id="S2.SS2.p4.1.2" class="ltx_text ltx_font_italic">SoT</span> token, we increase the confidence threshold by 20% of the current threshold to mitigate the excessive influence of the background. For the final classification, we compute the Intersection over Union (<span id="S2.SS2.p4.1.3" class="ltx_text ltx_font_italic">IoU</span>) for each cluster against the class-wise binary map, assigning the class with the highest <span id="S2.SS2.p4.1.4" class="ltx_text ltx_font_italic">IoU</span> to all pixels within that cluster for the defined thresholds. This process is repeated across all clusters and thresholds, with the most frequent class for each pixel determined by the mode. If the dominant class for a pixel constitutes 50% or less, the pixel is labeled as uncertain. This procedure yields a <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mrow id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml"><mn id="S2.SS2.p4.1.m1.1.1.2" xref="S2.SS2.p4.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p4.1.m1.1.1.1" xref="S2.SS2.p4.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS2.p4.1.m1.1.1.3" xref="S2.SS2.p4.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><apply id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1"><times id="S2.SS2.p4.1.m1.1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1.1"></times><cn type="integer" id="S2.SS2.p4.1.m1.1.1.2.cmml" xref="S2.SS2.p4.1.m1.1.1.2">64</cn><cn type="integer" id="S2.SS2.p4.1.m1.1.1.3.cmml" xref="S2.SS2.p4.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">64\times 64</annotation></semantics></math> resolution mask. To enhance this low-resolution mask to the image resolution, we utilize the output image knowledge from <span id="S2.SS2.p4.1.5" class="ltx_text ltx_font_italic">Stable Diffusion</span>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Mask Refinement</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">All previous operations were exclusively applied to the extracted latents of Stable Diffusion. For the final mask refinement, we utilize the RGB values of the output image and reintroduce the pixel positions. We employ <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">K-Means clustering</span> with a fixed cluster count of 20 and separate disconnected clusters. This approach allows us to identify regions that are coherent in terms of color and proximity, irrespective of the image’s semantic content. A class is assigned to these new clusters if the dominant class constitutes more than 66% of the pixels within the cluster, ensuring accurate classification with high confidence. Due to the large and fixed cluster count, the <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">K-Means clustering</span> algorithm may generate artifacts in certain segmentation masks. These artifacts are filled with the uncertainty class, and such pixels are excluded from further influence during training.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To evaluate the effectiveness of our dataset generation method, we produced just under <math id="S3.p1.1.m1.2" class="ltx_Math" alttext="10,\!000" display="inline"><semantics id="S3.p1.1.m1.2a"><mrow id="S3.p1.1.m1.2.3.2" xref="S3.p1.1.m1.2.3.1.cmml"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">10</mn><mpadded width="0.275em"><mo id="S3.p1.1.m1.2.3.2.1" xref="S3.p1.1.m1.2.3.1.cmml">,</mo></mpadded><mn id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.2b"><list id="S3.p1.1.m1.2.3.1.cmml" xref="S3.p1.1.m1.2.3.2"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">10</cn><cn type="integer" id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.2c">10,\!000</annotation></semantics></math> images for our final submission to the challenge, constrained by time limitations. To assess the quality of the synthetic dataset, we exclusively trained a <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">DeepLabv3</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> on the generated images. We identified the best-performing checkpoint by calculating the <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">mean Intersection over Union</span> (<span id="S3.p1.1.3" class="ltx_text ltx_font_italic">mIoU</span>) on the <span id="S3.p1.1.4" class="ltx_text ltx_font_italic">Pascal VOC</span> validation set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> at every 1000 training iterations. The checkpoint exhibiting the highest <span id="S3.p1.1.5" class="ltx_text ltx_font_italic">mIoU</span> was submitted to the challenge organizers, accompanied by its checksum.</p>
</div>
<figure id="S3.T2" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.5.2" class="ltx_text" style="font-size:90%;">Submission Results</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T2.1" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T2.1.2.1.1.1" class="ltx_text ltx_font_bold">Submission</span></th>
<th id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">mIoU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S3.T2.1.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S3.T2.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\dagger</annotation></semantics></math> Teddy Bear (HCMUS)</td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_t">51.66</td>
</tr>
<tr id="S3.T2.1.3.1" class="ltx_tr">
<td id="S3.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_border_r">HNU-VPAI</td>
<td id="S3.T2.1.3.1.2" class="ltx_td ltx_align_center">47.36</td>
</tr>
<tr id="S3.T2.1.4.2" class="ltx_tr">
<td id="S3.T2.1.4.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.1.4.2.1.1" class="ltx_text ltx_font_bold">Hot Coffee (Ours)</span></td>
<td id="S3.T2.1.4.2.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.4.2.2.1" class="ltx_text ltx_font_bold">46.25</span></td>
</tr>
<tr id="S3.T2.1.5.3" class="ltx_tr">
<td id="S3.T2.1.5.3.1" class="ltx_td ltx_align_center ltx_border_r">CVTEAMQ</td>
<td id="S3.T2.1.5.3.2" class="ltx_td ltx_align_center">45.88</td>
</tr>
<tr id="S3.T2.1.6.4" class="ltx_tr">
<td id="S3.T2.1.6.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Dataset Diffusion (Baseline)</td>
<td id="S3.T2.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">46.85</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.T2.2.1" class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:149.1pt;"><math id="S3.T2.2.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S3.T2.2.1.m1.1a"><mo mathsize="80%" id="S3.T2.2.1.m1.1.1" xref="S3.T2.2.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.1.m1.1b"><ci id="S3.T2.2.1.m1.1.1.cmml" xref="S3.T2.2.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.1.m1.1c">\dagger</annotation></semantics></math><span id="S3.T2.2.1.1" class="ltx_text" style="font-size:80%;"> The entry was disqualified due to accidental use of segmentation labels in a module. The listed score is the postmortem correction.</span></p>
</div>
<div class="ltx_flex_break"></div>
</div>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In <span id="S3.p2.1.1" class="ltx_text ltx_font_smallcaps">Table</span> <a href="#S3.T2" title="Table 2 ‣ 3 Results ‣ Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> the results on the private test set are presented. Our submitted model performed slightly below the baseline established by Nguyen et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, with a difference of <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="0.4" display="inline"><semantics id="S3.p2.1.m1.1a"><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><cn type="float" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">0.4</annotation></semantics></math> <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">mIoU</span>. Some illustrative examples are provided in the appendix in <span id="S3.p2.1.3" class="ltx_text ltx_font_smallcaps">Figure</span> <a href="#Sx1.F2" title="Figure 2 ‣ Appendix ‣ Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Further, a comprehensive analysis of the performance of a <span id="S3.p2.1.4" class="ltx_text ltx_font_italic">DeepLabv3</span> model trained on our dataset is detailed in <span id="S3.p2.1.5" class="ltx_text ltx_font_smallcaps">Table</span> <a href="#Sx1.T3" title="Table 3 ‣ Appendix ‣ Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> in the appendix, encompassing both the <span id="S3.p2.1.6" class="ltx_text ltx_font_italic">private Syntagen</span> and <span id="S3.p2.1.7" class="ltx_text ltx_font_italic">Pascal VOC</span> validation set.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Certain classes in the dataset exhibit varying levels of difficulty. Our method encountered significant challenges with the <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">person</span>
class due to its close embedding with other classes, which resulted in a lower <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">IoU</span>. Structurally challenging classes, such as sofa and <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">chair</span>, also presented difficulties.
In <span id="S4.p1.1.4" class="ltx_text ltx_font_italic">Pascal VOC</span>, the primary distinction between these two classes is that sofas are typically two-seaters, while chairs are usually
one-seaters. This subtle difference seems to be challenging for the model to accurately discern.
These challenges underscore the need for enhanced prompt engineering to improve performance.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Overall, we have developed a novel approach for semantically segmenting images generated by Stable Diffusion by leveraging only their latent representations. Our method is particularly effective for classes such as <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">potted plant, bird, dog, cat, sheep, boat</span>, producing highly precise masks (see <span id="S4.p2.1.2" class="ltx_text ltx_font_smallcaps">Figure</span> <a href="#Sx1.F2" title="Figure 2 ‣ Appendix ‣ Principal Component Clustering for Semantic Segmentation in Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> in the appendix). This precision is achieved by utilizing head-wise features from both the latent and overall image spaces.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Rethinking atrous convolution for semantic image segmentation, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.4.4.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Marcos-Manchón et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Pablo Marcos-Manchón, Roberto Alcover-Couso, Juan C. SanMiguel, and Jose M. Martínez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Open-vocabulary attention maps with token optimization for semantic segmentation in diffusion models, 2024.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Nguyen et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Quang Nguyen, Truong Vu, Anh Tran, and Khoi Nguyen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Dataset diffusion: Diffusion-based synthetic dataset generation for pixel-level semantic segmentation, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Rombach et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Tang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">What the daam: Interpreting stable diffusion using cross attention, 2022.
</span>
</span>
</li>
</ul>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix</h2>

<figure id="Sx1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_9368.jpg" id="Sx1.F2.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_17_img.jpg" id="Sx1.F2.2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_9377.jpg" id="Sx1.F2.3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_21_img.jpg" id="Sx1.F2.4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_91.jpg" id="Sx1.F2.5.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_9368.png" id="Sx1.F2.6.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_17_mask.png" id="Sx1.F2.7.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_9377.png" id="Sx1.F2.8.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_21_mask.png" id="Sx1.F2.9.g9" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_91.png" id="Sx1.F2.10.g10" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_29.jpg" id="Sx1.F2.11.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_48.jpg" id="Sx1.F2.12.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_149.jpg" id="Sx1.F2.13.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_9387.jpg" id="Sx1.F2.14.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_9411.jpg" id="Sx1.F2.15.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_29.png" id="Sx1.F2.16.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_48.png" id="Sx1.F2.17.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_149.png" id="Sx1.F2.18.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_9387.png" id="Sx1.F2.19.g9" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2406.17541/assets/images/multiple_examples/img_9411.png" id="Sx1.F2.20.g10" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="63" height="63" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx1.F2.22.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="Sx1.F2.23.2" class="ltx_text" style="font-size:90%;">Examples from our Submitted Dataset</span></figcaption>
</figure>
<figure id="Sx1.T3" class="ltx_table">
<table id="Sx1.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx1.T3.2.1.1" class="ltx_tr">
<th id="Sx1.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3"><span id="Sx1.T3.2.1.1.1.1" class="ltx_text ltx_font_bold">Syntagen Private Test Set</span></th>
<th id="Sx1.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3"><span id="Sx1.T3.2.1.1.2.1" class="ltx_text ltx_font_bold">Pascal VOC Validation Set</span></th>
</tr>
<tr id="Sx1.T3.2.2.2" class="ltx_tr">
<th id="Sx1.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="Sx1.T3.2.2.2.1.1" class="ltx_text ltx_font_bold">Class</span></th>
<th id="Sx1.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="Sx1.T3.2.2.2.2.1" class="ltx_text ltx_font_bold">IoU</span></th>
<th id="Sx1.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt"><span id="Sx1.T3.2.2.2.3.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="Sx1.T3.2.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="Sx1.T3.2.2.2.4.1" class="ltx_text ltx_font_bold">Class</span></th>
<th id="Sx1.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="Sx1.T3.2.2.2.5.1" class="ltx_text ltx_font_bold">IoU</span></th>
<th id="Sx1.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="Sx1.T3.2.2.2.6.1" class="ltx_text ltx_font_bold">Acc</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx1.T3.2.3.1" class="ltx_tr">
<td id="Sx1.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">background</td>
<td id="Sx1.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.86</td>
<td id="Sx1.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">90.44</td>
<td id="Sx1.T3.2.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">background</td>
<td id="Sx1.T3.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.29</td>
<td id="Sx1.T3.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">93.97</td>
</tr>
<tr id="Sx1.T3.2.4.2" class="ltx_tr">
<td id="Sx1.T3.2.4.2.1" class="ltx_td ltx_align_left ltx_border_r">aeroplane</td>
<td id="Sx1.T3.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">58.20</td>
<td id="Sx1.T3.2.4.2.3" class="ltx_td ltx_align_center ltx_border_rr">75.06</td>
<td id="Sx1.T3.2.4.2.4" class="ltx_td ltx_align_left ltx_border_r">aeroplane</td>
<td id="Sx1.T3.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r">77.37</td>
<td id="Sx1.T3.2.4.2.6" class="ltx_td ltx_align_center">92.89</td>
</tr>
<tr id="Sx1.T3.2.5.3" class="ltx_tr">
<td id="Sx1.T3.2.5.3.1" class="ltx_td ltx_align_left ltx_border_r">bicycle</td>
<td id="Sx1.T3.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r">49.48</td>
<td id="Sx1.T3.2.5.3.3" class="ltx_td ltx_align_center ltx_border_rr">63.64</td>
<td id="Sx1.T3.2.5.3.4" class="ltx_td ltx_align_left ltx_border_r">bicycle</td>
<td id="Sx1.T3.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r">33.06</td>
<td id="Sx1.T3.2.5.3.6" class="ltx_td ltx_align_center">83.77</td>
</tr>
<tr id="Sx1.T3.2.6.4" class="ltx_tr">
<td id="Sx1.T3.2.6.4.1" class="ltx_td ltx_align_left ltx_border_r">bird</td>
<td id="Sx1.T3.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r">49.11</td>
<td id="Sx1.T3.2.6.4.3" class="ltx_td ltx_align_center ltx_border_rr">50.66</td>
<td id="Sx1.T3.2.6.4.4" class="ltx_td ltx_align_left ltx_border_r">bird</td>
<td id="Sx1.T3.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r">75.64</td>
<td id="Sx1.T3.2.6.4.6" class="ltx_td ltx_align_center">83.44</td>
</tr>
<tr id="Sx1.T3.2.7.5" class="ltx_tr">
<td id="Sx1.T3.2.7.5.1" class="ltx_td ltx_align_left ltx_border_r">boat</td>
<td id="Sx1.T3.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r">35.39</td>
<td id="Sx1.T3.2.7.5.3" class="ltx_td ltx_align_center ltx_border_rr">37.21</td>
<td id="Sx1.T3.2.7.5.4" class="ltx_td ltx_align_left ltx_border_r">boat</td>
<td id="Sx1.T3.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r">64.25</td>
<td id="Sx1.T3.2.7.5.6" class="ltx_td ltx_align_center">84.42</td>
</tr>
<tr id="Sx1.T3.2.8.6" class="ltx_tr">
<td id="Sx1.T3.2.8.6.1" class="ltx_td ltx_align_left ltx_border_r">bottle</td>
<td id="Sx1.T3.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r">38.61</td>
<td id="Sx1.T3.2.8.6.3" class="ltx_td ltx_align_center ltx_border_rr">64.63</td>
<td id="Sx1.T3.2.8.6.4" class="ltx_td ltx_align_left ltx_border_r">bottle</td>
<td id="Sx1.T3.2.8.6.5" class="ltx_td ltx_align_center ltx_border_r">65.46</td>
<td id="Sx1.T3.2.8.6.6" class="ltx_td ltx_align_center">87.60</td>
</tr>
<tr id="Sx1.T3.2.9.7" class="ltx_tr">
<td id="Sx1.T3.2.9.7.1" class="ltx_td ltx_align_left ltx_border_r">bus</td>
<td id="Sx1.T3.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r">66.44</td>
<td id="Sx1.T3.2.9.7.3" class="ltx_td ltx_align_center ltx_border_rr">69.59</td>
<td id="Sx1.T3.2.9.7.4" class="ltx_td ltx_align_left ltx_border_r">bus</td>
<td id="Sx1.T3.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r">77.81</td>
<td id="Sx1.T3.2.9.7.6" class="ltx_td ltx_align_center">80.48</td>
</tr>
<tr id="Sx1.T3.2.10.8" class="ltx_tr">
<td id="Sx1.T3.2.10.8.1" class="ltx_td ltx_align_left ltx_border_r">car</td>
<td id="Sx1.T3.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r">28.72</td>
<td id="Sx1.T3.2.10.8.3" class="ltx_td ltx_align_center ltx_border_rr">31.20</td>
<td id="Sx1.T3.2.10.8.4" class="ltx_td ltx_align_left ltx_border_r">car</td>
<td id="Sx1.T3.2.10.8.5" class="ltx_td ltx_align_center ltx_border_r">69.94</td>
<td id="Sx1.T3.2.10.8.6" class="ltx_td ltx_align_center">76.28</td>
</tr>
<tr id="Sx1.T3.2.11.9" class="ltx_tr">
<td id="Sx1.T3.2.11.9.1" class="ltx_td ltx_align_left ltx_border_r">cat</td>
<td id="Sx1.T3.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r">65.68</td>
<td id="Sx1.T3.2.11.9.3" class="ltx_td ltx_align_center ltx_border_rr">75.47</td>
<td id="Sx1.T3.2.11.9.4" class="ltx_td ltx_align_left ltx_border_r">cat</td>
<td id="Sx1.T3.2.11.9.5" class="ltx_td ltx_align_center ltx_border_r">78.41</td>
<td id="Sx1.T3.2.11.9.6" class="ltx_td ltx_align_center">83.45</td>
</tr>
<tr id="Sx1.T3.2.12.10" class="ltx_tr">
<td id="Sx1.T3.2.12.10.1" class="ltx_td ltx_align_left ltx_border_r">chair</td>
<td id="Sx1.T3.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r">14.69</td>
<td id="Sx1.T3.2.12.10.3" class="ltx_td ltx_align_center ltx_border_rr">17.43</td>
<td id="Sx1.T3.2.12.10.4" class="ltx_td ltx_align_left ltx_border_r">chair</td>
<td id="Sx1.T3.2.12.10.5" class="ltx_td ltx_align_center ltx_border_r">26.84</td>
<td id="Sx1.T3.2.12.10.6" class="ltx_td ltx_align_center">48.69</td>
</tr>
<tr id="Sx1.T3.2.13.11" class="ltx_tr">
<td id="Sx1.T3.2.13.11.1" class="ltx_td ltx_align_left ltx_border_r">cow</td>
<td id="Sx1.T3.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r">52.68</td>
<td id="Sx1.T3.2.13.11.3" class="ltx_td ltx_align_center ltx_border_rr">54.56</td>
<td id="Sx1.T3.2.13.11.4" class="ltx_td ltx_align_left ltx_border_r">cow</td>
<td id="Sx1.T3.2.13.11.5" class="ltx_td ltx_align_center ltx_border_r">63.03</td>
<td id="Sx1.T3.2.13.11.6" class="ltx_td ltx_align_center">64.06</td>
</tr>
<tr id="Sx1.T3.2.14.12" class="ltx_tr">
<td id="Sx1.T3.2.14.12.1" class="ltx_td ltx_align_left ltx_border_r">diningtable</td>
<td id="Sx1.T3.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r">19.25</td>
<td id="Sx1.T3.2.14.12.3" class="ltx_td ltx_align_center ltx_border_rr">42.33</td>
<td id="Sx1.T3.2.14.12.4" class="ltx_td ltx_align_left ltx_border_r">diningtable</td>
<td id="Sx1.T3.2.14.12.5" class="ltx_td ltx_align_center ltx_border_r">41.72</td>
<td id="Sx1.T3.2.14.12.6" class="ltx_td ltx_align_center">74.39</td>
</tr>
<tr id="Sx1.T3.2.15.13" class="ltx_tr">
<td id="Sx1.T3.2.15.13.1" class="ltx_td ltx_align_left ltx_border_r">dog</td>
<td id="Sx1.T3.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r">55.52</td>
<td id="Sx1.T3.2.15.13.3" class="ltx_td ltx_align_center ltx_border_rr">58.95</td>
<td id="Sx1.T3.2.15.13.4" class="ltx_td ltx_align_left ltx_border_r">dog</td>
<td id="Sx1.T3.2.15.13.5" class="ltx_td ltx_align_center ltx_border_r">70.53</td>
<td id="Sx1.T3.2.15.13.6" class="ltx_td ltx_align_center">76.31</td>
</tr>
<tr id="Sx1.T3.2.16.14" class="ltx_tr">
<td id="Sx1.T3.2.16.14.1" class="ltx_td ltx_align_left ltx_border_r">horse</td>
<td id="Sx1.T3.2.16.14.2" class="ltx_td ltx_align_center ltx_border_r">63.67</td>
<td id="Sx1.T3.2.16.14.3" class="ltx_td ltx_align_center ltx_border_rr">74.73</td>
<td id="Sx1.T3.2.16.14.4" class="ltx_td ltx_align_left ltx_border_r">horse</td>
<td id="Sx1.T3.2.16.14.5" class="ltx_td ltx_align_center ltx_border_r">67.72</td>
<td id="Sx1.T3.2.16.14.6" class="ltx_td ltx_align_center">81.12</td>
</tr>
<tr id="Sx1.T3.2.17.15" class="ltx_tr">
<td id="Sx1.T3.2.17.15.1" class="ltx_td ltx_align_left ltx_border_r">motorbike</td>
<td id="Sx1.T3.2.17.15.2" class="ltx_td ltx_align_center ltx_border_r">63.79</td>
<td id="Sx1.T3.2.17.15.3" class="ltx_td ltx_align_center ltx_border_rr">73.84</td>
<td id="Sx1.T3.2.17.15.4" class="ltx_td ltx_align_left ltx_border_r">motorbike</td>
<td id="Sx1.T3.2.17.15.5" class="ltx_td ltx_align_center ltx_border_r">74.75</td>
<td id="Sx1.T3.2.17.15.6" class="ltx_td ltx_align_center">87.33</td>
</tr>
<tr id="Sx1.T3.2.18.16" class="ltx_tr">
<td id="Sx1.T3.2.18.16.1" class="ltx_td ltx_align_left ltx_border_r">person</td>
<td id="Sx1.T3.2.18.16.2" class="ltx_td ltx_align_center ltx_border_r">34.54</td>
<td id="Sx1.T3.2.18.16.3" class="ltx_td ltx_align_center ltx_border_rr">37.00</td>
<td id="Sx1.T3.2.18.16.4" class="ltx_td ltx_align_left ltx_border_r">person</td>
<td id="Sx1.T3.2.18.16.5" class="ltx_td ltx_align_center ltx_border_r">42.94</td>
<td id="Sx1.T3.2.18.16.6" class="ltx_td ltx_align_center">50.95</td>
</tr>
<tr id="Sx1.T3.2.19.17" class="ltx_tr">
<td id="Sx1.T3.2.19.17.1" class="ltx_td ltx_align_left ltx_border_r">pottedplant</td>
<td id="Sx1.T3.2.19.17.2" class="ltx_td ltx_align_center ltx_border_r">37.40</td>
<td id="Sx1.T3.2.19.17.3" class="ltx_td ltx_align_center ltx_border_rr">43.53</td>
<td id="Sx1.T3.2.19.17.4" class="ltx_td ltx_align_left ltx_border_r">pottedplant</td>
<td id="Sx1.T3.2.19.17.5" class="ltx_td ltx_align_center ltx_border_r">27.08</td>
<td id="Sx1.T3.2.19.17.6" class="ltx_td ltx_align_center">44.33</td>
</tr>
<tr id="Sx1.T3.2.20.18" class="ltx_tr">
<td id="Sx1.T3.2.20.18.1" class="ltx_td ltx_align_left ltx_border_r">sheep</td>
<td id="Sx1.T3.2.20.18.2" class="ltx_td ltx_align_center ltx_border_r">66.56</td>
<td id="Sx1.T3.2.20.18.3" class="ltx_td ltx_align_center ltx_border_rr">69.52</td>
<td id="Sx1.T3.2.20.18.4" class="ltx_td ltx_align_left ltx_border_r">sheep</td>
<td id="Sx1.T3.2.20.18.5" class="ltx_td ltx_align_center ltx_border_r">67.30</td>
<td id="Sx1.T3.2.20.18.6" class="ltx_td ltx_align_center">74.88</td>
</tr>
<tr id="Sx1.T3.2.21.19" class="ltx_tr">
<td id="Sx1.T3.2.21.19.1" class="ltx_td ltx_align_left ltx_border_r">sofa</td>
<td id="Sx1.T3.2.21.19.2" class="ltx_td ltx_align_center ltx_border_r">11.60</td>
<td id="Sx1.T3.2.21.19.3" class="ltx_td ltx_align_center ltx_border_rr">12.07</td>
<td id="Sx1.T3.2.21.19.4" class="ltx_td ltx_align_left ltx_border_r">sofa</td>
<td id="Sx1.T3.2.21.19.5" class="ltx_td ltx_align_center ltx_border_r">25.18</td>
<td id="Sx1.T3.2.21.19.6" class="ltx_td ltx_align_center">25.97</td>
</tr>
<tr id="Sx1.T3.2.22.20" class="ltx_tr">
<td id="Sx1.T3.2.22.20.1" class="ltx_td ltx_align_left ltx_border_r">train</td>
<td id="Sx1.T3.2.22.20.2" class="ltx_td ltx_align_center ltx_border_r">63.74</td>
<td id="Sx1.T3.2.22.20.3" class="ltx_td ltx_align_center ltx_border_rr">71.42</td>
<td id="Sx1.T3.2.22.20.4" class="ltx_td ltx_align_left ltx_border_r">train</td>
<td id="Sx1.T3.2.22.20.5" class="ltx_td ltx_align_center ltx_border_r">69.58</td>
<td id="Sx1.T3.2.22.20.6" class="ltx_td ltx_align_center">84.25</td>
</tr>
<tr id="Sx1.T3.2.23.21" class="ltx_tr">
<td id="Sx1.T3.2.23.21.1" class="ltx_td ltx_align_left ltx_border_r">tvmonitor</td>
<td id="Sx1.T3.2.23.21.2" class="ltx_td ltx_align_center ltx_border_r">36.30</td>
<td id="Sx1.T3.2.23.21.3" class="ltx_td ltx_align_center ltx_border_rr">41.57</td>
<td id="Sx1.T3.2.23.21.4" class="ltx_td ltx_align_left ltx_border_r">tvmonitor</td>
<td id="Sx1.T3.2.23.21.5" class="ltx_td ltx_align_center ltx_border_r">45.76</td>
<td id="Sx1.T3.2.23.21.6" class="ltx_td ltx_align_center">75.34</td>
</tr>
<tr id="Sx1.T3.2.24.22" class="ltx_tr">
<td id="Sx1.T3.2.24.22.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t"><span id="Sx1.T3.2.24.22.1.1" class="ltx_text ltx_font_bold">mIoU</span></td>
<td id="Sx1.T3.2.24.22.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t" colspan="2">46.25</td>
<td id="Sx1.T3.2.24.22.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t"><span id="Sx1.T3.2.24.22.3.1" class="ltx_text ltx_font_bold">mIoU</span></td>
<td id="Sx1.T3.2.24.22.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" colspan="2">59.56</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx1.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="Sx1.T3.4.2" class="ltx_text" style="font-size:90%;">Comparison of Class-Wise Intersection over Union and Accuracy for the Private Syntagen Set (Left) and Pascal VOC Validation Set (Right)</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.17540" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.17541" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.17541">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.17541" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.17542" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 19:36:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
