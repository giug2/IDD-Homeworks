<article class="ltx_document ltx_authors_1line ltx_pruned_first">
 <h1 class="ltx_title ltx_title_document">
  ModaVerse: Efficiently Transforming Modalities with LLMs
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Xinyu Wang
    <br class="ltx_break"/>
    University of Adelaide
    <br class="ltx_break"/>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Bohan Zhuang
    <br class="ltx_break"/>
    Monash University
    <br class="ltx_break"/>
   </span>
  </span>
  <span class="ltx_author_before">
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Qi Wu
    <br class="ltx_break"/>
    University of Adelaide
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id3.id1">
   Humans possess the capability to comprehend diverse modalities and seamlessly transfer information between them. In this work, we introduce ModaVerse, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio. Predominant MLLM frameworks have largely relied on the alignment of latent spaces of textual and non-textual features. This alignment process, which synchronizes a language model trained on textual data with encoders and decoders trained on multi-modal data, often necessitates extensive training of several projection layers in multiple stages. Inspired by LLM-as-agent methodologies, we propose a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language. It aligns the LLM’s output with the input of generative models, avoiding the complexities associated with latent feature alignments, and simplifying the multiple training stages of existing MLLMs into a single, efficient process. This conceptual advancement leads to significant reductions in both data and computational costs. By conducting experiments on several benchmarks, we demonstrate that our approach attains comparable performance with the state of the art while achieving considerable efficiencies in data usage and training duration.
  </p>
 </div>
 <div class="ltx_logical-block" id="id2">
  <div class="ltx_para" id="id2.p1">
   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="110" id="id1.g1" src="/html/2401.06395/assets/x1.png" width="528"/>
  </div>
  <figure class="ltx_figure ltx_align_center" id="S0.F1">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">
      Figure 1
     </span>
     :
    </span>
    <span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">
     Comparative illustration of MLLM paradigms: (a) Multi-modal Pre-training, where new modules such as vision encoders and decoders are integrated within the standard LLM framework. (b) Adaptor Training, illustrating the use of projection layers to connect LLMs to pre-existing modules. (c) LLM as an Agent, highlighting the strategic application of prompts in conjunction with external tools. (d) Adaptor+Agent (ours), transforming modalities with efficient language-based Input/Output (I/O) alignment. E, D, and L represent the Encoder, Decoder, and Linear Layer respectively. T-to-x denotes a text-to-x generative model, where x can be Image, Video, and Audio.
    </span>
   </figcaption>
  </figure>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Spanning from ancient inscriptions to contemporary online encyclopedias, texts have served as the quintessential medium for chronicling the expanse of human knowledge. Such a vast accumulation of textual data provides a fertile terrain for training Large Language Models (LLMs)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib30" title="">
      30
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib42" title="">
      42
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib43" title="">
      43
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib31" title="">
      31
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    . Through extensive training on massive corpora, LLMs undergo a transformative process, a phenomenon captured by the concept where quantitative increases result in qualitative behavioral shifts
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib47" title="">
      47
     </a>
     ]
    </cite>
    , thus emerging with human-like reasoning abilities. This enables them to comprehend and respond to human instructions with remarkable precision. Such proficiency dramatically widens the scope of LLM applications across various domains, such as chatbots, programming copilots, and robotic agents.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Yet, the advent of richer communication forms calls for an evolution beyond the traditional confines of text. In the era where
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">
     a picture is worth a thousand words
    </span>
    , the capability to interpret and integrate complex visual and auditory data is invaluable. The pursuit of enabling LLMs to process and generate information beyond textual data reflects the natural progression of AI, aspiring to mimic the full breadth of human communication. This has spurred the evolution of Multi-modal LLMs (MLLMs), which are designed to understand, transform, and produce content across various modalities, such as images, audio, and video. This growing interest has prompted a proliferation of research and innovation in the field
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib52" title="">
      52
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib57" title="">
      57
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib48" title="">
      48
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib39" title="">
      39
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib54" title="">
      54
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib46" title="">
      46
     </a>
     ]
    </cite>
    . Addressing the limitations of traditional text-only LLMs, multi-modal pretraining, adaptor training, and LLM as an agent emerge as three key paradigms for equipping LLMs with multi-modal capabilities. Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    compares the existing paradigm’s overview schematic, assessing its performance and efficiency across three dimensions. Specifically,
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">
     Training Complexity
    </span>
    refers to the volume of training data, computational resources consumed, and the number of training stages involved.
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">
     Consistency
    </span>
    denotes the extent to which output is affected by modifications to inputs or prompts.
    <span class="ltx_text ltx_font_italic" id="S1.p2.1.4">
     Flexibility
    </span>
    pertains to the capacity for a variety of interpreting and generating outputs under diverse conditions.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">
     Multi-modal Pre-training
    </span>
    (Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    (a)) expands the traditional LLM framework to accommodate non-textual inputs and outputs by integrating additional modality encoders and decoders into the existing framework. Through custom-designed pre-training tasks, the LLM learns to represent multiple modalities effectively, achieving superior consistency and flexibility compared to existing paradigms. However, adapting a text-based LLM, which has been pre-trained on extensive textual data, to a multi-modal context often requires significant fine-tuning or even complete retraining. Therefore, this adaptation demands considerable computational resources. For example, Emu
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib39" title="">
      39
     </a>
     ]
    </cite>
    combines Eva-CLIP
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib38" title="">
      38
     </a>
     ]
    </cite>
    , LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib42" title="">
      42
     </a>
     ]
    </cite>
    , and Stable Diffusion
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib33" title="">
      33
     </a>
     ]
    </cite>
    to develop a foundational multi-modal model. This development involves an intensive pre-training phase on a large-scale dataset and the use of hundreds of GPUs.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">
     Adaptor Training
    </span>
    (Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    (b)) offers a computationally economical alternative. This strategy, demonstrated by BLIP-2
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib18" title="">
      18
     </a>
     ]
    </cite>
    and MiniGPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib57" title="">
      57
     </a>
     ]
    </cite>
    , is typically built upon well-established LLMs and multi-modal encoders/decoders. It involves integrating these multi-modal modules with the LLM by training a set of projection layers while keeping the parameters of the LLM either frozen or fine-tuned using parameter-efficient techniques like LoRA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    . These layers translate non-textual representations, such as image features, into the textual domain of LLMs, thus avoiding extensive training but preserving flexibility. However, despite reducing training data volume and time, these methods still require a complex training procedure. For example, NExT-GPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib48" title="">
      48
     </a>
     ]
    </cite>
    employs a three-step training pipeline where the encode/decode-side projection layers and the LLM adaptor are each trained in distinct stages. This intricate setup substantially escalates the complexity and leads to redundancy in the training process.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">
     LLM as an Agent
    </span>
    (Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    (c)) demonstrates a training free framework. These methods utilize the zero-shot inference capabilities of LLMs, emphasizing strategic prompt crafting and workflow design. This approach guides the interpreting and generation of multi-modal content through interactions with external tools. For instance, HuggingGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib35" title="">
      35
     </a>
     ]
    </cite>
    has developed a four-step pipeline that prompts OpenAI’s ChatGPT to select and execute models from the HugingFace’s model zoo, thereby solving a variety of tasks. However, it is crucial to recognize that these methods, lacking targeted training, often rely on x-to-text or text-x models, for processing non-textual inputs. This reliance may result in limited flexibility in handling diverse data types. Additionally, the heavy dependence on the design of system prompts and the reasoning capabilities of LLMs can further lead to inconsistent results.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    So far, each paradigm presents a specialized approach for achieving functionality in MLLMs, each with its advantages and limitations. Considering these trade-offs, exploring the integration of their strengths into a cohesive approach is compelling. Specifically, this paper proposes Adaptor+Agent, an approach that aims to find a harmonious balance between the efficiencies of the LLM-as-agent approaches and the flexibility of adaptor training methods.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="192" id="S1.F2.g1" src="/html/2401.06395/assets/x2.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">
      Figure 2
     </span>
     :
    </span>
    <span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">
     Comparison of the overview schematic of recent proposed MLLMs. L represents linear projection layers.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para ltx_noindent" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    <span class="ltx_text ltx_font_bold" id="S1.p7.1.1">
     Adaptor+Agent
    </span>
    (Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    (d)) aims to combine the benefits of adaptor training with LLM-as-agent methods. As shown in the figure, to maintain the flexibility of accepting arbitrary combinations of input modalities, we train a set of linear adaptors to map the input’s non-textual features into the LLM’s textual space. This approach allows the model to comprehend multi-modal inputs while preserving training efficiency by only tuning the adaptors. For the output, we adopt an LLM-as-agent design, using established text-to-x models for generating non-text outputs. This strategy avoids the need for tuning additional output-side projection layers, thus enhancing efficiency. The primary challenge in the Adaptor+Agent framework is
    <span class="ltx_text ltx_font_italic" id="S1.p7.1.2">
     aligning the LLM’s output with the text-to-x models’ input
    </span>
    . To address this, we introduce Input/Output (I/O) Alignment. In contrast to previous adaptor-based approaches that focus on feature-level alignment between the LLM and generative models, our I/O Alignment strategy prompts the LLM to generate language-aligned meta-responses. These meta-responses contain detailed instructions for activating the generative models. We achieve this I/O Alignment through an instruction-following tuning process. As a result, in a single stage of tuning, the LLM is equipped to invoke external models for producing non-text outputs, thus bypassing the complex feature-level alignment typically required in the adaptor training paradigm.
   </p>
  </div>
  <div class="ltx_para" id="S1.p8">
   <p class="ltx_p" id="S1.p8.1">
    In summary, the technical contributions of this paper are:
   </p>
  </div>
  <div class="ltx_para" id="S1.p9">
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We introduce a new Adaptor+Agent training paradigm for Multi-modal Large Language Models that synthesizes the strengths of both adaptor training and the LLM-as-Agent approach. This integration effectively reaps the benefits of training efficiency and model flexibility.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       To address the alignment challenges inherent in the LLM-as-Agent methodology, we propose an I/O Alignment strategy. This strategy diverges from conventional feature-level alignment and instead operates at the natural language level, offering a more efficient alternative.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       Our final product, ModaVerse, demonstrates comparable performance to the current state of the arts on several widely used benchmarks while requiring fewer data and training resources, thereby offering a more efficient option without compromising effectiveness.
      </p>
     </div>
    </li>
   </ul>
  </div>
  <figure class="ltx_figure" id="S1.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="107" id="S1.F3.g1" src="/html/2401.06395/assets/x3.png" width="276"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="S1.F3.10.5.1" style="font-size:90%;">
      Figure 3
     </span>
     :
    </span>
    <span class="ltx_text" id="S1.F3.8.4" style="font-size:90%;">
     Overview of the Proposed ModaVerse Pipeline. In the input projection stage, multi-modal inputs
     <math alttext="I^{\prime}" class="ltx_Math" display="inline" id="S1.F3.5.1.m1.1">
      <semantics id="S1.F3.5.1.m1.1b">
       <msup id="S1.F3.5.1.m1.1.1" xref="S1.F3.5.1.m1.1.1.cmml">
        <mi id="S1.F3.5.1.m1.1.1.2" xref="S1.F3.5.1.m1.1.1.2.cmml">
         I
        </mi>
        <mo id="S1.F3.5.1.m1.1.1.3" xref="S1.F3.5.1.m1.1.1.3.cmml">
         ′
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S1.F3.5.1.m1.1c">
        <apply id="S1.F3.5.1.m1.1.1.cmml" xref="S1.F3.5.1.m1.1.1">
         <csymbol cd="ambiguous" id="S1.F3.5.1.m1.1.1.1.cmml" xref="S1.F3.5.1.m1.1.1">
          superscript
         </csymbol>
         <ci id="S1.F3.5.1.m1.1.1.2.cmml" xref="S1.F3.5.1.m1.1.1.2">
          𝐼
         </ci>
         <ci id="S1.F3.5.1.m1.1.1.3.cmml" xref="S1.F3.5.1.m1.1.1.3">
          ′
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S1.F3.5.1.m1.1d">
        I^{\prime}
       </annotation>
      </semantics>
     </math>
     are aligned to the LLM’s space
     <math alttext="O_{1}" class="ltx_Math" display="inline" id="S1.F3.6.2.m2.1">
      <semantics id="S1.F3.6.2.m2.1b">
       <msub id="S1.F3.6.2.m2.1.1" xref="S1.F3.6.2.m2.1.1.cmml">
        <mi id="S1.F3.6.2.m2.1.1.2" xref="S1.F3.6.2.m2.1.1.2.cmml">
         O
        </mi>
        <mn id="S1.F3.6.2.m2.1.1.3" xref="S1.F3.6.2.m2.1.1.3.cmml">
         1
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S1.F3.6.2.m2.1c">
        <apply id="S1.F3.6.2.m2.1.1.cmml" xref="S1.F3.6.2.m2.1.1">
         <csymbol cd="ambiguous" id="S1.F3.6.2.m2.1.1.1.cmml" xref="S1.F3.6.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S1.F3.6.2.m2.1.1.2.cmml" xref="S1.F3.6.2.m2.1.1.2">
          𝑂
         </ci>
         <cn id="S1.F3.6.2.m2.1.1.3.cmml" type="integer" xref="S1.F3.6.2.m2.1.1.3">
          1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S1.F3.6.2.m2.1d">
        O_{1}
       </annotation>
      </semantics>
     </math>
     using a series of trainable linear layers. During the meta-response generation stage, LLM is fine-tuned with a LoRA adaptor, prompting the generation of a meta-response
     <math alttext="O_{2}" class="ltx_Math" display="inline" id="S1.F3.7.3.m3.1">
      <semantics id="S1.F3.7.3.m3.1b">
       <msub id="S1.F3.7.3.m3.1.1" xref="S1.F3.7.3.m3.1.1.cmml">
        <mi id="S1.F3.7.3.m3.1.1.2" xref="S1.F3.7.3.m3.1.1.2.cmml">
         O
        </mi>
        <mn id="S1.F3.7.3.m3.1.1.3" xref="S1.F3.7.3.m3.1.1.3.cmml">
         2
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S1.F3.7.3.m3.1c">
        <apply id="S1.F3.7.3.m3.1.1.cmml" xref="S1.F3.7.3.m3.1.1">
         <csymbol cd="ambiguous" id="S1.F3.7.3.m3.1.1.1.cmml" xref="S1.F3.7.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="S1.F3.7.3.m3.1.1.2.cmml" xref="S1.F3.7.3.m3.1.1.2">
          𝑂
         </ci>
         <cn id="S1.F3.7.3.m3.1.1.3.cmml" type="integer" xref="S1.F3.7.3.m3.1.1.3">
          2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S1.F3.7.3.m3.1d">
        O_{2}
       </annotation>
      </semantics>
     </math>
     . In the final response generation stage, additional pretrained text-to-x models are utilized to generate the ultimate multi-modal response
     <math alttext="O^{\prime}" class="ltx_Math" display="inline" id="S1.F3.8.4.m4.1">
      <semantics id="S1.F3.8.4.m4.1b">
       <msup id="S1.F3.8.4.m4.1.1" xref="S1.F3.8.4.m4.1.1.cmml">
        <mi id="S1.F3.8.4.m4.1.1.2" xref="S1.F3.8.4.m4.1.1.2.cmml">
         O
        </mi>
        <mo id="S1.F3.8.4.m4.1.1.3" xref="S1.F3.8.4.m4.1.1.3.cmml">
         ′
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S1.F3.8.4.m4.1c">
        <apply id="S1.F3.8.4.m4.1.1.cmml" xref="S1.F3.8.4.m4.1.1">
         <csymbol cd="ambiguous" id="S1.F3.8.4.m4.1.1.1.cmml" xref="S1.F3.8.4.m4.1.1">
          superscript
         </csymbol>
         <ci id="S1.F3.8.4.m4.1.1.2.cmml" xref="S1.F3.8.4.m4.1.1.2">
          𝑂
         </ci>
         <ci id="S1.F3.8.4.m4.1.1.3.cmml" xref="S1.F3.8.4.m4.1.1.3">
          ′
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S1.F3.8.4.m4.1d">
        O^{\prime}
       </annotation>
      </semantics>
     </math>
     based on the parsed meta response.
    </span>
   </figcaption>
  </figure>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para ltx_noindent" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">
     Multi-modal Pretrained MLLM.
    </span>
    Multi-modal pertaining is not a novel concept. Early efforts
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib36" title="">
      36
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib20" title="">
      20
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib25" title="">
      25
     </a>
     ]
    </cite>
    have explored ways to extend the capabilities of language models to comprehend visual content. These models achieved promising performance on specific vision-language tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ]
    </cite>
    but failed to generalize to more general scenarios. Recent advancements, however, have revealed that simpler model architectures can yield impressive outcomes when subjected to extensive large-scale pretraining, thanks to advanced computational resources and diverse datasets. For example, PaLI
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ]
    </cite>
    demonstrates this by integrating a vision transformer with a language transformer and training on an extensive dataset of 10 billion image-text pairs. Similarly, CM3Leon
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib54" title="">
      54
     </a>
     ]
    </cite>
    employs a straightforward decoder-only transformer architecture, trained on 340 million image-text pairs. This approach has enabled remarkable flexibility in generating and modifying both text and images, showing strong performance in image-to-text and text-to-image conversions. In addition, to enable an LLM that can generate non-text content, Emu
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib39" title="">
      39
     </a>
     ]
    </cite>
    combines a stable diffusion model with the LLaMA as a decoder, trained on a diverse corpus of 82 million image-text and video-text pairs. This integration marks a significant stride in the field, showcasing the growing versatility of LLM in multi-modal contexts.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    <span class="ltx_text ltx_font_bold" id="S2.p2.1.1">
     Adaptor Trained MLLM:
    </span>
    Leveraging recent advancements in parameter-efficient fine-tuning techniques
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    and data-efficient approaches
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    , numerous studies have explored the feasibility of training adaptors for aligning features between LLMs and various non-textual modules. Flamingo
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    represents a pioneering effort in freezing the parameters of both visual encoders and LLMs, training a set of gated cross-attention layers to integrate visual knowledge into LLMs. However, it still necessitates extensive training on a massive dataset. Another notable example is BLIP-2
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    , which introduces a BERT-based Q-Former to translate image features into textual representations, thereby enabling LLMs to comprehend image content. This innovation has inspired subsequent research
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib37" title="">
      37
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    , revealing that the Q-Former structure can be further simplified to a single linear layer, significantly reducing the number of trainable parameters. However, these advances, while showing considerable promise, have been predominantly applied to image-text tasks. In an effort to create a more versatile MLLM that can process a broader array of input types, subsequent works
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib37" title="">
      37
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    replaced the dual-modality encoder, CLIP
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib32" title="">
      32
     </a>
     ]
    </cite>
    , with the more versatile six-modality encoder, ImageBind
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    . In addition, other works
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib48" title="">
      48
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib55" title="">
      55
     </a>
     ]
    </cite>
    try to align the output side of LLMs with generation models, enabling the utilization of latent features from LLMs to guide generative models in producing non-textual content. For example, a concurrent study, NExT-GPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib48" title="">
      48
     </a>
     ]
    </cite>
    , introduces a multi-stage training procedure. This procedure includes a series of adaptors that align the LLM with encoders and generative models at the feature level. Our work differs from NExT-GPT in that it aligns the generative models at the language level instead of the feature level, thereby significantly reducing training complexity.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S2.p3">
   <p class="ltx_p" id="S2.p3.1">
    <span class="ltx_text ltx_font_bold" id="S2.p3.1.1">
     LLM-as-agent MLLM:
    </span>
    The remarkable zero-shot inference capabilities of LLMs enable them to effectively utilize external tools
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib34" title="">
      34
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib40" title="">
      40
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib53" title="">
      53
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib26" title="">
      26
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib35" title="">
      35
     </a>
     ]
    </cite>
    . This potential facilitates the creation of specialized pipelines and associated prompts, guiding LLMs to understand or produce multi-modal content. For instance, HuggingGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib29" title="">
      29
     </a>
     ]
    </cite>
    has developed a multi-step pipeline. In this process, ChatGPT initially interprets human instructions and selects appropriate models from a model zoo to accomplish the given tasks. Subsequently, the outputs from these external models are fed back into ChatGPT for parsing and generating the final response. Another notable example is MM-React
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib53" title="">
      53
     </a>
     ]
    </cite>
    , which introduces the integration of vision experts, such as OCR, image captioning, and object detection models, to extend the LLMs’ ability to process visual content. For each pair of input images and instructions, the LLM employs the relevant vision expert to extract pertinent information from the images, thereafter generating relevant responses.
   </p>
  </div>
  <div class="ltx_para" id="S2.p4">
   <p class="ltx_p" id="S2.p4.1">
    Figure
    <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    presents a schematic overview of recently proposed MLLMs. It demonstrates the characteristic of adaptor training, wherein all models incorporate additional projection components, such as a linear layer or a Q-former, either before or after the LLM. These components are utilized to align textual and non-textual representations between the LLM and encoders/decoders. In contrast, multimodal pretraining methods usually feature a more straightforward and concise architecture. They direct the LLM itself to learn multimodal features, thus avoiding the projection structures between different modules. Furthermore, LLM-as-agent methods employ an external model zoo to assist in processing or producing non-textual content, without integrating trainable modules into the system. In comparison, the proposed Adaptor+Agent paradigm follows an adaptor structure on the encoder side, where linear projection layers are trained to align the input features with the LLM’s textual space. On the decoder side, the LLM is treated as an agent to invoke external models for generating non-text content.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   ModaVerse
  </h2>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Pipeline Overview
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Figure
     <a class="ltx_ref" href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     illustrates the comprehensive framework of the proposed ModaVerse, which contains three functional blocks, including input projection, meta-response generation, and final response generation.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">
      Input Projection:
     </span>
     To adapt a text-based LLM into an MLLM capable of interpreting non-textual inputs, it is essential to align the LLM’s textual features with various modalities during the input phase. Recent research
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib57" title="">
       57
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib5" title="">
       5
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib37" title="">
       37
      </a>
      ]
     </cite>
     has demonstrated the feasibility of aligning these different modalities using a single linear layer. Following this, we employ ImageBind
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib10" title="">
       10
      </a>
      ]
     </cite>
     as a unified encoder, which processes inputs from diverse data types, including images, videos, and audio, converting them into a specific embedding. Subsequently, for each modality, we learn a set of linear projection layers to map these encoded representations into the LLM’s text space. As a result, ModaVerse gains the capability to comprehend multi-modal inputs.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">
      Meta Response Generation:
     </span>
     Since the foundational LLM is pre-trained exclusively on text-only data, it lacks the capability to directly generate non-text outputs. To address this limitation, we treat the foundational LLM as an agent, designed to produce only meta-responses. As depicted in Figure
     <a class="ltx_ref" href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     , the meta-response comprises formatted information that includes the invocation details. For instance, according to the meta-response, the system might activate a text-to-image model to create an image based on the prompt “A photo of a cat”. This design circumvents the need for training an additional output-side projection layer to align the LLM’s feature space with that of generative models, thereby simplifying the training process.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">
      Final Response Generation:
     </span>
     This block incorporates several replaceable text-to-x models to generate the final response, which may include images
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib33" title="">
       33
      </a>
      ]
     </cite>
     , videos
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib27" title="">
       27
      </a>
      ]
     </cite>
     , and audio
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib22" title="">
       22
      </a>
      ]
     </cite>
     . Based on the invocation details parsed from the meta-responses, one or more models will be activated to produce the non-textual output.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p5">
    <p class="ltx_p" id="S3.SS1.p5.1">
     So far, the Adaptor+Agent paradigm has become clear. In this paradigm, the input projection is designed with a set of linear adaptors that map multimodal features to the LLM’s textual space. The LLM itself is treated as an agent, invoking external models to generate the final responses. The benefits of such a design are:
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p6">
    <ul class="ltx_itemize" id="S3.I1">
     <li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i1.p1">
       <p class="ltx_p" id="S3.I1.i1.p1.1">
        Training adaptors during the input phase preserve the details in the input data, while simultaneously reducing the training volume compared to full multimodal pretraining.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i2.p1">
       <p class="ltx_p" id="S3.I1.i2.p1.1">
        Treating the LLM as an agent during the output phase not only decouples it from external generative models, enabling a plug-and-play approach but also eliminates the need for additional projection layers. This means that running generative models during the training stage is unnecessary, thereby reducing training complexity.
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    I/O Alignment
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.3">
     Consider a text-based LLM:
     <math alttext="I\rightarrow O" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1">
      <semantics id="S3.SS2.p1.1.m1.1a">
       <mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">
        <mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">
         I
        </mi>
        <mo id="S3.SS2.p1.1.m1.1.1.1" stretchy="false" xref="S3.SS2.p1.1.m1.1.1.1.cmml">
         →
        </mo>
        <mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">
         O
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b">
        <apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">
         <ci id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1">
          →
         </ci>
         <ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">
          𝐼
         </ci>
         <ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">
          𝑂
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">
        I\rightarrow O
       </annotation>
      </semantics>
     </math>
     with its input and output sets defined as
     <math alttext="I=O=\{text\}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1">
      <semantics id="S3.SS2.p1.2.m2.1a">
       <mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">
        <mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">
         I
        </mi>
        <mo id="S3.SS2.p1.2.m2.1.1.4" xref="S3.SS2.p1.2.m2.1.1.4.cmml">
         =
        </mo>
        <mi id="S3.SS2.p1.2.m2.1.1.5" xref="S3.SS2.p1.2.m2.1.1.5.cmml">
         O
        </mi>
        <mo id="S3.SS2.p1.2.m2.1.1.6" xref="S3.SS2.p1.2.m2.1.1.6.cmml">
         =
        </mo>
        <mrow id="S3.SS2.p1.2.m2.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.2.cmml">
         <mo id="S3.SS2.p1.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS2.p1.2.m2.1.1.1.2.cmml">
          {
         </mo>
         <mrow id="S3.SS2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml">
          <mi id="S3.SS2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.2.cmml">
           t
          </mi>
          <mo id="S3.SS2.p1.2.m2.1.1.1.1.1.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.3.cmml">
           e
          </mi>
          <mo id="S3.SS2.p1.2.m2.1.1.1.1.1.1a" lspace="0em" rspace="0em" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.2.m2.1.1.1.1.1.4" xref="S3.SS2.p1.2.m2.1.1.1.1.1.4.cmml">
           x
          </mi>
          <mo id="S3.SS2.p1.2.m2.1.1.1.1.1.1b" lspace="0em" rspace="0em" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.2.m2.1.1.1.1.1.5" xref="S3.SS2.p1.2.m2.1.1.1.1.1.5.cmml">
           t
          </mi>
         </mrow>
         <mo id="S3.SS2.p1.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS2.p1.2.m2.1.1.1.2.cmml">
          }
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b">
        <apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">
         <and id="S3.SS2.p1.2.m2.1.1a.cmml" xref="S3.SS2.p1.2.m2.1.1">
         </and>
         <apply id="S3.SS2.p1.2.m2.1.1b.cmml" xref="S3.SS2.p1.2.m2.1.1">
          <eq id="S3.SS2.p1.2.m2.1.1.4.cmml" xref="S3.SS2.p1.2.m2.1.1.4">
          </eq>
          <ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">
           𝐼
          </ci>
          <ci id="S3.SS2.p1.2.m2.1.1.5.cmml" xref="S3.SS2.p1.2.m2.1.1.5">
           𝑂
          </ci>
         </apply>
         <apply id="S3.SS2.p1.2.m2.1.1c.cmml" xref="S3.SS2.p1.2.m2.1.1">
          <eq id="S3.SS2.p1.2.m2.1.1.6.cmml" xref="S3.SS2.p1.2.m2.1.1.6">
          </eq>
          <share href="#S3.SS2.p1.2.m2.1.1.5.cmml" id="S3.SS2.p1.2.m2.1.1d.cmml" xref="S3.SS2.p1.2.m2.1.1">
          </share>
          <set id="S3.SS2.p1.2.m2.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1">
           <apply id="S3.SS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1">
            <times id="S3.SS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.1">
            </times>
            <ci id="S3.SS2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.2">
             𝑡
            </ci>
            <ci id="S3.SS2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.3">
             𝑒
            </ci>
            <ci id="S3.SS2.p1.2.m2.1.1.1.1.1.4.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.4">
             𝑥
            </ci>
            <ci id="S3.SS2.p1.2.m2.1.1.1.1.1.5.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.5">
             𝑡
            </ci>
           </apply>
          </set>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">
        I=O=\{text\}
       </annotation>
      </semantics>
     </math>
     , the objective of ModaVerse is to discover an efficient transformation that extends LLM to a multimodal model capable of handling
     <math alttext="I^{\prime}=O^{\prime}=\{text,image,video,audio\}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.4">
      <semantics id="S3.SS2.p1.3.m3.4a">
       <mrow id="S3.SS2.p1.3.m3.4.4" xref="S3.SS2.p1.3.m3.4.4.cmml">
        <msup id="S3.SS2.p1.3.m3.4.4.6" xref="S3.SS2.p1.3.m3.4.4.6.cmml">
         <mi id="S3.SS2.p1.3.m3.4.4.6.2" xref="S3.SS2.p1.3.m3.4.4.6.2.cmml">
          I
         </mi>
         <mo id="S3.SS2.p1.3.m3.4.4.6.3" xref="S3.SS2.p1.3.m3.4.4.6.3.cmml">
          ′
         </mo>
        </msup>
        <mo id="S3.SS2.p1.3.m3.4.4.7" xref="S3.SS2.p1.3.m3.4.4.7.cmml">
         =
        </mo>
        <msup id="S3.SS2.p1.3.m3.4.4.8" xref="S3.SS2.p1.3.m3.4.4.8.cmml">
         <mi id="S3.SS2.p1.3.m3.4.4.8.2" xref="S3.SS2.p1.3.m3.4.4.8.2.cmml">
          O
         </mi>
         <mo id="S3.SS2.p1.3.m3.4.4.8.3" xref="S3.SS2.p1.3.m3.4.4.8.3.cmml">
          ′
         </mo>
        </msup>
        <mo id="S3.SS2.p1.3.m3.4.4.9" xref="S3.SS2.p1.3.m3.4.4.9.cmml">
         =
        </mo>
        <mrow id="S3.SS2.p1.3.m3.4.4.4.4" xref="S3.SS2.p1.3.m3.4.4.4.5.cmml">
         <mo id="S3.SS2.p1.3.m3.4.4.4.4.5" stretchy="false" xref="S3.SS2.p1.3.m3.4.4.4.5.cmml">
          {
         </mo>
         <mrow id="S3.SS2.p1.3.m3.1.1.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.1.1.cmml">
          <mi id="S3.SS2.p1.3.m3.1.1.1.1.1.2" xref="S3.SS2.p1.3.m3.1.1.1.1.1.2.cmml">
           t
          </mi>
          <mo id="S3.SS2.p1.3.m3.1.1.1.1.1.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.1.1.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.1.1.1.1.1.3" xref="S3.SS2.p1.3.m3.1.1.1.1.1.3.cmml">
           e
          </mi>
          <mo id="S3.SS2.p1.3.m3.1.1.1.1.1.1a" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.1.1.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.1.1.1.1.1.4" xref="S3.SS2.p1.3.m3.1.1.1.1.1.4.cmml">
           x
          </mi>
          <mo id="S3.SS2.p1.3.m3.1.1.1.1.1.1b" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.1.1.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.1.1.1.1.1.5" xref="S3.SS2.p1.3.m3.1.1.1.1.1.5.cmml">
           t
          </mi>
         </mrow>
         <mo id="S3.SS2.p1.3.m3.4.4.4.4.6" xref="S3.SS2.p1.3.m3.4.4.4.5.cmml">
          ,
         </mo>
         <mrow id="S3.SS2.p1.3.m3.2.2.2.2.2" xref="S3.SS2.p1.3.m3.2.2.2.2.2.cmml">
          <mi id="S3.SS2.p1.3.m3.2.2.2.2.2.2" xref="S3.SS2.p1.3.m3.2.2.2.2.2.2.cmml">
           i
          </mi>
          <mo id="S3.SS2.p1.3.m3.2.2.2.2.2.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.2.2.2.2.2.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.2.2.2.2.2.3" xref="S3.SS2.p1.3.m3.2.2.2.2.2.3.cmml">
           m
          </mi>
          <mo id="S3.SS2.p1.3.m3.2.2.2.2.2.1a" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.2.2.2.2.2.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.2.2.2.2.2.4" xref="S3.SS2.p1.3.m3.2.2.2.2.2.4.cmml">
           a
          </mi>
          <mo id="S3.SS2.p1.3.m3.2.2.2.2.2.1b" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.2.2.2.2.2.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.2.2.2.2.2.5" xref="S3.SS2.p1.3.m3.2.2.2.2.2.5.cmml">
           g
          </mi>
          <mo id="S3.SS2.p1.3.m3.2.2.2.2.2.1c" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.2.2.2.2.2.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.2.2.2.2.2.6" xref="S3.SS2.p1.3.m3.2.2.2.2.2.6.cmml">
           e
          </mi>
         </mrow>
         <mo id="S3.SS2.p1.3.m3.4.4.4.4.7" xref="S3.SS2.p1.3.m3.4.4.4.5.cmml">
          ,
         </mo>
         <mrow id="S3.SS2.p1.3.m3.3.3.3.3.3" xref="S3.SS2.p1.3.m3.3.3.3.3.3.cmml">
          <mi id="S3.SS2.p1.3.m3.3.3.3.3.3.2" xref="S3.SS2.p1.3.m3.3.3.3.3.3.2.cmml">
           v
          </mi>
          <mo id="S3.SS2.p1.3.m3.3.3.3.3.3.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.3.3.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.3.3.3.3.3.3" xref="S3.SS2.p1.3.m3.3.3.3.3.3.3.cmml">
           i
          </mi>
          <mo id="S3.SS2.p1.3.m3.3.3.3.3.3.1a" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.3.3.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.3.3.3.3.3.4" xref="S3.SS2.p1.3.m3.3.3.3.3.3.4.cmml">
           d
          </mi>
          <mo id="S3.SS2.p1.3.m3.3.3.3.3.3.1b" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.3.3.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.3.3.3.3.3.5" xref="S3.SS2.p1.3.m3.3.3.3.3.3.5.cmml">
           e
          </mi>
          <mo id="S3.SS2.p1.3.m3.3.3.3.3.3.1c" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.3.3.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.3.3.3.3.3.6" xref="S3.SS2.p1.3.m3.3.3.3.3.3.6.cmml">
           o
          </mi>
         </mrow>
         <mo id="S3.SS2.p1.3.m3.4.4.4.4.8" xref="S3.SS2.p1.3.m3.4.4.4.5.cmml">
          ,
         </mo>
         <mrow id="S3.SS2.p1.3.m3.4.4.4.4.4" xref="S3.SS2.p1.3.m3.4.4.4.4.4.cmml">
          <mi id="S3.SS2.p1.3.m3.4.4.4.4.4.2" xref="S3.SS2.p1.3.m3.4.4.4.4.4.2.cmml">
           a
          </mi>
          <mo id="S3.SS2.p1.3.m3.4.4.4.4.4.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.4.4.4.4.4.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.4.4.4.4.4.3" xref="S3.SS2.p1.3.m3.4.4.4.4.4.3.cmml">
           u
          </mi>
          <mo id="S3.SS2.p1.3.m3.4.4.4.4.4.1a" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.4.4.4.4.4.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.4.4.4.4.4.4" xref="S3.SS2.p1.3.m3.4.4.4.4.4.4.cmml">
           d
          </mi>
          <mo id="S3.SS2.p1.3.m3.4.4.4.4.4.1b" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.4.4.4.4.4.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.4.4.4.4.4.5" xref="S3.SS2.p1.3.m3.4.4.4.4.4.5.cmml">
           i
          </mi>
          <mo id="S3.SS2.p1.3.m3.4.4.4.4.4.1c" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m3.4.4.4.4.4.1.cmml">
           ​
          </mo>
          <mi id="S3.SS2.p1.3.m3.4.4.4.4.4.6" xref="S3.SS2.p1.3.m3.4.4.4.4.4.6.cmml">
           o
          </mi>
         </mrow>
         <mo id="S3.SS2.p1.3.m3.4.4.4.4.9" stretchy="false" xref="S3.SS2.p1.3.m3.4.4.4.5.cmml">
          }
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.4b">
        <apply id="S3.SS2.p1.3.m3.4.4.cmml" xref="S3.SS2.p1.3.m3.4.4">
         <and id="S3.SS2.p1.3.m3.4.4a.cmml" xref="S3.SS2.p1.3.m3.4.4">
         </and>
         <apply id="S3.SS2.p1.3.m3.4.4b.cmml" xref="S3.SS2.p1.3.m3.4.4">
          <eq id="S3.SS2.p1.3.m3.4.4.7.cmml" xref="S3.SS2.p1.3.m3.4.4.7">
          </eq>
          <apply id="S3.SS2.p1.3.m3.4.4.6.cmml" xref="S3.SS2.p1.3.m3.4.4.6">
           <csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.4.4.6.1.cmml" xref="S3.SS2.p1.3.m3.4.4.6">
            superscript
           </csymbol>
           <ci id="S3.SS2.p1.3.m3.4.4.6.2.cmml" xref="S3.SS2.p1.3.m3.4.4.6.2">
            𝐼
           </ci>
           <ci id="S3.SS2.p1.3.m3.4.4.6.3.cmml" xref="S3.SS2.p1.3.m3.4.4.6.3">
            ′
           </ci>
          </apply>
          <apply id="S3.SS2.p1.3.m3.4.4.8.cmml" xref="S3.SS2.p1.3.m3.4.4.8">
           <csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.4.4.8.1.cmml" xref="S3.SS2.p1.3.m3.4.4.8">
            superscript
           </csymbol>
           <ci id="S3.SS2.p1.3.m3.4.4.8.2.cmml" xref="S3.SS2.p1.3.m3.4.4.8.2">
            𝑂
           </ci>
           <ci id="S3.SS2.p1.3.m3.4.4.8.3.cmml" xref="S3.SS2.p1.3.m3.4.4.8.3">
            ′
           </ci>
          </apply>
         </apply>
         <apply id="S3.SS2.p1.3.m3.4.4c.cmml" xref="S3.SS2.p1.3.m3.4.4">
          <eq id="S3.SS2.p1.3.m3.4.4.9.cmml" xref="S3.SS2.p1.3.m3.4.4.9">
          </eq>
          <share href="#S3.SS2.p1.3.m3.4.4.8.cmml" id="S3.SS2.p1.3.m3.4.4d.cmml" xref="S3.SS2.p1.3.m3.4.4">
          </share>
          <set id="S3.SS2.p1.3.m3.4.4.4.5.cmml" xref="S3.SS2.p1.3.m3.4.4.4.4">
           <apply id="S3.SS2.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1.1.1">
            <times id="S3.SS2.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1.1.1.1">
            </times>
            <ci id="S3.SS2.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.1.1.1.2">
             𝑡
            </ci>
            <ci id="S3.SS2.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.1.1.1.3">
             𝑒
            </ci>
            <ci id="S3.SS2.p1.3.m3.1.1.1.1.1.4.cmml" xref="S3.SS2.p1.3.m3.1.1.1.1.1.4">
             𝑥
            </ci>
            <ci id="S3.SS2.p1.3.m3.1.1.1.1.1.5.cmml" xref="S3.SS2.p1.3.m3.1.1.1.1.1.5">
             𝑡
            </ci>
           </apply>
           <apply id="S3.SS2.p1.3.m3.2.2.2.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2.2">
            <times id="S3.SS2.p1.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2.2.1">
            </times>
            <ci id="S3.SS2.p1.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2.2.2">
             𝑖
            </ci>
            <ci id="S3.SS2.p1.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2.2.3">
             𝑚
            </ci>
            <ci id="S3.SS2.p1.3.m3.2.2.2.2.2.4.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2.2.4">
             𝑎
            </ci>
            <ci id="S3.SS2.p1.3.m3.2.2.2.2.2.5.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2.2.5">
             𝑔
            </ci>
            <ci id="S3.SS2.p1.3.m3.2.2.2.2.2.6.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2.2.6">
             𝑒
            </ci>
           </apply>
           <apply id="S3.SS2.p1.3.m3.3.3.3.3.3.cmml" xref="S3.SS2.p1.3.m3.3.3.3.3.3">
            <times id="S3.SS2.p1.3.m3.3.3.3.3.3.1.cmml" xref="S3.SS2.p1.3.m3.3.3.3.3.3.1">
            </times>
            <ci id="S3.SS2.p1.3.m3.3.3.3.3.3.2.cmml" xref="S3.SS2.p1.3.m3.3.3.3.3.3.2">
             𝑣
            </ci>
            <ci id="S3.SS2.p1.3.m3.3.3.3.3.3.3.cmml" xref="S3.SS2.p1.3.m3.3.3.3.3.3.3">
             𝑖
            </ci>
            <ci id="S3.SS2.p1.3.m3.3.3.3.3.3.4.cmml" xref="S3.SS2.p1.3.m3.3.3.3.3.3.4">
             𝑑
            </ci>
            <ci id="S3.SS2.p1.3.m3.3.3.3.3.3.5.cmml" xref="S3.SS2.p1.3.m3.3.3.3.3.3.5">
             𝑒
            </ci>
            <ci id="S3.SS2.p1.3.m3.3.3.3.3.3.6.cmml" xref="S3.SS2.p1.3.m3.3.3.3.3.3.6">
             𝑜
            </ci>
           </apply>
           <apply id="S3.SS2.p1.3.m3.4.4.4.4.4.cmml" xref="S3.SS2.p1.3.m3.4.4.4.4.4">
            <times id="S3.SS2.p1.3.m3.4.4.4.4.4.1.cmml" xref="S3.SS2.p1.3.m3.4.4.4.4.4.1">
            </times>
            <ci id="S3.SS2.p1.3.m3.4.4.4.4.4.2.cmml" xref="S3.SS2.p1.3.m3.4.4.4.4.4.2">
             𝑎
            </ci>
            <ci id="S3.SS2.p1.3.m3.4.4.4.4.4.3.cmml" xref="S3.SS2.p1.3.m3.4.4.4.4.4.3">
             𝑢
            </ci>
            <ci id="S3.SS2.p1.3.m3.4.4.4.4.4.4.cmml" xref="S3.SS2.p1.3.m3.4.4.4.4.4.4">
             𝑑
            </ci>
            <ci id="S3.SS2.p1.3.m3.4.4.4.4.4.5.cmml" xref="S3.SS2.p1.3.m3.4.4.4.4.4.5">
             𝑖
            </ci>
            <ci id="S3.SS2.p1.3.m3.4.4.4.4.4.6.cmml" xref="S3.SS2.p1.3.m3.4.4.4.4.4.6">
             𝑜
            </ci>
           </apply>
          </set>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.4c">
        I^{\prime}=O^{\prime}=\{text,image,video,audio\}
       </annotation>
      </semantics>
     </math>
     , described as follows:
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <table class="ltx_equation ltx_eqn_table" id="S3.E1">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="\left\{\begin{array}[]{ll}P:\texttt{ImageBind}(I^{\prime})\rightarrow O_{1},\\
LLM^{\prime}:O_{1}\rightarrow O_{2},\\
M:O_{2}\rightarrow O^{\prime}\end{array}\right." class="ltx_Math" display="block" id="S3.E1.m1.2">
         <semantics id="S3.E1.m1.2a">
          <mrow id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.1.cmml">
           <mo id="S3.E1.m1.2.3.2.1" xref="S3.E1.m1.2.3.1.1.cmml">
            {
           </mo>
           <mtable columnspacing="5pt" displaystyle="true" id="S3.E1.m1.2.2" rowspacing="0pt" xref="S3.E1.m1.2.2.cmml">
            <mtr id="S3.E1.m1.2.2a" xref="S3.E1.m1.2.2.cmml">
             <mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.2.2b" xref="S3.E1.m1.2.2.cmml">
              <mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">
               <mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">
                <mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">
                 P
                </mi>
                <mo id="S3.E1.m1.1.1.1.1.1.1.1.2" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">
                 :
                </mo>
                <mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">
                 <mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">
                  <mtext class="ltx_mathvariant_monospace" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3a.cmml">
                   ImageBind
                  </mtext>
                  <mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">
                   ​
                  </mo>
                  <mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                   <mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    (
                   </mo>
                   <msup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    <mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">
                     I
                    </mi>
                    <mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                     ′
                    </mo>
                   </msup>
                   <mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    )
                   </mo>
                  </mrow>
                 </mrow>
                 <mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">
                  →
                 </mo>
                 <msub id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">
                  <mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">
                   O
                  </mi>
                  <mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml">
                   1
                  </mn>
                 </msub>
                </mrow>
               </mrow>
               <mo id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">
                ,
               </mo>
              </mrow>
             </mtd>
             <mtd id="S3.E1.m1.2.2c" xref="S3.E1.m1.2.2.cmml">
             </mtd>
            </mtr>
            <mtr id="S3.E1.m1.2.2d" xref="S3.E1.m1.2.2.cmml">
             <mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.2.2e" xref="S3.E1.m1.2.2.cmml">
              <mrow id="S3.E1.m1.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.1.cmml">
               <mrow id="S3.E1.m1.2.2.2.1.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.1.cmml">
                <mrow id="S3.E1.m1.2.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.1.2.cmml">
                 <mi id="S3.E1.m1.2.2.2.1.1.1.1.2.2" xref="S3.E1.m1.2.2.2.1.1.1.1.2.2.cmml">
                  L
                 </mi>
                 <mo id="S3.E1.m1.2.2.2.1.1.1.1.2.1" lspace="0em" rspace="0em" xref="S3.E1.m1.2.2.2.1.1.1.1.2.1.cmml">
                  ​
                 </mo>
                 <mi id="S3.E1.m1.2.2.2.1.1.1.1.2.3" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3.cmml">
                  L
                 </mi>
                 <mo id="S3.E1.m1.2.2.2.1.1.1.1.2.1a" lspace="0em" rspace="0em" xref="S3.E1.m1.2.2.2.1.1.1.1.2.1.cmml">
                  ​
                 </mo>
                 <msup id="S3.E1.m1.2.2.2.1.1.1.1.2.4" xref="S3.E1.m1.2.2.2.1.1.1.1.2.4.cmml">
                  <mi id="S3.E1.m1.2.2.2.1.1.1.1.2.4.2" xref="S3.E1.m1.2.2.2.1.1.1.1.2.4.2.cmml">
                   M
                  </mi>
                  <mo id="S3.E1.m1.2.2.2.1.1.1.1.2.4.3" xref="S3.E1.m1.2.2.2.1.1.1.1.2.4.3.cmml">
                   ′
                  </mo>
                 </msup>
                </mrow>
                <mo id="S3.E1.m1.2.2.2.1.1.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.2.2.2.1.1.1.1.1.cmml">
                 :
                </mo>
                <mrow id="S3.E1.m1.2.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.cmml">
                 <msub id="S3.E1.m1.2.2.2.1.1.1.1.3.2" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2.cmml">
                  <mi id="S3.E1.m1.2.2.2.1.1.1.1.3.2.2" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2.2.cmml">
                   O
                  </mi>
                  <mn id="S3.E1.m1.2.2.2.1.1.1.1.3.2.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2.3.cmml">
                   1
                  </mn>
                 </msub>
                 <mo id="S3.E1.m1.2.2.2.1.1.1.1.3.1" stretchy="false" xref="S3.E1.m1.2.2.2.1.1.1.1.3.1.cmml">
                  →
                 </mo>
                 <msub id="S3.E1.m1.2.2.2.1.1.1.1.3.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.cmml">
                  <mi id="S3.E1.m1.2.2.2.1.1.1.1.3.3.2" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.2.cmml">
                   O
                  </mi>
                  <mn id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.cmml">
                   2
                  </mn>
                 </msub>
                </mrow>
               </mrow>
               <mo id="S3.E1.m1.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.1.cmml">
                ,
               </mo>
              </mrow>
             </mtd>
             <mtd id="S3.E1.m1.2.2f" xref="S3.E1.m1.2.2.cmml">
             </mtd>
            </mtr>
            <mtr id="S3.E1.m1.2.2g" xref="S3.E1.m1.2.2.cmml">
             <mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.2.2h" xref="S3.E1.m1.2.2.cmml">
              <mrow id="S3.E1.m1.2.2.3.1.1" xref="S3.E1.m1.2.2.3.1.1.cmml">
               <mi id="S3.E1.m1.2.2.3.1.1.2" xref="S3.E1.m1.2.2.3.1.1.2.cmml">
                M
               </mi>
               <mo id="S3.E1.m1.2.2.3.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.2.2.3.1.1.1.cmml">
                :
               </mo>
               <mrow id="S3.E1.m1.2.2.3.1.1.3" xref="S3.E1.m1.2.2.3.1.1.3.cmml">
                <msub id="S3.E1.m1.2.2.3.1.1.3.2" xref="S3.E1.m1.2.2.3.1.1.3.2.cmml">
                 <mi id="S3.E1.m1.2.2.3.1.1.3.2.2" xref="S3.E1.m1.2.2.3.1.1.3.2.2.cmml">
                  O
                 </mi>
                 <mn id="S3.E1.m1.2.2.3.1.1.3.2.3" xref="S3.E1.m1.2.2.3.1.1.3.2.3.cmml">
                  2
                 </mn>
                </msub>
                <mo id="S3.E1.m1.2.2.3.1.1.3.1" stretchy="false" xref="S3.E1.m1.2.2.3.1.1.3.1.cmml">
                 →
                </mo>
                <msup id="S3.E1.m1.2.2.3.1.1.3.3" xref="S3.E1.m1.2.2.3.1.1.3.3.cmml">
                 <mi id="S3.E1.m1.2.2.3.1.1.3.3.2" xref="S3.E1.m1.2.2.3.1.1.3.3.2.cmml">
                  O
                 </mi>
                 <mo id="S3.E1.m1.2.2.3.1.1.3.3.3" xref="S3.E1.m1.2.2.3.1.1.3.3.3.cmml">
                  ′
                 </mo>
                </msup>
               </mrow>
              </mrow>
             </mtd>
             <mtd id="S3.E1.m1.2.2i" xref="S3.E1.m1.2.2.cmml">
             </mtd>
            </mtr>
           </mtable>
           <mi id="S3.E1.m1.2.3.2.2" xref="S3.E1.m1.2.3.1.1.cmml">
           </mi>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b">
           <apply id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.2">
            <csymbol cd="latexml" id="S3.E1.m1.2.3.1.1.cmml" xref="S3.E1.m1.2.3.2.1">
             cases
            </csymbol>
            <matrix id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">
             <matrixrow id="S3.E1.m1.2.2a.cmml" xref="S3.E1.m1.2.2">
              <apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">
                :
               </ci>
               <ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">
                𝑃
               </ci>
               <apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">
                <ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">
                 →
                </ci>
                <apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1">
                 <times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2">
                 </times>
                 <ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">
                  <mtext class="ltx_mathvariant_monospace" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">
                   ImageBind
                  </mtext>
                 </ci>
                 <apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1">
                  <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1">
                   superscript
                  </csymbol>
                  <ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">
                   𝐼
                  </ci>
                  <ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">
                   ′
                  </ci>
                 </apply>
                </apply>
                <apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">
                 <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">
                  𝑂
                 </ci>
                 <cn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3">
                  1
                 </cn>
                </apply>
               </apply>
              </apply>
              <cerror id="S3.E1.m1.2.2b.cmml" xref="S3.E1.m1.2.2">
               <csymbol cd="ambiguous" id="S3.E1.m1.2.2c.cmml" xref="S3.E1.m1.2.2">
                missing-subexpression
               </csymbol>
              </cerror>
             </matrixrow>
             <matrixrow id="S3.E1.m1.2.2d.cmml" xref="S3.E1.m1.2.2">
              <apply id="S3.E1.m1.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1">
               <ci id="S3.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.1">
                :
               </ci>
               <apply id="S3.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2">
                <times id="S3.E1.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.1">
                </times>
                <ci id="S3.E1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.2">
                 𝐿
                </ci>
                <ci id="S3.E1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.3">
                 𝐿
                </ci>
                <apply id="S3.E1.m1.2.2.2.1.1.1.1.2.4.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.4">
                 <csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.2.4.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.4">
                  superscript
                 </csymbol>
                 <ci id="S3.E1.m1.2.2.2.1.1.1.1.2.4.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.4.2">
                  𝑀
                 </ci>
                 <ci id="S3.E1.m1.2.2.2.1.1.1.1.2.4.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.2.4.3">
                  ′
                 </ci>
                </apply>
               </apply>
               <apply id="S3.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3">
                <ci id="S3.E1.m1.2.2.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.1">
                 →
                </ci>
                <apply id="S3.E1.m1.2.2.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2">
                 <csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.2.2.2.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2.2">
                  𝑂
                 </ci>
                 <cn id="S3.E1.m1.2.2.2.1.1.1.1.3.2.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.1.1.1.1.3.2.3">
                  1
                 </cn>
                </apply>
                <apply id="S3.E1.m1.2.2.2.1.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3">
                 <csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.2.2.2.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.2">
                  𝑂
                 </ci>
                 <cn id="S3.E1.m1.2.2.2.1.1.1.1.3.3.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.1.1.1.1.3.3.3">
                  2
                 </cn>
                </apply>
               </apply>
              </apply>
              <cerror id="S3.E1.m1.2.2e.cmml" xref="S3.E1.m1.2.2">
               <csymbol cd="ambiguous" id="S3.E1.m1.2.2f.cmml" xref="S3.E1.m1.2.2">
                missing-subexpression
               </csymbol>
              </cerror>
             </matrixrow>
             <matrixrow id="S3.E1.m1.2.2g.cmml" xref="S3.E1.m1.2.2">
              <apply id="S3.E1.m1.2.2.3.1.1.cmml" xref="S3.E1.m1.2.2.3.1.1">
               <ci id="S3.E1.m1.2.2.3.1.1.1.cmml" xref="S3.E1.m1.2.2.3.1.1.1">
                :
               </ci>
               <ci id="S3.E1.m1.2.2.3.1.1.2.cmml" xref="S3.E1.m1.2.2.3.1.1.2">
                𝑀
               </ci>
               <apply id="S3.E1.m1.2.2.3.1.1.3.cmml" xref="S3.E1.m1.2.2.3.1.1.3">
                <ci id="S3.E1.m1.2.2.3.1.1.3.1.cmml" xref="S3.E1.m1.2.2.3.1.1.3.1">
                 →
                </ci>
                <apply id="S3.E1.m1.2.2.3.1.1.3.2.cmml" xref="S3.E1.m1.2.2.3.1.1.3.2">
                 <csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.3.1.1.3.2">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.2.2.3.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.3.1.1.3.2.2">
                  𝑂
                 </ci>
                 <cn id="S3.E1.m1.2.2.3.1.1.3.2.3.cmml" type="integer" xref="S3.E1.m1.2.2.3.1.1.3.2.3">
                  2
                 </cn>
                </apply>
                <apply id="S3.E1.m1.2.2.3.1.1.3.3.cmml" xref="S3.E1.m1.2.2.3.1.1.3.3">
                 <csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.3.1.1.3.3">
                  superscript
                 </csymbol>
                 <ci id="S3.E1.m1.2.2.3.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.3.1.1.3.3.2">
                  𝑂
                 </ci>
                 <ci id="S3.E1.m1.2.2.3.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.3.1.1.3.3.3">
                  ′
                 </ci>
                </apply>
               </apply>
              </apply>
              <cerror id="S3.E1.m1.2.2h.cmml" xref="S3.E1.m1.2.2">
               <csymbol cd="ambiguous" id="S3.E1.m1.2.2i.cmml" xref="S3.E1.m1.2.2">
                missing-subexpression
               </csymbol>
              </cerror>
             </matrixrow>
            </matrix>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.E1.m1.2c">
           \left\{\begin{array}[]{ll}P:\texttt{ImageBind}(I^{\prime})\rightarrow O_{1},\\
LLM^{\prime}:O_{1}\rightarrow O_{2},\\
M:O_{2}\rightarrow O^{\prime}\end{array}\right.
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (1)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.10">
     Each line of this equation corresponds to a stage depicted in Figure
     <a class="ltx_ref" href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     . The first line denotes a trainable projection
     <math alttext="P" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1">
      <semantics id="S3.SS2.p3.1.m1.1a">
       <mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">
        P
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b">
        <ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">
         𝑃
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">
        P
       </annotation>
      </semantics>
     </math>
     from the multimodal feature - extracted by ImageBind
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib10" title="">
       10
      </a>
      ]
     </cite>
     from the input
     <math alttext="I^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1">
      <semantics id="S3.SS2.p3.2.m2.1a">
       <msup id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">
        <mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">
         I
        </mi>
        <mo id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">
         ′
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b">
        <apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">
          𝐼
         </ci>
         <ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">
          ′
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">
        I^{\prime}
       </annotation>
      </semantics>
     </math>
     - into the textual space of the LLM,
     <math alttext="O_{1}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1">
      <semantics id="S3.SS2.p3.3.m3.1a">
       <msub id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">
        <mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">
         O
        </mi>
        <mn id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">
         1
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b">
        <apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">
          𝑂
         </ci>
         <cn id="S3.SS2.p3.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p3.3.m3.1.1.3">
          1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">
        O_{1}
       </annotation>
      </semantics>
     </math>
     . The second line involves tuning a LoRA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib12" title="">
       12
      </a>
      ]
     </cite>
     adaptor to prompt the adapted LLM, defined as
     <math alttext="LLM^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1">
      <semantics id="S3.SS2.p3.4.m4.1a">
       <mrow id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">
        <mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">
         L
        </mi>
        <mo id="S3.SS2.p3.4.m4.1.1.1" lspace="0em" rspace="0em" xref="S3.SS2.p3.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">
         L
        </mi>
        <mo id="S3.SS2.p3.4.m4.1.1.1a" lspace="0em" rspace="0em" xref="S3.SS2.p3.4.m4.1.1.1.cmml">
         ​
        </mo>
        <msup id="S3.SS2.p3.4.m4.1.1.4" xref="S3.SS2.p3.4.m4.1.1.4.cmml">
         <mi id="S3.SS2.p3.4.m4.1.1.4.2" xref="S3.SS2.p3.4.m4.1.1.4.2.cmml">
          M
         </mi>
         <mo id="S3.SS2.p3.4.m4.1.1.4.3" xref="S3.SS2.p3.4.m4.1.1.4.3.cmml">
          ′
         </mo>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b">
        <apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">
         <times id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1">
         </times>
         <ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">
          𝐿
         </ci>
         <ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">
          𝐿
         </ci>
         <apply id="S3.SS2.p3.4.m4.1.1.4.cmml" xref="S3.SS2.p3.4.m4.1.1.4">
          <csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.4.1.cmml" xref="S3.SS2.p3.4.m4.1.1.4">
           superscript
          </csymbol>
          <ci id="S3.SS2.p3.4.m4.1.1.4.2.cmml" xref="S3.SS2.p3.4.m4.1.1.4.2">
           𝑀
          </ci>
          <ci id="S3.SS2.p3.4.m4.1.1.4.3.cmml" xref="S3.SS2.p3.4.m4.1.1.4.3">
           ′
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">
        LLM^{\prime}
       </annotation>
      </semantics>
     </math>
     , to generate a meta-response
     <math alttext="O_{2}" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1">
      <semantics id="S3.SS2.p3.5.m5.1a">
       <msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">
        <mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">
         O
        </mi>
        <mn id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">
         2
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b">
        <apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">
          𝑂
         </ci>
         <cn id="S3.SS2.p3.5.m5.1.1.3.cmml" type="integer" xref="S3.SS2.p3.5.m5.1.1.3">
          2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">
        O_{2}
       </annotation>
      </semantics>
     </math>
     from the input feature
     <math alttext="O_{1}" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.1">
      <semantics id="S3.SS2.p3.6.m6.1a">
       <msub id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">
        <mi id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">
         O
        </mi>
        <mn id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml">
         1
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b">
        <apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2">
          𝑂
         </ci>
         <cn id="S3.SS2.p3.6.m6.1.1.3.cmml" type="integer" xref="S3.SS2.p3.6.m6.1.1.3">
          1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">
        O_{1}
       </annotation>
      </semantics>
     </math>
     . Finally, the third line, where
     <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m7.1">
      <semantics id="S3.SS2.p3.7.m7.1a">
       <mi id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml">
        M
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b">
        <ci id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">
         𝑀
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">
        M
       </annotation>
      </semantics>
     </math>
     represents the frozen, established text-to-x model zoo, utilizes the parsed meta-response to generate the final multimodal outputs
     <math alttext="O^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p3.8.m8.1">
      <semantics id="S3.SS2.p3.8.m8.1a">
       <msup id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml">
        <mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">
         O
        </mi>
        <mo id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml">
         ′
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b">
        <apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">
          𝑂
         </ci>
         <ci id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3">
          ′
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">
        O^{\prime}
       </annotation>
      </semantics>
     </math>
     . To achieve these objectives, we propose an instruction-following I/O Alignment. This alignment aims to simultaneously fit the
     <math alttext="I^{\prime}\rightarrow O_{1}" class="ltx_Math" display="inline" id="S3.SS2.p3.9.m9.1">
      <semantics id="S3.SS2.p3.9.m9.1a">
       <mrow id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml">
        <msup id="S3.SS2.p3.9.m9.1.1.2" xref="S3.SS2.p3.9.m9.1.1.2.cmml">
         <mi id="S3.SS2.p3.9.m9.1.1.2.2" xref="S3.SS2.p3.9.m9.1.1.2.2.cmml">
          I
         </mi>
         <mo id="S3.SS2.p3.9.m9.1.1.2.3" xref="S3.SS2.p3.9.m9.1.1.2.3.cmml">
          ′
         </mo>
        </msup>
        <mo id="S3.SS2.p3.9.m9.1.1.1" stretchy="false" xref="S3.SS2.p3.9.m9.1.1.1.cmml">
         →
        </mo>
        <msub id="S3.SS2.p3.9.m9.1.1.3" xref="S3.SS2.p3.9.m9.1.1.3.cmml">
         <mi id="S3.SS2.p3.9.m9.1.1.3.2" xref="S3.SS2.p3.9.m9.1.1.3.2.cmml">
          O
         </mi>
         <mn id="S3.SS2.p3.9.m9.1.1.3.3" xref="S3.SS2.p3.9.m9.1.1.3.3.cmml">
          1
         </mn>
        </msub>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b">
        <apply id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1">
         <ci id="S3.SS2.p3.9.m9.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1">
          →
         </ci>
         <apply id="S3.SS2.p3.9.m9.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.2">
          <csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.2.1.cmml" xref="S3.SS2.p3.9.m9.1.1.2">
           superscript
          </csymbol>
          <ci id="S3.SS2.p3.9.m9.1.1.2.2.cmml" xref="S3.SS2.p3.9.m9.1.1.2.2">
           𝐼
          </ci>
          <ci id="S3.SS2.p3.9.m9.1.1.2.3.cmml" xref="S3.SS2.p3.9.m9.1.1.2.3">
           ′
          </ci>
         </apply>
         <apply id="S3.SS2.p3.9.m9.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.3.1.cmml" xref="S3.SS2.p3.9.m9.1.1.3">
           subscript
          </csymbol>
          <ci id="S3.SS2.p3.9.m9.1.1.3.2.cmml" xref="S3.SS2.p3.9.m9.1.1.3.2">
           𝑂
          </ci>
          <cn id="S3.SS2.p3.9.m9.1.1.3.3.cmml" type="integer" xref="S3.SS2.p3.9.m9.1.1.3.3">
           1
          </cn>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">
        I^{\prime}\rightarrow O_{1}
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="O_{1}\rightarrow O_{2}" class="ltx_Math" display="inline" id="S3.SS2.p3.10.m10.1">
      <semantics id="S3.SS2.p3.10.m10.1a">
       <mrow id="S3.SS2.p3.10.m10.1.1" xref="S3.SS2.p3.10.m10.1.1.cmml">
        <msub id="S3.SS2.p3.10.m10.1.1.2" xref="S3.SS2.p3.10.m10.1.1.2.cmml">
         <mi id="S3.SS2.p3.10.m10.1.1.2.2" xref="S3.SS2.p3.10.m10.1.1.2.2.cmml">
          O
         </mi>
         <mn id="S3.SS2.p3.10.m10.1.1.2.3" xref="S3.SS2.p3.10.m10.1.1.2.3.cmml">
          1
         </mn>
        </msub>
        <mo id="S3.SS2.p3.10.m10.1.1.1" stretchy="false" xref="S3.SS2.p3.10.m10.1.1.1.cmml">
         →
        </mo>
        <msub id="S3.SS2.p3.10.m10.1.1.3" xref="S3.SS2.p3.10.m10.1.1.3.cmml">
         <mi id="S3.SS2.p3.10.m10.1.1.3.2" xref="S3.SS2.p3.10.m10.1.1.3.2.cmml">
          O
         </mi>
         <mn id="S3.SS2.p3.10.m10.1.1.3.3" xref="S3.SS2.p3.10.m10.1.1.3.3.cmml">
          2
         </mn>
        </msub>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m10.1b">
        <apply id="S3.SS2.p3.10.m10.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1">
         <ci id="S3.SS2.p3.10.m10.1.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1.1">
          →
         </ci>
         <apply id="S3.SS2.p3.10.m10.1.1.2.cmml" xref="S3.SS2.p3.10.m10.1.1.2">
          <csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.1.1.2.1.cmml" xref="S3.SS2.p3.10.m10.1.1.2">
           subscript
          </csymbol>
          <ci id="S3.SS2.p3.10.m10.1.1.2.2.cmml" xref="S3.SS2.p3.10.m10.1.1.2.2">
           𝑂
          </ci>
          <cn id="S3.SS2.p3.10.m10.1.1.2.3.cmml" type="integer" xref="S3.SS2.p3.10.m10.1.1.2.3">
           1
          </cn>
         </apply>
         <apply id="S3.SS2.p3.10.m10.1.1.3.cmml" xref="S3.SS2.p3.10.m10.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.1.1.3.1.cmml" xref="S3.SS2.p3.10.m10.1.1.3">
           subscript
          </csymbol>
          <ci id="S3.SS2.p3.10.m10.1.1.3.2.cmml" xref="S3.SS2.p3.10.m10.1.1.3.2">
           𝑂
          </ci>
          <cn id="S3.SS2.p3.10.m10.1.1.3.3.cmml" type="integer" xref="S3.SS2.p3.10.m10.1.1.3.3">
           2
          </cn>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.10.m10.1c">
        O_{1}\rightarrow O_{2}
       </annotation>
      </semantics>
     </math>
     alignments. As such, the trainable components, as depicted in Figure
     <a class="ltx_ref" href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     , consist of three linear layers and the LoRA adaptor. Specifically, the I/O Alignment involves two primary components: the construction of instructions, and the tuning of linear and LoRA adaptors.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p4">
    <p class="ltx_p" id="S3.SS2.p4.6">
     To implement I/O Alignment, two issues must be addressed. First, since an exact representation of
     <math alttext="O_{1}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1">
      <semantics id="S3.SS2.p4.1.m1.1a">
       <msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">
        <mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">
         O
        </mi>
        <mn id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">
         1
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b">
        <apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">
          𝑂
         </ci>
         <cn id="S3.SS2.p4.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1.3">
          1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">
        O_{1}
       </annotation>
      </semantics>
     </math>
     is not directly obtainable, existing adaptor-based methods typically train a direct mapping from
     <math alttext="I^{\prime}\rightarrow O_{2}" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1">
      <semantics id="S3.SS2.p4.2.m2.1a">
       <mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">
        <msup id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">
         <mi id="S3.SS2.p4.2.m2.1.1.2.2" xref="S3.SS2.p4.2.m2.1.1.2.2.cmml">
          I
         </mi>
         <mo id="S3.SS2.p4.2.m2.1.1.2.3" xref="S3.SS2.p4.2.m2.1.1.2.3.cmml">
          ′
         </mo>
        </msup>
        <mo id="S3.SS2.p4.2.m2.1.1.1" stretchy="false" xref="S3.SS2.p4.2.m2.1.1.1.cmml">
         →
        </mo>
        <msub id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">
         <mi id="S3.SS2.p4.2.m2.1.1.3.2" xref="S3.SS2.p4.2.m2.1.1.3.2.cmml">
          O
         </mi>
         <mn id="S3.SS2.p4.2.m2.1.1.3.3" xref="S3.SS2.p4.2.m2.1.1.3.3.cmml">
          2
         </mn>
        </msub>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b">
        <apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">
         <ci id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1">
          →
         </ci>
         <apply id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">
          <csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.2.1.cmml" xref="S3.SS2.p4.2.m2.1.1.2">
           superscript
          </csymbol>
          <ci id="S3.SS2.p4.2.m2.1.1.2.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2.2">
           𝐼
          </ci>
          <ci id="S3.SS2.p4.2.m2.1.1.2.3.cmml" xref="S3.SS2.p4.2.m2.1.1.2.3">
           ′
          </ci>
         </apply>
         <apply id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.3.1.cmml" xref="S3.SS2.p4.2.m2.1.1.3">
           subscript
          </csymbol>
          <ci id="S3.SS2.p4.2.m2.1.1.3.2.cmml" xref="S3.SS2.p4.2.m2.1.1.3.2">
           𝑂
          </ci>
          <cn id="S3.SS2.p4.2.m2.1.1.3.3.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.3.3">
           2
          </cn>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">
        I^{\prime}\rightarrow O_{2}
       </annotation>
      </semantics>
     </math>
     , using captions from paired datasets as
     <math alttext="O_{2}" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1">
      <semantics id="S3.SS2.p4.3.m3.1a">
       <msub id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">
        <mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">
         O
        </mi>
        <mn id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">
         2
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b">
        <apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">
          𝑂
         </ci>
         <cn id="S3.SS2.p4.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p4.3.m3.1.1.3">
          2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">
        O_{2}
       </annotation>
      </semantics>
     </math>
     to learn the projections, framing the process as a multi-modal captioning task. However, in our case,
     <math alttext="O_{2}" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.1">
      <semantics id="S3.SS2.p4.4.m4.1a">
       <msub id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">
        <mi id="S3.SS2.p4.4.m4.1.1.2" xref="S3.SS2.p4.4.m4.1.1.2.cmml">
         O
        </mi>
        <mn id="S3.SS2.p4.4.m4.1.1.3" xref="S3.SS2.p4.4.m4.1.1.3.cmml">
         2
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b">
        <apply id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.p4.4.m4.1.1.2">
          𝑂
         </ci>
         <cn id="S3.SS2.p4.4.m4.1.1.3.cmml" type="integer" xref="S3.SS2.p4.4.m4.1.1.3">
          2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">
        O_{2}
       </annotation>
      </semantics>
     </math>
     is a meta response rather than just the text descriptions of
     <math alttext="I^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m5.1">
      <semantics id="S3.SS2.p4.5.m5.1a">
       <msup id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml">
        <mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">
         I
        </mi>
        <mo id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">
         ′
        </mo>
       </msup>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b">
        <apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">
          superscript
         </csymbol>
         <ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">
          𝐼
         </ci>
         <ci id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">
          ′
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">
        I^{\prime}
       </annotation>
      </semantics>
     </math>
     . That is, given instructions and accompanying multi-modal inputs, the expected output should specifically identify
     <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.6.1">
      which
     </span>
     model to use and
     <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.6.2">
      how
     </span>
     to use it. For example, given the instruction, ‘Generate an image for an animal based on the provided audio clip of its vocalization’, along with an accompanying audio clip that records a cat’s meowing, the expected invocation information should be as follows: ‘{model: “text-to-image”, prompts: “a photo of a cat”}’. Therefore, simply using x-to-text datasets to train the projection layers under an image captioning task is not possible to facilitate such purposes. The second issue is the language-level misalignment due to the different training corpora of LLMs and generative models. For instance, to describe a landscape image, an LLM tends to produce coherent, literary paragraphs, whereas a text-to-image model typically prefers concise, descriptive prompts accompanied by attributes such as “4k” and “masterpiece”. Thus, I/O Alignment should also achieve
     <math alttext="O_{1}\rightarrow O_{2}" class="ltx_Math" display="inline" id="S3.SS2.p4.6.m6.1">
      <semantics id="S3.SS2.p4.6.m6.1a">
       <mrow id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml">
        <msub id="S3.SS2.p4.6.m6.1.1.2" xref="S3.SS2.p4.6.m6.1.1.2.cmml">
         <mi id="S3.SS2.p4.6.m6.1.1.2.2" xref="S3.SS2.p4.6.m6.1.1.2.2.cmml">
          O
         </mi>
         <mn id="S3.SS2.p4.6.m6.1.1.2.3" xref="S3.SS2.p4.6.m6.1.1.2.3.cmml">
          1
         </mn>
        </msub>
        <mo id="S3.SS2.p4.6.m6.1.1.1" stretchy="false" xref="S3.SS2.p4.6.m6.1.1.1.cmml">
         →
        </mo>
        <msub id="S3.SS2.p4.6.m6.1.1.3" xref="S3.SS2.p4.6.m6.1.1.3.cmml">
         <mi id="S3.SS2.p4.6.m6.1.1.3.2" xref="S3.SS2.p4.6.m6.1.1.3.2.cmml">
          O
         </mi>
         <mn id="S3.SS2.p4.6.m6.1.1.3.3" xref="S3.SS2.p4.6.m6.1.1.3.3.cmml">
          2
         </mn>
        </msub>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b">
        <apply id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">
         <ci id="S3.SS2.p4.6.m6.1.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1.1">
          →
         </ci>
         <apply id="S3.SS2.p4.6.m6.1.1.2.cmml" xref="S3.SS2.p4.6.m6.1.1.2">
          <csymbol cd="ambiguous" id="S3.SS2.p4.6.m6.1.1.2.1.cmml" xref="S3.SS2.p4.6.m6.1.1.2">
           subscript
          </csymbol>
          <ci id="S3.SS2.p4.6.m6.1.1.2.2.cmml" xref="S3.SS2.p4.6.m6.1.1.2.2">
           𝑂
          </ci>
          <cn id="S3.SS2.p4.6.m6.1.1.2.3.cmml" type="integer" xref="S3.SS2.p4.6.m6.1.1.2.3">
           1
          </cn>
         </apply>
         <apply id="S3.SS2.p4.6.m6.1.1.3.cmml" xref="S3.SS2.p4.6.m6.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS2.p4.6.m6.1.1.3.1.cmml" xref="S3.SS2.p4.6.m6.1.1.3">
           subscript
          </csymbol>
          <ci id="S3.SS2.p4.6.m6.1.1.3.2.cmml" xref="S3.SS2.p4.6.m6.1.1.3.2">
           𝑂
          </ci>
          <cn id="S3.SS2.p4.6.m6.1.1.3.3.cmml" type="integer" xref="S3.SS2.p4.6.m6.1.1.3.3">
           2
          </cn>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">
        O_{1}\rightarrow O_{2}
       </annotation>
      </semantics>
     </math>
     , ensuring language-level alignment between the meta-response and the input prompts required by generative models.
    </p>
   </div>
   <figure class="ltx_table" id="S3.T1">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.8">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S3.T1.8.9.1">
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.8.9.1.1" rowspan="2">
        <span class="ltx_text" id="S3.T1.8.9.1.1.1">
         Method
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.8.9.1.2" rowspan="2">
        <span class="ltx_text" id="S3.T1.8.9.1.2.1">
         Type
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.8.9.1.3" rowspan="2">
        <span class="ltx_text" id="S3.T1.8.9.1.3.1">
         Input
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.8.9.1.4" rowspan="2">
        <span class="ltx_text" id="S3.T1.8.9.1.4.1">
         Output
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S3.T1.8.9.1.5">
        Stage I
       </th>
       <th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S3.T1.8.9.1.6">
        Stage II
       </th>
       <th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_t" colspan="2" id="S3.T1.8.9.1.7">
        Stage III
       </th>
      </tr>
      <tr class="ltx_tr" id="S3.T1.8.10.2">
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S3.T1.8.10.2.1">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.10.2.1.1">
         <span class="ltx_p" id="S3.T1.8.10.2.1.1.1" style="width:14.2pt;">
          Data
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T1.8.10.2.2">
        GPU Time
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S3.T1.8.10.2.3">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.10.2.3.1">
         <span class="ltx_p" id="S3.T1.8.10.2.3.1.1" style="width:14.2pt;">
          Data
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T1.8.10.2.4">
        GPU Time
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="S3.T1.8.10.2.5">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.10.2.5.1">
         <span class="ltx_p" id="S3.T1.8.10.2.5.1.1" style="width:14.2pt;">
          Data
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.8.10.2.6">
        GPU Time
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S3.T1.3.3">
       <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.3.3.4">
        Emu
        <cite class="ltx_cite ltx_citemacro_cite">
         [
         <a class="ltx_ref" href="#bib.bib39" title="">
          39
         </a>
         ]
        </cite>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.5">
        pretrain
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.6">
        T,I,V
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.3.3.7">
        T,I
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.3.3.8">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.3.3.8.1">
         <span class="ltx_p" id="S3.T1.3.3.8.1.1" style="width:14.2pt;">
          82M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1">
        128
        <math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1">
         <semantics id="S3.T1.1.1.1.m1.1a">
          <mo id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">
           ×
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b">
           <times id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">
           </times>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">
           \times
          </annotation>
         </semantics>
        </math>
        2d
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.3.3.9">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.3.3.9.1">
         <span class="ltx_p" id="S3.T1.3.3.9.1.1" style="width:14.2pt;">
          24M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2">
        32
        <math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.2.2.2.m1.1">
         <semantics id="S3.T1.2.2.2.m1.1a">
          <mo id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml">
           ×
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b">
           <times id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">
           </times>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">
           \times
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.3.3.10">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.3.3.10.1">
         <span class="ltx_p" id="S3.T1.3.3.10.1.1" style="width:14.2pt;">
          1.28M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3">
        16
        <math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.3.3.3.m1.1">
         <semantics id="S3.T1.3.3.3.m1.1a">
          <mo id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">
           ×
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b">
           <times id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">
           </times>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">
           \times
          </annotation>
         </semantics>
        </math>
        16h
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.5.5">
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.5.3">
        LLaVA
        <cite class="ltx_cite ltx_citemacro_cite">
         [
         <a class="ltx_ref" href="#bib.bib24" title="">
          24
         </a>
         ]
        </cite>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.5.4">
        adaptor
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.5.5">
        T,I
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.5.6">
        T
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.5.5.7">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.5.7.1">
         <span class="ltx_p" id="S3.T1.5.5.7.1.1" style="width:14.2pt;">
          595k
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.4.4.1">
        8
        <math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.4.4.1.m1.1">
         <semantics id="S3.T1.4.4.1.m1.1a">
          <mo id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml">
           ×
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b">
           <times id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1">
           </times>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">
           \times
          </annotation>
         </semantics>
        </math>
        20h
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.5.5.8">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.5.8.1">
         <span class="ltx_p" id="S3.T1.5.5.8.1.1" style="width:14.2pt;">
          158k
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.5.5.2">
        8
        <math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.5.5.2.m1.1">
         <semantics id="S3.T1.5.5.2.m1.1a">
          <mo id="S3.T1.5.5.2.m1.1.1" xref="S3.T1.5.5.2.m1.1.1.cmml">
           ×
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.5.5.2.m1.1b">
           <times id="S3.T1.5.5.2.m1.1.1.cmml" xref="S3.T1.5.5.2.m1.1.1">
           </times>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.5.5.2.m1.1c">
           \times
          </annotation>
         </semantics>
        </math>
        10h
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.5.5.9">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.5.5.9.1">
         <span class="ltx_p" id="S3.T1.5.5.9.1.1" style="width:14.2pt;">
          N/A
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.5.5.10">
        N/A
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.7.7">
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.7.7.3">
        BLIP-2
        <cite class="ltx_cite ltx_citemacro_cite">
         [
         <a class="ltx_ref" href="#bib.bib17" title="">
          17
         </a>
         ]
        </cite>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.7.7.4">
        adaptor
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.7.7.5">
        T,I
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.7.7.6">
        T
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.7.7.7">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.7.7.7.1">
         <span class="ltx_p" id="S3.T1.7.7.7.1.1" style="width:14.2pt;">
          129M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.1">
        16
        <math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.6.6.1.m1.1">
         <semantics id="S3.T1.6.6.1.m1.1a">
          <mo id="S3.T1.6.6.1.m1.1.1" xref="S3.T1.6.6.1.m1.1.1.cmml">
           ×
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.6.6.1.m1.1b">
           <times id="S3.T1.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.1.m1.1.1">
           </times>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.6.6.1.m1.1c">
           \times
          </annotation>
         </semantics>
        </math>
        6d
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.7.7.8">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.7.7.8.1">
         <span class="ltx_p" id="S3.T1.7.7.8.1.1" style="width:14.2pt;">
          129M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.7.7.2">
        16
        <math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.7.7.2.m1.1">
         <semantics id="S3.T1.7.7.2.m1.1a">
          <mo id="S3.T1.7.7.2.m1.1.1" xref="S3.T1.7.7.2.m1.1.1.cmml">
           ×
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.7.7.2.m1.1b">
           <times id="S3.T1.7.7.2.m1.1.1.cmml" xref="S3.T1.7.7.2.m1.1.1">
           </times>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.7.7.2.m1.1c">
           \times
          </annotation>
         </semantics>
        </math>
        3d
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.7.7.9">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.7.7.9.1">
         <span class="ltx_p" id="S3.T1.7.7.9.1.1" style="width:14.2pt;">
          N/A
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.7.7.10">
        N/A
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.8.11.1">
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.8.11.1.1">
        NExT-GPT
        <cite class="ltx_cite ltx_citemacro_cite">
         [
         <a class="ltx_ref" href="#bib.bib48" title="">
          48
         </a>
         ]
        </cite>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.8.11.1.2">
        adaptor
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.8.11.1.3">
        T,I,V,A
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.8.11.1.4">
        T,I,V,A
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.8.11.1.5">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.11.1.5.1">
         <span class="ltx_p" id="S3.T1.8.11.1.5.1.1" style="width:14.2pt;">
          13M+
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.8.11.1.6">
        -
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.8.11.1.7">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.11.1.7.1">
         <span class="ltx_p" id="S3.T1.8.11.1.7.1.1" style="width:14.2pt;">
          13M+
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.8.11.1.8">
        -
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.8.11.1.9">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.11.1.9.1">
         <span class="ltx_p" id="S3.T1.8.11.1.9.1.1" style="width:14.2pt;">
          20k+
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S3.T1.8.11.1.10">
        -
       </td>
      </tr>
      <tr class="ltx_tr" id="S3.T1.8.8">
       <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.8.8.2">
        ModaVerse
       </td>
       <td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.8.8.3">
        adaptor+agent
       </td>
       <td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.8.8.4">
        T,I,V,A
       </td>
       <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.8.8.5">
        T,I,V,A
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S3.T1.8.8.6">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.8.6.1">
         <span class="ltx_p" id="S3.T1.8.8.6.1.1" style="width:14.2pt;">
          2M
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.8.8.1">
        4
        <math alttext="\times" class="ltx_Math" display="inline" id="S3.T1.8.8.1.m1.1">
         <semantics id="S3.T1.8.8.1.m1.1a">
          <mo id="S3.T1.8.8.1.m1.1.1" xref="S3.T1.8.8.1.m1.1.1.cmml">
           ×
          </mo>
          <annotation-xml encoding="MathML-Content" id="S3.T1.8.8.1.m1.1b">
           <times id="S3.T1.8.8.1.m1.1.1.cmml" xref="S3.T1.8.8.1.m1.1.1">
           </times>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.T1.8.8.1.m1.1c">
           \times
          </annotation>
         </semantics>
        </math>
        20h
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S3.T1.8.8.7">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.8.7.1">
         <span class="ltx_p" id="S3.T1.8.8.7.1.1" style="width:14.2pt;">
          N/A
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.8.8.8">
        N/A
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S3.T1.8.8.9">
        <span class="ltx_inline-block ltx_align_top" id="S3.T1.8.8.9.1">
         <span class="ltx_p" id="S3.T1.8.8.9.1.1" style="width:14.2pt;">
          N/A
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.8.8.10">
        N/A
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      <span class="ltx_text" id="S3.T1.10.1.1" style="font-size:90%;">
       Table 1
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.T1.11.2" style="font-size:90%;">
      Comparison of training complexity of ModaVerse with recent MLLMs. ‘N/A’ indicates stages that are not required, while ‘-’ denotes that the data was not disclosed by the authors. ‘T’, ‘I’, ‘V’, ‘A’ are the abbreviations of text, image, video, and audio, respectively.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS2.p5">
    <p class="ltx_p" id="S3.SS2.p5.1">
     To address the issues mentioned above, I/O Alignment employs an instruction-following training approach. The adaptors are trained with an input that includes both a language instruction and accompanying multi-modal elements, with the aim to produce a meta-response that details the following invocation. We utilize training datasets from generative models to create pairs of instructions and their corresponding ground truths (see Section
     <a class="ltx_ref" href="#S3.SS3" title="3.3 Instruction Generation and Training ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       3.3
      </span>
     </a>
     for the instruction generation procedure). This method is beneficial for several reasons: 1. Instruction-following tuning compels the LLM to fully comprehend multi-modal inputs, thereby aiding in aligning the input projection layers between multi-modal input and LLM. 2. The training datasets of generative models typically provide both non-textual data, such as images, videos, or audio, and their corresponding textual descriptions, thereby offering a solid foundation of aligned data samples. 3. Most open-source generative models are trained on the same publicly available datasets. Aligning the meta-response with the text descriptions from these datasets means it is possible to seamlessly switch between generative models, thus facilitating a plug-and-play approach.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p6">
    <p class="ltx_p" id="S3.SS2.p6.1">
     Based on this, we generate different types of instructions to fulfill the above objectives, which are as follows:
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p7">
    <p class="ltx_p" id="S3.SS2.p7.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p7.1.1">
      Input-side Alignment Instruction
     </span>
     focuses on aligning the LLM’s capability to comprehend inputs comprising combinations of various modal data, such as text+image, image+video, or image+audio+video. For instance, when presented with a combination of image, audio, and video inputs, the instruction “Describe the given image, audio, and video” directs the MLLM to sequentially describe the content in the image, followed by the audio, and then the video.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p8">
    <p class="ltx_p" id="S3.SS2.p8.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p8.1.1">
      Output-side Alignment Instruction
     </span>
     aims to align the LLM’s ability to generate meta-responses that include invocation details, such as the selected model and prompts. For example, the instruction “Generate an image based on the provided audio of an animal sound” teaches the model to utilize a text-to-image model to generate an image, potentially with a prompt like “A photo of a cat”.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p9">
    <p class="ltx_p" id="S3.SS2.p9.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p9.1.1">
      Reasoning Boosting Instruction
     </span>
     is designed to preserve and enhance the LLM’s reasoning capabilities through a diverse range of topics. For instance, an instruction like “Where might this audio clip have been recorded?” requires the LLM to make a reasoned inference based on the input data, thereby strengthening its reasoning skills.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Instruction Generation and Training
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     In this section, we introduce how to generate the instruction invocation pairs used in I/O Alignment.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS3.p2">
    <div class="ltx_listing ltx_lst_language_json ltx_lst_numbers_left ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S3.SS3.p2.1" style="border-color: #000000;">
     <div class="ltx_listing_data">
      <a download="" href="data:text/plain;base64,eyJpbnN0cnVjdGlvbiI6IFsiR2VuZXJhdGUgYW4gaW1hZ2Ugb2YgYW4gYW5pbWFsIGJhc2VkIG9uIHRoZSBwcm92aWRlZCB2b2NhbGl6YXRpb24uIiwgImNhdF9tZW93aW5nLndhdiIsIF0KImludm9jYXRpb24iOiBbKCJ0ZXh0LXRvLWltYWdlIiwgIkEgcGhvdG8gb2YgYSBjYXQiKSwgXX0=">
       ⬇
      </a>
     </div>
     <div class="ltx_listingline" id="lstnumberx1">
      <span class="ltx_tag ltx_tag_listingline">
       1
      </span>
      <span class="ltx_text ltx_font_typewriter" id="lstnumberx1.1" style="font-size:90%;">
       {
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx1.2" style="font-size:90%;">
       <span class="ltx_text" id="lstnumberx1.2.1" style="color:#A31414;">
        "instruction"
       </span>
      </span>
      <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx1.3" style="font-size:90%;color:#0000FF;">
       :
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.1">
       </span>
       ["Generate
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.2">
       </span>
       an
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.3">
       </span>
       image
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.4">
       </span>
       of
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.5">
       </span>
       an
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.6">
       </span>
       animal
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.7">
       </span>
       based
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.8">
       </span>
       on
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.9">
       </span>
       the
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.10">
       </span>
       provided
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.11">
       </span>
       vocalization.",
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.12">
       </span>
       "cat_meowing.wav",
       <span class="ltx_text ltx_lst_space" id="lstnumberx1.3.13">
       </span>
       ]
      </span>
     </div>
     <div class="ltx_listingline" id="lstnumberx2">
      <span class="ltx_tag ltx_tag_listingline">
       2
      </span>
      <span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx2.1" style="font-size:90%;">
       <span class="ltx_text" id="lstnumberx2.1.1" style="color:#A31414;">
        "invocation"
       </span>
      </span>
      <span class="ltx_text ltx_lst_comment ltx_font_typewriter" id="lstnumberx2.2" style="font-size:90%;color:#0000FF;">
       :
       <span class="ltx_text ltx_lst_space" id="lstnumberx2.2.1">
       </span>
       [("text-to-image",
       <span class="ltx_text ltx_lst_space" id="lstnumberx2.2.2">
       </span>
       "A
       <span class="ltx_text ltx_lst_space" id="lstnumberx2.2.3">
       </span>
       photo
       <span class="ltx_text ltx_lst_space" id="lstnumberx2.2.4">
       </span>
       of
       <span class="ltx_text ltx_lst_space" id="lstnumberx2.2.5">
       </span>
       a
       <span class="ltx_text ltx_lst_space" id="lstnumberx2.2.6">
       </span>
       cat"),
       <span class="ltx_text ltx_lst_space" id="lstnumberx2.2.7">
       </span>
       ]}
      </span>
     </div>
    </div>
   </div>
   <div class="ltx_para" id="S3.SS3.p3">
    <p class="ltx_p" id="S3.SS3.p3.1">
     As demonstrated in the code block above, an instruction-invocation pair consists of two parts: the instruction, which represents the input, and the invocation, which represents the expected output of the LLM. To obtain these forms of data samples, we constructed them from two sources. The first source utilizes components of existing instruction tuning works, such as LLaVA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib24" title="">
       24
      </a>
      ]
     </cite>
     , VideoChat
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib19" title="">
       19
      </a>
      ]
     </cite>
     , and InstructBLIP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib23" title="">
       23
      </a>
      ]
     </cite>
     . However, it should be noted that the instructions in these works primarily fall into two categories: Input-side Alignment Instructions and Reasoning Boosting Instructions. Thus, to generate Output-side Alignment Instructions, which are crucial for the success of ModaVerse, we created specific templates to assist the OpenAI ChatGPT API in producing new instructions on a large scale. Specifically, each query sent to the ChatGPT API comprises three components: Seed Examples, Candidate Descriptions, and Language References (see supplementary for more details).
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p4">
    <p class="ltx_p" id="S3.SS3.p4.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">
      Seed Examples
     </span>
     consist of a set of standard instruction invocation pairs, randomly selected from a manually crafted collection. These examples serve as guides for the ChatGPT API, demonstrating how to generate samples in the given format and providing an illustration of the task.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p5">
    <p class="ltx_p" id="S3.SS3.p5.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">
      Candidate Descriptions
     </span>
     comprise randomly selected text descriptions from the paired datasets, which include descriptions of images, audio, and videos. These descriptions aim to mimic the true inputs, while the ChatGPT API is requested to generate appropriate instructions and invocation details based on these candidates and seed examples.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS3.p6">
    <p class="ltx_p" id="S3.SS3.p6.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p6.1.1">
      Language References
     </span>
     include text descriptions randomly selected from the training set of generative models. These samples serve as a guide for the ChatGPT API to learn the language style of the prompts used in the generative models, helping to generate language-aligned invocation details.
    </p>
   </div>
   <figure class="ltx_table" id="S3.SS3.6">
    <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S3.SS3.6.6" style="width:496.9pt;">
     <div class="ltx_logical-block ltx_minipage ltx_align_bottom" id="S3.SS3.2.2.2" style="width:238.5pt;">
      <div class="ltx_para" id="S3.SS3.2.2.2.p1">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.SS3.1.1.1.1">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S3.SS3.1.1.1.1.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.SS3.1.1.1.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.1.1.1.1.1.2.1">
            Method
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.1.1.1.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.1.1.1.1.1.1.1">
            FID (
            <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.SS3.1.1.1.1.1.1.1.m1.1">
             <semantics id="S3.SS3.1.1.1.1.1.1.1.m1.1a">
              <mo id="S3.SS3.1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.SS3.1.1.1.1.1.1.1.m1.1.1.cmml">
               ↓
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.1.1.1.1.1.1.1.m1.1b">
               <ci id="S3.SS3.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS3.1.1.1.1.1.1.1.m1.1.1">
                ↓
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.1.1.1.1.1.1.1.m1.1c">
               \downarrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S3.SS3.1.1.1.1.2.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.SS3.1.1.1.1.2.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           CogVideo
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib7" title="">
             7
            </a>
            ]
           </cite>
           (NeurIPS’21)
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.1.1.1.1.2.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           27.10
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.1.1.1.1.3.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.1.1.1.1.3.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           GLIDE
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib28" title="">
             28
            </a>
            ]
           </cite>
           (ICML’22)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.1.1.1.1.3.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           12.24
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.1.1.1.1.4.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.1.1.1.1.4.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           CoDi
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib41" title="">
             41
            </a>
            ]
           </cite>
           (NeurIPS’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.1.1.1.1.4.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           11.26
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.1.1.1.1.5.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.1.1.1.1.5.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           SD
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib33" title="">
             33
            </a>
            ]
           </cite>
           (CVPR’22)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.1.1.1.1.5.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           11.21
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.1.1.1.1.6.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.1.1.1.1.6.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           NExT-GPT
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib48" title="">
             48
            </a>
            ]
           </cite>
           (arXiv’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.1.1.1.1.6.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           11.28
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.1.1.1.1.7.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.SS3.1.1.1.1.7.6.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           ModaVerse
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.1.1.1.1.7.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           11.24
          </td>
         </tr>
        </tbody>
       </table>
      </div>
      <figure class="ltx_table ltx_align_center" id="S3.T2">
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_table">
         <span class="ltx_text" id="S3.T2.2.1.1" style="font-size:90%;">
          Table 2
         </span>
         :
        </span>
        <span class="ltx_text" id="S3.T2.3.2" style="font-size:90%;">
         Text-to-image performance on COCO-caption
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib21" title="">
           21
          </a>
          ]
         </cite>
         .
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_logical-block ltx_minipage ltx_align_center ltx_align_bottom" id="S3.SS3.6.6.6" style="width:238.5pt;">
      <div class="ltx_para" id="S3.SS3.6.6.6.p1">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.SS3.5.5.5.3">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S3.SS3.5.5.5.3.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.SS3.5.5.5.3.3.4" style="padding-left:3.4pt;padding-right:3.4pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.5.5.5.3.3.4.1">
            Method
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.3.3.3.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.3.3.3.1.1.1.1">
            B@4 (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.3.3.3.1.1.1.1.m1.1">
             <semantics id="S3.SS3.3.3.3.1.1.1.1.m1.1a">
              <mo id="S3.SS3.3.3.3.1.1.1.1.m1.1.1" stretchy="false" xref="S3.SS3.3.3.3.1.1.1.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.3.3.3.1.1.1.1.m1.1b">
               <ci id="S3.SS3.3.3.3.1.1.1.1.m1.1.1.cmml" xref="S3.SS3.3.3.3.1.1.1.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.3.3.3.1.1.1.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.4.4.4.2.2.2" style="padding-left:3.4pt;padding-right:3.4pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.4.4.4.2.2.2.1">
            METEOR (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.4.4.4.2.2.2.1.m1.1">
             <semantics id="S3.SS3.4.4.4.2.2.2.1.m1.1a">
              <mo id="S3.SS3.4.4.4.2.2.2.1.m1.1.1" stretchy="false" xref="S3.SS3.4.4.4.2.2.2.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.4.4.4.2.2.2.1.m1.1b">
               <ci id="S3.SS3.4.4.4.2.2.2.1.m1.1.1.cmml" xref="S3.SS3.4.4.4.2.2.2.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.4.4.4.2.2.2.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.5.5.5.3.3.3" style="padding-left:3.4pt;padding-right:3.4pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.5.5.5.3.3.3.1">
            CIDEr (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.5.5.5.3.3.3.1.m1.1">
             <semantics id="S3.SS3.5.5.5.3.3.3.1.m1.1a">
              <mo id="S3.SS3.5.5.5.3.3.3.1.m1.1.1" stretchy="false" xref="S3.SS3.5.5.5.3.3.3.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.5.5.5.3.3.3.1.m1.1b">
               <ci id="S3.SS3.5.5.5.3.3.3.1.m1.1.1.cmml" xref="S3.SS3.5.5.5.3.3.3.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.5.5.5.3.3.3.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S3.SS3.5.5.5.3.4.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.SS3.5.5.5.3.4.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">
           Oscar
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib20" title="">
             20
            </a>
            ]
           </cite>
           (ECCV’20)
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.5.5.5.3.4.1.2" style="padding-left:3.4pt;padding-right:3.4pt;">
           36.58
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.5.5.5.3.4.1.3" style="padding-left:3.4pt;padding-right:3.4pt;">
           30.4
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.5.5.5.3.4.1.4" style="padding-left:3.4pt;padding-right:3.4pt;">
           124.12
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.5.5.5.3.5.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.5.5.5.3.5.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">
           BLIP-2
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib18" title="">
             18
            </a>
            ]
           </cite>
           (ICML’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.5.2.2" style="padding-left:3.4pt;padding-right:3.4pt;">
           43.7
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.5.2.3" style="padding-left:3.4pt;padding-right:3.4pt;">
           —
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.5.2.4" style="padding-left:3.4pt;padding-right:3.4pt;">
           145.8
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.5.5.5.3.6.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.5.5.5.3.6.3.1" style="padding-left:3.4pt;padding-right:3.4pt;">
           OFA
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib45" title="">
             45
            </a>
            ]
           </cite>
           (ICML’22)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.6.3.2" style="padding-left:3.4pt;padding-right:3.4pt;">
           44.9
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.6.3.3" style="padding-left:3.4pt;padding-right:3.4pt;">
           32.5
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.6.3.4" style="padding-left:3.4pt;padding-right:3.4pt;">
           154.9
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.5.5.5.3.7.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.5.5.5.3.7.4.1" style="padding-left:3.4pt;padding-right:3.4pt;">
           CoDi
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib41" title="">
             41
            </a>
            ]
           </cite>
           (NeurIPS’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.7.4.2" style="padding-left:3.4pt;padding-right:3.4pt;">
           40.2
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.7.4.3" style="padding-left:3.4pt;padding-right:3.4pt;">
           31.0
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.7.4.4" style="padding-left:3.4pt;padding-right:3.4pt;">
           149.9
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.5.5.5.3.8.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.5.5.5.3.8.5.1" style="padding-left:3.4pt;padding-right:3.4pt;">
           NExT-GPT
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib48" title="">
             48
            </a>
            ]
           </cite>
           (arXiv’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.8.5.2" style="padding-left:3.4pt;padding-right:3.4pt;">
           44.3
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.8.5.3" style="padding-left:3.4pt;padding-right:3.4pt;">
           32.9
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.5.5.5.3.8.5.4" style="padding-left:3.4pt;padding-right:3.4pt;">
           156.7
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.5.5.5.3.9.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.SS3.5.5.5.3.9.6.1" style="padding-left:3.4pt;padding-right:3.4pt;">
           ModaVerse
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.5.5.5.3.9.6.2" style="padding-left:3.4pt;padding-right:3.4pt;">
           43.9
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.5.5.5.3.9.6.3" style="padding-left:3.4pt;padding-right:3.4pt;">
           31.8
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.5.5.5.3.9.6.4" style="padding-left:3.4pt;padding-right:3.4pt;">
           151.4
          </td>
         </tr>
        </tbody>
       </table>
      </div>
      <figure class="ltx_table ltx_align_center" id="S3.T3">
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_table">
         <span class="ltx_text" id="S3.T3.2.1.1" style="font-size:90%;">
          Table 3
         </span>
         :
        </span>
        <span class="ltx_text" id="S3.T3.3.2" style="font-size:90%;">
         Image-to-text performance on COCO-caption data
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib21" title="">
           21
          </a>
          ]
         </cite>
         .
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
   </figure>
   <figure class="ltx_table" id="S3.SS3.12">
    <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S3.SS3.12.6" style="width:496.9pt;">
     <div class="ltx_logical-block ltx_minipage ltx_align_bottom" id="S3.SS3.9.3.3" style="width:238.5pt;">
      <div class="ltx_para" id="S3.SS3.9.3.3.p1">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.SS3.8.2.2.2">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S3.SS3.8.2.2.2.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.SS3.8.2.2.2.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.8.2.2.2.2.3.1">
            Method
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.7.1.1.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.7.1.1.1.1.1.1">
            FD (
            <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.SS3.7.1.1.1.1.1.1.m1.1">
             <semantics id="S3.SS3.7.1.1.1.1.1.1.m1.1a">
              <mo id="S3.SS3.7.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.SS3.7.1.1.1.1.1.1.m1.1.1.cmml">
               ↓
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.7.1.1.1.1.1.1.m1.1b">
               <ci id="S3.SS3.7.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS3.7.1.1.1.1.1.1.m1.1.1">
                ↓
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.7.1.1.1.1.1.1.m1.1c">
               \downarrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.8.2.2.2.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.8.2.2.2.2.2.1">
            IS (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.8.2.2.2.2.2.1.m1.1">
             <semantics id="S3.SS3.8.2.2.2.2.2.1.m1.1a">
              <mo id="S3.SS3.8.2.2.2.2.2.1.m1.1.1" stretchy="false" xref="S3.SS3.8.2.2.2.2.2.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.8.2.2.2.2.2.1.m1.1b">
               <ci id="S3.SS3.8.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.SS3.8.2.2.2.2.2.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.8.2.2.2.2.2.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S3.SS3.8.2.2.2.3.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.SS3.8.2.2.2.3.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           DiffSound
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib51" title="">
             51
            </a>
            ]
           </cite>
           (TASLP’23)
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.8.2.2.2.3.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           47.68
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.8.2.2.2.3.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">
           4.01
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.8.2.2.2.4.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.8.2.2.2.4.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           AudioLDM-S
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib22" title="">
             22
            </a>
            ]
           </cite>
           (ICML’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.8.2.2.2.4.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           29.48
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.8.2.2.2.4.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">
           6.90
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.8.2.2.2.5.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.8.2.2.2.5.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           AudioLDM-L
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib22" title="">
             22
            </a>
            ]
           </cite>
           (ICML’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.8.2.2.2.5.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           23.31
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.8.2.2.2.5.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">
           8.13
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.8.2.2.2.6.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.8.2.2.2.6.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           CoDi
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib41" title="">
             41
            </a>
            ]
           </cite>
           (NeurIPS’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.8.2.2.2.6.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           22.90
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.8.2.2.2.6.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">
           8.77
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.8.2.2.2.7.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.8.2.2.2.7.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           NExT-GPT
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib48" title="">
             48
            </a>
            ]
           </cite>
           (arXiv’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.8.2.2.2.7.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           23.58
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.8.2.2.2.7.5.3" style="padding-left:4.3pt;padding-right:4.3pt;">
           8.35
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.8.2.2.2.8.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.SS3.8.2.2.2.8.6.1" style="padding-left:4.3pt;padding-right:4.3pt;">
           ModaVerse
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.8.2.2.2.8.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">
           23.40
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.8.2.2.2.8.6.3" style="padding-left:4.3pt;padding-right:4.3pt;">
           8.22
          </td>
         </tr>
        </tbody>
       </table>
      </div>
      <figure class="ltx_table ltx_align_center" id="S3.T4">
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_table">
         <span class="ltx_text" id="S3.T4.2.1.1" style="font-size:90%;">
          Table 4
         </span>
         :
        </span>
        <span class="ltx_text" id="S3.T4.3.2" style="font-size:90%;">
         Text-to-audio performance on AudioCaps
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib14" title="">
           14
          </a>
          ]
         </cite>
         .
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_logical-block ltx_minipage ltx_align_center ltx_align_bottom" id="S3.SS3.12.6.6" style="width:238.5pt;">
      <div class="ltx_para" id="S3.SS3.12.6.6.p1">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.SS3.11.5.5.2">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S3.SS3.11.5.5.2.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.SS3.11.5.5.2.2.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.11.5.5.2.2.3.1">
            Method
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.10.4.4.1.1.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.10.4.4.1.1.1.1">
            SPIDEr (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.10.4.4.1.1.1.1.m1.1">
             <semantics id="S3.SS3.10.4.4.1.1.1.1.m1.1a">
              <mo id="S3.SS3.10.4.4.1.1.1.1.m1.1.1" stretchy="false" xref="S3.SS3.10.4.4.1.1.1.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.10.4.4.1.1.1.1.m1.1b">
               <ci id="S3.SS3.10.4.4.1.1.1.1.m1.1.1.cmml" xref="S3.SS3.10.4.4.1.1.1.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.10.4.4.1.1.1.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.11.5.5.2.2.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.11.5.5.2.2.2.1">
            CIDEr (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.11.5.5.2.2.2.1.m1.1">
             <semantics id="S3.SS3.11.5.5.2.2.2.1.m1.1a">
              <mo id="S3.SS3.11.5.5.2.2.2.1.m1.1.1" stretchy="false" xref="S3.SS3.11.5.5.2.2.2.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.11.5.5.2.2.2.1.m1.1b">
               <ci id="S3.SS3.11.5.5.2.2.2.1.m1.1.1.cmml" xref="S3.SS3.11.5.5.2.2.2.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.11.5.5.2.2.2.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S3.SS3.11.5.5.2.3.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.SS3.11.5.5.2.3.1.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           AudioCaps
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib14" title="">
             14
            </a>
            ]
           </cite>
           (NAACL’19)
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.11.5.5.2.3.1.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.369
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.11.5.5.2.3.1.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.593
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.11.5.5.2.4.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.11.5.5.2.4.2.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           BART
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib9" title="">
             9
            </a>
            ]
           </cite>
           (DCASE’21)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.11.5.5.2.4.2.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.465
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.11.5.5.2.4.2.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.753
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.11.5.5.2.5.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.11.5.5.2.5.3.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           AL-MixGen
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib15" title="">
             15
            </a>
            ]
           </cite>
           (ArXiv’22)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.11.5.5.2.5.3.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.466
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.11.5.5.2.5.3.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.755
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.11.5.5.2.6.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.11.5.5.2.6.4.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           CoDi
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib41" title="">
             41
            </a>
            ]
           </cite>
           (NeurIPS’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.11.5.5.2.6.4.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.480
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.11.5.5.2.6.4.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.789
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.11.5.5.2.7.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.11.5.5.2.7.5.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           NExT-GPT
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib48" title="">
             48
            </a>
            ]
           </cite>
           (arXiv’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.11.5.5.2.7.5.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.521
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.11.5.5.2.7.5.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.802
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.11.5.5.2.8.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.SS3.11.5.5.2.8.6.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           ModaVerse
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.11.5.5.2.8.6.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.494
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.11.5.5.2.8.6.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.792
          </td>
         </tr>
        </tbody>
       </table>
      </div>
      <figure class="ltx_table ltx_align_center" id="S3.T5">
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_table">
         <span class="ltx_text" id="S3.T5.2.1.1" style="font-size:90%;">
          Table 5
         </span>
         :
        </span>
        <span class="ltx_text" id="S3.T5.3.2" style="font-size:90%;">
         Audio-to-text performance on AudioCaps
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib14" title="">
           14
          </a>
          ]
         </cite>
         .
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
   </figure>
   <figure class="ltx_table" id="S3.SS3.18">
    <div class="ltx_logical-block ltx_minipage ltx_align_middle" id="S3.SS3.18.6" style="width:496.9pt;">
     <div class="ltx_logical-block ltx_minipage ltx_align_bottom" id="S3.SS3.15.3.3" style="width:238.5pt;">
      <div class="ltx_para" id="S3.SS3.15.3.3.p1">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.SS3.14.2.2.2">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S3.SS3.14.2.2.2.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.SS3.14.2.2.2.2.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.14.2.2.2.2.3.1">
            Method
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.13.1.1.1.1.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.13.1.1.1.1.1.1">
            FID (
            <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.SS3.13.1.1.1.1.1.1.m1.1">
             <semantics id="S3.SS3.13.1.1.1.1.1.1.m1.1a">
              <mo id="S3.SS3.13.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.SS3.13.1.1.1.1.1.1.m1.1.1.cmml">
               ↓
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.13.1.1.1.1.1.1.m1.1b">
               <ci id="S3.SS3.13.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS3.13.1.1.1.1.1.1.m1.1.1">
                ↓
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.13.1.1.1.1.1.1.m1.1c">
               \downarrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.14.2.2.2.2.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.14.2.2.2.2.2.1">
            CLIPSIM (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.14.2.2.2.2.2.1.m1.1">
             <semantics id="S3.SS3.14.2.2.2.2.2.1.m1.1a">
              <mo id="S3.SS3.14.2.2.2.2.2.1.m1.1.1" stretchy="false" xref="S3.SS3.14.2.2.2.2.2.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.14.2.2.2.2.2.1.m1.1b">
               <ci id="S3.SS3.14.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.SS3.14.2.2.2.2.2.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.14.2.2.2.2.2.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S3.SS3.14.2.2.2.3.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.SS3.14.2.2.2.3.1.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           CogVideo
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib11" title="">
             11
            </a>
            ]
           </cite>
           (NeurIPS’21)
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.14.2.2.2.3.1.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           23.59
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.14.2.2.2.3.1.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.2631
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.14.2.2.2.4.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.14.2.2.2.4.2.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           MakeVideo
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib13" title="">
             13
            </a>
            ]
           </cite>
           (ICML’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.4.2.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           13.17
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.4.2.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.3049
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.14.2.2.2.5.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.14.2.2.2.5.3.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           Latent-VDM
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib33" title="">
             33
            </a>
            ]
           </cite>
           (CVPR’22)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.5.3.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           14.25
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.5.3.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.2756
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.14.2.2.2.6.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.14.2.2.2.6.4.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           Latent-Shift
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib2" title="">
             2
            </a>
            ]
           </cite>
           (arXiv’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.6.4.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           15.23
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.6.4.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.2773
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.14.2.2.2.7.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.14.2.2.2.7.5.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           CoDi
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib41" title="">
             41
            </a>
            ]
           </cite>
           (NeurIPS’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.7.5.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           —
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.7.5.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.2890
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.14.2.2.2.8.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.14.2.2.2.8.6.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           NExT-GPT
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib48" title="">
             48
            </a>
            ]
           </cite>
           (arXiv’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.8.6.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           13.04
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.14.2.2.2.8.6.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.3085
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.14.2.2.2.9.7">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.SS3.14.2.2.2.9.7.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           ModaVerse
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.14.2.2.2.9.7.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           13.35
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.14.2.2.2.9.7.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           0.3014
          </td>
         </tr>
        </tbody>
       </table>
      </div>
      <figure class="ltx_table ltx_align_center" id="S3.T6">
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_table">
         <span class="ltx_text" id="S3.T6.2.1.1" style="font-size:90%;">
          Table 6
         </span>
         :
        </span>
        <span class="ltx_text" id="S3.T6.3.2" style="font-size:90%;">
         Text-to-video performance on MSR-VTT
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib50" title="">
           50
          </a>
          ]
         </cite>
         .
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_logical-block ltx_minipage ltx_align_center ltx_align_bottom" id="S3.SS3.18.6.6" style="width:223.6pt;">
      <div class="ltx_para" id="S3.SS3.18.6.6.p1">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.SS3.17.5.5.2">
        <thead class="ltx_thead">
         <tr class="ltx_tr" id="S3.SS3.17.5.5.2.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.SS3.17.5.5.2.2.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.17.5.5.2.2.3.1">
            Method
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.16.4.4.1.1.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.16.4.4.1.1.1.1">
            B@4 (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.16.4.4.1.1.1.1.m1.1">
             <semantics id="S3.SS3.16.4.4.1.1.1.1.m1.1a">
              <mo id="S3.SS3.16.4.4.1.1.1.1.m1.1.1" stretchy="false" xref="S3.SS3.16.4.4.1.1.1.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.16.4.4.1.1.1.1.m1.1b">
               <ci id="S3.SS3.16.4.4.1.1.1.1.m1.1.1.cmml" xref="S3.SS3.16.4.4.1.1.1.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.16.4.4.1.1.1.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
          <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.SS3.17.5.5.2.2.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           <span class="ltx_text ltx_font_bold" id="S3.SS3.17.5.5.2.2.2.1">
            METEOR (
            <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.SS3.17.5.5.2.2.2.1.m1.1">
             <semantics id="S3.SS3.17.5.5.2.2.2.1.m1.1a">
              <mo id="S3.SS3.17.5.5.2.2.2.1.m1.1.1" stretchy="false" xref="S3.SS3.17.5.5.2.2.2.1.m1.1.1.cmml">
               ↑
              </mo>
              <annotation-xml encoding="MathML-Content" id="S3.SS3.17.5.5.2.2.2.1.m1.1b">
               <ci id="S3.SS3.17.5.5.2.2.2.1.m1.1.1.cmml" xref="S3.SS3.17.5.5.2.2.2.1.m1.1.1">
                ↑
               </ci>
              </annotation-xml>
              <annotation encoding="application/x-tex" id="S3.SS3.17.5.5.2.2.2.1.m1.1c">
               \uparrow
              </annotation>
             </semantics>
            </math>
            )
           </span>
          </th>
         </tr>
        </thead>
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S3.SS3.17.5.5.2.3.1">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.SS3.17.5.5.2.3.1.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           ORG-TRL
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib56" title="">
             56
            </a>
            ]
           </cite>
           (CVPR’20)
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.17.5.5.2.3.1.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           43.6
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.SS3.17.5.5.2.3.1.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           28.8
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.17.5.5.2.4.2">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.17.5.5.2.4.2.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           GIT
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib44" title="">
             44
            </a>
            ]
           </cite>
           (TMLR’22)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.17.5.5.2.4.2.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           54.8
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.17.5.5.2.4.2.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           33.1
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.17.5.5.2.5.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.17.5.5.2.5.3.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           mPLUG-2
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib49" title="">
             49
            </a>
            ]
           </cite>
           (ICML’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.17.5.5.2.5.3.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           57.8
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.17.5.5.2.5.3.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           34.9
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.17.5.5.2.6.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.17.5.5.2.6.4.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           CoDi
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib41" title="">
             41
            </a>
            ]
           </cite>
           (NeurIPS’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.17.5.5.2.6.4.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           52.1
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.17.5.5.2.6.4.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           32.5
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.17.5.5.2.7.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.SS3.17.5.5.2.7.5.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           NExT-GPT
           <cite class="ltx_cite ltx_citemacro_cite">
            [
            <a class="ltx_ref" href="#bib.bib48" title="">
             48
            </a>
            ]
           </cite>
           (arXiv’23)
          </th>
          <td class="ltx_td ltx_align_center" id="S3.SS3.17.5.5.2.7.5.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           58.4
          </td>
          <td class="ltx_td ltx_align_center" id="S3.SS3.17.5.5.2.7.5.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           38.5
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.SS3.17.5.5.2.8.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.SS3.17.5.5.2.8.6.1" style="padding-left:3.1pt;padding-right:3.1pt;">
           ModaVerse
          </th>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.17.5.5.2.8.6.2" style="padding-left:3.1pt;padding-right:3.1pt;">
           56.5
          </td>
          <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.SS3.17.5.5.2.8.6.3" style="padding-left:3.1pt;padding-right:3.1pt;">
           35.2
          </td>
         </tr>
        </tbody>
       </table>
      </div>
      <figure class="ltx_table ltx_align_center" id="S3.T7">
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_table">
         <span class="ltx_text" id="S3.T7.2.1.1" style="font-size:90%;">
          Table 7
         </span>
         :
        </span>
        <span class="ltx_text" id="S3.T7.3.2" style="font-size:90%;">
         Video-to-text performance on MSR-VTT
         <cite class="ltx_cite ltx_citemacro_cite">
          [
          <a class="ltx_ref" href="#bib.bib50" title="">
           50
          </a>
          ]
         </cite>
         .
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
   </figure>
   <div class="ltx_para" id="S3.SS3.p7">
    <p class="ltx_p" id="S3.SS3.p7.1">
     For training, we use LLaMA2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib43" title="">
       43
      </a>
      ]
     </cite>
     as the foundation LLM, the trainable parts of the proposed ModaVerse (see Figure
     <a class="ltx_ref" href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     ) consist only of three linear layers and the LoRA adaptor of the LLM. Together, these components comprise about 40M trainable parameters. Table
     <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.2 I/O Alignment ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     compares the training complexity of ModaVerse with that of some recently proposed MLLMs. It shows that the proposed method enjoys lower training complexity.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="S3.F4.g1" src="/html/2401.06395/assets/x4.png" width="392"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">
       Figure 4
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">
      Qualitative examples of the proposed ModaVerse interpreting and producing data presented in combinations of various modalities. Blue and Red dashed boxes represent input and output respectively.
     </span>
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Quantitative Results
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.2">
     To evaluate the proposed ModaVerse, we follow previous works
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib41" title="">
       41
      </a>
      ]
     </cite>
     to assess the model’s understanding ability (x
     <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1">
      <semantics id="S4.SS1.p1.1.m1.1a">
       <mo id="S4.SS1.p1.1.m1.1.1" stretchy="false" xref="S4.SS1.p1.1.m1.1.1.cmml">
        →
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b">
        <ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">
         →
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">
        \rightarrow
       </annotation>
      </semantics>
     </math>
     text) and its generation ability (text
     <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1">
      <semantics id="S4.SS1.p1.2.m2.1a">
       <mo id="S4.SS1.p1.2.m2.1.1" stretchy="false" xref="S4.SS1.p1.2.m2.1.1.cmml">
        →
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b">
        <ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">
         →
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">
        \rightarrow
       </annotation>
      </semantics>
     </math>
     x), where x can be image, audio, and video. Tables
     <a class="ltx_ref" href="#S3.T2" title="Table 2 ‣ 3.3 Instruction Generation and Training ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     and
     <a class="ltx_ref" href="#S3.T3" title="Table 3 ‣ 3.3 Instruction Generation and Training ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     illustrate the text-to-image and image-to-text performance on the COCO caption dataset
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib21" title="">
       21
      </a>
      ]
     </cite>
     . The results demonstrate that our method achieved an 11.28 FID score in the image generation task, comparable to recent methods. In terms of image understanding capability, the proposed ModaVerse outperforms the any-to-any diffuser CoDi
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib41" title="">
       41
      </a>
      ]
     </cite>
     in both B@4 and METEOR metrics, though it is slightly lower than NExT-GPT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib48" title="">
       48
      </a>
      ]
     </cite>
     and OFA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib45" title="">
       45
      </a>
      ]
     </cite>
     . The text-to-audio and audio-to-text performances on the AudioCaps dataset
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib14" title="">
       14
      </a>
      ]
     </cite>
     are showed in Tables
     <a class="ltx_ref" href="#S3.T4" title="Table 4 ‣ 3.3 Instruction Generation and Training ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     and
     <a class="ltx_ref" href="#S3.T5" title="Table 5 ‣ 3.3 Instruction Generation and Training ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     . The proposed method outperforms NExT-GPT and is narrowly eclipsed by CoDi. In audio captioning, our approach secures the second-best performance, rivaling the state-of-the-art models. Tables
     <a class="ltx_ref" href="#S3.T6" title="Table 6 ‣ 3.3 Instruction Generation and Training ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     and
     <a class="ltx_ref" href="#S3.T7" title="Table 7 ‣ 3.3 Instruction Generation and Training ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     showcase the text-to-video and video-to-text performances on MSR-VTT
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib50" title="">
       50
      </a>
      ]
     </cite>
     . Our method achieved a 13.35 FID score and a 0.3014 CLIPSIM score in video generation, demonstrating parity with top-tier methods. Similarly, our approach shows competitive results in video-to-text tasks, as evidenced by its B@4 and METEOR scores.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     Although our method does not outperform all state-of-the-art methods (including two concurrent arXiv submissions) across the six benchmarks, it is important to note the efficiency of the proposed ModaVerse. First, our method is capable of converting a variety of modalities, whereas some state-of-the-art methods, such as SD
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib33" title="">
       33
      </a>
      ]
     </cite>
     and OFA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib45" title="">
       45
      </a>
      ]
     </cite>
     , are specifically designed for single-route conversions like text-to-image. Second, as Table
     <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.2 I/O Alignment ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     illustrates, ModaVerse benefits from a more efficient training paradigm. It requires less data and fewer computational resources. Specifically, in contrast to NExT-GPT which necessitates three stages to independently train the projection layers, our method streamlines this process into a single stage. Regarding training data, our approach uses less than 2% of the data volume required by Emu
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib16" title="">
       16
      </a>
      ]
     </cite>
     and BLIP-2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib17" title="">
       17
      </a>
      ]
     </cite>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Qualitative Results
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Since publicly available datasets are limited to certain common modality combinations, such as image-to-text and text-to-video, this limitation may not comprehensively capture the full extent of ModaVerse’s capabilities. Therefore, Figure
     <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.3 Instruction Generation and Training ‣ 3 ModaVerse ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     showcases various qualitative results of ModaVerse across different modalities. For instance, examples (a), (c), (f), and (l) emphasize the model’s conditioned generative abilities. In addition, examples (g), (h), and (i) demonstrate its proficiency in answering questions with inputs from a variety of modalities. Moreover, examples (d) and (e) demonstrate the potential for style transfer.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Limitations and Failure Cases
   </h3>
   <figure class="ltx_figure" id="S4.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="129" id="S4.F5.g1" src="/html/2401.06395/assets/x5.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">
       Figure 5
      </span>
      :
     </span>
     <span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">
      Failure cases of ModaVerse. (a) The model can only generate entirely new images and cannot modify the original pixels. (b) The model tends to generate irrelevant outputs in the absence of language instructions during the input phase.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     In exploring the capabilities of ModaVerse, Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.3 Limitations and Failure Cases ‣ 4 Experiments ‣ ModaVerse: Efficiently Transforming Modalities with LLMs">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     includes some challenging scenarios where the model’s performance can be further enhanced. Specifically, Example (a) illustrates a current limitation of the model in image editing tasks, where it fails to retain the original background and layout of the input images. Instead of modifying the existing image, the model generates an entirely new one. This limitation highlights a specific challenge in our approach, particularly for tasks that require fidelity to the original image’s resolution and details. However, this can potentially be addressed by integrating an additional editing model into the model zoo at the final response generation stage, a development we leave for future work. Another notable case is Example (b), where, in the absence of language clues at the input phase, the model tends to produce random, irrelevant outputs. This issue arises because the instruction-following trained model relies on given language instructions for reasoning out the expected response. Without such clues, it may struggle to produce appropriate responses.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this paper, we have presented ModaVerse, a MLLM capable of interpreting and generating data in various modalities. This model diverges from existing MLLM frameworks by adopting a synergistic approach that merges adaptor training with the LLM-as-agent methodology. By employing adaptors, ModaVerse effectively aligns the text-based LLM with multi-modal inputs through a set of linear projection layers. This enhances its capability to interpret a diverse array of input modalities. On the output side, instead of training additional projection layers to align the output space with generative models, we treat the LLM as an agent. This agent produces a meta-response containing invocation details, which are then parsed to activate generative models for generating the final response. This integrative Adaptor+Agent training paradigm not only streamlines the complex multi-stage feature alignment process but also significantly boosts the efficiency of the training process, offering an alternative for the training of MLLMs.
For future work, we aim to address the current framework’s limitations and weaknesses, such as preserving the original layout information of inputs, thereby broadening its applicability to scenarios requiring original information, like image and video editing.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Alayrac et al. [2022]
    </span>
    <span class="ltx_bibblock">
     Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.
    </span>
    <span class="ltx_bibblock">
     Flamingo: a visual language model for few-shot learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      NeurIPS
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     An et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin.
    </span>
    <span class="ltx_bibblock">
     Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      arXiv preprint arXiv:2304.08477
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Antol et al. [2015]
    </span>
    <span class="ltx_bibblock">
     Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
    </span>
    <span class="ltx_bibblock">
     Vqa: Visual question answering.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      ICCV
     </em>
     , 2015.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. [2020]
    </span>
    <span class="ltx_bibblock">
     Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
    </span>
    <span class="ltx_bibblock">
     Language models are few-shot learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      NeurIPS
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. [2023a]
    </span>
    <span class="ltx_bibblock">
     Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
    </span>
    <span class="ltx_bibblock">
     Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2310.09478
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. [2023b]
    </span>
    <span class="ltx_bibblock">
     Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al.
    </span>
    <span class="ltx_bibblock">
     Pali: A jointly-scaled multilingual language-image model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      ICLR
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ding et al. [2021]
    </span>
    <span class="ltx_bibblock">
     Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al.
    </span>
    <span class="ltx_bibblock">
     Cogview: Mastering text-to-image generation via transformers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      NeurIPS
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dong et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al.
    </span>
    <span class="ltx_bibblock">
     Dreamllm: Synergistic multimodal comprehension and creation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      arXiv preprint arXiv:2309.11499
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gontier et al. [2021]
    </span>
    <span class="ltx_bibblock">
     Félix Gontier, Romain Serizel, and Christophe Cerisara.
    </span>
    <span class="ltx_bibblock">
     Automated audio captioning by fine-tuning bart with audioset tags.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      DCASE
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Han et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al.
    </span>
    <span class="ltx_bibblock">
     Imagebind-llm: Multi-modality instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      arXiv preprint arXiv:2309.03905
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hong et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang.
    </span>
    <span class="ltx_bibblock">
     Cogvideo: Large-scale pretraining for text-to-video generation via transformers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      ICLR
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. [2022]
    </span>
    <span class="ltx_bibblock">
     Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
    </span>
    <span class="ltx_bibblock">
     Lora: Low-rank adaptation of large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      ICLR
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao.
    </span>
    <span class="ltx_bibblock">
     Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      ICML
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim et al. [2019]
    </span>
    <span class="ltx_bibblock">
     Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim.
    </span>
    <span class="ltx_bibblock">
     Audiocaps: Generating captions for audios in the wild.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      NAACL
     </em>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim et al. [2022]
    </span>
    <span class="ltx_bibblock">
     Eungbeom Kim, Jinhee Kim, Yoori Oh, Kyungsu Kim, Minju Park, Jaeheon Sim, Jinwoo Lee, and Kyogu Lee.
    </span>
    <span class="ltx_bibblock">
     Improving audio-language learning with mixgen and multi-level test-time augmentation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2210.17143
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Koh et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.
    </span>
    <span class="ltx_bibblock">
     Generating images with multimodal language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      NeurIPS
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. [2023a]
    </span>
    <span class="ltx_bibblock">
     Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
    </span>
    <span class="ltx_bibblock">
     Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      ICML
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. [2023b]
    </span>
    <span class="ltx_bibblock">
     Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi.
    </span>
    <span class="ltx_bibblock">
     BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      ICML
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. [2023c]
    </span>
    <span class="ltx_bibblock">
     KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
    </span>
    <span class="ltx_bibblock">
     Videochat: Chat-centric video understanding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      arXiv preprint arXiv:2305.06355
     </em>
     , 2023c.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. [2020]
    </span>
    <span class="ltx_bibblock">
     Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.
    </span>
    <span class="ltx_bibblock">
     Oscar: Object-semantics aligned pre-training for vision-language tasks.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      ECCV
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lin et al. [2014]
    </span>
    <span class="ltx_bibblock">
     Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.
    </span>
    <span class="ltx_bibblock">
     Microsoft coco: Common objects in context.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      ECCV
     </em>
     . Springer, 2014.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. [2023a]
    </span>
    <span class="ltx_bibblock">
     Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley.
    </span>
    <span class="ltx_bibblock">
     Audioldm: Text-to-audio generation with latent diffusion models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      ICML
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. [2023b]
    </span>
    <span class="ltx_bibblock">
     Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
    </span>
    <span class="ltx_bibblock">
     Improved baselines with visual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2310.03744
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. [2023c]
    </span>
    <span class="ltx_bibblock">
     Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    </span>
    <span class="ltx_bibblock">
     Visual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      NeurIPS
     </em>
     , 2023c.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. [2019]
    </span>
    <span class="ltx_bibblock">
     Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
    </span>
    <span class="ltx_bibblock">
     Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      NeurIPS
     </em>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.
    </span>
    <span class="ltx_bibblock">
     Chameleon: Plug-and-play compositional reasoning with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      NeurIPS
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Luo et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan.
    </span>
    <span class="ltx_bibblock">
     Videofusion: Decomposed diffusion models for high-quality video generation.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      CVPR
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nichol et al. [2022]
    </span>
    <span class="ltx_bibblock">
     Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.
    </span>
    <span class="ltx_bibblock">
     Glide: Towards photorealistic image generation and editing with text-guided diffusion models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      ICML
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI [2023]
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     Chatgpt.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" target="_blank" title="">
      https://openai.com/blog/chatgpt
     </a>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. [2018]
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    </span>
    <span class="ltx_bibblock">
     Improving language understanding by generative pre-training.
    </span>
    <span class="ltx_bibblock">
     2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. [2019]
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
    </span>
    <span class="ltx_bibblock">
     Language models are unsupervised multitask learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      OpenAI blog
     </em>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. [2021]
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
    </span>
    <span class="ltx_bibblock">
     Learning transferable visual models from natural language supervision.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      ICML
     </em>
     . PMLR, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rombach et al. [2022]
    </span>
    <span class="ltx_bibblock">
     Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
    </span>
    <span class="ltx_bibblock">
     High-resolution image synthesis with latent diffusion models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      CVPR
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      NeurIPS
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      NeurIPS
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Su et al. [2020]
    </span>
    <span class="ltx_bibblock">
     Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
    </span>
    <span class="ltx_bibblock">
     Vl-bert: Pre-training of generic visual-linguistic representations.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      ICLR
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Su et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.
    </span>
    <span class="ltx_bibblock">
     Pandagpt: One model to instruction-follow them all.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      arXiv preprint arXiv:2305.16355
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al. [2023a]
    </span>
    <span class="ltx_bibblock">
     Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
    </span>
    <span class="ltx_bibblock">
     Eva-clip: Improved training techniques for clip at scale.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      arXiv preprint arXiv:2303.15389
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al. [2023b]
    </span>
    <span class="ltx_bibblock">
     Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
    </span>
    <span class="ltx_bibblock">
     Generative pretraining in multimodality.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      arXiv preprint arXiv:2307.05222
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Surís et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Dídac Surís, Sachit Menon, and Carl Vondrick.
    </span>
    <span class="ltx_bibblock">
     Vipergpt: Visual inference via python execution for reasoning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      ICCV
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tang et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal.
    </span>
    <span class="ltx_bibblock">
     Any-to-any generation via composable diffusion.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      NeurIPS
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. [2023a]
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      arXiv preprint arXiv:2302.13971
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. [2023b]
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open foundation and fine-tuned chat models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      arXiv preprint arXiv:2307.09288
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. [2022a]
    </span>
    <span class="ltx_bibblock">
     Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
    </span>
    <span class="ltx_bibblock">
     GIT: A generative image-to-text transformer for vision and language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      TMLR
     </em>
     , 2022a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. [2022b]
    </span>
    <span class="ltx_bibblock">
     Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.
    </span>
    <span class="ltx_bibblock">
     OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      ICML
     </em>
     , 2022b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Xinyu Wang, Bohan Zhuang, and Qi Wu.
    </span>
    <span class="ltx_bibblock">
     Switchgpt: Adapting large language models for non-text outputs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      arXiv preprint arXiv:2309.07623
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. [2022]
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.
    </span>
    <span class="ltx_bibblock">
     Emergent abilities of large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      TMLR
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua.
    </span>
    <span class="ltx_bibblock">
     Next-gpt: Any-to-any multimodal llm.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      arXiv preprint arXiv:2309.05519
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al.
    </span>
    <span class="ltx_bibblock">
     mplug-2: A modularized multi-modal foundation model across text, image and video.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      ICML
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. [2016]
    </span>
    <span class="ltx_bibblock">
     Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
    </span>
    <span class="ltx_bibblock">
     Msr-vtt: A large video description dataset for bridging video and language.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      CVPR
     </em>
     , pages 5288–5296, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. [2023a]
    </span>
    <span class="ltx_bibblock">
     Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.
    </span>
    <span class="ltx_bibblock">
     Diffsound: Discrete diffusion model for text-to-sound generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      TASLP
     </em>
     , 2023a.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. [2023b]
    </span>
    <span class="ltx_bibblock">
     Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.
    </span>
    <span class="ltx_bibblock">
     The dawn of lmms: Preliminary explorations with gpt-4v (ision).
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">
      arXiv preprint arXiv:2309.17421
     </em>
     , 2023b.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. [2023c]
    </span>
    <span class="ltx_bibblock">
     Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang.
    </span>
    <span class="ltx_bibblock">
     Mm-react: Prompting chatgpt for multimodal reasoning and action.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">
      arXiv preprint arXiv:2303.11381
     </em>
     , 2023c.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al.
    </span>
    <span class="ltx_bibblock">
     Scaling autoregressive multi-modal models: Pretraining and instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">
      arXiv preprint arXiv:2309.02591
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al.
    </span>
    <span class="ltx_bibblock">
     Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">
      arXiv preprint arXiv:2309.15112
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. [2020]
    </span>
    <span class="ltx_bibblock">
     Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and Zheng-Jun Zha.
    </span>
    <span class="ltx_bibblock">
     Object relational graph with teacher-recommended learning for video captioning.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">
      CVPR
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
    </span>
    <span class="ltx_bibblock">
     Minigpt-4: Enhancing vision-language understanding with advanced large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">
      arXiv preprint arXiv:2304.10592
     </em>
     , 2023.
    </span>
   </li>
  </ul>
 </section>
</article>
