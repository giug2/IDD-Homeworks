<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.05076] Towards Scalable Wireless Federated Learning: Challenges and Solutions</title><meta property="og:description" content="The explosive growth of smart devices (e.g., mobile phones, vehicles, drones) with sensing, communication, and computation capabilities gives rise to an unprecedented amount of data.
The generated massive data together‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Scalable Wireless Federated Learning: Challenges and Solutions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Scalable Wireless Federated Learning: Challenges and Solutions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.05076">

<!--Generated on Wed Feb 28 02:21:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Scalable Wireless Federated Learning: Challenges and Solutions
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong Zhou, Yuanming Shi, Haibo Zhou, Jingjing Wang, Liqun Fu, and Yang Yang 
<br class="ltx_break">
</span><span class="ltx_author_notes">Y. Zhou and Y. Shi (corresponding author) are with ShanghaiTech University. H. Zhou is with Nanjing University. J. Wang is with Beihang University. L. Fu is with Xiameng University. Y. Yang is with Hong Kong University of Science and Technology (Guangzhou).
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The explosive growth of smart devices (e.g., mobile phones, vehicles, drones) with sensing, communication, and computation capabilities gives rise to an unprecedented amount of data.
The generated massive data together with the rapid advancement of machine learning (ML) techniques spark a variety of intelligent applications.
To distill intelligence for supporting these applications, federated learning (FL) emerges as an effective distributed ML framework, given its potential to enable privacy-preserving model training at the network edge.
In this article, we discuss the challenges and solutions of achieving scalable wireless FL from the perspectives of both network design and resource orchestration.
For network design, we discuss how task-oriented model aggregation affects the performance of wireless FL, followed by proposing effective wireless techniques to enhance the communication scalability via reducing the model aggregation distortion and improving the device participation.
For resource orchestration, we identify the limitations of the existing optimization-based algorithms and propose three task-oriented learning algorithms to enhance the algorithmic scalability via achieving computation-efficient resource allocation for wireless FL.
We highlight several potential research issues that deserve further study.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the commercial deployment of 5G systems worldwide, researchers from both academia and industry have kicked off to study the vision, drivers, use cases, and system requirements of 6G.
Among many others, ubiquitous intelligence is envisioned to be a key feature of 6G and support various intelligent applications (e.g., smart city, autonomous driving) across the network coverage area.
For intelligence distillation via centralized learning, an unprecedented volume of data generated by massive devices should be transferred to a central server, which may lead to large communication burdens and severe privacy leakages.
Thus, edge artificial intelligence (AI) emerges as a promising technique for enabling scalable and trustworthy model training at the network edge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning (FL) is recognized as an effective distributed machine learning (ML) framework, given its potential to enable privacy-preserving edge intelligence distillation while keeping the data locally <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
According to the principle of FL, multiple devices collaboratively train a global ML model, where only the model parameters are communicated.
Specifically, local training and model aggregation are two essential steps that are repeatedly performed, as shown in Fig. <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ Towards Scalable Wireless Federated Learning: Challenges and Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
In the local training step, each device updates its local model according to the current global model and the local dataset.
In the model aggregation step, an edge server aggregates the locally trained models to generate a new global model, which is then disseminated to the devices for the next round of training.
By exploiting geographically distributed computing power and multi-modal data, FL pushes intelligence to the network edge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To facilitate wireless FL, the following three challenges need to be tackled.
First, wireless FL relies on the frequent exchange of model parameters, which are usually high-dimensional.
With limited radio resources, only partial devices can participate in each of the communication rounds, thereby limiting the amount of data to be exploited and degrading the learning performance.
Second, because of wireless channel impairment, the model parameters transmitted over fading channels suffer from distortion that degrades the model aggregation accuracy.
Third, time-varying wireless channels and dynamic network topology lead to the communication straggler issue, which reduces the learning efficiency.
In general, more devices participating in FL facilitates the exploitation of more data, but aggravates the model distortion and communication straggler issues due to the occurrence of devices with poor channel conditions, which limits the communication scalability.
To break this dilemma, i.e., improving the communication scalability of wireless FL, it is essential to design task-oriented communication techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Involving a large number of participating devices complicates the resource allocation due to the augmented dimension of optimization problems and the scarce radio resources, which demands efficient resource allocation.
Meanwhile, because of the unique features of wireless FL (e.g., multiple communication rounds), the conventional optimization-based resource allocation algorithms achieve a high spectrum efficiency at the cost of suffering from high computation complexity as well as poor algorithmic scalability and generalizability.
Learning to optimize has the potential to tackle the limitations of optimization-based algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Specifically, by exploiting the universal approximation capability of deep neural networks (DNN), learning-based algorithms establish a direct mapping between channel conditions and resource allocation decisions.
However, data-driven learning algorithms have the limitations of relying on a high volume of training samples, converging slowly, and lacking of interpretability.
This motivates the development of task-oriented learning algorithms that exploit the benefits of both optimization-based and data-driven learning algorithms to enable scalable resource allocation for wireless FL.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this article, we present a unified framework to enhance the scalability of wireless FL from two perspectives, i.e., network design and resource orchestration, by exploiting wireless for AI and AI for wireless, respectively.
On one hand, we delineate the primary impediments to the performance of wireless FL (e.g., inevitable model distortion, insufficient data exploitation), and develop effective wireless techniques to address these issues, thereby augmenting the scalability of task-oriented model aggregation.
On the other hand, we put forth several task-oriented learning algorithms that exploit domain knowledge (e.g., mathematical model, network topology) and incorporate the goal (e.g., convergence optimality and rate) into the algorithm design to enable computation-efficient resource allocation for wireless FL, thereby improving the algorithmic scalability.
By unifying the scalable resource allocation techniques with scalable network design, our proposed framework has the potential to support both communication- and computation-efficient intelligent services.
We also highlight several open research issues that deserve further study.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2310.05076/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="163" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">An illustration of the training process of FL.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Network Design for Wireless FL</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we discuss how task-oriented model aggregation over wireless networks affects the performance of FL and propose several wireless techniques to improve communication scalability.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Model aggregation plays an essential role in distilling intelligence from multiple devices by aggregating their model parameters in each communication round.
With appropriately designed model aggregation frequency and weights, many challenges of FL, e.g., communication bottleneck, system heterogeneity, and statistical heterogeneity, can be alleviated.
However, due to wireless channel impairment and limited radio resources, implementing model aggregation over multiple-access fading channels suffers from inevitable distortion, including model aggregation errors due to deep fading and co-channel interference, and insufficient data exploitation due to limited device participation.
These issues are critical as the accuracy of model aggregation and the amount of data exploited in each round determine the convergence optimality of FL.
Below, we first elaborate how task-oriented communications can be applied for model aggregation and then present effective techniques that alleviate model aggregation distortion and improve data exploitation for enhancing communication scalability of wireless FL.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2310.05076/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="297" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Different network architectures for FL over wireless networks.</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Propagation Environment Reconfiguration</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">With conventional orthogonal multiple access schemes, the edge server performs model aggregation until the model updates from all participating devices are successfully received, which incurs high transmission delay.
However, to perform the global model aggregation, the edge server only requires a specific function of local model updates, rather than each of the local model updates.
Over-the-air computation (AirComp) as a task-oriented communication scheme receives much attention, given its capability of facilitating the concurrent transmission of multiple devices over the same channel and enabling the edge server to directly receive a specific aggregation of local model updates.
Such a task-oriented model aggregation scheme ensures that the transmission delay does not scale with the number of participating devices.
Note that time synchronization is essential for achieving the desired signal superposition via AirComp, which can be realized by advancing or retarding the signal transmission from different devices according to the timing advance technique.
Besides, a time-triggered synchronization strategy can be adopted to balance the computation and communication efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
However, for AirComp-assisted FL, the communication straggler enlarges the model aggregation error.
This is because all participating devices should adjust their transmit powers to align the signal magnitude at the edge server.
Hence, the devices with poor channel conditions, referred to as communication stragglers, lower the magnitude to be aligned at the edge server due to limited transmit power, thereby degrading the aggregation accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
The dynamic network topology further aggravates the communication straggler issue and complicates the transceiver design.
Fortunately, the communication straggler issue can be mitigated by reconfiguring the propagation environment.
Unmanned aerial vehicle (UAV) and reconfigurable intelligent surface (RIS) are two promising techniques that can reconfigure the propagation environment by exploiting the mobility of UAVs and by introducing additional configurable propagation paths, respectively.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">UAV can extend the network coverage area and provide broadband access in rural and underdeveloped areas by serving as mobile aerial edge server.
As shown in Fig. 2(a), with an UAV-mounted edge server, many devices can be prevented from being communication stragglers by enabling UAVs to proactively establish short-distance wireless links <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
However, as devices are usually geographically dispersed, the channel qualities of different uplink channels can be very different, which necessities the joint trajectory design and device scheduling for enhancing the performance of UAV-assisted FL.
By dynamically adjusting the UAV‚Äôs location, each device has the opportunity to participate in FL, which enhances the learning performance by enlarging the data exploitation.
In general, scheduling more devices increases the transmission duration of each communication round, but reduces the number of communication rounds.
Balancing such a tradeoff is essential for unleashing the full potential of UAVs to improve the scalability of task-oriented model aggregation.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">RIS is capable of enlarging the received signal power by dynamically configuring its phase-shifts based on instantaneous channel state information (CSI).
With this salient feature, RIS can be leveraged to mitigate the communication straggler issue by reducing the channel heterogeneity among different devices, as shown in Fig. 2(b).
The tradeoff between the model aggregation error and number of participating devices is critical.
Having more devices participating in model training increases the channel heterogeneity and leads to a greater model aggregation error.
RIS can balance such a tradeoff, which, however, is challenging as the explicit relationship between these two aspects should be characterized, necessitating the task-oriented design.
To tackle this challenge, the convergence analysis of RIS-assisted FL should be conducted and further exploited to formulate an optimization problem that maximizes the learning performance (e.g., minimizing the training loss, maximizing the test accuracy) of wireless FL.
Effectively solving such a problem helps in exploiting the benefits of RIS for enhancing the communication scalability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Cooperative Interference Management</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The deep integration of wireless and AI technologies promotes the thriving of edge intelligence, making it normal to support multiple intelligent services over the same wireless network.
To allow geographically dispersed devices receiving diverse intelligent services, a typical strategy is to deploy multiple edge servers to execute different FL tasks in a multi-cell network, as shown in Fig. 2(c), where all devices and edge servers share the same radio channel for uplink and downlink model exchanges.
However, the resultant inter-cell interference inevitably leads to model distortion during both the uplink model aggregation and downlink model dissemination processes as well as learning performance tradeoff among different FL tasks.
Under this circumstance, the inter-cell interference not only reduces the model aggregation accuracy but also limits the number of participating devices, and hence is a performance-limiting factor of scalable FL.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Interference management should be able to balance the accuracy of model transmissions in different cells by not only aligning the intra-cell signals but also limiting the inter-cell interference.
However, interference management for conventional data transmissions (e.g., inter-cell interference coordination) cannot be directly applied to multi-cell FL due to the following challenges.
First, the metric for characterizing the performance of wireless FL is task-oriented and may not be explicitly expressed.
Hence, the impact of inter-cell interference on the learning performance is difficult to be characterized.
Second, the performance of wireless FL should be characterized from a long-term perspective, i.e., considering the impact of downlink/uplink model distortions accumulated over all communication rounds on the ultimate learning performance.
Third, the achievable learning performance among different FL tasks is coupled.
Hence, it is necessary to jointly design effective learning and communication schemes that account for task-oriented performance metrics and inter-cell interference.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">To facilitate interference management for multiple FL tasks, it is necessary to develop a cooperative optimization framework to balance the model aggregation accuracy achieved by different FL tasks, thereby enhancing the overall learning performance.
Specifically, the downlink/uplink transmission distortions lead to a non-vanishing optimality gap, which detrimentally affects the convergence performance of all FL tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Intuitively, increasing devices‚Äô transmit power for one FL task is beneficial for reducing its model distortion, but enlarges the inter-cell interference to other FL tasks and in turn degrades their learning performance.
Hence, the Pareto boundary in terms of the training loss due to model distortion can be adopted to characterize the performance tradeoff among FL tasks in different cells, thereby facilitating the cooperative design.
As shown in Fig. <a href="#S2.F3" title="Figure 3 ‚Ä£ II-B Cooperative Interference Management ‚Ä£ II Network Design for Wireless FL ‚Ä£ Towards Scalable Wireless Federated Learning: Challenges and Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, enabling cooperation among different cells can mitigate the detrimental impact of inter-cell interference, which in turn enhances the average test accuracy of multiple tasks in both two-cell and three-cell FL systems.
This demonstrates the effectiveness of cooperative interference management for supporting scalable wireless FL.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2310.05076/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="178" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Average test accuracy versus number of training rounds under cooperative interference management and non-cooperative schemes in both two-cell and three-cell FL systems. The cooperative design refers to the joint transceiver design among different cells for interference management and model distortion minimization, while the non-cooperative design refers to the separate design that ignores downlink/uplink inter-cell interference among different cells.
</span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Cooperative Model Aggregation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Although mitigating the communication straggler enhances the learning performance, the aforementioned techniques may not be applicable in large-scale wireless networks with massive dispersed devices that aim to learn a shared ML model.
It is desirable to extend the network coverage area by deploying multiple edge servers.
This not only shortens the communication distances to enhance the model aggregation accuracy, but also enables more devices to participate in FL and enlarges the amount of data to be exploited.
However, it is challenging to enable cooperation among different edge servers while guaranteeing the communication efficiency.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">By performing centralized baseband processing at the baseband unit (BBU) pool and deploying multiple remote radio heads (RRHs) as edge servers, cloud radio access network (Cloud-RAN) can be adopted as a multi-tier computing network to support cooperative model aggregation from massive devices.
FL over Cloud-RAN consists of two phases to achieve cooperative model aggregation, i.e., the transmission of local updates from devices to RRHs via AirComp, and the transmission of aggregated signals from RRHs to the BBU via fronthaul links, as shown in Fig. 2(d).
This procedure not only enlarges the coverage area using multiple RRHs, but also reduces the deployment cost by moving the baseband signal processing to the BBU.
However, both the model aggregation error due to AirComp and the quantization error due to limited fronthaul capacity degrade the model aggregation accuracy.
Convergence analysis can be conducted to show that both the transmission distortion and quantization noise in the uplink and downlink prevent the FL algorithm from converging to the optimal solution.
This necessities the investigation of how the model aggregation error degrades the learning performance as well as the joint optimization of communication and
learning performance.
Taking into account the limited transmit power and fronthaul capacity constraints, an efficient task-oriented resource allocation algorithm should be developed to minimize the optimality gap that reflects the detrimental impact of model distortion.
With an appropriate design, Cloud-RAN enabled FL can achieve cooperative model aggregation to enhance the communication scalability by improving the data exploitation and reducing the model distortion.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Resource Orchestration for Wireless FL</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To support wireless FL, it is essential to effectively allocate scarce communication and computation resources.
This section focuses on computation-efficient resource orchestration for wireless FL and elaborates the advantage and design of task-oriented learning algorithms.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Knowledge-Guided Learning for Transceiver Design</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As FL is a long-term process, its convergence performance is determined by the accumulated model aggregation error.
Hence, to enhance the convergence performance of AirComp-assisted FL, it is essential to minimize the time-average model distortion, while considering the dynamic channel conditions and the average transmit power constraints of devices.
To this end, the devices‚Äô transmit powers and the receive beamforming vector (or the receive normalizing factor) of the multi-antenna (or single-antenna) edge server, i.e., AirComp transceiver design, should be jointly optimized, leading to a non-convex resource allocation problem.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">This long-term model distortion minimization problem can be decomposed into multiple per-round transceiver design sub-problems, which can be solved by developing an alternating optimization algorithm.
However, such an optimization-based algorithm suffers from two limitations.
First, it usually takes hundreds of iterations to converge to a stationary solution and has to solve a series of convex sub-problems in each iteration, which may incur a prohibitively high computation complexity. In addition, with time-varying channel conditions, this optimization-based algorithm should be executed in each channel coherence time, which further aggravates the computation complexity and reduces the training efficiency of FL.
Second, the optimization-based algorithm relies on the CSI of all rounds to minimize the time-average model distortion.
However, it is impossible to obtain the non-causal CSI (e.g., CSI of future communication rounds) in dynamic wireless networks.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">With the advancement of ML techniques, learning-based resource allocation algorithms can be developed to tackle the aforementioned limitations of optimization-based algorithms.
Specifically, DNN can solve resource allocation problems by establishing a direct mapping between the instantaneous CSI and the AirComp transceiver design to support wireless FL.
In general, learning-based resource allocation algorithms can be categorized into data-driven and model-driven learning algorithms.
Although simple and widely adopted, data-driven learning algorithms take DNN as a black box and suffer from the limitations of relying on a high volume of training samples, converging slowly, and lacking of interpretability.
As FL typically involves a large amount of devices and the corresponding resource allocation problem is high-dimensional, it is necessary to design task-oriented learning algorithms, where the neural networks are designed based on the domain knowledge to support both communication- and computation-efficient resource allocation for wireless FL, while achieving performance guarantee.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The mathematical expression of optimization variables can be exploited to develop model-driven learning algorithms for effectively solving resource allocation problems.
To minimize the time-average model distortion for AirComp-assisted FL, the optimal transmit power of each device can be explicitly expressed in term of the instantaneous CSI and dual variables by applying the Lagrangian-duality method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
The structure of the analytical expression of the transmit power can be exploited as domain knowledge to design a structure mapping layer when designing the neural networks.
Hence, the task-oriented learning algorithm can be developed by setting the regularized time-average model distortion as the loss function for unsupervised learning with the DNN containing a knowledge-guided mapping layer.
Such a knowledge-guided design is capable of reducing the searching space of transmit powers and hence reducing the computation complexity, while achieving comparable performance as optimization-based algorithms.
The knowledge-guided design only requires the instantaneous CSI to enable real-time resource allocation.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Algorithm Unrolling for Sparse Gradient Recovery</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To enable collaborative training, high-dimensional model parameters need to be exchanged between devices and the edge server, which imposes a heavy communication burden.
To alleviate this issue, various model compression techniques (e.g., sparsification, quantization) have been applied to achieve communication-efficient model transmission, without severely degrading the performance of FL.
In particular, gradient sparsification exploits the fact that different entries of the local gradients have unequal contributions to the model update, where the elements with small magnitudes have little effects on the model convergence.
Meanwhile, most entries of gradients tend to be zero as the model training towards convergence, which further enlarges the gradient sparsity.
Therefore, gradient sparsification can be adopted to extract task-relevant information and reduce the parameter dimension for AirComp-assisted FL, where each device applies the Top-<math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">K</annotation></semantics></math> algorithm on the gradient and employs a random projection matrix before transmission.
The recovery of the average gradient at the edge server is a compressive sensing problem, which can be tackled by adopting the approximate message passing (AMP) algorithms.
However, the AMP algorithm performs in an iterative manner and may diverge when the projection matrix is ill-conditioned.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2310.05076/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="191" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.12.6.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.10.5" class="ltx_text" style="font-size:90%;">An algorithm unrolling architecture that first replaces each algorithm iteration with a neural network layer and then cascades these layers as an RNN, where <math id="S3.F4.6.1.m1.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S3.F4.6.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.F4.6.1.m1.1.1" xref="S3.F4.6.1.m1.1.1.cmml">‚Ñ±</mi><annotation-xml encoding="MathML-Content" id="S3.F4.6.1.m1.1c"><ci id="S3.F4.6.1.m1.1.1.cmml" xref="S3.F4.6.1.m1.1.1">‚Ñ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.1.m1.1d">\mathcal{F}</annotation></semantics></math> refers to the iteration (or activation) function, <math id="S3.F4.7.2.m2.1" class="ltx_Math" alttext="\bm{\theta}_{t}" display="inline"><semantics id="S3.F4.7.2.m2.1b"><msub id="S3.F4.7.2.m2.1.1" xref="S3.F4.7.2.m2.1.1.cmml"><mi id="S3.F4.7.2.m2.1.1.2" xref="S3.F4.7.2.m2.1.1.2.cmml">ùúΩ</mi><mi id="S3.F4.7.2.m2.1.1.3" xref="S3.F4.7.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F4.7.2.m2.1c"><apply id="S3.F4.7.2.m2.1.1.cmml" xref="S3.F4.7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F4.7.2.m2.1.1.1.cmml" xref="S3.F4.7.2.m2.1.1">subscript</csymbol><ci id="S3.F4.7.2.m2.1.1.2.cmml" xref="S3.F4.7.2.m2.1.1.2">ùúΩ</ci><ci id="S3.F4.7.2.m2.1.1.3.cmml" xref="S3.F4.7.2.m2.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.7.2.m2.1d">\bm{\theta}_{t}</annotation></semantics></math> refers to the hyperparameters (or trainable parameters) in the <math id="S3.F4.8.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.F4.8.3.m3.1b"><mi id="S3.F4.8.3.m3.1.1" xref="S3.F4.8.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.F4.8.3.m3.1c"><ci id="S3.F4.8.3.m3.1.1.cmml" xref="S3.F4.8.3.m3.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.8.3.m3.1d">t</annotation></semantics></math>-th iteration (or layer) of iterative algorithms (or RNNs), <math id="S3.F4.9.4.m4.1" class="ltx_Math" alttext="\bm{v}_{t}" display="inline"><semantics id="S3.F4.9.4.m4.1b"><msub id="S3.F4.9.4.m4.1.1" xref="S3.F4.9.4.m4.1.1.cmml"><mi id="S3.F4.9.4.m4.1.1.2" xref="S3.F4.9.4.m4.1.1.2.cmml">ùíó</mi><mi id="S3.F4.9.4.m4.1.1.3" xref="S3.F4.9.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F4.9.4.m4.1c"><apply id="S3.F4.9.4.m4.1.1.cmml" xref="S3.F4.9.4.m4.1.1"><csymbol cd="ambiguous" id="S3.F4.9.4.m4.1.1.1.cmml" xref="S3.F4.9.4.m4.1.1">subscript</csymbol><ci id="S3.F4.9.4.m4.1.1.2.cmml" xref="S3.F4.9.4.m4.1.1.2">ùíó</ci><ci id="S3.F4.9.4.m4.1.1.3.cmml" xref="S3.F4.9.4.m4.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.9.4.m4.1d">\bm{v}_{t}</annotation></semantics></math> is the gradient to be recovered, and <math id="S3.F4.10.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.F4.10.5.m5.1b"><mi id="S3.F4.10.5.m5.1.1" xref="S3.F4.10.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.F4.10.5.m5.1c"><ci id="S3.F4.10.5.m5.1.1.cmml" xref="S3.F4.10.5.m5.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.10.5.m5.1d">T</annotation></semantics></math> denotes the total number of iterations (or layers).</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2310.05076/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="357" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">A GNN-based learning architecture for joint AirComp transceiver and RIS phase-shift design.
The GNN-based learning architecture consists of the initialization layer that encodes the network topology and CSI into GRVs, the graphical mapping layer that updates the GRVs, and the generation layer that outputs the optimized receive normalizing factor, RIS phase-shifts, and transmit powers.</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Algorithm unrolling is capable of transforming an iterative algorithm to a neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
One can replace each algorithm iteration with a neural network layer, and form a recurrent neural network (RNN) by cascading them layer-by-layer.
This inspires us develop a task-oriented unrolling algorithm to unfold the AMP algorithm by incorporating the objective of recovering compressed model parameters into the RNN training, as shown in Fig. <a href="#S3.F4" title="Figure 4 ‚Ä£ III-B Algorithm Unrolling for Sparse Gradient Recovery ‚Ä£ III Resource Orchestration for Wireless FL ‚Ä£ Towards Scalable Wireless Federated Learning: Challenges and Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
By exploiting the structure of iterative algorithms, the AMP-based unrolling algorithm can be applied for sparse signal recovery, thereby achieving both communication- and computation-efficient model transmission.
The advantages of the algorithm unrolling for sparse gradient recovery are three-fold.
First, the hyperparamaters need to be manually tuned for iterative algorithms, but can be directly optimized during neural network training for algorithm unrolling.
Second, to solve high-dimensional resource allocation problems for wireless FL, the iterative algorithms are computationally expensive, while the well-trained neural network via algorithm unrolling requires a low computation complexity in the inference stage.
Third, algorithm unrolling, as a model-driven approach, inherits the structure of iterative algorithms and embeds the domain knowledge and the goal (e.g., convergence optimality and rate) into the neural network design.
This not only reduces the computation complexity but also leads to excellent performance for wireless FL.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">GNN-based Learning for Joint Transceiver and RIS Design</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To leverage the full potential of RIS for mitigating the communication bottleneck and enhancing the model aggregation accuracy, the RIS phase-shifts and AirComp transceiver should be jointly optimized to minimize the time-average model distortion.
Such a joint optimization problem is non-convex because of optimization variable coupling and unit modulus constraints introduced by the passive nature of RIS reflecting elements.
This problem can be tackled by alternately optimizing the AirComp transceiver and the RIS phase-shifts, where the AirComp transceiver can be optimized by applying the Lagrangian-duality method.
Additionally, the RIS phase-shifts can be optimized by transforming the original optimization problem to a rank-constrained semidefinite programming problem.
The rank-one constraints can be tackled by applying the semidefinite relaxation technique, the difference-of-convex representation, or the Riemannian conjugate gradient technique.
However, all the aforementioned methods generally incur a high computation complexity, especially when the devices and RIS phase-shifts are large in quantity.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">With the capability of learning representation for network data and modeling complex data relationship, graph neural network (GNN) has shown its great potential in achieving scalable resource allocation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Wireless networks, consisting of many devices interconnected via wireless links, have intrinsic structures and can be well represented by graphs.
This motivates the development of GNN-based learning algorithms that are capable of exploiting the network topology as a prior knowledge to efficiently solve the non-convex resource allocation problem for the aforementioned task, as shown in Fig. <a href="#S3.F5" title="Figure 5 ‚Ä£ III-B Algorithm Unrolling for Sparse Gradient Recovery ‚Ä£ III Resource Orchestration for Wireless FL ‚Ä£ Towards Scalable Wireless Federated Learning: Challenges and Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
With GNN, a graph representation vector (GRV) is defined for each node (e.g., edge server, RIS, and devices) to incorporate the local CSI.
Subsequently, the GNN-based learning algorithm updates the GRVs via multiple graphical mapping layers that consist of multiple aggregation and combining modules to integrate information from other nodes, and generates the optimization variables via a generation layer.
The task-oriented GNN-based learning algorithm is capable of jointly optimizing AirComp transceiver and RIS phase-shifts to facilitate effective resource allocation for wireless FL, and outperforms the conventional fully connected neural networks in the following aspects.
First, because of the representation capability of graphs and the unique combining and aggregation operations, GNN captures the interaction among the edge server, RIS, and devices, thereby enabling the joint optimization and improving the resource utilization for wireless FL.
Second, because of the inherent permutation equivalence (PE) of GNN, the channel permutation leads to the same permutation of the transmit power of devices.
With the permutation invariance (PI), the permutation of channels does not affect the optimization of RIS phase-shifts as well as the receive normalizing factor.
These two properties significantly reduce the searching space of optimization variables and enhance the algorithmic scalability for resource orchestration.
Third, each GNN layer consists of multiple aggregation and combining operations that share the same structures.
This prominent feature ensures that the GNN-based learning algorithm is scalable to the number of devices, as only the number of modules needs to be adjusted and the neural networks do not need to be re-trained.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Open Research Issues</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we discuss several critical issues that deserve further study.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Integrating Sensing into FL</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The learning performance of wireless FL heavily relies on the quantity and quality of data samples that are available at devices.
Meanwhile, most of the related studies assume that the data samples are readily available.
However, data sensing is a critical component in many practical FL applications, e.g., wireless sensing enabled human motion recognition, where each device needs to perform wireless sensing to obtain its local data samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
To incorporate wireless sensing into the framework of FL, many issues need to be considered.
First, to obtain high-quality data, both effective waveform design and noisy sensing signal processing need to be studied.
Second, as wireless sensing shares power and spectrum resources with task-oriented model updating and exchange, it is important to jointly optimize the resource allocation to enhance the overall learning performance.
Third, real-time wireless sensing leads to the generation of streaming data, which should be considered for the design and analysis of wireless FL.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Developing Trustworthy FL</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To facilitate the practical deployment of FL, it is critically important to develop effective schemes to ensure the privacy and security, thereby achieving trustworthy FL.
Although FL mitigates the privacy concern to a certain extent by keeping the data locally, the exchange of model parameters may still leak privacy by applying model inversion attack and the global model may be intentionally manipulated by malicious devices.
These issues motivate the development of effective privacy-preserving and secure model aggregation strategies to reduce the disclosure of local datasets and to defend the Byzantine attack, respectively.
In this regard, many issues deserve further study.
First, differential privacy introduces the random perturbations to the disclosed model parameters to enhance the privacy level, at the cost of enlarging the model distortion. Hence, it is necessary to develop effective task-oriented strategies to balance the tradeoff between the learning and privacy performance.
Second, to defend the Byzantine attack, the edge server needs to perform non-linear model aggregation (e.g., geometric median), where AirComp cannot be directly applied.
Developing a task-oriented communication scheme to achieve Byzantine-resilient model aggregation is necessary.
Third, as differential privacy and Byzantine-resilience are two critical aspects of FL trustworthiness, it is essential to develop effective strategies that are not only robust against the Byzantine attack, but also capable of protecting the privacy of each device.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Exploiting Diversified Datasets for Learning to Optimize</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To enable ubiquitous intelligence, future wireless networks shall support multiple intelligent services (e.g., FL tasks) over geographically dispersed cells.
As the number of devices, radio propagation environment, and network topology of different cells are generally heterogeneous, each cell has to train a separate ML model to perform resource allocation based on the local dataset that is usually limited in size and only related to the local environment.
Hence, the convergence rate, test accuracy, and robustness of each learned model for resource allocation may be limited.
To mitigate these limitations, an effective method is to exploit diversified datasets, i.e., multiple cells collaborate to provide diversified datasets that can be used to train a high-quality ML model for resource allocation.
As different cells may belong to different service providers, the direct sharing of data is prohibited.
Under this circumstance, FL among different cells can be enabled, thereby enhancing the learning performance.
For example, federated graph learning is a promising framework for the joint training of multiple GNNs to achieve effective resource allocation, while protecting the data privacy and enhancing the model quality.
To implement federated graph learning, it is necessary to develop an effective method to mitigate the inherent heterogeneity of data distributions and graph structures in different cells.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">FL over wireless networks, as a multidisciplinary research area that involves wireless communications, ML, and operation research, empowers many intelligent applications and gives rise to many new research opportunities.
With an objective to enhance communication scalability of wireless FL, we advocated several wireless techniques to enhance task-oriented model aggregation and improve the device participation.
Moreover, by exploiting the domain knowledge, we further proposed three task-oriented learning algorithms to enable computation-efficient resource allocation for wireless FL.
Achieving effective network design and resource orchestration is an essential step towards scalable wireless FL.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K.¬†Letaief¬†<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúEdge artificial intelligence for 6G: Vision,
enabling technologies, and applications,‚Äù <em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas Commun.</em>,
vol.¬†40, no.¬†1, pp. 5‚Äì36, Jan. 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Z.¬†Wang¬†<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúFederated learning via intelligent reflecting
surface,‚Äù <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, vol.¬†21, no.¬†2, pp. 808‚Äì822,
Feb. 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
X.¬†Liu¬†<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúWireless distributed learning: A new hybrid split and
federated learning approach,‚Äù <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, vol.¬†22,
no.¬†4, pp. 2650 ‚Äì 2665, Apr. 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y.¬†Shi¬†<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúTask-oriented communications for 6G: Vision,
principles, and technologies,‚Äù <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">IEEE Wireless Commun.</em>, vol.¬†30, no.¬†3,
pp. 78 ‚Äì 85, Jun. 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.¬†Shao¬†<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúTask-oriented communication for multidevice
cooperative edge inference,‚Äù <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, vol.¬†22,
no.¬†1, pp. 73‚Äì87, Jan. 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W.¬†Zhang¬†<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúOptimizing federated learning in distributed
industrial IoT: A multi-agent approach,‚Äù <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas
Commun.</em>, vol.¬†39, no.¬†12, pp. 3688‚Äì3703, Dec. 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
X.¬†Zhou¬†<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúTime-triggered federated learning over wireless
networks,‚Äù <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, vol.¬†21, no.¬†12, pp.
11‚Äâ066‚Äì11‚Äâ079, Dec. 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K.¬†Yang¬†<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúFederated learning via over-the-air computation,‚Äù
<em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>, vol.¬†19, no.¬†3, pp. 2022‚Äì2035, Mar.
2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
G.¬†Zhu¬†<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúToward an intelligent edge: Wireless communication
meets machine learning,‚Äù <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">IEEE Commun. Mag.</em>, vol.¬†58, no.¬†1, pp.
19‚Äì25, Jan. 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y.¬†Zeng¬†<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúAccessing from the sky: A tutorial on UAV
communications for 5G and beyond,‚Äù <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE</em>, vol. 107, no.¬†12,
pp. 2327‚Äì2375, Dec. 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z.¬†Wang¬†<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúInterference management for over-the-air federated
learning in multi-cell wireless networks,‚Äù <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas
Commun.</em>, vol.¬†40, no.¬†8, pp. 2361‚Äì2377, Aug. 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.¬†Zou¬†<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúKnowledge-guided learning for transceiver design in
over-the-air federated learning,‚Äù <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Wireless Commun.</em>,
vol.¬†22, no.¬†1, pp. 270‚Äì285, Jan. 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
V.¬†Monga¬†<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúAlgorithm unrolling: Interpretable, efficient deep
learning for signal and image processing,‚Äù <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">IEEE Sig. Process. Mag.</em>,
vol.¬†38, no.¬†2, pp. 18‚Äì44, Feb. 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M.¬†Eisen¬†<em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúOptimal wireless resource allocation with random edge
graph neural networks,‚Äù <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Sig. Process.</em>, vol.¬†68, pp.
2977‚Äì2991, Apr. 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
P.¬†Liu¬†<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ‚ÄúVertical federated edge learning with distributed
integrated sensing and communication,‚Äù <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">IEEE Commun. Lett.</em>, vol.¬†26,
no.¬†9, pp. 2091‚Äì2095, Sep. 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.05075" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.05076" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.05076">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.05076" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.05077" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 02:21:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
