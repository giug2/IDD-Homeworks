<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.12729] Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios</title><meta property="og:description" content="Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art OD models perfo‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.12729">

<!--Generated on Tue Feb 27 07:59:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Small Object Detection,  Class Balancing,  Synthetic Data Generation,  YOLOv7,  YOLOv5,  SSD.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:173%;">Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jibinraj Antony23, Vinit Hegiste1, Ali Nazeri2, Hooman Tavakoli2, Snehal Walunj2, 
<br class="ltx_break">Christiane Plociennik2, Martin Ruskowski 21
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">2German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">1Rheinland-Pf√§lzische Technische Universit√§t (RPTU) Kaiserslautern-Landau, Kaiserslautern, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">3<span id="id1.1.id1" class="ltx_text ltx_font_italic">Corresponding Author. Tel.: +49-176 6290 8490 ; E-mail : jibinraj.antony@dfki.de
<br class="ltx_break"></span>**<span id="id2.2.id2" class="ltx_text ltx_font_italic">The first five Authors contributed to the Research equally.</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art OD models perform well on medium and large sized objects, they seem to under perform on small objects. In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors. Additionally, those datasets are likely to be unbalanced and often results in inefficient model convergence. To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models. Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and a balanced distribution of dataset can be created. This paper discusses the effects of a simple proportional class-balancing technique, to enable better anchor matching of the OD models. A comparison has been made on the performances of the state-of-the-art OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and synthetic datasets within an industrial use case.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Small Object Detection, Class Balancing, Synthetic Data Generation, YOLOv7, YOLOv5, SSD.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The birth of Convolution Neural Networks (CNN) set down an important milestone in the performance improvement of image classification challenges, where networks like AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and their successors exceed human level performances. As the researches in CNN advanced, so did the academic interest in the localization of the specific objects, where networks like OverFeat <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> made their first success with a deep CNN architecture.
Further improvements in the field of Object Detection (OD) were possible with networks like Regions with CNN features (R-CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, Fast R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and its variants. These networks and their current adaptations were the catalyst in solving some of the most challenging OD problems, enabling it to be integrated into the industry with a high degree of confidence.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2401.12729/assets/images/detection_leaderboard_COCO.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Detection Leader board of COCO Dataset Object Detection Challenge from 2020. The Average Precision of Small (<math id="S1.F1.3.m1.1" class="ltx_Math" alttext="AP^{S}" display="inline"><semantics id="S1.F1.3.m1.1b"><mrow id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml"><mi id="S1.F1.3.m1.1.1.2" xref="S1.F1.3.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S1.F1.3.m1.1.1.1" xref="S1.F1.3.m1.1.1.1.cmml">‚Äã</mo><msup id="S1.F1.3.m1.1.1.3" xref="S1.F1.3.m1.1.1.3.cmml"><mi id="S1.F1.3.m1.1.1.3.2" xref="S1.F1.3.m1.1.1.3.2.cmml">P</mi><mi id="S1.F1.3.m1.1.1.3.3" xref="S1.F1.3.m1.1.1.3.3.cmml">S</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><apply id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1"><times id="S1.F1.3.m1.1.1.1.cmml" xref="S1.F1.3.m1.1.1.1"></times><ci id="S1.F1.3.m1.1.1.2.cmml" xref="S1.F1.3.m1.1.1.2">ùê¥</ci><apply id="S1.F1.3.m1.1.1.3.cmml" xref="S1.F1.3.m1.1.1.3"><csymbol cd="ambiguous" id="S1.F1.3.m1.1.1.3.1.cmml" xref="S1.F1.3.m1.1.1.3">superscript</csymbol><ci id="S1.F1.3.m1.1.1.3.2.cmml" xref="S1.F1.3.m1.1.1.3.2">ùëÉ</ci><ci id="S1.F1.3.m1.1.1.3.3.cmml" xref="S1.F1.3.m1.1.1.3.3">ùëÜ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">AP^{S}</annotation></semantics></math>) and Large (<math id="S1.F1.4.m2.1" class="ltx_Math" alttext="AP^{L}" display="inline"><semantics id="S1.F1.4.m2.1b"><mrow id="S1.F1.4.m2.1.1" xref="S1.F1.4.m2.1.1.cmml"><mi id="S1.F1.4.m2.1.1.2" xref="S1.F1.4.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S1.F1.4.m2.1.1.1" xref="S1.F1.4.m2.1.1.1.cmml">‚Äã</mo><msup id="S1.F1.4.m2.1.1.3" xref="S1.F1.4.m2.1.1.3.cmml"><mi id="S1.F1.4.m2.1.1.3.2" xref="S1.F1.4.m2.1.1.3.2.cmml">P</mi><mi id="S1.F1.4.m2.1.1.3.3" xref="S1.F1.4.m2.1.1.3.3.cmml">L</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><apply id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1"><times id="S1.F1.4.m2.1.1.1.cmml" xref="S1.F1.4.m2.1.1.1"></times><ci id="S1.F1.4.m2.1.1.2.cmml" xref="S1.F1.4.m2.1.1.2">ùê¥</ci><apply id="S1.F1.4.m2.1.1.3.cmml" xref="S1.F1.4.m2.1.1.3"><csymbol cd="ambiguous" id="S1.F1.4.m2.1.1.3.1.cmml" xref="S1.F1.4.m2.1.1.3">superscript</csymbol><ci id="S1.F1.4.m2.1.1.3.2.cmml" xref="S1.F1.4.m2.1.1.3.2">ùëÉ</ci><ci id="S1.F1.4.m2.1.1.3.3.cmml" xref="S1.F1.4.m2.1.1.3.3">ùêø</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">AP^{L}</annotation></semantics></math>) objects in the dataset for the top performing models are highlighted in the figure.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Detecting small objects is particularly challenging as they have very small features-space to offer for training. As per COCO evaluation matrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, the small objects are the objects in a image having an area less than 32x32 pixels. Although the state-of-the-art Machine Learning (ML) models are excellent in general OD applications involving prominently visible or dominant objects, they perform poorly in scenarios involving small objects. For instance, the latest COCO Dataset Object Detection challenge shows that even the top performing models tends to perform badly on small objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (see Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). More specifically, taking a YOLOv4 with CSPDarknet-53 backbone on MS COCO Dataset as example <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the mean average precision (mAP) for the smaller object is only 20%, where mAP for the dominant objects are more than twice as much (mAP of 45% for the medium and 56% for the large objects), highlights the big gap in the performance of models in smaller OD problems.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, when it comes to the integration of ML application into the industry, the data driven nature of these approaches make them highly dependent on the data used, both quality and quantity, which is why data availability is one of the most crucial factors in the project‚Äôs success. But in industrial environments, the collection of enough useful data is unfortunately a painful process, accounting for additional costs, efforts, and time. Data has to be collected error-free and has to be categorized correctly in order to ensure its effectiveness in the ML application. Since the industrial machines of today are less prone to errors, the availability of the useful data necessary for the ML implementation becomes even more limited, turning the industrial realization a challenging and complex task.
In some cases, the data obtained from the industrial environment is unbalanced, which consequently affects the model‚Äôs performance on all classes. The low performance for the detection of small objects is often attributed to their unbalanced distribution over the entire dataset.
A balanced dataset could eventually improve the learning of the model and hence perform better in all associated tasks.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">As the manual labelling of small objects on image data is a time and effort intensive task, the latest advancements of synthetic data generation methods tend to address these difficulties in data preparation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. For many industrial objects, CAD data is available and can be utilized as a blue-print for the generation of synthetic data. Although CAD data lacks photo-realism, a simple rendering can be imparted and the corresponding images of the target class could be generated from their CAD models without high computational costs. Upon adding the newly generated images, the class distribution of the dataset could be balanced. This approach of leveraging synthetic data generation together with real datasets for class focused data balancing and its effectiveness in improving the performance of use cases with small object detection has been addressed in this paper.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The structure of the paper is as follows: In section <a href="#S2" title="II Background &amp; Related Works ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we discuss the related works in OD applications in industry, as well as the various data manipulation techniques commonly used for the ML applications. This is followed by detailed description of the methodology and the experimental setup performed in section <a href="#S3" title="III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. In section <a href="#S4" title="IV Results ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> we present the results of our approach and compares it with the other popular implementations. Finally, we conclude with the summary of the work and discussion on the future scope in section <a href="#S5" title="V Conclusion ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background &amp; Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Object Detection is a crucial problem in computer vision that involves recognizing and localizing objects of interest within an image or a video stream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In recent years, deep learning (DL) techniques using CNNs, have achieved remarkable success on improving the accuracy of OD models, even in challenging scenarios where the object features are partially occluded, poorly illuminated, or exhibit low contrast. Single Shot Detector (SSD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and You Only Look Once (YOLO) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> are two CNN-based models that have gained popularity in the field of OD. These models have various applications in transportation, military, and industrial use cases. For example, an OD-model based on YOLOv2 was utilized in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for the surface inspection on conveyor belts, while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> compares the effect of SSD, F-RCNN, YOLOv3 and YOLOv5 OD models in detecting surface defects in metals using a generic dataset.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Beyond general OD applications, the small OD finds its application in areal object inspection, industrial scenarios, etc. Small OD also forms the basis of many other computer vision applications such as object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, instance segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and others. The field of application of small OD is therefore diverse, where it is used for spider detection and removal application <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, in autonomous driving application <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, remote sensing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, or even for improving the synchronization of Digital Twin of a manufacturing system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Due to the data-driven nature of small OD models, they are highly dependent on the available data. In most industrial situations, collecting new datasets to train a model is a challenging, expensive and a time-consuming task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. One way to address this is data augmentation, which is a powerful technique to alleviate over-fitting. Many researchers have proposed data augmentation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to artificially increase the size of training data using the data in hand without collecting new data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. One or more morphs are applied to the data while preserving the labels during transformation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and can be generated based on custom requirements and other controlled conditions. More importantly, such a dataset should follow the underlying distribution of the real dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Synthetic data generation can be achieved generally via two techniques, first by using 3D rendering and simulation tools such as Game Engines and second by using DL technique such as Variational Autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, Denoising Diffusion Probabilistic Models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> (such as StyleGANs3) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, etc.
The most simple form of synthetic data generation is using the cut-and-paste method , where one can have various types of background and the dedicated objects of different types needed to be detected. Then a combination of these objects with the background is undertaken to create an image, i.e. to cut the object and paste it on to the background image. This method is good to train OD for some simple tasks, but would not yield better results for complex OD tasks, since this approach does not help in model generalization.
While DL techniques are faster than normal cut-paste in creating a large dataset, a similar issue would arise as these models do not guarantee the desired outcome with specific angles, backgrounds, and lighting conditions. These issues can be easily tackled with synthetic data created using 3D rendering and simulation tools <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Usage of synthetic data for OD makes many applications easier to implement. The authors, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, utilize synthetic data generated using DeepGTAV framework to work on Unmanned Aerial Vehicles scenarios. While synthetic data was leveraged in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> to create OD models for robotic object grasping application. CAD models offer an important asset for synthesizing image data. Leveraging the geometric precision and comprehensive annotations inherent in CAD models, it is possible to create diverse and labeled synthetic datasets that enhance the performance and generalization of OD models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Object-related CAD models are available in a highly-detailed geometric form since they are necessary attributes in manufacturing. Game engines allow the use of these CAD models of compatible formats within a simulation. Taking advantage of this feature of the renderer space and a game camera, the dataset capturing process can be simulated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. There are also packages like Unity Perception package that enable the generation of synthetic datasets for multi-OD and segmentation applications.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">In addition to the benefits associated with data generation, synthetic data generated using CAD models exhibits a notable challenge in terms of domain dissimilarity when compared to real-image-based test data.
On the other hand, the synthetic data generated from CAD models come with the problem of being inherently different to the domain of the real images. Although there are various domain adaptation techniques in the literature such as synthetic-to-real domain adaptation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, which aim to modify synthetic data, there are also works, which present how photo-realistic rendering alone cannot reduce the domain gap and therefore attempt to resolve the issue by domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Furthermore, investigating the combined effect of synthetic and real data as a hybrid dataset, without altering its nature, presents an intriguing avenue for exploration. In scenarios where there is an amount of limited real data, this approach could help leverage it, with synthetic data alongside.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The synthetic data generation approaches described in section <a href="#S2" title="II Background &amp; Related Works ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, could address the data limitations hindering the successful development of small OD-models, by generating class specific balanced datasets. A photo-realistic synthetic datasets may not be necessary for some scenarios, where the generated synthetic datasets are mixed along with the available real dataset.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To validate this approach, an industrial use case of a manual assembly scenario has been taken into consideration. In the assembly process, an actor/worker performs a circuit bread-board assembly of a previously designed configuration, using the specific electronics components, while wearing an Augmented Reality (AR) or Mixed Reality (MR) glasses. The camera sensor in the AR glasses can capture the scene consisting of various objects available within the worker‚Äôs field-of-view and feed it into an OD-model to perform object recognition. Based on the obtained results, a worker assistance-systems in AR will assist the worker by providing instructions and visual augmentations. The worker assistance and AR methods will not be discussed further, as they are outside the scope of this paper. The focus will be primarily only on the performance of the OD model.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">After identifying the distribution of each target class in the original dataset, synthetic data generation will be used to balance the classes. With this approach, we are able to create a proportionally balanced dataset, with more instances of small objects in the combined dataset. Later, the state-of-the-art OD models were trained in combinations of experiments and the results were evaluated.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Data Distribution</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The data driven nature of DL/ML models makes the data distribution a significant factor in the project‚Äôs success. The focus of this paper is the detection of small objects in the manual assembly scenario, where a correct identification is crucial in the worker assistance context. As the first step of this work, the original distribution of the data was identified. There were <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Five</span> objects in consideration in the target dataset, out of which the <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">Three</span> objects (LED, Resister &amp; Button) fit the criteria of a small object i.e. below 32x32 pixels. The other <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">Two</span> objects (Buzzer, Arduino) are between 32x32 pixels and 64x64 pixels and are thus considered as medium-sized objects.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2401.12729/assets/images/data_distri.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="287" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Data Distribution of Initial real dataset (left) to the combined dataset DS-3 (right). The target classes have been proportionally balanced, giving more significance for the less occurring classes.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">After identifying the distribution of the target objects, a proportionally balanced dataset was created using synthetic data generation. The Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ III-A Data Distribution ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the initial and final distribution of the dataset (original dataset on left and the combined dataset DS-3 on the right. Details on DS-3 are shown in table <a href="#S3.T1" title="TABLE I ‚Ä£ III-B Dataset Generation ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Dataset Generation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In order to create a balanced dataset, synthetic data generation techniques was utilized. The table <a href="#S3.T1" title="TABLE I ‚Ä£ III-B Dataset Generation ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> lists the datasets and their combination of real and synthetic data instances used for this work. A total of <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Five</span> datasets were generated combining the real and synthetic data and different OD models were trained on those datasets with various combinations of hyperparameters. The datasets are later referenced using the acronyms given in this table <a href="#S3.T1" title="TABLE I ‚Ä£ III-B Dataset Generation ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Exp. No</td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Dataset</td>
</tr>
<tr id="S3.T1.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Name</td>
</tr>
</table>
</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Real Data</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Synthetic Data</td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.5.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Total Size of</td>
</tr>
<tr id="S3.T1.1.1.1.5.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Training Data</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1</td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DS-1</td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">2</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DS-2</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100 (33% more)</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">400</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">3</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DS-3</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">150 (50% more)</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">450</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">4</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DS-4</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300 (100% more)</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">600</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">5</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">DS-5</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">300</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">300</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The list of datasets and their division of real and synthetic data instances used. The datasets are later referenced using the acronyms given in this table.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The datasets are generated using a scene simulation with a script for image capturing of the game-engine camera. Initially the CAD models are imported in the Unity scene in compatible format. On imparting the 3D models with materials and textures with minimal rendering. The dataset is simple and consists of objects in assembled and disassembled states. The object backgrounds and scales in the images are randomly set within a predefined scale range. We also varied illumination and object viewpoints. We used white lights and yellow lights with random illumination in the virtual scene while capturing images for the dataset. We do not add variable viewpoints or object occlusions in the dataset. In our use case, the objects are perceived from a limited number of viewpoints. Hence, it‚Äôs a simple dataset that could be generated using the available CAD models of industrial parts. A sample of synthetic image from the dataset is shown in <a href="#S3.F3" title="Figure 3 ‚Ä£ III-B Dataset Generation ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2401.12729/assets/images/000097.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A sample synthetic image generated using the 3D rendering Game Engine, used in the dataset.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2401.12729/assets/images/34.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A sample of real image data from the Assembly Scenario, taken from the live-stream video of AR device.</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Figure <a href="#S3.F4" title="Figure 4 ‚Ä£ III-B Dataset Generation ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows a sample original image from the dataset and figure <a href="#S3.F3" title="Figure 3 ‚Ä£ III-B Dataset Generation ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a synthetically generated image. As it is observable, the generated data is rather simple and mainly focuses in balancing the target object instances, rather than creating a photo-realistic scene.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Objected Detection Models</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">After generating the datasets and its combinations, they were used to train and compare state-of-the-art OD-models. Based on popularity and ease of implementation, three models were selected and utilized for the experiments. Two of the models were based on YOLO family developed on PyTorch framework and the other model was the SSD based on TensorFlow. These models are considered to be effective on small OD use cases, faster in inferences and having remarkable performances.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS1.4.1.1" class="ltx_text">III-C</span>1 </span>YOLOv5</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">YOLO (You Only Look Once) is a family of OD algorithms, which performs detection in an image through a single forward pass, unlike other algorithms like Fast-RCNN or Faster-RCNN using two stages. YOLO was the first OD algorithm to combine the procedure of predicting bounding box with class label in an end to end differentiable network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. YOLOv5 is open source and original version of the code is written in python with PyTorch framework. YOLO models use a Non-Maxima Suppression (NMS) as a post-processing step to obtain the final bounding box for each detected object. This technique filters out redundant bounding boxes based on their overlap values and ensures that only the most relevant and accurate predictions remain in the final results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.YOLOv5 offers a range of architectures, each tailored to specific application requirements and datasets. The YOLOv5s (small), YOLOv5m (medium), YOLOv5l (large), and YOLOv5x (Extra Large) architectures exhibit increasing complexity in their design. As the architecture complexity grows, so does the accuracy of the object detection results. However, this improvement in accuracy comes at the expense of reduced model speed during object detection. Therefore, the choice of architecture depends on striking the right balance between accuracy and real-time object detection speed, considering the specific needs of the application and datasets at hand.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS2.4.1.1" class="ltx_text">III-C</span>2 </span>YOLOv7</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Wang et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, introduced YOLOv7 as an extension of the YOLO series of real-time OD models. It introduces various models tailored for different GPU environments, including edge GPU, normal GPU, and cloud GPU. Examples of these models include YOLOv7-tiny, YOLOv7, and YOLOv7-W6. Additionally, YOLOv7 incorporates model scaling techniques to cater to diverse service requirements, resulting in the development of models such as YOLOv7-X, YOLOv7-E6, YOLOv7-D6, and YOLOv7-E6E. They apply stack scaling to the neck component and employ the suggested compound scaling technique to increase the depth and width of the entire model, resulting in YOLOv7-X. On the other hand, for YOLOv7-W6, they utilize the newly introduced compound scaling method to derive YOLOv7-E6 and YOLOv7-D6.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">YOLOv7 in OD offers notable improvements in speed and accuracy compared to previous models. The speed ranges from 5 to 160 frames per second (FPS), enabling real-time applications. In terms of accuracy, YOLOv7 achieves the highest average precision (AP) of 56.8% among all real-time object detectors operating at 30 FPS or higher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS3.4.1.1" class="ltx_text">III-C</span>3 </span>SSD</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">SSD (Single Shot MultiBox Detector) is a prominent OD model that considers bounding box prediction as a regression problem. It starts by selecting the anchor box with the highest intersection over union (IoU) with the ground truth bounding box and gradually refines the prediction by minimizing the loss between the predicted and ground truth boxes. This iterative regression process allows SSD to achieve precise localization of objects in the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This model provides a straightforward procedure for eliminating duplicate predictions, while SSD‚Äôs regression-based approach focuses on refining the bounding box estimate by progressively minimizing the error.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Hyper-Parameters</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">After identifying and selecting the OD models as described above, the experiments have been performed. To ensure a comprehensive comparison among the three OD algorithms, it is essential to maintain consistent hyper-parameters during the training and testing phases on the identical dataset. In order to achieve this, the experiments employed the following hyper-parameters:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Training Batch size : 8</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Input Image size : 1080 x 1080 pixels (height x width)</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">To optimize the training process, all the experiments utilized the Adam optimizer, which has been widely recognized for its efficiency in deep learning tasks. The Adam optimizer effectively combines the benefits of adaptive gradient algorithms and momentum-based optimization methods, facilitating faster convergence and improved performance during training.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">To evaluate the performance of the algorithms, experiments were conducted for different numbers of Epochs: specifically, 30, 50 and 100 Epochs. By observing the algorithms‚Äô performance over multiple epochs, insights into their convergence rates and overall accuracy as the training progresses are observed.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">The OD models are then trained on the dataset combinations and evaluated on a general test dataset consisted of real images. The model performances are evaluated on the COCO test evaluation matrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and the results are reviewed and observations are noted.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">No</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Epoch</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Dataset</th>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">mAP</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">AP50</td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APs</td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APm</td>
<td id="S3.T2.1.1.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APl</td>
<td id="S3.T2.1.1.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARs</td>
<td id="S3.T2.1.1.1.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARm</td>
<td id="S3.T2.1.1.1.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARl</td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1</th>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.2.2.3.1" class="ltx_text">DS-1</span></th>
<td id="S3.T2.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.922</td>
<td id="S3.T2.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.912</td>
<td id="S3.T2.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.373</td>
<td id="S3.T2.1.2.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.516</td>
<td id="S3.T2.1.2.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3</td>
<td id="S3.T2.1.2.2.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.446</td>
<td id="S3.T2.1.2.2.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.574</td>
<td id="S3.T2.1.2.2.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.339</td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<th id="S3.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2</th>
<th id="S3.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.946</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.939</td>
<td id="S3.T2.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.47</td>
<td id="S3.T2.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.532</td>
<td id="S3.T2.1.3.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.35</td>
<td id="S3.T2.1.3.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.515</td>
<td id="S3.T2.1.3.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.588</td>
<td id="S3.T2.1.3.3.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.375</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<th id="S3.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">3</th>
<th id="S3.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.976</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.967</td>
<td id="S3.T2.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.493</td>
<td id="S3.T2.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.557</td>
<td id="S3.T2.1.4.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.373</td>
<td id="S3.T2.1.4.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.564</td>
<td id="S3.T2.1.4.4.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.61</td>
<td id="S3.T2.1.4.4.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<th id="S3.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">4</th>
<th id="S3.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.5.5.3.1" class="ltx_text">DS-2</span></th>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.936</td>
<td id="S3.T2.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.929</td>
<td id="S3.T2.1.5.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.401</td>
<td id="S3.T2.1.5.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.511</td>
<td id="S3.T2.1.5.5.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.329</td>
<td id="S3.T2.1.5.5.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.476</td>
<td id="S3.T2.1.5.5.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.583</td>
<td id="S3.T2.1.5.5.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.364</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<th id="S3.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">5</th>
<th id="S3.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.953</td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.949</td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.475</td>
<td id="S3.T2.1.6.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.576</td>
<td id="S3.T2.1.6.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.362</td>
<td id="S3.T2.1.6.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.537</td>
<td id="S3.T2.1.6.6.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.631</td>
<td id="S3.T2.1.6.6.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.394</td>
</tr>
<tr id="S3.T2.1.7.7" class="ltx_tr">
<th id="S3.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">6</th>
<th id="S3.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T2.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.955</td>
<td id="S3.T2.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.952</td>
<td id="S3.T2.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.543</td>
<td id="S3.T2.1.7.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.605</td>
<td id="S3.T2.1.7.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.366</td>
<td id="S3.T2.1.7.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.594</td>
<td id="S3.T2.1.7.7.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.654</td>
<td id="S3.T2.1.7.7.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.397</td>
</tr>
<tr id="S3.T2.1.8.8" class="ltx_tr">
<th id="S3.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">7</th>
<th id="S3.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.8.8.3.1" class="ltx_text">DS-3</span></th>
<td id="S3.T2.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.955</td>
<td id="S3.T2.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.953</td>
<td id="S3.T2.1.8.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.475</td>
<td id="S3.T2.1.8.8.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.576</td>
<td id="S3.T2.1.8.8.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.33</td>
<td id="S3.T2.1.8.8.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.53</td>
<td id="S3.T2.1.8.8.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.63</td>
<td id="S3.T2.1.8.8.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.367</td>
</tr>
<tr id="S3.T2.1.9.9" class="ltx_tr">
<th id="S3.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">8</th>
<th id="S3.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T2.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.959</td>
<td id="S3.T2.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.953</td>
<td id="S3.T2.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.488</td>
<td id="S3.T2.1.9.9.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.566</td>
<td id="S3.T2.1.9.9.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.364</td>
<td id="S3.T2.1.9.9.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.354</td>
<td id="S3.T2.1.9.9.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.624</td>
<td id="S3.T2.1.9.9.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.398</td>
</tr>
<tr id="S3.T2.1.10.10" class="ltx_tr">
<th id="S3.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">9</th>
<th id="S3.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T2.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.986</td>
<td id="S3.T2.1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.982</td>
<td id="S3.T2.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.594</td>
<td id="S3.T2.1.10.10.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.611</td>
<td id="S3.T2.1.10.10.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.369</td>
<td id="S3.T2.1.10.10.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.659</td>
<td id="S3.T2.1.10.10.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.667</td>
<td id="S3.T2.1.10.10.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.395</td>
</tr>
<tr id="S3.T2.1.11.11" class="ltx_tr">
<th id="S3.T2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">10</th>
<th id="S3.T2.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T2.1.11.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.11.11.3.1" class="ltx_text">DS-4</span></th>
<td id="S3.T2.1.11.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.966</td>
<td id="S3.T2.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.959</td>
<td id="S3.T2.1.11.11.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.449</td>
<td id="S3.T2.1.11.11.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.594</td>
<td id="S3.T2.1.11.11.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.346</td>
<td id="S3.T2.1.11.11.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.484</td>
<td id="S3.T2.1.11.11.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.661</td>
<td id="S3.T2.1.11.11.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.375</td>
</tr>
<tr id="S3.T2.1.12.12" class="ltx_tr">
<th id="S3.T2.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">11</th>
<th id="S3.T2.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T2.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.977</td>
<td id="S3.T2.1.12.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.968</td>
<td id="S3.T2.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.577</td>
<td id="S3.T2.1.12.12.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.613</td>
<td id="S3.T2.1.12.12.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.382</td>
<td id="S3.T2.1.12.12.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.627</td>
<td id="S3.T2.1.12.12.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.672</td>
<td id="S3.T2.1.12.12.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.402</td>
</tr>
<tr id="S3.T2.1.13.13" class="ltx_tr">
<th id="S3.T2.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">12</th>
<th id="S3.T2.1.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T2.1.13.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.983</td>
<td id="S3.T2.1.13.13.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.979</td>
<td id="S3.T2.1.13.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.607</td>
<td id="S3.T2.1.13.13.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.628</td>
<td id="S3.T2.1.13.13.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.384</td>
<td id="S3.T2.1.13.13.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.658</td>
<td id="S3.T2.1.13.13.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.688</td>
<td id="S3.T2.1.13.13.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.41</td>
</tr>
<tr id="S3.T2.1.14.14" class="ltx_tr">
<th id="S3.T2.1.14.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">13</th>
<th id="S3.T2.1.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T2.1.14.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T2.1.14.14.3.1" class="ltx_text">DS-5</span></th>
<td id="S3.T2.1.14.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.369</td>
<td id="S3.T2.1.14.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.357</td>
<td id="S3.T2.1.14.14.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.143</td>
<td id="S3.T2.1.14.14.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.093</td>
<td id="S3.T2.1.14.14.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.137</td>
<td id="S3.T2.1.14.14.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.143</td>
<td id="S3.T2.1.14.14.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.169</td>
<td id="S3.T2.1.14.14.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.183</td>
</tr>
<tr id="S3.T2.1.15.15" class="ltx_tr">
<th id="S3.T2.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">14</th>
<th id="S3.T2.1.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T2.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.336</td>
<td id="S3.T2.1.15.15.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.323</td>
<td id="S3.T2.1.15.15.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.057</td>
<td id="S3.T2.1.15.15.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.104</td>
<td id="S3.T2.1.15.15.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.170</td>
<td id="S3.T2.1.15.15.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.057</td>
<td id="S3.T2.1.15.15.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.139</td>
<td id="S3.T2.1.15.15.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.207</td>
</tr>
<tr id="S3.T2.1.16.16" class="ltx_tr">
<th id="S3.T2.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">15</th>
<th id="S3.T2.1.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">100</th>
<td id="S3.T2.1.16.16.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.412</td>
<td id="S3.T2.1.16.16.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.399</td>
<td id="S3.T2.1.16.16.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.083</td>
<td id="S3.T2.1.16.16.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.121</td>
<td id="S3.T2.1.16.16.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.149</td>
<td id="S3.T2.1.16.16.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.128</td>
<td id="S3.T2.1.16.16.9" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.182</td>
<td id="S3.T2.1.16.16.10" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.193</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Experiment Results for YOLOv5 Model, the experiment instance corresponding to DS-3 at 100 Epochs seem to give the optimum results, even comparing the DS-4 with a lot more synthetic data.</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">No</th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Epoch</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Dataset</th>
<td id="S3.T3.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">mAP</td>
<td id="S3.T3.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">AP50</td>
<td id="S3.T3.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APs</td>
<td id="S3.T3.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APm</td>
<td id="S3.T3.1.1.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APl</td>
<td id="S3.T3.1.1.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARs</td>
<td id="S3.T3.1.1.1.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARm</td>
<td id="S3.T3.1.1.1.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARl</td>
</tr>
<tr id="S3.T3.1.2.2" class="ltx_tr">
<th id="S3.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1</th>
<th id="S3.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.2.2.3.1" class="ltx_text">DS-1</span></th>
<td id="S3.T3.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.885</td>
<td id="S3.T3.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.873</td>
<td id="S3.T3.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.259</td>
<td id="S3.T3.1.2.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.416</td>
<td id="S3.T3.1.2.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.273</td>
<td id="S3.T3.1.2.2.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.306</td>
<td id="S3.T3.1.2.2.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.487</td>
<td id="S3.T3.1.2.2.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.309</td>
</tr>
<tr id="S3.T3.1.3.3" class="ltx_tr">
<th id="S3.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2</th>
<th id="S3.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T3.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.905</td>
<td id="S3.T3.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.897</td>
<td id="S3.T3.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.36</td>
<td id="S3.T3.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.45</td>
<td id="S3.T3.1.3.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.285</td>
<td id="S3.T3.1.3.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.447</td>
<td id="S3.T3.1.3.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.505</td>
<td id="S3.T3.1.3.3.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.328</td>
</tr>
<tr id="S3.T3.1.4.4" class="ltx_tr">
<th id="S3.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">3</th>
<th id="S3.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T3.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.951</td>
<td id="S3.T3.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.946</td>
<td id="S3.T3.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.48</td>
<td id="S3.T3.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.566</td>
<td id="S3.T3.1.4.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.361</td>
<td id="S3.T3.1.4.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.538</td>
<td id="S3.T3.1.4.4.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.621</td>
<td id="S3.T3.1.4.4.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.393</td>
</tr>
<tr id="S3.T3.1.5.5" class="ltx_tr">
<th id="S3.T3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">4</th>
<th id="S3.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T3.1.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.5.5.3.1" class="ltx_text">DS-2</span></th>
<td id="S3.T3.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.838</td>
<td id="S3.T3.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.831</td>
<td id="S3.T3.1.5.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.331</td>
<td id="S3.T3.1.5.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.404</td>
<td id="S3.T3.1.5.5.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.313</td>
<td id="S3.T3.1.5.5.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.403</td>
<td id="S3.T3.1.5.5.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.461</td>
<td id="S3.T3.1.5.5.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.35</td>
</tr>
<tr id="S3.T3.1.6.6" class="ltx_tr">
<th id="S3.T3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">5</th>
<th id="S3.T3.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T3.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.914</td>
<td id="S3.T3.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.907</td>
<td id="S3.T3.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.463</td>
<td id="S3.T3.1.6.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.479</td>
<td id="S3.T3.1.6.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.347</td>
<td id="S3.T3.1.6.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.56</td>
<td id="S3.T3.1.6.6.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.538</td>
<td id="S3.T3.1.6.6.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.372</td>
</tr>
<tr id="S3.T3.1.7.7" class="ltx_tr">
<th id="S3.T3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">6</th>
<th id="S3.T3.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T3.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.949</td>
<td id="S3.T3.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.939</td>
<td id="S3.T3.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.499</td>
<td id="S3.T3.1.7.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.588</td>
<td id="S3.T3.1.7.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.379</td>
<td id="S3.T3.1.7.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.552</td>
<td id="S3.T3.1.7.7.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.635</td>
<td id="S3.T3.1.7.7.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.412</td>
</tr>
<tr id="S3.T3.1.8.8" class="ltx_tr">
<th id="S3.T3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">7</th>
<th id="S3.T3.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T3.1.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.8.8.3.1" class="ltx_text">DS-3</span></th>
<td id="S3.T3.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T3.1.8.8.4.1" class="ltx_text ltx_font_bold">0.947</span></td>
<td id="S3.T3.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T3.1.8.8.5.1" class="ltx_text ltx_font_bold">0.943</span></td>
<td id="S3.T3.1.8.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.424</td>
<td id="S3.T3.1.8.8.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.514</td>
<td id="S3.T3.1.8.8.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.32</td>
<td id="S3.T3.1.8.8.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.475</td>
<td id="S3.T3.1.8.8.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.588</td>
<td id="S3.T3.1.8.8.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.352</td>
</tr>
<tr id="S3.T3.1.9.9" class="ltx_tr">
<th id="S3.T3.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">8</th>
<th id="S3.T3.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T3.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T3.1.9.9.3.1" class="ltx_text ltx_font_bold">0.957</span></td>
<td id="S3.T3.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T3.1.9.9.4.1" class="ltx_text ltx_font_bold">0.95</span></td>
<td id="S3.T3.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.42</td>
<td id="S3.T3.1.9.9.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.575</td>
<td id="S3.T3.1.9.9.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.34</td>
<td id="S3.T3.1.9.9.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.472</td>
<td id="S3.T3.1.9.9.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.64</td>
<td id="S3.T3.1.9.9.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.375</td>
</tr>
<tr id="S3.T3.1.10.10" class="ltx_tr">
<th id="S3.T3.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">9</th>
<th id="S3.T3.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T3.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T3.1.10.10.3.1" class="ltx_text ltx_font_bold">0.976</span></td>
<td id="S3.T3.1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T3.1.10.10.4.1" class="ltx_text ltx_font_bold">0.967</span></td>
<td id="S3.T3.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.512</td>
<td id="S3.T3.1.10.10.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.624</td>
<td id="S3.T3.1.10.10.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.38</td>
<td id="S3.T3.1.10.10.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.553</td>
<td id="S3.T3.1.10.10.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.689</td>
<td id="S3.T3.1.10.10.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.404</td>
</tr>
<tr id="S3.T3.1.11.11" class="ltx_tr">
<th id="S3.T3.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">10</th>
<th id="S3.T3.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T3.1.11.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.11.11.3.1" class="ltx_text">DS-4</span></th>
<td id="S3.T3.1.11.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.906</td>
<td id="S3.T3.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.902</td>
<td id="S3.T3.1.11.11.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.305</td>
<td id="S3.T3.1.11.11.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.538</td>
<td id="S3.T3.1.11.11.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.319</td>
<td id="S3.T3.1.11.11.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.349</td>
<td id="S3.T3.1.11.11.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.618</td>
<td id="S3.T3.1.11.11.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.354</td>
</tr>
<tr id="S3.T3.1.12.12" class="ltx_tr">
<th id="S3.T3.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">11</th>
<th id="S3.T3.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T3.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.928</td>
<td id="S3.T3.1.12.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.922</td>
<td id="S3.T3.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.372</td>
<td id="S3.T3.1.12.12.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.51</td>
<td id="S3.T3.1.12.12.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.322</td>
<td id="S3.T3.1.12.12.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.434</td>
<td id="S3.T3.1.12.12.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.616</td>
<td id="S3.T3.1.12.12.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.359</td>
</tr>
<tr id="S3.T3.1.13.13" class="ltx_tr">
<th id="S3.T3.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">12</th>
<th id="S3.T3.1.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T3.1.13.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.946</td>
<td id="S3.T3.1.13.13.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.939</td>
<td id="S3.T3.1.13.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.47</td>
<td id="S3.T3.1.13.13.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.578</td>
<td id="S3.T3.1.13.13.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.578</td>
<td id="S3.T3.1.13.13.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.519</td>
<td id="S3.T3.1.13.13.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.659</td>
<td id="S3.T3.1.13.13.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.398</td>
</tr>
<tr id="S3.T3.1.14.14" class="ltx_tr">
<th id="S3.T3.1.14.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">13</th>
<th id="S3.T3.1.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T3.1.14.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T3.1.14.14.3.1" class="ltx_text">DS-5</span></th>
<td id="S3.T3.1.14.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.243</td>
<td id="S3.T3.1.14.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.235</td>
<td id="S3.T3.1.14.14.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.083</td>
<td id="S3.T3.1.14.14.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.059</td>
<td id="S3.T3.1.14.14.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.072</td>
<td id="S3.T3.1.14.14.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.103</td>
<td id="S3.T3.1.14.14.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.108</td>
<td id="S3.T3.1.14.14.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.147</td>
</tr>
<tr id="S3.T3.1.15.15" class="ltx_tr">
<th id="S3.T3.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">14</th>
<th id="S3.T3.1.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T3.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.304</td>
<td id="S3.T3.1.15.15.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3</td>
<td id="S3.T3.1.15.15.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.105</td>
<td id="S3.T3.1.15.15.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.075</td>
<td id="S3.T3.1.15.15.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.085</td>
<td id="S3.T3.1.15.15.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.114</td>
<td id="S3.T3.1.15.15.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.117</td>
<td id="S3.T3.1.15.15.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.145</td>
</tr>
<tr id="S3.T3.1.16.16" class="ltx_tr">
<th id="S3.T3.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">15</th>
<th id="S3.T3.1.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">100</th>
<td id="S3.T3.1.16.16.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.374</td>
<td id="S3.T3.1.16.16.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.364</td>
<td id="S3.T3.1.16.16.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.122</td>
<td id="S3.T3.1.16.16.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.102</td>
<td id="S3.T3.1.16.16.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.129</td>
<td id="S3.T3.1.16.16.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.144</td>
<td id="S3.T3.1.16.16.9" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.173</td>
<td id="S3.T3.1.16.16.10" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.246</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Experiment Results for YOLOv7 Model. The DS-3 dataset clearly gives best results compared to the other dataset combinations.</figcaption>
</figure>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">No</th>
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Epoch</th>
<th id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Dataset</th>
<td id="S3.T4.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">mAP</td>
<td id="S3.T4.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">AP50</td>
<td id="S3.T4.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APs</td>
<td id="S3.T4.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APm</td>
<td id="S3.T4.1.1.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">APl</td>
<td id="S3.T4.1.1.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARs</td>
<td id="S3.T4.1.1.1.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARm</td>
<td id="S3.T4.1.1.1.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ARl</td>
</tr>
<tr id="S3.T4.1.2.2" class="ltx_tr">
<th id="S3.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1</th>
<th id="S3.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.2.2.3.1" class="ltx_text">DS-1</span></th>
<td id="S3.T4.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.492</td>
<td id="S3.T4.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.480</td>
<td id="S3.T4.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.015</td>
<td id="S3.T4.1.2.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.174</td>
<td id="S3.T4.1.2.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.254</td>
<td id="S3.T4.1.2.2.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.051</td>
<td id="S3.T4.1.2.2.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.251</td>
<td id="S3.T4.1.2.2.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.290</td>
</tr>
<tr id="S3.T4.1.3.3" class="ltx_tr">
<th id="S3.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2</th>
<th id="S3.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T4.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.630</td>
<td id="S3.T4.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.617</td>
<td id="S3.T4.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.079</td>
<td id="S3.T4.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.240</td>
<td id="S3.T4.1.3.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.300</td>
<td id="S3.T4.1.3.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.170</td>
<td id="S3.T4.1.3.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.292</td>
<td id="S3.T4.1.3.3.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.330</td>
</tr>
<tr id="S3.T4.1.4.4" class="ltx_tr">
<th id="S3.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">3</th>
<th id="S3.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T4.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.826</td>
<td id="S3.T4.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.765</td>
<td id="S3.T4.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.204</td>
<td id="S3.T4.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.329</td>
<td id="S3.T4.1.4.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.314</td>
<td id="S3.T4.1.4.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.265</td>
<td id="S3.T4.1.4.4.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.411</td>
<td id="S3.T4.1.4.4.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.347</td>
</tr>
<tr id="S3.T4.1.5.5" class="ltx_tr">
<th id="S3.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">4</th>
<th id="S3.T4.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T4.1.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.5.5.3.1" class="ltx_text">DS-2</span></th>
<td id="S3.T4.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.414</td>
<td id="S3.T4.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.409</td>
<td id="S3.T4.1.5.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.036</td>
<td id="S3.T4.1.5.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.142</td>
<td id="S3.T4.1.5.5.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.274</td>
<td id="S3.T4.1.5.5.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.062</td>
<td id="S3.T4.1.5.5.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.214</td>
<td id="S3.T4.1.5.5.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.304</td>
</tr>
<tr id="S3.T4.1.6.6" class="ltx_tr">
<th id="S3.T4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">5</th>
<th id="S3.T4.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T4.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.556</td>
<td id="S3.T4.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.540</td>
<td id="S3.T4.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.079</td>
<td id="S3.T4.1.6.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.177</td>
<td id="S3.T4.1.6.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.263</td>
<td id="S3.T4.1.6.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.113</td>
<td id="S3.T4.1.6.6.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.232</td>
<td id="S3.T4.1.6.6.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.305</td>
</tr>
<tr id="S3.T4.1.7.7" class="ltx_tr">
<th id="S3.T4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">6</th>
<th id="S3.T4.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T4.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.813</td>
<td id="S3.T4.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.752</td>
<td id="S3.T4.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.147</td>
<td id="S3.T4.1.7.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.301</td>
<td id="S3.T4.1.7.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.315</td>
<td id="S3.T4.1.7.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.217</td>
<td id="S3.T4.1.7.7.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.368</td>
<td id="S3.T4.1.7.7.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.352</td>
</tr>
<tr id="S3.T4.1.8.8" class="ltx_tr">
<th id="S3.T4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">7</th>
<th id="S3.T4.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T4.1.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.8.8.3.1" class="ltx_text">DS-3</span></th>
<td id="S3.T4.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.333</td>
<td id="S3.T4.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.307</td>
<td id="S3.T4.1.8.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.007</td>
<td id="S3.T4.1.8.8.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.087</td>
<td id="S3.T4.1.8.8.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.246</td>
<td id="S3.T4.1.8.8.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.027</td>
<td id="S3.T4.1.8.8.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.134</td>
<td id="S3.T4.1.8.8.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.289</td>
</tr>
<tr id="S3.T4.1.9.9" class="ltx_tr">
<th id="S3.T4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">8</th>
<th id="S3.T4.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T4.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.574</td>
<td id="S3.T4.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.555</td>
<td id="S3.T4.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.033</td>
<td id="S3.T4.1.9.9.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.175</td>
<td id="S3.T4.1.9.9.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.242</td>
<td id="S3.T4.1.9.9.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.091</td>
<td id="S3.T4.1.9.9.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.244</td>
<td id="S3.T4.1.9.9.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.280</td>
</tr>
<tr id="S3.T4.1.10.10" class="ltx_tr">
<th id="S3.T4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">9</th>
<th id="S3.T4.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T4.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.850</td>
<td id="S3.T4.1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.742</td>
<td id="S3.T4.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.190</td>
<td id="S3.T4.1.10.10.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.302</td>
<td id="S3.T4.1.10.10.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.329</td>
<td id="S3.T4.1.10.10.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.265</td>
<td id="S3.T4.1.10.10.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.379</td>
<td id="S3.T4.1.10.10.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.355</td>
</tr>
<tr id="S3.T4.1.11.11" class="ltx_tr">
<th id="S3.T4.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">10</th>
<th id="S3.T4.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T4.1.11.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.11.11.3.1" class="ltx_text">DS-4</span></th>
<td id="S3.T4.1.11.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.253</td>
<td id="S3.T4.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.232</td>
<td id="S3.T4.1.11.11.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.002</td>
<td id="S3.T4.1.11.11.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.041</td>
<td id="S3.T4.1.11.11.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.230</td>
<td id="S3.T4.1.11.11.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.021</td>
<td id="S3.T4.1.11.11.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.079</td>
<td id="S3.T4.1.11.11.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.263</td>
</tr>
<tr id="S3.T4.1.12.12" class="ltx_tr">
<th id="S3.T4.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">11</th>
<th id="S3.T4.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T4.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.446</td>
<td id="S3.T4.1.12.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.430</td>
<td id="S3.T4.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.028</td>
<td id="S3.T4.1.12.12.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.144</td>
<td id="S3.T4.1.12.12.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.210</td>
<td id="S3.T4.1.12.12.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.072</td>
<td id="S3.T4.1.12.12.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.214</td>
<td id="S3.T4.1.12.12.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.242</td>
</tr>
<tr id="S3.T4.1.13.13" class="ltx_tr">
<th id="S3.T4.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">12</th>
<th id="S3.T4.1.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S3.T4.1.13.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.775</td>
<td id="S3.T4.1.13.13.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.720</td>
<td id="S3.T4.1.13.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.155</td>
<td id="S3.T4.1.13.13.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.264</td>
<td id="S3.T4.1.13.13.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.305</td>
<td id="S3.T4.1.13.13.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.227</td>
<td id="S3.T4.1.13.13.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.322</td>
<td id="S3.T4.1.13.13.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.329</td>
</tr>
<tr id="S3.T4.1.14.14" class="ltx_tr">
<th id="S3.T4.1.14.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">13</th>
<th id="S3.T4.1.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">30</th>
<th id="S3.T4.1.14.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="3"><span id="S3.T4.1.14.14.3.1" class="ltx_text">DS-5</span></th>
<td id="S3.T4.1.14.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.081</td>
<td id="S3.T4.1.14.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.076</td>
<td id="S3.T4.1.14.14.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.011</td>
<td id="S3.T4.1.14.14.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.034</td>
<td id="S3.T4.1.14.14.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.063</td>
<td id="S3.T4.1.14.14.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.062</td>
<td id="S3.T4.1.14.14.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.071</td>
<td id="S3.T4.1.14.14.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.092</td>
</tr>
<tr id="S3.T4.1.15.15" class="ltx_tr">
<th id="S3.T4.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">14</th>
<th id="S3.T4.1.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">50</th>
<td id="S3.T4.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.121</td>
<td id="S3.T4.1.15.15.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.099</td>
<td id="S3.T4.1.15.15.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.067</td>
<td id="S3.T4.1.15.15.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.059</td>
<td id="S3.T4.1.15.15.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.091</td>
<td id="S3.T4.1.15.15.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.079</td>
<td id="S3.T4.1.15.15.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.108</td>
<td id="S3.T4.1.15.15.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.136</td>
</tr>
<tr id="S3.T4.1.16.16" class="ltx_tr">
<th id="S3.T4.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">15</th>
<th id="S3.T4.1.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">100</th>
<td id="S3.T4.1.16.16.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.198</td>
<td id="S3.T4.1.16.16.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.142</td>
<td id="S3.T4.1.16.16.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.088</td>
<td id="S3.T4.1.16.16.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.101</td>
<td id="S3.T4.1.16.16.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.123</td>
<td id="S3.T4.1.16.16.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.092</td>
<td id="S3.T4.1.16.16.9" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.132</td>
<td id="S3.T4.1.16.16.10" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.169</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Experiment Results for SSD Model. SSD Model seem to perform not really good on the data, but comparing the overall performance, the model performed comparatively better on DS-3.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">After training the Three OD models on the five sets of datasets with three hyper-parameter combinations, the models are tested on a common test dataset, that consists of real images from the Assembly scenario. The test was carried out using the COCO evaluation matrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, examining the performance on small, medium and large sized objects and the results are noted. Table <a href="#S3.T2" title="TABLE II ‚Ä£ III-D Hyper-Parameters ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, Table <a href="#S3.T3" title="TABLE III ‚Ä£ III-D Hyper-Parameters ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> and Table <a href="#S3.T4" title="TABLE IV ‚Ä£ III-D Hyper-Parameters ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> represents the evaluation results from YOLOv5, YOLOv7 and SSD models respectively. The silent feature of this table are the Average Precision and Average Recall of Small objects (APs &amp; ARs) which calculates the actual Precision and Recall of the model based on the true bounding box and predicted bounding box overlap for small objects (less than 32x32 pixels). This was necessary because only by observing the mAP, a true analysis cannot be made as the models are performing well on most of the cases.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Upon comparing the models, the results from Table <a href="#S3.T2" title="TABLE II ‚Ä£ III-D Hyper-Parameters ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> show that YOLOv5 performs the best out of all the three models, especially for small objects.
Upon comparing the datasets, the models trained on a dataset containing real data components (DS-1, DS-2, DS-3, DS-5) seems to perform better on the dataset with only synthetic data (DS-4). This is due to the synthetic data generation features, as mentioned in section <a href="#S3.SS2" title="III-B Dataset Generation ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, where the samples created were non-photo-realistic and therefore upon testing in real dataset, the model trained only on synthetic dataset seem to perform inadequately.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">It was also observed that as the number of synthetic data points increases (from DS-1 to DS-3), the models tend to learn more features through the targeted class balancing technique and reaches a maximum performance in the DS-3 dataset, where the number of synthetic data in the dataset is exactly the half of real data instances. Upon further increasing the synthetic dataset (DS-4 &amp; DS-5) the models tend to learn more features from the prominent synthetic data and therefore performed poorly upon testing in the real world. And comparing the hyper-parameters, the models trained for 100 epochs seem to perform better compared to the other instances.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Comparing the recent advancements in the YOLO model families, the YOLOv7 model performs very well detecting small objects in the scenario of this work. The results of this model for different experiments which are presented in Table <a href="#S3.T3" title="TABLE III ‚Ä£ III-D Hyper-Parameters ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, clearly show that by increasing the number of synthetic data samples, can improve the accuracy of the model to detect small objects as well as medium and large objects, but there is a limitation for that and increasing these samples too much can affect the results negatively. The results show that the highest performance occurs for DS-3, when the number of synthetic data is equal to half of real data samples.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">The results of SSD model which are presented in Table <a href="#S3.T4" title="TABLE IV ‚Ä£ III-D Hyper-Parameters ‚Ä£ III Methodology ‚Ä£ Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> show that while this model can not achieve the best results compared to the other YOLO models, using and combining an amount of synthetic data with the real data samples to train the model can affect the performance and improve the accuracy of the model. The table shows that the results of training the SSD model with only synthetic data are very weak because of the difference between the synthetic dataset and the real object images, especially considering the background. On the other hand, when the amount of synthetic data samples is half of the real dataset, the performance of the SSD model is the highest (same as the YOLOv7 model), which means combining one-third of synthetic data with two-third of real dataset can affect and improve the performance of the SSD model to achieve the best result.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">In a nutshell, adding synthetic data with class balancing technique can improve the performance especially for small objects, even by synthetically generating additional data points build using simple open source CAD models. The results show an increase of up to 11.4% in precision on small objects (APs) to the base model trained only on real data set (DS-1) and best combination ratio of real and synthetic dataset (DS-3) for 100 epochs. Similar percentage increase in precision and recall can be observed for models trained on 30 and 50 epochs as well.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Certain scenarios, where real-world dataset are be limited and biased towards prominent objects, present challenges for detecting less prominent and significant objects, such as small-sized objects. However, by increasing the dataset through targeted class balancing techniques using synthetic data generation, there appears to be an improvement in the performance of data-driven object detection models. The results from our use case show that even with low quality of synthetic dataset, an increase of 10% in the mAP for small objects can be achieved using this technique. But Adding more synthetic images seem to degrade the model‚Äôs performance as the generated data were not photo-realistic compared to the real world data. In comparison with three state-of-the-art OD Models, YOLOv5 performed better in this particular use case. It is worth to note that, even though YOLOv7 claims to perform best for small objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, it did not perform as well as its predecessor YOLOv5 under similar training conditions for small objects.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The task of annotating the small object dataset is tedious, and using synthetic data for automatic annotations is an optimum approach to improve the model performance. Together with class balanced data generation techniques, through synthetic data generation without implementing photo-realistic rending, a better performing models can be easily generated without much effort. For certain industrial use cases, these techniques can be really helpful in achieving the business goals.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Although this study gives a clear comparison of state-of-the-art OD models, it has potential for many future works. By extending the quality of the generated images to more photo-realistic and bring closer to the real world could potentially improve the performance a lot. But the data diversity challenges in the real world and the corresponding model performance and comparing it to the costs of data generation can still debatable.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research work is funded by the German BMBF - Bundesministerium f√ºr Bildung und Forschung (0IW19002, project InCoRAP). We would like to thank Al Harith Farhad for proofreading the paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Krizhevsky,¬†A., Sutskever,¬†I., and Hinton,¬†G.¬†E., ‚ÄúImagenet classification
with deep convolutional neural networks,‚Äù in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, Pereira,¬†F., Burges,¬†C. J.¬†C., Bottou,¬†L.,
and Weinberger,¬†K.¬†Q., Eds., vol.¬†25.¬†¬†¬†Curran Associates, Inc., 2012. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</span>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Sermanet,¬†P., Eigen,¬†D., Zhang,¬†X., Mathieu,¬†M., Fergus,¬†R., and LeCun,¬†Y.,
‚ÄúOverfeat: Integrated recognition, localization and detection using
convolutional networks,‚Äù <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.6229</em>, 2013.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Girshick,¬†R.¬†B., Donahue,¬†J., Darrell,¬†T., and Malik,¬†J., ‚ÄúRich feature
hierarchies for accurate object detection and semantic segmentation,‚Äù
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1311.2524, 2013. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1311.2524</span>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Girshick,¬†R., ‚ÄúFast r-cnn,‚Äù in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on
Computer Vision (ICCV)</em>, 2015, pp. 1440‚Äì1448.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ren,¬†S., He,¬†K., Girshick,¬†R., and Sun,¬†J., ‚ÄúFaster r-cnn: Towards real-time
object detection with region proposal networks,‚Äù <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Pattern Analysis and Machine Intelligence</em>, vol.¬†39, no.¬†6, pp. 1137‚Äì1149,
2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Redmon,¬†J., Divvala,¬†S., Girshick,¬†R., and Farhadi,¬†A., ‚ÄúYou only look once:
Unified, real-time object detection,‚Äù in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2016, pp. 779‚Äì788.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chen,¬†X., Fang,¬†H., Lin,¬†T., Vedantam,¬†R., Gupta,¬†S., Doll√°r,¬†P., and
Zitnick,¬†C.¬†L., ‚ÄúMicrosoft COCO captions: Data collection and evaluation
server,‚Äù <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1504.00325, 2015. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1504.00325</span>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Papers with code - coco test-dev benchmark (object detection). [Online].
Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://paperswithcode.com/sota/object-detection-on-coco?metric=APS</span>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Bochkovskiy,¬†A., Wang,¬†C., and Liao,¬†H.¬†M., ‚ÄúYolov4: Optimal speed and
accuracy of object detection,‚Äù <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2004.10934, 2020.
[Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2004.10934</span>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Borkman,¬†S., Crespi,¬†A., Dhakad,¬†S., Ganguly,¬†S., Hogins,¬†J., Jhang,¬†Y.-C.,
Kamalzadeh,¬†M., Li,¬†B., Leal,¬†S., Parisi,¬†P. <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúUnity
perception: Generate synthetic data for computer vision,‚Äù <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2107.04259</em>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ren,¬†S., He,¬†K., Girshick,¬†R., and Sun,¬†J., ‚ÄúFaster r-cnn: Towards real-time
object detection with region proposal networks,‚Äù <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol.¬†28, 2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Liu,¬†W., Anguelov,¬†D., Erhan,¬†D., Szegedy,¬†C., Reed,¬†S., Fu,¬†C.-Y., and
Berg,¬†A.¬†C., ‚ÄúSsd: Single shot multibox detector,‚Äù in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Computer
Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
October 11‚Äì14, 2016, Proceedings, Part I 14</em>.¬†¬†¬†Springer, 2016, pp. 21‚Äì37.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Redmon,¬†J., Divvala,¬†S., Girshick,¬†R., and Farhadi,¬†A., ‚ÄúYou only look once:
Unified, real-time object detection,‚Äù in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 779‚Äì788.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
D‚ÄôAngelo,¬†T., Mendes,¬†M., Keller,¬†B., Ferreira,¬†R., Delabrida,¬†S., Rabelo,¬†R.,
Azpurua,¬†H., and Bianchi,¬†A., ‚ÄúDeep learning-based object detection for
digital inspection in the mining industry,‚Äù in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2019 18th IEEE
International Conference On Machine Learning And Applications (ICMLA)</em>, 2019,
pp. 633‚Äì640.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Usamentiaga,¬†R., Lema,¬†D.¬†G., Pedrayes,¬†O.¬†D., and Garcia,¬†D.¬†F., ‚ÄúAutomated
surface defect detection in metals: A comparative review of object detection
and semantic segmentation using deep learning,‚Äù <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Industry Applications</em>, vol.¬†58, no.¬†3, pp. 4203‚Äì4213, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kang,¬†K., Li,¬†H., Yan,¬†J., Zeng,¬†X., Yang,¬†B., Xiao,¬†T., Zhang,¬†C., Wang,¬†Z.,
Wang,¬†R., Wang,¬†X., and Ouyang,¬†W., ‚ÄúT-cnn: Tubelets with convolutional
neural networks for object detection from videos,‚Äù <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions
on Circuits and Systems for Video Technology</em>, vol.¬†28, no.¬†10, pp.
2896‚Äì2907, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Dai,¬†J., He,¬†K., and Sun,¬†J., ‚ÄúInstance-aware semantic segmentation via
multi-task network cascades,‚Äù in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, 2016, pp. 3150‚Äì3158.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Herath,¬†S., Harandi,¬†M., and Porikli,¬†F., ‚ÄúGoing deeper into action
recognition: A survey,‚Äù <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Image and Vision Computing</em>, vol.¬†60, pp.
4‚Äì21, 2017, regularization Techniques for High-Dimensional Data Analysis.
[Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S0262885617300343</span>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Menikdiwela,¬†M., Nguyen,¬†C., Li,¬†H., and Shaw,¬†M., ‚ÄúCnn-based small object
detection and visualization with feature activation mapping,‚Äù in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2017
International Conference on Image and Vision Computing New Zealand (IVCNZ)</em>,
2017, pp. 1‚Äì5.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Benjumea,¬†A., Teeti,¬†I., Cuzzolin,¬†F., and Bradley,¬†A., ‚ÄúYolo-z: Improving
small object detection in yolov5 for autonomous vehicles,‚Äù 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Zhang,¬†W., Wang,¬†S., Thachan,¬†S., Chen,¬†J., and Qian,¬†Y., ‚ÄúDeconv r-cnn for
small object detection on remote sensing images,‚Äù in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IGARSS 2018 -
2018 IEEE International Geoscience and Remote Sensing Symposium</em>, 2018, pp.
2483‚Äì2486.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Zhou,¬†X., Xu,¬†X., Liang,¬†W., Zeng,¬†Z., Shimizu,¬†S., Yang,¬†L.¬†T., and Jin,¬†Q.,
‚ÄúIntelligent small object detection for digital twin in smart manufacturing
with industrial cyber-physical systems,‚Äù <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Industrial Informatics</em>, vol.¬†18, no.¬†2, pp. 1377‚Äì1386, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Zheng,¬†X., Zheng,¬†S., Kong,¬†Y., and Chen,¬†J., ‚ÄúRecent advances in surface
defect inspection of industrial products using deep learning techniques,‚Äù
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">The International Journal of Advanced Manufacturing Technology</em>, vol.
113, 03 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
L√≥pez de la Rosa,¬†F., G√≥mez-Sirvent,¬†J.¬†L., S√°nchez-Reolid,¬†R.,
Morales,¬†R., and Fern√°ndez-Caballero,¬†A., ‚ÄúGeometric transformation-based
data augmentation on defect classification of segmented images of
semiconductor materials using a resnet50 convolutional neural network,‚Äù
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 206, p. 117731, 2022. [Online].
Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S0957417422010120</span>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Wang,¬†J. and Lee,¬†S., ‚ÄúData augmentation methods applying grayscale images for
convolutional neural networks in machine vision,‚Äù <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>,
vol.¬†11, no.¬†15, 2021. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.mdpi.com/2076-3417/11/15/6721</span>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Martins,¬†D.¬†H., de¬†Lima,¬†A.¬†A., Pinto,¬†M.¬†F., Hemerly,¬†D. d.¬†O., Prego,¬†T.
d.¬†M., Tarrataca,¬†L., Monteiro,¬†U.¬†A., Guti√©rrez,¬†R.¬†H., Haddad,¬†D.¬†B.
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúHybrid data augmentation method for combined failure
recognition in rotating machines,‚Äù <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">Journal of Intelligent
Manufacturing</em>, pp. 1‚Äì19, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Salamon,¬†J. and Bello,¬†J.¬†P., ‚ÄúDeep convolutional neural networks and data
augmentation for environmental sound classification,‚Äù <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Signal
Processing Letters</em>, vol.¬†24, no.¬†3, pp. 279‚Äì283, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Emam,¬†K., Mosquera,¬†L., and Hoptroff,¬†R., <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Chapter 1: Introducing
Synthetic Data Generation</em>.¬†¬†¬†O‚ÄôReilly
Media, Inc., 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Figueira,¬†A. and Vaz,¬†B., ‚ÄúSurvey on synthetic data generation, evaluation
methods and gans,‚Äù <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Mathematics</em>, vol.¬†10, no.¬†15, p. 2733, 2022.
[Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://dx.doi.org/10.3390/math10152733</span>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kingma,¬†D.¬†P. and Welling,¬†M., ‚ÄúAn introduction to variational autoencoders,‚Äù
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends¬Æ in Machine Learning</em>, vol.¬†12,
no.¬†4, pp. 307‚Äì392, 2019. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1561\%2F2200000056</span>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Ho,¬†J., Jain,¬†A., and Abbeel,¬†P., ‚ÄúDenoising diffusion probabilistic models,‚Äù
in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Neural
Information Processing Systems</em>, ser. NIPS‚Äô20.¬†¬†¬†Red Hook, NY, USA: Curran Associates Inc., 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Goodfellow,¬†I., Pouget-Abadie,¬†J., Mirza,¬†M., Xu,¬†B., Warde-Farley,¬†D.,
Ozair,¬†S., Courville,¬†A., and Bengio,¬†Y., ‚ÄúGenerative adversarial nets,‚Äù in
<em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, Ghahramani,¬†Z.,
Welling,¬†M., Cortes,¬†C., Lawrence,¬†N., and Weinberger,¬†K., Eds.,
vol.¬†27.¬†¬†¬†Curran Associates, Inc.,
2014. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf</span>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Karras,¬†T., Aittala,¬†M., Laine,¬†S., H√§rk√∂nen,¬†E., Hellsten,¬†J., Lehtinen,¬†J.,
and Aila,¬†T., ‚ÄúAlias-free generative adversarial networks,‚Äù 2021. [Online].
Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2106.12423</span>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Rajpura,¬†P.¬†S., Bojinov,¬†H., and Hegde,¬†R.¬†S., ‚ÄúObject detection using deep
cnns trained on synthetic images,‚Äù <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.06782</em>,
2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Kiefer,¬†B., Ott,¬†D., and Zell,¬†A., ‚ÄúLeveraging synthetic data in object
detection on unmanned aerial vehicles,‚Äù in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2022 26th International
Conference on Pattern Recognition (ICPR)</em>, 2022, pp. 3564‚Äì3571.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Josifovski,¬†J., Kerzel,¬†M., Pregizer,¬†C., Posniak,¬†L., and Wermter,¬†S.,
‚ÄúObject detection and pose estimation based on convolutional neural networks
trained with synthetic data,‚Äù in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)</em>, 2018, pp. 6269‚Äì6276.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Planche,¬†B., Wu,¬†Z., Ma,¬†K., Sun,¬†S., Kluckner,¬†S., Lehmann,¬†O., Chen,¬†T.,
Hutter,¬†A., Zakharov,¬†S., Kosch,¬†H. <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúDepthsynth: Real-time
realistic synthetic data generation from cad models for 2.5 d recognition,‚Äù
in <em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic">2017 International conference on 3d vision (3DV)</em>.¬†¬†¬†IEEE, 2017, pp. 1‚Äì10.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Nikolenko,¬†S.¬†I., <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Synthetic-to-Real Domain Adaptation and
Refinement</em>.¬†¬†¬†Cham: Springer
International Publishing, 2021, pp. 235‚Äì268.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Tremblay,¬†J., Prakash,¬†A., Acuna,¬†D., Brophy,¬†M., Jampani,¬†V., Anil,¬†C.,
To,¬†T., Cameracci,¬†E., Boochoon,¬†S., and Birchfield,¬†S., ‚ÄúTraining deep
networks with synthetic data: Bridging the reality gap by domain
randomization,‚Äù in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition workshops</em>, 2018, pp. 969‚Äì977.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Man,¬†K. and Chahl,¬†J., ‚ÄúA review of synthetic image data and its use in
computer vision,‚Äù <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Journal of Imaging</em>, vol.¬†8, no.¬†11, 2022. [Online].
Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.mdpi.com/2313-433X/8/11/310</span>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Redmon,¬†J., Divvala,¬†S., Girshick,¬†R., and Farhadi,¬†A., ‚ÄúYou only look once:
Unified, real-time object detection,‚Äù 2016.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Wang,¬†C.-Y., Bochkovskiy,¬†A., and Liao,¬†H.-Y.¬†M., ‚ÄúYolov7: Trainable
bag-of-freebies sets new state-of-the-art for real-time object detectors,‚Äù
in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</em>, 2023, pp. 7464‚Äì7475.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Lin,¬†T.-Y., Maire,¬†M., Belongie,¬†S., Bourdev,¬†L., Girshick,¬†R., Hays,¬†J.,
Perona,¬†P., Ramanan,¬†D., Zitnick,¬†C.¬†L., and Doll√°r,¬†P., ‚ÄúMicrosoft coco:
Common objects in context,‚Äù 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.12728" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.12729" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.12729">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.12729" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.12730" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 07:59:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
