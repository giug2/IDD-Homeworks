<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1810.10093] Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data</title><meta property="og:description" content="We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure and context of the scene.
In contrast to DR, which places objects and distractors randomly …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1810.10093">

<!--Generated on Sat Mar 16 23:49:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aayush Prakash
<br class="ltx_break">Eric Cameracci
</span><span class="ltx_author_notes">Authors are affiliated with NVIDIA. Email: <span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">{aayushp, sboochoon, markb, dacunamarrer, ecameracci, gstate, oshapira, sbirchfield}@nvidia.com</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shaad Boochoon 
<br class="ltx_break">Gavriel State
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mark Brophy 
<br class="ltx_break">Omer Shapira
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Acuna 
<br class="ltx_break">Stan Birchfield
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure and context of the scene.
In contrast to DR, which places objects and distractors randomly according to a uniform probability distribution, SDR places objects and distractors randomly according to probability distributions that arise from the specific problem at hand.
In this manner, SDR-generated imagery enables the neural network to take the context around an object into consideration during detection.
We demonstrate the power of SDR for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data.
On the KITTI easy, moderate, and hard tasks, we show that SDR outperforms other approaches to generating synthetic data (VKITTI, Sim 200k, or DR), as well as real data collected in a different domain (BDD100K). Moreover, synthetic SDR data combined with real KITTI data outperforms real KITTI data alone.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Training deep networks for computer vision tasks typically requires large amounts of labeled training data.
Annotating such data is laborious and time-consuming, thus making it cost-prohibitive for tasks for which the labels are particularly difficult to acquire, such as instance segmentation, optical flow estimation, or depth estimation.
Even for problems like 2D bounding box detection, there is a motivation to avoid the expensive labeling process.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Synthetic data is an attractive alternative because data annotation is essentially free.
Recently a number of synthetic datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> have been generated for training deep networks.
These datasets require either carefully designed simulation environments or the existence of annotated real data as a starting point.
To alleviate such difficulties, Domain Randomization (DR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposes to randomize the input so as to minimize the need for artistic design of the environment or prior real data.
Recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> demonstrated the ability of DR to achieve state-of-the-art in 2D bounding box detection of cars in the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, the results of that research were limited to larger objects (KITTI Easy) for which sufficient pixels exist within the bounding box for neural networks to make a decision without the surrounding context.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper we extend that work to handle more challenging criteria (KITTI Moderate and Hard). The ground truth of these criteria include small, occluded (partial to heavy), and significantly truncated objects. In these cases, such objects occupy only a few pixels in the image, thus making it necessary to take into account the surrounding context of the scene. To address this problem, we propose <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">structured domain randomization (SDR)</em>, which adds structure and context to domain randomization (DR).
We present a methodology to train deep networks for object detection using only synthetic data generated by SDR, and we show that the results from this process not only outperform other approaches to generating synthetic data, but also real data from a different domain.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Video is at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://youtu.be/1WdjWJYx9AY</span>.</span></span></span>
Our contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a context-aware domain randomization procedure called structured domain randomization (SDR).
We describe an implementation of SDR for object detection that takes into account the structure of the scene when randomly placing objects for data generation, which enables the neural network to learn to utilize context when detecting objects.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We demonstrate that SDR achieves state-of-the-art for 2D object detection on KITTI (easy, moderate, and hard). The performance achieved is better than both virtual KITTI (VKITTI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and GTA-based Sim 200k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the two most commonly used synthetic datasets for object detection. Performance is also better than real data, BDD100K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, from a different domain.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Synthetic data has been used in a myriad of vision tasks where labeling images ranges from
tedious to borderline impossible. Applications like optical flow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>,
scene flow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>,
classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>,
stereo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>,
semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>,
3D keypoint extraction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>,
object pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and
3D reconstruction  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> have all
benefited from the use of synthetic training data.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Synthetic data has also been utilized for object detection, the problem for which our SDR
algorithm proposes an approach.
Gaidon <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
created a synthetic clone of five videos from the KITTI detection dataset called Virtual KITTI (VKITTI).
They demonstrated the ability to
train a model for object detection using synthetic data that learns features which are generalizable to real images.
Johnson-Roberson <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> used synthetic data
captured from GTA V to train an object detector for cars and demonstrated that
photo-realism aided in the training of DNNs.
Mueller <span id="S2.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> built Sim4CV for autonomous navigation and tracking
on top of Unreal Engine, focusing on generating realistic synthetic scenes like
those seen in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> but further studying problems of control and
autonomous navigation when utilizing synthetic data.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Hinterstoisser <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> eschewed the idea of photorealism and
generated images by adding Gaussian noise to the foreground of the rendered image and performing
Gaussian blurring on the object edges to better integrate with the background image.
The resulting synthetic data was used to train the later layers of a neural network while freezing the early layers pretrained on real data (<em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, ImageNet).
Dwibedi <span id="S2.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> extended this idea, using
multiple blending techniques and
varying the blending parameters to make the detector more robust to object boundaries
and thus improve performance.
Mayer <span id="S2.p3.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> found that when training neural networks
to perform optical flow estimation, simplistic data with augmentation is sufficient
(they conclude: “realism is overrated”) but concede that the same may not be true for
high-level tasks like object recognition. Indeed, our work offers further evidence in
this direction.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Tobin <span id="S2.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> introduced the concept of Domain Randomization (DR), in which realistic rendering is avoided in favor of random variation.
Their approach randomly varies the texture
and color of the foreground object, the background image, the number of lights in the
scene, the pose of the lights, the camera position, and the foreground objects. The goal is to
close the reality gap by generating synthetic data with sufficient variation that the network views real-world data as just another variation.
Using DR, they trained a neural network to estimate the 3D world position of various shape-based objects with respect to a robotic arm fixed to a table. Sundermeyer <span id="S2.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> use DR for object detection and 6D pose estimation, achieving competitive results compared to pose estimation using real data.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Our previous work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> used DR to train an object detector for cars, which was tested on KITTI, similar to the work presented here.
In that research, we learned that DR requires a large amount of data to train given the amount of variation, often the network finds it hard to learn the correct features, and the lack of context prevents DR from detecting small vehicles.
The research described in this paper aims to address these limitations.
Other researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> have also found context
to be important. Georgakis <span id="S2.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> create training data for indoor robotics by
locating planes in background images and pasting foreground objects onto them, an acknowledgment of
the importance of context.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Structured Domain Randomization (SDR)</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Structured Domain Randomization (SDR) is a general technique for procedurally generating synthetic random images that preserve the structure, or context, of the problem at hand.
In our formulation, SDR involves three types of components: 1) global parameters, 2) one or more context splines, and 3) objects that are placed along the splines.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The joint probability of generating a particular image <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">I</annotation></semantics></math> and the parameters, splines, and objects is given by the following:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.7" class="ltx_math_unparsed" alttext="\displaystyle p(I,s,{\bf{g}},{\bf{o}}_{1..n_{o}},{\bf{c}}_{1..n_{c}})" display="inline"><semantics id="S3.Ex1.m1.7a"><mrow id="S3.Ex1.m1.7.7"><mi id="S3.Ex1.m1.7.7.4">p</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.7.7.3">​</mo><mrow id="S3.Ex1.m1.7.7.2.2"><mo stretchy="false" id="S3.Ex1.m1.7.7.2.2.3">(</mo><mi id="S3.Ex1.m1.3.3">I</mi><mo id="S3.Ex1.m1.7.7.2.2.4">,</mo><mi id="S3.Ex1.m1.4.4">s</mi><mo id="S3.Ex1.m1.7.7.2.2.5">,</mo><mi id="S3.Ex1.m1.5.5">𝐠</mi><mo id="S3.Ex1.m1.7.7.2.2.6">,</mo><msub id="S3.Ex1.m1.6.6.1.1.1"><mi id="S3.Ex1.m1.6.6.1.1.1.2">𝐨</mi><mrow id="S3.Ex1.m1.1.1.1"><mn id="S3.Ex1.m1.1.1.1.1">1</mn><mo lspace="0em" rspace="0.0835em" id="S3.Ex1.m1.1.1.1.2">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.Ex1.m1.1.1.1.3">.</mo><msub id="S3.Ex1.m1.1.1.1.4"><mi id="S3.Ex1.m1.1.1.1.4.2">n</mi><mi id="S3.Ex1.m1.1.1.1.4.3">o</mi></msub></mrow></msub><mo id="S3.Ex1.m1.7.7.2.2.7">,</mo><msub id="S3.Ex1.m1.7.7.2.2.2"><mi id="S3.Ex1.m1.7.7.2.2.2.2">𝐜</mi><mrow id="S3.Ex1.m1.2.2.1"><mn id="S3.Ex1.m1.2.2.1.1">1</mn><mo lspace="0em" rspace="0.0835em" id="S3.Ex1.m1.2.2.1.2">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.Ex1.m1.2.2.1.3">.</mo><msub id="S3.Ex1.m1.2.2.1.4"><mi id="S3.Ex1.m1.2.2.1.4.2">n</mi><mi id="S3.Ex1.m1.2.2.1.4.3">c</mi></msub></mrow></msub><mo stretchy="false" id="S3.Ex1.m1.7.7.2.2.8">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.Ex1.m1.7b">\displaystyle p(I,s,{\bf{g}},{\bf{o}}_{1..n_{o}},{\bf{c}}_{1..n_{c}})</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.5" class="ltx_math_unparsed" alttext="\displaystyle=p(I|s,{\bf{g}},{\bf{o}}_{1..n_{o}},{\bf{c}}_{1..n_{c}})" display="inline"><semantics id="S3.Ex1.m2.5a"><mrow id="S3.Ex1.m2.5.5"><mi id="S3.Ex1.m2.5.5.3"></mi><mo id="S3.Ex1.m2.5.5.2">=</mo><mrow id="S3.Ex1.m2.5.5.1"><mi id="S3.Ex1.m2.5.5.1.3">p</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.5.5.1.2">​</mo><mrow id="S3.Ex1.m2.5.5.1.1.1"><mo stretchy="false" id="S3.Ex1.m2.5.5.1.1.1.2">(</mo><mrow id="S3.Ex1.m2.5.5.1.1.1.1"><mi id="S3.Ex1.m2.5.5.1.1.1.1.4">I</mi><mo fence="false" id="S3.Ex1.m2.5.5.1.1.1.1.3">|</mo><mrow id="S3.Ex1.m2.5.5.1.1.1.1.2.2"><mi id="S3.Ex1.m2.3.3">s</mi><mo id="S3.Ex1.m2.5.5.1.1.1.1.2.2.3">,</mo><mi id="S3.Ex1.m2.4.4">𝐠</mi><mo id="S3.Ex1.m2.5.5.1.1.1.1.2.2.4">,</mo><msub id="S3.Ex1.m2.5.5.1.1.1.1.1.1.1"><mi id="S3.Ex1.m2.5.5.1.1.1.1.1.1.1.2">𝐨</mi><mrow id="S3.Ex1.m2.1.1.1"><mn id="S3.Ex1.m2.1.1.1.1">1</mn><mo lspace="0em" rspace="0.0835em" id="S3.Ex1.m2.1.1.1.2">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.Ex1.m2.1.1.1.3">.</mo><msub id="S3.Ex1.m2.1.1.1.4"><mi id="S3.Ex1.m2.1.1.1.4.2">n</mi><mi id="S3.Ex1.m2.1.1.1.4.3">o</mi></msub></mrow></msub><mo id="S3.Ex1.m2.5.5.1.1.1.1.2.2.5">,</mo><msub id="S3.Ex1.m2.5.5.1.1.1.1.2.2.2"><mi id="S3.Ex1.m2.5.5.1.1.1.1.2.2.2.2">𝐜</mi><mrow id="S3.Ex1.m2.2.2.1"><mn id="S3.Ex1.m2.2.2.1.1">1</mn><mo lspace="0em" rspace="0.0835em" id="S3.Ex1.m2.2.2.1.2">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.Ex1.m2.2.2.1.3">.</mo><msub id="S3.Ex1.m2.2.2.1.4"><mi id="S3.Ex1.m2.2.2.1.4.2">n</mi><mi id="S3.Ex1.m2.2.2.1.4.3">c</mi></msub></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.Ex1.m2.5.5.1.1.1.3">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S3.Ex1.m2.5b">\displaystyle=p(I|s,{\bf{g}},{\bf{o}}_{1..n_{o}},{\bf{c}}_{1..n_{c}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\cdot\prod_{j=1}^{n_{o}}p({\bf{o}}_{j}|{\bf{c}}_{i})\prod_{i=1}^{n_{c}}p({\bf{c}}_{i}|{\bf{g}})p({\bf{g}}|s)p(s)," display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1b"><mo rspace="0.222em" id="S3.E1.m1.1.1">⋅</mo><mstyle displaystyle="true" id="S3.E1.m1.1.2"><munderover id="S3.E1.m1.1.2a"><mo movablelimits="false" id="S3.E1.m1.1.2.2.2">∏</mo><mrow id="S3.E1.m1.1.2.2.3"><mi id="S3.E1.m1.1.2.2.3.2">j</mi><mo id="S3.E1.m1.1.2.2.3.1">=</mo><mn id="S3.E1.m1.1.2.2.3.3">1</mn></mrow><msub id="S3.E1.m1.1.2.3"><mi id="S3.E1.m1.1.2.3.2">n</mi><mi id="S3.E1.m1.1.2.3.3">o</mi></msub></munderover></mstyle><mi id="S3.E1.m1.1.3">p</mi><mrow id="S3.E1.m1.1.4"><mo stretchy="false" id="S3.E1.m1.1.4.1">(</mo><msub id="S3.E1.m1.1.4.2"><mi id="S3.E1.m1.1.4.2.2">𝐨</mi><mi id="S3.E1.m1.1.4.2.3">j</mi></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E1.m1.1.4.3">|</mo><msub id="S3.E1.m1.1.4.4"><mi id="S3.E1.m1.1.4.4.2">𝐜</mi><mi id="S3.E1.m1.1.4.4.3">i</mi></msub><mo stretchy="false" id="S3.E1.m1.1.4.5">)</mo></mrow><mstyle displaystyle="true" id="S3.E1.m1.1.5"><munderover id="S3.E1.m1.1.5a"><mo movablelimits="false" id="S3.E1.m1.1.5.2.2">∏</mo><mrow id="S3.E1.m1.1.5.2.3"><mi id="S3.E1.m1.1.5.2.3.2">i</mi><mo id="S3.E1.m1.1.5.2.3.1">=</mo><mn id="S3.E1.m1.1.5.2.3.3">1</mn></mrow><msub id="S3.E1.m1.1.5.3"><mi id="S3.E1.m1.1.5.3.2">n</mi><mi id="S3.E1.m1.1.5.3.3">c</mi></msub></munderover></mstyle><mi id="S3.E1.m1.1.6">p</mi><mrow id="S3.E1.m1.1.7"><mo stretchy="false" id="S3.E1.m1.1.7.1">(</mo><msub id="S3.E1.m1.1.7.2"><mi id="S3.E1.m1.1.7.2.2">𝐜</mi><mi id="S3.E1.m1.1.7.2.3">i</mi></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E1.m1.1.7.3">|</mo><mi id="S3.E1.m1.1.7.4">𝐠</mi><mo stretchy="false" id="S3.E1.m1.1.7.5">)</mo></mrow><mi id="S3.E1.m1.1.8">p</mi><mrow id="S3.E1.m1.1.9"><mo stretchy="false" id="S3.E1.m1.1.9.1">(</mo><mi id="S3.E1.m1.1.9.2">𝐠</mi><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E1.m1.1.9.3">|</mo><mi id="S3.E1.m1.1.9.4">s</mi><mo stretchy="false" id="S3.E1.m1.1.9.5">)</mo></mrow><mi id="S3.E1.m1.1.10">p</mi><mrow id="S3.E1.m1.1.11"><mo stretchy="false" id="S3.E1.m1.1.11.1">(</mo><mi id="S3.E1.m1.1.11.2">s</mi><mo stretchy="false" id="S3.E1.m1.1.11.3">)</mo></mrow><mo id="S3.E1.m1.1.12">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle\cdot\prod_{j=1}^{n_{o}}p({\bf{o}}_{j}|{\bf{c}}_{i})\prod_{i=1}^{n_{c}}p({\bf{c}}_{i}|{\bf{g}})p({\bf{g}}|s)p(s),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p2.2" class="ltx_p">which is depicted in Fig. <a href="#S3.F1" title="Figure 1 ‣ III Structured Domain Randomization (SDR) ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1810.10093/assets/figures/graphical_model.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="150" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Probabilistic relationship among different components of SDR. The scenario (<math id="S3.F1.5.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.F1.5.m1.1b"><mi id="S3.F1.5.m1.1.1" xref="S3.F1.5.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.F1.5.m1.1c"><ci id="S3.F1.5.m1.1.1.cmml" xref="S3.F1.5.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.5.m1.1d">s</annotation></semantics></math>) determines the global parameters (<math id="S3.F1.6.m2.1" class="ltx_Math" alttext="{\bf{g}}" display="inline"><semantics id="S3.F1.6.m2.1b"><mi id="S3.F1.6.m2.1.1" xref="S3.F1.6.m2.1.1.cmml">𝐠</mi><annotation-xml encoding="MathML-Content" id="S3.F1.6.m2.1c"><ci id="S3.F1.6.m2.1.1.cmml" xref="S3.F1.6.m2.1.1">𝐠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.6.m2.1d">{\bf{g}}</annotation></semantics></math>), which govern the context splines (<math id="S3.F1.7.m3.1" class="ltx_Math" alttext="{\bf{c}}_{i}" display="inline"><semantics id="S3.F1.7.m3.1b"><msub id="S3.F1.7.m3.1.1" xref="S3.F1.7.m3.1.1.cmml"><mi id="S3.F1.7.m3.1.1.2" xref="S3.F1.7.m3.1.1.2.cmml">𝐜</mi><mi id="S3.F1.7.m3.1.1.3" xref="S3.F1.7.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F1.7.m3.1c"><apply id="S3.F1.7.m3.1.1.cmml" xref="S3.F1.7.m3.1.1"><csymbol cd="ambiguous" id="S3.F1.7.m3.1.1.1.cmml" xref="S3.F1.7.m3.1.1">subscript</csymbol><ci id="S3.F1.7.m3.1.1.2.cmml" xref="S3.F1.7.m3.1.1.2">𝐜</ci><ci id="S3.F1.7.m3.1.1.3.cmml" xref="S3.F1.7.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.7.m3.1d">{\bf{c}}_{i}</annotation></semantics></math>), upon which the objects (<math id="S3.F1.8.m4.1" class="ltx_Math" alttext="{\bf{o}}_{j}" display="inline"><semantics id="S3.F1.8.m4.1b"><msub id="S3.F1.8.m4.1.1" xref="S3.F1.8.m4.1.1.cmml"><mi id="S3.F1.8.m4.1.1.2" xref="S3.F1.8.m4.1.1.2.cmml">𝐨</mi><mi id="S3.F1.8.m4.1.1.3" xref="S3.F1.8.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F1.8.m4.1c"><apply id="S3.F1.8.m4.1.1.cmml" xref="S3.F1.8.m4.1.1"><csymbol cd="ambiguous" id="S3.F1.8.m4.1.1.1.cmml" xref="S3.F1.8.m4.1.1">subscript</csymbol><ci id="S3.F1.8.m4.1.1.2.cmml" xref="S3.F1.8.m4.1.1.2">𝐨</ci><ci id="S3.F1.8.m4.1.1.3.cmml" xref="S3.F1.8.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.8.m4.1d">{\bf{o}}_{j}</annotation></semantics></math>) are placed. The context splines capture the structure of the scene. The image is rendered from these parameters, splines, and objects.</figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">First, a scenario <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">s</annotation></semantics></math> is determined randomly. In our implementation, there are approximately 20 scenarios, such as “rural 2-lane road”, “suburban 4-lane road with a sidewalk”, or “urban 6-lane road with a grassy median and a sidewalk”. The scenario is chosen from a uniform distribution across all possibilities. Figure <a href="#S3.F2" title="Figure 2 ‣ III Structured Domain Randomization (SDR) ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows examples of some scenarios.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.F2.9" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S3.F2.3.3" class="ltx_tr">
<td id="S3.F2.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/Detail_01.jpg" id="S3.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="180" height="87" alt="Refer to caption"></td>
<td id="S3.F2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/Detail_12.jpg" id="S3.F2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="180" height="84" alt="Refer to caption"></td>
<td id="S3.F2.3.3.3" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/Detail_07.jpg" id="S3.F2.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="180" height="85" alt="Refer to caption"></td>
</tr>
<tr id="S3.F2.6.6" class="ltx_tr">
<td id="S3.F2.4.4.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/Detail_17a.jpg" id="S3.F2.4.4.1.g1" class="ltx_graphics ltx_img_landscape" width="180" height="85" alt="Refer to caption"></td>
<td id="S3.F2.5.5.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/Detail_13a.jpg" id="S3.F2.5.5.2.g1" class="ltx_graphics ltx_img_landscape" width="180" height="85" alt="Refer to caption"></td>
<td id="S3.F2.6.6.3" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/Detail_10.jpg" id="S3.F2.6.6.3.g1" class="ltx_graphics ltx_img_landscape" width="180" height="87" alt="Refer to caption"></td>
</tr>
<tr id="S3.F2.9.9" class="ltx_tr">
<td id="S3.F2.7.7.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/City_03.jpg" id="S3.F2.7.7.1.g1" class="ltx_graphics ltx_img_landscape" width="180" height="85" alt="Refer to caption"></td>
<td id="S3.F2.8.8.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/Residential_03.jpg" id="S3.F2.8.8.2.g1" class="ltx_graphics ltx_img_landscape" width="180" height="85" alt="Refer to caption"></td>
<td id="S3.F2.9.9.3" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/method/Rural_05.jpg" id="S3.F2.9.9.3.g1" class="ltx_graphics ltx_img_landscape" width="180" height="85" alt="Refer to caption"></td>
</tr>
<tr id="S3.F2.9.10" class="ltx_tr">
<td id="S3.F2.9.10.1" class="ltx_td ltx_align_center">Urban</td>
<td id="S3.F2.9.10.2" class="ltx_td ltx_align_center">Suburban</td>
<td id="S3.F2.9.10.3" class="ltx_td ltx_align_center">Rural</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>In Structured Domain Randomization (SDR), a scenario is chosen at random, then global parameters (road curvature, lighting, camera pose, etc.), which cause context splines (road lanes, sidewalks, etc.) to be generated, upon which objects (cars, trucks, pedestrians, cyclists, houses, buildings, etc.) are placed. The context splines are shown as thin overlaid white lines with white dots indicating control points. Note that these illustrative images were generated from camera viewpoints different from those used for training.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F2.10" class="ltx_p ltx_figure_panel ltx_align_center">.</p>
</div>
</div>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.2" class="ltx_p">Once the scenario has been chosen, the global parameters <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="{\bf{g}}" display="inline"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">𝐠</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">𝐠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">{\bf{g}}</annotation></semantics></math> are determined. These include the spline shape, which is specified by <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="n_{s}=100" display="inline"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><msub id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml"><mi id="S3.p4.2.m2.1.1.2.2" xref="S3.p4.2.m2.1.1.2.2.cmml">n</mi><mi id="S3.p4.2.m2.1.1.2.3" xref="S3.p4.2.m2.1.1.2.3.cmml">s</mi></msub><mo id="S3.p4.2.m2.1.1.1" xref="S3.p4.2.m2.1.1.1.cmml">=</mo><mn id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><eq id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1"></eq><apply id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.2.1.cmml" xref="S3.p4.2.m2.1.1.2">subscript</csymbol><ci id="S3.p4.2.m2.1.1.2.2.cmml" xref="S3.p4.2.m2.1.1.2.2">𝑛</ci><ci id="S3.p4.2.m2.1.1.2.3.cmml" xref="S3.p4.2.m2.1.1.2.3">𝑠</ci></apply><cn type="integer" id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">n_{s}=100</annotation></semantics></math> control points, with a random right/left/straight decision made after every fixed subset of control points. Each right/left turn is fixed at 30 degrees and only allowed if the road is already heading the opposite direction (so as to avoid hairpin turns). Other global parameters include the azimuth/elevation of the sun, time of day, the color temperature and intensity of the sun, sky color, cloud density/positions, the camera yaw/pitch/FOV, the maximum number of vehicles per lane, and so forth. The global parameters also include the number of lanes, whether a median exists, when a sidewalk exists, and so forth.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.3" class="ltx_p">The parameters of each context spline <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="{\bf{c}}_{i}" display="inline"><semantics id="S3.p5.1.m1.1a"><msub id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">𝐜</mi><mi id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">𝐜</ci><ci id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">{\bf{c}}_{i}</annotation></semantics></math> are determined by the global parameters. The <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="n_{c}" display="inline"><semantics id="S3.p5.2.m2.1a"><msub id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml"><mi id="S3.p5.2.m2.1.1.2" xref="S3.p5.2.m2.1.1.2.cmml">n</mi><mi id="S3.p5.2.m2.1.1.3" xref="S3.p5.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><apply id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p5.2.m2.1.1.1.cmml" xref="S3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.p5.2.m2.1.1.2.cmml" xref="S3.p5.2.m2.1.1.2">𝑛</ci><ci id="S3.p5.2.m2.1.1.3.cmml" xref="S3.p5.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">n_{c}</annotation></semantics></math> context splines are adjacent to one another and share their shape. There is one context spline for each lane, one for the median, one for each sidewalk, one for each gutter, and one for each side stretch. These splines receive random colors and textures that govern their appearance, such as the type of grass, darkness of the asphalt, and type of concrete. Overlaid on these splines are various imperfections, such as potholes, cracks, and oil spills on the road.
Figure <a href="#S3.F2" title="Figure 2 ‣ III Structured Domain Randomization (SDR) ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows these different type of context splines <math id="S3.p5.3.m3.1" class="ltx_Math" alttext="{\bf{c}}_{i}" display="inline"><semantics id="S3.p5.3.m3.1a"><msub id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml"><mi id="S3.p5.3.m3.1.1.2" xref="S3.p5.3.m3.1.1.2.cmml">𝐜</mi><mi id="S3.p5.3.m3.1.1.3" xref="S3.p5.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><apply id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p5.3.m3.1.1.1.cmml" xref="S3.p5.3.m3.1.1">subscript</csymbol><ci id="S3.p5.3.m3.1.1.2.cmml" xref="S3.p5.3.m3.1.1.2">𝐜</ci><ci id="S3.p5.3.m3.1.1.3.cmml" xref="S3.p5.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">{\bf{c}}_{i}</annotation></semantics></math> as white lines with control points.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.4" class="ltx_p">The <math id="S3.p6.1.m1.1" class="ltx_Math" alttext="n_{o}" display="inline"><semantics id="S3.p6.1.m1.1a"><msub id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml"><mi id="S3.p6.1.m1.1.1.2" xref="S3.p6.1.m1.1.1.2.cmml">n</mi><mi id="S3.p6.1.m1.1.1.3" xref="S3.p6.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><apply id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p6.1.m1.1.1.1.cmml" xref="S3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.p6.1.m1.1.1.2.cmml" xref="S3.p6.1.m1.1.1.2">𝑛</ci><ci id="S3.p6.1.m1.1.1.3.cmml" xref="S3.p6.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">n_{o}</annotation></semantics></math> objects <math id="S3.p6.2.m2.1" class="ltx_Math" alttext="{\bf{o}}_{j}" display="inline"><semantics id="S3.p6.2.m2.1a"><msub id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml"><mi id="S3.p6.2.m2.1.1.2" xref="S3.p6.2.m2.1.1.2.cmml">𝐨</mi><mi id="S3.p6.2.m2.1.1.3" xref="S3.p6.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><apply id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p6.2.m2.1.1.1.cmml" xref="S3.p6.2.m2.1.1">subscript</csymbol><ci id="S3.p6.2.m2.1.1.2.cmml" xref="S3.p6.2.m2.1.1.2">𝐨</ci><ci id="S3.p6.2.m2.1.1.3.cmml" xref="S3.p6.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">{\bf{o}}_{j}</annotation></semantics></math> are placed randomly on the context splines. We associate different kind of objects with different kind of context splines. Lane splines receive vehicles, sidewalks receive pedestrians and cyclists, side stretch splines receive buildings, houses, and street signs, and so forth. Vehicles are placed in lanes as follows: First, a maximum number of vehicles is determined randomly for each lane, up to some global maximum. Within each lane, a single vehicle is placed at a random distance from the observing vehicle, near the center of the lane and aligned with the road direction. The second vehicle is placed randomly in the road with a minimum offset distance between it and the first vehicle. This process continues until either the maximum number of vehicles for that lane, or the total maximum number of vehicles for the image, has been reached. Similar procedures govern the placement of pedestrians, cyclists, buildings, and road signs.
Figure <a href="#S3.F2" title="Figure 2 ‣ III Structured Domain Randomization (SDR) ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows these objects <math id="S3.p6.3.m3.1" class="ltx_Math" alttext="{\bf{o}}_{j}" display="inline"><semantics id="S3.p6.3.m3.1a"><msub id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml"><mi id="S3.p6.3.m3.1.1.2" xref="S3.p6.3.m3.1.1.2.cmml">𝐨</mi><mi id="S3.p6.3.m3.1.1.3" xref="S3.p6.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><apply id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p6.3.m3.1.1.1.cmml" xref="S3.p6.3.m3.1.1">subscript</csymbol><ci id="S3.p6.3.m3.1.1.2.cmml" xref="S3.p6.3.m3.1.1.2">𝐨</ci><ci id="S3.p6.3.m3.1.1.3.cmml" xref="S3.p6.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">{\bf{o}}_{j}</annotation></semantics></math> placed on the white context splines <math id="S3.p6.4.m4.1" class="ltx_Math" alttext="{\bf{c}}_{i}" display="inline"><semantics id="S3.p6.4.m4.1a"><msub id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml"><mi id="S3.p6.4.m4.1.1.2" xref="S3.p6.4.m4.1.1.2.cmml">𝐜</mi><mi id="S3.p6.4.m4.1.1.3" xref="S3.p6.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.1b"><apply id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p6.4.m4.1.1.1.cmml" xref="S3.p6.4.m4.1.1">subscript</csymbol><ci id="S3.p6.4.m4.1.1.2.cmml" xref="S3.p6.4.m4.1.1.2">𝐜</ci><ci id="S3.p6.4.m4.1.1.3.cmml" xref="S3.p6.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.1c">{\bf{c}}_{i}</annotation></semantics></math>.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.2" class="ltx_p">By contrast, with DR, the probability of an image <math id="S3.p7.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p7.1.m1.1a"><mi id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><ci id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">I</annotation></semantics></math> being generated is not dependent on context. Rather, the objects are placed randomly in the scene with backgrounds from image datasets such as COCO<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> or ImageNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. DR lacks the structure present in SDR, that is, it does not have the conditional dependence <math id="S3.p7.2.m2.1" class="ltx_Math" alttext="p({\bf{o}}_{j}|{\bf{c}}_{i})" display="inline"><semantics id="S3.p7.2.m2.1a"><mrow id="S3.p7.2.m2.1.1" xref="S3.p7.2.m2.1.1.cmml"><mi id="S3.p7.2.m2.1.1.3" xref="S3.p7.2.m2.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.p7.2.m2.1.1.2" xref="S3.p7.2.m2.1.1.2.cmml">​</mo><mrow id="S3.p7.2.m2.1.1.1.1" xref="S3.p7.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p7.2.m2.1.1.1.1.2" xref="S3.p7.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.p7.2.m2.1.1.1.1.1" xref="S3.p7.2.m2.1.1.1.1.1.cmml"><msub id="S3.p7.2.m2.1.1.1.1.1.2" xref="S3.p7.2.m2.1.1.1.1.1.2.cmml"><mi id="S3.p7.2.m2.1.1.1.1.1.2.2" xref="S3.p7.2.m2.1.1.1.1.1.2.2.cmml">𝐨</mi><mi id="S3.p7.2.m2.1.1.1.1.1.2.3" xref="S3.p7.2.m2.1.1.1.1.1.2.3.cmml">j</mi></msub><mo fence="false" id="S3.p7.2.m2.1.1.1.1.1.1" xref="S3.p7.2.m2.1.1.1.1.1.1.cmml">|</mo><msub id="S3.p7.2.m2.1.1.1.1.1.3" xref="S3.p7.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.p7.2.m2.1.1.1.1.1.3.2" xref="S3.p7.2.m2.1.1.1.1.1.3.2.cmml">𝐜</mi><mi id="S3.p7.2.m2.1.1.1.1.1.3.3" xref="S3.p7.2.m2.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.p7.2.m2.1.1.1.1.3" xref="S3.p7.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.1b"><apply id="S3.p7.2.m2.1.1.cmml" xref="S3.p7.2.m2.1.1"><times id="S3.p7.2.m2.1.1.2.cmml" xref="S3.p7.2.m2.1.1.2"></times><ci id="S3.p7.2.m2.1.1.3.cmml" xref="S3.p7.2.m2.1.1.3">𝑝</ci><apply id="S3.p7.2.m2.1.1.1.1.1.cmml" xref="S3.p7.2.m2.1.1.1.1"><csymbol cd="latexml" id="S3.p7.2.m2.1.1.1.1.1.1.cmml" xref="S3.p7.2.m2.1.1.1.1.1.1">conditional</csymbol><apply id="S3.p7.2.m2.1.1.1.1.1.2.cmml" xref="S3.p7.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p7.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.p7.2.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p7.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.p7.2.m2.1.1.1.1.1.2.2">𝐨</ci><ci id="S3.p7.2.m2.1.1.1.1.1.2.3.cmml" xref="S3.p7.2.m2.1.1.1.1.1.2.3">𝑗</ci></apply><apply id="S3.p7.2.m2.1.1.1.1.1.3.cmml" xref="S3.p7.2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p7.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.p7.2.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.p7.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.p7.2.m2.1.1.1.1.1.3.2">𝐜</ci><ci id="S3.p7.2.m2.1.1.1.1.1.3.3.cmml" xref="S3.p7.2.m2.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.1c">p({\bf{o}}_{j}|{\bf{c}}_{i})</annotation></semantics></math> of the objects on the context.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">The procedure is implemented using the scene generator of the UE4 game engine.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.unrealengine.com/</span></span></span></span>
The scene generator uses an exporter to generate labels for supervised learning.
The exporter reads various render buffers from UE4 (e.g., lit, depth, stencil) and saves them as image files representing the rendered scene, ground truth depth, and segmentation mask.
The exporter also uses the vertex data of 3D object meshes to generate 2D bounding boxes, object-oriented or axis-aligned 3D bounding boxes, truncation, and occlusion values.</p>
</div>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.1" class="ltx_p">Our implementation of SDR includes 74 car models, 13 truck models, 5 bicycle models, 41 building models, 87 house models, 24 tree models, 20 pedestrian models, and 100 road sign models. Other models included are street lights, walls, fences, fire hydrants, recycling bins, telephone poles, traffic lights, and utility boxes—with a small number (1–3) of each.
For DR data, we used these same models as distractors and the same cars as objects of interest.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p">We use Substance<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.allegorithmic.com/substance</span></span></span></span> to randomize the materials of both object and context splines. These include the paint of vehicles based on 9 standard colors, lightness variation, roughness, and metallic properties. For DR data, the textures on objects, distractors and background are random images from Flickr 8k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S3.p11" class="ltx_para">
<p id="S3.p11.1" class="ltx_p">Some sample images generated using this algorithm are shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III Structured Domain Randomization (SDR) ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> along with images from other synthetic datasets used for object detection.
GTA-based data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> uses a large variety of assets and extensive computation time to generate realistic driving simulations, but they are not designed for object detection.
The geometry of the GTA environment is static, that is, roads, buildings, trees and foliage are always in the same positions.
SDR however can produce more variability in terms of scene geometry.
For instance it can produce countless array of road segments with varying widths and undulations, including trees, foliage and buildings that are randomly placed.
As shown in the next section, SDR outperforms the GTA-based synthetic data of Sim 200k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, even though the latter uses many more assets and models, because SDR provides more variability in the geometry of the scenes as mentioned above.
VKITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is a replica of the KITTI scenes and therefore is highly correlated with KITTI.
DR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> generates random object placement, random object texture, random backgrounds, random distractors, and random lighting, but it lacks proper context and structure, leading to extremely non-realistic images.
In contrast, SDR uses context to place objects in realistic ways, respecting the geometry of the context boundaries, while still randomizing the position, texture, lighting, saturation, and so forth.</p>
</div>
<div id="S3.p12" class="ltx_para">
<p id="S3.p12.1" class="ltx_p">Sample images from various synthetic datasets for object detection are shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III Structured Domain Randomization (SDR) ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, along with images from DR and SDR.
GTA-based data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> use a large number of assets to generate realistic driving simulations.
The geometry of the GTA environment is static, so that roads, buildings, and trees are always in the same positions.
In contrast, SDR varies these parameters.
As shown in the next section, this variability is key to successful object detection.
VKITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is a replica of the KITTI scenes and therefore is highly correlated with KITTI.
DR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> generates random object placement, random object texture, random backgrounds, random distractors, and random lighting, but it lacks proper context and structure, leading to extremely non-realistic images.
In contrast, SDR uses context to place objects in realistic ways, respecting the geometry of the context boundaries, while still randomizing the position, texture, lighting, saturation, and so forth.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<table id="S3.F3.12" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.F3.3.3" class="ltx_tr">
<td id="S3.F3.3.3.4" class="ltx_td ltx_align_center"><span id="S3.F3.3.3.4.1" class="ltx_text" style="position:relative; bottom:17.2pt;">
<span id="S3.F3.3.3.4.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.8pt;vertical-align:-7.5pt;"><span class="ltx_transformed_inner" style="width:21.7pt;transform:translate(-7.45pt,0pt) rotate(-90deg) ;">
<span id="S3.F3.3.3.4.1.1.1" class="ltx_p">GTA</span>
</span></span></span></td>
<td id="S3.F3.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/GTA/gta_01.jpg" id="S3.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
<td id="S3.F3.2.2.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/GTA/gta_04.jpg" id="S3.F3.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
<td id="S3.F3.3.3.3" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/GTA/gta_05.jpg" id="S3.F3.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
</tr>
<tr id="S3.F3.6.6" class="ltx_tr">
<td id="S3.F3.6.6.4" class="ltx_td ltx_align_center"><span id="S3.F3.6.6.4.1" class="ltx_text" style="position:relative; bottom:17.2pt;">
<span id="S3.F3.6.6.4.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:37pt;vertical-align:-15.1pt;"><span class="ltx_transformed_inner" style="width:36.9pt;transform:translate(-15.06pt,0pt) rotate(-90deg) ;">
<span id="S3.F3.6.6.4.1.1.1" class="ltx_p">VKITTI</span>
</span></span></span></td>
<td id="S3.F3.4.4.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/VKITTI/00006.jpg" id="S3.F3.4.4.1.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
<td id="S3.F3.5.5.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/VKITTI/00169.jpg" id="S3.F3.5.5.2.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
<td id="S3.F3.6.6.3" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/VKITTI/00196.jpg" id="S3.F3.6.6.3.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
</tr>
<tr id="S3.F3.9.9" class="ltx_tr">
<td id="S3.F3.9.9.4" class="ltx_td ltx_align_center"><span id="S3.F3.9.9.4.1" class="ltx_text" style="position:relative; bottom:17.2pt;">
<span id="S3.F3.9.9.4.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:15pt;vertical-align:-4.1pt;"><span class="ltx_transformed_inner" style="width:15.0pt;transform:translate(-4.08pt,0pt) rotate(-90deg) ;">
<span id="S3.F3.9.9.4.1.1.1" class="ltx_p">DR</span>
</span></span></span></td>
<td id="S3.F3.7.7.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/DR/000006.jpg" id="S3.F3.7.7.1.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
<td id="S3.F3.8.8.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/DR/000010.jpg" id="S3.F3.8.8.2.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
<td id="S3.F3.9.9.3" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/DR/000017.jpg" id="S3.F3.9.9.3.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
</tr>
<tr id="S3.F3.12.12" class="ltx_tr">
<td id="S3.F3.12.12.4" class="ltx_td ltx_align_center">
<span id="S3.F3.12.12.4.1" class="ltx_text" style="position:relative; bottom:17.2pt;">
<span id="S3.F3.12.12.4.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:20.6pt;vertical-align:-6.9pt;"><span class="ltx_transformed_inner" style="width:20.6pt;transform:translate(-6.86pt,0pt) rotate(-90deg) ;">
<span id="S3.F3.12.12.4.1.1.1" class="ltx_p">SDR</span>
</span></span></span>
<span id="S3.F3.12.12.4.2" class="ltx_text" style="position:relative; bottom:17.2pt;">
<span id="S3.F3.12.12.4.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:10.0pt;height:26.2pt;vertical-align:-10.6pt;"><span class="ltx_transformed_inner" style="width:26.2pt;transform:translate(-8.1pt,3.75pt) rotate(-90deg) ;">
<span id="S3.F3.12.12.4.2.1.1" class="ltx_p">(ours)</span>
</span></span></span>
</td>
<td id="S3.F3.10.10.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/SDR/000003.jpg" id="S3.F3.10.10.1.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
<td id="S3.F3.11.11.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/SDR/000005.jpg" id="S3.F3.11.11.2.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
<td id="S3.F3.12.12.3" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/images/SDR/000006.jpg" id="S3.F3.12.12.3.g1" class="ltx_graphics ltx_img_landscape" width="171" height="52" alt="Refer to caption"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Synthetic datasets used for training object detection models. Whereas GTA-based and Virtual KITTI both produce photo-realistic images, domain randomization (DR) intentionally avoids photorealism for variety. Structured domain randomization (SDR) strikes a balance between these two extremes, producing images that are realistic in many respects but nevertheless exhibit large variety.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Evaluation</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The proposed approach of SDR was evaluated for the problem of 2D bounding box detection of vehicles (cars) in the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
In the following subsections we compare SDR against other approaches for generating synthetic data (§<a href="#S4.SS1" title="IV-A Comparative Study ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>), then against using real data from the same and another domain (§<a href="#S4.SS2" title="IV-B Domain Gap ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>). Afterwards we show the power of SDR as an initialization strategy (§<a href="#S4.SS3" title="IV-C SDR as an Initialization Strategy ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>), followed by an ablation study (§<a href="#S4.SS4" title="IV-D Ablation Study ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Comparative Study</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We used the well-known Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> detector, which utilizes a two-stage approach.
The first stage is a region proposal network (RPN) that generates candidate regions of interest using extracted features along with the likelihood of finding an object in each of the proposed regions.
In the second stage, features are cropped from the image using the proposed regions and fed to the remainder of the feature extractor, which predicts a probability density function over object class along with a refined class-specific bounding box for each proposal.
The architecture was trained in an end-to-end fashion using a multi-task loss.
For training, we used momentum <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> with a value of 0.9, and a learning rate of 0.0003.
Resnet V1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> was used as the feature extractor.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We trained Faster-RCNN with different synthetically-generated datasets: Virtual KITTI (VKITTI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, Sim 200k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, DR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and our SDR approach.
For DR and SDR we generated 25k images each, whereas Virtual KITTI consists of 21k images, and Sim 200k consists of 200k images.
All our experiments include standard data augmentations such as random contrast, brightness, mirror flips, and crops.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Results of detection of vehicles on the full dataset of 7500 real KITTI images are shown in Table <a href="#S4.T1" title="TABLE I ‣ IV-A Comparative Study ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, with the performance metric AP evaluated at 0.7 IOU overlap.
The KITTI ground truth bounding boxes are classified as Easy, Moderate, and Hard, depending upon the minimum bounding box height and maximum occlusion, with each category subsuming the previous one.
Thus, the Hard category includes all the bounding boxes, whereas Moderate includes a subset of Hard, and Easy includes a subset of Moderate.
From the table, it is clear that our SDR approach outperforms other synthetic datasets on all three criteria.
Although DR works well on detecting larger objects in the scene (Easy), it performs poorly on smaller objects (Moderate, Hard)
because such objects require the network to utilize context.
SDR improves upon DR by incorporating context, achieving results that are more than 2x better than DR.
Note that, although VKITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> matches the distribution of the KITTI test data, it lacks the variability that SDR provides, thus causing the network to overfit to the VKITTI distribution compared with SDR.
We also trained on only the 2.2k clone videos of VKITTI, achieving noticeably worse performance due to the reduced variability.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Dataset</span></span>
</span>
</td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Size</span></span>
</span>
</td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.1.3.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">Easy</span></span>
</span>
</td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.1.4.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">Moderate</span></span>
</span>
</td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.1.5.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">Hard</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T1.1.2.1.1.1.1" class="ltx_text" style="font-size:70%;">VKITTI clones </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T1.1.2.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.2.2.1.1.1" class="ltx_text" style="font-size:70%;">2.2k</span></span>
</span>
</td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.2.3.1.1.1" class="ltx_text" style="font-size:70%;">49.6</span></span>
</span>
</td>
<td id="S4.T1.1.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.2.4.1.1.1" class="ltx_text" style="font-size:70%;">44.8</span></span>
</span>
</td>
<td id="S4.T1.1.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.2.5.1.1.1" class="ltx_text" style="font-size:70%;">33.6</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T1.1.3.1.1.1.1" class="ltx_text" style="font-size:70%;">VKITTI </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T1.1.3.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.3.2.1.1.1" class="ltx_text" style="font-size:70%;">21k</span></span>
</span>
</td>
<td id="S4.T1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.3.3.1.1.1" class="ltx_text" style="font-size:70%;">70.3</span></span>
</span>
</td>
<td id="S4.T1.1.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.3.4.1.1.1" class="ltx_text" style="font-size:70%;">53.6</span></span>
</span>
</td>
<td id="S4.T1.1.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.3.5.1.1.1" class="ltx_text" style="font-size:70%;">39.9</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T1.1.4.1.1.1.1" class="ltx_text" style="font-size:70%;">Sim 200k </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S4.T1.1.4.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.4.2.1.1.1" class="ltx_text" style="font-size:70%;">200k</span></span>
</span>
</td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.4.3.1.1.1" class="ltx_text" style="font-size:70%;">68.0</span></span>
</span>
</td>
<td id="S4.T1.1.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.4.4.1.1.1" class="ltx_text" style="font-size:70%;">52.6</span></span>
</span>
</td>
<td id="S4.T1.1.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.4.5.1.1.1" class="ltx_text" style="font-size:70%;">42.1</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T1.1.5.1.1.1.1" class="ltx_text" style="font-size:70%;">DR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.1.5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S4.T1.1.5.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.5.2.1.1.1" class="ltx_text" style="font-size:70%;">25k</span></span>
</span>
</td>
<td id="S4.T1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.5.3.1.1.1" class="ltx_text" style="font-size:70%;">56.7</span></span>
</span>
</td>
<td id="S4.T1.1.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.5.4.1.1.1" class="ltx_text" style="font-size:70%;">38.8</span></span>
</span>
</td>
<td id="S4.T1.1.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.5.5.1.1.1" class="ltx_text" style="font-size:70%;">24.0</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.6" class="ltx_tr">
<td id="S4.T1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T1.1.6.1.1.1.1" class="ltx_text" style="font-size:70%;">SDR (ours)</span></span>
</span>
</td>
<td id="S4.T1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.2.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.6.2.1.1.1" class="ltx_text" style="font-size:70%;">25k</span></span>
</span>
</td>
<td id="S4.T1.1.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.6.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">77.3</span></span>
</span>
</td>
<td id="S4.T1.1.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.1.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T1.1.6.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">65.6</span></span>
</span>
</td>
<td id="S4.T1.1.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.1.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T1.1.6.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">52.2</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of Faster-RCNN trained on various synthetic datasets. Shown are AP@0.7 IOU for detecting vehicles on the entire real-world KITTI dataset consisting of 7500 images.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<table id="S4.F4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.F4.2.2" class="ltx_tr">
<td id="S4.F4.1.1.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/results/result_1.jpg" id="S4.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="87" alt="Refer to caption"></td>
<td id="S4.F4.2.2.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/results/result_2.jpg" id="S4.F4.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="87" alt="Refer to caption"></td>
</tr>
<tr id="S4.F4.4.4" class="ltx_tr">
<td id="S4.F4.3.3.1" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/results/result_3.jpg" id="S4.F4.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="87" alt="Refer to caption"></td>
<td id="S4.F4.4.4.2" class="ltx_td ltx_align_center"><img src="/html/1810.10093/assets/figures/results/result_4.jpg" id="S4.F4.4.4.2.g1" class="ltx_graphics ltx_img_landscape" width="287" height="87" alt="Refer to caption"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative results on KITTI of Faster-RCNN trained only on SDR-generated synthetic data. Note the successful detection of severely occluded vehicles. (Green boxes: detections, black boxes: ground truth.)</figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Qualitative results of the detector trained on SDR are shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-A Comparative Study ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Predictions are shown via green boxes, whereas ground truth is displayed via black boxes.
These results highlight the ability of an SDR-trained network to detect in complicated situations even when the network has never seen a single real KITTI image.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Table <a href="#S4.T2" title="TABLE II ‣ IV-A Comparative Study ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows the effect of dataset size on performance. Note that performance of SDR saturates quickly around 10k images and that with just 1000 images we already achieve 43.7 AP. In contrast, the performance of DR does not saturate until approximately 50k images.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Dataset size</td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">1k</td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">2.2k</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">10k</td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">25k</td>
<td id="S4.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">50k</td>
<td id="S4.T2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">100k</td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_t">DR</td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">20.6</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">22.1</td>
<td id="S4.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">23.2</td>
<td id="S4.T2.1.2.5" class="ltx_td ltx_align_center ltx_border_t">24.0</td>
<td id="S4.T2.1.2.6" class="ltx_td ltx_align_center ltx_border_t">25.8</td>
<td id="S4.T2.1.2.7" class="ltx_td ltx_align_center ltx_border_t">25.6</td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">SDR</td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.3.2.1" class="ltx_text ltx_font_bold">43.7</span></td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.3.3.1" class="ltx_text ltx_font_bold">46.0</span></td>
<td id="S4.T2.1.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.3.4.1" class="ltx_text ltx_font_bold">51.9</span></td>
<td id="S4.T2.1.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.3.5.1" class="ltx_text ltx_font_bold">52.5</span></td>
<td id="S4.T2.1.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.3.6.1" class="ltx_text ltx_font_bold">51.1</span></td>
<td id="S4.T2.1.3.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.3.7.1" class="ltx_text ltx_font_bold">51.6</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Effect of dataset size on AP at 0.7 IOU for detection of vehicles on KITTI Hard by DR and SDR when evaluated on a subset of real KITTI images. Performance saturates around 10k for SDR and 50k for DR.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Domain Gap</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To compare training using synthetic versus real data, we evaluate using Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> on a subset of 1500 randomly selected real KITTI images, allowing us to use the remaining 6k real KITTI images for training.
The results are shown in Table <a href="#S4.T3" title="TABLE III ‣ IV-B Domain Gap ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
Since the distributions of KITTI training and test images match each other, it is difficult for a network trained only on synthetic data to compete.
Nevertheless, these results highlight that there is not only a reality gap between synthetic and real data, but there are also significant domain gaps between various real-world datasets.
These gaps are evident by the relatively poor performance of networks trained on real data from a different domain (BDD100K) but tested on KITTI.
Significantly, SDR outperforms this real dataset.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Dataset</span></span>
</span>
</td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.2.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Type</span></span>
</span>
</td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.3.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Size</span></span>
</span>
</td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.4.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.1.4.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">Easy</span></span>
</span>
</td>
<td id="S4.T3.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.5.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.1.5.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">Moderate</span></span>
</span>
</td>
<td id="S4.T3.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.1.6.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">Hard</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T3.1.2.1.1.1.1" class="ltx_text" style="font-size:70%;">DR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.2.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S4.T3.1.2.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.2.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.2.2.1.1.1" class="ltx_text" style="font-size:70%;">synth</span></span>
</span>
</td>
<td id="S4.T3.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.3.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.2.3.1.1.1" class="ltx_text" style="font-size:70%;">25k</span></span>
</span>
</td>
<td id="S4.T3.1.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.4.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.2.4.1.1.1" class="ltx_text" style="font-size:70%;">56.8</span></span>
</span>
</td>
<td id="S4.T3.1.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.5.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.2.5.1.1.1" class="ltx_text" style="font-size:70%;">38.0</span></span>
</span>
</td>
<td id="S4.T3.1.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.2.6.1.1.1" class="ltx_text" style="font-size:70%;">23.9</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T3.1.3.1.1.1.1" class="ltx_text" style="font-size:70%;">SDR (ours)</span></span>
</span>
</td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.3.2.1.1.1" class="ltx_text" style="font-size:70%;">synth</span></span>
</span>
</td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.3.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.3.3.1.1.1" class="ltx_text" style="font-size:70%;">25k</span></span>
</span>
</td>
<td id="S4.T3.1.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.4.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.3.4.1.1.1" class="ltx_text" style="font-size:70%;">69.6</span></span>
</span>
</td>
<td id="S4.T3.1.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.5.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.3.5.1.1.1" class="ltx_text" style="font-size:70%;">65.8</span></span>
</span>
</td>
<td id="S4.T3.1.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T3.1.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.3.6.1.1.1" class="ltx_text" style="font-size:70%;">52.5</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T3.1.4.1.1.1.1" class="ltx_text" style="font-size:70%;">BDD100K </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S4.T3.1.4.1.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite></span>
</span>
</td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.2.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.4.2.1.1.1" class="ltx_text" style="font-size:70%;">real</span></span>
</span>
</td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.3.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.4.3.1.1.1" class="ltx_text" style="font-size:70%;">70k</span></span>
</span>
</td>
<td id="S4.T3.1.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.4.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.4.4.1.1.1" class="ltx_text" style="font-size:70%;">59.7</span></span>
</span>
</td>
<td id="S4.T3.1.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.5.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.4.5.1.1.1" class="ltx_text" style="font-size:70%;">54.3</span></span>
</span>
</td>
<td id="S4.T3.1.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T3.1.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.4.6.1.1.1" class="ltx_text" style="font-size:70%;">45.6</span></span>
</span>
</td>
</tr>
<tr id="S4.T3.1.5" class="ltx_tr">
<td id="S4.T3.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T3.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T3.1.5.1.1.1.1" class="ltx_text" style="font-size:70%;">KITTI</span></span>
</span>
</td>
<td id="S4.T3.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T3.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.2.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.5.2.1.1.1" class="ltx_text" style="font-size:70%;">real</span></span>
</span>
</td>
<td id="S4.T3.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T3.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.3.1.1" class="ltx_p" style="width:22.8pt;"><span id="S4.T3.1.5.3.1.1.1" class="ltx_text" style="font-size:70%;">6k</span></span>
</span>
</td>
<td id="S4.T3.1.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T3.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.4.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.5.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">85.1</span></span>
</span>
</td>
<td id="S4.T3.1.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T3.1.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.5.1.1" class="ltx_p" style="width:31.3pt;"><span id="S4.T3.1.5.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">88.3</span></span>
</span>
</td>
<td id="S4.T3.1.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T3.1.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S4.T3.1.5.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">88.8</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of Faster-RCNN trained on synthetic data (DR, SDR) or real data (BDD100K, KITTI). Shown are AP@0.7 IOU for vehicle detection from a subset of 1500 images from the real-world KITTI dataset. Although it is difficult for synthetic data to outperform real data from the same distribution as the test set (KITTI), our SDR approach nevertheless outperforms real data from other distribution (BDD100K).</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">SDR as an Initialization Strategy</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">SDR is also a good way to initialize a network when an insufficient amount of labeled real data is available.
The results of the initialization/fine-tuning experiment are shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-C SDR as an Initialization Strategy ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
Training solely using SDR yields an AP of 52.5 on the subset of 1500 real KITTI images, as mentioned above.
We then fine-tune using some number of real KITTI images (that is, some percentage of the remaining 6000 images), using momentum <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> with a value of 0.9, and a learning rate of 0.0003.
For comparison, we also train on the same number of real KITTI images using the same learning rate.
As seen in the figure, the performance of SDR+real KITTI is always higher than KITTI alone, showing the importance of using SDR for initializing a network even when real labeled data is available for training.
The performance of SDR+real is also better than DR+real, especially for smaller labeled datasets.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1810.10093/assets/figures/fine_tune/init.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="424" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The performance of real data is boosted when pretrained with SDR-generated synthetic data. Improvement is especially pronounced when only a few labeled real images are available.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Ablation Study</span>
</h3>

<figure id="S4.F6" class="ltx_figure"><img src="/html/1810.10093/assets/x1.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Ablation study for full SDR, without context (C), without scene randomization (SR), without high contrast (HC), without random saturation (RS), without random light (RL) and without multiple pose (MP).</figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In this section we present the effects of individual SDR parameters.
For this experiment we used the same Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> network and Resnet V1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as feature extractor and the same validation set as Sec. <a href="#S4.SS2" title="IV-B Domain Gap ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>.
Our previous DR parameter study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> showed lighting to be most important parameter.
For SDR we find other parameters (e.g., context, saturation and contrast) to be more important, as shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D Ablation Study ‣ IV Evaluation ‣ Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The details of the ablation study are as follows. <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">C:</span> Instead of roads, sidewalks, trees, and other 3D objects placed in the scene, random 2D background images were used. This result shows the importance of context. <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_bold">SR:</span> Instead of a variety of scenarios, images were generated from only rural (46.0 AP), suburban (47.7 AP), or urban (51.9 AP) scenes. This result reveals the importance of variety in the scenes. <span id="S4.SS4.p2.1.3" class="ltx_text ltx_font_bold">HC:</span> For SDR, contrast is fixed at 150% of normal; for this experiment, contrast was set to 100%. <span id="S4.SS4.p2.1.4" class="ltx_text ltx_font_bold">RS:</span> Random saturation was removed. This change has the largest effect, suggesting the importance of the texture gap between real and synthetic data. <span id="S4.SS4.p2.1.5" class="ltx_text ltx_font_bold">RL:</span> Lighting was fixed to a single time of day (broad daylight). <span id="S4.SS4.p2.1.6" class="ltx_text ltx_font_bold">MP:</span> Vehicle poses were always fixed to be within a lane, thus degrading performance when detecting parked vehicles or vehicles on side streets.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have introduced structured domain randomization (SDR), which imposes structure onto domain randomization (DR) in order to provide context. For detecting vehicles, for example, SDR places the vehicles on roads, thus enabling the neural network during training to learn the relationship between them. Through experiments we show that this improves performance significantly over DR. In this paper we have shown that SDR achieves state-of-the-art results for vehicle detection on the KITTI dataset compared with both other synthetically-generated data as well as real-world data from a different domain. We have also shown that pretraining on SDR improves results from real data. In future research, we intend to study SDR for detecting multiple object classes, semantic segmentation, instance segmentation, and other computer vision problems.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors thank Sean Taylor, Felipe Alves, and Liila Torabi for their help with the project.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic open
source movie for optical flow evaluation,” in </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on
Computer Vision (ECCV)</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, Oct. 2012, pp. 611–625.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
A. Handa, V. Pătrăucean, V. Badrinarayanan, S. Stent, and R. Cipolla,
“SceneNet: Understanding real world indoor scenes with synthetic data,”
in </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
A. Dosovitskiy, P. Fischer, E. Ilg, P. Häusser, C. Hazırbaş,
V. Golkov, P. Smagt, D. Cremers, and T. Brox, “FlowNet: Learning optical
flow with convolutional networks,” in </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE International Conference on
Computer Vision (ICCV)</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and
T. Brox, “A large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation,” in </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
W. Qiu and A. Yuille, “UnrealCV: Connecting computer vision to Unreal
Engine,” in </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1609.01326</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhang, W. Qiu, Q. Chen, X. Hu, and A. Yuille, “UnrealStereo: A
synthetic dataset for analyzing stereo vision,” in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1612.04647</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">,
2016.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
J. McCormac, A. Handa, and S. Leutenegger, “SceneNet RGB-D: 5M
photorealistic images of synthetic indoor trajectories with ground truth,”
in </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez, “The SYNTHIA
dataset: A large collection of synthetic images for semantic segmentation
of urban scenes,” in </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">, June 2016.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
S. R. Richter, V. Vineet, S. Roth, and V. Koltun, “Playing for data: Ground
truth from computer games,” in </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, “Virtual worlds as proxy for
multi-object tracking analysis,” in </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
M. Mueller, V. Casser, J. Lahoud, N. Smith, and B. Ghanem, “Sim4CV: A
photo-realistic simulator for computer vision applications,” in
</span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1708.05869</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
A. Tsirikoglou, J. Kronander, M. Wrenninge, and J. Unger, “Procedural modeling
and physically based rendering for synthetic data generation in automotive
applications,” in </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1710.06270</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
randomization for transferring deep neural networks from simulation to the
real world,” in </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
J. Tremblay, A. Prakash, D. Acuna, M. Brophy, V. Jampani, C. Anil, T. To,
E. Cameracci, S. Boochoon, and S. Birchfield, “Training deep networks with
synthetic data: Bridging the reality gap by domain randomization,” in
</span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR Workshop on Autonomous Driving (WAD)</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
The KITTI vision benchmark suite,” in </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
M. Johnson-Roberson, C. Barto, R. Mehta, S. N. Sridhar, K. Rosaen, and
R. Vasudevan, “Driving in the matrix: Can virtual worlds replace
human-generated annotations for real world tasks?” in </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICRA</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,
“Bdd100k: A diverse driving video database with scalable annotation
tooling,” </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1805.04687</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
J. Borrego, A. Dehban, R. Figueiredo, P. Moreno, A. Bernardino, and
J. Santos-Victor, “Applying domain randomization to synthetic data for
object category detection,” </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1807.09834</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
S. Suwajanakorn, N. Snavely, J. Tompson, and M. Norouzi, “Discovery of latent
3D keypoints via end-to-end geometric reasoning,” </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
S. Hinterstoisser, V. Lepetit, P. Wohlhart, and K. Konolige, “On pre-trained
image features and synthetic images for deep learning,” in
</span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1710.10710</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
D. Dwibedi, I. Misra, and M. Hebert, “Cut, paste and learn: Surprisingly
easy synthesis for instance detection,” in </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
N. Mayer, E. Ilg, P. Fischer, C. Hazirbas, D. Cremers, A. Dosovitskiy, and
T. Brox, “What makes good synthetic training data for learning disparity and
optical flow estimation?” </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IJCV</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, vol. 126, pp. 942–960, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker, and R. Triebel, “Implicit
3D orientation learning for 6D object detection from RGB images,” in
</span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECCV</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, “segDeepM: Exploiting
segmentation and context in deep neural networks for object detection,” in
</span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
G. Georgakis, A. Mousavian, A. C. Berg, and J. Kosecka, “Synthesizing training
data for object detection in indoor scenes,” in </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">RSS</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays,
P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft
COCO: Common objects in context,” in </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A
large-scale hierarchical image database,” in </span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
M. Hodosh, P. Young, and J. Hockenmaier, “Framing image description as a
ranking task: Data, models and evaluation metrics,” </span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of
Artificial Intelligence Research</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:90%;">, vol. 47, pp. 853–899, 2013.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
S. R. Richter, Z. Hayder, and V. Koltun, “Playing for benchmarks,” in
</span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time
object detection with region proposal networks,” in </span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
N. Qian, “On the momentum term in gradient descent learning algorithms,”
</span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neural Networks</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:90%;">, vol. 12, no. 1, pp. 145–151, Jan. 1999.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in </span><em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib32.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1810.10092" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1810.10093" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1810.10093">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1810.10093" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1810.10094" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 23:49:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
