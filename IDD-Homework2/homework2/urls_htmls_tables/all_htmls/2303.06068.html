<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.06068] EEG Synthetic Data Generation Using Probabilistic Diffusion Models</title><meta property="og:description" content="Electroencephalography (EEG) plays a significant role in the Brain Computer Interface (BCI) domain, due to its non–invasive nature, low cost, and ease of use, making it a highly desirable option for widespread adoption…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EEG Synthetic Data Generation Using Probabilistic Diffusion Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="EEG Synthetic Data Generation Using Probabilistic Diffusion Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.06068">

<!--Generated on Thu Feb 29 20:49:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on February 2023.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">EEG Synthetic Data Generation Using Probabilistic Diffusion Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giulio Tosato<sup id="id4.2.id1" class="ltx_sup">1</sup>
<br class="ltx_break"><a href="mailto:g.tosato@tilburguniveristy.edu" title="" class="ltx_ref ltx_href">g.tosato@tilburguniversity.edu</a>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cesare M. Dalbagno<sup id="id5.2.id1" class="ltx_sup">1</sup>
<br class="ltx_break"><a href="mailto:c.m.dalbagno@tilburguniversity.edu" title="" class="ltx_ref ltx_href">c.m.dalbagno@tilburguniversity.edu</a>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesco Fumagalli <sup id="id6.2.id1" class="ltx_sup">1</sup>
<br class="ltx_break"><a href="mailto:f.fumagalli@tilburguniversity.edu" title="" class="ltx_ref ltx_href">f.fumagalli@tilburguniversity.edu</a>
</span></span>
</div>
<div class="ltx_dates">(February 2023)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p"><span id="id7.id1.1" class="ltx_text">Electroencephalography (EEG) plays a significant role in the Brain Computer Interface (BCI) domain, due to its non–invasive nature, low cost, and ease of use, making it a highly desirable option for widespread adoption by the general public. This technology is commonly used in conjunction with deep learning techniques, the success of which is largely dependent on the quality and quantity of data used for training. To address the challenge of obtaining sufficient EEG data from individual participants while minimizing user effort and maintaining accuracy, this study proposes an advanced methodology for data augmentation: generating synthetic EEG data using denoising diffusion probabilistic models. The synthetic data are generated from electrode-frequency distribution maps (EFDMs) of emotionally labeled EEG recordings. To assess the validity of the synthetic data generated, both a qualitative and a quantitative comparison with real EEG data were successfully conducted. This study opens up the possibility for an open–source accessible and versatile toolbox that can process and generate data in both time and frequency dimensions, regardless of the number of channels involved. Finally, the proposed methodology has potential implications for the broader field of neuroscience research by enabling the creation of large, publicly available synthetic EEG datasets without privacy concerns.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The use of Electroencephalogram (EEG) recordings for Brain-Computer Interface (BCI) has gained significant attention in the scientific community due to its various applications (A. Roman-Gonzalez, 2012). EEG-BCI technology has been used to develop communication tools as well as rehabilitation and assistive technologies
for disabled individuals, such as those
in a complete locked-in state or with severe motor impairments (Minguillon et al., 2017; Neuper et al., 2006). In the future, EEG-based BCIs have the potential to enhance human performance, connecting prosthetics and remotely controlled devices to allow for greater physical and mental abilities (Bright et al., 2016; Christoph Guger &amp; Carin Hertnaes, n.d.; Penghai &amp; Baikun, 2007; Scherer et al., 2004).
<br class="ltx_break"></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, the practical implementation of EEG-BCI technologies is hindered by limitations such as low quality and availability of EEG data, difficulty in collecting long recordings, low spatial resolution, and the need for a large amount of data to train accurate models (Mona Sazgar &amp; Michael G. Young, 2019).
<br class="ltx_break"></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The proposed solution is to use a denoising diffusion probabilistic model (DDPM) for EEG data synthesis, which was proven to outperform Generative Adversarial Networks (GANs, Yun Luo, 2018; Shovon et al., 2019) in image generation (P. Dhariwal, 2021). Even though a similar study, parallel to ours, has just been conducted (Torma &amp; Szegletes, 2023), an important question remained still unanswered: is the model producing novel data or simply replicating examples from the original dataset?
<br class="ltx_break"></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The focus of our work is to address this question while also creating an open-source synthetic EEG toolbox freely available and easily accessible for specific usage.
The diffusion model was trained on electrode-frequency distribution maps (EFDMs, Wang et al., 2020) developed from a large emotion-labeled EEG dataset, due to the fact that frequency features assure better accuracy in classifying EEG signals (Huang et al., 2022; J. Wang et al., 2017; X.-W. Wang et al., 2011). To demonstrate the quality and novelty of the generated data, we compared the accuracy of a classifier on both original and synthetic data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>EEG</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Electroencephalography (EEG) is a widely used non-invasive brain imaging technique that measures the electrical activity of the brain using electrodes placed on the scalp (Niedermeyer and da Silva, 2005). EEG recordings are made by recording the voltage fluctuations between electrodes, which reflect the electrical activity of the brain. EEG has high temporal resolution, low cost, and great portability, making it ideal for studying brain function in the fields of neuroscience, psychology, and clinical medicine (Darvas et al., 2004). Additionally, compared to other brain imaging techniques, it is safer, because no ionizing radiation nor harmful substances are used during the recording process (Prasad &amp; Tvk, 2014; M. Teplan, 2002). EEG is particularly useful for studying oscillatory brain activity, rhythmic patterns of neural activity that occur at specific frequencies. These oscillations are thought to play a role in cognitive and perceptual processes such as perception, attention, and memory (Tallon-Baudry and Bertrand, 1999).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Short-time Fourier Transform</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The Short-time Fourier Transform (STFT) is a widely used technique in analysing the frequency properties of time series data. It operates by dividing a nonstationary time series into smaller segments and performing a discrete Fourier transform (DFT) on each segment. Unlike the fast Fourier transform (FFT), the STFT provides time-localized frequency information, making it well suited for signals that exhibit changes in frequency content or contain transient events (Shovon et al., 2019; F. Wang et al., 2020).</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The FFT, a popular method for performing the DFT and analyzing signals, assumes that the time series being analyzed are stationary. However, this is often not the case for signals such as EEG recordings (Paluš, 1996). The STFT overcomes this limitation by computing the DFT over windowed segments of the signal, making it a powerful tool for analyzing nonstationary signals.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Denoising probabilistic diffusion models</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">DDPMS are a type of generative model that uses a parameterized Markov chain and variational inference to generate synthetic samples that match real data (Ho et al., 2020). The transitions of this chain are trained to reverse the diffusion process, which on its own is a gradual introduction of Gaussian noise in the data. The diffusion process is represented as a latent variable model that uses a fixed Markov chain to map to the latent space. The purpose of training a diffusion model is to learn how to perform the reverse process, which can be used to generate synthetic data (Introduction to Diffusion Models for Machine Learning, 2022). The chain gradually introduces noise to the data until the signal is fully obliterated, and only noise remains (Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3 Denoising probabilistic diffusion models ‣ 2 Methods ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2303.06068/assets/noise.jpg" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Progressive addition of Gaussian noise.</figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">The approximate posterior distribution q(x1:T—x0) (Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3 Denoising probabilistic diffusion models ‣ 2 Methods ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), is obtained, where x1, …xT are the latent variables with the same dimensionality as x0. The final asymptotic transformation of the data is to pure Gaussian noise.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2303.06068/assets/denoise.jpg" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="114" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Progressive subtraction of Gaussian noise.</figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>OpenAI improved-diffusion model</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The <a target="_blank" href="https://github.com/openai/improved-diffusion" title="" class="ltx_ref ltx_href"> OpenAI improved-diffusion</a> model is a cutting-edge deep learning model that has demonstrated exceptional performance on a diverse range of datasets (Nichol &amp; Dhariwal, 2021), as evidenced by numerous citations in the field. We aim to create a versatile toolbox that can handle a wide variety of tasks, and the improved-diffusion model represents the best compromise between optimized performance and optimized code. This allows us to create a powerful tool that is accessible to a wide range of users, including researchers outside the field of AI.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Classifier</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">In this study, a classifier was developed using the Pytorch framework (Paszke et al., 2017). Initially, the model was trained with two classes, but it was designed with the capability to handle multiple classes to enhance its versatility. Thus, CrossEntropyLoss was adopted as loss function. The model was optimized with a batch size of 128, a learning rate of 0.0001, and the GELU activation function. The specific details of the model configuration can be found in Table <a href="#S2.T1" title="Table 1 ‣ 2.5 Classifier ‣ 2 Methods ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and illustrated in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.5 Classifier ‣ 2 Methods ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Layer Type</span></td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Input Shape</span></td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Hyperparameters</span></td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Conv2d</td>
<td id="S2.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3x104x62</td>
<td id="S2.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">kernel_size=(12, 1), stride=(1, 1), padding=(2, 0)</td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MaxPool2d</td>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16x28x62</td>
<td id="S2.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">kernel_size=(4, 1), stride=(4, 1), padding=0,</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Conv2d</td>
<td id="S2.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3x14x62</td>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">kernel_size=(8, 1), stride=(1, 1), padding=(1, 0)</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MaxPool2d</td>
<td id="S2.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64x14x</td>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">kernel_size=(2, 1), stride=(2, 1), padding=0,</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Conv2d</td>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3x7x62</td>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">kernel_size=(4, 1), stride=(1, 1), padding=(1, 0)</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MaxPool2d</td>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">128x100x62</td>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">kernel_size=(2, 1), stride=(2, 1), padding=0</td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">LazyLinear</td>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3x100x62</td>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">in_features=0, out_features=5000, bias=True</td>
</tr>
<tr id="S2.T1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Linear</td>
<td id="S2.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5000</td>
<td id="S2.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">in_features=5000, out_features=2500, bias=True</td>
</tr>
<tr id="S2.T1.1.10.10" class="ltx_tr">
<td id="S2.T1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Linear</td>
<td id="S2.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2500</td>
<td id="S2.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">in_features=2500, out_features=1000, bias=True</td>
</tr>
<tr id="S2.T1.1.11.11" class="ltx_tr">
<td id="S2.T1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Linear</td>
<td id="S2.T1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1000</td>
<td id="S2.T1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">in_features=1000, out_features=2, bias=True</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Classifier architecture.</figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2303.06068/assets/image.jpg" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="718" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Classifier architecture.</figcaption>
</figure>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Hardware used</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">Intel(R) Xeon(R) Gold 6346 CPU @ 3.10GHz equipped with 2 NVIDIA A40 45 GB of ram.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Procedure</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For this study, the <a target="_blank" href="https://bcmi.sjtu.edu.cn/home/seed/" title="" class="ltx_ref ltx_href"> SEED V</a> dataset provided by Shanghai Jiao Tong University was used. Specifically, the EEG raw labeled data from 16 participants in three sessions. During each session, the participants were presented with four video clips designed to elicit emotions such as Disgust, Fear, Neutral, Sadness, or Happiness. Only happy and sad data were used in our study.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>EFDMs creation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The preprocessing of the EEG data and the creation of Electrode-frequency Distribution Maps (EFDMs, Wang et al., 2020) were conducted according to the pipeline available on the <a target="_blank" href="https://github.com/DevJake/EEG-diffusion-pytorch" title="" class="ltx_ref ltx_href">GitHub repository</a>. The MNE-Python library (Gramfort et al., 2013) was used to read the raw data and create a two-dimensional matrix, excluding the unlabeled time steps, with channels as columns and time steps as rows. The Short-time Fourier Transform (STFT) was then applied to the data using the mne.time_frequency.stft function. The modules of the real and imaginary part of each value in the matrix were calculated and normalized cutting up to 100Hz, resulting in 2D arrays known as the EFDMs. These are grayscale images that plot the intensity values (µV) in each channel (1-62) at each frequency (up to 100Hz) each transform step.
The EFDMs were first squared (128x128) by adding padding, flipped upside down and then converted to RGB images by multiplying by 255 and converting to np.uint8 to meet the input format the model required. Two pickle lists, one for each emotion, were created, containing the EFDMs for each of the 3 sessions for each of the 16 participants.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Classifier training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The classifier was trained using the same dataset as the diffusion model: 24000 images per emotion; its accuracy was assessed using a separate dataset of 6000 images per emotion previously unseen by both the classifier and the diffusion model.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Diffusion model training</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We adapted the OpenAI improved diffusion model to our needs (code available at our <a target="_blank" href="https://github.com/DevJake/EEG-diffusion-pytorch" title="" class="ltx_ref ltx_href">GitHub repository</a>). 
<br class="ltx_break">To train the diffusion model (Figure <a href="#S3.F10" title="Figure 10 ‣ 3.3 Diffusion model training ‣ 3 Procedure ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>), the following parameters were used: 
<br class="ltx_break">MODEL_FLAGS=”-image_size 128 –num_channels 128 –num_res_blocks 3”
<br class="ltx_break">DIFFUSION_FLAGS=”–diffusion_steps 1000 –noise_schedule linear”
<br class="ltx_break">TRAIN_FLAGS=”–lr 1e-4 –batch_size 32”
<br class="ltx_break"></p>
</div>
<figure id="S3.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F10.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:108.4pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/img199_0.png" id="S3.F10.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="598" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F10.1.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F10.1.1.1" class="ltx_text" style="font-size:90%;">(a) 0 epochs</span></p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F10.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:108.4pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/img199_1.png" id="S3.F10.2.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="598" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F10.2.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F10.2.1.1" class="ltx_text" style="font-size:90%;">(b) 10 epochs</span></p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F10.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:108.4pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/img199_2.png" id="S3.F10.3.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="598" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F10.3.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F10.3.1.1" class="ltx_text" style="font-size:90%;">(c) 20 epochs</span></p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F10.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:108.4pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/img199_3.png" id="S3.F10.4.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="598" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F10.4.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F10.4.1.1" class="ltx_text" style="font-size:90%;">(d) 30 epochs</span></p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.5" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:108.4pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/img199_4.png" id="S3.F10.5.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="598" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F10.5.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F10.5.1.1" class="ltx_text" style="font-size:90%;">(e) 40 epochs</span></p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.6" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:108.4pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/img199_5.png" id="S3.F10.6.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="598" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F10.6.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F10.6.1.1" class="ltx_text" style="font-size:90%;">(f) 50 epochs</span></p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Images generated by the Diffusion Model during training (each image is generated upside down, with the x-axis representing the channels and the y-axis representing the frequencies).</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We tested the performance of the classifier trained on real data for 20 epochs on synthetic data to assess whether the generated data were reliable and if they could improve the performance of the classifier model (Figure <a href="#S4.F11" title="Figure 11 ‣ 4 Results ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>).</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2303.06068/assets/x1.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span> Evaluation accuracy of the classifier trained 20 times on original data with different initializations and evaluated on synthetic data generated by the diffusion model at different epochs represented in the x-axis (the solid line represents the average, the shadow represents the 95% confidence interval).</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Initially, there were concerns that the diffusion model may have overfitted the training data, resulting in the generation of mere copies of the original dataset rather than creating novel samples (Somepalli et al., 2022). To test this, the classifier was trained on both synthetic and real data and then tested on never-before-seen real data. The performance of this model was compared with that of a model trained solely on real data, under the assumption that if generated data did not provide additional information, the performances of the two would be identical.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">To mitigate the impact of chance on the classifier’s performance, a two-fold training approach was employed. The classifier was first trained 20 times with different random states using only real images (trained on 24000 and tested on 6000 images per emotion), followed by 20 more training with a combination of real (24000 per emotion) and synthetic images (15000 per emotion) then tested on 6000 images per emotion never seen before by both models (Figure <a href="#S4.F14" title="Figure 14 ‣ 4 Results ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> and Figure <a href="#S4.F17" title="Figure 17 ‣ 4 Results ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>). This method allowed for a more robust and reliable evaluation of the classifier’s accuracy (Torch.Manual_seed(3407) Is All You Need, 2021).</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The improvement observed in Figure <a href="#S4.F20" title="Figure 20 ‣ 4 Results ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> and Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, suggests that the model is not just replicating the original data, but creating synthetic data that can be useful for data augmentation. The results of this study highlight the potential of using synthetic data in combination with real data to improve the accuracy of classifiers.</p>
</div>
<figure id="S4.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F14.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/x2.png" id="S4.F14.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="307" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F14.1.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F14.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span></p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F14.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/x3.png" id="S4.F14.2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="307" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F14.2.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F14.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span></p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Comparison of train and validation accuracy averaged over 20 runs for both models (the solid line represents the average, the shadow represents the 95% confidence interval).</figcaption>
</figure>
<figure id="S4.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F17.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/x4.png" id="S4.F17.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="307" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F17.1.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F17.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span></p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F17.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/x5.png" id="S4.F17.2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="307" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F17.2.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F17.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span></p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Comparison of train and validation loss averaged over 20 runs for both models (the solid line represents the average, the shadow represents the 95% confidence interval).</figcaption>
</figure>
<figure id="S4.F20" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F20.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:433.6pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>*</figcaption>
<p id="S4.F20.1.1" class="ltx_p"><span id="S4.F20.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span>

<img src="/html/2303.06068/assets/x6.png" id="S4.F20.1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption"></p>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F20.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:433.6pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>*</figcaption>
<p id="S4.F20.2.1" class="ltx_p"><span id="S4.F20.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span>

<img src="/html/2303.06068/assets/x7.png" id="S4.F20.2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption"></p>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Comparison of validation accuracy between 20 runs of the same model trained with original data only (”Original”) and 20 runs trained with both original and synthetic data (”Augmented”). Synthetic data were generated by a diffusion model trained for 40 epochs (a) and 60 epochs (b) (the solid line represents the average, the shadow represents the 95% confidence interval).</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Classifier type</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Max. Average Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Original</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.434</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Augmented 40 epochs</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.634</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Augmented 60 epochs</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">92.984</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy comparison between the models trained 20 times with real data only (”Original”), and the models trained 20 times with both original and synthetic data generated by the diffusion model trained for 40 epochs (”Augmented 40 epochs”) and 60 epochs (”Augmented 60 epochs”).</figcaption>
</figure>
<figure id="S4.F23" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F23.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/img199_6.png" id="S4.F23.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="598" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F23.1.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F23.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span></p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F23.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.06068/assets/im_199.png" id="S4.F23.2.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="598" height="598" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>*</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F23.2.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F23.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span></p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Comparison between a synthetic image generated by the Diffusion Model trained for 60 epochs (left) and an original image (right).</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Future directions</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In our forthcoming iteration of the paper, we aim to enhance the efficiency, precision, and versatility by directly processing arrays in the diffusion model. This optimization will streamline the pipeline by eliminating the need for converting between arrays and images. Furthermore, the modified model will accept arrays of any 3d shape as inputs; we will also compare the results with data obtained from time series, where the x-axis represents the time domain and the y-axis represents the EEG cap channels. Our experiments have demonstrated that the diffusion model can process images with a resolution up to 256x256 pixels, enabling the processing of approximately 33 seconds of EEG signals from an 8-channel EEG cap with a sampling rate of 250Hz (8x250x33). The capability of the diffusion model to generate ampler time windows in the data can facilitate more complex analyses of brain activity. To further advance the significance of our work, we plan to explore ways to create connected and coherent samples, such as slices of a single long recording.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">In addition, we know that brains are different, and this difference results in unique EEG registered activity (Riding et al., 1997; Smit et al., 2012): this poses big challenges in training patient-specific deep learning models due to the large amount of data required. A last idea is to adapt the concept of Diffusion Model few-shot learning (Giannone et al., 2022) to fine-tune a large model pre-trained on multiple people’s data using a limited amount of data recorded on a single individual to create a large personalized dataset, therefore improving the performance of the model being used for their specific needs. This technique could cut down sampling time and make it cheaper and more efficient to get an EEG scan, since just a couple of minutes of recording could be augmented into ample datasets.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Possible limitations</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Diffusion models are a powerful tool, yet computationally expensive. In a following update, we will compare the quality of DDPMs’ generated data with data generated using more traditional methods, such as introducing random Gaussian noise in the dataset. If the diffusion-generated data perform better on the same classification test, it will be an indication that diffusion models actually are one of the best ways to create accurate synthetic samples, fundamental for tasks involving human brain enhancement.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our study aimed to address the issue of limited availability of high-quality and accessible EEG data, which has become increasingly important in various research areas, including rehabilitation and enhancement of human capacities.
In this study, we proposed a solution to this problem by using data augmentation via DDPMs. Our classifier, trained on real data only, achieved over 90% average accuracy when tested on our generated samples (Figure <a href="#S4.F11" title="Figure 11 ‣ 4 Results ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>), indicating that this type of data can be used as useful samples to augment the original dataset. To validate that the DDPM was not simply replicating the training data, we compared the performance of a classifier trained on both synthetic and real data to that of a model trained solely on real data (Figure <a href="#S4.F20" title="Figure 20 ‣ 4 Results ‣ EEG Synthetic Data Generation Using Probabilistic Diffusion Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>). The hybrid model consistently outperformed the original classifier, indicating that the generated data contains information that cannot be found in the original data.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">This finding provides evidence that generated data can improve the performance of EEG related applications. Furthermore, the generated data can be shared without any privacy concern since they are not directly sampled from individuals. Future versions of this paper will make the toolbox available on our GitHub repository more versatile and accessible, thereby creating a useful instrument for data generation in various fields.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Bibliography</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Avid Roman-Gonzalez. (2012). EEG Signal Processing for BCI Applications: Vol. 98 (1). HAL. <a target="_blank" href="https://hal.science/hal-00742211" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://hal.science/hal-00742211</a></p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Bracewell, R. N. (1989). The Fourier Transform. Scientific American, 260(6), 86–95. <a target="_blank" href="https://www.jstor.org/stable/24987290" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.jstor.org/stable/24987290</a></p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Bright, D., Nair, A., Salvekar, D., &amp; Bhisikar, S. (2016). EEG-based brain controlled prosthetic arm. 2016 Conference on Advances in Signal Processing (CASP), 479–483. <a target="_blank" href="https://doi.org/10.1109/CASP.2016.7746219" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CASP.2016.7746219</a></p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">Christoph Guger, W. H., &amp; Carin Hertnaes, G. P. (n.d.). Prosthetic Control by an EEG-based BrainComputer Interface (BCI).</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">Darvas, F., Pantazis, D., Kucukaltun-Yildirim, E., &amp; Leahy, R. M. (2004). Mapping human brain function with MEG and EEG: Methods and validation. NeuroImage, 23, S289–S299. <a target="_blank" href="https://doi.org/10.1016/j.neuroimage.2004.07.014" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.neuroimage.2004.07.014</a></p>
</div>
<div id="S7.p6" class="ltx_para">
<p id="S7.p6.1" class="ltx_p">Dhariwal, P., &amp; Nichol, A. Q. (2022, January 26). Diffusion Models Beat GANs on Image Synthesis. Advances in Neural Information Processing Systems. <a target="_blank" href="https://openreview.net/forum?id=AAWuCvzaVt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=AAWuCvzaVt</a></p>
</div>
<div id="S7.p7" class="ltx_para">
<p id="S7.p7.1" class="ltx_p">Giannone, G., Nielsen, D., &amp; Winther, O. (2022). Few-Shot Diffusion Models (arXiv:2205.15463). arXiv. <a target="_blank" href="https://doi.org/10.48550/arXiv.2205.15463" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2205.15463</a></p>
</div>
<div id="S7.p8" class="ltx_para">
<p id="S7.p8.1" class="ltx_p">Gramfort, A., Luessi, M., Larson, E., Engemann, D., Strohmeier, D., Brodbeck, C., Goj, R., Jas, M., Brooks, T., Parkkonen, L., &amp; Hämäläinen, M. (2013). MEG and EEG data analysis with MNE-Python. Frontiers in Neuroscience, 7. <a target="_blank" href="https://www.frontiersin.org/articles/10.3389/fnins.2013.00267" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.frontiersin.org/articles/10.3389/fnins.2013.00267</a></p>
</div>
<div id="S7.p9" class="ltx_para">
<p id="S7.p9.1" class="ltx_p">Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. Advances in Neural Information Processing Systems, 33, 6840–6851. <a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html</a></p>
</div>
<div id="S7.p10" class="ltx_para">
<p id="S7.p10.1" class="ltx_p">Huang, E., Zheng, X., Fang, Y., &amp; Zhang, Z. (2022). Classification of Motor Imagery EEG Based on Time-Domain and Frequency-Domain Dual-Stream Convolutional Neural Network. IRBM, 43(2), 107–113. <a target="_blank" href="https://doi.org/10.1016/j.irbm.2021.04.004" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.irbm.2021.04.004</a></p>
</div>
<div id="S7.p11" class="ltx_para">
<p id="S7.p11.1" class="ltx_p">Introduction to Diffusion Models for Machine Learning. (2022, May 12). News, Tutorials, AI Research. <a target="_blank" href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/</a></p>
</div>
<div id="S7.p12" class="ltx_para">
<p id="S7.p12.1" class="ltx_p">Luo, Y., &amp; Lu, B.-L. (2018). EEG Data Augmentation for Emotion Recognition Using a Conditional Wasserstein GAN. 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2535–2538. <a target="_blank" href="https://doi.org/10.1109/EMBC.2018.8512865" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/EMBC.2018.8512865</a></p>
</div>
<div id="S7.p13" class="ltx_para">
<p id="S7.p13.1" class="ltx_p">M. Teplan. (2002). FUNDAMENTALS OF EEG MEASUREMENT. Institute of Measurement Science, Slovak Academy of Sciences, Dúbravská Cesta 9, 841 04 Bratislava, Slovakia, MEASUREMENT SCIENCE REVIEW, Volume 2, Section 2, 2002.</p>
</div>
<div id="S7.p14" class="ltx_para">
<p id="S7.p14.1" class="ltx_p">Minguillon, J., Lopez-Gordo, M. A., &amp; Pelayo, F. (2017). Trends in EEG-BCI for daily-life: Requirements for artifact removal. Biomedical Signal Processing and Control, 31, 407–418. <a target="_blank" href="https://doi.org/10.1016/j.bspc.2016.09.005" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.bspc.2016.09.005</a></p>
</div>
<div id="S7.p15" class="ltx_para">
<p id="S7.p15.1" class="ltx_p">Neuper, C., Müller-Putz, G. R., Scherer, R., &amp; Pfurtscheller, G. (2006). Motor imagery and EEG-based control of spelling devices and neuroprostheses. In C. Neuper &amp; W. Klimesch (Eds.), Progress in Brain Research (Vol. 159, pp. 393–409). Elsevier. <a target="_blank" href="https://doi.org/10.1016/S0079-6123(06)59025-9" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/S0079-6123(06)59025-9</a></p>
</div>
<div id="S7.p16" class="ltx_para">
<p id="S7.p16.1" class="ltx_p">Nichol, A., &amp; Dhariwal, P. (2021). Improved Denoising Diffusion Probabilistic Models (arXiv:2102.09672). arXiv. <a target="_blank" href="https://doi.org/10.48550/arXiv.2102.09672" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2102.09672</a></p>
</div>
<div id="S7.p17" class="ltx_para">
<p id="S7.p17.1" class="ltx_p">Niedermeyer, E., &amp; Silva, F. H. L. da. (2005). Electroencephalography: Basic Principles, Clinical Applications, and Related Fields. Lippincott Williams &amp; Wilkins.</p>
</div>
<div id="S7.p18" class="ltx_para">
<p id="S7.p18.1" class="ltx_p">Paluš, M. (1996). Nonlinearity in normal human EEG: Cycles, temporal asymmetry, nonstationarity and randomness, not chaos. Biological Cybernetics, 75(5), 389–396. <a target="_blank" href="https://doi.org/10.1007/s004220050304" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s004220050304</a></p>
</div>
<div id="S7.p19" class="ltx_para">
<p id="S7.p19.1" class="ltx_p">Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., &amp; Lerer, A. (2017). Automatic differentiation in PyTorch. <a target="_blank" href="https://openreview.net/forum?id=BJJsrmfCZ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=BJJsrmfCZ</a></p>
</div>
<div id="S7.p20" class="ltx_para">
<p id="S7.p20.1" class="ltx_p">Penghai, L., &amp; Baikun, W. (2007). A Study on EEG Alpha Wave-based Brain-Computer Interface Remote Control System. 2007 International Conference on Mechatronics and Automation, 3179–3184. <a target="_blank" href="https://doi.org/10.1109/ICMA.2007.4304070" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICMA.2007.4304070</a></p>
</div>
<div id="S7.p21" class="ltx_para">
<p id="S7.p21.1" class="ltx_p">Prasad, T., &amp; Tvk, R. (2014). Survey on EEG Signal Processing Methods.</p>
</div>
<div id="S7.p22" class="ltx_para">
<p id="S7.p22.1" class="ltx_p">Riding, R. J., Glass, A., Butler, S. R., &amp; Pleydell‐Pearce, C. W. (1997). Cognitive Style and Individual Differences in EEG Alpha During Information Processing. Educational Psychology, 17(1–2), 219–234. <a target="_blank" href="https://doi.org/10.1080/0144341970170117" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/0144341970170117</a></p>
</div>
<div id="S7.p23" class="ltx_para">
<p id="S7.p23.1" class="ltx_p">Sazgar, M., &amp; Young, M. G. (2019). EEG Artifacts. In M. Sazgar &amp; M. G. Young (Eds.), Absolute Epilepsy and EEG Rotation Review: Essentials for Trainees (pp. 149–162). Springer International Publishing. <a target="_blank" href="https://doi.org/10.1007/978-3-030-03511-2_8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-03511-2_8</a></p>
</div>
<div id="S7.p24" class="ltx_para">
<p id="S7.p24.1" class="ltx_p">Scherer, R., Muller, G. R., Neuper, C., Graimann, B., &amp; Pfurtscheller, G. (2004). An asynchronously controlled EEG-based virtual keyboard: Improvement of the spelling rate. IEEE Transactions on Biomedical Engineering, 51(6), 979–984. <a target="_blank" href="https://doi.org/10.1109/TBME.2004.827062" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TBME.2004.827062</a></p>
</div>
<div id="S7.p25" class="ltx_para">
<p id="S7.p25.1" class="ltx_p">Shovon, T. H., Nazi, Z. A., Dash, S., &amp; Hossain, Md. F. (2019). Classification of Motor Imagery EEG Signals with multi-input Convolutional Neural Network by augmenting STFT. 2019 5th International Conference on Advances in Electrical Engineering (ICAEE), 398–403. <a target="_blank" href="https://doi.org/10.1109/ICAEE48663.2019.8975578" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICAEE48663.2019.8975578</a></p>
</div>
<div id="S7.p26" class="ltx_para">
<p id="S7.p26.1" class="ltx_p">Smit, D. J. A., Boomsma, D. I., Schnack, H. G., Pol, H. E. H., &amp; Geus, E. J. C. de. (2012). Individual Differences in EEG Spectral Power Reflect Genetic Variance in Gray and White Matter Volumes. Twin Research and Human Genetics, 15(3), 384–392. <a target="_blank" href="https://doi.org/10.1017/thg.2012.6" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1017/thg.2012.6</a></p>
</div>
<div id="S7.p27" class="ltx_para">
<p id="S7.p27.1" class="ltx_p">Somepalli, G., Singla, V., Goldblum, M., Geiping, J., &amp; Goldstein, T. (2022). Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models (arXiv:2212.03860). arXiv. <a target="_blank" href="https://doi.org/10.48550/arXiv.2212.03860" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2212.03860</a></p>
</div>
<div id="S7.p28" class="ltx_para">
<p id="S7.p28.1" class="ltx_p">Tallon-Baudry, C., &amp; Bertrand, O. (1999). Oscillatory gamma activity in humans and its role in object representation. Trends in Cognitive Sciences, 3(4), 151–162. <a target="_blank" href="https://doi.org/10.1016/S1364-6613(99)01299-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/S1364-6613(99)01299-1</a></p>
</div>
<div id="S7.p29" class="ltx_para">
<p id="S7.p29.1" class="ltx_p">Torma, S., &amp; Szegletes, D. L. (2023). Brain Signal Generation and Data Augmentation with a Single-Step Diffusion Probabilistic Model. <a target="_blank" href="https://openreview.net/forum?id=woOQ5Hb1oOF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=woOQ5Hb1oOF</a></p>
</div>
<div id="S7.p30" class="ltx_para">
<p id="S7.p30.1" class="ltx_p">Wang, F., Wu, S., Zhang, W., Xu, Z., Zhang, Y., Wu, C., &amp; Coleman, S. (2020). Emotion recognition with convolutional neural network and EEG-based EFDMs. Neuropsychologia, 146, 107506. <a target="_blank" href="https://doi.org/10.1016/j.neuropsychologia.2020.107506" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.neuropsychologia.2020.107506</a></p>
</div>
<div id="S7.p31" class="ltx_para">
<p id="S7.p31.1" class="ltx_p">Wang, J., Feng, Z., &amp; Lu, N. (2017). Feature extraction by common spatial pattern in frequency domain for motor imagery tasks classification. 2017 29th Chinese Control And Decision Conference (CCDC), 5883–5888. <a target="_blank" href="https://doi.org/10.1109/CCDC.2017.7978220" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CCDC.2017.7978220</a></p>
</div>
<div id="S7.p32" class="ltx_para">
<p id="S7.p32.1" class="ltx_p">Wang, X.-W., Nie, D., &amp; Lu, B.-L. (2011). EEG-Based Emotion Recognition Using Frequency Domain Features and Support Vector Machines. In B.-L. Lu, L. Zhang, &amp; J. Kwok (Eds.), Neural Information Processing (pp. 734–743). Springer. <a target="_blank" href="https://doi.org/10.1007/978-3-642-24955-6_87" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-642-24955-6_87</a>
Wong, S. C., Gatt, A., Stamatescu, V., &amp; McDonnell, M. D. (2016). Understanding Data Augmentation for Classification: When to Warp? 2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA), 1–6. <a target="_blank" href="https://doi.org/10.1109/DICTA.2016.7797091" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/DICTA.2016.7797091</a></p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.06067" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.06068" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.06068">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.06068" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.06069" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 20:49:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
