<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2201.08270] Towards Energy Efficient Distributed Federated Learning for 6G Networks</title><meta property="og:description" content="The provision of communication services via portable and mobile devices, such as aerial base stations, is a crucial concept to be realized in 5G/6G networks. Conventionally, IoT/edge devices need to transmit the data d…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Energy Efficient Distributed Federated Learning for 6G Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Energy Efficient Distributed Federated Learning for 6G Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2201.08270">

<!--Generated on Wed Mar  6 18:38:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Energy Efficient Distributed Federated Learning for 6G Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sunder Ali Khowaja<sup id="id5.4.id1" class="ltx_sup">†</sup>, , Kapal Dev*<sup id="id6.5.id2" class="ltx_sup">†</sup>, , Parus Khowaja, and Paolo Bellavista
</span><span class="ltx_author_notes"><sup id="id7.6.id1" class="ltx_sup">†</sup>Joint first authors, with equal contributions to this paper*Corresponding authorSunder Ali Khowaja with Faculty of Engineering and Technology, University of Sindh, Jamshoro, Pakistan and Department of Mechatronics Engineering, Korea Polytechnic University, Republic of Korea. Email: sandar.ali@usindh.edu.pk, sunderali@kpu.ac.krParus Khowaja is with University of Sindh, Jamshoro. (e-mail:Parus.khuwaja@usindh.edu.pk).Kapal Dev is associated with the Department of Institute of Intelligent Systems, University of Johannesburg, South Africa, e-mail: (kapal.dev@ieee.org).Paolo Bellavista is associated with University of Bologna, Italy. e-mail: (paolo.bellavista@unibo.it).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">The provision of communication services via portable and mobile devices, such as aerial base stations, is a crucial concept to be realized in 5G/6G networks. Conventionally, IoT/edge devices need to transmit the data directly to the base station for training the model using machine learning techniques. The data transmission introduces privacy issues that might lead to security concerns and monetary losses. Recently, Federated learning was proposed to partially solve privacy issues via model-sharing with base station. However, the centralized nature of federated learning only allow the devices within the vicinity of base stations to share the trained models. Furthermore, the long-range communication compels the devices to increase transmission power, which raises the energy efficiency concerns. In this work, we propose distributed federated learning (DBFL) framework that overcomes the connectivity and energy efficiency issues for distant devices. The DBFL framework is compatible with mobile edge computing architecture that connects the devices in a distributed manner using clustering protocols. Experimental results show that the framework increases the classification performance by 7.4% in comparison to conventional federated learning while reducing the energy consumption.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, a fifth-generation communication system (5G) has been rolled out to provide enhanced communication services in many developed countries. However, it is envisioned that 5G would suffer from a bottleneck when dealing with the increased number of communication embedded devices and applications demanding high bandwidth such as mixed reality, smart grid 2.0, unmanned aerial vehicles (UAVs), and industrial Internet of Things (IIoT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Moreover, services such as enhanced energy efficiency, smart environments, and high-precision manufacturing in Industry 5.0 will not be effectively handled by the 5G communication system as they require sophisticated control, sensing, and computing functionalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
To facilitate the above-mentioned requirements, the sixth-generation (6G) communication system is in works that will be an ultra-dense network, extremely homogeneous, highly dynamic, and innately intelligent. To be specific, the 6G network is designed to operate at blistering data rate, i.e. multi-Tbps along with ultra-low latency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
With the emergence of 6G communications, the provision of services to heterogeneous and massive machine-type communications (mMTC) devices has been gaining a lot of interest. The use of space-air-terrestrial-sea in 6G ensures service provisioning to the devices where communication infrastructure is not available <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> by deploying a UAV as an aerial base station <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The UAVs can also be used as mobile relays or data collection units, respectively. As suggested earlier, 6G needs to accommodate mMTC devices with ultra-low latency, therefore, AI assisted techniques, specifically machine learning algorithms, need to be integrated in UAVs to provide effective service.
Recently, various strategies to train machine learning models have been proposed such as transfer learning, active learning, and federated learning. The federated learning approach has garnered a lot of interest due to its co-operative training approach and security characteristics. In principle, the technique acquires a trained machine learning model using the private data on local devices or machines. The updates from the trained models are sent to the UAV serving as an aerial base station without altering or accessing the private data. The UAV then aggregates all local updates from the devices to create a global model and broadcasts it to all the devices within its coverage area. The process continues till the convergence of global model with an adequate test accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The whole process can better preserve the security and privacy of data with evolving cyber attacks on the horizon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Existing studies suggest that the federated learning approach is also effective for reducing the traffic load, computational overhead, and latency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. It is apparent that the federated learning provides an edge in comparison to transfer learning and active learning in the context of 6G communication systems.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2201.08270/assets/Picture1.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="431" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Visual Comparison of Centralized and Conventional Federated Learning with Distributed Federated learning for Futuristic Networks</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, due to the continuous communication with a stationary or aerial base station, the problem of energy consumption concerning sustainable wireless infrastructure still persists <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Note that the problem stands for both, the users in the coverage area and the base station itself, since they are battery or electricity powered devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. To overcome the aforementioned issues, we propose the use of distributed federated learning which corresponds to a combination of federated and distributed learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> that enables the local clients to update their trained model without being directly connected to the corresponding UAV. Similarly, the UAV does not have to update all the clients within its range with the global model. The UAVs can send the updated global model to nearby devices, which then propagate the model further to other clients via device-to-device communication. Such type of communication not only preserves data privacy but also helps in providing long-range communication, accommodate massive machine-type communication devices, and helps in designing sustainable energy infrastructures. To the best of our knowledge, a detailed architecture for a distributed federated learning approach in compliance with mobile edge computing framework has not been proposed yet. The contributions of this work are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a distributed federated learning based framework for energy efficient communication in futuristic networks.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show that the proposed framework can handle heterogeneous devices and labels for detection tasks.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We show that the proposed network improves the energy efficiency through experimental analysis.</p>
</div>
</li>
</ul>
<p id="S1.p2.2" class="ltx_p">The rest of the paper is structured as follows: Section 2 provides an overview of the conventional and distributed federated learning. Section
3 presents the proposed DBFL framework. Section 4 discusses the handling of heterogeneous feature space. Section 5 presents the preliminary analysis and its comparison with the conventional federated learning approach. Section 6 concludes the study and highlights some future research directions.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2201.08270/assets/framework.jpg" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Proposed Distributed Federated Learning. The layers on the left side correspond to the compliance with mobile edge
computing framework (MECF) while the layers on the right represent DBL framework</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Conventional and Distributed Federated Learning</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The federated learning approach was originally proposed to cope with the centralized learning system that collects the data from local clients, trains the machine learning model, and transmits it to the devices for further usage. The main advantage of the centralized learning approach is that the wireless network does not affect the model performance since it is trained at the base station. However, the centralized approach is not secure as it is prone to data leakage. Furthermore, the devices and network need to add significant communication resources and overhead (due to the direct transmission of data) to execute centralized learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. 
<br class="ltx_break">The conventional federated learning (CVFL) framework was first proposed by Google that allows the machine learning model to be trained in a cooperative manner without sharing the data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. The CVFL assumes that the employed machine learning architecture for both the local client and the aerial base station is the same. The CVFL approach takes into account the data available to the edge device and trains the model locally. All the locally trained models are then sent to the aerial base station for further aggregation. This process is performed in an iterative manner until the convergence. The local device models are then updated with the aggregated (global model) once it converges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. The main advantage of the CVFL is the privacy of data that is preserved when training and updating the machine learning model.
In comparison to the centralized learning, CVFL also uses less computational overhead. CVFL disadvantages include the detection performance that is affected by the wireless transmission, increased
convergence time, reduced energy efficiency, and at times unavailability of communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. 
<br class="ltx_break">In practical communication systems, all devices are unable to connect to an aerial base station potentially due to transmission delays and energy constraints. To cope with this issue, distributed federated learning (DBFL) is proposed that can participate in model convergence as well as receive the aggregated model without being connected to the aerial base station. As the name suggests, the DBFL works on the principle of multinode communication where the local clients can associate with their neighboring nodes for indirect connection to the aerial base station. The idea behind DBFL is that, if the local clients are not able to directly connect to the aerial base station, they can still perform the federated learning through their neighboring devices selected through nearest neighbor or cluster head selection algorithms. The iterative training process and transmission of aggregated models will also be performed in a distributed manner. The DBFL reduces the energy consumption and computational overhead by reducing the long-range communication and sharing the computational load with
other devices, respectively. This reduction in consumption and computational load leads to energy efficient communication. Furthermore, the DBFL also cope with the cold-start problem considering that the newly registered device in the cell has not enough data to train the model locally. The model shared with DBFL can help in making the inferences at start followed by fine-tuning the already trained model. The CL, CVFL, and DBFL are compared visually in Figure 1 and the key differences are highlighted in Table 1.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>KEY DIFFERENCES BETWEEN CVFL AND DBFL</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">No</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">CVFL</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">DBFL</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.2.1.2.1.1" class="ltx_tr">
<td id="S2.T1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">All devices needs to have a</td>
</tr>
<tr id="S2.T1.1.2.1.2.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">reliable and direct connection</td>
</tr>
<tr id="S2.T1.1.2.1.2.1.3" class="ltx_tr">
<td id="S2.T1.1.2.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">to BS.</td>
</tr>
</table>
</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.2.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.2.1.3.1.1" class="ltx_tr">
<td id="S2.T1.1.2.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Devices can either connect</td>
</tr>
<tr id="S2.T1.1.2.1.3.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">directly to BS or to any other</td>
</tr>
<tr id="S2.T1.1.2.1.3.1.3" class="ltx_tr">
<td id="S2.T1.1.2.1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">device within the cluster having</td>
</tr>
<tr id="S2.T1.1.2.1.3.1.4" class="ltx_tr">
<td id="S2.T1.1.2.1.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">a reliable connection.</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.3.2.2.1.1" class="ltx_tr">
<td id="S2.T1.1.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Number of users in CVFL</td>
</tr>
<tr id="S2.T1.1.3.2.2.1.2" class="ltx_tr">
<td id="S2.T1.1.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">are limited.</td>
</tr>
</table>
</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.3.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.3.2.3.1.1" class="ltx_tr">
<td id="S2.T1.1.3.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Can accommodate more devices</td>
</tr>
<tr id="S2.T1.1.3.2.3.1.2" class="ltx_tr">
<td id="S2.T1.1.3.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">in comparison to CVFL.</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">3</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.4.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.4.3.2.1.1" class="ltx_tr">
<td id="S2.T1.1.4.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Only homogeneous data</td>
</tr>
<tr id="S2.T1.1.4.3.2.1.2" class="ltx_tr">
<td id="S2.T1.1.4.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">and feature space are</td>
</tr>
<tr id="S2.T1.1.4.3.2.1.3" class="ltx_tr">
<td id="S2.T1.1.4.3.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">supported.</td>
</tr>
</table>
</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.4.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.4.3.3.1.1" class="ltx_tr">
<td id="S2.T1.1.4.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Supports homogeneous as well</td>
</tr>
<tr id="S2.T1.1.4.3.3.1.2" class="ltx_tr">
<td id="S2.T1.1.4.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">as heterogeneous data and</td>
</tr>
<tr id="S2.T1.1.4.3.3.1.3" class="ltx_tr">
<td id="S2.T1.1.4.3.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">feature space.</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">4</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.5.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.5.4.2.1.1" class="ltx_tr">
<td id="S2.T1.1.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">The support for energy</td>
</tr>
<tr id="S2.T1.1.5.4.2.1.2" class="ltx_tr">
<td id="S2.T1.1.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">efficiency is limited.</td>
</tr>
</table>
</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.5.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.5.4.3.1.1" class="ltx_tr">
<td id="S2.T1.1.5.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Is designed to improve the</td>
</tr>
<tr id="S2.T1.1.5.4.3.1.2" class="ltx_tr">
<td id="S2.T1.1.5.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">energy efficiency.</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">5</th>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S2.T1.1.6.5.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.6.5.2.1.1" class="ltx_tr">
<td id="S2.T1.1.6.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Low computational</td>
</tr>
<tr id="S2.T1.1.6.5.2.1.2" class="ltx_tr">
<td id="S2.T1.1.6.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">overhead.</td>
</tr>
</table>
</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S2.T1.1.6.5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.6.5.3.1.1" class="ltx_tr">
<td id="S2.T1.1.6.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Slightly increased</td>
</tr>
<tr id="S2.T1.1.6.5.3.1.2" class="ltx_tr">
<td id="S2.T1.1.6.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">computational complexity.</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Distributed Federated Learning Framework</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As suggested earlier, 6G and futuristic networks demand drastic increase in network traffic which also results in increasing computational load on the aerial base stations. Only relying on the aerial base station would result in comprised quality of service (QoS). Additionally, the energy efficiency is also affected due to increased communication service. Therefore, it is a necessity to utilize the computation as well as communication resources in an effective manner that could be the basis of designing sustainable energy infrastructures. In this regard, we propose the distributed federated learning framework as shown in Figure 2. The framework proposes four layers, i.e., Local Edge/IoT/Vehicular device layer, Processing layer, Aerial base station layer, and AI cloud layer. Since the AI cloud layer aggregates the model in a same way as the Aerial base station layer, but on a larger scale, we discuss the first three layers in detail. The architecture is compliant with the Mobile Edge Computing framework (MECF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> that focuses on enhancing the communication and computing capability of the communication network. The DBFL layers (on the right) are mapped to the corresponding MECF layers (on the left) in Figure 2.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_italic">The Local Edge/IoT/Vehicular layer:</span> This includes both stationary and mobile devices, accordingly. The framework assumes that each of the devices in this layer comprises of a local database and computational resources for storing the data and training the model locally. It is also assumed that each of the devices that trains the model has homogeneous data, feature space, and labels, respectively. The DBFL framework can also deal with heterogeneous data and feature space, but it requires additional processing. This layer is in compliance with the edge user equipment in MECF that is responsible for collecting the data and transferring it to the base station or the subsequent layer, accordingly. 
<br class="ltx_break"><span id="S3.p2.1.2" class="ltx_text ltx_font_italic">Processing Layer:</span> The processing layer in DBFL framework maps to the mobile edge computing layer in MECF. We highlight the difference between DBFL and MECF with respect to each corresponding layer. The processing layer in DBFL is further divided into three processing units, i.e. <span id="S3.p2.1.3" class="ltx_text ltx_font_italic">Device-to-Device Clusters</span>, <span id="S3.p2.1.4" class="ltx_text ltx_font_italic">Device Head Selection</span>, and <span id="S3.p2.1.5" class="ltx_text ltx_font_italic">Local Model Aggregation.</span> The <span id="S3.p2.1.6" class="ltx_text ltx_font_italic">Device-to-Device Cluster (D2DC)</span> comprises of two tasks, the first is to find the devices within the pre-specified range to form a cluster and the second is to check the compatibility in terms of homogeneity of the data and labels. The inference rules, optimization algorithms and clustering techniques can be used for the formation of clusters based on a certain criterion. For instance, in the example shown in Figure 2, the criterion can be considered as follows:</p>
</div>
<div id="S3.p3" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">A cluster should consist of three devices at most.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">A cluster should have at least one device that can directly be connected to aerial base station.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Further characterizations to the cluster formation can be added based on the domain application, or user preference. The second task which D2DC needs to perform is the homogeneity of the data and labels. For instance, the model of cars and the model of a bicycle have compatible feature space (same number of features and dimensions) but may differ in class labels, therefore, the homogeneity criterion is not fulfilled. Once the homogeneity is checked, the cluster formation can be performed recursively in case of the cluster devices are not compatible. The D2DC unit in MECF is also responsible for forming clusters based on similar characteristics, however, the formation of clusters is rather hypothetical and can be modified, accordingly.
<br class="ltx_break">After the formation of clusters using D2DC, the <span id="S3.p4.1.1" class="ltx_text ltx_font_italic">Device Head Selection</span> (DHS) needs to be performed. It is similar to the cluster head selection techniques where a cluster head is selected and is held responsible for all communications. However, considering the energy and mobility constraints, further inference rules or optimization algorithms can be opted for selecting the cluster head. An example of inference rules is given below:</p>
</div>
<div id="S3.p5" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">A device head should be capable of direct communication to the aerial base station</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">A device head should have lower aggregated distance from the devices within a cluster</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">In case of the same distance, the device head should be preferred with respect to battery life</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p">In case of same battery life, a stationary device head should be preferred</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p">In case all the devices are stationary, the device head with lower latency to the aerial base station should be preferred</p>
</div>
</li>
</ul>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">The DHS should typically be performed after every 2-5 minutes or lower considering the mobility and energy constraints. In comparison to MECF, the DHS combines some of the characteristics of D2DC and Local Mobile Edge Computing Server (LMECS). The MECF presumes that the devices are capable of communicating and processing data in an autonomous manner. The assumption that all the devices are of same standard and computation capabilities is far from realization. The DHS in DBFL adds some constraints while selecting a device so that the computation is handled in an effective and energy efficient manner.
The last processing unit is the <span id="S3.p6.1.1" class="ltx_text ltx_font_italic">Local Model Aggregation</span> (LMA) which will be aggregated by the device head. Considering that the data, feature, and label space are homogeneous, the aggregation can be performed by either model weighted averaging, model adaptive weighted averaging,
meta-learning, or simply retraining. The selection of the aggregation method depends on the availability of computation and energy resources at the end of the device head. This selection can be performed either based on an optimization algorithm or static rule based inference engine. The models from devices in a cluster are sent to the device head which can perform any of the aforementioned methods to aggregate the trained models. The weighted averaging simply uses static weights for averaging the class probabilities and selects the model closest to the aggregated one <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The adaptive weighted averaging optimizes the weight of each corresponding class label before averaging the class probabilities. Once averaged, the model closest to the aggregated one is selected for further propagation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The meta-learning technique trains an individual shallow classifier on the output class probabilities of the obtained models. It has been proven by the existing studies that meta-learning approach achieves better results than the weighted and adaptive weighted averaging method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. However, there is a trade-off when using meta-learning, i.e. user willing to share data (security) and sending an extra meta-classifier (computational and storage overhead). If the domain requires high accuracy and the data is not sensitive, then the meta-learning can be used; otherwise, the averaging method can be employed. A similar case can be made for re-training that requires not just labels but complete data along with the models. The LMA unit also combines the partial functionalities of LMECS and the core network in MECF. The model aggregation process in MECF is performed at the core network stage, however, it requires the data to be sent directly or through edge servers to the base station. As discussed, the transmission of data raises security and privacy concerns, therefore, the LMA unit in DBFL performs the task locally on the device selected as the cluster head and sends the model to the aerial base station, accordingly, thus reducing the possibilities of security breach or data interception.
<br class="ltx_break"><span id="S3.p6.1.2" class="ltx_text ltx_font_italic">Aerial Base Station</span>: Further communication will be carried out between the device heads and the aerial base station. All device heads will send the aggregated models to the aerial base station for global model aggregation. The aerial base station can perform the model aggregation in a similar manner, i.e. weighted and adaptive weighted averaging schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. However, if provided with the corresponding labels, meta-learning approach can also be dealt at the aerial base station layer. Once the global model is aggregated, the model is then transmitted to the local edge/IoT/vehicular devices through the device heads. This process will not only reduce the communication processes but will also balance the computational overhead amongst all devices involved in the model update process, which eventually leads to increased energy efficiency. The core network layer in MECF and aerial base station in DBFL are almost compliant with the difference in scale of computations and processing. The core network in MECF can be regarded as a centralized approach shown in Figure 1 that only considers the devices within the available range, whereas the DBFL is decentralized, covers a larger area through D2DC extension, and relies on enhancing energy efficiency.
<br class="ltx_break">A hierarchical model updated in diversified manner can be performed using AI cloud which may collect the global aggregated models from multiple aerial base stations and update them to form a meta-global model that can be then shared to the local devices via aerial base station and the corresponding device head.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Generalized Homogeneity using DBFL Framework</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We briefly discuss an example of model training in terms of homogeneous and heterogeneous feature space using DBFL framework. The CVFL assumes that the same model should be opted by all local devices as well as the base station to maintain the homogeneity of data, feature, and label space. However, the assumption of using artificial neural networks and deep learning approaches requires huge amount of data to train the model in an effective way. The shallow learning methods are still preferred to deal with limited amount of data, but those methods heavily rely on feature engineering techniques in order to attain higher levels of accuracy. Subsequently, the feature space may vary in terms of number of dimensions, characteristics, sampling rate, and so forth. This phenomenon is referred to as heterogeneous feature space that can create problems when sending the model to device heads and model aggregation. In this regard, we propose the use of autoencoders (AE) stacked with the features extracted for making the feature space homogeneous <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
<br class="ltx_break">For the homogeneous part, the raw data at local database of user devices will undergo the training process using the artificial neural networks having same parameters. This results in a homogeneous feature space, thus does not create any problem during the transmission and aggregation of models. However, if some feature extraction techniques are used, such as principle component analysis, Fourier analysis, and more, the feature space varies vastly and a trained shallow learning model using different set of features can cause difficulty in aggregation. In this regard, we propose the transformation of feature space using AEs that constructs a latent space representation, which is then trained with the shallow classifier. Furthermore, existing studies have shown that the AEs can be generalized to multiple data modalities such as image, text, and speech, however, the layers, number of neurons, objective function, and parameters needs to be varied, modified and optimized, accordingly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2201.08270/assets/Figure4-rev.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="1116" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of Scenarios for Preliminary Analysis</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experimental analysis</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We performed the analysis to compare the CVFL and DBFL approach in terms of accuracy and energy efficiency to show the proposed work’s effectiveness. We consider three scenarios as shown in Figure 3. All the scenarios consider one aerial base station and five devices in the coverage area. The number of devices were set to five (i) for dealing with a relatively large but manageable and easy-to-understand number of devices and (ii) for evenly dividing them into two clusters considering the example shown in Figure 2. The first scenario represents the CVFL approach where each of the devices in the area needs to connect to the aerial base station. However, it can be seen that 2 out of 5 devices in this scenario cannot connect to the aerial base station due to high latency issues. The second scenario represents the DBFL approach where devices can connect with a device head in a cluster which then sends the aggregated model to the aerial base station. This scenario uses a homogeneous feature space and for the sake of simplicity, we show different hypothetical D2D clusters without performing actual operations. The third scenario represents the DBFL approach with similar characteristics as the second scenario, but a heterogeneous feature space is used. We employ the IoT device type identification dataset proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> that has 9 different types of IoT devices. The reason for the selection of the said dataset is two-fold. The first is the availability of the dataset in public domain and the second is the compliance of dataset characteristics with the domain of the proposed study, i.e., Futuristic Networks. Currently, 5G and 6G networks extensively use preamble detection and active user detection for new radios that somehow can bear a resemblance to the device type classification. Furthermore, in DBFL, the device type needs to be identified in order to select the device head, therefore, the dataset seems to be compliant and provides features for a wide variety of IoT devices or user equipment that can be used for 6G networks. 
<br class="ltx_break">For homogeneous CVFL and DBFL part, it is assumed that each device is trained with artificial neural network having 80 neurons and 3500 data samples. The maximum transmission time is set to 0.1 seconds, beyond this time, the device cannot connect to either the aerial base station or the device head. The latency for each device was manually set to comply with the designed scenarios. The DBFL framework automatically chooses the weighted averaging for model aggregation by default due to its low computational complexity in comparison to other methods. The method averages the class probabilities to aggregate the model at the device head and aerial base station level. 
<br class="ltx_break">For heterogeneous DBFL, we assume that each device uses different dimensions of features but undergoes the AE to get the unified feature space dimension. For instance, the employed dataset has around 274 features. One device may use 60 features while the others might use number of features within the range of 1-274. These features will be the input to the AEs which generates, say, 15 dimensions of feature space. For the sake of simplicity, we chose the output of 25 while rest of the parameters are the same with the Scenario<math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\#2" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">​</mo><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></times><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">#</ci><cn type="integer" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\#2</annotation></semantics></math>, respectively.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.5" class="ltx_p">The device type classification accuracies for all three scenarios over 100 iterations are shown in Figure 4 (a). It is evident from the analysis that the CVFL (Scenario<math id="S5.p2.1.m1.1" class="ltx_Math" alttext="\#1" display="inline"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">​</mo><mn id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><times id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1"></times><ci id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2">#</ci><cn type="integer" id="S5.p2.1.m1.1.1.3.cmml" xref="S5.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\#1</annotation></semantics></math>) gets stuck around 0.81-0.87 which is by assumption due to the fact that only two devices can connect to the aerial base station as the other two exceed the transmission time, thus, their connection to the base station is denied. Furthermore, the error bars indicate that the deviation in the accuracies are quite high for low data volume in the initial iterations. The DBFL-Homogeneous (Scenario<math id="S5.p2.2.m2.1" class="ltx_Math" alttext="\#2" display="inline"><semantics id="S5.p2.2.m2.1a"><mrow id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.p2.2.m2.1.1.1" xref="S5.p2.2.m2.1.1.1.cmml">​</mo><mn id="S5.p2.2.m2.1.1.3" xref="S5.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><times id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1.1"></times><ci id="S5.p2.2.m2.1.1.2.cmml" xref="S5.p2.2.m2.1.1.2">#</ci><cn type="integer" id="S5.p2.2.m2.1.1.3.cmml" xref="S5.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">\#2</annotation></semantics></math>) and DBFL-Heterogeneous (Scenario<math id="S5.p2.3.m3.1" class="ltx_Math" alttext="\#3" display="inline"><semantics id="S5.p2.3.m3.1a"><mrow id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S5.p2.3.m3.1.1.2" xref="S5.p2.3.m3.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.p2.3.m3.1.1.1" xref="S5.p2.3.m3.1.1.1.cmml">​</mo><mn id="S5.p2.3.m3.1.1.3" xref="S5.p2.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><apply id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1"><times id="S5.p2.3.m3.1.1.1.cmml" xref="S5.p2.3.m3.1.1.1"></times><ci id="S5.p2.3.m3.1.1.2.cmml" xref="S5.p2.3.m3.1.1.2">#</ci><cn type="integer" id="S5.p2.3.m3.1.1.3.cmml" xref="S5.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">\#3</annotation></semantics></math>) performs better and converges faster than the CVFL. In Scenario<math id="S5.p2.4.m4.1" class="ltx_Math" alttext="\#2" display="inline"><semantics id="S5.p2.4.m4.1a"><mrow id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml"><mi mathvariant="normal" id="S5.p2.4.m4.1.1.2" xref="S5.p2.4.m4.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.p2.4.m4.1.1.1" xref="S5.p2.4.m4.1.1.1.cmml">​</mo><mn id="S5.p2.4.m4.1.1.3" xref="S5.p2.4.m4.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><apply id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"><times id="S5.p2.4.m4.1.1.1.cmml" xref="S5.p2.4.m4.1.1.1"></times><ci id="S5.p2.4.m4.1.1.2.cmml" xref="S5.p2.4.m4.1.1.2">#</ci><cn type="integer" id="S5.p2.4.m4.1.1.3.cmml" xref="S5.p2.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">\#2</annotation></semantics></math> it can be seen that the all the devices in a D2D cluster participate for data aggregation purposes, which increase the performance level in terms of accuracy and faster convergence. Furthermore, it should also be noted that the transmission delays are reduced due to the device head being closer than the aerial base station. Scenario<math id="S5.p2.5.m5.1" class="ltx_Math" alttext="\#3" display="inline"><semantics id="S5.p2.5.m5.1a"><mrow id="S5.p2.5.m5.1.1" xref="S5.p2.5.m5.1.1.cmml"><mi mathvariant="normal" id="S5.p2.5.m5.1.1.2" xref="S5.p2.5.m5.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.p2.5.m5.1.1.1" xref="S5.p2.5.m5.1.1.1.cmml">​</mo><mn id="S5.p2.5.m5.1.1.3" xref="S5.p2.5.m5.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.5.m5.1b"><apply id="S5.p2.5.m5.1.1.cmml" xref="S5.p2.5.m5.1.1"><times id="S5.p2.5.m5.1.1.1.cmml" xref="S5.p2.5.m5.1.1.1"></times><ci id="S5.p2.5.m5.1.1.2.cmml" xref="S5.p2.5.m5.1.1.2">#</ci><cn type="integer" id="S5.p2.5.m5.1.1.3.cmml" xref="S5.p2.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.5.m5.1c">\#3</annotation></semantics></math> slightly improves the accuracy and converges slightly faster. However, the improvement is recorded on the expense of added computation as each device has to transform the feature space using AEs followed by training and transmission. In terms of error bars, the deviations are almost or less than half in comparison to CVFL. Therefore, we have restricted the number of features to 50 for each device, however, there are more than 250 features available in the dataset. We presume that the accuracy can further be improved by adding more features at the cost of slightly increased computational complexity.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2201.08270/assets/Figure5-up.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparative analysis for (a) IoT device type identification accuracy with multiple scenarios and (b) Energy Consumption while varying transmission delay with multiple scenarios</figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.2" class="ltx_p">We also performed an experimental analysis on the energy consumption of local edge/IoT/vehicular devices while varying the transmission delay. A total of 5 nodes were created with three fixed and two mobile nodes. The attenuation factor was set to 2.0 and the energy values for the nodes were set between 80-100, where 100 was assigned to the device head. The consumption were cycle was varied between the values 0.2-0.35 with respect to the communication distance and computational overhead. The simulation has been carried out in MATLAB R2019b. The results for comparative analysis in terms of energy consumption is shown in Figure 4 (b). It is well established that increase in communication distance increases the transmission delay, which eventually decreases the energy efficiency. In Scenario<math id="S5.p3.1.m1.1" class="ltx_Math" alttext="\#2" display="inline"><semantics id="S5.p3.1.m1.1a"><mrow id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.p3.1.m1.1.1.2" xref="S5.p3.1.m1.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.p3.1.m1.1.1.1" xref="S5.p3.1.m1.1.1.1.cmml">​</mo><mn id="S5.p3.1.m1.1.1.3" xref="S5.p3.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><times id="S5.p3.1.m1.1.1.1.cmml" xref="S5.p3.1.m1.1.1.1"></times><ci id="S5.p3.1.m1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.2">#</ci><cn type="integer" id="S5.p3.1.m1.1.1.3.cmml" xref="S5.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">\#2</annotation></semantics></math>, it is shown that the transmission delay increases as the devices get farther and farther. This leads the devices to increase the transmission power for establishing the communication with the aerial base station. Hence, the energy consumption of the devices increases accordingly. In Scenario<math id="S5.p3.2.m2.1" class="ltx_Math" alttext="\#2" display="inline"><semantics id="S5.p3.2.m2.1a"><mrow id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.p3.2.m2.1.1.2" xref="S5.p3.2.m2.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S5.p3.2.m2.1.1.1" xref="S5.p3.2.m2.1.1.1.cmml">​</mo><mn id="S5.p3.2.m2.1.1.3" xref="S5.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><apply id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1"><times id="S5.p3.2.m2.1.1.1.cmml" xref="S5.p3.2.m2.1.1.1"></times><ci id="S5.p3.2.m2.1.1.2.cmml" xref="S5.p3.2.m2.1.1.2">#</ci><cn type="integer" id="S5.p3.2.m2.1.1.3.cmml" xref="S5.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">\#2</annotation></semantics></math>, the transmission delay is overall decreased by adding a device head to D2D cluster. The effect decreases the transmission power and thus leads to an energy efficient communication. The DBFL-Heterogeneous slightly increases the energy consumption, which, we believe is due to the increased computational complexity.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">DBFL Open Issues and Challenges</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The DBFL framework largely targets the networks 5G/6G and beyond, therefore, it is important to highlight the open issues and challenges that could help in improving the DBFL’s performance.
<br class="ltx_break"><span id="S6.p1.1.1" class="ltx_text ltx_font_italic">Objective function:</span> The objective function in federated learning can be customized based on the domain application. However, the computing resources also vary with respect to the employed objective function. The parameters such as number of iterations and model size also vary accordingly. It should be noted that the model size increases every time the model is aggregated. The convergence time is also related with the objective function. Therefore, the design of an objective function that is less computational complex and reduces the model size is of great importance.
<br class="ltx_break"><span id="S6.p1.1.2" class="ltx_text ltx_font_italic">Scalability and Aggregation: </span>The concept of massive machine-type communication suggests that a single aerial base station has to accommodate a large number of devices that open doors for vast research opportunities. As suggested in earlier sections, the method for selecting device heads on an alternative basis can also help in increasing energy efficiency and accommodating more number of devices. On the other hand, the model aggregation considers the weighted averaging method in a hierarchical manner, however, there are many sophisticated methods that could be used for the said purpose or combining the decision class probabilities for increasing the detection/classification performance. 
<br class="ltx_break"><span id="S6.p1.1.3" class="ltx_text ltx_font_italic">Verification and Validation Measures:</span> Although this study performs the evaluation of said tasks based on accuracy, energy consumption, and transmission delay as suggested in existing works, the use of verification and validation measures remains an open issue. Furthermore, these measures vastly vary with respect to the application domain, type of system, and type of data, accordingly. For instance, the hard real-time systems should be validated in terms of computation time and complexity as the systems need to respond in matter of microseconds. Meanwhile, if system deals with medical reports, the validation measure should be accuracy and F1-score instead of complexity and run-time. Similarly, if systems deal with sensitive data and the priority is to secure it, then the validation measure needs to be in compliance with GDPR regulations. Considering the aforementioned facts, the selection of verification and validation measures still remains an open issue concerning DBFL.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, we proposed the use of a distributed federated learning approach that not only resolves the issues associated with centralized systems but also exhibits scalable, faster, and energy-efficient communication when exchanging the trained models. This DBFL also caters to the data- and feature-space heterogeneity issues for model aggregation via autoencoders. We performed experiments to show that the DBFL-Homogeneous performs better in terms of accuracy as well as energy efficiency in comparison to the CVFL. We also provided some potential future directions that can be considered when carrying the DBFL framework forward for further testing and prototype implementation purposes.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
X. Jian, L. Wu, K. Yu, M. Aloqaily, and J. Ben-Othman, “Energy-efficient user
association with load-balancing for cooperative IIoT network within B5G
era,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Network and Computer Applications</em>, vol. 189, pp.
103–110, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Zhang, J. Liu, H. Guo, M. Qi, and N. Kato, “Envisioning Device-to-Device
Communications in 6G,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 34, no. 3, pp. 86–91, may
2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Q. Qi, X. Chen, C. Zhong, and Z. Zhang, “Integration of Energy, Computation
and Communication in 6G Cellular Internet of Things,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE
Communications Letters</em>, vol. 24, no. 6, pp. 1333–1337, jun 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Niknam, H. S. Dhillon, and J. H. Reed, “Federated Learning for Wireless
Communications: Motivation, Opportunities, and Challenges,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE
Communications Magazine</em>, vol. 58, no. 6, pp. 46–51, jun 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Samarakoon, M. Bennis, W. Saad, and M. Debbah, “Distributed Federated
Learning for Ultra-Reliable Low-Latency Vehicular Communications,”
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Communications</em>, vol. 68, no. 2, pp. 1146–1159,
feb 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. A. Khowaja and P. Khuwaja, “Q-Learning and LSTM based Deep Active Learning
Strategy for Malware Defense in Industrial IoT Applications,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>, vol. 80, pp. 14 637–14 663,
2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R. Yu and P. Li, “Toward Resource-Efficient Federated Learning in Mobile Edge
Computing,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 35, no. 1, pp. 148–155, jan 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. A. B. Salameh, M. B. Irshaid, A. Al-Ajlouni, and M. Aloqaily,
“Energy-Efficient Cross-Layer Spectrum Sharing in CR Green IoT Networks,”
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Green Communications and Networking</em>, vol. 5,
no. 3, pp. 1091–1100, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Bian, H. Dai, and L. Yang, “Throughput and energy efficiency maximization
for UAV-assisted vehicular networks,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Physical Communication</em>,
vol. 42, pp. 101–136, oct 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Elgabli, J. Park, A. S. Bedi, M. Bennis, and V. Aggarwal, “GADMM: Fast and
Communication Efficient Framework for Distributed Machine Learning,”
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, vol. 21, no. 76, pp. 1–39,
2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Chen, H. V. Poor, W. Saad, and S. Cui, “Wireless Communications for
Collaborative Federated Learning,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>,
vol. 58, no. 12, pp. 48–54, dec 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein,
H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for mobile
keyboard prediction,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D. Ye, R. Yu, M. Pan, and Z. Han, “Federated Learning in Vehicular Edge
Computing: A Selective Model Aggregation Approach,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>,
vol. 8, pp. 23 920–23 935, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. A. Khowaja and S.-L. Lee, “Hybrid and hierarchical fusion networks: a deep
cross-modal learning architecture for action recognition,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Neural
Computing and Applications</em>, vol. 32, no. 14, pp. 10 423–10 434, jul 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Meidan, M. Bohadana, A. Shabtai, M. Ochoa, N. O. Tippenhauer, J. D.
Guarnizo, and Y. Elovici, “Detection of unauthorized iot devices using
machine learning techniques,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1709.04647</em>, 2017.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td">
<span id="tab1.1.1.1.1" class="ltx_inline-block">
<span id="tab1.1.1.1.1.1" class="ltx_p"><span id="tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dr. Sunder Ali Khowaja</span>  Dr. Sunder Ali Khowaja received the Ph.D. degree in Industrial and Information Systems Engineering from Hankuk University of Foreign Studies, South Korea. He has served as an Assistant Professor at Department of Telecommunication Engineering, University of Sindh, Pakistan. He is currently associated with Department of Mechatronics Engineering, Korea Polytechnic University, Republic of Korea, in the capacity of postdoctoral research fellow. He is also serving as a reviewer for many reputed journals, including, IEEE Transactions on Industrial Informatics, IEEE Access, IEEE Internet of Things Journal, IEEE Transactions on Network Science and Engineering, IEEE Transactions on Medical Imaging, and others. He also served as a Technical Program Committee member in CCNC 2021, Mobicom 2021, and Globecom 2021 workshops. He is currently assisting in the capacity of Guest Editor at Computers and Electrical Engineering, Human-Centric Computing and Information Sciences, and Sustainable Energy Assessment and Technologies Journals. His research interests include Data Analytics, Deep Learning, and Communication Systems based applications.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id4" class="ltx_float biography">
<table id="id4.1" class="ltx_tabular">
<tr id="id4.1.1" class="ltx_tr">
<td id="id4.1.1.1" class="ltx_td">
<span id="id4.1.1.1.1" class="ltx_inline-block">
<span id="id4.1.1.1.1.1" class="ltx_p"><span id="id4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Kapal Dev</span>  is Senior Researcher at MTU, Ireland and senior research associate at University of Johannesburg, South Africa. He is AE in Springer Wireless Networks, Elsevier Physical Communication, IET Quantum Communication, IET Networks, Topic Editor in MDPI Network. He is contributing as GE in Q1 journals; IEEE TII, TNSE, TGCN, Elsevier COMCOM and COMNET. He served(ing) as Lead chair in MobiCom 2021, Globecom2021, <math id="id4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="id4.1.1.1.1.1.m1.1a"><mo id="id4.1.1.1.1.1.m1.1.1" xref="id4.1.1.1.1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="id4.1.1.1.1.1.m1.1b"><and id="id4.1.1.1.1.1.m1.1.1.cmml" xref="id4.1.1.1.1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="id4.1.1.1.1.1.m1.1c">\&amp;</annotation></semantics></math> CCNC 2021 workshops. He contributed as PI for Erasmus + ICM, CBHE, and H2020 Co-Fund projects. His research interests include Blockchain, Wireless Networks and Artificial Intelligence.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td">
<span id="tab2.1.1.1.1" class="ltx_inline-block">
<span id="tab2.1.1.1.1.1" class="ltx_p"><span id="tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Parus Khowaja</span>  is pursuing her Ph.D. degree in financial analytics from University of Sindh, Jamshoro. She is currently working as an Assistant Professor at University of Sindh, Jamshoro. Her interests include Data analytics, Machine learning for Ambient Intelligence, Stock Portfolios, and Financial securities.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td">
<span id="tab3.1.1.1.1" class="ltx_inline-block">
<span id="tab3.1.1.1.1.1" class="ltx_p"><span id="tab3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Paolo Bellavista</span>  received MSc and PhD degrees in computer science engineering from the University of Bologna, Italy, where he is now a full professor of distributed and mobile systems. His research activities span from pervasive wireless computing to online big data processing under quality constraints, from edge cloud computing to middleware for Industry 4.0 applications. He serves on several Editorial Boards, including IEEE COMST (Associate EiC), ACM CSUR, and Elsevier JNCA and PMC. He is the scientific coordinator of the H2020 BigData project IoTwins.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2201.08269" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2201.08270" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2201.08270">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2201.08270" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2201.08271" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 18:38:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
