<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Token Turing Machines are Efficient Vision Models</title>
<!--Generated on Wed Sep 11 20:49:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.07613v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S1" title="In Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S2" title="In Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S2.SS1" title="In 2 Related Work ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Vision Transformers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S2.SS2" title="In 2 Related Work ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Token Sparsification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S2.SS3" title="In 2 Related Work ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Memory Augmented Neural Networks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3" title="In Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Vision Token Turing Machines (ViTTM)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS1" title="In 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Intuition and Comparison to Prior Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2" title="In 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>ViTTM Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS1" title="In 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Initializing Process and Memory Tokens</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS2" title="In 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Read-Write Heads</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS3" title="In 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Process-Memory Fusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS4" title="In 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>Processing in the Memory Stream</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4" title="In Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS1" title="In 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2" title="In 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Ablations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS1" title="In 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Memory and Process Initialization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS2" title="In 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Read-Write Head and Fusion Choice</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS3" title="In 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Effect of Read-Write Head Latent Dimension</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS4" title="In 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>Effect of Memory and Process Token Number</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS5" title="In 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.5 </span>Effect of Memory Stream Non-linearity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS3" title="In 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparison with other work</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS3.SSS1" title="In 4.3 Comparison with other work ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Image Classification (ImageNet-1K)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS3.SSS2" title="In 4.3 Comparison with other work ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Semantic Segmentation (ADE20K)</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S5" title="In Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S6" title="In Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Token Turing Machines are Efficient Vision Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Purvish Jajal
<br class="ltx_break"/>Purdue University
<br class="ltx_break"/>West Lafayette, Indiana, USA
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1" style="font-size:90%;">pjajal@purdue.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nick John Eliopoulos
<br class="ltx_break"/>Purdue University
<br class="ltx_break"/>West Lafayette, Indiana, USA
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.1.id1" style="font-size:90%;">neliopou@purdue.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Benjamin Shiue-Hal Chou
<br class="ltx_break"/>Purdue University
<br class="ltx_break"/>West Lafayette, Indiana, USA
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.1.id1" style="font-size:90%;">chou150@purdue.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">George K. Thiravathukal
<br class="ltx_break"/>Loyola University Chicago
<br class="ltx_break"/>Chicago, Illinois, USA
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.1.id1" style="font-size:90%;">gkt@cs.luc.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">James C. Davis
<br class="ltx_break"/>Purdue University
<br class="ltx_break"/>West Lafayette, Indiana, USA
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.1.id1" style="font-size:90%;">davisjam@purdue.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yung-Hsiang Lu
<br class="ltx_break"/>Purdue University
<br class="ltx_break"/>West Lafayette, Indiana, USA
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id7.1.id1" style="font-size:90%;">yunglu@purdue.edu</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency, memory-augmented Vision Transformer (ViT).
Our approach builds on Neural Turing Machines and Token Turing Machines, which were applied to NLP and sequential visual understanding tasks.
ViTTMs are designed for non-sequential computer vision tasks such as image classification and segmentation.
Our model creates two sets of tokens: <span class="ltx_text ltx_font_italic" id="id1.1.1">process</span> tokens and <span class="ltx_text ltx_font_italic" id="id1.1.2">memory</span> tokens; process tokens pass through encoder blocks and read-write from memory tokens at each encoder block in the network, allowing them to store and retrieve information from memory.
By ensuring that there are fewer process tokens than memory tokens, we are able to reduce the inference time of the network while maintaining its accuracy.
On ImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0% accuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4<math alttext="\times" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">×</annotation></semantics></math> fewer FLOPs, with an accuracy of 82.9%.
On ADE20K semantic segmentation, ViT-B achieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model acheives a 45.17 mIoU with 26.8 FPS (+94%).</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib8" title="">8</a>]</cite> are used for a variety of vision tasks such as image recognition, semantic segmentation, object detection, and even image generation.
ViTs are a building block for many applications and models, as both backbones and foundation models.
However, high-performing ViTs incur large computational costs, due to their quadratic complexity with respect to input size.
Computational costs can be reduced using token sparsification: the removal of uninformative tokens during inference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib1" title="">1</a>]</cite>.
However, sparsification compromises the accuracy
and, fine-tuning or architectural modifications are required to recover accuracy.
</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="830" id="S1.F1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.15.4.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.6.3" style="font-size:90%;">
Comparison of our architecture with state-of-the-art methods.
It is evident that ViTTM-B<sub class="ltx_sub" id="S1.F1.6.3.1"><span class="ltx_text ltx_font_italic" id="S1.F1.6.3.1.1">(28,28)</span></sub> has lower latency than its accuracy equivalents (<em class="ltx_emph ltx_font_italic" id="S1.F1.6.3.2">e.g</em>.<span class="ltx_text" id="S1.F1.6.3.3"></span> Lookup-ViT<sub class="ltx_sub" id="S1.F1.6.3.4"><span class="ltx_text ltx_font_italic" id="S1.F1.6.3.4.1">7×7</span></sub>) while having higher accuracy than its latency equivalents (<em class="ltx_emph ltx_font_italic" id="S1.F1.6.3.5">e.g</em>.<span class="ltx_text" id="S1.F1.6.3.6"></span> Lookup-ViT<sub class="ltx_sub" id="S1.F1.6.3.7"><span class="ltx_text ltx_font_italic" id="S1.F1.6.3.7.1">3×3</span></sub>).
</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Architectural components such as memory mechanisms can improve accuracy in various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib26" title="">26</a>]</cite>.
They are fixed size, and prior work has effectively used memory to enhance models on sequential language and algorithmic tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib14" title="">14</a>]</cite>.
ViTs trained with memory can significantly improve their accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib32" title="">32</a>]</cite>.
Recently, Token Turing Machines (TTMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib30" title="">30</a>]</cite> have employed memory with Video Transformers to efficiently process video.
LookupViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib20" title="">20</a>]</cite> uses lookup tokens (akin to memory) for efficient process images.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="134" id="S1.F2.g1" src="extracted/5849051/figures/AAAI_2024-PJ-Figure-2-DoubleCol.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.24.11.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.20.10" style="font-size:90%;">
Comparison of NTM/TTMs, ViTs, and ViTTMs.
<span class="ltx_text ltx_font_italic" id="S1.F2.20.10.1">(a)</span> NTMs are sequential models that process an input sequence of size <math alttext="T" class="ltx_Math" display="inline" id="S1.F2.11.1.m1.1"><semantics id="S1.F2.11.1.m1.1b"><mi id="S1.F2.11.1.m1.1.1" xref="S1.F2.11.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S1.F2.11.1.m1.1c"><ci id="S1.F2.11.1.m1.1.1.cmml" xref="S1.F2.11.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.11.1.m1.1d">T</annotation><annotation encoding="application/x-llamapun" id="S1.F2.11.1.m1.1e">italic_T</annotation></semantics></math>, where inputs <math alttext="x_{t}" class="ltx_Math" display="inline" id="S1.F2.12.2.m2.1"><semantics id="S1.F2.12.2.m2.1b"><msub id="S1.F2.12.2.m2.1.1" xref="S1.F2.12.2.m2.1.1.cmml"><mi id="S1.F2.12.2.m2.1.1.2" xref="S1.F2.12.2.m2.1.1.2.cmml">x</mi><mi id="S1.F2.12.2.m2.1.1.3" xref="S1.F2.12.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.12.2.m2.1c"><apply id="S1.F2.12.2.m2.1.1.cmml" xref="S1.F2.12.2.m2.1.1"><csymbol cd="ambiguous" id="S1.F2.12.2.m2.1.1.1.cmml" xref="S1.F2.12.2.m2.1.1">subscript</csymbol><ci id="S1.F2.12.2.m2.1.1.2.cmml" xref="S1.F2.12.2.m2.1.1.2">𝑥</ci><ci id="S1.F2.12.2.m2.1.1.3.cmml" xref="S1.F2.12.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.12.2.m2.1d">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S1.F2.12.2.m2.1e">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> are processed at each time step <math alttext="t" class="ltx_Math" display="inline" id="S1.F2.13.3.m3.1"><semantics id="S1.F2.13.3.m3.1b"><mi id="S1.F2.13.3.m3.1.1" xref="S1.F2.13.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S1.F2.13.3.m3.1c"><ci id="S1.F2.13.3.m3.1.1.cmml" xref="S1.F2.13.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.13.3.m3.1d">t</annotation><annotation encoding="application/x-llamapun" id="S1.F2.13.3.m3.1e">italic_t</annotation></semantics></math> and the memory <math alttext="M" class="ltx_Math" display="inline" id="S1.F2.14.4.m4.1"><semantics id="S1.F2.14.4.m4.1b"><mi id="S1.F2.14.4.m4.1.1" xref="S1.F2.14.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S1.F2.14.4.m4.1c"><ci id="S1.F2.14.4.m4.1.1.cmml" xref="S1.F2.14.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.14.4.m4.1d">M</annotation><annotation encoding="application/x-llamapun" id="S1.F2.14.4.m4.1e">italic_M</annotation></semantics></math> is read from and written at each time step.
<span class="ltx_text ltx_font_italic" id="S1.F2.20.10.2">(b)</span> ViTs process a single input <math alttext="f_{0}" class="ltx_Math" display="inline" id="S1.F2.15.5.m5.1"><semantics id="S1.F2.15.5.m5.1b"><msub id="S1.F2.15.5.m5.1.1" xref="S1.F2.15.5.m5.1.1.cmml"><mi id="S1.F2.15.5.m5.1.1.2" xref="S1.F2.15.5.m5.1.1.2.cmml">f</mi><mn id="S1.F2.15.5.m5.1.1.3" xref="S1.F2.15.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S1.F2.15.5.m5.1c"><apply id="S1.F2.15.5.m5.1.1.cmml" xref="S1.F2.15.5.m5.1.1"><csymbol cd="ambiguous" id="S1.F2.15.5.m5.1.1.1.cmml" xref="S1.F2.15.5.m5.1.1">subscript</csymbol><ci id="S1.F2.15.5.m5.1.1.2.cmml" xref="S1.F2.15.5.m5.1.1.2">𝑓</ci><cn id="S1.F2.15.5.m5.1.1.3.cmml" type="integer" xref="S1.F2.15.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.15.5.m5.1d">f_{0}</annotation><annotation encoding="application/x-llamapun" id="S1.F2.15.5.m5.1e">italic_f start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> (<math alttext="=x_{0}" class="ltx_Math" display="inline" id="S1.F2.16.6.m6.1"><semantics id="S1.F2.16.6.m6.1b"><mrow id="S1.F2.16.6.m6.1.1" xref="S1.F2.16.6.m6.1.1.cmml"><mi id="S1.F2.16.6.m6.1.1.2" xref="S1.F2.16.6.m6.1.1.2.cmml"></mi><mo id="S1.F2.16.6.m6.1.1.1" xref="S1.F2.16.6.m6.1.1.1.cmml">=</mo><msub id="S1.F2.16.6.m6.1.1.3" xref="S1.F2.16.6.m6.1.1.3.cmml"><mi id="S1.F2.16.6.m6.1.1.3.2" xref="S1.F2.16.6.m6.1.1.3.2.cmml">x</mi><mn id="S1.F2.16.6.m6.1.1.3.3" xref="S1.F2.16.6.m6.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.16.6.m6.1c"><apply id="S1.F2.16.6.m6.1.1.cmml" xref="S1.F2.16.6.m6.1.1"><eq id="S1.F2.16.6.m6.1.1.1.cmml" xref="S1.F2.16.6.m6.1.1.1"></eq><csymbol cd="latexml" id="S1.F2.16.6.m6.1.1.2.cmml" xref="S1.F2.16.6.m6.1.1.2">absent</csymbol><apply id="S1.F2.16.6.m6.1.1.3.cmml" xref="S1.F2.16.6.m6.1.1.3"><csymbol cd="ambiguous" id="S1.F2.16.6.m6.1.1.3.1.cmml" xref="S1.F2.16.6.m6.1.1.3">subscript</csymbol><ci id="S1.F2.16.6.m6.1.1.3.2.cmml" xref="S1.F2.16.6.m6.1.1.3.2">𝑥</ci><cn id="S1.F2.16.6.m6.1.1.3.3.cmml" type="integer" xref="S1.F2.16.6.m6.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.16.6.m6.1d">=x_{0}</annotation><annotation encoding="application/x-llamapun" id="S1.F2.16.6.m6.1e">= italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>), through a series of <math alttext="T" class="ltx_Math" display="inline" id="S1.F2.17.7.m7.1"><semantics id="S1.F2.17.7.m7.1b"><mi id="S1.F2.17.7.m7.1.1" xref="S1.F2.17.7.m7.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S1.F2.17.7.m7.1c"><ci id="S1.F2.17.7.m7.1.1.cmml" xref="S1.F2.17.7.m7.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.17.7.m7.1d">T</annotation><annotation encoding="application/x-llamapun" id="S1.F2.17.7.m7.1e">italic_T</annotation></semantics></math> layers, where each layer is indexed by <math alttext="t" class="ltx_Math" display="inline" id="S1.F2.18.8.m8.1"><semantics id="S1.F2.18.8.m8.1b"><mi id="S1.F2.18.8.m8.1.1" xref="S1.F2.18.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S1.F2.18.8.m8.1c"><ci id="S1.F2.18.8.m8.1.1.cmml" xref="S1.F2.18.8.m8.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.18.8.m8.1d">t</annotation><annotation encoding="application/x-llamapun" id="S1.F2.18.8.m8.1e">italic_t</annotation></semantics></math>, the output features of each layer are denoted <math alttext="f_{t}" class="ltx_Math" display="inline" id="S1.F2.19.9.m9.1"><semantics id="S1.F2.19.9.m9.1b"><msub id="S1.F2.19.9.m9.1.1" xref="S1.F2.19.9.m9.1.1.cmml"><mi id="S1.F2.19.9.m9.1.1.2" xref="S1.F2.19.9.m9.1.1.2.cmml">f</mi><mi id="S1.F2.19.9.m9.1.1.3" xref="S1.F2.19.9.m9.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.19.9.m9.1c"><apply id="S1.F2.19.9.m9.1.1.cmml" xref="S1.F2.19.9.m9.1.1"><csymbol cd="ambiguous" id="S1.F2.19.9.m9.1.1.1.cmml" xref="S1.F2.19.9.m9.1.1">subscript</csymbol><ci id="S1.F2.19.9.m9.1.1.2.cmml" xref="S1.F2.19.9.m9.1.1.2">𝑓</ci><ci id="S1.F2.19.9.m9.1.1.3.cmml" xref="S1.F2.19.9.m9.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.19.9.m9.1d">f_{t}</annotation><annotation encoding="application/x-llamapun" id="S1.F2.19.9.m9.1e">italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.
Our ViTTMs are a synthesis of the NTM and ViT architectures.
ViTTMs integrate memory into the ViT architecture on a per-layer basis, processing a sequence of features <math alttext="f_{t}" class="ltx_Math" display="inline" id="S1.F2.20.10.m10.1"><semantics id="S1.F2.20.10.m10.1b"><msub id="S1.F2.20.10.m10.1.1" xref="S1.F2.20.10.m10.1.1.cmml"><mi id="S1.F2.20.10.m10.1.1.2" xref="S1.F2.20.10.m10.1.1.2.cmml">f</mi><mi id="S1.F2.20.10.m10.1.1.3" xref="S1.F2.20.10.m10.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F2.20.10.m10.1c"><apply id="S1.F2.20.10.m10.1.1.cmml" xref="S1.F2.20.10.m10.1.1"><csymbol cd="ambiguous" id="S1.F2.20.10.m10.1.1.1.cmml" xref="S1.F2.20.10.m10.1.1">subscript</csymbol><ci id="S1.F2.20.10.m10.1.1.2.cmml" xref="S1.F2.20.10.m10.1.1.2">𝑓</ci><ci id="S1.F2.20.10.m10.1.1.3.cmml" xref="S1.F2.20.10.m10.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.20.10.m10.1d">f_{t}</annotation><annotation encoding="application/x-llamapun" id="S1.F2.20.10.m10.1e">italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> rather than input sequences.
</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we present ViTTM, a NTM-ViT hybrid that integrates memory within ViT on a per-layer basis, creating an architecture that is efficient (low latency) and accurate (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S1.F2" title="In 1 Introduction ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>).
Unlike NTMs, ViTTM processes a sequence of features, not input sequences.
By employing a fixed size memory, ViTTM reduces the number of tokens processed through each layer compared to ViTs.
Integrating memory ensures that ViTTM is <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">efficient</span> by processing fewer tokens and <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">accurate</span> by storing pertinent information in memory.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.2">We evaluate ViTTM on the ImageNet-1K dataset and the ADE20K semantic segmentation dataset.
ViTTM has competitive accuracy-latency tradeoffs on ImageNet1K classification and expands the Pareto front.
Our ViTTM-B, comparable with ViT-B, requires 2.4<math alttext="\times" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mo id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><times id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">×</annotation></semantics></math> fewer FLOPs, reducing latency by 56%
while having a 1.9% higher accuracy on ImageNet-1K (82.9%) than ViT-B.
Our model is also competitive on ADE20K semantic segmentation.
Compared with ViT-B, ViTTM-B achieves an mIoU of 45.17 (<math alttext="-" class="ltx_Math" display="inline" id="S1.p4.2.m2.1"><semantics id="S1.p4.2.m2.1a"><mo id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><minus id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">-</annotation><annotation encoding="application/x-llamapun" id="S1.p4.2.m2.1d">-</annotation></semantics></math>0.48) while achieving 94% higher FPS on an NVIDIA A30 GPU.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We make the following contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present the ViTTM architecture, an efficient memory-augmented ViT that has competitive accuracy-latency trade-offs on classification and semantic segmentation (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2" title="3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>).
</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We conduct a comprehensive ablation of the ViTTM architecture, exploring various designs and identify key decisions that impact the quality of the architecture (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2" title="4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section we present prior work in ViTs, token-sparsification, and memory-augmented neural networks.
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S2.SS1" title="2.1 Vision Transformers ‣ 2 Related Work ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">2.1</span></a> presents prior work in ViTs and highlight their differences with ViTTM.
We presents prior work in token sparsification in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S2.SS2" title="2.2 Token Sparsification ‣ 2 Related Work ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">2.2</span></a> highlight the benefits of ViTTM over sparsification.
Prior work in memory-augmented neural networks (MANNs) is presented in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S2.SS3" title="2.3 Memory Augmented Neural Networks ‣ 2 Related Work ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">2.3</span></a> and we highlight how we adapt MANNs to non-sequential tasks.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vision Transformers</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">After transformers achieved success in language processing tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib37" title="">37</a>]</cite>, researchers adapted transformer models for computer vision tasks.
ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib8" title="">8</a>]</cite>, which shows that transformers are effective on computer vision tasks.
Subsequent works have focused on developing: (1) training regimes that improve ViT task performance, and (2) alternative ViT architectures to achieve better accuracy-latency trade-offs.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">First, alternative training recipes and self-supervised training are effective at improving ViTs.
Touvron <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">et al</em>.<span class="ltx_text" id="S2.SS1.p2.1.2"></span> show that ViTs can achieve high accuracy by using data augmentation strategies, without using large datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib36" title="">36</a>]</cite>.
Steiner <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.3">et al</em>.<span class="ltx_text" id="S2.SS1.p2.1.4"></span> conduct an in-depth empirical study on the training of ViTs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib33" title="">33</a>]</cite>.
They highlight the interplay between <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.5">compute</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.6">data</span>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.7">model size</span>, and <span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.8">regularization</span> for the effective training of ViTs.
In addition, self-supervised learning (SSL) has become a feature of ViT training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Second, a variety of ViT architectures have been proposed that offer different, or strictly better, accuracy-latency trade-offs.
These include hierarchical ViTs like SwinTransformer and MViT, which create multi-scale feature maps, and CNN-ViT hybrids that incorporate convolutional biases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib11" title="">11</a>]</cite>.
Among these are <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.1">two-stream</span> architectures — our work is most similar to these.
Cross-ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib3" title="">3</a>]</cite>, one of the first two-stream ViT designs, processes tokens at different patch sizes in separate branches and then merges them through an efficient cross-attention mechanism.
ViT-CoMer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib41" title="">41</a>]</cite>, utilizes bi-directional interactions with CNNs using a fusion block.
Reversible Vision Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib23" title="">23</a>]</cite>, create a two-stream architecture with the goal of decoupling GPU memory requirements from model depth by reducing activation caching during training.
A contemporaneous work, LookupViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib20" title="">20</a>]</cite>, processes inputs using two sets of tokens, higher resolution (lookup) tokens and compressed tokens, with information being exchanged between them.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">In this work we present a novel two-stream architecture.
Unlike prior two-stream architectures, we conduct the majority of the computations in a lightweight stream, thus being computationally efficient.
We highlight differences between our work and prior work in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS1" title="3.1 Intuition and Comparison to Prior Work ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>.
The closest work, LookupViT is conceptually similar but differs in design.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Token Sparsification</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Token sparsification is a technique used to accelerate ViTs by reducing the overall number of tokens, which also reduces compute cost.
However, sparsification will degrade accuracy without a good heuristic to distinguish informative and uninformative tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib24" title="">24</a>]</cite>.
Sparsification methods can be broadly categorized into two approaches: <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">pruning</span> methods and <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">merging</span> methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Pruning-based methods identify and remove uninformative tokens during inference, thereby reducing computation costs.
Popular approaches include using prediction modules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib27" title="">27</a>]</cite>, or using intermediate attention computations as a heuristic to identify unimportant tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Merging-based methods reduce the total token count by identifying and combining similar tokens.
Boyla <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.1">et al</em>.<span class="ltx_text" id="S2.SS2.p3.1.2"></span> propose a training-free merging approach, which averages similar tokens at every layer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib1" title="">1</a>]</cite>.
Renggli <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.3">et al</em>.<span class="ltx_text" id="S2.SS2.p3.1.4"></span> uses learned modules to merge tokens at a single layer, reducing tokens of up to 24<math alttext="\times" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><mo id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><times id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">×</annotation></semantics></math> and consequently large reductions in computational costs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib28" title="">28</a>]</cite>.
Similary, Ryo <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.5">et al</em>.<span class="ltx_text" id="S2.SS2.p3.1.6"></span> improve ViTs on video tasks by learning to merge tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Similar to token sparsification, our ViTTM architecture processes fewer tokens at a time by increasing patch size though our approach is not necessarily analogous to pruning or merging methods.
Unlike sparsification, we have access to a memory unit that mitigates the accuracy degradation associated with removing tokens.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Memory Augmented Neural Networks</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Memory Augmented Neural Networks (MANNs) are an extension to neural networks that enhance their capabilities with memory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib26" title="">26</a>]</cite>.
Originally, MANNs enabled RNNs to better tackle various sequential algorithmic, language, and reasoning tasks.
Subsequently, memory has been used to enhance neural networks in various vision tasks.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Neural Turing Machines are one of the first MANNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib14" title="">14</a>]</cite>.
NTMs consist of an RNN equipped with a set of read-write heads that interact with an external memory bank, these modifications allow NTMs to out perform RNNs in copy, sorting, and algorithmic tasks.
Memory Networks are a concurrent approach that focuses upon language and reasoning tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib38" title="">38</a>]</cite>.
Differentiable Neural Computers (DNCs) further extend these NTMs by adding dynamic memory allocation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib15" title="">15</a>]</cite>. Similar to prior works, DNCs show effectiveness on a variety of algorithmic, language, and reasoning tasks.
Recent works, augment transformers with memory sequence modelling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Recent work has adapted MANNs to both sequential and non-sequential vision tasks.
ViTs have been augmented with learnable memory tokens at every layer to enhance fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib32" title="">32</a>]</cite>.
Token Turing Machines (TTM) extend NTMs to transformer models for sequential visual understanding tasks, such as video activity detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib30" title="">30</a>]</cite>.
TTM uses an external memory for reading and writing, and introduce a token summarization module that maintains constant computational cost regardless of sequence length.
Our insight is that NTMs can be extended to ViTs by treating intermediate features as a sequence.
In contrast to prior work, we create an architecture that uses memory for the processing of non-sequential tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Vision Token Turing Machines (ViTTM)</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We begin by providing the intuition behind ViTTMs and how this work is related to prior work and draw parallels with NTMs in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS1" title="3.1 Intuition and Comparison to Prior Work ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>.
Then, we elaborate on the ViTTM architecture in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2" title="3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="371" id="S3.F3.g1" src="extracted/5849051/figures/vittm-arch.png" width="195"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.14.5.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.8.4" style="font-size:90%;">
ViTTM Architecture.
The ViTTM architecture is a NTM-ViT hybrid.
In particular, ViTTM creates two views (or streams) of an input image, <math alttext="x" class="ltx_Math" display="inline" id="S3.F3.5.1.m1.1"><semantics id="S3.F3.5.1.m1.1b"><mi id="S3.F3.5.1.m1.1.1" xref="S3.F3.5.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.F3.5.1.m1.1c"><ci id="S3.F3.5.1.m1.1.1.cmml" xref="S3.F3.5.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.5.1.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S3.F3.5.1.m1.1e">italic_x</annotation></semantics></math>, using two patch embedding layers.
The memory stream, <math alttext="M" class="ltx_Math" display="inline" id="S3.F3.6.2.m2.1"><semantics id="S3.F3.6.2.m2.1b"><mi id="S3.F3.6.2.m2.1.1" xref="S3.F3.6.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.F3.6.2.m2.1c"><ci id="S3.F3.6.2.m2.1.1.cmml" xref="S3.F3.6.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.6.2.m2.1d">M</annotation><annotation encoding="application/x-llamapun" id="S3.F3.6.2.m2.1e">italic_M</annotation></semantics></math>, is created by a memory embedding layer, whereas the process stream, <math alttext="P" class="ltx_Math" display="inline" id="S3.F3.7.3.m3.1"><semantics id="S3.F3.7.3.m3.1b"><mi id="S3.F3.7.3.m3.1.1" xref="S3.F3.7.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.F3.7.3.m3.1c"><ci id="S3.F3.7.3.m3.1.1.cmml" xref="S3.F3.7.3.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.7.3.m3.1d">P</annotation><annotation encoding="application/x-llamapun" id="S3.F3.7.3.m3.1e">italic_P</annotation></semantics></math>, is created with a process embedding layer.
Choose the memory stream to contain a greater number of tokens than the process <em class="ltx_emph ltx_font_italic" id="S3.F3.8.4.1">i.e</em>.<span class="ltx_text" id="S3.F3.8.4.2"></span> <math alttext="T&gt;K" class="ltx_Math" display="inline" id="S3.F3.8.4.m4.1"><semantics id="S3.F3.8.4.m4.1b"><mrow id="S3.F3.8.4.m4.1.1" xref="S3.F3.8.4.m4.1.1.cmml"><mi id="S3.F3.8.4.m4.1.1.2" xref="S3.F3.8.4.m4.1.1.2.cmml">T</mi><mo id="S3.F3.8.4.m4.1.1.1" xref="S3.F3.8.4.m4.1.1.1.cmml">&gt;</mo><mi id="S3.F3.8.4.m4.1.1.3" xref="S3.F3.8.4.m4.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.8.4.m4.1c"><apply id="S3.F3.8.4.m4.1.1.cmml" xref="S3.F3.8.4.m4.1.1"><gt id="S3.F3.8.4.m4.1.1.1.cmml" xref="S3.F3.8.4.m4.1.1.1"></gt><ci id="S3.F3.8.4.m4.1.1.2.cmml" xref="S3.F3.8.4.m4.1.1.2">𝑇</ci><ci id="S3.F3.8.4.m4.1.1.3.cmml" xref="S3.F3.8.4.m4.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.4.m4.1d">T&gt;K</annotation><annotation encoding="application/x-llamapun" id="S3.F3.8.4.m4.1e">italic_T &gt; italic_K</annotation></semantics></math>.
The process and memory streams exchange information using <span class="ltx_text ltx_font_italic" id="S3.F3.8.4.3">read</span> and <span class="ltx_text ltx_font_italic" id="S3.F3.8.4.4">write</span> layers, followed by a fusion operation.
</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Intuition and Comparison to Prior Work</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The ViTTM architecture is inspired by two key insights:
(1) NTMs and TTMs have shown that neural networks can learn to use memory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib30" title="">30</a>]</cite>;
(2) <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">fewer tokens</span> through transformer blocks makes inference <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">faster</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">more tokens</span> mean <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.4">better accuracy</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib24" title="">24</a>]</cite>.
By integrating these insights, ViTTMs are able to process only a few tokens at a time (<span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.5">efficiency</span>), while being able to access the information stored in many tokens (<span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.6">accuracy</span>) by using memory.
The end result is an architecture that is both efficient and accurate.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">ViTTMs processes images using two token streams, one stream is compute-heavy but processes fewer tokens, while the other contains a larger number of tokens.
Our intuition is that we can learn to exchange information between the two streams using <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">read-write heads</span>.
The information in the compute-heavy stream undergoes more processing, thus we call them <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">process</span> tokens.
Likewise, following NTM, we expect the stream with more tokens to be used to recall and store information, so we call them <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">memory</span> tokens.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">This approach distinguishes ViTTMs from previous two-stream architectures like CrossViT and ViT-CoMer, as ViTTM concentrates substantial processing in a single stream.
And, unlike Reversible ViTs, which reduce activation caching during training, ViTTM is specifically designed for computational efficiency during inference.
Furthermore, ViTTMs offer advantages over token sparsification techniques.
By processing fewer tokens while storing their information in memory, ViTTMs maintain both accuracy and efficiency.
Unlike NTMs, which leverage memory for sequential tasks, our work applies memory to ViTs for a non-sequential task, image recognition.
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S1.F2" title="In 1 Introduction ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows a comparison of NTMs to ViTTMs.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Comparison to LookupViT (ECCV 2024):</span>
The contemporaneous LookupViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib20" title="">20</a>]</cite>, is also guided the observation that <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">processing fewer tokens through transformer blocks makes inference faster</span>.
Similar to our approach, they exploit this fact by processing two sets of tokens, with the fewer tokens being relegated to the compute-heavy stream.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">However, our effort differs from LookupViT, both conceptually and in implementation.
LookupViT primarily utilizes cross attention, while we study various read-write and fusion mechanisms (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2" title="4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>) following prior work in NTMs.
Furthermore, in contrast with LookupViT, we do not refine memory tokens.
This follows NTM design, where memory is solely <em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.1.1">read from</em> or <em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.1.2">written to</em> at any time.
Our decisions
expand the LookupViT Pareto front by creating a model that is faster while being more accurate (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S1.F1" title="In 1 Introduction ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ViTTM Architecture</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.4"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.F3" title="In 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> provides an overview of the ViTTM architecture.
The ViTTM architecture is synthesis of the ViT and NTM architectures.
Like ViT we convert an input image, <math alttext="x\in\mathbb{R}^{3\times H\times W}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">x</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml"><mn id="S3.SS2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S3.SS2.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.3.cmml">H</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.4" xref="S3.SS2.p1.1.m1.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><in id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></in><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑥</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3"><times id="S3.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.1"></times><cn id="S3.SS2.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3.3.2">3</cn><ci id="S3.SS2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.3">𝐻</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">x\in\mathbb{R}^{3\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_x ∈ blackboard_R start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>, into non-overlapping patches.
But unlike ViT, we create two sets of patches, not one, and convert each set into tokens.
These patches are converted using embedding blocks into <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_T</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.4.1">memory</span> tokens and <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_K</annotation></semantics></math> <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.4.2">process</span> tokens of dimension <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_d</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.7">We define the memory and process tokens (streams) as <math alttext="M\in\mathbb{R}^{T\times d}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">M</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.cmml">T</mi><mo id="S3.SS2.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><in id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></in><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑀</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"><times id="S3.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2">𝑇</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">M\in\mathbb{R}^{T\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_M ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="P\in\mathbb{R}^{K\times d}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">P</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.3.2" xref="S3.SS2.p2.2.m2.1.1.3.3.2.cmml">K</mi><mo id="S3.SS2.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><in id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></in><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑃</ci><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3"><times id="S3.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.2">𝐾</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">P\in\mathbb{R}^{K\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_P ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, respectively.
After embedding, the tokens are processed using <math alttext="N" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_N</annotation></semantics></math> repeating blocks.
We index each block with an index <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">italic_l</annotation></semantics></math>.
Each of these ViTTM blocks, performs three operations in sequence: (1) <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.7.1">read from memory</span>, (2) <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.7.2">processing</span>, and (3) <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.7.3">write to memory</span>.
The read and write operations return “read tokens” and “write tokens”.
We define the read and write tokens as <math alttext="R\in\mathbb{R}^{K\times d}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">R</mi><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.3.2" xref="S3.SS2.p2.5.m5.1.1.3.3.2.cmml">K</mi><mo id="S3.SS2.p2.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.5.m5.1.1.3.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><in id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></in><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝑅</ci><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3"><times id="S3.SS2.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.1"></times><ci id="S3.SS2.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.2">𝐾</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">R\in\mathbb{R}^{K\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_R ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="W\in\mathbb{R}^{T\times d}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">W</mi><mo id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml"><mi id="S3.SS2.p2.6.m6.1.1.3.2" xref="S3.SS2.p2.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.6.m6.1.1.3.3" xref="S3.SS2.p2.6.m6.1.1.3.3.cmml"><mi id="S3.SS2.p2.6.m6.1.1.3.3.2" xref="S3.SS2.p2.6.m6.1.1.3.3.2.cmml">T</mi><mo id="S3.SS2.p2.6.m6.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.6.m6.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.6.m6.1.1.3.3.3" xref="S3.SS2.p2.6.m6.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><in id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></in><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝑊</ci><apply id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.3.1.cmml" xref="S3.SS2.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.3.2.cmml" xref="S3.SS2.p2.6.m6.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.6.m6.1.1.3.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3"><times id="S3.SS2.p2.6.m6.1.1.3.3.1.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3.1"></times><ci id="S3.SS2.p2.6.m6.1.1.3.3.2.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3.2">𝑇</ci><ci id="S3.SS2.p2.6.m6.1.1.3.3.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">W\in\mathbb{R}^{T\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, respectively.
To ensure efficiency, we depart from ViT’s approach of processing all tokens and instead apply the <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.7.4">processing</span> step only to the process tokens, which are fewer in number than the memory tokens (i.e., <math alttext="K&lt;&lt;T" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml">K</mi><mo id="S3.SS2.p2.7.m7.1.1.1" xref="S3.SS2.p2.7.m7.1.1.1.cmml">&lt;&lt;</mo><mi id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><csymbol cd="latexml" id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1">much-less-than</csymbol><ci id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2">𝐾</ci><ci id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">K&lt;&lt;T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">italic_K &lt; &lt; italic_T</annotation></semantics></math>).
The sequence is formally defined as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R^{l}" class="ltx_Math" display="inline" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><msup id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">R</mi><mi id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1">superscript</csymbol><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">𝑅</ci><ci id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle R^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathrm{Read}(P^{l-1},M^{l})" class="ltx_Math" display="inline" id="S3.E1.m2.2"><semantics id="S3.E1.m2.2a"><mrow id="S3.E1.m2.2.2" xref="S3.E1.m2.2.2.cmml"><mi id="S3.E1.m2.2.2.4" xref="S3.E1.m2.2.2.4.cmml"></mi><mo id="S3.E1.m2.2.2.3" xref="S3.E1.m2.2.2.3.cmml">=</mo><mrow id="S3.E1.m2.2.2.2" xref="S3.E1.m2.2.2.2.cmml"><mi id="S3.E1.m2.2.2.2.4" xref="S3.E1.m2.2.2.2.4.cmml">Read</mi><mo id="S3.E1.m2.2.2.2.3" xref="S3.E1.m2.2.2.2.3.cmml">⁢</mo><mrow id="S3.E1.m2.2.2.2.2.2" xref="S3.E1.m2.2.2.2.2.3.cmml"><mo id="S3.E1.m2.2.2.2.2.2.3" stretchy="false" xref="S3.E1.m2.2.2.2.2.3.cmml">(</mo><msup id="S3.E1.m2.1.1.1.1.1.1" xref="S3.E1.m2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.1.1.1.1.1.1.2" xref="S3.E1.m2.1.1.1.1.1.1.2.cmml">P</mi><mrow id="S3.E1.m2.1.1.1.1.1.1.3" xref="S3.E1.m2.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m2.1.1.1.1.1.1.3.2" xref="S3.E1.m2.1.1.1.1.1.1.3.2.cmml">l</mi><mo id="S3.E1.m2.1.1.1.1.1.1.3.1" xref="S3.E1.m2.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E1.m2.1.1.1.1.1.1.3.3" xref="S3.E1.m2.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msup><mo id="S3.E1.m2.2.2.2.2.2.4" xref="S3.E1.m2.2.2.2.2.3.cmml">,</mo><msup id="S3.E1.m2.2.2.2.2.2.2" xref="S3.E1.m2.2.2.2.2.2.2.cmml"><mi id="S3.E1.m2.2.2.2.2.2.2.2" xref="S3.E1.m2.2.2.2.2.2.2.2.cmml">M</mi><mi id="S3.E1.m2.2.2.2.2.2.2.3" xref="S3.E1.m2.2.2.2.2.2.2.3.cmml">l</mi></msup><mo id="S3.E1.m2.2.2.2.2.2.5" stretchy="false" xref="S3.E1.m2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.2b"><apply id="S3.E1.m2.2.2.cmml" xref="S3.E1.m2.2.2"><eq id="S3.E1.m2.2.2.3.cmml" xref="S3.E1.m2.2.2.3"></eq><csymbol cd="latexml" id="S3.E1.m2.2.2.4.cmml" xref="S3.E1.m2.2.2.4">absent</csymbol><apply id="S3.E1.m2.2.2.2.cmml" xref="S3.E1.m2.2.2.2"><times id="S3.E1.m2.2.2.2.3.cmml" xref="S3.E1.m2.2.2.2.3"></times><ci id="S3.E1.m2.2.2.2.4.cmml" xref="S3.E1.m2.2.2.2.4">Read</ci><interval closure="open" id="S3.E1.m2.2.2.2.2.3.cmml" xref="S3.E1.m2.2.2.2.2.2"><apply id="S3.E1.m2.1.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.1.1.1.1.1.1.2">𝑃</ci><apply id="S3.E1.m2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.1.1.1.1.1.1.3"><minus id="S3.E1.m2.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m2.1.1.1.1.1.1.3.1"></minus><ci id="S3.E1.m2.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m2.1.1.1.1.1.1.3.2">𝑙</ci><cn id="S3.E1.m2.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E1.m2.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E1.m2.2.2.2.2.2.2.cmml" xref="S3.E1.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m2.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E1.m2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m2.2.2.2.2.2.2.2">𝑀</ci><ci id="S3.E1.m2.2.2.2.2.2.2.3.cmml" xref="S3.E1.m2.2.2.2.2.2.2.3">𝑙</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.2c">\displaystyle=\mathrm{Read}(P^{l-1},M^{l})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m2.2d">= roman_Read ( italic_P start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT , italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P^{l}" class="ltx_Math" display="inline" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><msup id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">P</mi><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">𝑃</ci><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle P^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_P start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathrm{Encoder}(\mathrm{Fusion}(P^{l-1},R^{l}))" class="ltx_Math" display="inline" id="S3.E2.m2.1"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml"><mi id="S3.E2.m2.1.1.3" xref="S3.E2.m2.1.1.3.cmml"></mi><mo id="S3.E2.m2.1.1.2" xref="S3.E2.m2.1.1.2.cmml">=</mo><mrow id="S3.E2.m2.1.1.1" xref="S3.E2.m2.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.3" xref="S3.E2.m2.1.1.1.3.cmml">Encoder</mi><mo id="S3.E2.m2.1.1.1.2" xref="S3.E2.m2.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m2.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.cmml"><mo id="S3.E2.m2.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m2.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.4" xref="S3.E2.m2.1.1.1.1.1.1.4.cmml">Fusion</mi><mo id="S3.E2.m2.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.E2.m2.1.1.1.1.1.1.2.2" xref="S3.E2.m2.1.1.1.1.1.1.2.3.cmml"><mo id="S3.E2.m2.1.1.1.1.1.1.2.2.3" stretchy="false" xref="S3.E2.m2.1.1.1.1.1.1.2.3.cmml">(</mo><msup id="S3.E2.m2.1.1.1.1.1.1.1.1.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml">P</mi><mrow id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mo id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msup><mo id="S3.E2.m2.1.1.1.1.1.1.2.2.4" xref="S3.E2.m2.1.1.1.1.1.1.2.3.cmml">,</mo><msup id="S3.E2.m2.1.1.1.1.1.1.2.2.2" xref="S3.E2.m2.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m2.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m2.1.1.1.1.1.1.2.2.2.2.cmml">R</mi><mi id="S3.E2.m2.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m2.1.1.1.1.1.1.2.2.2.3.cmml">l</mi></msup><mo id="S3.E2.m2.1.1.1.1.1.1.2.2.5" stretchy="false" xref="S3.E2.m2.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m2.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"><eq id="S3.E2.m2.1.1.2.cmml" xref="S3.E2.m2.1.1.2"></eq><csymbol cd="latexml" id="S3.E2.m2.1.1.3.cmml" xref="S3.E2.m2.1.1.3">absent</csymbol><apply id="S3.E2.m2.1.1.1.cmml" xref="S3.E2.m2.1.1.1"><times id="S3.E2.m2.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.2"></times><ci id="S3.E2.m2.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.3">Encoder</ci><apply id="S3.E2.m2.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1"><times id="S3.E2.m2.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.3"></times><ci id="S3.E2.m2.1.1.1.1.1.1.4.cmml" xref="S3.E2.m2.1.1.1.1.1.1.4">Fusion</ci><interval closure="open" id="S3.E2.m2.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.2"><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.2">𝑃</ci><apply id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3"><minus id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.2">𝑙</ci><cn id="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E2.m2.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E2.m2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m2.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S3.E2.m2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.2.2.2">𝑅</ci><ci id="S3.E2.m2.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m2.1.1.1.1.1.1.2.2.2.3">𝑙</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=\mathrm{Encoder}(\mathrm{Fusion}(P^{l-1},R^{l}))</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m2.1d">= roman_Encoder ( roman_Fusion ( italic_P start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT , italic_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle W^{l}" class="ltx_Math" display="inline" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><msup id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">W</mi><mi id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1">superscript</csymbol><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">𝑊</ci><ci id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle W^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathrm{Write}(P^{l},M^{l})" class="ltx_Math" display="inline" id="S3.E3.m2.2"><semantics id="S3.E3.m2.2a"><mrow id="S3.E3.m2.2.2" xref="S3.E3.m2.2.2.cmml"><mi id="S3.E3.m2.2.2.4" xref="S3.E3.m2.2.2.4.cmml"></mi><mo id="S3.E3.m2.2.2.3" xref="S3.E3.m2.2.2.3.cmml">=</mo><mrow id="S3.E3.m2.2.2.2" xref="S3.E3.m2.2.2.2.cmml"><mi id="S3.E3.m2.2.2.2.4" xref="S3.E3.m2.2.2.2.4.cmml">Write</mi><mo id="S3.E3.m2.2.2.2.3" xref="S3.E3.m2.2.2.2.3.cmml">⁢</mo><mrow id="S3.E3.m2.2.2.2.2.2" xref="S3.E3.m2.2.2.2.2.3.cmml"><mo id="S3.E3.m2.2.2.2.2.2.3" stretchy="false" xref="S3.E3.m2.2.2.2.2.3.cmml">(</mo><msup id="S3.E3.m2.1.1.1.1.1.1" xref="S3.E3.m2.1.1.1.1.1.1.cmml"><mi id="S3.E3.m2.1.1.1.1.1.1.2" xref="S3.E3.m2.1.1.1.1.1.1.2.cmml">P</mi><mi id="S3.E3.m2.1.1.1.1.1.1.3" xref="S3.E3.m2.1.1.1.1.1.1.3.cmml">l</mi></msup><mo id="S3.E3.m2.2.2.2.2.2.4" xref="S3.E3.m2.2.2.2.2.3.cmml">,</mo><msup id="S3.E3.m2.2.2.2.2.2.2" xref="S3.E3.m2.2.2.2.2.2.2.cmml"><mi id="S3.E3.m2.2.2.2.2.2.2.2" xref="S3.E3.m2.2.2.2.2.2.2.2.cmml">M</mi><mi id="S3.E3.m2.2.2.2.2.2.2.3" xref="S3.E3.m2.2.2.2.2.2.2.3.cmml">l</mi></msup><mo id="S3.E3.m2.2.2.2.2.2.5" stretchy="false" xref="S3.E3.m2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.2b"><apply id="S3.E3.m2.2.2.cmml" xref="S3.E3.m2.2.2"><eq id="S3.E3.m2.2.2.3.cmml" xref="S3.E3.m2.2.2.3"></eq><csymbol cd="latexml" id="S3.E3.m2.2.2.4.cmml" xref="S3.E3.m2.2.2.4">absent</csymbol><apply id="S3.E3.m2.2.2.2.cmml" xref="S3.E3.m2.2.2.2"><times id="S3.E3.m2.2.2.2.3.cmml" xref="S3.E3.m2.2.2.2.3"></times><ci id="S3.E3.m2.2.2.2.4.cmml" xref="S3.E3.m2.2.2.2.4">Write</ci><interval closure="open" id="S3.E3.m2.2.2.2.2.3.cmml" xref="S3.E3.m2.2.2.2.2.2"><apply id="S3.E3.m2.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m2.1.1.1.1.1.1.2">𝑃</ci><ci id="S3.E3.m2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m2.1.1.1.1.1.1.3">𝑙</ci></apply><apply id="S3.E3.m2.2.2.2.2.2.2.cmml" xref="S3.E3.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m2.2.2.2.2.2.2.1.cmml" xref="S3.E3.m2.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E3.m2.2.2.2.2.2.2.2.cmml" xref="S3.E3.m2.2.2.2.2.2.2.2">𝑀</ci><ci id="S3.E3.m2.2.2.2.2.2.2.3.cmml" xref="S3.E3.m2.2.2.2.2.2.2.3">𝑙</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.2c">\displaystyle=\mathrm{Write}(P^{l},M^{l})</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m2.2d">= roman_Write ( italic_P start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle M^{l+1}" class="ltx_Math" display="inline" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><msup id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">M</mi><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">l</mi><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><mn id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1">superscript</csymbol><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">𝑀</ci><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><plus id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></plus><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">𝑙</ci><cn id="S3.E4.m1.1.1.3.3.cmml" type="integer" xref="S3.E4.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle M^{l+1}</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">italic_M start_POSTSUPERSCRIPT italic_l + 1 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathrm{Fusion}(M^{l},W^{l})" class="ltx_Math" display="inline" id="S3.E4.m2.2"><semantics id="S3.E4.m2.2a"><mrow id="S3.E4.m2.2.2" xref="S3.E4.m2.2.2.cmml"><mi id="S3.E4.m2.2.2.4" xref="S3.E4.m2.2.2.4.cmml"></mi><mo id="S3.E4.m2.2.2.3" xref="S3.E4.m2.2.2.3.cmml">=</mo><mrow id="S3.E4.m2.2.2.2" xref="S3.E4.m2.2.2.2.cmml"><mi id="S3.E4.m2.2.2.2.4" xref="S3.E4.m2.2.2.2.4.cmml">Fusion</mi><mo id="S3.E4.m2.2.2.2.3" xref="S3.E4.m2.2.2.2.3.cmml">⁢</mo><mrow id="S3.E4.m2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.3.cmml"><mo id="S3.E4.m2.2.2.2.2.2.3" stretchy="false" xref="S3.E4.m2.2.2.2.2.3.cmml">(</mo><msup id="S3.E4.m2.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.cmml"><mi id="S3.E4.m2.1.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.1.2.cmml">M</mi><mi id="S3.E4.m2.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.3.cmml">l</mi></msup><mo id="S3.E4.m2.2.2.2.2.2.4" xref="S3.E4.m2.2.2.2.2.3.cmml">,</mo><msup id="S3.E4.m2.2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.2.2.cmml"><mi id="S3.E4.m2.2.2.2.2.2.2.2" xref="S3.E4.m2.2.2.2.2.2.2.2.cmml">W</mi><mi id="S3.E4.m2.2.2.2.2.2.2.3" xref="S3.E4.m2.2.2.2.2.2.2.3.cmml">l</mi></msup><mo id="S3.E4.m2.2.2.2.2.2.5" stretchy="false" xref="S3.E4.m2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.2b"><apply id="S3.E4.m2.2.2.cmml" xref="S3.E4.m2.2.2"><eq id="S3.E4.m2.2.2.3.cmml" xref="S3.E4.m2.2.2.3"></eq><csymbol cd="latexml" id="S3.E4.m2.2.2.4.cmml" xref="S3.E4.m2.2.2.4">absent</csymbol><apply id="S3.E4.m2.2.2.2.cmml" xref="S3.E4.m2.2.2.2"><times id="S3.E4.m2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.3"></times><ci id="S3.E4.m2.2.2.2.4.cmml" xref="S3.E4.m2.2.2.2.4">Fusion</ci><interval closure="open" id="S3.E4.m2.2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.2.2"><apply id="S3.E4.m2.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E4.m2.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.2">𝑀</ci><ci id="S3.E4.m2.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.3">𝑙</ci></apply><apply id="S3.E4.m2.2.2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m2.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E4.m2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m2.2.2.2.2.2.2.2">𝑊</ci><ci id="S3.E4.m2.2.2.2.2.2.2.3.cmml" xref="S3.E4.m2.2.2.2.2.2.2.3">𝑙</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.2c">\displaystyle=\mathrm{Fusion}(M^{l},W^{l})</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m2.2d">= roman_Fusion ( italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The read and write operations are performed using <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.1">read-write</span> heads.
Prior to the <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.2">processing</span> operation the read tokens are merged into the process tokens using a <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.3">fusion</span> operation, and a processing operation is carried out by the encoder block.
After the <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.4">processing</span> operation the write tokens are merged into memory tokens.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The goal of ViTTMs is to be efficient while preserving accuracy.
From <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.F3" title="In 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> it is evident that the number of process tokens and the efficiency of the read-write heads will influence latency.
This is because the encoder block’s latency scales with the number of process tokens (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S2.SS2" title="2.2 Token Sparsification ‣ 2 Related Work ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>), and repeated read/write can negate the benefits of reducing tokens.
In addition, the effective use of memory impacts the task performance.
Thus, for ViTTMs to be both efficient and effectively utilize memory, four core design decisions must be addressed:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">How to initialize memory and process tokens?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;padding-top:1.5pt;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">How tokens will be selected for read/write?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;padding-top:1.5pt;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">How will the selected tokens be fused into the process/memory tokens?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;padding-top:1.5pt;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Should memory tokens undergo processing?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Next we present the most effective choices for these design decisions, as identified by our ablations (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2" title="4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Initializing Process and Memory Tokens</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.4">Our tokenization process for memory tokens follows the original ViT architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib8" title="">8</a>]</cite>.
The process is two steps:
First, we convert an image, <math alttext="x\in\mathbb{R}^{3\times H\times W}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">x</mi><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml">H</mi><mo id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><in id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></in><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">𝑥</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3"><times id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1"></times><cn id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2">3</cn><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3">𝐻</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">x\in\mathbb{R}^{3\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_x ∈ blackboard_R start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math> into a set of non-overlapping patches of size <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mi id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m2.1d">italic_p</annotation></semantics></math>.
Second, we apply a learned linear projection, <math alttext="f:\mathbb{R}^{3\times H\times W}\mapsto\mathbb{R}^{\frac{H}{p}\times\frac{W}{p%
}\times d}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3.1"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mrow id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml">f</mi><mo id="S3.SS2.SSS1.p1.3.m3.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml">:</mo><mrow id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml"><msup id="S3.SS2.SSS1.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.cmml"><mn id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.2.cmml">3</mn><mo id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.1.cmml">×</mo><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.3.cmml">H</mi><mo id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.1.cmml">×</mo><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.4" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.4.cmml">W</mi></mrow></msup><mo id="S3.SS2.SSS1.p1.3.m3.1.1.3.1" stretchy="false" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.1.cmml">↦</mo><msup id="S3.SS2.SSS1.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.cmml"><mfrac id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.2.cmml">H</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.3.cmml">p</mi></mfrac><mo id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.1.cmml">×</mo><mfrac id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.2.cmml">W</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.3.cmml">p</mi></mfrac><mo id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.4" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.4.cmml">d</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><ci id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1">:</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">𝑓</ci><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3"><csymbol cd="latexml" id="S3.SS2.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.1">maps-to</csymbol><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2">superscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.2">ℝ</ci><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3"><times id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.1"></times><cn id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.2.cmml" type="integer" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.2">3</cn><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.3">𝐻</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.4.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.3.4">𝑊</ci></apply></apply><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.2">ℝ</ci><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3"><times id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.1"></times><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2"><divide id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2"></divide><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.2">𝐻</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.2.3">𝑝</ci></apply><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3"><divide id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3"></divide><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.2">𝑊</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.3.3">𝑝</ci></apply><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.4.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.3.4">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">f:\mathbb{R}^{3\times H\times W}\mapsto\mathbb{R}^{\frac{H}{p}\times\frac{W}{p%
}\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.3.m3.1d">italic_f : blackboard_R start_POSTSUPERSCRIPT 3 × italic_H × italic_W end_POSTSUPERSCRIPT ↦ blackboard_R start_POSTSUPERSCRIPT divide start_ARG italic_H end_ARG start_ARG italic_p end_ARG × divide start_ARG italic_W end_ARG start_ARG italic_p end_ARG × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, to tokens of length <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.4.m4.1"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mi id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.4.m4.1d">italic_d</annotation></semantics></math>.
The intuition is that the memory tokens should contain a rich representation of the image.
However, the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.4.1">process</span> tokens requires further consideration, because it is not obvious how they should be created.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">We explore three options for initializing the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.1">process</span> tokens:
(1) <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.2">Latent</span> – learning a set of latent tokens;
(2) <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.3">Down-sample</span> – down-sampling the memory tokens to create the process tokens; and
(3) <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.4">Patch</span> – learning a projection for the process tokens.
In practice, they have identical performance.
We opt to <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.5">learn separate projections for <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.5.1">memory</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.5.2">process</span> tokens</span>, since this is most similar ViT embeddings.
To maintain ViTTM’s efficiency, we ensure that there are fewer process tokens than memory tokens.
This is achieved by using a larger patch size for process tokens.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Read-Write Heads</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Read-write heads play a crucial role in selecting relevant information to read from or write to memory.
A key consideration is the efficiency of reads and writes.
To this end, we evaluate several designs for read-write heads: TokenSummary <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib30" title="">30</a>]</cite>, Cross Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib37" title="">37</a>]</cite>, Latent Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib7" title="">7</a>]</cite>, and Linear Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">In this work, we adopt <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.1">Linear Attention</span>, as introduced by Katharopoulos <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p2.1.2">et al</em>.<span class="ltx_text" id="S3.SS2.SSS2.p2.1.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib18" title="">18</a>]</cite>, to implement our read-write head.
We justify our use of Linear Attention based on theoretical analysis and empirical results from our ablation studies (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2" title="4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>).
It has the following benefits:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">The computational complexity of Linear Attention depends on the length of <span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.1">one</span> input sequence, whereas Cross Attention depends on the length of <span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.2">both</span> sequences.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Linear attention is independent of sequence length, whereas TokenSummary is not.
As a result, off-the-shelf token sparsification methods such as ToMe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib1" title="">1</a>]</cite> can be used along side ViTTM.
Additionally, we can use training methods such as MAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib17" title="">17</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">We find that <span class="ltx_text ltx_font_italic" id="S3.I2.i3.p1.1.1">Linear attention</span> has similar accuracy to cross-attention, while being more efficient.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.7">Our read-write head
computes linear attention between two input sequences <math alttext="X_{1}\in\mathbb{R}^{T\times d}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.1.m1.1"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><mrow id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml"><msub id="S3.SS2.SSS2.p3.1.m1.1.1.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.2.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.2.cmml">X</mi><mn id="S3.SS2.SSS2.p3.1.m1.1.1.2.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p3.1.m1.1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p3.1.m1.1.1.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS2.p3.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.2.cmml">T</mi><mo id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><apply id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1"><in id="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.1"></in><apply id="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.2">𝑋</ci><cn id="S3.SS2.SSS2.p3.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.3">1</cn></apply><apply id="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3"><times id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.2">𝑇</ci><ci id="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">X_{1}\in\mathbb{R}^{T\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.1.m1.1d">italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="X_{2}\in\mathbb{R}^{K\times d}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.2.m2.1"><semantics id="S3.SS2.SSS2.p3.2.m2.1a"><mrow id="S3.SS2.SSS2.p3.2.m2.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.cmml"><msub id="S3.SS2.SSS2.p3.2.m2.1.1.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.2.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.2.cmml">X</mi><mn id="S3.SS2.SSS2.p3.2.m2.1.1.2.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS2.p3.2.m2.1.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p3.2.m2.1.1.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS2.p3.2.m2.1.1.3.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.2.cmml">K</mi><mo id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.1b"><apply id="S3.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1"><in id="S3.SS2.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.1"></in><apply id="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.2">𝑋</ci><cn id="S3.SS2.SSS2.p3.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.3">2</cn></apply><apply id="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3"><times id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.2">𝐾</ci><ci id="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.1c">X_{2}\in\mathbb{R}^{K\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.2.m2.1d">italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>.
We compute the query, key, and value matrices using the weight matrices <math alttext="W_{q}\in\mathbb{R}^{d\times c}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.3.m3.1"><semantics id="S3.SS2.SSS2.p3.3.m3.1a"><mrow id="S3.SS2.SSS2.p3.3.m3.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.cmml"><msub id="S3.SS2.SSS2.p3.3.m3.1.1.2" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.3.m3.1.1.2.2" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.2.cmml">W</mi><mi id="S3.SS2.SSS2.p3.3.m3.1.1.2.3" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.3.cmml">q</mi></msub><mo id="S3.SS2.SSS2.p3.3.m3.1.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p3.3.m3.1.1.3" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.3.m3.1.1.3.2" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS2.p3.3.m3.1.1.3.3" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.3.m3.1.1.3.3.2" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.3.2.cmml">d</mi><mo id="S3.SS2.SSS2.p3.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS2.p3.3.m3.1.1.3.3.3" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.3.3.cmml">c</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m3.1b"><apply id="S3.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1"><in id="S3.SS2.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.1"></in><apply id="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.2">𝑊</ci><ci id="S3.SS2.SSS2.p3.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.3">𝑞</ci></apply><apply id="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS2.p3.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.3"><times id="S3.SS2.SSS2.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.3.2">𝑑</ci><ci id="S3.SS2.SSS2.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.3.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m3.1c">W_{q}\in\mathbb{R}^{d\times c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.3.m3.1d">italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_c end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="W_{k}\in\mathbb{R}^{d\times c}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.4.m4.1"><semantics id="S3.SS2.SSS2.p3.4.m4.1a"><mrow id="S3.SS2.SSS2.p3.4.m4.1.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.cmml"><msub id="S3.SS2.SSS2.p3.4.m4.1.1.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.1.1.2.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.2.2.cmml">W</mi><mi id="S3.SS2.SSS2.p3.4.m4.1.1.2.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.2.3.cmml">k</mi></msub><mo id="S3.SS2.SSS2.p3.4.m4.1.1.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p3.4.m4.1.1.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.1.1.3.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS2.p3.4.m4.1.1.3.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.2" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.2.cmml">d</mi><mo id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.3" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.3.cmml">c</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.4.m4.1b"><apply id="S3.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1"><in id="S3.SS2.SSS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.1"></in><apply id="S3.SS2.SSS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.2.2">𝑊</ci><ci id="S3.SS2.SSS2.p3.4.m4.1.1.2.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.2.3">𝑘</ci></apply><apply id="S3.SS2.SSS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3"><times id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.2">𝑑</ci><ci id="S3.SS2.SSS2.p3.4.m4.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.3.3.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.4.m4.1c">W_{k}\in\mathbb{R}^{d\times c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.4.m4.1d">italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_c end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="W_{v}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.5.m5.1"><semantics id="S3.SS2.SSS2.p3.5.m5.1a"><mrow id="S3.SS2.SSS2.p3.5.m5.1.1" xref="S3.SS2.SSS2.p3.5.m5.1.1.cmml"><msub id="S3.SS2.SSS2.p3.5.m5.1.1.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.1.1.2.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.2.2.cmml">W</mi><mi id="S3.SS2.SSS2.p3.5.m5.1.1.2.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.2.3.cmml">v</mi></msub><mo id="S3.SS2.SSS2.p3.5.m5.1.1.1" xref="S3.SS2.SSS2.p3.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p3.5.m5.1.1.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.1.1.3.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS2.p3.5.m5.1.1.3.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.5.m5.1.1.3.3.2" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3.2.cmml">d</mi><mo id="S3.SS2.SSS2.p3.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS2.p3.5.m5.1.1.3.3.3" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.5.m5.1b"><apply id="S3.SS2.SSS2.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1"><in id="S3.SS2.SSS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.1"></in><apply id="S3.SS2.SSS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.2.2">𝑊</ci><ci id="S3.SS2.SSS2.p3.5.m5.1.1.2.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.2.3">𝑣</ci></apply><apply id="S3.SS2.SSS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.5.m5.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.5.m5.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS2.p3.5.m5.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3"><times id="S3.SS2.SSS2.p3.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3.1"></times><ci id="S3.SS2.SSS2.p3.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3.2">𝑑</ci><ci id="S3.SS2.SSS2.p3.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.5.m5.1c">W_{v}\in\mathbb{R}^{d\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.5.m5.1d">italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.6.m6.1"><semantics id="S3.SS2.SSS2.p3.6.m6.1a"><mi id="S3.SS2.SSS2.p3.6.m6.1.1" xref="S3.SS2.SSS2.p3.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.6.m6.1b"><ci id="S3.SS2.SSS2.p3.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p3.6.m6.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.6.m6.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.6.m6.1d">italic_d</annotation></semantics></math> represents the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p3.7.1">embedding dimension</span> and <math alttext="c" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.7.m7.1"><semantics id="S3.SS2.SSS2.p3.7.m7.1a"><mi id="S3.SS2.SSS2.p3.7.m7.1.1" xref="S3.SS2.SSS2.p3.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.7.m7.1b"><ci id="S3.SS2.SSS2.p3.7.m7.1.1.cmml" xref="S3.SS2.SSS2.p3.7.m7.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.7.m7.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.7.m7.1d">italic_c</annotation></semantics></math> represents the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p3.7.2">latent embedding dimension</span>.
The query, key, and value matrices are calculated as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx2">
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle Q" class="ltx_Math" display="inline" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle Q</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">italic_Q</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=X_{2}W_{q}\in\mathbb{R}^{K\times c}" class="ltx_Math" display="inline" id="S3.E5.m2.1"><semantics id="S3.E5.m2.1a"><mrow id="S3.E5.m2.1.1" xref="S3.E5.m2.1.1.cmml"><mi id="S3.E5.m2.1.1.2" xref="S3.E5.m2.1.1.2.cmml"></mi><mo id="S3.E5.m2.1.1.3" xref="S3.E5.m2.1.1.3.cmml">=</mo><mrow id="S3.E5.m2.1.1.4" xref="S3.E5.m2.1.1.4.cmml"><msub id="S3.E5.m2.1.1.4.2" xref="S3.E5.m2.1.1.4.2.cmml"><mi id="S3.E5.m2.1.1.4.2.2" xref="S3.E5.m2.1.1.4.2.2.cmml">X</mi><mn id="S3.E5.m2.1.1.4.2.3" xref="S3.E5.m2.1.1.4.2.3.cmml">2</mn></msub><mo id="S3.E5.m2.1.1.4.1" xref="S3.E5.m2.1.1.4.1.cmml">⁢</mo><msub id="S3.E5.m2.1.1.4.3" xref="S3.E5.m2.1.1.4.3.cmml"><mi id="S3.E5.m2.1.1.4.3.2" xref="S3.E5.m2.1.1.4.3.2.cmml">W</mi><mi id="S3.E5.m2.1.1.4.3.3" xref="S3.E5.m2.1.1.4.3.3.cmml">q</mi></msub></mrow><mo id="S3.E5.m2.1.1.5" xref="S3.E5.m2.1.1.5.cmml">∈</mo><msup id="S3.E5.m2.1.1.6" xref="S3.E5.m2.1.1.6.cmml"><mi id="S3.E5.m2.1.1.6.2" xref="S3.E5.m2.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E5.m2.1.1.6.3" xref="S3.E5.m2.1.1.6.3.cmml"><mi id="S3.E5.m2.1.1.6.3.2" xref="S3.E5.m2.1.1.6.3.2.cmml">K</mi><mo id="S3.E5.m2.1.1.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E5.m2.1.1.6.3.1.cmml">×</mo><mi id="S3.E5.m2.1.1.6.3.3" xref="S3.E5.m2.1.1.6.3.3.cmml">c</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.1b"><apply id="S3.E5.m2.1.1.cmml" xref="S3.E5.m2.1.1"><and id="S3.E5.m2.1.1a.cmml" xref="S3.E5.m2.1.1"></and><apply id="S3.E5.m2.1.1b.cmml" xref="S3.E5.m2.1.1"><eq id="S3.E5.m2.1.1.3.cmml" xref="S3.E5.m2.1.1.3"></eq><csymbol cd="latexml" id="S3.E5.m2.1.1.2.cmml" xref="S3.E5.m2.1.1.2">absent</csymbol><apply id="S3.E5.m2.1.1.4.cmml" xref="S3.E5.m2.1.1.4"><times id="S3.E5.m2.1.1.4.1.cmml" xref="S3.E5.m2.1.1.4.1"></times><apply id="S3.E5.m2.1.1.4.2.cmml" xref="S3.E5.m2.1.1.4.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.4.2.1.cmml" xref="S3.E5.m2.1.1.4.2">subscript</csymbol><ci id="S3.E5.m2.1.1.4.2.2.cmml" xref="S3.E5.m2.1.1.4.2.2">𝑋</ci><cn id="S3.E5.m2.1.1.4.2.3.cmml" type="integer" xref="S3.E5.m2.1.1.4.2.3">2</cn></apply><apply id="S3.E5.m2.1.1.4.3.cmml" xref="S3.E5.m2.1.1.4.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.4.3.1.cmml" xref="S3.E5.m2.1.1.4.3">subscript</csymbol><ci id="S3.E5.m2.1.1.4.3.2.cmml" xref="S3.E5.m2.1.1.4.3.2">𝑊</ci><ci id="S3.E5.m2.1.1.4.3.3.cmml" xref="S3.E5.m2.1.1.4.3.3">𝑞</ci></apply></apply></apply><apply id="S3.E5.m2.1.1c.cmml" xref="S3.E5.m2.1.1"><in id="S3.E5.m2.1.1.5.cmml" xref="S3.E5.m2.1.1.5"></in><share href="https://arxiv.org/html/2409.07613v1#S3.E5.m2.1.1.4.cmml" id="S3.E5.m2.1.1d.cmml" xref="S3.E5.m2.1.1"></share><apply id="S3.E5.m2.1.1.6.cmml" xref="S3.E5.m2.1.1.6"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.6.1.cmml" xref="S3.E5.m2.1.1.6">superscript</csymbol><ci id="S3.E5.m2.1.1.6.2.cmml" xref="S3.E5.m2.1.1.6.2">ℝ</ci><apply id="S3.E5.m2.1.1.6.3.cmml" xref="S3.E5.m2.1.1.6.3"><times id="S3.E5.m2.1.1.6.3.1.cmml" xref="S3.E5.m2.1.1.6.3.1"></times><ci id="S3.E5.m2.1.1.6.3.2.cmml" xref="S3.E5.m2.1.1.6.3.2">𝐾</ci><ci id="S3.E5.m2.1.1.6.3.3.cmml" xref="S3.E5.m2.1.1.6.3.3">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.1c">\displaystyle=X_{2}W_{q}\in\mathbb{R}^{K\times c}</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m2.1d">= italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_c end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle K" class="ltx_Math" display="inline" id="S3.E6.m1.1"><semantics id="S3.E6.m1.1a"><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\displaystyle K</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.1d">italic_K</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=X_{1}W_{k}\in\mathbb{R}^{T\times c}" class="ltx_Math" display="inline" id="S3.E6.m2.1"><semantics id="S3.E6.m2.1a"><mrow id="S3.E6.m2.1.1" xref="S3.E6.m2.1.1.cmml"><mi id="S3.E6.m2.1.1.2" xref="S3.E6.m2.1.1.2.cmml"></mi><mo id="S3.E6.m2.1.1.3" xref="S3.E6.m2.1.1.3.cmml">=</mo><mrow id="S3.E6.m2.1.1.4" xref="S3.E6.m2.1.1.4.cmml"><msub id="S3.E6.m2.1.1.4.2" xref="S3.E6.m2.1.1.4.2.cmml"><mi id="S3.E6.m2.1.1.4.2.2" xref="S3.E6.m2.1.1.4.2.2.cmml">X</mi><mn id="S3.E6.m2.1.1.4.2.3" xref="S3.E6.m2.1.1.4.2.3.cmml">1</mn></msub><mo id="S3.E6.m2.1.1.4.1" xref="S3.E6.m2.1.1.4.1.cmml">⁢</mo><msub id="S3.E6.m2.1.1.4.3" xref="S3.E6.m2.1.1.4.3.cmml"><mi id="S3.E6.m2.1.1.4.3.2" xref="S3.E6.m2.1.1.4.3.2.cmml">W</mi><mi id="S3.E6.m2.1.1.4.3.3" xref="S3.E6.m2.1.1.4.3.3.cmml">k</mi></msub></mrow><mo id="S3.E6.m2.1.1.5" xref="S3.E6.m2.1.1.5.cmml">∈</mo><msup id="S3.E6.m2.1.1.6" xref="S3.E6.m2.1.1.6.cmml"><mi id="S3.E6.m2.1.1.6.2" xref="S3.E6.m2.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E6.m2.1.1.6.3" xref="S3.E6.m2.1.1.6.3.cmml"><mi id="S3.E6.m2.1.1.6.3.2" xref="S3.E6.m2.1.1.6.3.2.cmml">T</mi><mo id="S3.E6.m2.1.1.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E6.m2.1.1.6.3.1.cmml">×</mo><mi id="S3.E6.m2.1.1.6.3.3" xref="S3.E6.m2.1.1.6.3.3.cmml">c</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m2.1b"><apply id="S3.E6.m2.1.1.cmml" xref="S3.E6.m2.1.1"><and id="S3.E6.m2.1.1a.cmml" xref="S3.E6.m2.1.1"></and><apply id="S3.E6.m2.1.1b.cmml" xref="S3.E6.m2.1.1"><eq id="S3.E6.m2.1.1.3.cmml" xref="S3.E6.m2.1.1.3"></eq><csymbol cd="latexml" id="S3.E6.m2.1.1.2.cmml" xref="S3.E6.m2.1.1.2">absent</csymbol><apply id="S3.E6.m2.1.1.4.cmml" xref="S3.E6.m2.1.1.4"><times id="S3.E6.m2.1.1.4.1.cmml" xref="S3.E6.m2.1.1.4.1"></times><apply id="S3.E6.m2.1.1.4.2.cmml" xref="S3.E6.m2.1.1.4.2"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.4.2.1.cmml" xref="S3.E6.m2.1.1.4.2">subscript</csymbol><ci id="S3.E6.m2.1.1.4.2.2.cmml" xref="S3.E6.m2.1.1.4.2.2">𝑋</ci><cn id="S3.E6.m2.1.1.4.2.3.cmml" type="integer" xref="S3.E6.m2.1.1.4.2.3">1</cn></apply><apply id="S3.E6.m2.1.1.4.3.cmml" xref="S3.E6.m2.1.1.4.3"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.4.3.1.cmml" xref="S3.E6.m2.1.1.4.3">subscript</csymbol><ci id="S3.E6.m2.1.1.4.3.2.cmml" xref="S3.E6.m2.1.1.4.3.2">𝑊</ci><ci id="S3.E6.m2.1.1.4.3.3.cmml" xref="S3.E6.m2.1.1.4.3.3">𝑘</ci></apply></apply></apply><apply id="S3.E6.m2.1.1c.cmml" xref="S3.E6.m2.1.1"><in id="S3.E6.m2.1.1.5.cmml" xref="S3.E6.m2.1.1.5"></in><share href="https://arxiv.org/html/2409.07613v1#S3.E6.m2.1.1.4.cmml" id="S3.E6.m2.1.1d.cmml" xref="S3.E6.m2.1.1"></share><apply id="S3.E6.m2.1.1.6.cmml" xref="S3.E6.m2.1.1.6"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.6.1.cmml" xref="S3.E6.m2.1.1.6">superscript</csymbol><ci id="S3.E6.m2.1.1.6.2.cmml" xref="S3.E6.m2.1.1.6.2">ℝ</ci><apply id="S3.E6.m2.1.1.6.3.cmml" xref="S3.E6.m2.1.1.6.3"><times id="S3.E6.m2.1.1.6.3.1.cmml" xref="S3.E6.m2.1.1.6.3.1"></times><ci id="S3.E6.m2.1.1.6.3.2.cmml" xref="S3.E6.m2.1.1.6.3.2">𝑇</ci><ci id="S3.E6.m2.1.1.6.3.3.cmml" xref="S3.E6.m2.1.1.6.3.3">𝑐</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m2.1c">\displaystyle=X_{1}W_{k}\in\mathbb{R}^{T\times c}</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m2.1d">= italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_c end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle V" class="ltx_Math" display="inline" id="S3.E7.m1.1"><semantics id="S3.E7.m1.1a"><mi id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><ci id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\displaystyle V</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.1d">italic_V</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=X_{1}W_{v}\in\mathbb{R}^{T\times d}" class="ltx_Math" display="inline" id="S3.E7.m2.1"><semantics id="S3.E7.m2.1a"><mrow id="S3.E7.m2.1.1" xref="S3.E7.m2.1.1.cmml"><mi id="S3.E7.m2.1.1.2" xref="S3.E7.m2.1.1.2.cmml"></mi><mo id="S3.E7.m2.1.1.3" xref="S3.E7.m2.1.1.3.cmml">=</mo><mrow id="S3.E7.m2.1.1.4" xref="S3.E7.m2.1.1.4.cmml"><msub id="S3.E7.m2.1.1.4.2" xref="S3.E7.m2.1.1.4.2.cmml"><mi id="S3.E7.m2.1.1.4.2.2" xref="S3.E7.m2.1.1.4.2.2.cmml">X</mi><mn id="S3.E7.m2.1.1.4.2.3" xref="S3.E7.m2.1.1.4.2.3.cmml">1</mn></msub><mo id="S3.E7.m2.1.1.4.1" xref="S3.E7.m2.1.1.4.1.cmml">⁢</mo><msub id="S3.E7.m2.1.1.4.3" xref="S3.E7.m2.1.1.4.3.cmml"><mi id="S3.E7.m2.1.1.4.3.2" xref="S3.E7.m2.1.1.4.3.2.cmml">W</mi><mi id="S3.E7.m2.1.1.4.3.3" xref="S3.E7.m2.1.1.4.3.3.cmml">v</mi></msub></mrow><mo id="S3.E7.m2.1.1.5" xref="S3.E7.m2.1.1.5.cmml">∈</mo><msup id="S3.E7.m2.1.1.6" xref="S3.E7.m2.1.1.6.cmml"><mi id="S3.E7.m2.1.1.6.2" xref="S3.E7.m2.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E7.m2.1.1.6.3" xref="S3.E7.m2.1.1.6.3.cmml"><mi id="S3.E7.m2.1.1.6.3.2" xref="S3.E7.m2.1.1.6.3.2.cmml">T</mi><mo id="S3.E7.m2.1.1.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E7.m2.1.1.6.3.1.cmml">×</mo><mi id="S3.E7.m2.1.1.6.3.3" xref="S3.E7.m2.1.1.6.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m2.1b"><apply id="S3.E7.m2.1.1.cmml" xref="S3.E7.m2.1.1"><and id="S3.E7.m2.1.1a.cmml" xref="S3.E7.m2.1.1"></and><apply id="S3.E7.m2.1.1b.cmml" xref="S3.E7.m2.1.1"><eq id="S3.E7.m2.1.1.3.cmml" xref="S3.E7.m2.1.1.3"></eq><csymbol cd="latexml" id="S3.E7.m2.1.1.2.cmml" xref="S3.E7.m2.1.1.2">absent</csymbol><apply id="S3.E7.m2.1.1.4.cmml" xref="S3.E7.m2.1.1.4"><times id="S3.E7.m2.1.1.4.1.cmml" xref="S3.E7.m2.1.1.4.1"></times><apply id="S3.E7.m2.1.1.4.2.cmml" xref="S3.E7.m2.1.1.4.2"><csymbol cd="ambiguous" id="S3.E7.m2.1.1.4.2.1.cmml" xref="S3.E7.m2.1.1.4.2">subscript</csymbol><ci id="S3.E7.m2.1.1.4.2.2.cmml" xref="S3.E7.m2.1.1.4.2.2">𝑋</ci><cn id="S3.E7.m2.1.1.4.2.3.cmml" type="integer" xref="S3.E7.m2.1.1.4.2.3">1</cn></apply><apply id="S3.E7.m2.1.1.4.3.cmml" xref="S3.E7.m2.1.1.4.3"><csymbol cd="ambiguous" id="S3.E7.m2.1.1.4.3.1.cmml" xref="S3.E7.m2.1.1.4.3">subscript</csymbol><ci id="S3.E7.m2.1.1.4.3.2.cmml" xref="S3.E7.m2.1.1.4.3.2">𝑊</ci><ci id="S3.E7.m2.1.1.4.3.3.cmml" xref="S3.E7.m2.1.1.4.3.3">𝑣</ci></apply></apply></apply><apply id="S3.E7.m2.1.1c.cmml" xref="S3.E7.m2.1.1"><in id="S3.E7.m2.1.1.5.cmml" xref="S3.E7.m2.1.1.5"></in><share href="https://arxiv.org/html/2409.07613v1#S3.E7.m2.1.1.4.cmml" id="S3.E7.m2.1.1d.cmml" xref="S3.E7.m2.1.1"></share><apply id="S3.E7.m2.1.1.6.cmml" xref="S3.E7.m2.1.1.6"><csymbol cd="ambiguous" id="S3.E7.m2.1.1.6.1.cmml" xref="S3.E7.m2.1.1.6">superscript</csymbol><ci id="S3.E7.m2.1.1.6.2.cmml" xref="S3.E7.m2.1.1.6.2">ℝ</ci><apply id="S3.E7.m2.1.1.6.3.cmml" xref="S3.E7.m2.1.1.6.3"><times id="S3.E7.m2.1.1.6.3.1.cmml" xref="S3.E7.m2.1.1.6.3.1"></times><ci id="S3.E7.m2.1.1.6.3.2.cmml" xref="S3.E7.m2.1.1.6.3.2">𝑇</ci><ci id="S3.E7.m2.1.1.6.3.3.cmml" xref="S3.E7.m2.1.1.6.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m2.1c">\displaystyle=X_{1}W_{v}\in\mathbb{R}^{T\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m2.1d">= italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS2.p3.8">Following Katharopoulos <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS2.p3.8.1">et al</em>.<span class="ltx_text" id="S3.SS2.SSS2.p3.8.2"></span>, we compute the output sequence, <math alttext="V^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.8.m1.1"><semantics id="S3.SS2.SSS2.p3.8.m1.1a"><msup id="S3.SS2.SSS2.p3.8.m1.1.1" xref="S3.SS2.SSS2.p3.8.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p3.8.m1.1.1.2" xref="S3.SS2.SSS2.p3.8.m1.1.1.2.cmml">V</mi><mo id="S3.SS2.SSS2.p3.8.m1.1.1.3" xref="S3.SS2.SSS2.p3.8.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.8.m1.1b"><apply id="S3.SS2.SSS2.p3.8.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.8.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.8.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS2.p3.8.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.8.m1.1.1.2">𝑉</ci><ci id="S3.SS2.SSS2.p3.8.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.8.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.8.m1.1c">V^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.8.m1.1d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx3">
<tbody id="S3.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle V^{\prime}" class="ltx_Math" display="inline" id="S3.E8.m1.1"><semantics id="S3.E8.m1.1a"><msup id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><mi id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml">V</mi><mo id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1">superscript</csymbol><ci id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2">𝑉</ci><ci id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">\displaystyle V^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.E8.m1.1d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\phi(Q)\,(\phi(K)^{T}\,V)" class="ltx_Math" display="inline" id="S3.E8.m2.3"><semantics id="S3.E8.m2.3a"><mrow id="S3.E8.m2.3.3" xref="S3.E8.m2.3.3.cmml"><mi id="S3.E8.m2.3.3.3" xref="S3.E8.m2.3.3.3.cmml"></mi><mo id="S3.E8.m2.3.3.2" xref="S3.E8.m2.3.3.2.cmml">=</mo><mrow id="S3.E8.m2.3.3.1" xref="S3.E8.m2.3.3.1.cmml"><mi id="S3.E8.m2.3.3.1.3" xref="S3.E8.m2.3.3.1.3.cmml">ϕ</mi><mo id="S3.E8.m2.3.3.1.2" xref="S3.E8.m2.3.3.1.2.cmml">⁢</mo><mrow id="S3.E8.m2.3.3.1.4.2" xref="S3.E8.m2.3.3.1.cmml"><mo id="S3.E8.m2.3.3.1.4.2.1" stretchy="false" xref="S3.E8.m2.3.3.1.cmml">(</mo><mi id="S3.E8.m2.1.1" xref="S3.E8.m2.1.1.cmml">Q</mi><mo id="S3.E8.m2.3.3.1.4.2.2" stretchy="false" xref="S3.E8.m2.3.3.1.cmml">)</mo></mrow><mo id="S3.E8.m2.3.3.1.2a" lspace="0.170em" xref="S3.E8.m2.3.3.1.2.cmml">⁢</mo><mrow id="S3.E8.m2.3.3.1.1.1" xref="S3.E8.m2.3.3.1.1.1.1.cmml"><mo id="S3.E8.m2.3.3.1.1.1.2" stretchy="false" xref="S3.E8.m2.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E8.m2.3.3.1.1.1.1" xref="S3.E8.m2.3.3.1.1.1.1.cmml"><mi id="S3.E8.m2.3.3.1.1.1.1.2" xref="S3.E8.m2.3.3.1.1.1.1.2.cmml">ϕ</mi><mo id="S3.E8.m2.3.3.1.1.1.1.1" xref="S3.E8.m2.3.3.1.1.1.1.1.cmml">⁢</mo><msup id="S3.E8.m2.3.3.1.1.1.1.3" xref="S3.E8.m2.3.3.1.1.1.1.3.cmml"><mrow id="S3.E8.m2.3.3.1.1.1.1.3.2.2" xref="S3.E8.m2.3.3.1.1.1.1.3.cmml"><mo id="S3.E8.m2.3.3.1.1.1.1.3.2.2.1" stretchy="false" xref="S3.E8.m2.3.3.1.1.1.1.3.cmml">(</mo><mi id="S3.E8.m2.2.2" xref="S3.E8.m2.2.2.cmml">K</mi><mo id="S3.E8.m2.3.3.1.1.1.1.3.2.2.2" stretchy="false" xref="S3.E8.m2.3.3.1.1.1.1.3.cmml">)</mo></mrow><mi id="S3.E8.m2.3.3.1.1.1.1.3.3" xref="S3.E8.m2.3.3.1.1.1.1.3.3.cmml">T</mi></msup><mo id="S3.E8.m2.3.3.1.1.1.1.1a" xref="S3.E8.m2.3.3.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E8.m2.3.3.1.1.1.1.4" xref="S3.E8.m2.3.3.1.1.1.1.4.cmml">V</mi></mrow><mo id="S3.E8.m2.3.3.1.1.1.3" stretchy="false" xref="S3.E8.m2.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m2.3b"><apply id="S3.E8.m2.3.3.cmml" xref="S3.E8.m2.3.3"><eq id="S3.E8.m2.3.3.2.cmml" xref="S3.E8.m2.3.3.2"></eq><csymbol cd="latexml" id="S3.E8.m2.3.3.3.cmml" xref="S3.E8.m2.3.3.3">absent</csymbol><apply id="S3.E8.m2.3.3.1.cmml" xref="S3.E8.m2.3.3.1"><times id="S3.E8.m2.3.3.1.2.cmml" xref="S3.E8.m2.3.3.1.2"></times><ci id="S3.E8.m2.3.3.1.3.cmml" xref="S3.E8.m2.3.3.1.3">italic-ϕ</ci><ci id="S3.E8.m2.1.1.cmml" xref="S3.E8.m2.1.1">𝑄</ci><apply id="S3.E8.m2.3.3.1.1.1.1.cmml" xref="S3.E8.m2.3.3.1.1.1"><times id="S3.E8.m2.3.3.1.1.1.1.1.cmml" xref="S3.E8.m2.3.3.1.1.1.1.1"></times><ci id="S3.E8.m2.3.3.1.1.1.1.2.cmml" xref="S3.E8.m2.3.3.1.1.1.1.2">italic-ϕ</ci><apply id="S3.E8.m2.3.3.1.1.1.1.3.cmml" xref="S3.E8.m2.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m2.3.3.1.1.1.1.3.1.cmml" xref="S3.E8.m2.3.3.1.1.1.1.3">superscript</csymbol><ci id="S3.E8.m2.2.2.cmml" xref="S3.E8.m2.2.2">𝐾</ci><ci id="S3.E8.m2.3.3.1.1.1.1.3.3.cmml" xref="S3.E8.m2.3.3.1.1.1.1.3.3">𝑇</ci></apply><ci id="S3.E8.m2.3.3.1.1.1.1.4.cmml" xref="S3.E8.m2.3.3.1.1.1.1.4">𝑉</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m2.3c">\displaystyle=\phi(Q)\,(\phi(K)^{T}\,V)</annotation><annotation encoding="application/x-llamapun" id="S3.E8.m2.3d">= italic_ϕ ( italic_Q ) ( italic_ϕ ( italic_K ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_V )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
<tbody id="S3.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\phi(x)" class="ltx_Math" display="inline" id="S3.E9.m1.1"><semantics id="S3.E9.m1.1a"><mrow id="S3.E9.m1.1.2" xref="S3.E9.m1.1.2.cmml"><mi id="S3.E9.m1.1.2.2" xref="S3.E9.m1.1.2.2.cmml">ϕ</mi><mo id="S3.E9.m1.1.2.1" xref="S3.E9.m1.1.2.1.cmml">⁢</mo><mrow id="S3.E9.m1.1.2.3.2" xref="S3.E9.m1.1.2.cmml"><mo id="S3.E9.m1.1.2.3.2.1" stretchy="false" xref="S3.E9.m1.1.2.cmml">(</mo><mi id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml">x</mi><mo id="S3.E9.m1.1.2.3.2.2" stretchy="false" xref="S3.E9.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.2.cmml" xref="S3.E9.m1.1.2"><times id="S3.E9.m1.1.2.1.cmml" xref="S3.E9.m1.1.2.1"></times><ci id="S3.E9.m1.1.2.2.cmml" xref="S3.E9.m1.1.2.2">italic-ϕ</ci><ci id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">\displaystyle\phi(x)</annotation><annotation encoding="application/x-llamapun" id="S3.E9.m1.1d">italic_ϕ ( italic_x )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=1+\text{elu}(x)" class="ltx_Math" display="inline" id="S3.E9.m2.1"><semantics id="S3.E9.m2.1a"><mrow id="S3.E9.m2.1.2" xref="S3.E9.m2.1.2.cmml"><mi id="S3.E9.m2.1.2.2" xref="S3.E9.m2.1.2.2.cmml"></mi><mo id="S3.E9.m2.1.2.1" xref="S3.E9.m2.1.2.1.cmml">=</mo><mrow id="S3.E9.m2.1.2.3" xref="S3.E9.m2.1.2.3.cmml"><mn id="S3.E9.m2.1.2.3.2" xref="S3.E9.m2.1.2.3.2.cmml">1</mn><mo id="S3.E9.m2.1.2.3.1" xref="S3.E9.m2.1.2.3.1.cmml">+</mo><mrow id="S3.E9.m2.1.2.3.3" xref="S3.E9.m2.1.2.3.3.cmml"><mtext id="S3.E9.m2.1.2.3.3.2" xref="S3.E9.m2.1.2.3.3.2a.cmml">elu</mtext><mo id="S3.E9.m2.1.2.3.3.1" xref="S3.E9.m2.1.2.3.3.1.cmml">⁢</mo><mrow id="S3.E9.m2.1.2.3.3.3.2" xref="S3.E9.m2.1.2.3.3.cmml"><mo id="S3.E9.m2.1.2.3.3.3.2.1" stretchy="false" xref="S3.E9.m2.1.2.3.3.cmml">(</mo><mi id="S3.E9.m2.1.1" xref="S3.E9.m2.1.1.cmml">x</mi><mo id="S3.E9.m2.1.2.3.3.3.2.2" stretchy="false" xref="S3.E9.m2.1.2.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m2.1b"><apply id="S3.E9.m2.1.2.cmml" xref="S3.E9.m2.1.2"><eq id="S3.E9.m2.1.2.1.cmml" xref="S3.E9.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.E9.m2.1.2.2.cmml" xref="S3.E9.m2.1.2.2">absent</csymbol><apply id="S3.E9.m2.1.2.3.cmml" xref="S3.E9.m2.1.2.3"><plus id="S3.E9.m2.1.2.3.1.cmml" xref="S3.E9.m2.1.2.3.1"></plus><cn id="S3.E9.m2.1.2.3.2.cmml" type="integer" xref="S3.E9.m2.1.2.3.2">1</cn><apply id="S3.E9.m2.1.2.3.3.cmml" xref="S3.E9.m2.1.2.3.3"><times id="S3.E9.m2.1.2.3.3.1.cmml" xref="S3.E9.m2.1.2.3.3.1"></times><ci id="S3.E9.m2.1.2.3.3.2a.cmml" xref="S3.E9.m2.1.2.3.3.2"><mtext id="S3.E9.m2.1.2.3.3.2.cmml" xref="S3.E9.m2.1.2.3.3.2">elu</mtext></ci><ci id="S3.E9.m2.1.1.cmml" xref="S3.E9.m2.1.1">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m2.1c">\displaystyle=1+\text{elu}(x)</annotation><annotation encoding="application/x-llamapun" id="S3.E9.m2.1d">= 1 + elu ( italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS2.p3.15">During the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p3.15.1">read</span> operation, we set <math alttext="X_{2}=P^{l-1}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.9.m1.1"><semantics id="S3.SS2.SSS2.p3.9.m1.1a"><mrow id="S3.SS2.SSS2.p3.9.m1.1.1" xref="S3.SS2.SSS2.p3.9.m1.1.1.cmml"><msub id="S3.SS2.SSS2.p3.9.m1.1.1.2" xref="S3.SS2.SSS2.p3.9.m1.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.9.m1.1.1.2.2" xref="S3.SS2.SSS2.p3.9.m1.1.1.2.2.cmml">X</mi><mn id="S3.SS2.SSS2.p3.9.m1.1.1.2.3" xref="S3.SS2.SSS2.p3.9.m1.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS2.p3.9.m1.1.1.1" xref="S3.SS2.SSS2.p3.9.m1.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.9.m1.1.1.3" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.9.m1.1.1.3.2" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.2.cmml">P</mi><mrow id="S3.SS2.SSS2.p3.9.m1.1.1.3.3" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p3.9.m1.1.1.3.3.2" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.3.2.cmml">l</mi><mo id="S3.SS2.SSS2.p3.9.m1.1.1.3.3.1" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.3.1.cmml">−</mo><mn id="S3.SS2.SSS2.p3.9.m1.1.1.3.3.3" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.3.3.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.9.m1.1b"><apply id="S3.SS2.SSS2.p3.9.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1"><eq id="S3.SS2.SSS2.p3.9.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.1"></eq><apply id="S3.SS2.SSS2.p3.9.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.9.m1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.9.m1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.2.2">𝑋</ci><cn id="S3.SS2.SSS2.p3.9.m1.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.9.m1.1.1.2.3">2</cn></apply><apply id="S3.SS2.SSS2.p3.9.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.9.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.9.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.2">𝑃</ci><apply id="S3.SS2.SSS2.p3.9.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.3"><minus id="S3.SS2.SSS2.p3.9.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.3.1"></minus><ci id="S3.SS2.SSS2.p3.9.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.3.2">𝑙</ci><cn id="S3.SS2.SSS2.p3.9.m1.1.1.3.3.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.9.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.9.m1.1c">X_{2}=P^{l-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.9.m1.1d">italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_P start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="X_{1}=M^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.10.m2.1"><semantics id="S3.SS2.SSS2.p3.10.m2.1a"><mrow id="S3.SS2.SSS2.p3.10.m2.1.1" xref="S3.SS2.SSS2.p3.10.m2.1.1.cmml"><msub id="S3.SS2.SSS2.p3.10.m2.1.1.2" xref="S3.SS2.SSS2.p3.10.m2.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.10.m2.1.1.2.2" xref="S3.SS2.SSS2.p3.10.m2.1.1.2.2.cmml">X</mi><mn id="S3.SS2.SSS2.p3.10.m2.1.1.2.3" xref="S3.SS2.SSS2.p3.10.m2.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p3.10.m2.1.1.1" xref="S3.SS2.SSS2.p3.10.m2.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.10.m2.1.1.3" xref="S3.SS2.SSS2.p3.10.m2.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.10.m2.1.1.3.2" xref="S3.SS2.SSS2.p3.10.m2.1.1.3.2.cmml">M</mi><mi id="S3.SS2.SSS2.p3.10.m2.1.1.3.3" xref="S3.SS2.SSS2.p3.10.m2.1.1.3.3.cmml">l</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.10.m2.1b"><apply id="S3.SS2.SSS2.p3.10.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1"><eq id="S3.SS2.SSS2.p3.10.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1.1"></eq><apply id="S3.SS2.SSS2.p3.10.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.10.m2.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.10.m2.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1.2.2">𝑋</ci><cn id="S3.SS2.SSS2.p3.10.m2.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.10.m2.1.1.2.3">1</cn></apply><apply id="S3.SS2.SSS2.p3.10.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.10.m2.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.10.m2.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1.3.2">𝑀</ci><ci id="S3.SS2.SSS2.p3.10.m2.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.10.m2.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.10.m2.1c">X_{1}=M^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.10.m2.1d">italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="V^{\prime}=R^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.11.m3.1"><semantics id="S3.SS2.SSS2.p3.11.m3.1a"><mrow id="S3.SS2.SSS2.p3.11.m3.1.1" xref="S3.SS2.SSS2.p3.11.m3.1.1.cmml"><msup id="S3.SS2.SSS2.p3.11.m3.1.1.2" xref="S3.SS2.SSS2.p3.11.m3.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.11.m3.1.1.2.2" xref="S3.SS2.SSS2.p3.11.m3.1.1.2.2.cmml">V</mi><mo id="S3.SS2.SSS2.p3.11.m3.1.1.2.3" xref="S3.SS2.SSS2.p3.11.m3.1.1.2.3.cmml">′</mo></msup><mo id="S3.SS2.SSS2.p3.11.m3.1.1.1" xref="S3.SS2.SSS2.p3.11.m3.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.11.m3.1.1.3" xref="S3.SS2.SSS2.p3.11.m3.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.11.m3.1.1.3.2" xref="S3.SS2.SSS2.p3.11.m3.1.1.3.2.cmml">R</mi><mi id="S3.SS2.SSS2.p3.11.m3.1.1.3.3" xref="S3.SS2.SSS2.p3.11.m3.1.1.3.3.cmml">l</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.11.m3.1b"><apply id="S3.SS2.SSS2.p3.11.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1"><eq id="S3.SS2.SSS2.p3.11.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.1"></eq><apply id="S3.SS2.SSS2.p3.11.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.11.m3.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS2.p3.11.m3.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.2.2">𝑉</ci><ci id="S3.SS2.SSS2.p3.11.m3.1.1.2.3.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.2.3">′</ci></apply><apply id="S3.SS2.SSS2.p3.11.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.11.m3.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.11.m3.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.3.2">𝑅</ci><ci id="S3.SS2.SSS2.p3.11.m3.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.11.m3.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.11.m3.1c">V^{\prime}=R^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.11.m3.1d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>.
Conversely, during the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p3.15.2">write</span> operation, we set <math alttext="X_{M}=M^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.12.m4.1"><semantics id="S3.SS2.SSS2.p3.12.m4.1a"><mrow id="S3.SS2.SSS2.p3.12.m4.1.1" xref="S3.SS2.SSS2.p3.12.m4.1.1.cmml"><msub id="S3.SS2.SSS2.p3.12.m4.1.1.2" xref="S3.SS2.SSS2.p3.12.m4.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.12.m4.1.1.2.2" xref="S3.SS2.SSS2.p3.12.m4.1.1.2.2.cmml">X</mi><mi id="S3.SS2.SSS2.p3.12.m4.1.1.2.3" xref="S3.SS2.SSS2.p3.12.m4.1.1.2.3.cmml">M</mi></msub><mo id="S3.SS2.SSS2.p3.12.m4.1.1.1" xref="S3.SS2.SSS2.p3.12.m4.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.12.m4.1.1.3" xref="S3.SS2.SSS2.p3.12.m4.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.12.m4.1.1.3.2" xref="S3.SS2.SSS2.p3.12.m4.1.1.3.2.cmml">M</mi><mi id="S3.SS2.SSS2.p3.12.m4.1.1.3.3" xref="S3.SS2.SSS2.p3.12.m4.1.1.3.3.cmml">l</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.12.m4.1b"><apply id="S3.SS2.SSS2.p3.12.m4.1.1.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1"><eq id="S3.SS2.SSS2.p3.12.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.1"></eq><apply id="S3.SS2.SSS2.p3.12.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.12.m4.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.12.m4.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.2.2">𝑋</ci><ci id="S3.SS2.SSS2.p3.12.m4.1.1.2.3.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.2.3">𝑀</ci></apply><apply id="S3.SS2.SSS2.p3.12.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.12.m4.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.12.m4.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.3.2">𝑀</ci><ci id="S3.SS2.SSS2.p3.12.m4.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.12.m4.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.12.m4.1c">X_{M}=M^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.12.m4.1d">italic_X start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT = italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="X_{1}=P^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.13.m5.1"><semantics id="S3.SS2.SSS2.p3.13.m5.1a"><mrow id="S3.SS2.SSS2.p3.13.m5.1.1" xref="S3.SS2.SSS2.p3.13.m5.1.1.cmml"><msub id="S3.SS2.SSS2.p3.13.m5.1.1.2" xref="S3.SS2.SSS2.p3.13.m5.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.13.m5.1.1.2.2" xref="S3.SS2.SSS2.p3.13.m5.1.1.2.2.cmml">X</mi><mn id="S3.SS2.SSS2.p3.13.m5.1.1.2.3" xref="S3.SS2.SSS2.p3.13.m5.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p3.13.m5.1.1.1" xref="S3.SS2.SSS2.p3.13.m5.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.13.m5.1.1.3" xref="S3.SS2.SSS2.p3.13.m5.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.13.m5.1.1.3.2" xref="S3.SS2.SSS2.p3.13.m5.1.1.3.2.cmml">P</mi><mi id="S3.SS2.SSS2.p3.13.m5.1.1.3.3" xref="S3.SS2.SSS2.p3.13.m5.1.1.3.3.cmml">l</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.13.m5.1b"><apply id="S3.SS2.SSS2.p3.13.m5.1.1.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1"><eq id="S3.SS2.SSS2.p3.13.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1.1"></eq><apply id="S3.SS2.SSS2.p3.13.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.13.m5.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.13.m5.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1.2.2">𝑋</ci><cn id="S3.SS2.SSS2.p3.13.m5.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.13.m5.1.1.2.3">1</cn></apply><apply id="S3.SS2.SSS2.p3.13.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.13.m5.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.13.m5.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1.3.2">𝑃</ci><ci id="S3.SS2.SSS2.p3.13.m5.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.13.m5.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.13.m5.1c">X_{1}=P^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.13.m5.1d">italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_P start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>, and <math alttext="V^{\prime}=W^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.14.m6.1"><semantics id="S3.SS2.SSS2.p3.14.m6.1a"><mrow id="S3.SS2.SSS2.p3.14.m6.1.1" xref="S3.SS2.SSS2.p3.14.m6.1.1.cmml"><msup id="S3.SS2.SSS2.p3.14.m6.1.1.2" xref="S3.SS2.SSS2.p3.14.m6.1.1.2.cmml"><mi id="S3.SS2.SSS2.p3.14.m6.1.1.2.2" xref="S3.SS2.SSS2.p3.14.m6.1.1.2.2.cmml">V</mi><mo id="S3.SS2.SSS2.p3.14.m6.1.1.2.3" xref="S3.SS2.SSS2.p3.14.m6.1.1.2.3.cmml">′</mo></msup><mo id="S3.SS2.SSS2.p3.14.m6.1.1.1" xref="S3.SS2.SSS2.p3.14.m6.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS2.p3.14.m6.1.1.3" xref="S3.SS2.SSS2.p3.14.m6.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.14.m6.1.1.3.2" xref="S3.SS2.SSS2.p3.14.m6.1.1.3.2.cmml">W</mi><mi id="S3.SS2.SSS2.p3.14.m6.1.1.3.3" xref="S3.SS2.SSS2.p3.14.m6.1.1.3.3.cmml">l</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.14.m6.1b"><apply id="S3.SS2.SSS2.p3.14.m6.1.1.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1"><eq id="S3.SS2.SSS2.p3.14.m6.1.1.1.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.1"></eq><apply id="S3.SS2.SSS2.p3.14.m6.1.1.2.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.14.m6.1.1.2.1.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS2.p3.14.m6.1.1.2.2.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.2.2">𝑉</ci><ci id="S3.SS2.SSS2.p3.14.m6.1.1.2.3.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.2.3">′</ci></apply><apply id="S3.SS2.SSS2.p3.14.m6.1.1.3.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.14.m6.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p3.14.m6.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.3.2">𝑊</ci><ci id="S3.SS2.SSS2.p3.14.m6.1.1.3.3.cmml" xref="S3.SS2.SSS2.p3.14.m6.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.14.m6.1c">V^{\prime}=W^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.14.m6.1d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>.
In our evalutation we use <math alttext="c=d/4" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.15.m7.1"><semantics id="S3.SS2.SSS2.p3.15.m7.1a"><mrow id="S3.SS2.SSS2.p3.15.m7.1.1" xref="S3.SS2.SSS2.p3.15.m7.1.1.cmml"><mi id="S3.SS2.SSS2.p3.15.m7.1.1.2" xref="S3.SS2.SSS2.p3.15.m7.1.1.2.cmml">c</mi><mo id="S3.SS2.SSS2.p3.15.m7.1.1.1" xref="S3.SS2.SSS2.p3.15.m7.1.1.1.cmml">=</mo><mrow id="S3.SS2.SSS2.p3.15.m7.1.1.3" xref="S3.SS2.SSS2.p3.15.m7.1.1.3.cmml"><mi id="S3.SS2.SSS2.p3.15.m7.1.1.3.2" xref="S3.SS2.SSS2.p3.15.m7.1.1.3.2.cmml">d</mi><mo id="S3.SS2.SSS2.p3.15.m7.1.1.3.1" xref="S3.SS2.SSS2.p3.15.m7.1.1.3.1.cmml">/</mo><mn id="S3.SS2.SSS2.p3.15.m7.1.1.3.3" xref="S3.SS2.SSS2.p3.15.m7.1.1.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.15.m7.1b"><apply id="S3.SS2.SSS2.p3.15.m7.1.1.cmml" xref="S3.SS2.SSS2.p3.15.m7.1.1"><eq id="S3.SS2.SSS2.p3.15.m7.1.1.1.cmml" xref="S3.SS2.SSS2.p3.15.m7.1.1.1"></eq><ci id="S3.SS2.SSS2.p3.15.m7.1.1.2.cmml" xref="S3.SS2.SSS2.p3.15.m7.1.1.2">𝑐</ci><apply id="S3.SS2.SSS2.p3.15.m7.1.1.3.cmml" xref="S3.SS2.SSS2.p3.15.m7.1.1.3"><divide id="S3.SS2.SSS2.p3.15.m7.1.1.3.1.cmml" xref="S3.SS2.SSS2.p3.15.m7.1.1.3.1"></divide><ci id="S3.SS2.SSS2.p3.15.m7.1.1.3.2.cmml" xref="S3.SS2.SSS2.p3.15.m7.1.1.3.2">𝑑</ci><cn id="S3.SS2.SSS2.p3.15.m7.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS2.p3.15.m7.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.15.m7.1c">c=d/4</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p3.15.m7.1d">italic_c = italic_d / 4</annotation></semantics></math>, based on our ablations (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T2" title="In 4.2.3 Effect of Read-Write Head Latent Dimension ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Process-Memory Fusion</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.2">The <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p1.2.1">fusion</span> operation combines the read tokens <math alttext="R^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.1.m1.1"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><msup id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p1.1.m1.1.1.2" xref="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml">R</mi><mi id="S3.SS2.SSS3.p1.1.m1.1.1.3" xref="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><apply id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.2">𝑅</ci><ci id="S3.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">R^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">italic_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math> and write tokens <math alttext="W^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.2.m2.1"><semantics id="S3.SS2.SSS3.p1.2.m2.1a"><msup id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p1.2.m2.1.1.2" xref="S3.SS2.SSS3.p1.2.m2.1.1.2.cmml">W</mi><mi id="S3.SS2.SSS3.p1.2.m2.1.1.3" xref="S3.SS2.SSS3.p1.2.m2.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.1b"><apply id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.2">𝑊</ci><ci id="S3.SS2.SSS3.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.1c">W^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.2.m2.1d">italic_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>, from the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p1.2.2">read-write heads</span> into the process and memory streams, respectively.
We explore three implementations of fusion: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p1.2.3">erase</span>, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p1.2.4">add</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p1.2.5">add-erase</span> fusion (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.F4" title="In 3.2.3 Process-Memory Fusion ‣ 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="147" id="S3.F4.g1" src="extracted/5849051/figures/fusion.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.4.2.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.2.1" style="font-size:90%;">
Illustration of our fusion implementations.
(a) Erase (b) Add (c) Add-Erase. <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.F4.2.1.m1.1"><semantics id="S3.F4.2.1.m1.1b"><mi id="S3.F4.2.1.m1.1.1" xref="S3.F4.2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F4.2.1.m1.1c"><ci id="S3.F4.2.1.m1.1.1.cmml" xref="S3.F4.2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.1.m1.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.F4.2.1.m1.1e">italic_α</annotation></semantics></math> is computed according to <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.E10" title="In 3.2.3 Process-Memory Fusion ‣ 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">10</span></a>.
Depending on the location of the fusion operation the inputs vary (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.F3" title="In 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>).
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.13"><span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p2.13.1">Erase</span> replaces the previous process tokens (<math alttext="P^{l-1}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.1.m1.1"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><msup id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS3.p2.1.m1.1.1.2" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.cmml">P</mi><mrow id="S3.SS2.SSS3.p2.1.m1.1.1.3" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS3.p2.1.m1.1.1.3.2" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.2.cmml">l</mi><mo id="S3.SS2.SSS3.p2.1.m1.1.1.3.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.1.cmml">−</mo><mn id="S3.SS2.SSS3.p2.1.m1.1.1.3.3" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><apply id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.2">𝑃</ci><apply id="S3.SS2.SSS3.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.3"><minus id="S3.SS2.SSS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.1"></minus><ci id="S3.SS2.SSS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.2">𝑙</ci><cn id="S3.SS2.SSS3.p2.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">P^{l-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.1.m1.1d">italic_P start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>) with the read tokens (<math alttext="R^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.2.m2.1"><semantics id="S3.SS2.SSS3.p2.2.m2.1a"><msup id="S3.SS2.SSS3.p2.2.m2.1.1" xref="S3.SS2.SSS3.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS3.p2.2.m2.1.1.2" xref="S3.SS2.SSS3.p2.2.m2.1.1.2.cmml">R</mi><mi id="S3.SS2.SSS3.p2.2.m2.1.1.3" xref="S3.SS2.SSS3.p2.2.m2.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.2.m2.1b"><apply id="S3.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1.2">𝑅</ci><ci id="S3.SS2.SSS3.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.2.m2.1c">R^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.2.m2.1d">italic_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>), i.e. erasing the prior process tokens.
<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p2.13.2">Add</span> sums the read tokens (<math alttext="R^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.3.m3.1"><semantics id="S3.SS2.SSS3.p2.3.m3.1a"><msup id="S3.SS2.SSS3.p2.3.m3.1.1" xref="S3.SS2.SSS3.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS3.p2.3.m3.1.1.2" xref="S3.SS2.SSS3.p2.3.m3.1.1.2.cmml">R</mi><mi id="S3.SS2.SSS3.p2.3.m3.1.1.3" xref="S3.SS2.SSS3.p2.3.m3.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.3.m3.1b"><apply id="S3.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1.2">𝑅</ci><ci id="S3.SS2.SSS3.p2.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.3.m3.1c">R^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.3.m3.1d">italic_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math>) with the process tokens (<math alttext="P^{l-1}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.4.m4.1"><semantics id="S3.SS2.SSS3.p2.4.m4.1a"><msup id="S3.SS2.SSS3.p2.4.m4.1.1" xref="S3.SS2.SSS3.p2.4.m4.1.1.cmml"><mi id="S3.SS2.SSS3.p2.4.m4.1.1.2" xref="S3.SS2.SSS3.p2.4.m4.1.1.2.cmml">P</mi><mrow id="S3.SS2.SSS3.p2.4.m4.1.1.3" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS3.p2.4.m4.1.1.3.2" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.2.cmml">l</mi><mo id="S3.SS2.SSS3.p2.4.m4.1.1.3.1" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.1.cmml">−</mo><mn id="S3.SS2.SSS3.p2.4.m4.1.1.3.3" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.4.m4.1b"><apply id="S3.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.2">𝑃</ci><apply id="S3.SS2.SSS3.p2.4.m4.1.1.3.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.3"><minus id="S3.SS2.SSS3.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.1"></minus><ci id="S3.SS2.SSS3.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.2">𝑙</ci><cn id="S3.SS2.SSS3.p2.4.m4.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS3.p2.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.4.m4.1c">P^{l-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.4.m4.1d">italic_P start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>).
<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p2.13.3">Add-Erase</span> creates the new process tokens <math alttext="P^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.5.m5.1"><semantics id="S3.SS2.SSS3.p2.5.m5.1a"><msup id="S3.SS2.SSS3.p2.5.m5.1.1" xref="S3.SS2.SSS3.p2.5.m5.1.1.cmml"><mi id="S3.SS2.SSS3.p2.5.m5.1.1.2" xref="S3.SS2.SSS3.p2.5.m5.1.1.2.cmml">P</mi><mi id="S3.SS2.SSS3.p2.5.m5.1.1.3" xref="S3.SS2.SSS3.p2.5.m5.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.5.m5.1b"><apply id="S3.SS2.SSS3.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.5.m5.1.1.1.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p2.5.m5.1.1.2.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.2">𝑃</ci><ci id="S3.SS2.SSS3.p2.5.m5.1.1.3.cmml" xref="S3.SS2.SSS3.p2.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.5.m5.1c">P^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.5.m5.1d">italic_P start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math> as a convex combination of the read tokens <math alttext="R^{l}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.6.m6.1"><semantics id="S3.SS2.SSS3.p2.6.m6.1a"><msup id="S3.SS2.SSS3.p2.6.m6.1.1" xref="S3.SS2.SSS3.p2.6.m6.1.1.cmml"><mi id="S3.SS2.SSS3.p2.6.m6.1.1.2" xref="S3.SS2.SSS3.p2.6.m6.1.1.2.cmml">R</mi><mi id="S3.SS2.SSS3.p2.6.m6.1.1.3" xref="S3.SS2.SSS3.p2.6.m6.1.1.3.cmml">l</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.6.m6.1b"><apply id="S3.SS2.SSS3.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.6.m6.1.1.1.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p2.6.m6.1.1.2.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.2">𝑅</ci><ci id="S3.SS2.SSS3.p2.6.m6.1.1.3.cmml" xref="S3.SS2.SSS3.p2.6.m6.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.6.m6.1c">R^{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.6.m6.1d">italic_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT</annotation></semantics></math> and the previous process tokens <math alttext="P^{l-1}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.7.m7.1"><semantics id="S3.SS2.SSS3.p2.7.m7.1a"><msup id="S3.SS2.SSS3.p2.7.m7.1.1" xref="S3.SS2.SSS3.p2.7.m7.1.1.cmml"><mi id="S3.SS2.SSS3.p2.7.m7.1.1.2" xref="S3.SS2.SSS3.p2.7.m7.1.1.2.cmml">P</mi><mrow id="S3.SS2.SSS3.p2.7.m7.1.1.3" xref="S3.SS2.SSS3.p2.7.m7.1.1.3.cmml"><mi id="S3.SS2.SSS3.p2.7.m7.1.1.3.2" xref="S3.SS2.SSS3.p2.7.m7.1.1.3.2.cmml">l</mi><mo id="S3.SS2.SSS3.p2.7.m7.1.1.3.1" xref="S3.SS2.SSS3.p2.7.m7.1.1.3.1.cmml">−</mo><mn id="S3.SS2.SSS3.p2.7.m7.1.1.3.3" xref="S3.SS2.SSS3.p2.7.m7.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.7.m7.1b"><apply id="S3.SS2.SSS3.p2.7.m7.1.1.cmml" xref="S3.SS2.SSS3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.7.m7.1.1.1.cmml" xref="S3.SS2.SSS3.p2.7.m7.1.1">superscript</csymbol><ci id="S3.SS2.SSS3.p2.7.m7.1.1.2.cmml" xref="S3.SS2.SSS3.p2.7.m7.1.1.2">𝑃</ci><apply id="S3.SS2.SSS3.p2.7.m7.1.1.3.cmml" xref="S3.SS2.SSS3.p2.7.m7.1.1.3"><minus id="S3.SS2.SSS3.p2.7.m7.1.1.3.1.cmml" xref="S3.SS2.SSS3.p2.7.m7.1.1.3.1"></minus><ci id="S3.SS2.SSS3.p2.7.m7.1.1.3.2.cmml" xref="S3.SS2.SSS3.p2.7.m7.1.1.3.2">𝑙</ci><cn id="S3.SS2.SSS3.p2.7.m7.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS3.p2.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.7.m7.1c">P^{l-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.7.m7.1d">italic_P start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT</annotation></semantics></math>, using combination weights <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.8.m8.1"><semantics id="S3.SS2.SSS3.p2.8.m8.1a"><mi id="S3.SS2.SSS3.p2.8.m8.1.1" xref="S3.SS2.SSS3.p2.8.m8.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.8.m8.1b"><ci id="S3.SS2.SSS3.p2.8.m8.1.1.cmml" xref="S3.SS2.SSS3.p2.8.m8.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.8.m8.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.8.m8.1d">italic_α</annotation></semantics></math> and <math alttext="1-\alpha" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.9.m9.1"><semantics id="S3.SS2.SSS3.p2.9.m9.1a"><mrow id="S3.SS2.SSS3.p2.9.m9.1.1" xref="S3.SS2.SSS3.p2.9.m9.1.1.cmml"><mn id="S3.SS2.SSS3.p2.9.m9.1.1.2" xref="S3.SS2.SSS3.p2.9.m9.1.1.2.cmml">1</mn><mo id="S3.SS2.SSS3.p2.9.m9.1.1.1" xref="S3.SS2.SSS3.p2.9.m9.1.1.1.cmml">−</mo><mi id="S3.SS2.SSS3.p2.9.m9.1.1.3" xref="S3.SS2.SSS3.p2.9.m9.1.1.3.cmml">α</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.9.m9.1b"><apply id="S3.SS2.SSS3.p2.9.m9.1.1.cmml" xref="S3.SS2.SSS3.p2.9.m9.1.1"><minus id="S3.SS2.SSS3.p2.9.m9.1.1.1.cmml" xref="S3.SS2.SSS3.p2.9.m9.1.1.1"></minus><cn id="S3.SS2.SSS3.p2.9.m9.1.1.2.cmml" type="integer" xref="S3.SS2.SSS3.p2.9.m9.1.1.2">1</cn><ci id="S3.SS2.SSS3.p2.9.m9.1.1.3.cmml" xref="S3.SS2.SSS3.p2.9.m9.1.1.3">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.9.m9.1c">1-\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.9.m9.1d">1 - italic_α</annotation></semantics></math>.
The combination weights <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.10.m10.1"><semantics id="S3.SS2.SSS3.p2.10.m10.1a"><mi id="S3.SS2.SSS3.p2.10.m10.1.1" xref="S3.SS2.SSS3.p2.10.m10.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.10.m10.1b"><ci id="S3.SS2.SSS3.p2.10.m10.1.1.cmml" xref="S3.SS2.SSS3.p2.10.m10.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.10.m10.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.10.m10.1d">italic_α</annotation></semantics></math> are computed according to Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.E10" title="In 3.2.3 Process-Memory Fusion ‣ 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">10</span></a> (bias omitted).
We project the average read tokens, <math alttext="R_{avg.}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.11.m11.1"><semantics id="S3.SS2.SSS3.p2.11.m11.1a"><mrow id="S3.SS2.SSS3.p2.11.m11.1.2" xref="S3.SS2.SSS3.p2.11.m11.1.2.cmml"><msub id="S3.SS2.SSS3.p2.11.m11.1.2.2" xref="S3.SS2.SSS3.p2.11.m11.1.2.2.cmml"><mi id="S3.SS2.SSS3.p2.11.m11.1.2.2.2" xref="S3.SS2.SSS3.p2.11.m11.1.2.2.2.cmml">R</mi><mrow id="S3.SS2.SSS3.p2.11.m11.1.1.1.1" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.2.cmml"><mrow id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.2" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.2.cmml">a</mi><mo id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.1" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.3" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.3.cmml">v</mi><mo id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.1a" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.4" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.4.cmml">g</mi></mrow><mo id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.2" lspace="0em" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.2.cmml">.</mo></mrow></msub><mo id="S3.SS2.SSS3.p2.11.m11.1.2.1" xref="S3.SS2.SSS3.p2.11.m11.1.2.1.cmml">∈</mo><msup id="S3.SS2.SSS3.p2.11.m11.1.2.3" xref="S3.SS2.SSS3.p2.11.m11.1.2.3.cmml"><mi id="S3.SS2.SSS3.p2.11.m11.1.2.3.2" xref="S3.SS2.SSS3.p2.11.m11.1.2.3.2.cmml">ℝ</mi><mi id="S3.SS2.SSS3.p2.11.m11.1.2.3.3" xref="S3.SS2.SSS3.p2.11.m11.1.2.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.11.m11.1b"><apply id="S3.SS2.SSS3.p2.11.m11.1.2.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2"><in id="S3.SS2.SSS3.p2.11.m11.1.2.1.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2.1"></in><apply id="S3.SS2.SSS3.p2.11.m11.1.2.2.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.11.m11.1.2.2.1.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS3.p2.11.m11.1.2.2.2.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2.2.2">𝑅</ci><list id="S3.SS2.SSS3.p2.11.m11.1.1.1.2.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1"><apply id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1"><times id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.2">𝑎</ci><ci id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.3">𝑣</ci><ci id="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.4.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.1.1.1.1.4">𝑔</ci></apply></list></apply><apply id="S3.SS2.SSS3.p2.11.m11.1.2.3.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.11.m11.1.2.3.1.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2.3">superscript</csymbol><ci id="S3.SS2.SSS3.p2.11.m11.1.2.3.2.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2.3.2">ℝ</ci><ci id="S3.SS2.SSS3.p2.11.m11.1.2.3.3.cmml" xref="S3.SS2.SSS3.p2.11.m11.1.2.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.11.m11.1c">R_{avg.}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.11.m11.1d">italic_R start_POSTSUBSCRIPT italic_a italic_v italic_g . end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, using the weight matrix <math alttext="W_{\alpha}\in\mathbb{R}^{K\times d}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.12.m12.1"><semantics id="S3.SS2.SSS3.p2.12.m12.1a"><mrow id="S3.SS2.SSS3.p2.12.m12.1.1" xref="S3.SS2.SSS3.p2.12.m12.1.1.cmml"><msub id="S3.SS2.SSS3.p2.12.m12.1.1.2" xref="S3.SS2.SSS3.p2.12.m12.1.1.2.cmml"><mi id="S3.SS2.SSS3.p2.12.m12.1.1.2.2" xref="S3.SS2.SSS3.p2.12.m12.1.1.2.2.cmml">W</mi><mi id="S3.SS2.SSS3.p2.12.m12.1.1.2.3" xref="S3.SS2.SSS3.p2.12.m12.1.1.2.3.cmml">α</mi></msub><mo id="S3.SS2.SSS3.p2.12.m12.1.1.1" xref="S3.SS2.SSS3.p2.12.m12.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS3.p2.12.m12.1.1.3" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.cmml"><mi id="S3.SS2.SSS3.p2.12.m12.1.1.3.2" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS3.p2.12.m12.1.1.3.3" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.3.cmml"><mi id="S3.SS2.SSS3.p2.12.m12.1.1.3.3.2" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.3.2.cmml">K</mi><mo id="S3.SS2.SSS3.p2.12.m12.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS3.p2.12.m12.1.1.3.3.3" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.12.m12.1b"><apply id="S3.SS2.SSS3.p2.12.m12.1.1.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1"><in id="S3.SS2.SSS3.p2.12.m12.1.1.1.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.1"></in><apply id="S3.SS2.SSS3.p2.12.m12.1.1.2.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.12.m12.1.1.2.1.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS3.p2.12.m12.1.1.2.2.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.2.2">𝑊</ci><ci id="S3.SS2.SSS3.p2.12.m12.1.1.2.3.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.2.3">𝛼</ci></apply><apply id="S3.SS2.SSS3.p2.12.m12.1.1.3.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p2.12.m12.1.1.3.1.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS3.p2.12.m12.1.1.3.2.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS3.p2.12.m12.1.1.3.3.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.3"><times id="S3.SS2.SSS3.p2.12.m12.1.1.3.3.1.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.3.1"></times><ci id="S3.SS2.SSS3.p2.12.m12.1.1.3.3.2.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.3.2">𝐾</ci><ci id="S3.SS2.SSS3.p2.12.m12.1.1.3.3.3.cmml" xref="S3.SS2.SSS3.p2.12.m12.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.12.m12.1c">W_{\alpha}\in\mathbb{R}^{K\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.12.m12.1d">italic_W start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_K × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and apply the sigmoid non-linearity <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.13.m13.1"><semantics id="S3.SS2.SSS3.p2.13.m13.1a"><mi id="S3.SS2.SSS3.p2.13.m13.1.1" xref="S3.SS2.SSS3.p2.13.m13.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.13.m13.1b"><ci id="S3.SS2.SSS3.p2.13.m13.1.1.cmml" xref="S3.SS2.SSS3.p2.13.m13.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.13.m13.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.13.m13.1d">italic_σ</annotation></semantics></math>.</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S6.EGx4">
<tbody id="S3.E10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\alpha" class="ltx_Math" display="inline" id="S3.E10.m1.1"><semantics id="S3.E10.m1.1a"><mi id="S3.E10.m1.1.1" xref="S3.E10.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.E10.m1.1b"><ci id="S3.E10.m1.1.1.cmml" xref="S3.E10.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.1c">\displaystyle\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.E10.m1.1d">italic_α</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sigma(W_{\alpha}R^{l}_{avg.})\in\mathbb{R}^{K}" class="ltx_Math" display="inline" id="S3.E10.m2.2"><semantics id="S3.E10.m2.2a"><mrow id="S3.E10.m2.2.2" xref="S3.E10.m2.2.2.cmml"><mi id="S3.E10.m2.2.2.3" xref="S3.E10.m2.2.2.3.cmml"></mi><mo id="S3.E10.m2.2.2.4" xref="S3.E10.m2.2.2.4.cmml">=</mo><mrow id="S3.E10.m2.2.2.1" xref="S3.E10.m2.2.2.1.cmml"><mi id="S3.E10.m2.2.2.1.3" xref="S3.E10.m2.2.2.1.3.cmml">σ</mi><mo id="S3.E10.m2.2.2.1.2" xref="S3.E10.m2.2.2.1.2.cmml">⁢</mo><mrow id="S3.E10.m2.2.2.1.1.1" xref="S3.E10.m2.2.2.1.1.1.1.cmml"><mo id="S3.E10.m2.2.2.1.1.1.2" stretchy="false" xref="S3.E10.m2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E10.m2.2.2.1.1.1.1" xref="S3.E10.m2.2.2.1.1.1.1.cmml"><msub id="S3.E10.m2.2.2.1.1.1.1.2" xref="S3.E10.m2.2.2.1.1.1.1.2.cmml"><mi id="S3.E10.m2.2.2.1.1.1.1.2.2" xref="S3.E10.m2.2.2.1.1.1.1.2.2.cmml">W</mi><mi id="S3.E10.m2.2.2.1.1.1.1.2.3" xref="S3.E10.m2.2.2.1.1.1.1.2.3.cmml">α</mi></msub><mo id="S3.E10.m2.2.2.1.1.1.1.1" xref="S3.E10.m2.2.2.1.1.1.1.1.cmml">⁢</mo><msubsup id="S3.E10.m2.2.2.1.1.1.1.3" xref="S3.E10.m2.2.2.1.1.1.1.3.cmml"><mi id="S3.E10.m2.2.2.1.1.1.1.3.2.2" xref="S3.E10.m2.2.2.1.1.1.1.3.2.2.cmml">R</mi><mrow id="S3.E10.m2.1.1.1.1" xref="S3.E10.m2.1.1.1.2.cmml"><mrow id="S3.E10.m2.1.1.1.1.1" xref="S3.E10.m2.1.1.1.1.1.cmml"><mi id="S3.E10.m2.1.1.1.1.1.2" xref="S3.E10.m2.1.1.1.1.1.2.cmml">a</mi><mo id="S3.E10.m2.1.1.1.1.1.1" xref="S3.E10.m2.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E10.m2.1.1.1.1.1.3" xref="S3.E10.m2.1.1.1.1.1.3.cmml">v</mi><mo id="S3.E10.m2.1.1.1.1.1.1a" xref="S3.E10.m2.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E10.m2.1.1.1.1.1.4" xref="S3.E10.m2.1.1.1.1.1.4.cmml">g</mi></mrow><mo id="S3.E10.m2.1.1.1.1.2" lspace="0em" xref="S3.E10.m2.1.1.1.2.cmml">.</mo></mrow><mi id="S3.E10.m2.2.2.1.1.1.1.3.2.3" xref="S3.E10.m2.2.2.1.1.1.1.3.2.3.cmml">l</mi></msubsup></mrow><mo id="S3.E10.m2.2.2.1.1.1.3" stretchy="false" xref="S3.E10.m2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E10.m2.2.2.5" xref="S3.E10.m2.2.2.5.cmml">∈</mo><msup id="S3.E10.m2.2.2.6" xref="S3.E10.m2.2.2.6.cmml"><mi id="S3.E10.m2.2.2.6.2" xref="S3.E10.m2.2.2.6.2.cmml">ℝ</mi><mi id="S3.E10.m2.2.2.6.3" xref="S3.E10.m2.2.2.6.3.cmml">K</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m2.2b"><apply id="S3.E10.m2.2.2.cmml" xref="S3.E10.m2.2.2"><and id="S3.E10.m2.2.2a.cmml" xref="S3.E10.m2.2.2"></and><apply id="S3.E10.m2.2.2b.cmml" xref="S3.E10.m2.2.2"><eq id="S3.E10.m2.2.2.4.cmml" xref="S3.E10.m2.2.2.4"></eq><csymbol cd="latexml" id="S3.E10.m2.2.2.3.cmml" xref="S3.E10.m2.2.2.3">absent</csymbol><apply id="S3.E10.m2.2.2.1.cmml" xref="S3.E10.m2.2.2.1"><times id="S3.E10.m2.2.2.1.2.cmml" xref="S3.E10.m2.2.2.1.2"></times><ci id="S3.E10.m2.2.2.1.3.cmml" xref="S3.E10.m2.2.2.1.3">𝜎</ci><apply id="S3.E10.m2.2.2.1.1.1.1.cmml" xref="S3.E10.m2.2.2.1.1.1"><times id="S3.E10.m2.2.2.1.1.1.1.1.cmml" xref="S3.E10.m2.2.2.1.1.1.1.1"></times><apply id="S3.E10.m2.2.2.1.1.1.1.2.cmml" xref="S3.E10.m2.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E10.m2.2.2.1.1.1.1.2.1.cmml" xref="S3.E10.m2.2.2.1.1.1.1.2">subscript</csymbol><ci id="S3.E10.m2.2.2.1.1.1.1.2.2.cmml" xref="S3.E10.m2.2.2.1.1.1.1.2.2">𝑊</ci><ci id="S3.E10.m2.2.2.1.1.1.1.2.3.cmml" xref="S3.E10.m2.2.2.1.1.1.1.2.3">𝛼</ci></apply><apply id="S3.E10.m2.2.2.1.1.1.1.3.cmml" xref="S3.E10.m2.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m2.2.2.1.1.1.1.3.1.cmml" xref="S3.E10.m2.2.2.1.1.1.1.3">subscript</csymbol><apply id="S3.E10.m2.2.2.1.1.1.1.3.2.cmml" xref="S3.E10.m2.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E10.m2.2.2.1.1.1.1.3.2.1.cmml" xref="S3.E10.m2.2.2.1.1.1.1.3">superscript</csymbol><ci id="S3.E10.m2.2.2.1.1.1.1.3.2.2.cmml" xref="S3.E10.m2.2.2.1.1.1.1.3.2.2">𝑅</ci><ci id="S3.E10.m2.2.2.1.1.1.1.3.2.3.cmml" xref="S3.E10.m2.2.2.1.1.1.1.3.2.3">𝑙</ci></apply><list id="S3.E10.m2.1.1.1.2.cmml" xref="S3.E10.m2.1.1.1.1"><apply id="S3.E10.m2.1.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1.1"><times id="S3.E10.m2.1.1.1.1.1.1.cmml" xref="S3.E10.m2.1.1.1.1.1.1"></times><ci id="S3.E10.m2.1.1.1.1.1.2.cmml" xref="S3.E10.m2.1.1.1.1.1.2">𝑎</ci><ci id="S3.E10.m2.1.1.1.1.1.3.cmml" xref="S3.E10.m2.1.1.1.1.1.3">𝑣</ci><ci id="S3.E10.m2.1.1.1.1.1.4.cmml" xref="S3.E10.m2.1.1.1.1.1.4">𝑔</ci></apply></list></apply></apply></apply></apply><apply id="S3.E10.m2.2.2c.cmml" xref="S3.E10.m2.2.2"><in id="S3.E10.m2.2.2.5.cmml" xref="S3.E10.m2.2.2.5"></in><share href="https://arxiv.org/html/2409.07613v1#S3.E10.m2.2.2.1.cmml" id="S3.E10.m2.2.2d.cmml" xref="S3.E10.m2.2.2"></share><apply id="S3.E10.m2.2.2.6.cmml" xref="S3.E10.m2.2.2.6"><csymbol cd="ambiguous" id="S3.E10.m2.2.2.6.1.cmml" xref="S3.E10.m2.2.2.6">superscript</csymbol><ci id="S3.E10.m2.2.2.6.2.cmml" xref="S3.E10.m2.2.2.6.2">ℝ</ci><ci id="S3.E10.m2.2.2.6.3.cmml" xref="S3.E10.m2.2.2.6.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m2.2c">\displaystyle=\sigma(W_{\alpha}R^{l}_{avg.})\in\mathbb{R}^{K}</annotation><annotation encoding="application/x-llamapun" id="S3.E10.m2.2d">= italic_σ ( italic_W start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a italic_v italic_g . end_POSTSUBSCRIPT ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">In our evaluation we adopt <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p3.1.1">Add</span> fusion because this has the best performance with <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p3.1.2">Linear Attention</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T4" title="In 4.2.4 Effect of Memory and Process Token Number ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Processing in the Memory Stream</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">Whether the memory tokens should undergo processing is a key consideration.
Processing allows for the further enrichment of memory tokens, while trading off efficiency.
We explore the effect of adding a multi-layer preceptron (MLP) in the memory stream with varying bottleneck dimensions in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T4" title="In 4.2.4 Effect of Memory and Process Token Number ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
Unlike LookupViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib20" title="">20</a>]</cite>,
the benefit of an MLP is marginal and we do not process the memory stream explicitly.
We find that not refining the memory stream can make training unstable — normalization helps, but increasing batch-size is more effective.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate ViTTM design decisions and compare with existing work.
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS1" title="4.1 Experimental Setup ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> describes our experimental setup, and <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2" title="4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> explores design decisions of ViTTM’s.
We compare with other methods on classification and segmentation in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS3" title="4.3 Comparison with other work ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Training:</span> Training was conducted on high-performance computing systems equipped with up to 40 NVIDIA A100 GPUs (40GB).
We follow ViT training recipes laid out in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib33" title="">33</a>]</cite>.
Specifically, we pre-train our models on ImageNet-21K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib29" title="">29</a>]</cite>, and subsequently fine-tune them on ImageNet-1K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib6" title="">6</a>]</cite>.
For semantic segmentation, we initialize our backbone models with the fine-tuned ImageNet-1K weights.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Measurement:</span> FLOP measurements were performed with the <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">fvcore</span> library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib10" title="">10</a>]</cite>, and latency was measured using the PyTorch <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.3">benchmark</span> module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib25" title="">25</a>]</cite> with a batch size of 256.
Segmentation experiments and benchmarking were performed using the MMSegmentation library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib5" title="">5</a>]</cite>.
Benchmark and evaluations were performed on both a single 80GB A100 GPU and a 24GB A30 GPU.
For reproducibility, our code is open source.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.2.1">Notation:</span> We denote our ViTTM configurations as follows: ViTTM-B<sub class="ltx_sub" id="S4.SS1.p3.2.2"><span class="ltx_text ltx_font_italic" id="S4.SS1.p3.2.2.1">(49,64)</span></sub> is a “ViTTM-B model with 49 process tokens and 64 memory tokens”.
For models <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.2.3">without</span> memory tokens (<em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.2.4">i.e</em>.<span class="ltx_text" id="S4.SS1.p3.2.5"></span> baseline models), we use <math alttext="\phi" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">italic_ϕ</annotation></semantics></math> to denote the absence of memory tokens.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ablations</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In this section, we perform ablation studies on the four core design decisions of ViTTMs (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2" title="3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>).
We conduct ablations to answer the following:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">How to initialize memory and process tokens? (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS1" title="4.2.1 Memory and Process Initialization ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS4" title="4.2.4 Effect of Memory and Process Token Number ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.4</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;padding-top:1.5pt;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">How tokens will be selected for read/write? (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS2" title="4.2.2 Read-Write Head and Fusion Choice ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS3" title="4.2.3 Effect of Read-Write Head Latent Dimension ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.3</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;padding-top:1.5pt;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">How will the selected tokens be fused into the process/memory tokens? (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS2" title="4.2.2 Read-Write Head and Fusion Choice ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.2</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;padding-top:1.5pt;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Should memory tokens undergo processing? (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS5" title="4.2.5 Effect of Memory Stream Non-linearity ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.5</span></a>)
</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S4.SS2.p1.2">To assess the efficacy of each design choice, we fine-tune and evaluate on ImageNet-1K.
We allocate a fixed amount training time, 100 epochs, for each ablation.
The ablations in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS1" title="4.2.1 Memory and Process Initialization ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS2" title="4.2.2 Read-Write Head and Fusion Choice ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.2</span></a> are conducted using a ViTTM-S model, whereas those in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS3" title="4.2.3 Effect of Read-Write Head Latent Dimension ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS4" title="4.2.4 Effect of Memory and Process Token Number ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.4</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2.SSS5" title="4.2.5 Effect of Memory Stream Non-linearity ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2.5</span></a> ablations use a ViTTM-B model.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Memory and Process Initialization</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We evaluate the initialization method for process tokens as defined in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS1" title="3.2.1 Initializing Process and Memory Tokens ‣ 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
Ultimately, the choice of initialization scheme has no impact on accuracy.
Therefore, we opted for <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p1.1.1">Patch</span> initalization for all of our models since it is the standard method for ViT architectures.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Read-Write Head and Fusion Choice</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">We explore combinations of read-write heads and fusion mechanisms outlined in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2" title="3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>.
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T1" title="In 4.2.2 Read-Write Head and Fusion Choice ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> displays the ablation results for read-write head and fusion mechanisms.
We observe that all read-write heads provide comprable accuracy, primarily differing in computational cost.
As stated in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS2" title="3.2.2 Read-Write Heads ‣ 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>, we use Linear Attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib18" title="">18</a>]</cite> due to its efficiency and comparable accuracy to accuracy to Cross Attention.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T1.2.1.1.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.1.1">Fusion</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S4.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.1.2.1">Read-Write Head</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2">
<td class="ltx_td ltx_align_right" id="S4.T1.2.2.2.1">Token</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.2.2.2">Cross</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.2.2.3">Latent</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.2.2.4">Linear</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3.3">
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.1">Summary</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.2">Attn.</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.3">Attn.</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.4">Attn.</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T1.2.4.4.1">Erase</th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.2.4.4.2">72.49</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.2.4.4.3">3.94</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.2.4.4.4">0.33</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.2.4.4.5">2.43</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.5.5.1">Add</th>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.2">75.86</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.3">76.26</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.5.4.1">76.17</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.5.5.1">76.26</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.2.6.6.1">Add-Erase</th>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.6.6.2.1">76.26</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.6.6.3.1">76.33</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.4">75.82</td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.5">74.74</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T1.2.7.7.1">GFLOPs</th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.2.7.7.2">1.86</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.2.7.7.3">2.92</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.2.7.7.4">3.07</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.2.7.7.5">2.62</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.7.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.8.2" style="font-size:90%;">
Ablation over read-write head and fusion mechanisms.
Each cell represents the Top-1 ImageNet1K accuracy for a read-write head and fusion mechanism combination.
The GFLOPs are measured for each read-write head using <span class="ltx_text ltx_font_italic" id="S4.T1.8.2.1">Add</span> fusion (fusion contributions to GFLOPs negligible).
TokenSummary has the lowest computational cost, followed by Linear Attention, Cross Attention, and finally Latent Attention.
<span class="ltx_text ltx_font_italic" id="S4.T1.8.2.2">Add</span> fusion works best, whereas <span class="ltx_text ltx_font_italic" id="S4.T1.8.2.3">Add-Erase</span> fusion degrades the performance of Latent and Linear Attention.
<span class="ltx_text ltx_font_italic" id="S4.T1.8.2.4">Erase</span> fusion performs the worst for the attention-based heads, and best when used with Token Summary.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Effect of Read-Write Head Latent Dimension</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.4"><a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T2" title="In 4.2.3 Effect of Read-Write Head Latent Dimension ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a> illustrates the impact of varying the latent embedding dimension <math alttext="c" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.1.m1.1"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><mi id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><ci id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">c</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.1.m1.1d">italic_c</annotation></semantics></math>.
Reducing the latent embedding dimension relative to the embedding dimension, <math alttext="d" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.2.m2.1"><semantics id="S4.SS2.SSS3.p1.2.m2.1a"><mi id="S4.SS2.SSS3.p1.2.m2.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.2.m2.1b"><ci id="S4.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.2.m2.1d">italic_d</annotation></semantics></math>, does not reduce the accuracy but can give substantial latency benefits.
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T2" title="In 4.2.3 Effect of Read-Write Head Latent Dimension ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, setting <math alttext="c=d/4" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.3.m3.1"><semantics id="S4.SS2.SSS3.p1.3.m3.1a"><mrow id="S4.SS2.SSS3.p1.3.m3.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS3.p1.3.m3.1.1.2" xref="S4.SS2.SSS3.p1.3.m3.1.1.2.cmml">c</mi><mo id="S4.SS2.SSS3.p1.3.m3.1.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.1.cmml">=</mo><mrow id="S4.SS2.SSS3.p1.3.m3.1.1.3" xref="S4.SS2.SSS3.p1.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS3.p1.3.m3.1.1.3.2" xref="S4.SS2.SSS3.p1.3.m3.1.1.3.2.cmml">d</mi><mo id="S4.SS2.SSS3.p1.3.m3.1.1.3.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.3.1.cmml">/</mo><mn id="S4.SS2.SSS3.p1.3.m3.1.1.3.3" xref="S4.SS2.SSS3.p1.3.m3.1.1.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.3.m3.1b"><apply id="S4.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1"><eq id="S4.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.2">𝑐</ci><apply id="S4.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.3"><divide id="S4.SS2.SSS3.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.3.1"></divide><ci id="S4.SS2.SSS3.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1.3.2">𝑑</ci><cn id="S4.SS2.SSS3.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S4.SS2.SSS3.p1.3.m3.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.3.m3.1c">c=d/4</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.3.m3.1d">italic_c = italic_d / 4</annotation></semantics></math> reduces latency by 100ms with negligible impact on Top-1 accuracy.
Given this result, we choose <math alttext="c=d/4" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p1.4.m4.1"><semantics id="S4.SS2.SSS3.p1.4.m4.1a"><mrow id="S4.SS2.SSS3.p1.4.m4.1.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.cmml"><mi id="S4.SS2.SSS3.p1.4.m4.1.1.2" xref="S4.SS2.SSS3.p1.4.m4.1.1.2.cmml">c</mi><mo id="S4.SS2.SSS3.p1.4.m4.1.1.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S4.SS2.SSS3.p1.4.m4.1.1.3" xref="S4.SS2.SSS3.p1.4.m4.1.1.3.cmml"><mi id="S4.SS2.SSS3.p1.4.m4.1.1.3.2" xref="S4.SS2.SSS3.p1.4.m4.1.1.3.2.cmml">d</mi><mo id="S4.SS2.SSS3.p1.4.m4.1.1.3.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.3.1.cmml">/</mo><mn id="S4.SS2.SSS3.p1.4.m4.1.1.3.3" xref="S4.SS2.SSS3.p1.4.m4.1.1.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.4.m4.1b"><apply id="S4.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1"><eq id="S4.SS2.SSS3.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1.1"></eq><ci id="S4.SS2.SSS3.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1.2">𝑐</ci><apply id="S4.SS2.SSS3.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1.3"><divide id="S4.SS2.SSS3.p1.4.m4.1.1.3.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1.3.1"></divide><ci id="S4.SS2.SSS3.p1.4.m4.1.1.3.2.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1.3.2">𝑑</ci><cn id="S4.SS2.SSS3.p1.4.m4.1.1.3.3.cmml" type="integer" xref="S4.SS2.SSS3.p1.4.m4.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.4.m4.1c">c=d/4</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p1.4.m4.1d">italic_c = italic_d / 4</annotation></semantics></math> for all evaluations in this work.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="c" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">c</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">italic_c</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.1">GFLOPs</span> (M)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.1">Latency</span> (ms)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.m1.1a"><mo id="S4.T2.3.3.3.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T2.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.1">Top-1</span> (%)<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.m1.1"><semantics id="S4.T2.4.4.4.m1.1a"><mo id="S4.T2.4.4.4.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T2.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T2.5.5.1" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="d/4" class="ltx_Math" display="inline" id="S4.T2.5.5.1.m1.1"><semantics id="S4.T2.5.5.1.m1.1a"><mrow id="S4.T2.5.5.1.m1.1.1" xref="S4.T2.5.5.1.m1.1.1.cmml"><mi id="S4.T2.5.5.1.m1.1.1.2" xref="S4.T2.5.5.1.m1.1.1.2.cmml">d</mi><mo id="S4.T2.5.5.1.m1.1.1.1" xref="S4.T2.5.5.1.m1.1.1.1.cmml">/</mo><mn id="S4.T2.5.5.1.m1.1.1.3" xref="S4.T2.5.5.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.m1.1b"><apply id="S4.T2.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1"><divide id="S4.T2.5.5.1.m1.1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1.1"></divide><ci id="S4.T2.5.5.1.m1.1.1.2.cmml" xref="S4.T2.5.5.1.m1.1.1.2">𝑑</ci><cn id="S4.T2.5.5.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.5.5.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.1.m1.1c">d/4</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.1.m1.1d">italic_d / 4</annotation></semantics></math></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.5.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">7.10</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.5.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">250.7</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.5.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">78.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.6.6.1" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="d/2" class="ltx_Math" display="inline" id="S4.T2.6.6.1.m1.1"><semantics id="S4.T2.6.6.1.m1.1a"><mrow id="S4.T2.6.6.1.m1.1.1" xref="S4.T2.6.6.1.m1.1.1.cmml"><mi id="S4.T2.6.6.1.m1.1.1.2" xref="S4.T2.6.6.1.m1.1.1.2.cmml">d</mi><mo id="S4.T2.6.6.1.m1.1.1.1" xref="S4.T2.6.6.1.m1.1.1.1.cmml">/</mo><mn id="S4.T2.6.6.1.m1.1.1.3" xref="S4.T2.6.6.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.1.m1.1b"><apply id="S4.T2.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.1.m1.1.1"><divide id="S4.T2.6.6.1.m1.1.1.1.cmml" xref="S4.T2.6.6.1.m1.1.1.1"></divide><ci id="S4.T2.6.6.1.m1.1.1.2.cmml" xref="S4.T2.6.6.1.m1.1.1.2">𝑑</ci><cn id="S4.T2.6.6.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.6.6.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.1.m1.1c">d/2</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.1.m1.1d">italic_d / 2</annotation></semantics></math></th>
<td class="ltx_td ltx_align_right" id="S4.T2.6.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">8.04</td>
<td class="ltx_td ltx_align_right" id="S4.T2.6.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">288.9</td>
<td class="ltx_td ltx_align_right" id="S4.T2.6.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">78.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T2.7.7.1" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="d" class="ltx_Math" display="inline" id="S4.T2.7.7.1.m1.1"><semantics id="S4.T2.7.7.1.m1.1a"><mi id="S4.T2.7.7.1.m1.1.1" xref="S4.T2.7.7.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.1.m1.1b"><ci id="S4.T2.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.1.m1.1d">italic_d</annotation></semantics></math></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.7.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">9.92</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.7.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">352.2</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.7.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">78.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.19.5.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.15.4" style="font-size:90%;">
Ablation over the <span class="ltx_text ltx_font_italic" id="S4.T2.15.4.1">latent embedding dimension</span>, <math alttext="c" class="ltx_Math" display="inline" id="S4.T2.12.1.m1.1"><semantics id="S4.T2.12.1.m1.1b"><mi id="S4.T2.12.1.m1.1.1" xref="S4.T2.12.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.T2.12.1.m1.1c"><ci id="S4.T2.12.1.m1.1.1.cmml" xref="S4.T2.12.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.1.m1.1d">c</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.1.m1.1e">italic_c</annotation></semantics></math> for the read-write head on ViTTM-B (<math alttext="d=768" class="ltx_Math" display="inline" id="S4.T2.13.2.m2.1"><semantics id="S4.T2.13.2.m2.1b"><mrow id="S4.T2.13.2.m2.1.1" xref="S4.T2.13.2.m2.1.1.cmml"><mi id="S4.T2.13.2.m2.1.1.2" xref="S4.T2.13.2.m2.1.1.2.cmml">d</mi><mo id="S4.T2.13.2.m2.1.1.1" xref="S4.T2.13.2.m2.1.1.1.cmml">=</mo><mn id="S4.T2.13.2.m2.1.1.3" xref="S4.T2.13.2.m2.1.1.3.cmml">768</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.13.2.m2.1c"><apply id="S4.T2.13.2.m2.1.1.cmml" xref="S4.T2.13.2.m2.1.1"><eq id="S4.T2.13.2.m2.1.1.1.cmml" xref="S4.T2.13.2.m2.1.1.1"></eq><ci id="S4.T2.13.2.m2.1.1.2.cmml" xref="S4.T2.13.2.m2.1.1.2">𝑑</ci><cn id="S4.T2.13.2.m2.1.1.3.cmml" type="integer" xref="S4.T2.13.2.m2.1.1.3">768</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.2.m2.1d">d=768</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.2.m2.1e">italic_d = 768</annotation></semantics></math>).
Accuracy is computed using the memory tokens rather than the process tokens.
We use <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.14.3.m3.1"><semantics id="S4.T2.14.3.m3.1b"><mo id="S4.T2.14.3.m3.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T2.14.3.m3.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.3.m3.1c"><ci id="S4.T2.14.3.m3.1.1.cmml" xref="S4.T2.14.3.m3.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.3.m3.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.3.m3.1e">↑</annotation></semantics></math>/<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.15.4.m4.1"><semantics id="S4.T2.15.4.m4.1b"><mo id="S4.T2.15.4.m4.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T2.15.4.m4.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.15.4.m4.1c"><ci id="S4.T2.15.4.m4.1.1.cmml" xref="S4.T2.15.4.m4.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.4.m4.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.15.4.m4.1e">↓</annotation></semantics></math> notation to indicate <span class="ltx_text ltx_font_italic" id="S4.T2.15.4.2">“larger/smaller is better”</span>.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Effect of Memory and Process Token Number</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T3" title="In 4.2.4 Effect of Memory and Process Token Number ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we present how the number of memory and process tokens affects both the computational cost and accuracy of ViTTM-B.
We find: (1) <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p1.1.1">memory tokens improve accuracy</span>, (2) <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p1.1.2">the marginal accuracy increase of additional memory tokens is small</span>, and (3) <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p1.1.3">the marginal accuracy increase of additional process tokens is large</span>.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="S4.T3.3.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.4.1">Process</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="S4.T3.3.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.5.1">Memory</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1">GFLOPs<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.1">Latency</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T3.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.1">Top-1</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.m1.1d">↑</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.6.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.6.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.6.1.1.1">Tokens</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.6.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.6.1.2.1">Tokens</span></th>
<td class="ltx_td" id="S4.T3.5.6.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.6.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">(ms)</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.6.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">(%)</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="S4.T3.4.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">49</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="S4.T3.4.4.1" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="\phi" class="ltx_Math" display="inline" id="S4.T3.4.4.1.m1.1"><semantics id="S4.T3.4.4.1.m1.1a"><mi id="S4.T3.4.4.1.m1.1.1" xref="S4.T3.4.4.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.1.m1.1b"><ci id="S4.T3.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.1.m1.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.1.m1.1d">italic_ϕ</annotation></semantics></math></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">4.37</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">138.3</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T3.4.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">72.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">196</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.5.1" style="padding-left:4.0pt;padding-right:4.0pt;"><math alttext="\phi" class="ltx_Math" display="inline" id="S4.T3.5.5.1.m1.1"><semantics id="S4.T3.5.5.1.m1.1a"><mi id="S4.T3.5.5.1.m1.1.1" xref="S4.T3.5.5.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.1.m1.1b"><ci id="S4.T3.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.1.m1.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.1.m1.1d">italic_ϕ</annotation></semantics></math></th>
<td class="ltx_td ltx_align_right" id="S4.T3.5.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">16.87</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">527.4</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">81.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.7.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.7.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">16</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.7.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">64</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.7.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">3.39</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.7.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">125.0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.7.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">70.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.8.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.8.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">16</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.8.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">196</th>
<td class="ltx_td ltx_align_right" id="S4.T3.5.8.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">6.37</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.8.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">248.0</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.8.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">70.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.9.4">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.9.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">16</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.9.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">256</th>
<td class="ltx_td ltx_align_right" id="S4.T3.5.9.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">7.72</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.9.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">298.9</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.9.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">71.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.10.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.10.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">49</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.10.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">64</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.10.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">6.94</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.10.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">233.9</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.10.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">79.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.11.6">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.11.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">49</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.11.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">196</th>
<td class="ltx_td ltx_align_right" id="S4.T3.5.11.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">9.92</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.11.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">357.7</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.11.6.5" style="padding-left:4.0pt;padding-right:4.0pt;">79.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.12.7">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.12.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">49</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.12.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">256</th>
<td class="ltx_td ltx_align_right" id="S4.T3.5.12.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">11.27</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.12.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">408.7</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.12.7.5" style="padding-left:4.0pt;padding-right:4.0pt;">79.8</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.13.8">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.13.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">64</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T3.5.13.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">64</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.13.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">8.56</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.13.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">285.3</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.5.13.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">80.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.14.9">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.14.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">64</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T3.5.14.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">196</th>
<td class="ltx_td ltx_align_right" id="S4.T3.5.14.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">11.53</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.14.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">409.0</td>
<td class="ltx_td ltx_align_right" id="S4.T3.5.14.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">80.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.15.10">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S4.T3.5.15.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">64</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S4.T3.5.15.10.2" style="padding-left:4.0pt;padding-right:4.0pt;">256</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.5.15.10.3" style="padding-left:4.0pt;padding-right:4.0pt;">12.88</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.5.15.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">460.0</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.5.15.10.5" style="padding-left:4.0pt;padding-right:4.0pt;">80.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.20.4.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.11.3" style="font-size:90%;">
Ablation of number of tokens for the process and memory streams on ViTTM-B; we use the <math alttext="\phi" class="ltx_Math" display="inline" id="S4.T3.9.1.m1.1"><semantics id="S4.T3.9.1.m1.1b"><mi id="S4.T3.9.1.m1.1.1" xref="S4.T3.9.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.9.1.m1.1c"><ci id="S4.T3.9.1.m1.1.1.cmml" xref="S4.T3.9.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.1.m1.1d">\phi</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.1.m1.1e">italic_ϕ</annotation></semantics></math> to indicate models <span class="ltx_text ltx_font_italic" id="S4.T3.11.3.1">without</span> memory tokens <em class="ltx_emph ltx_font_italic" id="S4.T3.11.3.2">i.e</em>.<span class="ltx_text" id="S4.T3.11.3.3"></span> baseline models.
Although memory tokens improve accuracy, the marginal improvement is small, <em class="ltx_emph ltx_font_italic" id="S4.T3.11.3.4">e.g</em>.<span class="ltx_text" id="S4.T3.11.3.5"></span>, 64 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T3.10.2.m2.1"><semantics id="S4.T3.10.2.m2.1b"><mo id="S4.T3.10.2.m2.1.1" stretchy="false" xref="S4.T3.10.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.10.2.m2.1c"><ci id="S4.T3.10.2.m2.1.1.cmml" xref="S4.T3.10.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.2.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.2.m2.1e">→</annotation></semantics></math> 256 memory tokens.
Increasing the number of process tokens has the greatest impact on accuracy, <em class="ltx_emph ltx_font_italic" id="S4.T3.11.3.6">e.g</em>.<span class="ltx_text" id="S4.T3.11.3.7"></span>, 49 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T3.11.3.m3.1"><semantics id="S4.T3.11.3.m3.1b"><mo id="S4.T3.11.3.m3.1.1" stretchy="false" xref="S4.T3.11.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T3.11.3.m3.1c"><ci id="S4.T3.11.3.m3.1.1.cmml" xref="S4.T3.11.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.3.m3.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.3.m3.1e">→</annotation></semantics></math> 64 process tokens.
</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS4.p2">
<p class="ltx_p" id="S4.SS2.SSS4.p2.7">First, it is clear that adding memory tokens improves accuracy.
When comparing ViT-B<sub class="ltx_sub" id="S4.SS2.SSS4.p2.7.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p2.7.1.1">(49,ϕ)</span></sub> to ViTTM-B<sub class="ltx_sub" id="S4.SS2.SSS4.p2.7.2"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p2.7.2.1">(49,64)</span></sub>, there is a 7.2% improvement in Top-1 accuracy.
Second, the marginal benefit of extra memory tokens is minimal.
This is evident when we compare ViTTM-B<sub class="ltx_sub" id="S4.SS2.SSS4.p2.7.3"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p2.7.3.1">(49,64)</span></sub>, ViTTM-B<sub class="ltx_sub" id="S4.SS2.SSS4.p2.7.4"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p2.7.4.1">(49,196)</span></sub>, and ViTTM-B<sub class="ltx_sub" id="S4.SS2.SSS4.p2.7.5"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p2.7.5.1">(49,256)</span></sub>, where adding extra memory tokens yield minimal accuracy gains.
Third, the marginal benefit of extra process tokens is high.
As seen when comparing ViTTM-B<sub class="ltx_sub" id="S4.SS2.SSS4.p2.7.6"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p2.7.6.1">(49,64)</span></sub> to ViTTM-B<sub class="ltx_sub" id="S4.SS2.SSS4.p2.7.7"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS4.p2.7.7.1">(64,64)</span></sub>.
The addition of process tokens, while keeping memory tokens constant, increases accuracy by 1.2%.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.3.3.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.4.1" style="font-size:90%;">Mem. Stream</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T4.3.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.5.1" style="font-size:90%;">Params</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1" style="font-size:90%;">GFLOPs</span><span class="ltx_text" id="S4.T4.1.1.1.2" style="font-size:90%;"> </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.m1.1.1" mathcolor="#0000FF" mathsize="90%" stretchy="false" xref="S4.T4.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1" style="font-size:90%;">Latency</span><span class="ltx_text" id="S4.T4.2.2.2.2" style="font-size:90%;"> </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.m1.1.1" mathcolor="#0000FF" mathsize="90%" stretchy="false" xref="S4.T4.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T4.3.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.3.3.3.1" style="font-size:90%;">Top-1</span><span class="ltx_text" id="S4.T4.3.3.3.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.m1.1"><semantics id="S4.T4.3.3.3.m1.1a"><mo id="S4.T4.3.3.3.m1.1.1" mathcolor="#0000FF" mathsize="90%" stretchy="false" xref="S4.T4.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
<tr class="ltx_tr" id="S4.T4.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T4.6.7.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.7.1.1.1" style="font-size:90%;">Block</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T4.6.7.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.7.1.2.1" style="font-size:90%;">(M)</span></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T4.6.7.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T4.6.7.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.7.1.4.1" style="font-size:90%;">(ms)</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S4.T4.6.7.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.7.1.5.1" style="font-size:90%;">(%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.6.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T4.6.8.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.8.1.1.1" style="font-size:90%;">None</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.6.8.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.8.1.2.1" style="font-size:90%;">37.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.6.8.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.8.1.3.1" style="font-size:90%;">9.92</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.6.8.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.8.1.4.1" style="font-size:90%;">358.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T4.6.8.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.8.1.5.1" style="font-size:90%;">78.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.4.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S4.T4.4.4.1.1" style="font-size:90%;">MLP (</span><math alttext="r=0.5" class="ltx_Math" display="inline" id="S4.T4.4.4.1.m1.1"><semantics id="S4.T4.4.4.1.m1.1a"><mrow id="S4.T4.4.4.1.m1.1.1" xref="S4.T4.4.4.1.m1.1.1.cmml"><mi id="S4.T4.4.4.1.m1.1.1.2" mathsize="90%" xref="S4.T4.4.4.1.m1.1.1.2.cmml">r</mi><mo id="S4.T4.4.4.1.m1.1.1.1" mathsize="90%" xref="S4.T4.4.4.1.m1.1.1.1.cmml">=</mo><mn id="S4.T4.4.4.1.m1.1.1.3" mathsize="90%" xref="S4.T4.4.4.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.1.m1.1b"><apply id="S4.T4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1"><eq id="S4.T4.4.4.1.m1.1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1.1"></eq><ci id="S4.T4.4.4.1.m1.1.1.2.cmml" xref="S4.T4.4.4.1.m1.1.1.2">𝑟</ci><cn id="S4.T4.4.4.1.m1.1.1.3.cmml" type="float" xref="S4.T4.4.4.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.1.m1.1c">r=0.5</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.1.m1.1d">italic_r = 0.5</annotation></semantics></math><span class="ltx_text" id="S4.T4.4.4.1.2" style="font-size:90%;">)</span>
</th>
<td class="ltx_td ltx_align_right" id="S4.T4.4.4.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.4.4.2.1" style="font-size:90%;">39.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.4.4.3.1" style="font-size:90%;">11.31</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.4.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.4.4.4.1" style="font-size:90%;">403.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.4.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.4.4.5.1" style="font-size:90%;">78.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.5.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S4.T4.5.5.1.1" style="font-size:90%;">MLP (</span><math alttext="r=1.0" class="ltx_Math" display="inline" id="S4.T4.5.5.1.m1.1"><semantics id="S4.T4.5.5.1.m1.1a"><mrow id="S4.T4.5.5.1.m1.1.1" xref="S4.T4.5.5.1.m1.1.1.cmml"><mi id="S4.T4.5.5.1.m1.1.1.2" mathsize="90%" xref="S4.T4.5.5.1.m1.1.1.2.cmml">r</mi><mo id="S4.T4.5.5.1.m1.1.1.1" mathsize="90%" xref="S4.T4.5.5.1.m1.1.1.1.cmml">=</mo><mn id="S4.T4.5.5.1.m1.1.1.3" mathsize="90%" xref="S4.T4.5.5.1.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.1.m1.1b"><apply id="S4.T4.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.1.m1.1.1"><eq id="S4.T4.5.5.1.m1.1.1.1.cmml" xref="S4.T4.5.5.1.m1.1.1.1"></eq><ci id="S4.T4.5.5.1.m1.1.1.2.cmml" xref="S4.T4.5.5.1.m1.1.1.2">𝑟</ci><cn id="S4.T4.5.5.1.m1.1.1.3.cmml" type="float" xref="S4.T4.5.5.1.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.1.m1.1c">r=1.0</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.1.m1.1d">italic_r = 1.0</annotation></semantics></math><span class="ltx_text" id="S4.T4.5.5.1.2" style="font-size:90%;">)</span>
</th>
<td class="ltx_td ltx_align_right" id="S4.T4.5.5.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.5.5.2.1" style="font-size:90%;">41.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.5.5.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.5.5.3.1" style="font-size:90%;">12.70</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.5.5.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.5.5.4.1" style="font-size:90%;">444.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T4.5.5.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.5.5.5.1" style="font-size:90%;">78.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.6.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text" id="S4.T4.6.6.1.1" style="font-size:90%;">MLP (</span><math alttext="r=2.0" class="ltx_Math" display="inline" id="S4.T4.6.6.1.m1.1"><semantics id="S4.T4.6.6.1.m1.1a"><mrow id="S4.T4.6.6.1.m1.1.1" xref="S4.T4.6.6.1.m1.1.1.cmml"><mi id="S4.T4.6.6.1.m1.1.1.2" mathsize="90%" xref="S4.T4.6.6.1.m1.1.1.2.cmml">r</mi><mo id="S4.T4.6.6.1.m1.1.1.1" mathsize="90%" xref="S4.T4.6.6.1.m1.1.1.1.cmml">=</mo><mn id="S4.T4.6.6.1.m1.1.1.3" mathsize="90%" xref="S4.T4.6.6.1.m1.1.1.3.cmml">2.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.1.m1.1b"><apply id="S4.T4.6.6.1.m1.1.1.cmml" xref="S4.T4.6.6.1.m1.1.1"><eq id="S4.T4.6.6.1.m1.1.1.1.cmml" xref="S4.T4.6.6.1.m1.1.1.1"></eq><ci id="S4.T4.6.6.1.m1.1.1.2.cmml" xref="S4.T4.6.6.1.m1.1.1.2">𝑟</ci><cn id="S4.T4.6.6.1.m1.1.1.3.cmml" type="float" xref="S4.T4.6.6.1.m1.1.1.3">2.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.1.m1.1c">r=2.0</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.1.m1.1d">italic_r = 2.0</annotation></semantics></math><span class="ltx_text" id="S4.T4.6.6.1.2" style="font-size:90%;">)</span>
</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T4.6.6.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.6.2.1" style="font-size:90%;">45.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T4.6.6.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.6.3.1" style="font-size:90%;">15.47</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T4.6.6.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.6.4.1" style="font-size:90%;">525.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T4.6.6.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S4.T4.6.6.5.1" style="font-size:90%;">78.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>
Ablation over the choice of non-linearity for the memory stream on ViTTM-B.
<math alttext="r" class="ltx_Math" display="inline" id="S4.T4.10.m1.1"><semantics id="S4.T4.10.m1.1b"><mi id="S4.T4.10.m1.1.1" xref="S4.T4.10.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.T4.10.m1.1c"><ci id="S4.T4.10.m1.1.1.cmml" xref="S4.T4.10.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.m1.1d">r</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.m1.1e">italic_r</annotation></semantics></math> is the scaling ratio applied to the embedding dimension <math alttext="d" class="ltx_Math" display="inline" id="S4.T4.11.m2.1"><semantics id="S4.T4.11.m2.1b"><mi id="S4.T4.11.m2.1.1" xref="S4.T4.11.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.T4.11.m2.1c"><ci id="S4.T4.11.m2.1.1.cmml" xref="S4.T4.11.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.m2.1d">d</annotation><annotation encoding="application/x-llamapun" id="S4.T4.11.m2.1e">italic_d</annotation></semantics></math>, thus the hidden dimension of the MLP is <math alttext="r\times d" class="ltx_Math" display="inline" id="S4.T4.12.m3.1"><semantics id="S4.T4.12.m3.1b"><mrow id="S4.T4.12.m3.1.1" xref="S4.T4.12.m3.1.1.cmml"><mi id="S4.T4.12.m3.1.1.2" xref="S4.T4.12.m3.1.1.2.cmml">r</mi><mo id="S4.T4.12.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T4.12.m3.1.1.1.cmml">×</mo><mi id="S4.T4.12.m3.1.1.3" xref="S4.T4.12.m3.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.12.m3.1c"><apply id="S4.T4.12.m3.1.1.cmml" xref="S4.T4.12.m3.1.1"><times id="S4.T4.12.m3.1.1.1.cmml" xref="S4.T4.12.m3.1.1.1"></times><ci id="S4.T4.12.m3.1.1.2.cmml" xref="S4.T4.12.m3.1.1.2">𝑟</ci><ci id="S4.T4.12.m3.1.1.3.cmml" xref="S4.T4.12.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.m3.1d">r\times d</annotation><annotation encoding="application/x-llamapun" id="S4.T4.12.m3.1e">italic_r × italic_d</annotation></semantics></math>.
The inclusion of an MLP layer seems to have no effect on accuracy while increasing the FLOPs and latency of our model.
</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5 </span>Effect of Memory Stream Non-linearity</h4>
<div class="ltx_para" id="S4.SS2.SSS5.p1">
<p class="ltx_p" id="S4.SS2.SSS5.p1.2">LookupViT proposed using a non-linearity to their lookup tokens (analogous to our memory stream) for accuracy.
We report that this is inefficient (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T4" title="In 4.2.4 Effect of Memory and Process Token Number ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>).
An MLP increases FLOPS by 14%<math alttext="-" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p1.1.m1.1"><semantics id="S4.SS2.SSS5.p1.1.m1.1a"><mo id="S4.SS2.SSS5.p1.1.m1.1.1" xref="S4.SS2.SSS5.p1.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p1.1.m1.1b"><minus id="S4.SS2.SSS5.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS5.p1.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p1.1.m1.1c">-</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p1.1.m1.1d">-</annotation></semantics></math>56% and latency by 13%<math alttext="-" class="ltx_Math" display="inline" id="S4.SS2.SSS5.p1.2.m2.1"><semantics id="S4.SS2.SSS5.p1.2.m2.1a"><mo id="S4.SS2.SSS5.p1.2.m2.1.1" xref="S4.SS2.SSS5.p1.2.m2.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS5.p1.2.m2.1b"><minus id="S4.SS2.SSS5.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS5.p1.2.m2.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS5.p1.2.m2.1c">-</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS5.p1.2.m2.1d">-</annotation></semantics></math>47%, while having negligible effects on accuracy.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T5.3.3.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.3.4.1">Model Class</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.3.3.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.3.5.1">Model</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.3.3.6" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T5.3.3.6.1">Params</span> (M)</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1">GFLOPs</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T5.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.m1.1a"><mo id="S4.T5.1.1.1.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T5.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T5.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T5.2.2.2.1">Latency</span> (ms)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T5.2.2.2.m1.1"><semantics id="S4.T5.2.2.2.m1.1a"><mo id="S4.T5.2.2.2.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T5.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T5.3.3.3" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T5.3.3.3.1">Top-1</span>(%)<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T5.3.3.3.m1.1"><semantics id="S4.T5.3.3.3.m1.1a"><mo id="S4.T5.3.3.3.m1.1.1" mathcolor="#0000FF" stretchy="false" xref="S4.T5.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.m1.1b"><ci id="S4.T5.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.10.11.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T5.10.11.1.1" rowspan="3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S4.T5.10.11.1.1.1">ViT/DeiT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T5.10.11.1.2" style="padding-left:8.0pt;padding-right:8.0pt;">ViT-B/32</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="S4.T5.10.11.1.3" style="padding-left:8.0pt;padding-right:8.0pt;">88</th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.10.11.1.4" style="padding-left:8.0pt;padding-right:8.0pt;">4.37</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.10.11.1.5" style="padding-left:8.0pt;padding-right:8.0pt;">138.3</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T5.10.11.1.6" style="padding-left:8.0pt;padding-right:8.0pt;">72.2</td>
</tr>
<tr class="ltx_tr" id="S4.T5.10.12.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.10.12.2.1" style="padding-left:8.0pt;padding-right:8.0pt;">ViT-B/16</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.10.12.2.2" style="padding-left:8.0pt;padding-right:8.0pt;">87</th>
<td class="ltx_td ltx_align_right" id="S4.T5.10.12.2.3" style="padding-left:8.0pt;padding-right:8.0pt;">16.87</td>
<td class="ltx_td ltx_align_right" id="S4.T5.10.12.2.4" style="padding-left:8.0pt;padding-right:8.0pt;">529.5</td>
<td class="ltx_td ltx_align_right" id="S4.T5.10.12.2.5" style="padding-left:8.0pt;padding-right:8.0pt;">81.0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.10.13.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.10.13.3.1" style="padding-left:8.0pt;padding-right:8.0pt;">DeiT-B/16</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.10.13.3.2" style="padding-left:8.0pt;padding-right:8.0pt;">87</th>
<td class="ltx_td ltx_align_right" id="S4.T5.10.13.3.3" style="padding-left:8.0pt;padding-right:8.0pt;">16.87</td>
<td class="ltx_td ltx_align_right" id="S4.T5.10.13.3.4" style="padding-left:8.0pt;padding-right:8.0pt;">529.7</td>
<td class="ltx_td ltx_align_right" id="S4.T5.10.13.3.5" style="padding-left:8.0pt;padding-right:8.0pt;">81.8</td>
</tr>
<tr class="ltx_tr" id="S4.T5.10.14.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.10.14.4.1" rowspan="8" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S4.T5.10.14.4.1.1">Two-Stream</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.10.14.4.2" style="padding-left:8.0pt;padding-right:8.0pt;">CrossViT-B</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T5.10.14.4.3" style="padding-left:8.0pt;padding-right:8.0pt;">105</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.10.14.4.4" style="padding-left:8.0pt;padding-right:8.0pt;">21.22</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.10.14.4.5" style="padding-left:8.0pt;padding-right:8.0pt;">728.1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.10.14.4.6" style="padding-left:8.0pt;padding-right:8.0pt;">82.2</td>
</tr>
<tr class="ltx_tr" id="S4.T5.10.15.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.10.15.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">CrossViT-18</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.10.15.5.2" style="padding-left:8.0pt;padding-right:8.0pt;">43</th>
<td class="ltx_td ltx_align_right" id="S4.T5.10.15.5.3" style="padding-left:8.0pt;padding-right:8.0pt;">9.05</td>
<td class="ltx_td ltx_align_right" id="S4.T5.10.15.5.4" style="padding-left:8.0pt;padding-right:8.0pt;">374.1</td>
<td class="ltx_td ltx_align_right" id="S4.T5.10.15.5.5" style="padding-left:8.0pt;padding-right:8.0pt;">82.5</td>
</tr>
<tr class="ltx_tr" id="S4.T5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.4.4.1" style="padding-left:8.0pt;padding-right:8.0pt;">CrossViT-18<math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T5.4.4.1.m1.1"><semantics id="S4.T5.4.4.1.m1.1a"><mo id="S4.T5.4.4.1.m1.1.1" xref="S4.T5.4.4.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.1.m1.1b"><ci id="S4.T5.4.4.1.m1.1.1.cmml" xref="S4.T5.4.4.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T5.4.4.1.m1.1d">†</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.4.4.2" style="padding-left:8.0pt;padding-right:8.0pt;">44</th>
<td class="ltx_td ltx_align_right" id="S4.T5.4.4.3" style="padding-left:8.0pt;padding-right:8.0pt;">9.50</td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.4.4" style="padding-left:8.0pt;padding-right:8.0pt;">378.2</td>
<td class="ltx_td ltx_align_right" id="S4.T5.4.4.5" style="padding-left:8.0pt;padding-right:8.0pt;">82.8</td>
</tr>
<tr class="ltx_tr" id="S4.T5.10.16.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.10.16.6.1" style="padding-left:8.0pt;padding-right:8.0pt;">Rev-ViT-B</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.10.16.6.2" style="padding-left:8.0pt;padding-right:8.0pt;">86</th>
<td class="ltx_td ltx_align_right" id="S4.T5.10.16.6.3" style="padding-left:8.0pt;padding-right:8.0pt;">17.49</td>
<td class="ltx_td ltx_align_right" id="S4.T5.10.16.6.4" style="padding-left:8.0pt;padding-right:8.0pt;">556.5</td>
<td class="ltx_td ltx_align_right" id="S4.T5.10.16.6.5" style="padding-left:8.0pt;padding-right:8.0pt;">81.8</td>
</tr>
<tr class="ltx_tr" id="S4.T5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.5.5.1" style="padding-left:8.0pt;padding-right:8.0pt;">LookupViT<sub class="ltx_sub" id="S4.T5.5.5.1.1"><span class="ltx_text ltx_font_italic" id="S4.T5.5.5.1.1.1">3×3</span></sub>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.5.5.2" style="padding-left:8.0pt;padding-right:8.0pt;">90</th>
<td class="ltx_td ltx_align_right" id="S4.T5.5.5.3" style="padding-left:8.0pt;padding-right:8.0pt;">5.26</td>
<td class="ltx_td ltx_align_right" id="S4.T5.5.5.4" style="padding-left:8.0pt;padding-right:8.0pt;">230.5</td>
<td class="ltx_td ltx_align_right" id="S4.T5.5.5.5" style="padding-left:8.0pt;padding-right:8.0pt;">77.9</td>
</tr>
<tr class="ltx_tr" id="S4.T5.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.6.6.1" style="padding-left:8.0pt;padding-right:8.0pt;">LookupViT<sub class="ltx_sub" id="S4.T5.6.6.1.1"><span class="ltx_text ltx_font_italic" id="S4.T5.6.6.1.1.1">5×5</span></sub>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.6.6.2" style="padding-left:8.0pt;padding-right:8.0pt;">90</th>
<td class="ltx_td ltx_align_right" id="S4.T5.6.6.3" style="padding-left:8.0pt;padding-right:8.0pt;">6.94</td>
<td class="ltx_td ltx_align_right" id="S4.T5.6.6.4" style="padding-left:8.0pt;padding-right:8.0pt;">297.2</td>
<td class="ltx_td ltx_align_right" id="S4.T5.6.6.5" style="padding-left:8.0pt;padding-right:8.0pt;">81.6</td>
</tr>
<tr class="ltx_tr" id="S4.T5.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.7.7.1" style="padding-left:8.0pt;padding-right:8.0pt;">LookupViT<sub class="ltx_sub" id="S4.T5.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S4.T5.7.7.1.1.1">7×7</span></sub>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.7.7.2" style="padding-left:8.0pt;padding-right:8.0pt;">90</th>
<td class="ltx_td ltx_align_right" id="S4.T5.7.7.3" style="padding-left:8.0pt;padding-right:8.0pt;">9.45</td>
<td class="ltx_td ltx_align_right" id="S4.T5.7.7.4" style="padding-left:8.0pt;padding-right:8.0pt;">379.5</td>
<td class="ltx_td ltx_align_right" id="S4.T5.7.7.5" style="padding-left:8.0pt;padding-right:8.0pt;">83.0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.8.8.1" style="padding-left:8.0pt;padding-right:8.0pt;">LookupViT<sub class="ltx_sub" id="S4.T5.8.8.1.1"><span class="ltx_text ltx_font_italic" id="S4.T5.8.8.1.1.1">10×10</span></sub>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T5.8.8.2" style="padding-left:8.0pt;padding-right:8.0pt;">90</th>
<td class="ltx_td ltx_align_right" id="S4.T5.8.8.3" style="padding-left:8.0pt;padding-right:8.0pt;">14.80</td>
<td class="ltx_td ltx_align_right" id="S4.T5.8.8.4" style="padding-left:8.0pt;padding-right:8.0pt;">563.4</td>
<td class="ltx_td ltx_align_right" id="S4.T5.8.8.5" style="padding-left:8.0pt;padding-right:8.0pt;">83.9</td>
</tr>
<tr class="ltx_tr" id="S4.T5.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T5.9.9.2" rowspan="2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S4.T5.9.9.2.1">Ours</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.9.9.1" style="padding-left:8.0pt;padding-right:8.0pt;">ViTTM-B<sub class="ltx_sub" id="S4.T5.9.9.1.1"><span class="ltx_text ltx_font_italic" id="S4.T5.9.9.1.1.1">(28,28)</span></sub>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T5.9.9.3" style="padding-left:8.0pt;padding-right:8.0pt;">127</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.9.9.4" style="padding-left:8.0pt;padding-right:8.0pt;">7.08</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.9.9.5" style="padding-left:8.0pt;padding-right:8.0pt;">234.1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T5.9.9.6" style="padding-left:8.0pt;padding-right:8.0pt;">82.9</td>
</tr>
<tr class="ltx_tr" id="S4.T5.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T5.10.10.1" style="padding-left:8.0pt;padding-right:8.0pt;">ViTTM-B<sub class="ltx_sub" id="S4.T5.10.10.1.1"><span class="ltx_text ltx_font_italic" id="S4.T5.10.10.1.1.1">(32,16)</span></sub>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S4.T5.10.10.2" style="padding-left:8.0pt;padding-right:8.0pt;">125</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T5.10.10.3" style="padding-left:8.0pt;padding-right:8.0pt;">7.10</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T5.10.10.4" style="padding-left:8.0pt;padding-right:8.0pt;">251.5</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T5.10.10.5" style="padding-left:8.0pt;padding-right:8.0pt;">80.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.13.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.14.2" style="font-size:90%;">
Comparison of ViTTM with state-of-the-art methods on image classification (ImageNet-1K).
ViTTMs are much faster (lower latency) than state-of-the-art methods while matching their accuracy is most cases.
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S1.F1" title="In 1 Introduction ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> depicts the data in this table.
Latency was measured on a 80GB A100 with batch size 256.
<span class="ltx_text ltx_font_italic" id="S4.T5.14.2.1">Note</span>: LookupViT does not have a public implementation, as such we implement a version following the paper.
</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison with other work</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We evaluate our ViTTM models on ImageNet-1K classification (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS3.SSS1" title="4.3.1 Image Classification (ImageNet-1K) ‣ 4.3 Comparison with other work ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3.1</span></a>) and ADE20K semantic segmentation (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS3.SSS2" title="4.3.2 Semantic Segmentation (ADE20K) ‣ 4.3 Comparison with other work ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3.2</span></a>) using Segmenter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib34" title="">34</a>]</cite> from the MMSegmentation library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib5" title="">5</a>]</cite>.
For image classification, we compare against existing two-stream architectures, and use ViT and DeiT models as baselines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib35" title="">35</a>]</cite>.
For semantic segmentation, we compare with a ViT-B/16 model from the timm library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib39" title="">39</a>]</cite>.
Based on our ablations we configure our ViTTM model with <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Linear Attention</span> read-write heads (<math alttext="c=d/4" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">c</mi><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml"><mi id="S4.SS3.p1.1.m1.1.1.3.2" xref="S4.SS3.p1.1.m1.1.1.3.2.cmml">d</mi><mo id="S4.SS3.p1.1.m1.1.1.3.1" xref="S4.SS3.p1.1.m1.1.1.3.1.cmml">/</mo><mn id="S4.SS3.p1.1.m1.1.1.3.3" xref="S4.SS3.p1.1.m1.1.1.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><eq id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></eq><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝑐</ci><apply id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><divide id="S4.SS3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3.1"></divide><ci id="S4.SS3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2">𝑑</ci><cn id="S4.SS3.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">c=d/4</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_c = italic_d / 4</annotation></semantics></math>), <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.2">Add</span> fusion, and with <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.3">no</span> memory stream non-linearity.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Image Classification (ImageNet-1K)</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.4">ViTTM offers competitive accuracy-latency trade-offs on ImageNet-1K.
<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T5" title="In 4.2.5 Effect of Memory Stream Non-linearity ‣ 4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a> presents our evaluation with existing methods.
We achieve lower latency while matching or nearly matching accuracy of other methods, thereby expanding the Pareto frontier (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S1.F1" title="In 1 Introduction ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>).
For example, compared with the similar latency LookupViT<sub class="ltx_sub" id="S4.SS3.SSS1.p1.4.1"><span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.4.1.1">3×3</span></sub>, ViTTM-B<sub class="ltx_sub" id="S4.SS3.SSS1.p1.4.2"><span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.4.2.1">(28,28)</span></sub> has 5.5% greater accuracy.
When compared with the similar accuracy LookupViT<sub class="ltx_sub" id="S4.SS3.SSS1.p1.4.3"><span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.4.3.1">7×7</span></sub>, our ViTTM-B<sub class="ltx_sub" id="S4.SS3.SSS1.p1.4.4"><span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.4.4.1">(28,28)</span></sub> has a 145.1ms (38%) lower latency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Semantic Segmentation (ADE20K)</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">ViTTM also achieves competitive results on ADE20K, as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.T6" title="In 4.3.2 Semantic Segmentation (ADE20K) ‣ 4.3 Comparison with other work ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.
Our ViTTM-B<sub class="ltx_sub" id="S4.SS3.SSS2.p1.1.1"><span class="ltx_text ltx_font_italic" id="S4.SS3.SSS2.p1.1.1.1">(28,28)</span></sub> matches ViT-B/16 on ADE20K semantic segmentation while improving FPS by 37% (23.8 FPS to 32.5 FPS) on an A100 GPU, and by 94% (13.8 FPS to 26.8 FPS) on an A30 GPU.
</p>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T6.3.3.4">
<span class="ltx_text ltx_font_bold" id="S4.T6.3.3.4.1" style="font-size:90%;">Model</span><span class="ltx_text" id="S4.T6.3.3.4.2" style="font-size:90%;"> (M)</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1" style="font-size:90%;">FPS (A100)</span><span class="ltx_text" id="S4.T6.1.1.1.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.1.1.1.m1.1"><semantics id="S4.T6.1.1.1.m1.1a"><mo id="S4.T6.1.1.1.m1.1.1" mathcolor="#0000FF" mathsize="90%" stretchy="false" xref="S4.T6.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T6.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T6.2.2.2.1" style="font-size:90%;">FPS (A30)</span><span class="ltx_text" id="S4.T6.2.2.2.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.2.2.2.m1.1"><semantics id="S4.T6.2.2.2.m1.1a"><mo id="S4.T6.2.2.2.m1.1.1" mathcolor="#0000FF" mathsize="90%" stretchy="false" xref="S4.T6.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.m1.1b"><ci id="S4.T6.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T6.3.3.3">
<span class="ltx_text ltx_font_bold" id="S4.T6.3.3.3.1" style="font-size:90%;">mIoU</span><span class="ltx_text" id="S4.T6.3.3.3.2" style="font-size:90%;"> </span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.3.3.3.m1.1"><semantics id="S4.T6.3.3.3.m1.1a"><mo id="S4.T6.3.3.3.m1.1.1" mathcolor="#0000FF" mathsize="90%" stretchy="false" xref="S4.T6.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.m1.1b"><ci id="S4.T6.3.3.3.m1.1.1.cmml" xref="S4.T6.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T6.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T6.5.6.1.1"><span class="ltx_text" id="S4.T6.5.6.1.1.1" style="font-size:90%;">ViT-B/16</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T6.5.6.1.2"><span class="ltx_text" id="S4.T6.5.6.1.2.1" style="font-size:90%;">23.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T6.5.6.1.3"><span class="ltx_text" id="S4.T6.5.6.1.3.1" style="font-size:90%;">13.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T6.5.6.1.4"><span class="ltx_text" id="S4.T6.5.6.1.4.1" style="font-size:90%;">45.65</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.4.4.1">
<span class="ltx_text" id="S4.T6.4.4.1.1" style="font-size:90%;">ViTTM-B</span><sub class="ltx_sub" id="S4.T6.4.4.1.2"><span class="ltx_text ltx_font_italic" id="S4.T6.4.4.1.2.1" style="font-size:90%;">(28,28)</span></sub>
</th>
<td class="ltx_td ltx_align_right" id="S4.T6.4.4.2">
<span class="ltx_text ltx_font_italic" id="S4.T6.4.4.2.1" style="font-size:90%;">(+37%)</span><span class="ltx_text" id="S4.T6.4.4.2.2" style="font-size:90%;"> 32.5</span>
</td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.4.3">
<span class="ltx_text ltx_font_italic" id="S4.T6.4.4.3.1" style="font-size:90%;">(+94%)</span><span class="ltx_text" id="S4.T6.4.4.3.2" style="font-size:90%;"> 26.8</span>
</td>
<td class="ltx_td ltx_align_right" id="S4.T6.4.4.4"><span class="ltx_text" id="S4.T6.4.4.4.1" style="font-size:90%;">45.17</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T6.5.5.1">
<span class="ltx_text" id="S4.T6.5.5.1.1" style="font-size:90%;">ViTTM-B</span><sub class="ltx_sub" id="S4.T6.5.5.1.2"><span class="ltx_text ltx_font_italic" id="S4.T6.5.5.1.2.1" style="font-size:90%;">(32,16)</span></sub>
</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T6.5.5.2">
<span class="ltx_text ltx_font_italic" id="S4.T6.5.5.2.1" style="font-size:90%;">(+38%)</span><span class="ltx_text" id="S4.T6.5.5.2.2" style="font-size:90%;"> 32.8</span>
</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T6.5.5.3">
<span class="ltx_text ltx_font_italic" id="S4.T6.5.5.3.1" style="font-size:90%;">(+94%)</span><span class="ltx_text" id="S4.T6.5.5.3.2" style="font-size:90%;"> 26.7</span>
</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T6.5.5.4"><span class="ltx_text" id="S4.T6.5.5.4.1" style="font-size:90%;">43.60</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>
Results for semantic segmentaion (ADE20K) using the Segmenter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib34" title="">34</a>]</cite> segmentation method.
FPS measurements are taken on 80GB NVIDIA A100 and 24GB A30 GPUs.
</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Two-Stream Architectures:</span> It is evident from both prior work and our experiments that two-stream architectures can be used to create both efficient and accurate vision models, <em class="ltx_emph ltx_font_italic" id="S5.p1.1.2">e.g</em>.<span class="ltx_text" id="S5.p1.1.3"></span>, LookupViT, CrossViT, and ViTTMs.
However, two-stream architectures present more design choices than ViTs, leaving much to explore about what constitutes an optimal design.
For example, although we ablate many design choices (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S4.SS2" title="4.2 Ablations ‣ 4 Evaluation ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>), we do not consider differing embedding dimensions per stream, varying depth, or the effect of self-supervised learning.
Previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib3" title="">3</a>]</cite> have demonstrated the effectiveness of altering embedding dimensions, yet its interplay with other parameters remains unclear.
Changing the embedding dimension has been shown to be effective in prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib3" title="">3</a>]</cite>, but how it is related to other parameters is unknown.
Moreover, we also find that two-stream architecture can suffer from training instabilities (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS4" title="3.2.4 Processing in the Memory Stream ‣ 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2.4</span></a>).
An important direction for future work is the rigorous study of both architectural choices and the training dynamics of two-stream architectures.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Read-Write Heads:</span> An important consideration in our design is the choice of read-write heads.
As stated in <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS2" title="3.2.2 Read-Write Heads ‣ 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>, read-write heads must be efficient.
We believe that efficient implementation of read-write mechanisms is an important direction for future work.
Actually obtaining latency reductions from FLOP reductions is important for the further development and use of alternative architectures.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Uses of Memory Stream for Multi-Modal application:</span>
In this work we ablate how the process stream is be initialized, <em class="ltx_emph ltx_font_italic" id="S5.p3.1.2">i.e</em>.<span class="ltx_text" id="S5.p3.1.3"></span> how it is populated with tokens (<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#S3.SS2.SSS1" title="3.2.1 Initializing Process and Memory Tokens ‣ 3.2 ViTTM Architecture ‣ 3 Vision Token Turing Machines (ViTTM) ‣ Token Turing Machines are Efficient Vision Models"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>).
However, we do not explore how the memory tokens should be initialized.
It is possible to initialize the memory tokens using another modality, thus creating a multi-modal model.
Such a model would read/write (fuse) information across modalities.
Recent work, in generative models use two-stream architectures for this purpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.07613v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduced ViTTM, a ViT-NTM hybrid, that integrates memory into vision transformers to create an efficient and accurate vision model.
Evaluation on ImageNet1K and ADE20K demonstrated that ViTTM accuracy matches or exceeds prior work.
ViTTM-B is 56% faster (2.4<math alttext="\times" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mo id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><times id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">×</annotation></semantics></math> fewer FLOPs) than ViT-B while having an accuracy of 82.9% (+1.9%) on ImageNet1K.
On ADE20K, ViTTM-B achieves 37%-94% higher FPS than ViT-B.
Our current work illustrated the effectiveness of ViTTMs, and we anticipate that their efficiency can be improved and extended to other tasks.
Future work that explores latency optimization of the read-write mechanisms, such as fusing operators, would be an effective extension.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">Token merging: Your vit but faster, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Emerging properties in self-supervised vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, pages 9650–9660, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Chun-Fu Chen, Quanfu Fan, and Rameswar Panda.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Crossvit: Cross-attention multi-scale vision transformer for image classification, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Xinlei Chen, Saining Xie, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">An empirical study of training self-supervised vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib4.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib4.5.3" style="font-size:90%;">, pages 9640–9649, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
MMSegmentation Contributors.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/open-mmlab/mmsegmentation" style="font-size:90%;" title="">https://github.com/open-mmlab/mmsegmentation</a><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">ImageNet: A Large-Scale Hierarchical Image Database.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">CVPR09</span><span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Rares Dolga, Marius Cobzarenco, and David Barber.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Latent attention for linear time transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">arXiv preprint arXiv:2402.17512</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">arXiv</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Scaling rectified flow transformers for high-resolution image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib9.4.2" style="font-size:90%;">Forty-first International Conference on Machine Learning</span><span class="ltx_text" id="bib.bib9.5.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
FAIR.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">fvcore.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/fvcore" style="font-size:90%;" title="">https://github.com/facebookresearch/fvcore</a><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Multiscale vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib11.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</span><span class="ltx_text" id="bib.bib11.5.3" style="font-size:90%;">, pages 6824–6835, October 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Mohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and Jürgen Gall.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Adaptive token sampling for efficient vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib12.4.2" style="font-size:90%;">European Conference on Computer Vision</span><span class="ltx_text" id="bib.bib12.5.3" style="font-size:90%;">, pages 396–414. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Flux that plays music.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">arXiv preprint arXiv:2409.00587</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Alex Graves, Greg Wayne, and Ivo Danihelka.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Neural turing machines, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Hybrid computing using a neural network with dynamic external memory.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.3.1" style="font-size:90%;">Nature</span><span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">, 538(7626):471–476, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Joakim Bruslund Haurum, Sergio Escalera, Graham W. Taylor, and Thomas B. Moeslund.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Which tokens to use? investigating token reduction in vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, pages 773–783, October 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Masked autoencoders are scalable vision learners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 15979–15988, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Transformers are rnns: Fast autoregressive transformers with linear attention.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib18.4.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib18.5.3" style="font-size:90%;">, pages 5156–5165. PMLR, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Learned token pruning for transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</span><span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">, pages 784–794, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Rajat Koner, Gagan Jain, Prateek Jain, Volker Tresp, and Sujoy Paul.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Lookupvit: Compressing visual information to a limited number of tokens.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1" style="font-size:90%;">arXiv</span><span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Not all patches are what you need: Expediting vision transformers via token reorganizations, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted windows.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, pages 10012–10022, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feichtenhofer, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Reversible vision transformers, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Intriguing properties of vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">arXiv</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
PyTorch Foundation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">PyTorch Benchmark: PyTorch Tutorials Documentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.3.1" style="font-size:90%;">url: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pytorch.org/tutorials/recipes/recipes/benchmark.html" style="font-size:90%;" title="">https://pytorch.org/tutorials/recipes/recipes/benchmark.html</a><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, and Timothy P Lillicrap.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Scaling memory-augmented neural networks with sparse reads and writes, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Dynamicvit: Efficient vision transformers with dynamic token sparsification, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Cedric Renggli, André Susano Pinto, Neil Houlsby, Basil Mustafa, Joan Puigcerver, and Carlos Riquelme.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Learning to merge tokens in vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">arXiv preprint arXiv:2202.12015</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Imagenet-21k pretraining for the masses, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Michael S. Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Julian Ibarz, and Anurag Arnab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Token turing machines.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib30.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib30.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Michael S. Ryoo, A. J. Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Tokenlearner: What can 8 learned tokens do for images and videos?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="font-size:90%;">arXiv</span><span class="ltx_text" id="bib.bib31.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, and Andrew Jackson.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Fine-tuning image transformers using learnable memory.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib32.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib32.5.3" style="font-size:90%;">, pages 12155–12164, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">How to train your vit? data, augmentation, and regularization in vision transformers, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Segmenter: Transformer for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, pages 7262–7272, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Training data-efficient image transformers &amp; distillation through attention.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib35.4.2" style="font-size:90%;">International Conference on Machine Learning</span><span class="ltx_text" id="bib.bib35.5.3" style="font-size:90%;">, volume 139, pages 10347–10357, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Hugo Touvron, Matthieu Cord, and Hervé Jégou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">DeiT III: Revenge of the ViT, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Attention Is All You Need.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib37.4.2" style="font-size:90%;">arXiv</span><span class="ltx_text" id="bib.bib37.5.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Jason Weston, Sumit Chopra, and Antoine Bordes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Memory networks, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Ross Wightman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Pytorch image models.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/rwightman/pytorch-image-models" style="font-size:90%;" title="">https://github.com/rwightman/pytorch-image-models</a><span class="ltx_text" id="bib.bib39.3.1" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Memformer: A memory-augmented transformer for sequence modeling.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.3.1" style="font-size:90%;">In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib40.4.2" style="font-size:90%;">Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022</span><span class="ltx_text" id="bib.bib40.5.3" style="font-size:90%;">, pages 308–318, Online only, Nov. 2022. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Chunlong Xia, Xinliang Wang, Feng Lv, Xin Hao, and Yifeng Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">Vit-comer: Vision transformer with convolutional multi-scale feature interaction for dense predictions, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">A-vit: Adaptive tokens for efficient vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib42.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib42.5.3" style="font-size:90%;">, pages 10809–10818, 2022.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 11 20:49:43 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
