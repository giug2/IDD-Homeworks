<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2105.09400] Separation of Powers in Federated Learning</title><meta property="og:description" content="Federated Learning (FL) enables collaborative training among mutually distrusting parties. Model updates, rather than training data, are concentrated and fused in a central aggregation server.
A key security challenge …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Separation of Powers in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Separation of Powers in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2105.09400">

<!--Generated on Sat Mar  2 05:35:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_ERROR undefined">\ul</span>








































































</p>
</div>
<h1 class="ltx_title ltx_title_document">Separation of Powers in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pau-Chen Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">IBM Research</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_state">New York</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:pau@us.ibm.com">pau@us.ibm.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kevin Eykholt
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">IBM Research</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_state">New York</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kheykholt@ibm.com">kheykholt@ibm.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhongshu Gu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">IBM Research</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_state">New York</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zgu@us.ibm.com">zgu@us.ibm.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hani Jamjoom
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">IBM Research</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_state">New York</span><span id="id12.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jamjoom@us.ibm.com">jamjoom@us.ibm.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">K. R. Jayaram
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">IBM Research</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_state">New York</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jayaramkr@us.ibm.com">jayaramkr@us.ibm.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Enriquillo Valdez
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id16.1.id1" class="ltx_text ltx_affiliation_institution">IBM Research</span><span id="id17.2.id2" class="ltx_text ltx_affiliation_state">New York</span><span id="id18.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:rvaldez@us.ibm.com">rvaldez@us.ibm.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ashish Verma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id19.1.id1" class="ltx_text ltx_affiliation_institution">IBM Research</span><span id="id20.2.id2" class="ltx_text ltx_affiliation_state">New York</span><span id="id21.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ashish.verma1@ibm.com">ashish.verma1@ibm.com</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id22.id1" class="ltx_p"><span title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Federated Learning</span></span> (<abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr>) enables collaborative training among mutually distrusting parties. Model updates, rather than training data, are concentrated and fused in a central aggregation server.
A key security challenge in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> is that an untrustworthy or compromised aggregation process might lead to unforeseeable information leakage. This challenge is especially acute due to recently demonstrated attacks that have reconstructed large fractions of training data from ostensibly “sanitized” model updates.</p>
<p id="id23.id2" class="ltx_p">In this paper, we introduce <span id="id23.id2.1" class="ltx_text ltx_font_smallcaps">Truda</span>, a new cross-silo <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> system, employing a trustworthy and decentralized aggregation architecture to break down information concentration with regard to a single aggregator.
Based on the unique computational properties of model-fusion algorithms, all exchanged model updates in <span id="id23.id2.2" class="ltx_text ltx_font_smallcaps">Truda</span> are disassembled at the parameter-granularity and re-stitched to random partitions designated for multiple TEE-protected aggregators. Thus, each aggregator only has a fragmentary and shuffled view of model updates and is oblivious to the model architecture. Our new security mechanisms can fundamentally mitigate training data reconstruction attacks, while still preserving the final accuracy of trained models and keeping performance overheads low.</p>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Federated Learning</span></span> (<abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2017a</a>)</cite> provides a collaborative training mechanism, which allows multiple parties to build a joint  <span title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">machine learning</span></span> (<abbr title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr>) model.
<abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> allows parties to retain private data within their controlled domains. Only model updates are shared to a central aggregation server.
The security setting of <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> is especially attractive for mutually distrusting/competing training participants as well as holders of sensitive data (e.g., health and financial data) seeking to preserve data privacy.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> is <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">typically</em> deployed in two scenarios: <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">cross-device</em> and <em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">cross-silo</em> <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>.
The <em id="S1.p2.1.4" class="ltx_emph ltx_font_italic">cross-device</em> scenario involves a large number of parties (<math id="S1.p2.1.m1.1" class="ltx_Math" alttext="&gt;1000" display="inline"><semantics id="S1.p2.1.m1.1a"><mrow id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml"></mi><mo id="S1.p2.1.m1.1.1.1" xref="S1.p2.1.m1.1.1.1.cmml">&gt;</mo><mn id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><gt id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">&gt;1000</annotation></semantics></math>), but each party has a small number of data items, constrained compute capability, and limited energy reserve (e.g., mobile phones or IoT devices). They are highly unreliable and are expected to drop and join frequently. Examples include a large organization learning from data stored on employees’ devices and a device manufacturer training a model from private data located on millions of its devices (e.g., Google Gboard <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>). A <em id="S1.p2.1.5" class="ltx_emph ltx_font_italic">trusted authority</em>, which performs aggregation and orchestrates training, is typically present in a <em id="S1.p2.1.6" class="ltx_emph ltx_font_italic">cross-device</em> scenario. Contrarily, in the <em id="S1.p2.1.7" class="ltx_emph ltx_font_italic">cross-silo</em> scenario, the number of parties is small, but each party has extensive compute capabilities (with stable access to electric power and/or equipped with hardware <abbr title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr> accelerators) and large amounts of data. The parties have reliable participation throughout the entire <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training life-cycle, but are more susceptible to sensitive data leakage. Examples include multiple hospitals collaborating to train a tumor detection model on radiographs, multiple banks collaborating to train a credit card fraud detection model, etc. In <em id="S1.p2.1.8" class="ltx_emph ltx_font_italic">cross-silo</em> scenarios, there exists <em id="S1.p2.1.9" class="ltx_emph ltx_font_italic">no presumed central trusted authority</em>. All parties involved in the training are <em id="S1.p2.1.10" class="ltx_emph ltx_font_italic">equal</em> collaborators. The deployments often involve hosting aggregation in public clouds, or alternatively one of the parties acting as, and providing infrastructure for aggregation. In this paper, we focus on examining the trustworthiness of collaborative learning in <em id="S1.p2.1.11" class="ltx_emph ltx_font_italic">cross-silo</em> scenarios.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">There was a <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">misconception</em> that the exchanged model updates in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> communications would contain
far less, if any, information about the raw training data. Thus, sharing model updates was considered to
be “privacy-preserving.” However, although not easily discernible, training data information is still embedded in the model updates. Recent research <cite class="ltx_cite ltx_citemacro_citep">(Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2019</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>; Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2020</a>; Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>
has demonstrated the feasibility and ease of inferring private attributes and reconstructing large fractions
of training data by exploiting model updates, thereby challenging the privacy promises of <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr>
in the presence of honest-but-curious aggregation servers.
Furthermore, since aggregation often runs on untrustworthy cloud or third-party infrastructures in the <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">cross-silo</em> scenarios, we need to re-examine the trust model and system architecture of current <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> frameworks under this new attack scenario.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Existing solutions that reinforce <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> privacy include (1)  <span title="Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Differential Privacy</span></span> (<abbr title="Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DP</span></abbr>) based aggregation through the addition of statistical noise to model updates <cite class="ltx_cite ltx_citemacro_citep">(McMahan
et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2017b</a>; Geyer
et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>; Shokri and
Shmatikov, <a href="#bib.bib51" title="" class="ltx_ref">2015</a>; Bhowmick et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>; Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite>, and (2) using  <span title="Secure Multi-Party Computation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Secure Multi-Party Computation</span></span> (<abbr title="Secure Multi-Party Computation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SMC</span></abbr>) or  <span title="Homomorphic Encryption" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Homomorphic Encryption</span></span> (<abbr title="Homomorphic Encryption" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HE</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(Aono et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>; Hardy et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2017</a>; Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>; Mohassel and
Zhang, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>)</cite> for aggregation. Both techniques have several drawbacks. The former often significantly decreases the accuracy of the trained model and needs careful hyper-parameter tuning to minimize accuracy loss. The latter is computationally expensive, with aggregation overheads significantly outpacing training time <cite class="ltx_cite ltx_citemacro_citep">(Jayaram et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our work is motivated by the following key insights: (I) The <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">concentration</em> of model updates in a central <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregator discloses significantly more information than what is required by the aggregation/model-fusion algorithms. This gap, indeed, facilitates data reconstruction attacks if the central aggregator is compromised. (II) <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> model-fusion algorithms are <em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">bijective</em> and only involve <em id="S1.p5.1.3" class="ltx_emph ltx_font_italic">coordinate-wise</em> arithmetic operations across model updates. Partitioning and (internally) shuffling model updates do not change the fusion results, thus having no impact on the final model accuracy and convergence rate compared to traditional (insecure) <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training. We only need to ensure that the local transformation of model updates is deterministic, reversible, and synchronized across parties. From the attacker’s point of view, partitioning and shuffling operations fully disrupt the completeness and data-order of model updates, which are indispensable for reconstructing training data. (III) <span title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">Trusted Execution Environments</span></span> can help establish confidential and remote-attestable execution entities on untrustworthy servers. The missing link is how to authenticate and connect all individual entities to bootstrap trust in a distributed <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> ecosystem.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this paper, we introduce <span id="S1.p6.1.1" class="ltx_text ltx_font_smallcaps">Truda<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_upright">1</span></span><span id="footnote1.5" class="ltx_text">Truda</span><span id="footnote1.6" class="ltx_text ltx_font_upright"> stands for </span><span id="footnote1.7" class="ltx_text ltx_font_upright ltx_framed ltx_framed_underline">TRU</span><span id="footnote1.8" class="ltx_text ltx_font_upright">stworthy and </span><span id="footnote1.9" class="ltx_text ltx_font_upright ltx_framed ltx_framed_underline">D</span><span id="footnote1.10" class="ltx_text ltx_font_upright">ecentralized </span><span id="footnote1.11" class="ltx_text ltx_font_upright ltx_framed ltx_framed_underline">A</span><span id="footnote1.12" class="ltx_text ltx_font_upright">ggregation</span></span></span></span></span>, a new cross-silo <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> system for  <span title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">deep neural network</span></span> (<abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr>) training. In <span id="S1.p6.1.2" class="ltx_text ltx_font_smallcaps">Truda</span>, we employ multiple, rather than one, <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEE</span></abbr>-protected decentralized aggregators. All model updates are disassembled at the parameter-granularity and re-stitched (with shuffling) to random partitions designated for different aggregators. Thus, each aggregator only has a fragmentary and out-of-order view of each model update and is oblivious to the model architecture. This new system architecture can fundamentally minimize the information leakage surface for data reconstruction attacks, but with no utility loss regarding model aggregation. We have implemented three composable security-reinforced mechanisms in <span id="S1.p6.1.3" class="ltx_text ltx_font_smallcaps">Truda</span>:</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">Trustworthy Aggregation.</span>
We leverage <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> to protect the model-fusion process. Every aggregator runs within an  <span title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">encrypted virtual machine</span></span> (<abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr>) via AMD  <span title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Secure Encrypted Virtualization</span></span> (<abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>). All in-memory data are kept encrypted at runtime during model aggregation. To bootstrap trust between parties and aggregators, we design a two-phase attestation protocol and develop a series of tools for integrating/automating confidential computing in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr>. Each party can authenticate trustworthy aggregators before participating in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training. End-to-end secure channels, from the parties to the <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr>, are established after attestation to protect model updates in transit.</p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_bold">Decentralized Aggregation.</span>
Decentralization was primarily investigated in distributed learning <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Vanhaesebrouck et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2017</a>; Bellet
et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Koloskova
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Lalitha et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> as a load balancing technique for enhancing system performance;
model updates were distributed among aggregator replicas with each update assigned to a single replica and each replica seeing the entire model update from a party. This still provided a fertile ground for data reconstruction attacks on the aggregator side.
Instead, we choose a security-centric decentralization strategy by employing fine-grained model partitioning to break down information concentration. We launch multiple aggregators within <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr>. Each aggregator only receives a fraction of model
update with no knowledge of the whole model architecture. The parties share a model-mapper at parameter-granularity for disassembling and re-stitching a local model update into disjoint partitions, which are dispatched to corresponding aggregators. Furthermore, users can deploy multiple aggregators to physical servers at different geo-locations and potentially with diversified <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr>, e.g., Intel <abbr title="Software Guard Extensions" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SGX</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(McKeen et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2013</a>)</cite>/ <abbr title="Trust Domain Extensions" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TDX</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Intel, <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>, IBM <abbr title="Protected Execution Facility" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">PEF</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Hunt
et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, etc. Thus, we can prevent model aggregation from becoming a single point of failure (i.e., leaking entire and intact model updates) under security attacks.</p>
</div>
<div id="S1.p9" class="ltx_para ltx_noindent">
<p id="S1.p9.1" class="ltx_p"><span id="S1.p9.1.1" class="ltx_text ltx_font_bold">Shuffled Aggregation.</span> Based on the bijective property of model-fusion algorithms, we allow parties to permute the fragmentary model updates to further obfuscate the information sent to each aggregator. The permutation changes dynamically at each training round. This strategy guarantees that even if <em id="S1.p9.1.2" class="ltx_emph ltx_font_italic">all</em> decentralized aggregators are breached, adversaries will still require the permutation key to be able to decipher the correct ordering of the model updates and reconstruct the training data.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">In our security analysis, we reproduced three state-of-the-art training data reconstruction attacks <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>; Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2020</a>; Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> and plugged in <span id="S1.p10.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> for generating the model updates. Our experiments demonstrate that <span id="S1.p10.1.2" class="ltx_text ltx_font_smallcaps">Truda</span> renders all the attacks ineffective for reconstructing local training data. In our performance evaluation, we measured the accuracy/loss and latency for training deep learning models on the datasets, <em id="S1.p10.1.3" class="ltx_emph ltx_font_italic">MNIST</em>, <em id="S1.p10.1.4" class="ltx_emph ltx_font_italic">CIFAR-10</em>, and <em id="S1.p10.1.5" class="ltx_emph ltx_font_italic">RVL-CDIP</em><cite class="ltx_cite ltx_citemacro_citep">(Harley
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite>, with a spectrum of aggregation algorithms and <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> configurations. We demonstrate that <span id="S1.p10.1.6" class="ltx_text ltx_font_smallcaps">Truda</span> can achieve the same level of accuracy/loss and converge at the same rate, with minimal performance overheads compared to the traditional federated learning platform as baseline.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Threat Model</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our threat model assumes honest-but-curious aggregation servers, which are susceptible to compromise.
Adversaries attempt to attack aggregators and inspect model updates uploaded from parties. Their purpose is to reconstruct training data of parties that participate in the <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training.
We consider that the parties involved in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training are benign and will not collude with other parties to share training data.
This threat model is the same as in the <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> data reconstruction attacks <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>; Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2020</a>; Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In addition, our threat model is consistent with the one assumed in AMD <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>. We consider that parties involved in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> trust AMD  <span title="System-on-Chip" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">System-on-Chip</span></span> (<abbr title="System-on-Chip" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SoC</span></abbr>) and the <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr> launched to hold the model aggregation workloads. Adversaries can not only execute user-level code on the aggregator’s hosting machines, but can also execute malicious code at the level of privileged system software, e.g.,  <span title="operating system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">operating system</span></span> (<abbr title="operating system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OS</span></abbr>), hypervisor, BIOS, etc. The attacker may also have physical access to the DRAM of hosting machines. Our current <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> system implementation is based on the 1st-generation <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Kaplan
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2016</a>)</cite> on EPYC 7642 (ROME) microprocessor, which is the latest release available on the market. Based on the white papers, the following <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> generations, i.e., SEV-ES <cite class="ltx_cite ltx_citemacro_citep">(Kaplan, <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite> and SEV-SNP <cite class="ltx_cite ltx_citemacro_citep">(SEV-SNP, <a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite>, will include confidentiality protection for  <span title="virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">virtual machine</span></span> (<abbr title="virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VM</span></abbr>) register state and integrity protection to defend against memory corruption, aliasing, remapping, and replay attacks. Our threat model can be elevated to follow the stronger isolation protection of future <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> releases.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Recently, some research efforts <cite class="ltx_cite ltx_citemacro_citep">(Werner et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2019</a>; Buhren
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2019</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Wilke et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite> focus on discovering potential vulnerabilities of <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>. In this paper, we do not intend to address these problems and consider they will be fixed with the AMD’s firmware updates or in the upcoming SEV-SNP release. However, <em id="S2.p3.1.1" class="ltx_emph ltx_font_italic">even if <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr> are breached</em>, our design of decentralized and shuffled aggregation of model updates will still be effective at preventing adversaries from reconstructing training data.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Background</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section provides relevant information on common <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregation algorithms, data reconstruction attacks, computational properties of <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregation algorithms and AMD <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> that are leveraged by <span id="S3.p1.1.1" class="ltx_text ltx_font_smallcaps">Truda</span>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Aggregation Algorithms for DNN Training</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span title="Federated Stochastic Gradient Descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Federated Stochastic Gradient Descent</span></span> (<abbr title="Federated Stochastic Gradient Descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(Shokri and
Shmatikov, <a href="#bib.bib51" title="" class="ltx_ref">2015</a>)</cite> and  <span title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Federated Averaging</span></span> (<abbr title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2017a</a>)</cite> are the most common <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregation algorithms for <abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr> training, employing iterative merging and synchronizing model updates. Other <abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr> aggregation methods, e.g., Byzantine-robust fusions like <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Coordinate Median</em><cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2018</a>)</cite>/<em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Krum</em><cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite> and <em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">Paillier crypto fusion</em><cite class="ltx_cite ltx_citemacro_citep">(Aono et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>; Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite>, have the similar algorithmic structure with additional security enhancements. <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_smallcaps">Truda</span> can support all of them with no change. Here we describe the algorithms of <em id="S3.SS1.p1.1.5" class="ltx_emph ltx_font_italic"><abbr title="Federated Stochastic Gradient Descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></em> and <em id="S3.SS1.p1.1.6" class="ltx_emph ltx_font_italic"><abbr title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></em> in detail.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.11" class="ltx_p">We use <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\theta</annotation></semantics></math> to denote model parameters and <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">L</annotation></semantics></math> for the loss function. Each party has its own training data/label pairs <math id="S3.SS1.p2.3.m3.2" class="ltx_Math" alttext="(x_{i},y_{i})" display="inline"><semantics id="S3.SS1.p2.3.m3.2a"><mrow id="S3.SS1.p2.3.m3.2.2.2" xref="S3.SS1.p2.3.m3.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.2.2.2.3" xref="S3.SS1.p2.3.m3.2.2.3.cmml">(</mo><msub id="S3.SS1.p2.3.m3.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.3.m3.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.3.m3.2.2.2.4" xref="S3.SS1.p2.3.m3.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.3.m3.2.2.2.2" xref="S3.SS1.p2.3.m3.2.2.2.2.cmml"><mi id="S3.SS1.p2.3.m3.2.2.2.2.2" xref="S3.SS1.p2.3.m3.2.2.2.2.2.cmml">y</mi><mi id="S3.SS1.p2.3.m3.2.2.2.2.3" xref="S3.SS1.p2.3.m3.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.3.m3.2.2.2.5" xref="S3.SS1.p2.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.2b"><interval closure="open" id="S3.SS1.p2.3.m3.2.2.3.cmml" xref="S3.SS1.p2.3.m3.2.2.2"><apply id="S3.SS1.p2.3.m3.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.p2.3.m3.1.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.3.m3.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.2.2.2.2.1.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.2.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2.2">𝑦</ci><ci id="S3.SS1.p2.3.m3.2.2.2.2.3.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2.3">𝑖</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.2c">(x_{i},y_{i})</annotation></semantics></math>. The parties choose to share the gradients <math id="S3.SS1.p2.4.m4.2" class="ltx_Math" alttext="\nabla_{\theta}L_{\theta}(x_{i},y_{i})" display="inline"><semantics id="S3.SS1.p2.4.m4.2a"><mrow id="S3.SS1.p2.4.m4.2.2" xref="S3.SS1.p2.4.m4.2.2.cmml"><mrow id="S3.SS1.p2.4.m4.2.2.4" xref="S3.SS1.p2.4.m4.2.2.4.cmml"><msub id="S3.SS1.p2.4.m4.2.2.4.1" xref="S3.SS1.p2.4.m4.2.2.4.1.cmml"><mo id="S3.SS1.p2.4.m4.2.2.4.1.2" xref="S3.SS1.p2.4.m4.2.2.4.1.2.cmml">∇</mo><mi id="S3.SS1.p2.4.m4.2.2.4.1.3" xref="S3.SS1.p2.4.m4.2.2.4.1.3.cmml">θ</mi></msub><msub id="S3.SS1.p2.4.m4.2.2.4.2" xref="S3.SS1.p2.4.m4.2.2.4.2.cmml"><mi id="S3.SS1.p2.4.m4.2.2.4.2.2" xref="S3.SS1.p2.4.m4.2.2.4.2.2.cmml">L</mi><mi id="S3.SS1.p2.4.m4.2.2.4.2.3" xref="S3.SS1.p2.4.m4.2.2.4.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.2.2.3" xref="S3.SS1.p2.4.m4.2.2.3.cmml">​</mo><mrow id="S3.SS1.p2.4.m4.2.2.2.2" xref="S3.SS1.p2.4.m4.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.4.m4.2.2.2.2.3" xref="S3.SS1.p2.4.m4.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p2.4.m4.1.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.4.m4.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.4.m4.2.2.2.2.4" xref="S3.SS1.p2.4.m4.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.4.m4.2.2.2.2.2" xref="S3.SS1.p2.4.m4.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.4.m4.2.2.2.2.2.2" xref="S3.SS1.p2.4.m4.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS1.p2.4.m4.2.2.2.2.2.3" xref="S3.SS1.p2.4.m4.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.4.m4.2.2.2.2.5" xref="S3.SS1.p2.4.m4.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.2b"><apply id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2"><times id="S3.SS1.p2.4.m4.2.2.3.cmml" xref="S3.SS1.p2.4.m4.2.2.3"></times><apply id="S3.SS1.p2.4.m4.2.2.4.cmml" xref="S3.SS1.p2.4.m4.2.2.4"><apply id="S3.SS1.p2.4.m4.2.2.4.1.cmml" xref="S3.SS1.p2.4.m4.2.2.4.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.2.2.4.1.1.cmml" xref="S3.SS1.p2.4.m4.2.2.4.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.2.2.4.1.2.cmml" xref="S3.SS1.p2.4.m4.2.2.4.1.2">∇</ci><ci id="S3.SS1.p2.4.m4.2.2.4.1.3.cmml" xref="S3.SS1.p2.4.m4.2.2.4.1.3">𝜃</ci></apply><apply id="S3.SS1.p2.4.m4.2.2.4.2.cmml" xref="S3.SS1.p2.4.m4.2.2.4.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.2.2.4.2.1.cmml" xref="S3.SS1.p2.4.m4.2.2.4.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.2.2.4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2.4.2.2">𝐿</ci><ci id="S3.SS1.p2.4.m4.2.2.4.2.3.cmml" xref="S3.SS1.p2.4.m4.2.2.4.2.3">𝜃</ci></apply></apply><interval closure="open" id="S3.SS1.p2.4.m4.2.2.2.3.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2"><apply id="S3.SS1.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.4.m4.2.2.2.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2.2.2">𝑦</ci><ci id="S3.SS1.p2.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.4.m4.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.2c">\nabla_{\theta}L_{\theta}(x_{i},y_{i})</annotation></semantics></math> for a data batch to the aggregator. The aggregator computes the gradient sum of all parties and lets the parties synchronize their model parameters: <math id="S3.SS1.p2.5.m5.2" class="ltx_Math" alttext="\theta\leftarrow\theta-\eta\sum_{i=1}^{N}\nabla_{\theta}L_{\theta}(x_{i},y_{i})" display="inline"><semantics id="S3.SS1.p2.5.m5.2a"><mrow id="S3.SS1.p2.5.m5.2.2" xref="S3.SS1.p2.5.m5.2.2.cmml"><mi id="S3.SS1.p2.5.m5.2.2.4" xref="S3.SS1.p2.5.m5.2.2.4.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p2.5.m5.2.2.3" xref="S3.SS1.p2.5.m5.2.2.3.cmml">←</mo><mrow id="S3.SS1.p2.5.m5.2.2.2" xref="S3.SS1.p2.5.m5.2.2.2.cmml"><mi id="S3.SS1.p2.5.m5.2.2.2.4" xref="S3.SS1.p2.5.m5.2.2.2.4.cmml">θ</mi><mo id="S3.SS1.p2.5.m5.2.2.2.3" xref="S3.SS1.p2.5.m5.2.2.2.3.cmml">−</mo><mrow id="S3.SS1.p2.5.m5.2.2.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.cmml"><mi id="S3.SS1.p2.5.m5.2.2.2.2.4" xref="S3.SS1.p2.5.m5.2.2.2.2.4.cmml">η</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.2.2.2.2.3" xref="S3.SS1.p2.5.m5.2.2.2.2.3.cmml">​</mo><mrow id="S3.SS1.p2.5.m5.2.2.2.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.cmml"><msubsup id="S3.SS1.p2.5.m5.2.2.2.2.2.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.cmml"><mo id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.2.cmml">∑</mo><mrow id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.cmml"><mi id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.2.cmml">i</mi><mo id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.1" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.1.cmml">=</mo><mn id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.5.m5.2.2.2.2.2.3.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.3.cmml">N</mi></msubsup><mrow id="S3.SS1.p2.5.m5.2.2.2.2.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.cmml"><mrow id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.cmml"><msub id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.cmml"><mo rspace="0.167em" id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.2.cmml">∇</mo><mi id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.3.cmml">θ</mi></msub><msub id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.cmml"><mi id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.2.cmml">L</mi><mi id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.2.2.2.2.2.2.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.3.cmml">​</mo><mrow id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.4" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.2" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.3" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.5" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.2b"><apply id="S3.SS1.p2.5.m5.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2"><ci id="S3.SS1.p2.5.m5.2.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.3">←</ci><ci id="S3.SS1.p2.5.m5.2.2.4.cmml" xref="S3.SS1.p2.5.m5.2.2.4">𝜃</ci><apply id="S3.SS1.p2.5.m5.2.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2"><minus id="S3.SS1.p2.5.m5.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.3"></minus><ci id="S3.SS1.p2.5.m5.2.2.2.4.cmml" xref="S3.SS1.p2.5.m5.2.2.2.4">𝜃</ci><apply id="S3.SS1.p2.5.m5.2.2.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2"><times id="S3.SS1.p2.5.m5.2.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.3"></times><ci id="S3.SS1.p2.5.m5.2.2.2.2.4.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.4">𝜂</ci><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2"><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.2.2.2.2.3.1.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3">superscript</csymbol><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.1.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3">subscript</csymbol><sum id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.2"></sum><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3"><eq id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.1.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.1"></eq><ci id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.5.m5.2.2.2.2.2.3.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.3.3">𝑁</ci></apply><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2"><times id="S3.SS1.p2.5.m5.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.3"></times><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4"><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.1.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.2">∇</ci><ci id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.1.3">𝜃</ci></apply><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.1.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2">subscript</csymbol><ci id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.2">𝐿</ci><ci id="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.4.2.3">𝜃</ci></apply></apply><interval closure="open" id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2"><apply id="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.5.m5.2.2.2.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.2c">\theta\leftarrow\theta-\eta\sum_{i=1}^{N}\nabla_{\theta}L_{\theta}(x_{i},y_{i})</annotation></semantics></math>.
This aggregation algorithm is called <em id="S3.SS1.p2.11.1" class="ltx_emph ltx_font_italic"><abbr title="Federated Stochastic Gradient Descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></em>. Alternatively, the parties can also train for several epochs locally and upload the model parameters: <math id="S3.SS1.p2.6.m6.2" class="ltx_Math" alttext="\theta^{i}\leftarrow\theta^{i}-\eta\nabla_{\theta^{i}}L_{\theta^{i}}(x_{i},y_{i})" display="inline"><semantics id="S3.SS1.p2.6.m6.2a"><mrow id="S3.SS1.p2.6.m6.2.2" xref="S3.SS1.p2.6.m6.2.2.cmml"><msup id="S3.SS1.p2.6.m6.2.2.4" xref="S3.SS1.p2.6.m6.2.2.4.cmml"><mi id="S3.SS1.p2.6.m6.2.2.4.2" xref="S3.SS1.p2.6.m6.2.2.4.2.cmml">θ</mi><mi id="S3.SS1.p2.6.m6.2.2.4.3" xref="S3.SS1.p2.6.m6.2.2.4.3.cmml">i</mi></msup><mo stretchy="false" id="S3.SS1.p2.6.m6.2.2.3" xref="S3.SS1.p2.6.m6.2.2.3.cmml">←</mo><mrow id="S3.SS1.p2.6.m6.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.cmml"><msup id="S3.SS1.p2.6.m6.2.2.2.4" xref="S3.SS1.p2.6.m6.2.2.2.4.cmml"><mi id="S3.SS1.p2.6.m6.2.2.2.4.2" xref="S3.SS1.p2.6.m6.2.2.2.4.2.cmml">θ</mi><mi id="S3.SS1.p2.6.m6.2.2.2.4.3" xref="S3.SS1.p2.6.m6.2.2.2.4.3.cmml">i</mi></msup><mo id="S3.SS1.p2.6.m6.2.2.2.3" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml">−</mo><mrow id="S3.SS1.p2.6.m6.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.cmml"><mi id="S3.SS1.p2.6.m6.2.2.2.2.4" xref="S3.SS1.p2.6.m6.2.2.2.2.4.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="S3.SS1.p2.6.m6.2.2.2.2.3" xref="S3.SS1.p2.6.m6.2.2.2.2.3.cmml">​</mo><mrow id="S3.SS1.p2.6.m6.2.2.2.2.5" xref="S3.SS1.p2.6.m6.2.2.2.2.5.cmml"><msub id="S3.SS1.p2.6.m6.2.2.2.2.5.1" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.cmml"><mo rspace="0.167em" id="S3.SS1.p2.6.m6.2.2.2.2.5.1.2" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.2.cmml">∇</mo><msup id="S3.SS1.p2.6.m6.2.2.2.2.5.1.3" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.cmml"><mi id="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.2" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.2.cmml">θ</mi><mi id="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.3" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.3.cmml">i</mi></msup></msub><msub id="S3.SS1.p2.6.m6.2.2.2.2.5.2" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.cmml"><mi id="S3.SS1.p2.6.m6.2.2.2.2.5.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.2.cmml">L</mi><msup id="S3.SS1.p2.6.m6.2.2.2.2.5.2.3" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.cmml"><mi id="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.2" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.2.cmml">θ</mi><mi id="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.3" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.3.cmml">i</mi></msup></msub></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p2.6.m6.2.2.2.2.3a" xref="S3.SS1.p2.6.m6.2.2.2.2.3.cmml">​</mo><mrow id="S3.SS1.p2.6.m6.2.2.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.6.m6.2.2.2.2.2.2.3" xref="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.2" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.3" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p2.6.m6.2.2.2.2.2.2.4" xref="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p2.6.m6.2.2.2.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.3" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p2.6.m6.2.2.2.2.2.2.5" xref="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.2b"><apply id="S3.SS1.p2.6.m6.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2"><ci id="S3.SS1.p2.6.m6.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.3">←</ci><apply id="S3.SS1.p2.6.m6.2.2.4.cmml" xref="S3.SS1.p2.6.m6.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.4.1.cmml" xref="S3.SS1.p2.6.m6.2.2.4">superscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.4.2.cmml" xref="S3.SS1.p2.6.m6.2.2.4.2">𝜃</ci><ci id="S3.SS1.p2.6.m6.2.2.4.3.cmml" xref="S3.SS1.p2.6.m6.2.2.4.3">𝑖</ci></apply><apply id="S3.SS1.p2.6.m6.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2"><minus id="S3.SS1.p2.6.m6.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.3"></minus><apply id="S3.SS1.p2.6.m6.2.2.2.4.cmml" xref="S3.SS1.p2.6.m6.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.2.4.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.4">superscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.2.4.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.4.2">𝜃</ci><ci id="S3.SS1.p2.6.m6.2.2.2.4.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.4.3">𝑖</ci></apply><apply id="S3.SS1.p2.6.m6.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2"><times id="S3.SS1.p2.6.m6.2.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.3"></times><ci id="S3.SS1.p2.6.m6.2.2.2.2.4.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.4">𝜂</ci><apply id="S3.SS1.p2.6.m6.2.2.2.2.5.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5"><apply id="S3.SS1.p2.6.m6.2.2.2.2.5.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.2.2.5.1.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.2.2.5.1.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.2">∇</ci><apply id="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.3">superscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.2">𝜃</ci><ci id="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.1.3.3">𝑖</ci></apply></apply><apply id="S3.SS1.p2.6.m6.2.2.2.2.5.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.2.2.5.2.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2">subscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.2.2.5.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.2">𝐿</ci><apply id="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.3">superscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.2">𝜃</ci><ci id="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.5.2.3.3">𝑖</ci></apply></apply></apply><interval closure="open" id="S3.SS1.p2.6.m6.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2"><apply id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.2c">\theta^{i}\leftarrow\theta^{i}-\eta\nabla_{\theta^{i}}L_{\theta^{i}}(x_{i},y_{i})</annotation></semantics></math>
to the aggregator. The aggregator computes the weighted average of model parameters <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="\theta\leftarrow\sum_{i=1}^{N}\frac{n_{i}}{n}\theta^{i}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">θ</mi><mo rspace="0.111em" stretchy="false" id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">←</mo><mrow id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><msubsup id="S3.SS1.p2.7.m7.1.1.3.1" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml"><mo id="S3.SS1.p2.7.m7.1.1.3.1.2.2" xref="S3.SS1.p2.7.m7.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.SS1.p2.7.m7.1.1.3.1.2.3" xref="S3.SS1.p2.7.m7.1.1.3.1.2.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.1.2.3.2" xref="S3.SS1.p2.7.m7.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.SS1.p2.7.m7.1.1.3.1.2.3.1" xref="S3.SS1.p2.7.m7.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.SS1.p2.7.m7.1.1.3.1.2.3.3" xref="S3.SS1.p2.7.m7.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.7.m7.1.1.3.1.3" xref="S3.SS1.p2.7.m7.1.1.3.1.3.cmml">N</mi></msubsup><mrow id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml"><mfrac id="S3.SS1.p2.7.m7.1.1.3.2.2" xref="S3.SS1.p2.7.m7.1.1.3.2.2.cmml"><msub id="S3.SS1.p2.7.m7.1.1.3.2.2.2" xref="S3.SS1.p2.7.m7.1.1.3.2.2.2.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.2.2.2.2" xref="S3.SS1.p2.7.m7.1.1.3.2.2.2.2.cmml">n</mi><mi id="S3.SS1.p2.7.m7.1.1.3.2.2.2.3" xref="S3.SS1.p2.7.m7.1.1.3.2.2.2.3.cmml">i</mi></msub><mi id="S3.SS1.p2.7.m7.1.1.3.2.2.3" xref="S3.SS1.p2.7.m7.1.1.3.2.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.SS1.p2.7.m7.1.1.3.2.1" xref="S3.SS1.p2.7.m7.1.1.3.2.1.cmml">​</mo><msup id="S3.SS1.p2.7.m7.1.1.3.2.3" xref="S3.SS1.p2.7.m7.1.1.3.2.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.2.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.3.2.cmml">θ</mi><mi id="S3.SS1.p2.7.m7.1.1.3.2.3.3" xref="S3.SS1.p2.7.m7.1.1.3.2.3.3.cmml">i</mi></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><ci id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1">←</ci><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝜃</ci><apply id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><apply id="S3.SS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1">superscript</csymbol><apply id="S3.SS1.p2.7.m7.1.1.3.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.1.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1">subscript</csymbol><sum id="S3.SS1.p2.7.m7.1.1.3.1.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1.2.2"></sum><apply id="S3.SS1.p2.7.m7.1.1.3.1.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1.2.3"><eq id="S3.SS1.p2.7.m7.1.1.3.1.2.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1.2.3.1"></eq><ci id="S3.SS1.p2.7.m7.1.1.3.1.2.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.SS1.p2.7.m7.1.1.3.1.2.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.7.m7.1.1.3.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.1.3">𝑁</ci></apply><apply id="S3.SS1.p2.7.m7.1.1.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2"><times id="S3.SS1.p2.7.m7.1.1.3.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.1"></times><apply id="S3.SS1.p2.7.m7.1.1.3.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.2"><divide id="S3.SS1.p2.7.m7.1.1.3.2.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.2"></divide><apply id="S3.SS1.p2.7.m7.1.1.3.2.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.2.2.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.2.2.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.2.2.2">𝑛</ci><ci id="S3.SS1.p2.7.m7.1.1.3.2.2.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.2.2.3">𝑖</ci></apply><ci id="S3.SS1.p2.7.m7.1.1.3.2.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.2.3">𝑛</ci></apply><apply id="S3.SS1.p2.7.m7.1.1.3.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.2.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.3">superscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.2.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.3.2">𝜃</ci><ci id="S3.SS1.p2.7.m7.1.1.3.2.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\theta\leftarrow\sum_{i=1}^{N}\frac{n_{i}}{n}\theta^{i}</annotation></semantics></math>, where <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="n_{i}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><msub id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">n</mi><mi id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝑛</ci><ci id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">n_{i}</annotation></semantics></math> is the size of training data on party <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">i</annotation></semantics></math> and <math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><mi id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><ci id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">n</annotation></semantics></math> is the sum of all <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="n_{i}" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><msub id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml"><mi id="S3.SS1.p2.11.m11.1.1.2" xref="S3.SS1.p2.11.m11.1.1.2.cmml">n</mi><mi id="S3.SS1.p2.11.m11.1.1.3" xref="S3.SS1.p2.11.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><apply id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p2.11.m11.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1.2">𝑛</ci><ci id="S3.SS1.p2.11.m11.1.1.3.cmml" xref="S3.SS1.p2.11.m11.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">n_{i}</annotation></semantics></math>. Then, the aggregator sends the aggregated model parameters back to the parties for synchronization. This aggregation algorithm is called <em id="S3.SS1.p2.11.2" class="ltx_emph ltx_font_italic"><abbr title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></em>.
<em id="S3.SS1.p2.11.3" class="ltx_emph ltx_font_italic"><abbr title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></em> and <em id="S3.SS1.p2.11.4" class="ltx_emph ltx_font_italic"><abbr title="Federated Stochastic Gradient Descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></em> are equivalent if we train only one batch of data in a single <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training round and synchronize model parameters, as gradients can be computed from the difference of two successive model parameter uploads.
As <em id="S3.SS1.p2.11.5" class="ltx_emph ltx_font_italic"><abbr title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></em> allows parties to batch multiple SGD iterations before synchronizing updates, it would be more challenging for data reconstruction attacks to succeed.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Data Reconstruction Attacks</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Exchanging model updates in <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic"><abbr title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></em> and <em id="S3.SS2.p1.1.2" class="ltx_emph ltx_font_italic"><abbr title="Federated Stochastic Gradient Descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></em> was considered privacy-preserving as original training data were not directly included in communications. However, recent attacks, e.g.,  <span title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Deep Leakage from Gradients</span></span> (<abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite>,  <span title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Improved Deep Leakage from Gradients</span></span> (<abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>, and  <span title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Inverting Gradients</span></span> (<abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>, have demonstrated that it is possible to derive training data samples from the model updates.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">In <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite>, the attack reconstructed a training sample <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">x</annotation></semantics></math> based on the shared gradient updates. The attack randomly initialized a dummy input <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="x^{\prime}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msup id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">x</mi><mo id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑥</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">x^{\prime}</annotation></semantics></math> and label <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="y^{\prime}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msup id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">y</mi><mo id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝑦</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">y^{\prime}</annotation></semantics></math>, which were fed into the model in order to compute the loss gradients. Then, the attack used an L-BGFS solver to minimize the following cost function in order to reconstruct <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">x</annotation></semantics></math>:</p>
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.5" class="ltx_Math" alttext="\displaystyle\underset{x^{\prime},y^{\prime}}{\operatorname{argmin}}||\nabla_{\theta}L_{\theta}(x^{\prime},y^{\prime})-\nabla_{\theta}L_{\theta}(x,y)||^{2}" display="inline"><semantics id="S3.Ex1.m1.5a"><mrow id="S3.Ex1.m1.5.5" xref="S3.Ex1.m1.5.5.cmml"><munder accentunder="true" id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mi id="S3.Ex1.m1.2.2.3" xref="S3.Ex1.m1.2.2.3.cmml">argmin</mi><mrow id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.3.cmml"><msup id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml">x</mi><mo id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.Ex1.m1.2.2.2.2.3" xref="S3.Ex1.m1.2.2.2.3.cmml">,</mo><msup id="S3.Ex1.m1.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.cmml"><mi id="S3.Ex1.m1.2.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.2.cmml">y</mi><mo id="S3.Ex1.m1.2.2.2.2.2.3" xref="S3.Ex1.m1.2.2.2.2.2.3.cmml">′</mo></msup></mrow></munder><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.5.5.2" xref="S3.Ex1.m1.5.5.2.cmml">​</mo><msup id="S3.Ex1.m1.5.5.1" xref="S3.Ex1.m1.5.5.1.cmml"><mrow id="S3.Ex1.m1.5.5.1.1.1" xref="S3.Ex1.m1.5.5.1.1.2.cmml"><mo stretchy="false" id="S3.Ex1.m1.5.5.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.2.1.cmml">‖</mo><mrow id="S3.Ex1.m1.5.5.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.5.5.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.cmml"><mrow id="S3.Ex1.m1.5.5.1.1.1.1.2.4" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.cmml"><msub id="S3.Ex1.m1.5.5.1.1.1.1.2.4.1" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.cmml"><mo rspace="0.167em" id="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.2.cmml">∇</mo><mi id="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.3" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.3.cmml">θ</mi></msub><msub id="S3.Ex1.m1.5.5.1.1.1.1.2.4.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.2.cmml">L</mi><mi id="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.3" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.5.5.1.1.1.1.2.3" xref="S3.Ex1.m1.5.5.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.3" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.3.cmml">(</mo><msup id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.4" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.3.cmml">,</mo><msup id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.2.cmml">y</mi><mo id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.3" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.3.cmml">′</mo></msup><mo stretchy="false" id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.5" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.5.5.1.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.1.1.3.cmml">−</mo><mrow id="S3.Ex1.m1.5.5.1.1.1.1.4" xref="S3.Ex1.m1.5.5.1.1.1.1.4.cmml"><mrow id="S3.Ex1.m1.5.5.1.1.1.1.4.2" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.cmml"><msub id="S3.Ex1.m1.5.5.1.1.1.1.4.2.1" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.cmml"><mo rspace="0.167em" id="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.2.cmml">∇</mo><mi id="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.3" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.3.cmml">θ</mi></msub><msub id="S3.Ex1.m1.5.5.1.1.1.1.4.2.2" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.2" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.2.cmml">L</mi><mi id="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.3" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.5.5.1.1.1.1.4.1" xref="S3.Ex1.m1.5.5.1.1.1.1.4.1.cmml">​</mo><mrow id="S3.Ex1.m1.5.5.1.1.1.1.4.3.2" xref="S3.Ex1.m1.5.5.1.1.1.1.4.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.5.5.1.1.1.1.4.3.2.1" xref="S3.Ex1.m1.5.5.1.1.1.1.4.3.1.cmml">(</mo><mi id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml">x</mi><mo id="S3.Ex1.m1.5.5.1.1.1.1.4.3.2.2" xref="S3.Ex1.m1.5.5.1.1.1.1.4.3.1.cmml">,</mo><mi id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml">y</mi><mo stretchy="false" id="S3.Ex1.m1.5.5.1.1.1.1.4.3.2.3" xref="S3.Ex1.m1.5.5.1.1.1.1.4.3.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.Ex1.m1.5.5.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.Ex1.m1.5.5.1.3" xref="S3.Ex1.m1.5.5.1.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.5b"><apply id="S3.Ex1.m1.5.5.cmml" xref="S3.Ex1.m1.5.5"><times id="S3.Ex1.m1.5.5.2.cmml" xref="S3.Ex1.m1.5.5.2"></times><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><list id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2"><apply id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2">𝑥</ci><ci id="S3.Ex1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3">′</ci></apply><apply id="S3.Ex1.m1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2">superscript</csymbol><ci id="S3.Ex1.m1.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2">𝑦</ci><ci id="S3.Ex1.m1.2.2.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.3">′</ci></apply></list><ci id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.3">argmin</ci></apply><apply id="S3.Ex1.m1.5.5.1.cmml" xref="S3.Ex1.m1.5.5.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.2.cmml" xref="S3.Ex1.m1.5.5.1">superscript</csymbol><apply id="S3.Ex1.m1.5.5.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m1.5.5.1.1.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.2">norm</csymbol><apply id="S3.Ex1.m1.5.5.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1"><minus id="S3.Ex1.m1.5.5.1.1.1.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.3"></minus><apply id="S3.Ex1.m1.5.5.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2"><times id="S3.Ex1.m1.5.5.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.3"></times><apply id="S3.Ex1.m1.5.5.1.1.1.1.2.4.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4"><apply id="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.1">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.2">∇</ci><ci id="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.1.3">𝜃</ci></apply><apply id="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.2">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.2">𝐿</ci><ci id="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.4.2.3">𝜃</ci></apply></apply><interval closure="open" id="S3.Ex1.m1.5.5.1.1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.2"><apply id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1.1.1.1.3">′</ci></apply><apply id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2">superscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.2">𝑦</ci><ci id="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.2.2.3">′</ci></apply></interval></apply><apply id="S3.Ex1.m1.5.5.1.1.1.1.4.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4"><times id="S3.Ex1.m1.5.5.1.1.1.1.4.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.1"></times><apply id="S3.Ex1.m1.5.5.1.1.1.1.4.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2"><apply id="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.1">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.2">∇</ci><ci id="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.1.3">𝜃</ci></apply><apply id="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.2">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.2">𝐿</ci><ci id="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.2.2.3">𝜃</ci></apply></apply><interval closure="open" id="S3.Ex1.m1.5.5.1.1.1.1.4.3.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.4.3.2"><ci id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3">𝑥</ci><ci id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4">𝑦</ci></interval></apply></apply></apply><cn type="integer" id="S3.Ex1.m1.5.5.1.3.cmml" xref="S3.Ex1.m1.5.5.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.5c">\displaystyle\underset{x^{\prime},y^{\prime}}{\operatorname{argmin}}||\nabla_{\theta}L_{\theta}(x^{\prime},y^{\prime})-\nabla_{\theta}L_{\theta}(x,y)||^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.5" class="ltx_p">As the differentiation requires second order derivatives, the attack only works on models that are twice differentiable.
Zhao et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite> found that although the <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> attack was effective, the reconstructions and labels generated after optimization were sometimes of low quality and incorrect respectively. In their <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> attack, the authors demonstrated that the signs of the loss gradients with respect to the correct label are always opposite to the signs of the other labels. Thus, the ground truth labels can be inferred based on the model updates, which improve reconstruction quality.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p"><abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> makes two major modifications to <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> and <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr>. First, the authors asserted that it is not the magnitudes of the gradients that are important, but rather the directions of the gradients. Based on this reasoning, they used a cosine distance cost function, which encouraged the attack to find reconstructions that resulted in the same changes in gradients’ directions. Their new cost function is:</p>
<table id="A2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex2.m1.13" class="ltx_Math" alttext="\displaystyle\underset{x^{\prime}\in[0,1]}{\operatorname{argmin}}\;1-\frac{\langle\nabla_{\theta}L_{\theta}(x^{\prime},y),\nabla_{\theta}L_{\theta}(x,y)\rangle}{||\nabla_{\theta}L_{\theta}(x^{\prime},y)||||\nabla_{\theta}L_{\theta}(x,y)||}+\alpha TV(x^{\prime})" display="inline"><semantics id="S3.Ex2.m1.13a"><mrow id="S3.Ex2.m1.13.13" xref="S3.Ex2.m1.13.13.cmml"><mrow id="S3.Ex2.m1.13.13.3" xref="S3.Ex2.m1.13.13.3.cmml"><mrow id="S3.Ex2.m1.13.13.3.2" xref="S3.Ex2.m1.13.13.3.2.cmml"><munder accentunder="true" id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml"><mi id="S3.Ex2.m1.2.2.3" xref="S3.Ex2.m1.2.2.3.cmml">argmin</mi><mrow id="S3.Ex2.m1.2.2.2" xref="S3.Ex2.m1.2.2.2.cmml"><msup id="S3.Ex2.m1.2.2.2.4" xref="S3.Ex2.m1.2.2.2.4.cmml"><mi id="S3.Ex2.m1.2.2.2.4.2" xref="S3.Ex2.m1.2.2.2.4.2.cmml">x</mi><mo id="S3.Ex2.m1.2.2.2.4.3" xref="S3.Ex2.m1.2.2.2.4.3.cmml">′</mo></msup><mo id="S3.Ex2.m1.2.2.2.3" xref="S3.Ex2.m1.2.2.2.3.cmml">∈</mo><mrow id="S3.Ex2.m1.2.2.2.5.2" xref="S3.Ex2.m1.2.2.2.5.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.2.2.2.5.2.1" xref="S3.Ex2.m1.2.2.2.5.1.cmml">[</mo><mn id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml">0</mn><mo id="S3.Ex2.m1.2.2.2.5.2.2" xref="S3.Ex2.m1.2.2.2.5.1.cmml">,</mo><mn id="S3.Ex2.m1.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.cmml">1</mn><mo stretchy="false" id="S3.Ex2.m1.2.2.2.5.2.3" xref="S3.Ex2.m1.2.2.2.5.1.cmml">]</mo></mrow></mrow></munder><mo lspace="0.167em" rspace="0em" id="S3.Ex2.m1.13.13.3.2.1" xref="S3.Ex2.m1.13.13.3.2.1.cmml">​</mo><mn id="S3.Ex2.m1.13.13.3.2.2" xref="S3.Ex2.m1.13.13.3.2.2.cmml"> 1</mn></mrow><mo id="S3.Ex2.m1.13.13.3.1" xref="S3.Ex2.m1.13.13.3.1.cmml">−</mo><mstyle displaystyle="true" id="S3.Ex2.m1.12.12" xref="S3.Ex2.m1.12.12.cmml"><mfrac id="S3.Ex2.m1.12.12a" xref="S3.Ex2.m1.12.12.cmml"><mrow id="S3.Ex2.m1.7.7.5.5" xref="S3.Ex2.m1.7.7.5.6.cmml"><mo stretchy="false" id="S3.Ex2.m1.7.7.5.5.3" xref="S3.Ex2.m1.7.7.5.6.cmml">⟨</mo><mrow id="S3.Ex2.m1.6.6.4.4.1" xref="S3.Ex2.m1.6.6.4.4.1.cmml"><mrow id="S3.Ex2.m1.6.6.4.4.1.3" xref="S3.Ex2.m1.6.6.4.4.1.3.cmml"><msub id="S3.Ex2.m1.6.6.4.4.1.3.1" xref="S3.Ex2.m1.6.6.4.4.1.3.1.cmml"><mo rspace="0.167em" id="S3.Ex2.m1.6.6.4.4.1.3.1.2" xref="S3.Ex2.m1.6.6.4.4.1.3.1.2.cmml">∇</mo><mi id="S3.Ex2.m1.6.6.4.4.1.3.1.3" xref="S3.Ex2.m1.6.6.4.4.1.3.1.3.cmml">θ</mi></msub><msub id="S3.Ex2.m1.6.6.4.4.1.3.2" xref="S3.Ex2.m1.6.6.4.4.1.3.2.cmml"><mi id="S3.Ex2.m1.6.6.4.4.1.3.2.2" xref="S3.Ex2.m1.6.6.4.4.1.3.2.2.cmml">L</mi><mi id="S3.Ex2.m1.6.6.4.4.1.3.2.3" xref="S3.Ex2.m1.6.6.4.4.1.3.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.4.4.1.2" xref="S3.Ex2.m1.6.6.4.4.1.2.cmml">​</mo><mrow id="S3.Ex2.m1.6.6.4.4.1.1.1" xref="S3.Ex2.m1.6.6.4.4.1.1.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.6.6.4.4.1.1.1.2" xref="S3.Ex2.m1.6.6.4.4.1.1.2.cmml">(</mo><msup id="S3.Ex2.m1.6.6.4.4.1.1.1.1" xref="S3.Ex2.m1.6.6.4.4.1.1.1.1.cmml"><mi id="S3.Ex2.m1.6.6.4.4.1.1.1.1.2" xref="S3.Ex2.m1.6.6.4.4.1.1.1.1.2.cmml">x</mi><mo id="S3.Ex2.m1.6.6.4.4.1.1.1.1.3" xref="S3.Ex2.m1.6.6.4.4.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.Ex2.m1.6.6.4.4.1.1.1.3" xref="S3.Ex2.m1.6.6.4.4.1.1.2.cmml">,</mo><mi id="S3.Ex2.m1.3.3.1.1" xref="S3.Ex2.m1.3.3.1.1.cmml">y</mi><mo stretchy="false" id="S3.Ex2.m1.6.6.4.4.1.1.1.4" xref="S3.Ex2.m1.6.6.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.7.7.5.5.4" xref="S3.Ex2.m1.7.7.5.6.cmml">,</mo><mrow id="S3.Ex2.m1.7.7.5.5.2" xref="S3.Ex2.m1.7.7.5.5.2.cmml"><mrow id="S3.Ex2.m1.7.7.5.5.2.2" xref="S3.Ex2.m1.7.7.5.5.2.2.cmml"><msub id="S3.Ex2.m1.7.7.5.5.2.2.1" xref="S3.Ex2.m1.7.7.5.5.2.2.1.cmml"><mo rspace="0.167em" id="S3.Ex2.m1.7.7.5.5.2.2.1.2" xref="S3.Ex2.m1.7.7.5.5.2.2.1.2.cmml">∇</mo><mi id="S3.Ex2.m1.7.7.5.5.2.2.1.3" xref="S3.Ex2.m1.7.7.5.5.2.2.1.3.cmml">θ</mi></msub><msub id="S3.Ex2.m1.7.7.5.5.2.2.2" xref="S3.Ex2.m1.7.7.5.5.2.2.2.cmml"><mi id="S3.Ex2.m1.7.7.5.5.2.2.2.2" xref="S3.Ex2.m1.7.7.5.5.2.2.2.2.cmml">L</mi><mi id="S3.Ex2.m1.7.7.5.5.2.2.2.3" xref="S3.Ex2.m1.7.7.5.5.2.2.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.7.7.5.5.2.1" xref="S3.Ex2.m1.7.7.5.5.2.1.cmml">​</mo><mrow id="S3.Ex2.m1.7.7.5.5.2.3.2" xref="S3.Ex2.m1.7.7.5.5.2.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.7.7.5.5.2.3.2.1" xref="S3.Ex2.m1.7.7.5.5.2.3.1.cmml">(</mo><mi id="S3.Ex2.m1.4.4.2.2" xref="S3.Ex2.m1.4.4.2.2.cmml">x</mi><mo id="S3.Ex2.m1.7.7.5.5.2.3.2.2" xref="S3.Ex2.m1.7.7.5.5.2.3.1.cmml">,</mo><mi id="S3.Ex2.m1.5.5.3.3" xref="S3.Ex2.m1.5.5.3.3.cmml">y</mi><mo stretchy="false" id="S3.Ex2.m1.7.7.5.5.2.3.2.3" xref="S3.Ex2.m1.7.7.5.5.2.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex2.m1.7.7.5.5.5" xref="S3.Ex2.m1.7.7.5.6.cmml">⟩</mo></mrow><mrow id="S3.Ex2.m1.12.12.10" xref="S3.Ex2.m1.12.12.10.cmml"><mrow id="S3.Ex2.m1.11.11.9.4.1" xref="S3.Ex2.m1.11.11.9.4.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.11.11.9.4.1.2" xref="S3.Ex2.m1.11.11.9.4.2.1.cmml">‖</mo><mrow id="S3.Ex2.m1.11.11.9.4.1.1" xref="S3.Ex2.m1.11.11.9.4.1.1.cmml"><mrow id="S3.Ex2.m1.11.11.9.4.1.1.3" xref="S3.Ex2.m1.11.11.9.4.1.1.3.cmml"><msub id="S3.Ex2.m1.11.11.9.4.1.1.3.1" xref="S3.Ex2.m1.11.11.9.4.1.1.3.1.cmml"><mo rspace="0.167em" id="S3.Ex2.m1.11.11.9.4.1.1.3.1.2" xref="S3.Ex2.m1.11.11.9.4.1.1.3.1.2.cmml">∇</mo><mi id="S3.Ex2.m1.11.11.9.4.1.1.3.1.3" xref="S3.Ex2.m1.11.11.9.4.1.1.3.1.3.cmml">θ</mi></msub><msub id="S3.Ex2.m1.11.11.9.4.1.1.3.2" xref="S3.Ex2.m1.11.11.9.4.1.1.3.2.cmml"><mi id="S3.Ex2.m1.11.11.9.4.1.1.3.2.2" xref="S3.Ex2.m1.11.11.9.4.1.1.3.2.2.cmml">L</mi><mi id="S3.Ex2.m1.11.11.9.4.1.1.3.2.3" xref="S3.Ex2.m1.11.11.9.4.1.1.3.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.11.11.9.4.1.1.2" xref="S3.Ex2.m1.11.11.9.4.1.1.2.cmml">​</mo><mrow id="S3.Ex2.m1.11.11.9.4.1.1.1.1" xref="S3.Ex2.m1.11.11.9.4.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.11.11.9.4.1.1.1.1.2" xref="S3.Ex2.m1.11.11.9.4.1.1.1.2.cmml">(</mo><msup id="S3.Ex2.m1.11.11.9.4.1.1.1.1.1" xref="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.2" xref="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.2.cmml">x</mi><mo id="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.3" xref="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.Ex2.m1.11.11.9.4.1.1.1.1.3" xref="S3.Ex2.m1.11.11.9.4.1.1.1.2.cmml">,</mo><mi id="S3.Ex2.m1.8.8.6.1" xref="S3.Ex2.m1.8.8.6.1.cmml">y</mi><mo stretchy="false" id="S3.Ex2.m1.11.11.9.4.1.1.1.1.4" xref="S3.Ex2.m1.11.11.9.4.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex2.m1.11.11.9.4.1.3" xref="S3.Ex2.m1.11.11.9.4.2.1.cmml">‖</mo></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.12.12.10.6" xref="S3.Ex2.m1.12.12.10.6.cmml">​</mo><mrow id="S3.Ex2.m1.12.12.10.5.1" xref="S3.Ex2.m1.12.12.10.5.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.12.12.10.5.1.2" xref="S3.Ex2.m1.12.12.10.5.2.1.cmml">‖</mo><mrow id="S3.Ex2.m1.12.12.10.5.1.1" xref="S3.Ex2.m1.12.12.10.5.1.1.cmml"><mrow id="S3.Ex2.m1.12.12.10.5.1.1.2" xref="S3.Ex2.m1.12.12.10.5.1.1.2.cmml"><msub id="S3.Ex2.m1.12.12.10.5.1.1.2.1" xref="S3.Ex2.m1.12.12.10.5.1.1.2.1.cmml"><mo rspace="0.167em" id="S3.Ex2.m1.12.12.10.5.1.1.2.1.2" xref="S3.Ex2.m1.12.12.10.5.1.1.2.1.2.cmml">∇</mo><mi id="S3.Ex2.m1.12.12.10.5.1.1.2.1.3" xref="S3.Ex2.m1.12.12.10.5.1.1.2.1.3.cmml">θ</mi></msub><msub id="S3.Ex2.m1.12.12.10.5.1.1.2.2" xref="S3.Ex2.m1.12.12.10.5.1.1.2.2.cmml"><mi id="S3.Ex2.m1.12.12.10.5.1.1.2.2.2" xref="S3.Ex2.m1.12.12.10.5.1.1.2.2.2.cmml">L</mi><mi id="S3.Ex2.m1.12.12.10.5.1.1.2.2.3" xref="S3.Ex2.m1.12.12.10.5.1.1.2.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.12.12.10.5.1.1.1" xref="S3.Ex2.m1.12.12.10.5.1.1.1.cmml">​</mo><mrow id="S3.Ex2.m1.12.12.10.5.1.1.3.2" xref="S3.Ex2.m1.12.12.10.5.1.1.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.12.12.10.5.1.1.3.2.1" xref="S3.Ex2.m1.12.12.10.5.1.1.3.1.cmml">(</mo><mi id="S3.Ex2.m1.9.9.7.2" xref="S3.Ex2.m1.9.9.7.2.cmml">x</mi><mo id="S3.Ex2.m1.12.12.10.5.1.1.3.2.2" xref="S3.Ex2.m1.12.12.10.5.1.1.3.1.cmml">,</mo><mi id="S3.Ex2.m1.10.10.8.3" xref="S3.Ex2.m1.10.10.8.3.cmml">y</mi><mo stretchy="false" id="S3.Ex2.m1.12.12.10.5.1.1.3.2.3" xref="S3.Ex2.m1.12.12.10.5.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex2.m1.12.12.10.5.1.3" xref="S3.Ex2.m1.12.12.10.5.2.1.cmml">‖</mo></mrow></mrow></mfrac></mstyle></mrow><mo id="S3.Ex2.m1.13.13.2" xref="S3.Ex2.m1.13.13.2.cmml">+</mo><mrow id="S3.Ex2.m1.13.13.1" xref="S3.Ex2.m1.13.13.1.cmml"><mi id="S3.Ex2.m1.13.13.1.3" xref="S3.Ex2.m1.13.13.1.3.cmml">α</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.13.13.1.2" xref="S3.Ex2.m1.13.13.1.2.cmml">​</mo><mi id="S3.Ex2.m1.13.13.1.4" xref="S3.Ex2.m1.13.13.1.4.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.13.13.1.2a" xref="S3.Ex2.m1.13.13.1.2.cmml">​</mo><mi id="S3.Ex2.m1.13.13.1.5" xref="S3.Ex2.m1.13.13.1.5.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.13.13.1.2b" xref="S3.Ex2.m1.13.13.1.2.cmml">​</mo><mrow id="S3.Ex2.m1.13.13.1.1.1" xref="S3.Ex2.m1.13.13.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.13.13.1.1.1.2" xref="S3.Ex2.m1.13.13.1.1.1.1.cmml">(</mo><msup id="S3.Ex2.m1.13.13.1.1.1.1" xref="S3.Ex2.m1.13.13.1.1.1.1.cmml"><mi id="S3.Ex2.m1.13.13.1.1.1.1.2" xref="S3.Ex2.m1.13.13.1.1.1.1.2.cmml">x</mi><mo id="S3.Ex2.m1.13.13.1.1.1.1.3" xref="S3.Ex2.m1.13.13.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.Ex2.m1.13.13.1.1.1.3" xref="S3.Ex2.m1.13.13.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.13b"><apply id="S3.Ex2.m1.13.13.cmml" xref="S3.Ex2.m1.13.13"><plus id="S3.Ex2.m1.13.13.2.cmml" xref="S3.Ex2.m1.13.13.2"></plus><apply id="S3.Ex2.m1.13.13.3.cmml" xref="S3.Ex2.m1.13.13.3"><minus id="S3.Ex2.m1.13.13.3.1.cmml" xref="S3.Ex2.m1.13.13.3.1"></minus><apply id="S3.Ex2.m1.13.13.3.2.cmml" xref="S3.Ex2.m1.13.13.3.2"><times id="S3.Ex2.m1.13.13.3.2.1.cmml" xref="S3.Ex2.m1.13.13.3.2.1"></times><apply id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2"><apply id="S3.Ex2.m1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2"><in id="S3.Ex2.m1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.3"></in><apply id="S3.Ex2.m1.2.2.2.4.cmml" xref="S3.Ex2.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.2.4.1.cmml" xref="S3.Ex2.m1.2.2.2.4">superscript</csymbol><ci id="S3.Ex2.m1.2.2.2.4.2.cmml" xref="S3.Ex2.m1.2.2.2.4.2">𝑥</ci><ci id="S3.Ex2.m1.2.2.2.4.3.cmml" xref="S3.Ex2.m1.2.2.2.4.3">′</ci></apply><interval closure="closed" id="S3.Ex2.m1.2.2.2.5.1.cmml" xref="S3.Ex2.m1.2.2.2.5.2"><cn type="integer" id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1">0</cn><cn type="integer" id="S3.Ex2.m1.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2">1</cn></interval></apply><ci id="S3.Ex2.m1.2.2.3.cmml" xref="S3.Ex2.m1.2.2.3">argmin</ci></apply><cn type="integer" id="S3.Ex2.m1.13.13.3.2.2.cmml" xref="S3.Ex2.m1.13.13.3.2.2">1</cn></apply><apply id="S3.Ex2.m1.12.12.cmml" xref="S3.Ex2.m1.12.12"><divide id="S3.Ex2.m1.12.12.11.cmml" xref="S3.Ex2.m1.12.12"></divide><list id="S3.Ex2.m1.7.7.5.6.cmml" xref="S3.Ex2.m1.7.7.5.5"><apply id="S3.Ex2.m1.6.6.4.4.1.cmml" xref="S3.Ex2.m1.6.6.4.4.1"><times id="S3.Ex2.m1.6.6.4.4.1.2.cmml" xref="S3.Ex2.m1.6.6.4.4.1.2"></times><apply id="S3.Ex2.m1.6.6.4.4.1.3.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3"><apply id="S3.Ex2.m1.6.6.4.4.1.3.1.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.4.4.1.3.1.1.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3.1">subscript</csymbol><ci id="S3.Ex2.m1.6.6.4.4.1.3.1.2.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3.1.2">∇</ci><ci id="S3.Ex2.m1.6.6.4.4.1.3.1.3.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3.1.3">𝜃</ci></apply><apply id="S3.Ex2.m1.6.6.4.4.1.3.2.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.4.4.1.3.2.1.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3.2">subscript</csymbol><ci id="S3.Ex2.m1.6.6.4.4.1.3.2.2.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3.2.2">𝐿</ci><ci id="S3.Ex2.m1.6.6.4.4.1.3.2.3.cmml" xref="S3.Ex2.m1.6.6.4.4.1.3.2.3">𝜃</ci></apply></apply><interval closure="open" id="S3.Ex2.m1.6.6.4.4.1.1.2.cmml" xref="S3.Ex2.m1.6.6.4.4.1.1.1"><apply id="S3.Ex2.m1.6.6.4.4.1.1.1.1.cmml" xref="S3.Ex2.m1.6.6.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.4.4.1.1.1.1.1.cmml" xref="S3.Ex2.m1.6.6.4.4.1.1.1.1">superscript</csymbol><ci id="S3.Ex2.m1.6.6.4.4.1.1.1.1.2.cmml" xref="S3.Ex2.m1.6.6.4.4.1.1.1.1.2">𝑥</ci><ci id="S3.Ex2.m1.6.6.4.4.1.1.1.1.3.cmml" xref="S3.Ex2.m1.6.6.4.4.1.1.1.1.3">′</ci></apply><ci id="S3.Ex2.m1.3.3.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1">𝑦</ci></interval></apply><apply id="S3.Ex2.m1.7.7.5.5.2.cmml" xref="S3.Ex2.m1.7.7.5.5.2"><times id="S3.Ex2.m1.7.7.5.5.2.1.cmml" xref="S3.Ex2.m1.7.7.5.5.2.1"></times><apply id="S3.Ex2.m1.7.7.5.5.2.2.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2"><apply id="S3.Ex2.m1.7.7.5.5.2.2.1.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.7.7.5.5.2.2.1.1.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2.1">subscript</csymbol><ci id="S3.Ex2.m1.7.7.5.5.2.2.1.2.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2.1.2">∇</ci><ci id="S3.Ex2.m1.7.7.5.5.2.2.1.3.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2.1.3">𝜃</ci></apply><apply id="S3.Ex2.m1.7.7.5.5.2.2.2.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.7.7.5.5.2.2.2.1.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.7.7.5.5.2.2.2.2.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2.2.2">𝐿</ci><ci id="S3.Ex2.m1.7.7.5.5.2.2.2.3.cmml" xref="S3.Ex2.m1.7.7.5.5.2.2.2.3">𝜃</ci></apply></apply><interval closure="open" id="S3.Ex2.m1.7.7.5.5.2.3.1.cmml" xref="S3.Ex2.m1.7.7.5.5.2.3.2"><ci id="S3.Ex2.m1.4.4.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2">𝑥</ci><ci id="S3.Ex2.m1.5.5.3.3.cmml" xref="S3.Ex2.m1.5.5.3.3">𝑦</ci></interval></apply></list><apply id="S3.Ex2.m1.12.12.10.cmml" xref="S3.Ex2.m1.12.12.10"><times id="S3.Ex2.m1.12.12.10.6.cmml" xref="S3.Ex2.m1.12.12.10.6"></times><apply id="S3.Ex2.m1.11.11.9.4.2.cmml" xref="S3.Ex2.m1.11.11.9.4.1"><csymbol cd="latexml" id="S3.Ex2.m1.11.11.9.4.2.1.cmml" xref="S3.Ex2.m1.11.11.9.4.1.2">norm</csymbol><apply id="S3.Ex2.m1.11.11.9.4.1.1.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1"><times id="S3.Ex2.m1.11.11.9.4.1.1.2.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.2"></times><apply id="S3.Ex2.m1.11.11.9.4.1.1.3.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3"><apply id="S3.Ex2.m1.11.11.9.4.1.1.3.1.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.11.11.9.4.1.1.3.1.1.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3.1">subscript</csymbol><ci id="S3.Ex2.m1.11.11.9.4.1.1.3.1.2.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3.1.2">∇</ci><ci id="S3.Ex2.m1.11.11.9.4.1.1.3.1.3.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3.1.3">𝜃</ci></apply><apply id="S3.Ex2.m1.11.11.9.4.1.1.3.2.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.11.11.9.4.1.1.3.2.1.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3.2">subscript</csymbol><ci id="S3.Ex2.m1.11.11.9.4.1.1.3.2.2.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3.2.2">𝐿</ci><ci id="S3.Ex2.m1.11.11.9.4.1.1.3.2.3.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.3.2.3">𝜃</ci></apply></apply><interval closure="open" id="S3.Ex2.m1.11.11.9.4.1.1.1.2.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.1.1"><apply id="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.2">𝑥</ci><ci id="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.11.11.9.4.1.1.1.1.1.3">′</ci></apply><ci id="S3.Ex2.m1.8.8.6.1.cmml" xref="S3.Ex2.m1.8.8.6.1">𝑦</ci></interval></apply></apply><apply id="S3.Ex2.m1.12.12.10.5.2.cmml" xref="S3.Ex2.m1.12.12.10.5.1"><csymbol cd="latexml" id="S3.Ex2.m1.12.12.10.5.2.1.cmml" xref="S3.Ex2.m1.12.12.10.5.1.2">norm</csymbol><apply id="S3.Ex2.m1.12.12.10.5.1.1.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1"><times id="S3.Ex2.m1.12.12.10.5.1.1.1.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.1"></times><apply id="S3.Ex2.m1.12.12.10.5.1.1.2.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2"><apply id="S3.Ex2.m1.12.12.10.5.1.1.2.1.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.12.12.10.5.1.1.2.1.1.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2.1">subscript</csymbol><ci id="S3.Ex2.m1.12.12.10.5.1.1.2.1.2.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2.1.2">∇</ci><ci id="S3.Ex2.m1.12.12.10.5.1.1.2.1.3.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2.1.3">𝜃</ci></apply><apply id="S3.Ex2.m1.12.12.10.5.1.1.2.2.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.12.12.10.5.1.1.2.2.1.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2.2">subscript</csymbol><ci id="S3.Ex2.m1.12.12.10.5.1.1.2.2.2.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2.2.2">𝐿</ci><ci id="S3.Ex2.m1.12.12.10.5.1.1.2.2.3.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.2.2.3">𝜃</ci></apply></apply><interval closure="open" id="S3.Ex2.m1.12.12.10.5.1.1.3.1.cmml" xref="S3.Ex2.m1.12.12.10.5.1.1.3.2"><ci id="S3.Ex2.m1.9.9.7.2.cmml" xref="S3.Ex2.m1.9.9.7.2">𝑥</ci><ci id="S3.Ex2.m1.10.10.8.3.cmml" xref="S3.Ex2.m1.10.10.8.3">𝑦</ci></interval></apply></apply></apply></apply></apply><apply id="S3.Ex2.m1.13.13.1.cmml" xref="S3.Ex2.m1.13.13.1"><times id="S3.Ex2.m1.13.13.1.2.cmml" xref="S3.Ex2.m1.13.13.1.2"></times><ci id="S3.Ex2.m1.13.13.1.3.cmml" xref="S3.Ex2.m1.13.13.1.3">𝛼</ci><ci id="S3.Ex2.m1.13.13.1.4.cmml" xref="S3.Ex2.m1.13.13.1.4">𝑇</ci><ci id="S3.Ex2.m1.13.13.1.5.cmml" xref="S3.Ex2.m1.13.13.1.5">𝑉</ci><apply id="S3.Ex2.m1.13.13.1.1.1.1.cmml" xref="S3.Ex2.m1.13.13.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.13.13.1.1.1.1.1.cmml" xref="S3.Ex2.m1.13.13.1.1.1">superscript</csymbol><ci id="S3.Ex2.m1.13.13.1.1.1.1.2.cmml" xref="S3.Ex2.m1.13.13.1.1.1.1.2">𝑥</ci><ci id="S3.Ex2.m1.13.13.1.1.1.1.3.cmml" xref="S3.Ex2.m1.13.13.1.1.1.1.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.13c">\displaystyle\underset{x^{\prime}\in[0,1]}{\operatorname{argmin}}\;1-\frac{\langle\nabla_{\theta}L_{\theta}(x^{\prime},y),\nabla_{\theta}L_{\theta}(x,y)\rangle}{||\nabla_{\theta}L_{\theta}(x^{\prime},y)||||\nabla_{\theta}L_{\theta}(x,y)||}+\alpha TV(x^{\prime})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p3.1" class="ltx_p">They also (i) constrained their search space to <math id="S3.SS2.p3.1.m1.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S3.SS2.p3.1.m1.2a"><mrow id="S3.SS2.p3.1.m1.2.3.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.1.m1.2.3.2.1" xref="S3.SS2.p3.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p3.1.m1.2.3.2.2" xref="S3.SS2.p3.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p3.1.m1.2.2" xref="S3.SS2.p3.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p3.1.m1.2.3.2.3" xref="S3.SS2.p3.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.2b"><interval closure="closed" id="S3.SS2.p3.1.m1.2.3.1.cmml" xref="S3.SS2.p3.1.m1.2.3.2"><cn type="integer" id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">0</cn><cn type="integer" id="S3.SS2.p3.1.m1.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">[0,1]</annotation></semantics></math>, (ii) added total variation as an image prior, and (iii) minimized their cost function based on the signs of the loss gradients and the ADAM optimizer. This modification was inspired by adversarial attacks on <abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">DNNs</span></abbr>, which used a similar technique to generate adversarial inputs <cite class="ltx_cite ltx_citemacro_citep">(Szegedy et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2013</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Bijectivity of Aggregation Algorithms</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.4" class="ltx_p">Most <abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">DNNs</span></abbr> aggregation algorithms (e.g., <em id="S3.SS3.p1.4.1" class="ltx_emph ltx_font_italic"><abbr title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></em>, <em id="S3.SS3.p1.4.2" class="ltx_emph ltx_font_italic"><abbr title="Federated Stochastic Gradient Descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></em>, <em id="S3.SS3.p1.4.3" class="ltx_emph ltx_font_italic">Krum</em>, <em id="S3.SS3.p1.4.4" class="ltx_emph ltx_font_italic">Coordinate Median</em>, <em id="S3.SS3.p1.4.5" class="ltx_emph ltx_font_italic">Paillier crypto fusion</em>, etc.)
only involve bijective summation and averaging operations. In simple terms, if a model is represented as a flattened vector <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">M</annotation></semantics></math>, these algorithms perform coordinate-wise fusion across parties. That is, they add or average the elements at <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">M</annotation></semantics></math>[<math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">i</annotation></semantics></math>] from all parties — parameters at a given index <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">i</annotation></semantics></math> can be fused with no dependency on those at any other indices.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">By examining the methods of data reconstruction attacks, we observe that a <em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">global</em> view of model updates is required in the attacks’ optimization procedures. The completeness and data-order of model updates are crucial for reconstructing training data. Lack of either will lead to reconstruction failures. On the contrary, as <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregation algorithms for <abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr> training only involve coordinate-wise operations at the parameter granularity, data completeness and ordering are not required.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Therefore, we are able to partition an entire model update into multiple pieces, deploy them to multiple servers, and execute the same fusion algorithms independently across all servers. Furthermore, before aggregation, each partitioned vector can also be shuffled at each training round, as long as all parties permute in the same order. Parties can reverse the permutation and merge the aggregated partitions locally when they receive the aggregated model updates for synchronization.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>AMD SEV</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Kaplan
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2016</a>)</cite> is a confidential computing technology introduced by AMD. It aims to protect security-sensitive workloads in public cloud environments. <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> depends on AMD  <span title="Secure Memory Encryption" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Secure Memory Encryption</span></span> (<abbr title="Secure Memory Encryption" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SME</span></abbr>) to enable runtime memory encryption. Along with the AMD Virtualization (AMD-V) architecture, <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> can enforce cryptographic isolation between guest <abbr title="virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">VMs</span></abbr> and the hypervisor. Therefore, <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> can prevent privileged system administrators, e.g., at the hypervisor level, from accessing the data within the domain of an <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr>.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">When <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> is enabled, <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> hardware tags all code and data of a <abbr title="virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VM</span></abbr> with an  <span title="Address Space Identifier" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Address Space Identifier</span></span> (<abbr title="Address Space Identifier" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ASID</span></abbr>), which is associated with a distinct ephemeral  <span title="Advanced Encryption Standard" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Advanced Encryption Standard</span></span> (<abbr title="Advanced Encryption Standard" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AES</span></abbr>) key, called  <span title="VM Encryption Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">VM Encryption Key</span></span> (<abbr title="VM Encryption Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VEK</span></abbr>). The keys are managed by the AMD  <span title="Secure Processor" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Secure Processor</span></span> (<abbr title="Secure Processor" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SP</span></abbr>), which is a 32-bit ARM Cortex-A5 micro-controller integrated within the AMD <abbr title="System-on-Chip" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SoC</span></abbr>. Runtime memory encryption is performed via on-die memory controllers. Each memory controller has an <abbr title="Advanced Encryption Standard" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AES</span></abbr> engine that encrypts/decrypts data when it is written to main memory or is read into the <abbr title="System-on-Chip" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SoC</span></abbr>. The control over memory page encryption is via page tables. Physical address bit 47, a.k.a., <em id="S3.SS4.p2.1.1" class="ltx_emph ltx_font_italic">C-bit</em>, is used to mark whether the memory page is encrypted.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Similar to other <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr>, <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> also provides a remote attestation mechanism for authenticating hardware platforms and attesting <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr>. The authenticity of the platform is proven with an identity key signed by AMD and the platform owner. Before provisioning any secrets, <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr> owners should verify both the authenticity of <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>-enabled hardware and the measurement of  <span title="Open Virtual Machine Firmware" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Open Virtual Machine Firmware</span></span> (<abbr title="Open Virtual Machine Firmware" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OVMF</span></abbr>), which enables UEFI support for booting <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>System Design</h2>

<figure id="S4.F1" class="ltx_figure"><img src="/html/2105.09400/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="348" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>System Architecture of <span id="S4.F1.2.1" class="ltx_text ltx_font_smallcaps">Truda</span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we detail the design of <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> and demonstrate how it effectively mitigates information leakage channels for <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> data reconstruction attacks. We describe three key security mechanisms of <span id="S4.p1.1.2" class="ltx_text ltx_font_smallcaps">Truda</span>: (1) enabling trustworthy <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregation with a two-phase attestation protocol, (2) separating a central aggregator to multiple decentralized entities, each with only a fragmentary view of the model updates, and (3) shuffling model updates dynamically for each training round. These three mechanisms are composable and can operate with no mutual dependency. Such a multi-faceted design makes <span id="S4.p1.1.3" class="ltx_text ltx_font_smallcaps">Truda</span> more resilient when facing unanticipated security breaches. For example, assuming that memory encryption of <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> is broken due to some new zero-day vulnerabilities, the adversaries will still not be able to reconstruct training data from the model updates (leaked from <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr>) as they are fragmentary and obfuscated with mechanisms (2) and/or (3) enabled.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We give a concrete deployment example of <span id="S4.p2.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> in Figure <a href="#S4.F1" title="Figure 1 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and discuss the workflow step by step. Similar to traditional <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr>, in <span id="S4.p2.1.2" class="ltx_text ltx_font_smallcaps">Truda</span>, each party needs to register with the aggregators to participate in the training. One aggregator initiates the training process by notifying all parties. During training, aggregators engage in a number of training rounds with all parties. At each training round, each party uses its local training data to produce a new model update and upload it to the aggregators. The aggregators merge model updates from all parties and dispatch the aggregated version back to all parties. The global training ends once pre-determined training criteria (like number of epochs or accuracy) are met.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Different from traditional <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr>, <span id="S4.p3.1.1" class="ltx_text ltx_font_smallcaps">Truda</span>’s deployment involves multiple aggregators running within <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr>, rather than a single central aggregator. Aggregators need to communicate with each other for training synchronization. In addition, we also deploy an attestation server as a trusted anchor, which is responsible for attesting the aggregator’s workload and provisioning secrets on behalf of all parties.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2105.09400/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="274" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Model Partitioning and Dynamic Permutation</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Trustworthy Aggregation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As mentioned earlier, model updates exchanged between parties and aggregators may contain essential information for reverse engineering private training data. We need to eliminate the channels for adversaries to intercept and inspect model updates in transit and also in use. In our design, we enforce cryptographic isolation for <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregation via <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>. The aggregators execute within <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr>. Each <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr>’s memory is protected with a distinct ephemeral <abbr title="VM Encryption Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VEK</span></abbr>. Therefore, we can protect the confidentiality of model aggregation from unauthorized users, e.g., system administrators, and privileged software running on the hosting servers.
AMD provides attestation primitives for verifying the authenticity of individual <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> hardware/firmware. We design a new attestation protocol upon the primitives to bootstrap trust between parties and aggregators in the distributed <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> setting. This <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> attestation protocol consists of two phases:</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Phase 1: Launching Trustworthy Aggregators.</span>
First, we need to securely launch <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr> with aggregators running within. To establish the trust of <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr>, attestation must prove that (1) the platform is an authentic AMD <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>-enabled hardware providing the required security properties, and
(2) the <abbr title="Open Virtual Machine Firmware" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OVMF</span></abbr> image to launch the <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr> is not tampered. Once the remote attestation is completed, we can provision a secret, as a unique identifier of a trustworthy aggregator, to the <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr>. The secret is injected into <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr>’s encrypted physical memory and used for aggregator authentication in Phase 2.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In Figure <a href="#S4.F1" title="Figure 1 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, Step ❶ shows an attestation server that facilitates remote attestation. The <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr> owner instructs the AMD <abbr title="Secure Processor" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SP</span></abbr> to export the certificate chain from the  <span title="Platform Diffie-Hellman Public Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Platform Diffie-Hellman Public Key</span></span> (<abbr title="Platform Diffie-Hellman Public Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">PDH</span></abbr>) down to the  <span title="AMD Root Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">AMD Root Key</span></span> (<abbr title="AMD Root Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ARK</span></abbr>). This certificate chain can be verified by the AMD root certificates. The digest of <abbr title="Open Virtual Machine Firmware" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OVMF</span></abbr> image is also included in the attestation report along with the certificate chain.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">The attestation report is sent to the attestation server, which is provisioned with the AMD root certificates. The attestation server verifies the certificate chain to authenticate the hardware platform and check the integrity of <abbr title="Open Virtual Machine Firmware" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OVMF</span></abbr> firmware. Thereafter, the attestation server generates a launch blob and a  <span title="Guest Owner Diffie-Hellman Public Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Guest Owner Diffie-Hellman Public Key</span></span> (<abbr title="Guest Owner Diffie-Hellman Public Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GODH</span></abbr>) certificate. They are sent back to the <abbr title="Secure Processor" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SP</span></abbr> on the aggregator’s machine for negotiating a  <span title="Transport Encryption Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Transport Encryption Key</span></span> (<abbr title="Transport Encryption Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEK</span></abbr>) and a  <span title="Transport Integrity Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Transport Integrity Key</span></span> (<abbr title="Transport Integrity Key" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TIK</span></abbr>) through  <span title="Diffie-Hellman Key Exchange" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Diffie-Hellman Key Exchange</span></span> (<abbr title="Diffie-Hellman Key Exchange" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DHKE</span></abbr>) and launching the <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr>.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">We can retrieve the <abbr title="Open Virtual Machine Firmware" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OVMF</span></abbr> runtime measurement through the <abbr title="Secure Processor" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SP</span></abbr> by pausing the <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr> at launch time. We send this measurement (along with the <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> API version and the <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr> deployment policy) to the attestation server to prove the integrity of UEFI booting process. Only after verifying the measurement, the attestation server generates a packaged secret, which includes an ECDSA prime251v1 key. The hypervisor injects this secret into the <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr>’s physical memory space as a unique identifier of a trusted aggregator and continue the launching process. Our secret injection procedure follows the 1st-generation <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>’s remote attestation protocol. With the upcoming <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>-SNP, AMD <abbr title="Secure Processor" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SP</span></abbr> can also measure and attest the layout of <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr>’s initial memory and its contents <cite class="ltx_cite ltx_citemacro_citep">(SEV-SNP, <a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite>. Thus, the new <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>-SNP’s attestation mechanism can further reinforce the integrity of the launching process.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_bold">Phase 2: Aggregator Authentication.</span>
Parties participating in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> must ensure that they are interacting with trustworthy aggregators with runtime memory encryption protection. To enable aggregator authentication, in Phase 1, the attestation server provisions an ECDSA key as a secret during <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr> deployment. This key is used for signing challenge requests and thus serves to identify a legitimate aggregator. In step ❷ of Figure <a href="#S4.F1" title="Figure 1 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, before participating in FL, a party first attests an aggregator by engaging in a challenge response protocol. The party sends a randomly generated nonce to the aggregator. The aggregator digitally signs the nonce using its corresponding ECDSA key and then returns the signed nonce to the requesting party. The party verifies that the nonce is signed with the corresponding ECDSA key. If the verification is successful, the party then proceeds to register with the aggregator to participate in FL. This process is repeated for all aggregators.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">After registration, end-to-end secure channels can be established to protect communications between aggregators and parties for exchanging model updates. We enable TLS to support mutual authentication between a party and an aggregator. Thus, all model updates are protected both within <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr> and in transit.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Decentralized Aggregation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Enabling trustworthy aggregation alone may not be sufficient. We cannot expect that <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> are omnipotent and that there will be no security vulnerabilities discovered in the future. In fact, we have already observed some security breaches with regard to <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Werner et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2019</a>; Buhren
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2019</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Wilke et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite>. Therefore, we enhance our design to ensure that even if <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> are breached, adversaries cannot reconstruct training data from model updates.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Decentralization has been primarily explored (e.g., Bonawitz et al. <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>) in distributed learning as a load balancing technique to address the performance problems with respect to a central server, and to scale to a large number of parties. This has typically involved distributing parties among aggregator replicas (“party-partitioning”), and
the replicas co-ordinating among themselves to aggregate intermediate results. In this setting, each model update reaches
an aggregator replica in its entirety, making reverse engineering possible. In other research on fully-decentralized/peer-to-peer distributed learning<cite class="ltx_cite ltx_citemacro_citep">(Vanhaesebrouck et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2017</a>; Bellet
et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Koloskova
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Lalitha et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>, there exists no central aggregator. Each party exchanges model updates with their neighbors and merges the received updates with its local model. But this scheme cannot prevent information leakage either as the entire model updates are still circulated among parties in the network and susceptible to reconstruction attacks.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">In <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_smallcaps">Truda</span>, we choose a different decentralization strategy with model partitioning enabled. Each aggregator only receives a fraction of each model update and runs within an <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr>. For example, in Figure <a href="#S4.F1" title="Figure 1 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we establish three aggregators. Each party authenticates and registers with all aggregators respectively.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Inter-Aggregator Training Synchronization.</span> We maintain communication channels between aggregators for training synchronization, e.g., step ❸ of Figure <a href="#S4.F1" title="Figure 1 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Any one of the aggregators can start the training and become the <em id="S4.SS2.p4.1.2" class="ltx_emph ltx_font_italic">initiator</em> node by default. All the other aggregators become <em id="S4.SS2.p4.1.3" class="ltx_emph ltx_font_italic">follower</em> nodes and wait for the commands from the <em id="S4.SS2.p4.1.4" class="ltx_emph ltx_font_italic">initiator</em>. At each training round, the <em id="S4.SS2.p4.1.5" class="ltx_emph ltx_font_italic">initiator</em> first notifies all parties to start local training and retrieve the model updates for fusion. Thereafter, it notifies all the <em id="S4.SS2.p4.1.6" class="ltx_emph ltx_font_italic">follower</em> nodes to pull their corresponding model updates, aggregate them together, and distribute the aggregated updates back to the parties.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Randomized Model Partitioning.</span>
Due to the bijective nature of <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> fusion algorithms, we can split a model update into disjoint partitions based on the number of available aggregators. Before training starts, we randomly generate a model-mapper for each <abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr> model (to be trained). We allow the parties to choose the proportion of model parameters for each aggregator. This model-mapper is shared by all the parties that participate in the <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training. For example, in Figure <a href="#S4.F2" title="Figure 2 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mi id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><ci id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">k</annotation></semantics></math> parameters of a local model are mapped to three aggregators. The colors imply the aggregator attributes for each parameter within the model. The model update is disassembled and rearranged at the parameter-granularity for different aggregators (step ❹ in Figure <a href="#S4.F1" title="Figure 1 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Once parties receive fused model updates from different aggregators, they query the model-mapper again to merge model updates to its original positions within the local model (step ❺ in Figure <a href="#S4.F1" title="Figure 1 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). A shared model-mapper can be generated either by (i) using a consensus algorithm like Raft, or (ii) constructed at each party using random number generators seeded by the same random value (e.g., from League of Entropy<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://blog.cloudflare.com/league-of-entropy/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://blog.cloudflare.com/league-of-entropy/</a></span></span></span>).</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">Decentralized aggregation significantly defends against the illegitimate leak of model information at the aggregation point as aggregators no longer store the complete model updates nor the model architectures. As we demonstrate in Section <a href="#S6" title="6. Security Analysis ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, even missing a very small fraction of a model update totally renders the data reconstruction attacks ineffective. Thus, decentralized aggregation requires adversaries to compromise all <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEE</span></abbr>-protected aggregators (which can be isolated in different protected domains) and obtain the model-mapper (protected within party’s training devices) to piece together complete model updates in their original order.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Shuffled Aggregation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To further obfuscate the information transferred from the parties to the aggregators, we employ a dynamic permutation scheme to shuffle the partitioned model updates at every training round. Each permutation is seeded by the <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">combination</em> of a permutation key (e.g., dispatched from a trusted key generation server) agreed among all parties and a dynamically generated training identifier synchronized at the start of each training round. Thus, the permutation changes every training round, but is deterministic across all parties. After parties receive aggregated model updates, they rev-shuffle the results back to their original order as shown in Figure <a href="#S4.F2" title="Figure 2 ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The dynamic permutation scheme is based on the insight that the data-order of model updates is irrelevant for model-fusion algorithms, while it is crucial for optimization procedures used in data reconstruction attacks. With dynamic permutation enabled, adversaries only obtain obfuscated model updates and the data order dynamically changes at each training round. If the permutation seed is not leaked, it is infeasible for adversaries to reconstruct any training data even if they compromise all <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEE</span></abbr>-protected aggregators. In addition, this shuffled aggregation mechanism works in the deployments with either a central aggregator (as in traditional <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr>) or multiple decentralized aggregators (as in <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_smallcaps">Truda</span>).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Comparison of Fidelity Threshold (MSE) for DLG and iDLG with Model Partitioning and Permutation</figcaption>
<table id="S4.T1.11" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.11.12.1" class="ltx_tr">
<th id="S4.T1.11.12.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S4.T1.11.12.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">DLG (w/o perm)</th>
<th id="S4.T1.11.12.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">iDLG (w/o perm)</th>
<th id="S4.T1.11.12.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">DLG (with perm)</th>
<th id="S4.T1.11.12.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">iDLG (with perm)</th>
</tr>
<tr id="S4.T1.11.13.2" class="ltx_tr">
<th id="S4.T1.11.13.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S4.T1.11.13.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Partition %</th>
<th id="S4.T1.11.13.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Partition %</th>
<th id="S4.T1.11.13.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Partition %</th>
<th id="S4.T1.11.13.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">Partition %</th>
</tr>
<tr id="S4.T1.11.14.3" class="ltx_tr">
<th id="S4.T1.11.14.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T1.11.14.3.1.1" class="ltx_text" style="background-color:#C0C0C0;">Fidelity Threshold (MSE)</span></th>
<th id="S4.T1.11.14.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t">
<span id="S4.T1.11.14.3.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.11.14.3.2.2" class="ltx_text ltx_font_bold">100</span>
</th>
<th id="S4.T1.11.14.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">60</th>
<th id="S4.T1.11.14.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">20</th>
<th id="S4.T1.11.14.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T1.11.14.3.5.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.11.14.3.5.2" class="ltx_text ltx_font_bold">100</span>
</th>
<th id="S4.T1.11.14.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">60</th>
<th id="S4.T1.11.14.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">20</th>
<th id="S4.T1.11.14.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">100</th>
<th id="S4.T1.11.14.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">60</th>
<th id="S4.T1.11.14.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">20</th>
<th id="S4.T1.11.14.3.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">100</th>
<th id="S4.T1.11.14.3.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">60</th>
<th id="S4.T1.11.14.3.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">20</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1" class="ltx_tr" style="background-color:#FD6864;">
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T1.1.1.1.1" class="ltx_text" style="background-color:#C0C0C0;">[0, <math id="S4.T1.1.1.1.1.m1.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{-03}" display="inline"><semantics id="S4.T1.1.1.1.1.m1.3a"><mrow id="S4.T1.1.1.1.1.m1.3.3.3" xref="S4.T1.1.1.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.1.1.1.1.m1.1.1.1.1.1.1.1" xref="S4.T1.1.1.1.1.m1.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.1.1.1.1.m1.2.2.2.2.2.2.2" xref="S4.T1.1.1.1.1.m1.3.3.3.cmml">×</mtext><msup id="S4.T1.1.1.1.1.m1.3.3.3.3.3.3.3" xref="S4.T1.1.1.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.1.1.1.1.m1.3.3.3.3.3.3.3.2" xref="S4.T1.1.1.1.1.m1.3.3.3.cmml">10</mn><mrow id="S4.T1.1.1.1.1.m1.3.3.3.3.3.3.3.3.2" xref="S4.T1.1.1.1.1.m1.3.3.3.cmml"><mo mathbackground="#C0C0C0" id="S4.T1.1.1.1.1.m1.3.3.3.3.3.3.3.3.2a" xref="S4.T1.1.1.1.1.m1.3.3.3.cmml">−</mo><mn mathbackground="#C0C0C0" id="S4.T1.1.1.1.1.m1.3.3.3.3.3.3.3.3.2.2" xref="S4.T1.1.1.1.1.m1.3.3.3.cmml">03</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.3b"><csymbol cd="latexml" id="S4.T1.1.1.1.1.m1.3.3.3.cmml" xref="S4.T1.1.1.1.1.m1.3.3.3">1.0E-03</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.3c">1.0\text{\times}{10}^{-03}</annotation></semantics></math>)</span></th>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.1.1.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.1.1.2.2" class="ltx_text ltx_font_bold" style="background-color:#FD6864;">66.6%</span>
</td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.3.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.4.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.1.1.5.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.1.1.5.2" class="ltx_text ltx_font_bold" style="background-color:#FD6864;">83.7%</span>
</td>
<td id="S4.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.6.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.7.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.8.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.9.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.10.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.11.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.12.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
<td id="S4.T1.1.1.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.13.1" class="ltx_text" style="background-color:#FD6864;">0.0%</span></td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T1.3.3.2.2" class="ltx_text" style="background-color:#C0C0C0;">[<math id="S4.T1.2.2.1.1.m1.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{-03}" display="inline"><semantics id="S4.T1.2.2.1.1.m1.3a"><mrow id="S4.T1.2.2.1.1.m1.3.3.3" xref="S4.T1.2.2.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.2.2.1.1.m1.1.1.1.1.1.1.1" xref="S4.T1.2.2.1.1.m1.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.2.2.1.1.m1.2.2.2.2.2.2.2" xref="S4.T1.2.2.1.1.m1.3.3.3.cmml">×</mtext><msup id="S4.T1.2.2.1.1.m1.3.3.3.3.3.3.3" xref="S4.T1.2.2.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.2.2.1.1.m1.3.3.3.3.3.3.3.2" xref="S4.T1.2.2.1.1.m1.3.3.3.cmml">10</mn><mrow id="S4.T1.2.2.1.1.m1.3.3.3.3.3.3.3.3.2" xref="S4.T1.2.2.1.1.m1.3.3.3.cmml"><mo mathbackground="#C0C0C0" id="S4.T1.2.2.1.1.m1.3.3.3.3.3.3.3.3.2a" xref="S4.T1.2.2.1.1.m1.3.3.3.cmml">−</mo><mn mathbackground="#C0C0C0" id="S4.T1.2.2.1.1.m1.3.3.3.3.3.3.3.3.2.2" xref="S4.T1.2.2.1.1.m1.3.3.3.cmml">03</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.1.m1.3b"><csymbol cd="latexml" id="S4.T1.2.2.1.1.m1.3.3.3.cmml" xref="S4.T1.2.2.1.1.m1.3.3.3">1.0E-03</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.1.m1.3c">1.0\text{\times}{10}^{-03}</annotation></semantics></math>, <math id="S4.T1.3.3.2.2.m2.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{-02}" display="inline"><semantics id="S4.T1.3.3.2.2.m2.3a"><mrow id="S4.T1.3.3.2.2.m2.3.3.3" xref="S4.T1.3.3.2.2.m2.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.3.3.2.2.m2.1.1.1.1.1.1.1" xref="S4.T1.3.3.2.2.m2.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.3.3.2.2.m2.2.2.2.2.2.2.2" xref="S4.T1.3.3.2.2.m2.3.3.3.cmml">×</mtext><msup id="S4.T1.3.3.2.2.m2.3.3.3.3.3.3.3" xref="S4.T1.3.3.2.2.m2.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.3.3.2.2.m2.3.3.3.3.3.3.3.2" xref="S4.T1.3.3.2.2.m2.3.3.3.cmml">10</mn><mrow id="S4.T1.3.3.2.2.m2.3.3.3.3.3.3.3.3.2" xref="S4.T1.3.3.2.2.m2.3.3.3.cmml"><mo mathbackground="#C0C0C0" id="S4.T1.3.3.2.2.m2.3.3.3.3.3.3.3.3.2a" xref="S4.T1.3.3.2.2.m2.3.3.3.cmml">−</mo><mn mathbackground="#C0C0C0" id="S4.T1.3.3.2.2.m2.3.3.3.3.3.3.3.3.2.2" xref="S4.T1.3.3.2.2.m2.3.3.3.cmml">02</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.2.2.m2.3b"><csymbol cd="latexml" id="S4.T1.3.3.2.2.m2.3.3.3.cmml" xref="S4.T1.3.3.2.2.m2.3.3.3">1.0E-02</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.2.2.m2.3c">1.0\text{\times}{10}^{-02}</annotation></semantics></math>)</span></th>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.3.3.3.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.3.3.3.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0.8%</span>
</td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.4.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.5.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.3.3.6.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.3.3.6.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">1.2%</span>
</td>
<td id="S4.T1.3.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.7.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.8.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.9.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.10.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.3.11.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.12.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.13.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.3.3.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.14.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T1.5.5.2.2" class="ltx_text" style="background-color:#C0C0C0;">[<math id="S4.T1.4.4.1.1.m1.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{-02}" display="inline"><semantics id="S4.T1.4.4.1.1.m1.3a"><mrow id="S4.T1.4.4.1.1.m1.3.3.3" xref="S4.T1.4.4.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.4.4.1.1.m1.1.1.1.1.1.1.1" xref="S4.T1.4.4.1.1.m1.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.4.4.1.1.m1.2.2.2.2.2.2.2" xref="S4.T1.4.4.1.1.m1.3.3.3.cmml">×</mtext><msup id="S4.T1.4.4.1.1.m1.3.3.3.3.3.3.3" xref="S4.T1.4.4.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.4.4.1.1.m1.3.3.3.3.3.3.3.2" xref="S4.T1.4.4.1.1.m1.3.3.3.cmml">10</mn><mrow id="S4.T1.4.4.1.1.m1.3.3.3.3.3.3.3.3.2" xref="S4.T1.4.4.1.1.m1.3.3.3.cmml"><mo mathbackground="#C0C0C0" id="S4.T1.4.4.1.1.m1.3.3.3.3.3.3.3.3.2a" xref="S4.T1.4.4.1.1.m1.3.3.3.cmml">−</mo><mn mathbackground="#C0C0C0" id="S4.T1.4.4.1.1.m1.3.3.3.3.3.3.3.3.2.2" xref="S4.T1.4.4.1.1.m1.3.3.3.cmml">02</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.1.m1.3b"><csymbol cd="latexml" id="S4.T1.4.4.1.1.m1.3.3.3.cmml" xref="S4.T1.4.4.1.1.m1.3.3.3">1.0E-02</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.1.m1.3c">1.0\text{\times}{10}^{-02}</annotation></semantics></math>, <math id="S4.T1.5.5.2.2.m2.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{-01}" display="inline"><semantics id="S4.T1.5.5.2.2.m2.3a"><mrow id="S4.T1.5.5.2.2.m2.3.3.3" xref="S4.T1.5.5.2.2.m2.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.5.5.2.2.m2.1.1.1.1.1.1.1" xref="S4.T1.5.5.2.2.m2.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.5.5.2.2.m2.2.2.2.2.2.2.2" xref="S4.T1.5.5.2.2.m2.3.3.3.cmml">×</mtext><msup id="S4.T1.5.5.2.2.m2.3.3.3.3.3.3.3" xref="S4.T1.5.5.2.2.m2.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.5.5.2.2.m2.3.3.3.3.3.3.3.2" xref="S4.T1.5.5.2.2.m2.3.3.3.cmml">10</mn><mrow id="S4.T1.5.5.2.2.m2.3.3.3.3.3.3.3.3.2" xref="S4.T1.5.5.2.2.m2.3.3.3.cmml"><mo mathbackground="#C0C0C0" id="S4.T1.5.5.2.2.m2.3.3.3.3.3.3.3.3.2a" xref="S4.T1.5.5.2.2.m2.3.3.3.cmml">−</mo><mn mathbackground="#C0C0C0" id="S4.T1.5.5.2.2.m2.3.3.3.3.3.3.3.3.2.2" xref="S4.T1.5.5.2.2.m2.3.3.3.cmml">01</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.2.2.m2.3b"><csymbol cd="latexml" id="S4.T1.5.5.2.2.m2.3.3.3.cmml" xref="S4.T1.5.5.2.2.m2.3.3.3">1.0E-01</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.2.2.m2.3c">1.0\text{\times}{10}^{-01}</annotation></semantics></math>)</span></th>
<td id="S4.T1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.5.5.3.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.5.5.3.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0.3%</span>
</td>
<td id="S4.T1.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.4.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.5.5.5.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.5.5.6.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.5.5.6.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0.1%</span>
</td>
<td id="S4.T1.5.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.7.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.5.5.8.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.9.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.10.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.5.5.11.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.12.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.13.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.5.5.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.14.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
</tr>
<tr id="S4.T1.7.7" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T1.7.7.2.2" class="ltx_text" style="background-color:#C0C0C0;">[<math id="S4.T1.6.6.1.1.m1.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{-01}" display="inline"><semantics id="S4.T1.6.6.1.1.m1.3a"><mrow id="S4.T1.6.6.1.1.m1.3.3.3" xref="S4.T1.6.6.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.6.6.1.1.m1.1.1.1.1.1.1.1" xref="S4.T1.6.6.1.1.m1.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.6.6.1.1.m1.2.2.2.2.2.2.2" xref="S4.T1.6.6.1.1.m1.3.3.3.cmml">×</mtext><msup id="S4.T1.6.6.1.1.m1.3.3.3.3.3.3.3" xref="S4.T1.6.6.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.6.6.1.1.m1.3.3.3.3.3.3.3.2" xref="S4.T1.6.6.1.1.m1.3.3.3.cmml">10</mn><mrow id="S4.T1.6.6.1.1.m1.3.3.3.3.3.3.3.3.2" xref="S4.T1.6.6.1.1.m1.3.3.3.cmml"><mo mathbackground="#C0C0C0" id="S4.T1.6.6.1.1.m1.3.3.3.3.3.3.3.3.2a" xref="S4.T1.6.6.1.1.m1.3.3.3.cmml">−</mo><mn mathbackground="#C0C0C0" id="S4.T1.6.6.1.1.m1.3.3.3.3.3.3.3.3.2.2" xref="S4.T1.6.6.1.1.m1.3.3.3.cmml">01</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.1.1.m1.3b"><csymbol cd="latexml" id="S4.T1.6.6.1.1.m1.3.3.3.cmml" xref="S4.T1.6.6.1.1.m1.3.3.3">1.0E-01</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.1.1.m1.3c">1.0\text{\times}{10}^{-01}</annotation></semantics></math>, <math id="S4.T1.7.7.2.2.m2.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{00}" display="inline"><semantics id="S4.T1.7.7.2.2.m2.3a"><mrow id="S4.T1.7.7.2.2.m2.3.3.3" xref="S4.T1.7.7.2.2.m2.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.7.7.2.2.m2.1.1.1.1.1.1.1" xref="S4.T1.7.7.2.2.m2.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.7.7.2.2.m2.2.2.2.2.2.2.2" xref="S4.T1.7.7.2.2.m2.3.3.3.cmml">×</mtext><msup id="S4.T1.7.7.2.2.m2.3.3.3.3.3.3.3" xref="S4.T1.7.7.2.2.m2.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.7.7.2.2.m2.3.3.3.3.3.3.3.2" xref="S4.T1.7.7.2.2.m2.3.3.3.cmml">10</mn><mn mathbackground="#C0C0C0" id="S4.T1.7.7.2.2.m2.3.3.3.3.3.3.3.3" xref="S4.T1.7.7.2.2.m2.3.3.3.cmml">00</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.2.2.m2.3b"><csymbol cd="latexml" id="S4.T1.7.7.2.2.m2.3.3.3.cmml" xref="S4.T1.7.7.2.2.m2.3.3.3">1.0E+00</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.2.2.m2.3c">1.0\text{\times}{10}^{00}</annotation></semantics></math>)</span></th>
<td id="S4.T1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.7.7.3.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.7.7.3.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0.2%</span>
</td>
<td id="S4.T1.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.7.4.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.7.7.5.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.7.7.6.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.7.7.6.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0.2%</span>
</td>
<td id="S4.T1.7.7.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.7.7.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.7.7.8.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.7.9.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.7.10.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.7.7.11.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.7.12.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.7.13.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.7.7.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.7.14.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
</tr>
<tr id="S4.T1.9.9" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T1.9.9.2.2" class="ltx_text" style="background-color:#C0C0C0;">[<math id="S4.T1.8.8.1.1.m1.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{00}" display="inline"><semantics id="S4.T1.8.8.1.1.m1.3a"><mrow id="S4.T1.8.8.1.1.m1.3.3.3" xref="S4.T1.8.8.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.8.8.1.1.m1.1.1.1.1.1.1.1" xref="S4.T1.8.8.1.1.m1.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.8.8.1.1.m1.2.2.2.2.2.2.2" xref="S4.T1.8.8.1.1.m1.3.3.3.cmml">×</mtext><msup id="S4.T1.8.8.1.1.m1.3.3.3.3.3.3.3" xref="S4.T1.8.8.1.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.8.8.1.1.m1.3.3.3.3.3.3.3.2" xref="S4.T1.8.8.1.1.m1.3.3.3.cmml">10</mn><mn mathbackground="#C0C0C0" id="S4.T1.8.8.1.1.m1.3.3.3.3.3.3.3.3" xref="S4.T1.8.8.1.1.m1.3.3.3.cmml">00</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.1.1.m1.3b"><csymbol cd="latexml" id="S4.T1.8.8.1.1.m1.3.3.3.cmml" xref="S4.T1.8.8.1.1.m1.3.3.3">1.0E+00</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.1.1.m1.3c">1.0\text{\times}{10}^{00}</annotation></semantics></math>, <math id="S4.T1.9.9.2.2.m2.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{02}" display="inline"><semantics id="S4.T1.9.9.2.2.m2.3a"><mrow id="S4.T1.9.9.2.2.m2.3.3.3" xref="S4.T1.9.9.2.2.m2.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.9.9.2.2.m2.1.1.1.1.1.1.1" xref="S4.T1.9.9.2.2.m2.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.9.9.2.2.m2.2.2.2.2.2.2.2" xref="S4.T1.9.9.2.2.m2.3.3.3.cmml">×</mtext><msup id="S4.T1.9.9.2.2.m2.3.3.3.3.3.3.3" xref="S4.T1.9.9.2.2.m2.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.9.9.2.2.m2.3.3.3.3.3.3.3.2" xref="S4.T1.9.9.2.2.m2.3.3.3.cmml">10</mn><mn mathbackground="#C0C0C0" id="S4.T1.9.9.2.2.m2.3.3.3.3.3.3.3.3" xref="S4.T1.9.9.2.2.m2.3.3.3.cmml">02</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.2.2.m2.3b"><csymbol cd="latexml" id="S4.T1.9.9.2.2.m2.3.3.3.cmml" xref="S4.T1.9.9.2.2.m2.3.3.3">1.0E+02</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.2.2.m2.3c">1.0\text{\times}{10}^{02}</annotation></semantics></math>)</span></th>
<td id="S4.T1.9.9.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.9.9.3.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.9.9.3.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">8.1%</span>
</td>
<td id="S4.T1.9.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.9.4.1" class="ltx_text" style="background-color:#9AFF99;">38.9%</span></td>
<td id="S4.T1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.9.9.5.1" class="ltx_text" style="background-color:#9AFF99;">20.5%</span></td>
<td id="S4.T1.9.9.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.9.9.6.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.9.9.6.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">6.6%</span>
</td>
<td id="S4.T1.9.9.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.9.7.1" class="ltx_text" style="background-color:#9AFF99;">66.5%</span></td>
<td id="S4.T1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.9.9.8.1" class="ltx_text" style="background-color:#9AFF99;">18.3%</span></td>
<td id="S4.T1.9.9.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.9.9.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.9.9.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.9.10.1" class="ltx_text" style="background-color:#9AFF99;">0.0%</span></td>
<td id="S4.T1.9.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.9.9.11.1" class="ltx_text" style="background-color:#9AFF99;">0.2%</span></td>
<td id="S4.T1.9.9.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.9.12.1" class="ltx_text" style="background-color:#9AFF99;">0.2%</span></td>
<td id="S4.T1.9.9.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.9.13.1" class="ltx_text" style="background-color:#9AFF99;">0.2%</span></td>
<td id="S4.T1.9.9.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.9.9.14.1" class="ltx_text" style="background-color:#9AFF99;">0.7%</span></td>
</tr>
<tr id="S4.T1.11.11" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="background-color:#C0C0C0;">
<math id="S4.T1.10.10.1.m1.1" class="ltx_Math" style="background-color:#C0C0C0;" alttext="\geq" display="inline"><semantics id="S4.T1.10.10.1.m1.1a"><mo mathbackground="#C0C0C0" id="S4.T1.10.10.1.m1.1.1" xref="S4.T1.10.10.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.1.m1.1b"><geq id="S4.T1.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.1.m1.1c">\geq</annotation></semantics></math><span id="S4.T1.11.11.2.1" class="ltx_text" style="background-color:#C0C0C0;"> <math id="S4.T1.11.11.2.1.m1.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{02}" display="inline"><semantics id="S4.T1.11.11.2.1.m1.3a"><mrow id="S4.T1.11.11.2.1.m1.3.3.3" xref="S4.T1.11.11.2.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.11.11.2.1.m1.1.1.1.1.1.1.1" xref="S4.T1.11.11.2.1.m1.3.3.3.cmml">1.0</mn><mtext mathbackground="#C0C0C0" id="S4.T1.11.11.2.1.m1.2.2.2.2.2.2.2" xref="S4.T1.11.11.2.1.m1.3.3.3.cmml">×</mtext><msup id="S4.T1.11.11.2.1.m1.3.3.3.3.3.3.3" xref="S4.T1.11.11.2.1.m1.3.3.3.cmml"><mn mathbackground="#C0C0C0" id="S4.T1.11.11.2.1.m1.3.3.3.3.3.3.3.2" xref="S4.T1.11.11.2.1.m1.3.3.3.cmml">10</mn><mn mathbackground="#C0C0C0" id="S4.T1.11.11.2.1.m1.3.3.3.3.3.3.3.3" xref="S4.T1.11.11.2.1.m1.3.3.3.cmml">02</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.2.1.m1.3b"><csymbol cd="latexml" id="S4.T1.11.11.2.1.m1.3.3.3.cmml" xref="S4.T1.11.11.2.1.m1.3.3.3">1.0E+02</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.2.1.m1.3c">1.0\text{\times}{10}^{02}</annotation></semantics></math></span>
</th>
<td id="S4.T1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S4.T1.11.11.3.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.11.11.3.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">24.0%</span>
</td>
<td id="S4.T1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.11.11.4.1" class="ltx_text" style="background-color:#9AFF99;">61.1%</span></td>
<td id="S4.T1.11.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.11.11.5.1" class="ltx_text" style="background-color:#9AFF99;">79.5%</span></td>
<td id="S4.T1.11.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S4.T1.11.11.6.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T1.11.11.6.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">8.2%</span>
</td>
<td id="S4.T1.11.11.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.11.11.7.1" class="ltx_text" style="background-color:#9AFF99;">33.5%</span></td>
<td id="S4.T1.11.11.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.11.11.8.1" class="ltx_text" style="background-color:#9AFF99;">81.7%</span></td>
<td id="S4.T1.11.11.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.11.11.9.1" class="ltx_text" style="background-color:#9AFF99;">100.0%</span></td>
<td id="S4.T1.11.11.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.11.11.10.1" class="ltx_text" style="background-color:#9AFF99;">100.0%</span></td>
<td id="S4.T1.11.11.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.11.11.11.1" class="ltx_text" style="background-color:#9AFF99;">99.8%</span></td>
<td id="S4.T1.11.11.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.11.11.12.1" class="ltx_text" style="background-color:#9AFF99;">99.8%</span></td>
<td id="S4.T1.11.11.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.11.11.13.1" class="ltx_text" style="background-color:#9AFF99;">99.8%</span></td>
<td id="S4.T1.11.11.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.11.11.14.1" class="ltx_text" style="background-color:#9AFF99;">99.3%</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Comparison of Final Cosine Distance for IG with Model Partitioning and Permutation</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">IG (partition)</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">IG (partition+perm)</th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Partition %</th>
<th id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">Partition %</th>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T2.1.3.3.1.1" class="ltx_text" style="background-color:#C0C0C0;">Cosine Distance</span></th>
<th id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.1.3.3.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T2.1.3.3.2.2" class="ltx_text ltx_font_bold">100</span>
</th>
<th id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">60</th>
<th id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">20</th>
<th id="S4.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">100</th>
<th id="S4.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">60</th>
<th id="S4.T2.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">20</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.4.1" class="ltx_tr" style="background-color:#FD6864;">
<th id="S4.T2.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T2.1.4.1.1.1" class="ltx_text" style="background-color:#C0C0C0;">[0, 0.01)</span></th>
<td id="S4.T2.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.4.1.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T2.1.4.1.2.2" class="ltx_text ltx_font_bold" style="background-color:#FD6864;">100%</span>
</td>
<td id="S4.T2.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.4.1.3.1" class="ltx_text" style="background-color:#FD6864;">0%</span></td>
<td id="S4.T2.1.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.4.1.4.1" class="ltx_text" style="background-color:#FD6864;">0%</span></td>
<td id="S4.T2.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.4.1.5.1" class="ltx_text" style="background-color:#FD6864;">0%</span></td>
<td id="S4.T2.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.4.1.6.1" class="ltx_text" style="background-color:#FD6864;">0%</span></td>
<td id="S4.T2.1.4.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.4.1.7.1" class="ltx_text" style="background-color:#FD6864;">0%</span></td>
</tr>
<tr id="S4.T2.1.5.2" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T2.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T2.1.5.2.1.1" class="ltx_text" style="background-color:#C0C0C0;">[0.01, 0.2)</span></th>
<td id="S4.T2.1.5.2.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.5.2.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T2.1.5.2.2.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0%</span>
</td>
<td id="S4.T2.1.5.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.5.2.3.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.5.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.5.2.4.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.5.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.5.2.5.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.5.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.5.2.6.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.5.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.5.2.7.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
</tr>
<tr id="S4.T2.1.6.3" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T2.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T2.1.6.3.1.1" class="ltx_text" style="background-color:#C0C0C0;">[0.2, 0.4)</span></th>
<td id="S4.T2.1.6.3.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.6.3.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T2.1.6.3.2.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0%</span>
</td>
<td id="S4.T2.1.6.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.6.3.3.1" class="ltx_text" style="background-color:#9AFF99;">100%</span></td>
<td id="S4.T2.1.6.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.6.3.4.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.6.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.6.3.5.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.6.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.6.3.6.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.6.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.6.3.7.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
</tr>
<tr id="S4.T2.1.7.4" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T2.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T2.1.7.4.1.1" class="ltx_text" style="background-color:#C0C0C0;">[0.4, 0.6)</span></th>
<td id="S4.T2.1.7.4.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.7.4.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T2.1.7.4.2.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0%</span>
</td>
<td id="S4.T2.1.7.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.7.4.3.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.7.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.7.4.4.1" class="ltx_text" style="background-color:#9AFF99;">98%</span></td>
<td id="S4.T2.1.7.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.7.4.5.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.7.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.7.4.6.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.7.4.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.7.4.7.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
</tr>
<tr id="S4.T2.1.8.5" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T2.1.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T2.1.8.5.1.1" class="ltx_text" style="background-color:#C0C0C0;">[0.6, 0.8)</span></th>
<td id="S4.T2.1.8.5.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.8.5.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T2.1.8.5.2.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0%</span>
</td>
<td id="S4.T2.1.8.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.8.5.3.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.8.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.8.5.4.1" class="ltx_text" style="background-color:#9AFF99;">2%</span></td>
<td id="S4.T2.1.8.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.8.5.5.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.8.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.8.5.6.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.8.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.8.5.7.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
</tr>
<tr id="S4.T2.1.9.6" class="ltx_tr" style="background-color:#9AFF99;">
<th id="S4.T2.1.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="background-color:#C0C0C0;"><span id="S4.T2.1.9.6.1.1" class="ltx_text" style="background-color:#C0C0C0;">[0.8, 1]</span></th>
<td id="S4.T2.1.9.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S4.T2.1.9.6.2.1" class="ltx_ERROR undefined">\ul</span><span id="S4.T2.1.9.6.2.2" class="ltx_text ltx_font_bold" style="background-color:#9AFF99;">0%</span>
</td>
<td id="S4.T2.1.9.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.9.6.3.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.9.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.9.6.4.1" class="ltx_text" style="background-color:#9AFF99;">0%</span></td>
<td id="S4.T2.1.9.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.9.6.5.1" class="ltx_text" style="background-color:#9AFF99;">100%</span></td>
<td id="S4.T2.1.9.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.9.6.6.1" class="ltx_text" style="background-color:#9AFF99;">100%</span></td>
<td id="S4.T2.1.9.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.9.6.7.1" class="ltx_text" style="background-color:#9AFF99;">100%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Implementation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We developed <span id="S5.p1.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> by extending the publicly available IBM  <span title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Framework for Federated Learning</span></span> (<abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(Ludwig et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> to support trustworthy aggregation, decentralized multi-aggregators with model partitioning, and dynamic permutation of model updates. We containerized the aggregator application and employed Kata Container <cite class="ltx_cite ltx_citemacro_citep">(Kata Containers, <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> to deploy aggregator containers inside lightweight <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr>.
We used an AMD EPYC 7642 (ROME) microprocessor running <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> API Version 0.22 <cite class="ltx_cite ltx_citemacro_citep">(SEV API, <a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite>. We extended QEMU with Feldman-Fitzthum’s patch <cite class="ltx_cite ltx_citemacro_citep">(Feldman-Fitzthum, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This patch will be included in the release of QEMU 6.0</span></span></span> to support AMD <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> <em id="S5.p1.1.2" class="ltx_emph ltx_font_italic">LAUNCH_SECRET</em> and extended Kata-runtime to provide remote attestation via client-side gRPC <cite class="ltx_cite ltx_citemacro_citep">(gRPC, <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite> communication with the attestation server. Finally, We implemented our attestation server as a gRPC service using a modified version of the AMD <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>-Tool <cite class="ltx_cite ltx_citemacro_citep">(SEV-Tool, <a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite> to support <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">EVM</span></abbr> owners’ tasks, e.g., attesting the AMD <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr>-enabled hardware platform, verifying the <abbr title="Open Virtual Machine Firmware" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OVMF</span></abbr> launch measurement, and generating the launch blob.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Security Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.3" class="ltx_p">We evaluated the effectiveness of <span id="S6.p1.3.1" class="ltx_text ltx_font_smallcaps">Truda</span> against three <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> attacks that attempted to reconstruct training data based on model updates: <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite>, <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>, and <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. We used the implementations from their public git repositories <cite class="ltx_cite ltx_citemacro_citep">(DLG git
repository, <a href="#bib.bib11" title="" class="ltx_ref">2020</a>; iDLG git
repository, <a href="#bib.bib23" title="" class="ltx_ref">2020</a>; IG git repository, <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> for our experiments. First, we evaluated each attack with only model partitioning enabled, varying the partition factor by <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="40\%" display="inline"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mn id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">40</mn><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">40\%</annotation></semantics></math>, thus lowering the percentage of model updates accessible to the attack. A partition factor of <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S6.p1.2.m2.1a"><mrow id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mn id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml">100</mn><mo id="S6.p1.2.m2.1.1.1" xref="S6.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">100\%</annotation></semantics></math> means the attack had access to the entire model update. Then, we enabled dynamic permutation together with model partitioning and re-evaluated the model, again varying the partition factor by <math id="S6.p1.3.m3.1" class="ltx_Math" alttext="40\%" display="inline"><semantics id="S6.p1.3.m3.1a"><mrow id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml"><mn id="S6.p1.3.m3.1.1.2" xref="S6.p1.3.m3.1.1.2.cmml">40</mn><mo id="S6.p1.3.m3.1.1.1" xref="S6.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><apply id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1"><csymbol cd="latexml" id="S6.p1.3.m3.1.1.1.cmml" xref="S6.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S6.p1.3.m3.1.1.2.cmml" xref="S6.p1.3.m3.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">40\%</annotation></semantics></math>.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The current design of <span id="S6.p2.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> does not allow aggregators to maintain a global model, thus they have no knowledge of the model architecture. <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr>, <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr>, and <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> can neither retrieve the unmodified model updates associated with an input sample nor query the model for the dummy input’s current loss gradients. As such, in a real deployment of <span id="S6.p2.1.2" class="ltx_text ltx_font_smallcaps">Truda</span>, these attacks would not succeed as they lack both these two critical components. However, to analyze the effects of the security measures of <span id="S6.p2.1.3" class="ltx_text ltx_font_smallcaps">Truda</span>, we relaxed the constraints and allowed adversaries to query the complete, unperturbed model as a blackbox. Therefore, the attacks could compute the dummy inputs’ loss gradients, but the original inputs’ loss gradients were still transformed by <span id="S6.p2.1.4" class="ltx_text ltx_font_smallcaps">Truda</span>. In this stronger attack scenario, we demonstrate that <span id="S6.p2.1.5" class="ltx_text ltx_font_smallcaps">Truda</span> still remains effective and prevents the attacks from leaking information through reconstructing data from model updates.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>DLG and iDLG Results</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.3" class="ltx_p">We used a randomly initialized <em id="S6.SS1.p1.3.1" class="ltx_emph ltx_font_italic">LeNet</em> model for evaluation as done by prior works <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>; Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite> and evaluated both attacks using <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mn id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><cn type="integer" id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">1000</annotation></semantics></math> randomly selected inputs from the <em id="S6.SS1.p1.3.2" class="ltx_emph ltx_font_italic">CIFAR-100</em> dataset. <em id="S6.SS1.p1.3.3" class="ltx_emph ltx_font_italic">CIFAR-100</em> is a dataset with <math id="S6.SS1.p1.2.m2.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S6.SS1.p1.2.m2.1a"><mrow id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml"><mn id="S6.SS1.p1.2.m2.1.1.2" xref="S6.SS1.p1.2.m2.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS1.p1.2.m2.1.1.1" xref="S6.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S6.SS1.p1.2.m2.1.1.3" xref="S6.SS1.p1.2.m2.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><apply id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1"><times id="S6.SS1.p1.2.m2.1.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1.1"></times><cn type="integer" id="S6.SS1.p1.2.m2.1.1.2.cmml" xref="S6.SS1.p1.2.m2.1.1.2">32</cn><cn type="integer" id="S6.SS1.p1.2.m2.1.1.3.cmml" xref="S6.SS1.p1.2.m2.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">32\times 32</annotation></semantics></math> color images in <math id="S6.SS1.p1.3.m3.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S6.SS1.p1.3.m3.1a"><mn id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><cn type="integer" id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">100</annotation></semantics></math> classes. We ran each attack for 300 iterations. The effectiveness of <span id="S6.SS1.p1.3.4" class="ltx_text ltx_font_smallcaps">Truda</span> against <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> and <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> is reported in Table <a href="#S4.T1" title="Table 1 ‣ 4.3. Shuffled Aggregation ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.5" class="ltx_p">We partitioned the results into six ranges based on the  <span title="Mean Squared Error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Mean Squared Error</span></span> (<abbr title="Mean Squared Error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr>) of each image. <abbr title="Mean Squared Error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr> is the metric adopted in <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr>/<abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> for measuring the quality of reconstructed images in <em id="S6.SS1.p2.5.1" class="ltx_emph ltx_font_italic">CIFAR-100</em>. Through visual inspection, an <abbr title="Mean Squared Error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr> below <math id="S6.SS1.p2.1.m1.3" class="ltx_Math" alttext="1.0\text{\times}{10}^{-03}" display="inline"><semantics id="S6.SS1.p2.1.m1.3a"><mrow id="S6.SS1.p2.1.m1.3.3.3" xref="S6.SS1.p2.1.m1.3.3.3.cmml"><mn id="S6.SS1.p2.1.m1.1.1.1.1.1.1.1" xref="S6.SS1.p2.1.m1.3.3.3.cmml">1.0</mn><mtext id="S6.SS1.p2.1.m1.2.2.2.2.2.2.2" xref="S6.SS1.p2.1.m1.3.3.3.cmml">×</mtext><msup id="S6.SS1.p2.1.m1.3.3.3.3.3.3.3" xref="S6.SS1.p2.1.m1.3.3.3.cmml"><mn id="S6.SS1.p2.1.m1.3.3.3.3.3.3.3.2" xref="S6.SS1.p2.1.m1.3.3.3.cmml">10</mn><mrow id="S6.SS1.p2.1.m1.3.3.3.3.3.3.3.3.2" xref="S6.SS1.p2.1.m1.3.3.3.cmml"><mo id="S6.SS1.p2.1.m1.3.3.3.3.3.3.3.3.2a" xref="S6.SS1.p2.1.m1.3.3.3.cmml">−</mo><mn id="S6.SS1.p2.1.m1.3.3.3.3.3.3.3.3.2.2" xref="S6.SS1.p2.1.m1.3.3.3.cmml">03</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.3b"><csymbol cd="latexml" id="S6.SS1.p2.1.m1.3.3.3.cmml" xref="S6.SS1.p2.1.m1.3.3.3">1.0E-03</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.3c">1.0\text{\times}{10}^{-03}</annotation></semantics></math> compared to the original images resulted in recognizable reconstructions. We highlight this threshold in red in Table <a href="#S4.T1" title="Table 1 ‣ 4.3. Shuffled Aggregation ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Without <span id="S6.SS1.p2.5.2" class="ltx_text ltx_font_smallcaps">Truda</span> in place, <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> and <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr>, resulted in generating <math id="S6.SS1.p2.2.m2.1" class="ltx_Math" alttext="66.6\%" display="inline"><semantics id="S6.SS1.p2.2.m2.1a"><mrow id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml"><mn id="S6.SS1.p2.2.m2.1.1.2" xref="S6.SS1.p2.2.m2.1.1.2.cmml">66.6</mn><mo id="S6.SS1.p2.2.m2.1.1.1" xref="S6.SS1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><apply id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.p2.2.m2.1.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S6.SS1.p2.2.m2.1.1.2.cmml" xref="S6.SS1.p2.2.m2.1.1.2">66.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">66.6\%</annotation></semantics></math> and <math id="S6.SS1.p2.3.m3.1" class="ltx_Math" alttext="83.7\%" display="inline"><semantics id="S6.SS1.p2.3.m3.1a"><mrow id="S6.SS1.p2.3.m3.1.1" xref="S6.SS1.p2.3.m3.1.1.cmml"><mn id="S6.SS1.p2.3.m3.1.1.2" xref="S6.SS1.p2.3.m3.1.1.2.cmml">83.7</mn><mo id="S6.SS1.p2.3.m3.1.1.1" xref="S6.SS1.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.1b"><apply id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S6.SS1.p2.3.m3.1.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S6.SS1.p2.3.m3.1.1.2.cmml" xref="S6.SS1.p2.3.m3.1.1.2">83.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.1c">83.7\%</annotation></semantics></math> recognizable reconstructions respectively with a partition factor of <math id="S6.SS1.p2.4.m4.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S6.SS1.p2.4.m4.1a"><mrow id="S6.SS1.p2.4.m4.1.1" xref="S6.SS1.p2.4.m4.1.1.cmml"><mn id="S6.SS1.p2.4.m4.1.1.2" xref="S6.SS1.p2.4.m4.1.1.2.cmml">100</mn><mo id="S6.SS1.p2.4.m4.1.1.1" xref="S6.SS1.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.4.m4.1b"><apply id="S6.SS1.p2.4.m4.1.1.cmml" xref="S6.SS1.p2.4.m4.1.1"><csymbol cd="latexml" id="S6.SS1.p2.4.m4.1.1.1.cmml" xref="S6.SS1.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.p2.4.m4.1.1.2.cmml" xref="S6.SS1.p2.4.m4.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.4.m4.1c">100\%</annotation></semantics></math> (i.e., no model partitioning). These results are used as the baseline and highlighted in the two <em id="S6.SS1.p2.5.3" class="ltx_emph ltx_font_italic">underlined</em> columns. However, as soon as model partitioning is enabled, the reconstruction quality drops significantly. Due to model partitioning, both attacks’ estimates of the original gradients are increasingly inaccurate as fewer of the original model’s weights are available. In turn, both attacks cannot correctly minimize the cost function, which we observed during the attack process. With only a <math id="S6.SS1.p2.5.m5.1" class="ltx_Math" alttext="60\%" display="inline"><semantics id="S6.SS1.p2.5.m5.1a"><mrow id="S6.SS1.p2.5.m5.1.1" xref="S6.SS1.p2.5.m5.1.1.cmml"><mn id="S6.SS1.p2.5.m5.1.1.2" xref="S6.SS1.p2.5.m5.1.1.2.cmml">60</mn><mo id="S6.SS1.p2.5.m5.1.1.1" xref="S6.SS1.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.5.m5.1b"><apply id="S6.SS1.p2.5.m5.1.1.cmml" xref="S6.SS1.p2.5.m5.1.1"><csymbol cd="latexml" id="S6.SS1.p2.5.m5.1.1.1.cmml" xref="S6.SS1.p2.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.p2.5.m5.1.1.2.cmml" xref="S6.SS1.p2.5.m5.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.5.m5.1c">60\%</annotation></semantics></math> partition rate, neither attack is able to generate any recognizable reconstructions. Enabling dynamic permutation in addition to model partitioning adds an additional layer of protection against reconstruction attacks as evidenced by the increased <abbr title="Mean Squared Error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr> of the reconstructions in Table <a href="#S4.T1" title="Table 1 ‣ 4.3. Shuffled Aggregation ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Even with all of the model weights, neither attack is able to generate an recognizable reconstruction as dynamic permutation of the model weights prevents the attacks from correctly aligning their gradient estimations. In the first and second rows of Figure <a href="#S6.F3" title="Figure 3 ‣ 6.1. DLG and iDLG Results ‣ 6. Security Analysis ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present the reconstructions generated by the <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> and <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> attacks without and with <span id="S6.SS1.p2.5.4" class="ltx_text ltx_font_smallcaps">Truda</span> enabled for one of the images. In Appendix <a href="#A2" title="Appendix B Reconstruction Examples of DLG/iDLG/IG ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>, we include another example (Figure <a href="#A0.F7" title="Figure 7 ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) with more details of intermediate iterations.</p>
</div>
<figure id="S6.F3" class="ltx_figure"><img src="/html/2105.09400/assets/x3.png" id="S6.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Reconstruction Examples of DLG/iDLG/IG with Model Partitioning and Permutation</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>IG Results</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.4" class="ltx_p">We used a randomly initialized <em id="S6.SS2.p1.4.1" class="ltx_emph ltx_font_italic">ResNet-18</em> model for evaluation as done by prior work <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> and evaluated <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> using <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mn id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><cn type="integer" id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">50</annotation></semantics></math> randomly selected inputs from the <em id="S6.SS2.p1.4.2" class="ltx_emph ltx_font_italic">ImageNet</em> dataset. <em id="S6.SS2.p1.4.3" class="ltx_emph ltx_font_italic">ImageNet</em> is a dataset with <math id="S6.SS2.p1.2.m2.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S6.SS2.p1.2.m2.1a"><mrow id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml"><mn id="S6.SS2.p1.2.m2.1.1.2" xref="S6.SS2.p1.2.m2.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS2.p1.2.m2.1.1.1" xref="S6.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S6.SS2.p1.2.m2.1.1.3" xref="S6.SS2.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><apply id="S6.SS2.p1.2.m2.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1"><times id="S6.SS2.p1.2.m2.1.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S6.SS2.p1.2.m2.1.1.2.cmml" xref="S6.SS2.p1.2.m2.1.1.2">224</cn><cn type="integer" id="S6.SS2.p1.2.m2.1.1.3.cmml" xref="S6.SS2.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">224\times 224</annotation></semantics></math> color images in <math id="S6.SS2.p1.3.m3.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S6.SS2.p1.3.m3.1a"><mn id="S6.SS2.p1.3.m3.1.1" xref="S6.SS2.p1.3.m3.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.3.m3.1b"><cn type="integer" id="S6.SS2.p1.3.m3.1.1.cmml" xref="S6.SS2.p1.3.m3.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.3.m3.1c">1000</annotation></semantics></math> classes. We ran the attack for <math id="S6.SS2.p1.4.m4.2" class="ltx_Math" alttext="24,000" display="inline"><semantics id="S6.SS2.p1.4.m4.2a"><mrow id="S6.SS2.p1.4.m4.2.3.2" xref="S6.SS2.p1.4.m4.2.3.1.cmml"><mn id="S6.SS2.p1.4.m4.1.1" xref="S6.SS2.p1.4.m4.1.1.cmml">24</mn><mo id="S6.SS2.p1.4.m4.2.3.2.1" xref="S6.SS2.p1.4.m4.2.3.1.cmml">,</mo><mn id="S6.SS2.p1.4.m4.2.2" xref="S6.SS2.p1.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.4.m4.2b"><list id="S6.SS2.p1.4.m4.2.3.1.cmml" xref="S6.SS2.p1.4.m4.2.3.2"><cn type="integer" id="S6.SS2.p1.4.m4.1.1.cmml" xref="S6.SS2.p1.4.m4.1.1">24</cn><cn type="integer" id="S6.SS2.p1.4.m4.2.2.cmml" xref="S6.SS2.p1.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.4.m4.2c">24,000</annotation></semantics></math> iterations with two random restarts.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.7" class="ltx_p">The authors of <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> remarked that the reconstruction quality and amount of information leaked are highly dependent on the images. <abbr title="Mean Squared Error" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSE</span></abbr> is no longer an accurate metric for measuring image similarity for large-sized data samples of <em id="S6.SS2.p2.7.1" class="ltx_emph ltx_font_italic">ImageNet</em>. Instead, we measured the <em id="S6.SS2.p2.7.2" class="ltx_emph ltx_font_italic">cosine distance</em>, which is used as the cost function of <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr>, to show that <span id="S6.SS2.p2.7.3" class="ltx_text ltx_font_smallcaps">Truda</span> effectively hinders the optimization procedure. We partitioned the cosine distance (bounded in <math id="S6.SS2.p2.1.m1.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S6.SS2.p2.1.m1.2a"><mrow id="S6.SS2.p2.1.m1.2.3.2" xref="S6.SS2.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S6.SS2.p2.1.m1.2.3.2.1" xref="S6.SS2.p2.1.m1.2.3.1.cmml">[</mo><mn id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">0</mn><mo id="S6.SS2.p2.1.m1.2.3.2.2" xref="S6.SS2.p2.1.m1.2.3.1.cmml">,</mo><mn id="S6.SS2.p2.1.m1.2.2" xref="S6.SS2.p2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S6.SS2.p2.1.m1.2.3.2.3" xref="S6.SS2.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.2b"><interval closure="closed" id="S6.SS2.p2.1.m1.2.3.1.cmml" xref="S6.SS2.p2.1.m1.2.3.2"><cn type="integer" id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">0</cn><cn type="integer" id="S6.SS2.p2.1.m1.2.2.cmml" xref="S6.SS2.p2.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.2c">[0,1]</annotation></semantics></math>) into six ranges and present the results in Table <a href="#S4.T2" title="Table 2 ‣ 4.3. Shuffled Aggregation ‣ 4. System Design ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Without <span id="S6.SS2.p2.7.4" class="ltx_text ltx_font_smallcaps">Truda</span> in place, the cosine distance of <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr>’s cost function is always smaller than <math id="S6.SS2.p2.2.m2.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S6.SS2.p2.2.m2.1a"><mn id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><cn type="float" id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">0.01</annotation></semantics></math> with a partition factor of <math id="S6.SS2.p2.3.m3.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S6.SS2.p2.3.m3.1a"><mrow id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml"><mn id="S6.SS2.p2.3.m3.1.1.2" xref="S6.SS2.p2.3.m3.1.1.2.cmml">100</mn><mo id="S6.SS2.p2.3.m3.1.1.1" xref="S6.SS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><apply id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S6.SS2.p2.3.m3.1.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S6.SS2.p2.3.m3.1.1.2.cmml" xref="S6.SS2.p2.3.m3.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">100\%</annotation></semantics></math> (i.e., no model partitioning). These results are used as the baseline and highlighted in the column with the underlined values. With <span id="S6.SS2.p2.7.5" class="ltx_text ltx_font_smallcaps">Truda</span> enabled, <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> can no longer correctly minimize the cost function. The cosine distance values in the optimization procedures stuck at the level significantly larger than <math id="S6.SS2.p2.4.m4.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S6.SS2.p2.4.m4.1a"><mn id="S6.SS2.p2.4.m4.1.1" xref="S6.SS2.p2.4.m4.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.4.m4.1b"><cn type="float" id="S6.SS2.p2.4.m4.1.1.cmml" xref="S6.SS2.p2.4.m4.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.4.m4.1c">0.01</annotation></semantics></math>. For example, with a <math id="S6.SS2.p2.5.m5.1" class="ltx_Math" alttext="60\%" display="inline"><semantics id="S6.SS2.p2.5.m5.1a"><mrow id="S6.SS2.p2.5.m5.1.1" xref="S6.SS2.p2.5.m5.1.1.cmml"><mn id="S6.SS2.p2.5.m5.1.1.2" xref="S6.SS2.p2.5.m5.1.1.2.cmml">60</mn><mo id="S6.SS2.p2.5.m5.1.1.1" xref="S6.SS2.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.5.m5.1b"><apply id="S6.SS2.p2.5.m5.1.1.cmml" xref="S6.SS2.p2.5.m5.1.1"><csymbol cd="latexml" id="S6.SS2.p2.5.m5.1.1.1.cmml" xref="S6.SS2.p2.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S6.SS2.p2.5.m5.1.1.2.cmml" xref="S6.SS2.p2.5.m5.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.5.m5.1c">60\%</annotation></semantics></math> partition rate and no permutation, all cosine distance values are in the range of <math id="S6.SS2.p2.6.m6.2" class="ltx_Math" alttext="[0.2,0.4)" display="inline"><semantics id="S6.SS2.p2.6.m6.2a"><mrow id="S6.SS2.p2.6.m6.2.3.2" xref="S6.SS2.p2.6.m6.2.3.1.cmml"><mo stretchy="false" id="S6.SS2.p2.6.m6.2.3.2.1" xref="S6.SS2.p2.6.m6.2.3.1.cmml">[</mo><mn id="S6.SS2.p2.6.m6.1.1" xref="S6.SS2.p2.6.m6.1.1.cmml">0.2</mn><mo id="S6.SS2.p2.6.m6.2.3.2.2" xref="S6.SS2.p2.6.m6.2.3.1.cmml">,</mo><mn id="S6.SS2.p2.6.m6.2.2" xref="S6.SS2.p2.6.m6.2.2.cmml">0.4</mn><mo stretchy="false" id="S6.SS2.p2.6.m6.2.3.2.3" xref="S6.SS2.p2.6.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.6.m6.2b"><interval closure="closed-open" id="S6.SS2.p2.6.m6.2.3.1.cmml" xref="S6.SS2.p2.6.m6.2.3.2"><cn type="float" id="S6.SS2.p2.6.m6.1.1.cmml" xref="S6.SS2.p2.6.m6.1.1">0.2</cn><cn type="float" id="S6.SS2.p2.6.m6.2.2.cmml" xref="S6.SS2.p2.6.m6.2.2">0.4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.6.m6.2c">[0.2,0.4)</annotation></semantics></math>. With permutation enabled, the cosine distance is further increased to the range of <math id="S6.SS2.p2.7.m7.2" class="ltx_Math" alttext="[0.8,1]" display="inline"><semantics id="S6.SS2.p2.7.m7.2a"><mrow id="S6.SS2.p2.7.m7.2.3.2" xref="S6.SS2.p2.7.m7.2.3.1.cmml"><mo stretchy="false" id="S6.SS2.p2.7.m7.2.3.2.1" xref="S6.SS2.p2.7.m7.2.3.1.cmml">[</mo><mn id="S6.SS2.p2.7.m7.1.1" xref="S6.SS2.p2.7.m7.1.1.cmml">0.8</mn><mo id="S6.SS2.p2.7.m7.2.3.2.2" xref="S6.SS2.p2.7.m7.2.3.1.cmml">,</mo><mn id="S6.SS2.p2.7.m7.2.2" xref="S6.SS2.p2.7.m7.2.2.cmml">1</mn><mo stretchy="false" id="S6.SS2.p2.7.m7.2.3.2.3" xref="S6.SS2.p2.7.m7.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.7.m7.2b"><interval closure="closed" id="S6.SS2.p2.7.m7.2.3.1.cmml" xref="S6.SS2.p2.7.m7.2.3.2"><cn type="float" id="S6.SS2.p2.7.m7.1.1.cmml" xref="S6.SS2.p2.7.m7.1.1">0.8</cn><cn type="integer" id="S6.SS2.p2.7.m7.2.2.cmml" xref="S6.SS2.p2.7.m7.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.7.m7.2c">[0.8,1]</annotation></semantics></math>. Visual inspection of the results reveals that <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> cannot generate any recognizable reconstructions with <span id="S6.SS2.p2.7.6" class="ltx_text ltx_font_smallcaps">Truda</span>’s partitioning and permutation in place. In the third row of Figure <a href="#S6.F3" title="Figure 3 ‣ 6.1. DLG and iDLG Results ‣ 6. Security Analysis ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we present the reconstructions generated by the <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> attack without and with <span id="S6.SS2.p2.7.7" class="ltx_text ltx_font_smallcaps">Truda</span> enabled for one of the images. In Appendix <a href="#A2" title="Appendix B Reconstruction Examples of DLG/iDLG/IG ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>, we include five more reconstruction examples for <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> in Figure <a href="#A2.F8" title="Figure 8 ‣ Appendix B Reconstruction Examples of DLG/iDLG/IG ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Performance Evaluation</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We evaluated the performance of <span id="S7.p1.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> with two metrics. First, we measured the loss/accuracy of the models generated at each training round. It demonstrates that the convergence rate of <span id="S7.p1.1.2" class="ltx_text ltx_font_smallcaps">Truda</span> is aligned with the base system and <span id="S7.p1.1.3" class="ltx_text ltx_font_smallcaps">Truda</span> does not lead to model accuracy degradation. Second, we measured the latency of <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training. The latency of model training refers to the total time to finish a specified number of training rounds. We recorded the time after finishing each training round at the aggregator. The latency results reflect the performance overhead incurred by the security features added in <span id="S7.p1.1.4" class="ltx_text ltx_font_smallcaps">Truda</span>.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Our evaluation covers a spectrum of <em id="S7.p2.1.1" class="ltx_emph ltx_font_italic">cross-silo</em> <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training applications from three aspects: (1) adaptability to different <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregation algorithms, (2) performance comparisons with different numbers of participating parties, and (3) support for larger <abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr> models with non-IID training data distribution.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">In our evaluation environment, each party ran within a <abbr title="virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VM</span></abbr> in a datacenter. We assigned each <abbr title="virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VM</span></abbr> with 16 cores of Intel Xeon E5-2690 CPU, one Nvidia Tesla P100 GPU, 120 GB DRAM; the <abbr title="operating system" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OS</span></abbr> is Redhat Enterprise Linux 7.0-64. We set up three aggregators to run within the <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> <abbr title="encrypted virtual machine" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">EVMs</span></abbr>. The aggregators ran on a machine with AMD EPYC 7642 CPU; the host OS is Ubuntu 20.04 LTS. The baseline for comparison is IBM <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> with one central aggregator. The party’s configurations, model architectures, and hyper-parameter settings are the same as for <span id="S7.p3.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr>.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Training with Different Fusion Algorithms</h3>

<figure id="S7.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S7.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x4.png" id="S7.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="152" height="111" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Loss/Accuracy Comparison: Iterative Averaging</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S7.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x5.png" id="S7.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="152" height="111" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Loss/Accuracy Comparison: Coordinate Median</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S7.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x6.png" id="S7.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="152" height="111" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Loss/Accuracy Comparison: Paillier crypto system</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S7.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x7.png" id="S7.F4.sf4.g1" class="ltx_graphics ltx_img_landscape" width="152" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Latency Comparison: Iterative Averaging</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S7.F4.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x8.png" id="S7.F4.sf5.g1" class="ltx_graphics ltx_img_landscape" width="152" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Latency Comparison: Coordinate Median</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S7.F4.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x9.png" id="S7.F4.sf6.g1" class="ltx_graphics ltx_img_landscape" width="152" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Latency Comparison: Paillier crypto system</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>MNIST Loss/Accuracy/Latency Comparison Between <span id="S7.F4.2.1" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> (IID with Four Parties)</figcaption>
</figure>
<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">As indicated in the <span id="S7.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">Truda</span>’s design, we support aggregation algorithms with coordinate-wise arithmetic operations. We evaluated <span id="S7.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">Truda</span> respectively with three aggregation algorithms, i.e., <em id="S7.SS1.p1.1.3" class="ltx_emph ltx_font_italic">Iterative Averaging</em>, <em id="S7.SS1.p1.1.4" class="ltx_emph ltx_font_italic">Coordinate Median</em><cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2018</a>)</cite>, and <em id="S7.SS1.p1.1.5" class="ltx_emph ltx_font_italic">Paillier</em><cite class="ltx_cite ltx_citemacro_citep">(Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite>, with the <em id="S7.SS1.p1.1.6" class="ltx_emph ltx_font_italic">MNIST</em> dataset. <em id="S7.SS1.p1.1.7" class="ltx_emph ltx_font_italic">Iterative Averaging</em> is the base algorithm supporting <em id="S7.SS1.p1.1.8" class="ltx_emph ltx_font_italic"><abbr title="Federated Averaging" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedAvg</span></abbr></em> and <em id="S7.SS1.p1.1.9" class="ltx_emph ltx_font_italic"><abbr title="Federated Stochastic Gradient Descent" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FedSGD</span></abbr></em>. It sends queries to all registered parties at each training round to collect information, e.g., model updates or gradients, averages the updates, and broadcasts the fused results to all parties. <em id="S7.SS1.p1.1.10" class="ltx_emph ltx_font_italic">Coordinate Median</em> is a fusion algorithm that selects a coordinate-wise median from collected responses in order to tolerate Byzantine failures of adversarial parties. The <em id="S7.SS1.p1.1.11" class="ltx_emph ltx_font_italic">Paillier crypto fusion</em> algorithm supports aggregation with Additively Homomorphic Encryption<cite class="ltx_cite ltx_citemacro_citep">(Paillier, <a href="#bib.bib45" title="" class="ltx_ref">1999</a>)</cite>. We trained deep learning models on the <em id="S7.SS1.p1.1.12" class="ltx_emph ltx_font_italic">MNIST</em> dataset with ten training rounds for <em id="S7.SS1.p1.1.13" class="ltx_emph ltx_font_italic">Iterative Averaging</em>, <em id="S7.SS1.p1.1.14" class="ltx_emph ltx_font_italic">Coordinate Median</em> and three rounds for <em id="S7.SS1.p1.1.15" class="ltx_emph ltx_font_italic">Paillier</em>. Each round has three local epochs.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.2" class="ltx_p"><em id="S7.SS1.p2.2.1" class="ltx_emph ltx_font_italic">MNIST</em> contains <math id="S7.SS1.p2.1.m1.2" class="ltx_Math" alttext="60,000" display="inline"><semantics id="S7.SS1.p2.1.m1.2a"><mrow id="S7.SS1.p2.1.m1.2.3.2" xref="S7.SS1.p2.1.m1.2.3.1.cmml"><mn id="S7.SS1.p2.1.m1.1.1" xref="S7.SS1.p2.1.m1.1.1.cmml">60</mn><mo id="S7.SS1.p2.1.m1.2.3.2.1" xref="S7.SS1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S7.SS1.p2.1.m1.2.2" xref="S7.SS1.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p2.1.m1.2b"><list id="S7.SS1.p2.1.m1.2.3.1.cmml" xref="S7.SS1.p2.1.m1.2.3.2"><cn type="integer" id="S7.SS1.p2.1.m1.1.1.cmml" xref="S7.SS1.p2.1.m1.1.1">60</cn><cn type="integer" id="S7.SS1.p2.1.m1.2.2.cmml" xref="S7.SS1.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p2.1.m1.2c">60,000</annotation></semantics></math> examples in the training set. We randomly partitioned the training set into four equal sets for four parties. Each party has <math id="S7.SS1.p2.2.m2.2" class="ltx_Math" alttext="15,000" display="inline"><semantics id="S7.SS1.p2.2.m2.2a"><mrow id="S7.SS1.p2.2.m2.2.3.2" xref="S7.SS1.p2.2.m2.2.3.1.cmml"><mn id="S7.SS1.p2.2.m2.1.1" xref="S7.SS1.p2.2.m2.1.1.cmml">15</mn><mo id="S7.SS1.p2.2.m2.2.3.2.1" xref="S7.SS1.p2.2.m2.2.3.1.cmml">,</mo><mn id="S7.SS1.p2.2.m2.2.2" xref="S7.SS1.p2.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p2.2.m2.2b"><list id="S7.SS1.p2.2.m2.2.3.1.cmml" xref="S7.SS1.p2.2.m2.2.3.2"><cn type="integer" id="S7.SS1.p2.2.m2.1.1.cmml" xref="S7.SS1.p2.2.m2.1.1">15</cn><cn type="integer" id="S7.SS1.p2.2.m2.2.2.cmml" xref="S7.SS1.p2.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p2.2.m2.2c">15,000</annotation></semantics></math> examples for local training. The trained model is a  <span title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">convolutional neural network</span></span> (<abbr title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ConvNet</span></abbr>) with eight layers. The detailed model architecture can be found in Table <a href="#A1.T3" title="Table 3 ‣ Appendix A Tables for DNN Architectures in FL Training ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> of Appendix <a href="#A1" title="Appendix A Tables for DNN Architectures in FL Training ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para ltx_noindent">
<p id="S7.SS1.p3.1" class="ltx_p"><span id="S7.SS1.p3.1.1" class="ltx_text ltx_font_bold">Accuracy/Loss and Convergence Rate.</span>
We present the model loss and accuracy at each training round in Figures <a href="#S7.F4.sf1" title="In Figure 4 ‣ 7.1. Training with Different Fusion Algorithms ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4a</span></a>, <a href="#S7.F4.sf2" title="In Figure 4 ‣ 7.1. Training with Different Fusion Algorithms ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4b</span></a>, and <a href="#S7.F4.sf3" title="In Figure 4 ‣ 7.1. Training with Different Fusion Algorithms ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4c</span></a>. The horizontal axes are the number of training rounds. The left vertical axes show the loss and the right vertical axes present the model accuracy. It is clear that the loss/accuracy results of <span id="S7.SS1.p3.1.2" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> have the same patterns for all three fusion algorithms. <span id="S7.SS1.p3.1.3" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> converge at the same rate on <em id="S7.SS1.p3.1.4" class="ltx_emph ltx_font_italic">MNIST</em> after one training round. The final models achieve the same accuracy level (above <math id="S7.SS1.p3.1.m1.1" class="ltx_Math" alttext="98\%" display="inline"><semantics id="S7.SS1.p3.1.m1.1a"><mrow id="S7.SS1.p3.1.m1.1.1" xref="S7.SS1.p3.1.m1.1.1.cmml"><mn id="S7.SS1.p3.1.m1.1.1.2" xref="S7.SS1.p3.1.m1.1.1.2.cmml">98</mn><mo id="S7.SS1.p3.1.m1.1.1.1" xref="S7.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.1.m1.1b"><apply id="S7.SS1.p3.1.m1.1.1.cmml" xref="S7.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S7.SS1.p3.1.m1.1.1.1.cmml" xref="S7.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S7.SS1.p3.1.m1.1.1.2.cmml" xref="S7.SS1.p3.1.m1.1.1.2">98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.1.m1.1c">98\%</annotation></semantics></math>) for both <span id="S7.SS1.p3.1.5" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr>.</p>
</div>
<div id="S7.SS1.p4" class="ltx_para ltx_noindent">
<p id="S7.SS1.p4.4" class="ltx_p"><span id="S7.SS1.p4.4.1" class="ltx_text ltx_font_bold">Training Latency.</span>
We present the training latency data of <span id="S7.SS1.p4.4.2" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> in Figures <a href="#S7.F4.sf4" title="In Figure 4 ‣ 7.1. Training with Different Fusion Algorithms ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4d</span></a>, <a href="#S7.F4.sf5" title="In Figure 4 ‣ 7.1. Training with Different Fusion Algorithms ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4e</span></a>, and <a href="#S7.F4.sf6" title="In Figure 4 ‣ 7.1. Training with Different Fusion Algorithms ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4f</span></a>. The vertical axes are the accumulated time spent to finish that training round. We observed that for <em id="S7.SS1.p4.4.3" class="ltx_emph ltx_font_italic">Iterative Averaging</em>, <span id="S7.SS1.p4.4.4" class="ltx_text ltx_font_smallcaps">Truda</span> used <math id="S7.SS1.p4.1.m1.1" class="ltx_Math" alttext="75.83" display="inline"><semantics id="S7.SS1.p4.1.m1.1a"><mn id="S7.SS1.p4.1.m1.1.1" xref="S7.SS1.p4.1.m1.1.1.cmml">75.83</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.1.m1.1b"><cn type="float" id="S7.SS1.p4.1.m1.1.1.cmml" xref="S7.SS1.p4.1.m1.1.1">75.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.1.m1.1c">75.83</annotation></semantics></math> seconds to finish the 10-round training and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> used <math id="S7.SS1.p4.2.m2.1" class="ltx_Math" alttext="54.32" display="inline"><semantics id="S7.SS1.p4.2.m2.1a"><mn id="S7.SS1.p4.2.m2.1.1" xref="S7.SS1.p4.2.m2.1.1.cmml">54.32</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.2.m2.1b"><cn type="float" id="S7.SS1.p4.2.m2.1.1.cmml" xref="S7.SS1.p4.2.m2.1.1">54.32</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.2.m2.1c">54.32</annotation></semantics></math> seconds. Compared to the baseline <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> system, the added security features in <span id="S7.SS1.p4.4.5" class="ltx_text ltx_font_smallcaps">Truda</span> incurred additional <math id="S7.SS1.p4.3.m3.1" class="ltx_math_unparsed" alttext="0.40\times" display="inline"><semantics id="S7.SS1.p4.3.m3.1a"><mrow id="S7.SS1.p4.3.m3.1b"><mn id="S7.SS1.p4.3.m3.1.1">0.40</mn><mo lspace="0.222em" id="S7.SS1.p4.3.m3.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S7.SS1.p4.3.m3.1c">0.40\times</annotation></semantics></math> latency for training the <em id="S7.SS1.p4.4.6" class="ltx_emph ltx_font_italic">MNIST</em> model. Similarly for <em id="S7.SS1.p4.4.7" class="ltx_emph ltx_font_italic">Coordinate Median</em>, <span id="S7.SS1.p4.4.8" class="ltx_text ltx_font_smallcaps">Truda</span> incurred additional <math id="S7.SS1.p4.4.m4.1" class="ltx_math_unparsed" alttext="0.45\times" display="inline"><semantics id="S7.SS1.p4.4.m4.1a"><mrow id="S7.SS1.p4.4.m4.1b"><mn id="S7.SS1.p4.4.m4.1.1">0.45</mn><mo lspace="0.222em" id="S7.SS1.p4.4.m4.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S7.SS1.p4.4.m4.1c">0.45\times</annotation></semantics></math> in latency.</p>
</div>
<div id="S7.SS1.p5" class="ltx_para">
<p id="S7.SS1.p5.1" class="ltx_p">Due to heavyweight additively homomorphic encryption operations, <em id="S7.SS1.p5.1.1" class="ltx_emph ltx_font_italic">Paillier crypto fusion</em> is two orders slower for training the same <em id="S7.SS1.p5.1.2" class="ltx_emph ltx_font_italic">MNIST</em> model than <em id="S7.SS1.p5.1.3" class="ltx_emph ltx_font_italic">Iterative Averaging</em> and <em id="S7.SS1.p5.1.4" class="ltx_emph ltx_font_italic">Coordinate Median</em>. However, <span id="S7.SS1.p5.1.5" class="ltx_text ltx_font_smallcaps">Truda</span> finished training with <math id="S7.SS1.p5.1.m1.1" class="ltx_math_unparsed" alttext="0.04\times" display="inline"><semantics id="S7.SS1.p5.1.m1.1a"><mrow id="S7.SS1.p5.1.m1.1b"><mn id="S7.SS1.p5.1.m1.1.1">0.04</mn><mo lspace="0.222em" id="S7.SS1.p5.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S7.SS1.p5.1.m1.1c">0.04\times</annotation></semantics></math> improvement in latency compared to <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr>. The reason is that the dominant performance factors of <em id="S7.SS1.p5.1.6" class="ltx_emph ltx_font_italic">Paillier</em> fusion are the encryption/decryption operations. However, as the models are partitioned in <span id="S7.SS1.p5.1.7" class="ltx_text ltx_font_smallcaps">Truda</span> for multiple decentralized aggregators, the <em id="S7.SS1.p5.1.8" class="ltx_emph ltx_font_italic">Paillier</em> encryption/decryption and aggregation are accelerated — computed in parallel by operating on smaller model partitions both on the aggregators and on the parties.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Training with Different Numbers of Parties</h3>

<figure id="S7.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x10.png" id="S7.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="230" height="168" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Loss/Accuracy Comparison (4P vs. 8P)</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x11.png" id="S7.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="230" height="166" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Latency Comparison (4P vs. 8P)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>CIFAR10 Loss/Accuracy/Latency Comparison Between <span id="S7.F5.2.1" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> (IID with Four/Eight Parties)</figcaption>
</figure>
<figure id="S7.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x12.png" id="S7.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="230" height="168" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Loss/Accuracy Comparison</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2105.09400/assets/x13.png" id="S7.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="230" height="164" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Latency Comparison</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>RVL-CDIP Loss/Accuracy/Latency Comparison Between <span id="S7.F6.2.1" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> (non-IID with Eight Parties)</figcaption>
</figure>
<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.5" class="ltx_p">Here, we aim to understand the performance effects of involving more parties.
We trained a <abbr title="convolutional neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ConvNet</span></abbr> with <math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mn id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><cn type="integer" id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">23</annotation></semantics></math> layers on <em id="S7.SS2.p1.5.1" class="ltx_emph ltx_font_italic">CIFAR-10</em> with four and eight parties.
The detailed model architecture can be found in Table <a href="#A1.T4" title="Table 4 ‣ Appendix A Tables for DNN Architectures in FL Training ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> in Appendix <a href="#A1" title="Appendix A Tables for DNN Architectures in FL Training ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.
<em id="S7.SS2.p1.5.2" class="ltx_emph ltx_font_italic">CIFAR-10</em> is a dataset with <math id="S7.SS2.p1.2.m2.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S7.SS2.p1.2.m2.1a"><mrow id="S7.SS2.p1.2.m2.1.1" xref="S7.SS2.p1.2.m2.1.1.cmml"><mn id="S7.SS2.p1.2.m2.1.1.2" xref="S7.SS2.p1.2.m2.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.2.m2.1.1.1" xref="S7.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S7.SS2.p1.2.m2.1.1.3" xref="S7.SS2.p1.2.m2.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.2.m2.1b"><apply id="S7.SS2.p1.2.m2.1.1.cmml" xref="S7.SS2.p1.2.m2.1.1"><times id="S7.SS2.p1.2.m2.1.1.1.cmml" xref="S7.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S7.SS2.p1.2.m2.1.1.2.cmml" xref="S7.SS2.p1.2.m2.1.1.2">32</cn><cn type="integer" id="S7.SS2.p1.2.m2.1.1.3.cmml" xref="S7.SS2.p1.2.m2.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.2.m2.1c">32\times 32</annotation></semantics></math> color images in <math id="S7.SS2.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.SS2.p1.3.m3.1a"><mn id="S7.SS2.p1.3.m3.1.1" xref="S7.SS2.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.3.m3.1b"><cn type="integer" id="S7.SS2.p1.3.m3.1.1.cmml" xref="S7.SS2.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.3.m3.1c">10</annotation></semantics></math> classes. We randomly partition the training set into equal sets for the parties. Each party has <math id="S7.SS2.p1.4.m4.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="S7.SS2.p1.4.m4.2a"><mrow id="S7.SS2.p1.4.m4.2.3.2" xref="S7.SS2.p1.4.m4.2.3.1.cmml"><mn id="S7.SS2.p1.4.m4.1.1" xref="S7.SS2.p1.4.m4.1.1.cmml">10</mn><mo id="S7.SS2.p1.4.m4.2.3.2.1" xref="S7.SS2.p1.4.m4.2.3.1.cmml">,</mo><mn id="S7.SS2.p1.4.m4.2.2" xref="S7.SS2.p1.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.4.m4.2b"><list id="S7.SS2.p1.4.m4.2.3.1.cmml" xref="S7.SS2.p1.4.m4.2.3.2"><cn type="integer" id="S7.SS2.p1.4.m4.1.1.cmml" xref="S7.SS2.p1.4.m4.1.1">10</cn><cn type="integer" id="S7.SS2.p1.4.m4.2.2.cmml" xref="S7.SS2.p1.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.4.m4.2c">10,000</annotation></semantics></math> examples for <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training. We trained this model with <math id="S7.SS2.p1.5.m5.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S7.SS2.p1.5.m5.1a"><mn id="S7.SS2.p1.5.m5.1.1" xref="S7.SS2.p1.5.m5.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.5.m5.1b"><cn type="integer" id="S7.SS2.p1.5.m5.1.1.cmml" xref="S7.SS2.p1.5.m5.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.5.m5.1c">30</annotation></semantics></math> training rounds,
with each round consisting of one local epoch.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para ltx_noindent">
<p id="S7.SS2.p2.4" class="ltx_p"><span id="S7.SS2.p2.4.1" class="ltx_text ltx_font_bold">Accuracy/Loss and Convergence Rate.</span>
We present the model accuracy and loss at each training round in Figure <a href="#S7.F5.sf1" title="In Figure 5 ‣ 7.2. Training with Different Numbers of Parties ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5a</span></a>. The patterns for the loss and accuracy are similar for <span id="S7.SS2.p2.4.2" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> with both four parties and eight parties. It indicates that the models converge at a similar rate with different number of parties. The accuracy results of the final model trained with <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> are <math id="S7.SS2.p2.1.m1.1" class="ltx_Math" alttext="76.99\%" display="inline"><semantics id="S7.SS2.p2.1.m1.1a"><mrow id="S7.SS2.p2.1.m1.1.1" xref="S7.SS2.p2.1.m1.1.1.cmml"><mn id="S7.SS2.p2.1.m1.1.1.2" xref="S7.SS2.p2.1.m1.1.1.2.cmml">76.99</mn><mo id="S7.SS2.p2.1.m1.1.1.1" xref="S7.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.1.m1.1b"><apply id="S7.SS2.p2.1.m1.1.1.cmml" xref="S7.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S7.SS2.p2.1.m1.1.1.1.cmml" xref="S7.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S7.SS2.p2.1.m1.1.1.2.cmml" xref="S7.SS2.p2.1.m1.1.1.2">76.99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.1.m1.1c">76.99\%</annotation></semantics></math> (four parties) and <math id="S7.SS2.p2.2.m2.1" class="ltx_Math" alttext="76.43\%" display="inline"><semantics id="S7.SS2.p2.2.m2.1a"><mrow id="S7.SS2.p2.2.m2.1.1" xref="S7.SS2.p2.2.m2.1.1.cmml"><mn id="S7.SS2.p2.2.m2.1.1.2" xref="S7.SS2.p2.2.m2.1.1.2.cmml">76.43</mn><mo id="S7.SS2.p2.2.m2.1.1.1" xref="S7.SS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.2.m2.1b"><apply id="S7.SS2.p2.2.m2.1.1.cmml" xref="S7.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S7.SS2.p2.2.m2.1.1.1.cmml" xref="S7.SS2.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S7.SS2.p2.2.m2.1.1.2.cmml" xref="S7.SS2.p2.2.m2.1.1.2">76.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.2.m2.1c">76.43\%</annotation></semantics></math> (eight parties). The final model accuracy results with <span id="S7.SS2.p2.4.3" class="ltx_text ltx_font_smallcaps">Truda</span> are <math id="S7.SS2.p2.3.m3.1" class="ltx_Math" alttext="79.93\%" display="inline"><semantics id="S7.SS2.p2.3.m3.1a"><mrow id="S7.SS2.p2.3.m3.1.1" xref="S7.SS2.p2.3.m3.1.1.cmml"><mn id="S7.SS2.p2.3.m3.1.1.2" xref="S7.SS2.p2.3.m3.1.1.2.cmml">79.93</mn><mo id="S7.SS2.p2.3.m3.1.1.1" xref="S7.SS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.3.m3.1b"><apply id="S7.SS2.p2.3.m3.1.1.cmml" xref="S7.SS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S7.SS2.p2.3.m3.1.1.1.cmml" xref="S7.SS2.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S7.SS2.p2.3.m3.1.1.2.cmml" xref="S7.SS2.p2.3.m3.1.1.2">79.93</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.3.m3.1c">79.93\%</annotation></semantics></math> (four parties) and <math id="S7.SS2.p2.4.m4.1" class="ltx_Math" alttext="81.41\%" display="inline"><semantics id="S7.SS2.p2.4.m4.1a"><mrow id="S7.SS2.p2.4.m4.1.1" xref="S7.SS2.p2.4.m4.1.1.cmml"><mn id="S7.SS2.p2.4.m4.1.1.2" xref="S7.SS2.p2.4.m4.1.1.2.cmml">81.41</mn><mo id="S7.SS2.p2.4.m4.1.1.1" xref="S7.SS2.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.4.m4.1b"><apply id="S7.SS2.p2.4.m4.1.1.cmml" xref="S7.SS2.p2.4.m4.1.1"><csymbol cd="latexml" id="S7.SS2.p2.4.m4.1.1.1.cmml" xref="S7.SS2.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S7.SS2.p2.4.m4.1.1.2.cmml" xref="S7.SS2.p2.4.m4.1.1.2">81.41</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.4.m4.1c">81.41\%</annotation></semantics></math> (eight parties).</p>
</div>
<div id="S7.SS2.p3" class="ltx_para ltx_noindent">
<p id="S7.SS2.p3.8" class="ltx_p"><span id="S7.SS2.p3.8.1" class="ltx_text ltx_font_bold">Training Latency.</span>
We present the training latency data of <span id="S7.SS2.p3.8.2" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> in Figure <a href="#S7.F5.sf2" title="In Figure 5 ‣ 7.2. Training with Different Numbers of Parties ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5b</span></a>. In the four parties scenario, it took <span id="S7.SS2.p3.8.3" class="ltx_text ltx_font_smallcaps">Truda</span> <math id="S7.SS2.p3.1.m1.1" class="ltx_Math" alttext="182.91" display="inline"><semantics id="S7.SS2.p3.1.m1.1a"><mn id="S7.SS2.p3.1.m1.1.1" xref="S7.SS2.p3.1.m1.1.1.cmml">182.91</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.1.m1.1b"><cn type="float" id="S7.SS2.p3.1.m1.1.1.cmml" xref="S7.SS2.p3.1.m1.1.1">182.91</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.1.m1.1c">182.91</annotation></semantics></math> seconds to finish <math id="S7.SS2.p3.2.m2.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S7.SS2.p3.2.m2.1a"><mn id="S7.SS2.p3.2.m2.1.1" xref="S7.SS2.p3.2.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.2.m2.1b"><cn type="integer" id="S7.SS2.p3.2.m2.1.1.cmml" xref="S7.SS2.p3.2.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.2.m2.1c">30</annotation></semantics></math> rounds of training and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> <math id="S7.SS2.p3.3.m3.1" class="ltx_Math" alttext="157.41" display="inline"><semantics id="S7.SS2.p3.3.m3.1a"><mn id="S7.SS2.p3.3.m3.1.1" xref="S7.SS2.p3.3.m3.1.1.cmml">157.41</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.3.m3.1b"><cn type="float" id="S7.SS2.p3.3.m3.1.1.cmml" xref="S7.SS2.p3.3.m3.1.1">157.41</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.3.m3.1c">157.41</annotation></semantics></math> seconds. Our added features incurred additional <math id="S7.SS2.p3.4.m4.1" class="ltx_math_unparsed" alttext="0.16\times" display="inline"><semantics id="S7.SS2.p3.4.m4.1a"><mrow id="S7.SS2.p3.4.m4.1b"><mn id="S7.SS2.p3.4.m4.1.1">0.16</mn><mo lspace="0.222em" id="S7.SS2.p3.4.m4.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S7.SS2.p3.4.m4.1c">0.16\times</annotation></semantics></math> in latency for training the <em id="S7.SS2.p3.8.4" class="ltx_emph ltx_font_italic">CIFAR-10</em> model. In the eight parties scenario, it took <span id="S7.SS2.p3.8.5" class="ltx_text ltx_font_smallcaps">Truda</span> <math id="S7.SS2.p3.5.m5.1" class="ltx_Math" alttext="498.68" display="inline"><semantics id="S7.SS2.p3.5.m5.1a"><mn id="S7.SS2.p3.5.m5.1.1" xref="S7.SS2.p3.5.m5.1.1.cmml">498.68</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.5.m5.1b"><cn type="float" id="S7.SS2.p3.5.m5.1.1.cmml" xref="S7.SS2.p3.5.m5.1.1">498.68</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.5.m5.1c">498.68</annotation></semantics></math> seconds to finish <math id="S7.SS2.p3.6.m6.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S7.SS2.p3.6.m6.1a"><mn id="S7.SS2.p3.6.m6.1.1" xref="S7.SS2.p3.6.m6.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.6.m6.1b"><cn type="integer" id="S7.SS2.p3.6.m6.1.1.cmml" xref="S7.SS2.p3.6.m6.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.6.m6.1c">30</annotation></semantics></math> rounds of training and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> <math id="S7.SS2.p3.7.m7.1" class="ltx_Math" alttext="477.34" display="inline"><semantics id="S7.SS2.p3.7.m7.1a"><mn id="S7.SS2.p3.7.m7.1.1" xref="S7.SS2.p3.7.m7.1.1.cmml">477.34</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.7.m7.1b"><cn type="float" id="S7.SS2.p3.7.m7.1.1.cmml" xref="S7.SS2.p3.7.m7.1.1">477.34</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.7.m7.1c">477.34</annotation></semantics></math> seconds. The latency only increased by <math id="S7.SS2.p3.8.m8.1" class="ltx_math_unparsed" alttext="0.04\times" display="inline"><semantics id="S7.SS2.p3.8.m8.1a"><mrow id="S7.SS2.p3.8.m8.1b"><mn id="S7.SS2.p3.8.m8.1.1">0.04</mn><mo lspace="0.222em" id="S7.SS2.p3.8.m8.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S7.SS2.p3.8.m8.1c">0.04\times</annotation></semantics></math>. We also find that adding more parties increases the latency for both <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> and <span id="S7.SS2.p3.8.6" class="ltx_text ltx_font_smallcaps">Truda</span> at the same pace. The security features of <span id="S7.SS2.p3.8.7" class="ltx_text ltx_font_smallcaps">Truda</span> does not lead to additional latency with regard to more parties.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Training with Non-IID Training Data</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.8" class="ltx_p">We also measured the performance of training a larger, more complex deep learning model with non-IID training data distribution. We used a pre-trained <em id="S7.SS3.p1.8.1" class="ltx_emph ltx_font_italic">VGG-16</em> model on the <em id="S7.SS3.p1.8.2" class="ltx_emph ltx_font_italic">ImageNet</em> to train a document classifier on the <em id="S7.SS3.p1.8.3" class="ltx_emph ltx_font_italic">RVL-CDIP</em><cite class="ltx_cite ltx_citemacro_citep">(Harley
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite> dataset with <math id="S7.SS3.p1.1.m1.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S7.SS3.p1.1.m1.1a"><mn id="S7.SS3.p1.1.m1.1.1" xref="S7.SS3.p1.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.1.m1.1b"><cn type="integer" id="S7.SS3.p1.1.m1.1.1.cmml" xref="S7.SS3.p1.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.1.m1.1c">16</annotation></semantics></math> classes. For transfer learning with <em id="S7.SS3.p1.8.4" class="ltx_emph ltx_font_italic">RVL-CDIP</em> classification, we replaced the last three fully-connected layers of <em id="S7.SS3.p1.8.5" class="ltx_emph ltx_font_italic">VGG-16</em> due to differences in number of prediction classes. The detailed model architecture can be found in Table <a href="#A1.T5" title="Table 5 ‣ Appendix A Tables for DNN Architectures in FL Training ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> in Appendix <a href="#A1" title="Appendix A Tables for DNN Architectures in FL Training ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.
The <em id="S7.SS3.p1.8.6" class="ltx_emph ltx_font_italic">RVL-CDIP</em> dataset has <math id="S7.SS3.p1.2.m2.2" class="ltx_Math" alttext="320,000" display="inline"><semantics id="S7.SS3.p1.2.m2.2a"><mrow id="S7.SS3.p1.2.m2.2.3.2" xref="S7.SS3.p1.2.m2.2.3.1.cmml"><mn id="S7.SS3.p1.2.m2.1.1" xref="S7.SS3.p1.2.m2.1.1.cmml">320</mn><mo id="S7.SS3.p1.2.m2.2.3.2.1" xref="S7.SS3.p1.2.m2.2.3.1.cmml">,</mo><mn id="S7.SS3.p1.2.m2.2.2" xref="S7.SS3.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.2.m2.2b"><list id="S7.SS3.p1.2.m2.2.3.1.cmml" xref="S7.SS3.p1.2.m2.2.3.2"><cn type="integer" id="S7.SS3.p1.2.m2.1.1.cmml" xref="S7.SS3.p1.2.m2.1.1">320</cn><cn type="integer" id="S7.SS3.p1.2.m2.2.2.cmml" xref="S7.SS3.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.2.m2.2c">320,000</annotation></semantics></math> training images and <math id="S7.SS3.p1.3.m3.2" class="ltx_Math" alttext="40,000" display="inline"><semantics id="S7.SS3.p1.3.m3.2a"><mrow id="S7.SS3.p1.3.m3.2.3.2" xref="S7.SS3.p1.3.m3.2.3.1.cmml"><mn id="S7.SS3.p1.3.m3.1.1" xref="S7.SS3.p1.3.m3.1.1.cmml">40</mn><mo id="S7.SS3.p1.3.m3.2.3.2.1" xref="S7.SS3.p1.3.m3.2.3.1.cmml">,</mo><mn id="S7.SS3.p1.3.m3.2.2" xref="S7.SS3.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.3.m3.2b"><list id="S7.SS3.p1.3.m3.2.3.1.cmml" xref="S7.SS3.p1.3.m3.2.3.2"><cn type="integer" id="S7.SS3.p1.3.m3.1.1.cmml" xref="S7.SS3.p1.3.m3.1.1">40</cn><cn type="integer" id="S7.SS3.p1.3.m3.2.2.cmml" xref="S7.SS3.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.3.m3.2c">40,000</annotation></semantics></math> test images. We partitioned the training data based on the non-IID 90-10 skew data split for eight parties. Each party has approximately <math id="S7.SS3.p1.4.m4.2" class="ltx_Math" alttext="40,000" display="inline"><semantics id="S7.SS3.p1.4.m4.2a"><mrow id="S7.SS3.p1.4.m4.2.3.2" xref="S7.SS3.p1.4.m4.2.3.1.cmml"><mn id="S7.SS3.p1.4.m4.1.1" xref="S7.SS3.p1.4.m4.1.1.cmml">40</mn><mo id="S7.SS3.p1.4.m4.2.3.2.1" xref="S7.SS3.p1.4.m4.2.3.1.cmml">,</mo><mn id="S7.SS3.p1.4.m4.2.2" xref="S7.SS3.p1.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.4.m4.2b"><list id="S7.SS3.p1.4.m4.2.3.1.cmml" xref="S7.SS3.p1.4.m4.2.3.2"><cn type="integer" id="S7.SS3.p1.4.m4.1.1.cmml" xref="S7.SS3.p1.4.m4.1.1">40</cn><cn type="integer" id="S7.SS3.p1.4.m4.2.2.cmml" xref="S7.SS3.p1.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.4.m4.2c">40,000</annotation></semantics></math> training examples with skew distribution among different classes, i.e., the two dominant classes contain <math id="S7.SS3.p1.5.m5.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S7.SS3.p1.5.m5.1a"><mrow id="S7.SS3.p1.5.m5.1.1" xref="S7.SS3.p1.5.m5.1.1.cmml"><mn id="S7.SS3.p1.5.m5.1.1.2" xref="S7.SS3.p1.5.m5.1.1.2.cmml">90</mn><mo id="S7.SS3.p1.5.m5.1.1.1" xref="S7.SS3.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.5.m5.1b"><apply id="S7.SS3.p1.5.m5.1.1.cmml" xref="S7.SS3.p1.5.m5.1.1"><csymbol cd="latexml" id="S7.SS3.p1.5.m5.1.1.1.cmml" xref="S7.SS3.p1.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S7.SS3.p1.5.m5.1.1.2.cmml" xref="S7.SS3.p1.5.m5.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.5.m5.1c">90\%</annotation></semantics></math> of training data, while the remaining <math id="S7.SS3.p1.6.m6.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S7.SS3.p1.6.m6.1a"><mn id="S7.SS3.p1.6.m6.1.1" xref="S7.SS3.p1.6.m6.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.6.m6.1b"><cn type="integer" id="S7.SS3.p1.6.m6.1.1.cmml" xref="S7.SS3.p1.6.m6.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.6.m6.1c">14</annotation></semantics></math> classes have <math id="S7.SS3.p1.7.m7.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S7.SS3.p1.7.m7.1a"><mrow id="S7.SS3.p1.7.m7.1.1" xref="S7.SS3.p1.7.m7.1.1.cmml"><mn id="S7.SS3.p1.7.m7.1.1.2" xref="S7.SS3.p1.7.m7.1.1.2.cmml">10</mn><mo id="S7.SS3.p1.7.m7.1.1.1" xref="S7.SS3.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.7.m7.1b"><apply id="S7.SS3.p1.7.m7.1.1.cmml" xref="S7.SS3.p1.7.m7.1.1"><csymbol cd="latexml" id="S7.SS3.p1.7.m7.1.1.1.cmml" xref="S7.SS3.p1.7.m7.1.1.1">percent</csymbol><cn type="integer" id="S7.SS3.p1.7.m7.1.1.2.cmml" xref="S7.SS3.p1.7.m7.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.7.m7.1c">10\%</annotation></semantics></math>. We trained deep learning models with <math id="S7.SS3.p1.8.m8.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S7.SS3.p1.8.m8.1a"><mn id="S7.SS3.p1.8.m8.1.1" xref="S7.SS3.p1.8.m8.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.8.m8.1b"><cn type="integer" id="S7.SS3.p1.8.m8.1.1.cmml" xref="S7.SS3.p1.8.m8.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.8.m8.1c">30</annotation></semantics></math> training rounds. Each round has one epoch. As <em id="S7.SS3.p1.8.7" class="ltx_emph ltx_font_italic">RVL-CDIP</em> dataset it not officially supported in <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr>, we simulated the <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> implementation for performance comparison.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para ltx_noindent">
<p id="S7.SS3.p2.2" class="ltx_p"><span id="S7.SS3.p2.2.1" class="ltx_text ltx_font_bold">Accuracy/Loss and Convergence Rate.</span>
We present the model accuracy and loss at each training round in Figure <a href="#S7.F6.sf1" title="In Figure 6 ‣ 7.2. Training with Different Numbers of Parties ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6a</span></a>. Similarly, the models converge at a similar rate with <span id="S7.SS3.p2.2.2" class="ltx_text ltx_font_smallcaps">Truda</span> and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr>. The accuracy results of the final model trained with <span id="S7.SS3.p2.2.3" class="ltx_text ltx_font_smallcaps">Truda</span> is <math id="S7.SS3.p2.1.m1.1" class="ltx_Math" alttext="83.50\%" display="inline"><semantics id="S7.SS3.p2.1.m1.1a"><mrow id="S7.SS3.p2.1.m1.1.1" xref="S7.SS3.p2.1.m1.1.1.cmml"><mn id="S7.SS3.p2.1.m1.1.1.2" xref="S7.SS3.p2.1.m1.1.1.2.cmml">83.50</mn><mo id="S7.SS3.p2.1.m1.1.1.1" xref="S7.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p2.1.m1.1b"><apply id="S7.SS3.p2.1.m1.1.1.cmml" xref="S7.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S7.SS3.p2.1.m1.1.1.1.cmml" xref="S7.SS3.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S7.SS3.p2.1.m1.1.1.2.cmml" xref="S7.SS3.p2.1.m1.1.1.2">83.50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p2.1.m1.1c">83.50\%</annotation></semantics></math> and <math id="S7.SS3.p2.2.m2.1" class="ltx_Math" alttext="86.19\%" display="inline"><semantics id="S7.SS3.p2.2.m2.1a"><mrow id="S7.SS3.p2.2.m2.1.1" xref="S7.SS3.p2.2.m2.1.1.cmml"><mn id="S7.SS3.p2.2.m2.1.1.2" xref="S7.SS3.p2.2.m2.1.1.2.cmml">86.19</mn><mo id="S7.SS3.p2.2.m2.1.1.1" xref="S7.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p2.2.m2.1b"><apply id="S7.SS3.p2.2.m2.1.1.cmml" xref="S7.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S7.SS3.p2.2.m2.1.1.1.cmml" xref="S7.SS3.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S7.SS3.p2.2.m2.1.1.2.cmml" xref="S7.SS3.p2.2.m2.1.1.2">86.19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p2.2.m2.1c">86.19\%</annotation></semantics></math> with simulated <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr>.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para ltx_noindent">
<p id="S7.SS3.p3.4" class="ltx_p"><span id="S7.SS3.p3.4.1" class="ltx_text ltx_font_bold">Training Latency.</span>
We present the training latency data of <span id="S7.SS3.p3.4.2" class="ltx_text ltx_font_smallcaps">Truda</span> and simulated <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> in Figure <a href="#S7.F6.sf2" title="In Figure 6 ‣ 7.2. Training with Different Numbers of Parties ‣ 7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6b</span></a>. It took <span id="S7.SS3.p3.4.3" class="ltx_text ltx_font_smallcaps">Truda</span> <math id="S7.SS3.p3.1.m1.1" class="ltx_Math" alttext="2.85" display="inline"><semantics id="S7.SS3.p3.1.m1.1a"><mn id="S7.SS3.p3.1.m1.1.1" xref="S7.SS3.p3.1.m1.1.1.cmml">2.85</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.p3.1.m1.1b"><cn type="float" id="S7.SS3.p3.1.m1.1.1.cmml" xref="S7.SS3.p3.1.m1.1.1">2.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p3.1.m1.1c">2.85</annotation></semantics></math> hours and <abbr title="Framework for Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFL</span></abbr> <math id="S7.SS3.p3.2.m2.1" class="ltx_Math" alttext="2.46" display="inline"><semantics id="S7.SS3.p3.2.m2.1a"><mn id="S7.SS3.p3.2.m2.1.1" xref="S7.SS3.p3.2.m2.1.1.cmml">2.46</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.p3.2.m2.1b"><cn type="float" id="S7.SS3.p3.2.m2.1.1.cmml" xref="S7.SS3.p3.2.m2.1.1">2.46</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p3.2.m2.1c">2.46</annotation></semantics></math> hours to finish <math id="S7.SS3.p3.3.m3.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S7.SS3.p3.3.m3.1a"><mn id="S7.SS3.p3.3.m3.1.1" xref="S7.SS3.p3.3.m3.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.p3.3.m3.1b"><cn type="integer" id="S7.SS3.p3.3.m3.1.1.cmml" xref="S7.SS3.p3.3.m3.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p3.3.m3.1c">30</annotation></semantics></math> rounds of training. Our added security features in <span id="S7.SS3.p3.4.4" class="ltx_text ltx_font_smallcaps">Truda</span> incurred additional <math id="S7.SS3.p3.4.m4.1" class="ltx_math_unparsed" alttext="0.16\times" display="inline"><semantics id="S7.SS3.p3.4.m4.1a"><mrow id="S7.SS3.p3.4.m4.1b"><mn id="S7.SS3.p3.4.m4.1.1">0.16</mn><mo lspace="0.222em" id="S7.SS3.p3.4.m4.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S7.SS3.p3.4.m4.1c">0.16\times</annotation></semantics></math> in latency for training the <em id="S7.SS3.p3.4.5" class="ltx_emph ltx_font_italic">RVL-CDIP</em> model.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Related Work</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this section, we give an overview of the security defenses against the <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> privacy leakage attacks and analyze their pros and cons from the perspectives of security and utility trade-off. In addition, we compare <span id="S8.p1.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> with them to demonstrate the contributions of our work.</p>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1. </span><span title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">Trusted Execution Environments</span></span>
</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p"><abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> allow users to out-source their computation to third-party cloud servers with trust on the CPU package.
They are particularly attractive for collaborative <abbr title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr> computation, which may involve a large amount of privacy-sensitive training data, multiple distrusting parties, and stricter data protection regulations.
<abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> can act as trustworthy intermediaries for isolating and orchestrating <abbr title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr> processes and replace the expensive cryptographic primitives.
For example,  <span title="Software Guard Extensions" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Software Guard Extensions</span></span> (<abbr title="Software Guard Extensions" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SGX</span></abbr>) has been leveraged to support secure model inference <cite class="ltx_cite ltx_citemacro_citep">(Tramer and Boneh, <a href="#bib.bib53" title="" class="ltx_ref">2018</a>; Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite>, privacy-preserving multi-party machine learning <cite class="ltx_cite ltx_citemacro_citep">(Ohrimenko et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2016</a>; Hunt et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2018</a>; Hynes
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2019</a>; Mo et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>)</cite>, and analytics on sensitive data <cite class="ltx_cite ltx_citemacro_citep">(Schuster et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>; Bittau
et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2017</a>; Zheng et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2017</a>; Dave
et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S8.SS1.p2" class="ltx_para">
<p id="S8.SS1.p2.1" class="ltx_p">However, <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> are not panacea to address all trust problems. Existing <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEE</span></abbr> technologies have different performance and capacity constraints. For example, <abbr title="Software Guard Extensions" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SGX</span></abbr> has a limit (128 MB) on the enclave’s physical memory size. 1st-generation <abbr title="Secure Encrypted Virtualization" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SEV</span></abbr> does not provide integrity protection. External <abbr title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr> accelerators cannot be exploited for trusted execution with runtime memory encryption. In addition, <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> may still be susceptible to emerging security vulnerabilities <cite class="ltx_cite ltx_citemacro_citep">(van Schaik et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2020b</a>; van
Schaik et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2020a</a>; Van Bulck et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020</a>; Murdock et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2020</a>; Van Bulck
et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2018</a>; Werner et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2019</a>; Buhren
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2019</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Wilke et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite>. One small defect may break the foundation of the entire trustworthy system. Thus, to assess a <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEE</span></abbr>-integrated system, we need to distinguish security properties of different <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> and extend the threat model in case of <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEE</span></abbr> failure.</p>
</div>
<div id="S8.SS1.p3" class="ltx_para">
<p id="S8.SS1.p3.1" class="ltx_p">Sharing a similar goal with all the research works mentioned above, we leverage <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr> to support trustworthy aggregation as the first-line defense against privacy leakages. The unique contributions of our work are to decentralize the aggregation to multiple trusted execution entities and employ dynamic permutation to further obfuscate the model updates. Thus, we are still resilient to data reconstruction attacks even if model updates are leaked from breached <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">TEEs</span></abbr>.</p>
</div>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2. </span>Cryptographic Schemes</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p"><span title="Homomorphic Encryption" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Homomorphic Encryption</span></span> allows arithmetic operations on ciphertexts without decryption.
Aono et al. <cite class="ltx_cite ltx_citemacro_citep">(Aono et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite> used additively <abbr title="Homomorphic Encryption" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HE</span></abbr> to protect the privacy of gradients to prevent information leakage.
Hardy et al. <cite class="ltx_cite ltx_citemacro_citep">(Hardy et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite> encrypted vertically partitioned data with an additively <abbr title="Homomorphic Encryption" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HE</span></abbr> and learned a linear classifier in the <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> setting.
<span title="Secure Multi-Party Computation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Secure Multi-Party Computation</span></span> allows different parties to compute a joint function without revealing their inputs to each other and has been widely researched in collaborative analytics and multi-party learning <cite class="ltx_cite ltx_citemacro_citep">(Mohassel and
Zhang, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>; Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>; Zheng
et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2019</a>; Zheng et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2021</a>; Poddar et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2021</a>; Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>; Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para">
<p id="S8.SS2.p2.1" class="ltx_p">Compared to <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEE</span></abbr>-based approaches like <span id="S8.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">Truda</span>, using cryptographic schemes for privacy protection promise same-level privacy/accuracy and can exclude CPU vendors from the trust domain. But the trade-offs are the extra communication and computational overhead. The performance constraints for now may still hinder their applications for large-scale <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training scenarios.</p>
</div>
</section>
<section id="S8.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3. </span><span title="Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Differential Privacy</span></span>
</h3>

<div id="S8.SS3.p1" class="ltx_para">
<p id="S8.SS3.p1.1" class="ltx_p">In the <abbr title="machine learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ML</span></abbr> setting, <abbr title="Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DP</span></abbr> can be used to apply perturbations for mitigating information leakage. Compared to cryptographic schemes, <abbr title="Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DP</span></abbr> is more computationally efficient at the cost of a certain utility loss due to the added noise.
 <span title="Centralized Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Centralized Differential Privacy</span></span> (<abbr title="Centralized Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CDP</span></abbr>) data analysis is typically conducted under the assumption of a trusted central server. <abbr title="Centralized Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CDP</span></abbr> <cite class="ltx_cite ltx_citemacro_citep">(McMahan
et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2017b</a>; Geyer
et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> can achieve an acceptable balance between privacy and accuracy, but it does not fit a threat model where the aggregation server might be honest-but-curious, malicious, or compromised.
In an effort to remove the trusted central server assumption in the threat model,  <span title="Local Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Local Differential Privacy</span></span> (<abbr title="Local Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LDP</span></abbr>) <cite class="ltx_cite ltx_citemacro_citep">(Shokri and
Shmatikov, <a href="#bib.bib51" title="" class="ltx_ref">2015</a>; Bhowmick et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> lets each client conduct differentially private transformations of their private data before sharing them with the aggregator. However, achieving <abbr title="Local Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LDP</span></abbr> comes at the cost of utility loss as every participant must add enough noise to ensure <abbr title="Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DP</span></abbr> in isolation.</p>
</div>
<div id="S8.SS3.p2" class="ltx_para">
<p id="S8.SS3.p2.1" class="ltx_p">Due to conflicting threat models, the design of decentralized and shuffled aggregation in <span id="S8.SS3.p2.1.1" class="ltx_text ltx_font_smallcaps">Truda</span> is incompatible with <abbr title="Centralized Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CDP</span></abbr>, which requires a central aggregator in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> training. However, we can still enable trustworthy aggregation to strengthen <abbr title="Centralized Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CDP</span></abbr> with a <abbr title="Trusted Execution Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">TEE</span></abbr>-protected aggregator. <span id="S8.SS3.p2.1.2" class="ltx_text ltx_font_smallcaps">Truda</span> can be seamlessly integrated with <abbr title="Local Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LDP</span></abbr> as the <abbr title="Local Differential Privacy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LDP</span></abbr>’s perturbations only apply to model updates on the party machines.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusion</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">In light of the recent training data reconstruction attacks targeting <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> aggregation, we rethink the trust model and system architecture of <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> frameworks and identify the root causes of their susceptibility to such attacks. To address the problem, we exploit the unique computational properties of aggregation algorithms and propose protocol/architectural enhancements to minimize the leakage surface and break the information concentration. We have developed <span id="S9.p1.1.1" class="ltx_text ltx_font_smallcaps">Truda</span>, a new cross-silo <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr> system encompassing three security-reinforced mechanisms, i.e., trustworthy, decentralized, and shuffled aggregation. Therefore, each aggregator only has a fragmentary and obfuscated view of the model updates. We demonstrate that <span id="S9.p1.1.2" class="ltx_text ltx_font_smallcaps">Truda</span> can effectively mitigate all training data reconstruction attacks with no utility loss and low performance overheads.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aono et al<span id="bib.bib2.3.3.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yoshinori Aono, Takuya
Hayashi, Lihua Wang, Shiho Moriai,
et al<span id="bib.bib2.4.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning via additively
homomorphic encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.5.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics
and Security</em> 13, 5
(2017), 1333–1345.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bellet
et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Aurélien Bellet,
Rachid Guerraoui, Mahsa Taziki, and
Marc Tommasi. 2018.

</span>
<span class="ltx_bibblock">Personalized and private peer-to-peer machine
learning. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">International Conference on
Artificial Intelligence and Statistics</em>. PMLR,
473–481.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhowmick et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Abhishek Bhowmick, John
Duchi, Julien Freudiger, Gaurav Kapoor,
and Ryan Rogers. 2018.

</span>
<span class="ltx_bibblock">Protection against reconstruction and its
applications in private federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.00984</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bittau
et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Andrea Bittau, Úlfar
Erlingsson, Petros Maniatis, Ilya
Mironov, Ananth Raghunathan, David Lie,
Mitch Rudominer, Ushasree Kode,
Julien Tinnes, and Bernhard Seefeld.
2017.

</span>
<span class="ltx_bibblock">Prochlo: Strong privacy for analytics in the
crowd. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th Symposium on
Operating Systems Principles</em>. ACM,
441–459.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Peva Blanchard, El Mahdi
El Mhamdi, Rachid Guerraoui, and Julien
Stainer. 2017.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine
tolerant gradient descent. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
31st International Conference on Neural Information Processing Systems</em>.
118–128.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib7.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert
Eichner, Wolfgang Grieskamp, Dzmitry
Huba, Alex Ingerman, Vladimir Ivanov,
Chloe Kiddon, Jakub Konečnỳ,
Stefano Mazzocchi, H Brendan McMahan,
et al<span id="bib.bib7.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System
design.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.01046</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir
Ivanov, Ben Kreuter, Antonio Marcedone,
H Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and
Karn Seth. 2017.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving
machine learning. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em>.
ACM, 1175–1191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buhren
et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Robert Buhren, Christian
Werling, and Jean-Pierre Seifert.
2019.

</span>
<span class="ltx_bibblock">Insecure Until Proven Updated: Analyzing AMD SEV’s
Remote Attestation. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 ACM
SIGSAC Conference on Computer and Communications Security</em>.
ACM, 1087–1099.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dave
et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ankur Dave, Chester
Leung, Raluca Ada Popa, Joseph E
Gonzalez, and Ion Stoica.
2020.

</span>
<span class="ltx_bibblock">Oblivious coopetitive analytics using hardware
enclaves. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifteenth European
Conference on Computer Systems</em>. ACM,
1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DLG git
repository (2020)</span>
<span class="ltx_bibblock">
DLG git repository 2020.

</span>
<span class="ltx_bibblock">Deep Leakage From Gradients.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/mit-han-lab/dlg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mit-han-lab/dlg</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman-Fitzthum (2020)</span>
<span class="ltx_bibblock">
Tobin Feldman-Fitzthum.
2020.

</span>
<span class="ltx_bibblock">sev: add sev-inject-launch-secret.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://git.qemu.org/?p=qemu.git;a=commit;h=c7f7e6970d3b74c1454cafea4918187e06c473eb" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://git.qemu.org/?p=qemu.git;a=commit;h=c7f7e6970d3b74c1454cafea4918187e06c473eb</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut
Bauermeister, Hannah Dröge, and
Michael Moeller. 2020.

</span>
<span class="ltx_bibblock">Inverting Gradients–How easy is it to break
privacy in federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.14053</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer
et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Robin C Geyer, Tassilo
Klein, and Moin Nabi. 2017.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client
level perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07557</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">gRPC (2021)</span>
<span class="ltx_bibblock">
gRPC 2021.

</span>
<span class="ltx_bibblock">A high performance, open source universal RPC
framework.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://grpc.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://grpc.io/</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Zhongshu Gu, Heqing
Huang, Jialong Zhang, Dong Su,
Hani Jamjoom, Ankita Lamba,
Dimitrios Pendarakis, and Ian Molloy.
2018.

</span>
<span class="ltx_bibblock">Confidential Inference via Ternary Model
Partitioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.00969</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhongshu Gu, Hani
Jamjoom, Dong Su, Heqing Huang,
Jialong Zhang, Tengfei Ma,
Dimitrios Pendarakis, and Ian Molloy.
2019.

</span>
<span class="ltx_bibblock">Reaching data confidentiality and model
accountability on the caltrain. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">49th Annual
IEEE/IFIP International Conference on Dependable Systems and Networks</em>.
IEEE, 336–348.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardy et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Stephen Hardy, Wilko
Henecka, Hamish Ivey-Law, Richard Nock,
Giorgio Patrini, Guillaume Smith, and
Brian Thorne. 2017.

</span>
<span class="ltx_bibblock">Private federated learning on vertically
partitioned data via entity resolution and additively homomorphic
encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.10677</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harley
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Adam W Harley, Alex
Ufkes, and Konstantinos G Derpanis.
2015.

</span>
<span class="ltx_bibblock">Evaluation of Deep Convolutional Nets for Document
Image Classification and Retrieval. In
<em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and
Recognition</em>. IEEE, 991–995.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hunt
et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Guerney D. H. Hunt,
Ramachandra Pai, Michael Le,
Hani Jamjoom, Sukadev Bhattiprolu,
Rick Boivie, Laurent Dufour,
Brad Frey, Mohit Kapur,
Kenneth A. Goldman, Ryan Grimm,
Janani Janakirman, John M. Ludden,
Paul Mackerras, Cathy May,
Elaine R. Palmer, Bharata Bhasker Rao,
Lance Roy, William A. Starke,
Jeff Stuecheli, Ray Valdez, and
Wendel Voigt. 2021.

</span>
<span class="ltx_bibblock">Confidential Computing for OpenPOWER. In
<em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Sixteenth European Conference on
Computer Systems</em>. ACM, 294–310.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hunt et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tyler Hunt, Congzheng
Song, Reza Shokri, Vitaly Shmatikov,
and Emmett Witchel. 2018.

</span>
<span class="ltx_bibblock">Chiron: Privacy-preserving Machine Learning as a
Service.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05961</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hynes
et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Nick Hynes, Raymond
Cheng, and Dawn Song. 2018.

</span>
<span class="ltx_bibblock">Efficient Deep Learning on Multi-Source Private
Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.06689</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">iDLG git
repository (2020)</span>
<span class="ltx_bibblock">
iDLG git repository 2020.

</span>
<span class="ltx_bibblock">Improved Deep Leakage from Gradients.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">IG git repository (2020)</span>
<span class="ltx_bibblock">
IG git repository 2020.

</span>
<span class="ltx_bibblock">Inverting Gradients - How easy is it to break
Privacy in Federated Learning?

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/JonasGeiping/invertinggradients" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/JonasGeiping/invertinggradients</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Intel (2020)</span>
<span class="ltx_bibblock">
Intel. 2020.

</span>
<span class="ltx_bibblock">Intel Trust Domain Extensions.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://software.intel.com/content/dam/develop/external/us/en/documents/tdx-whitepaper-final9-17.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://software.intel.com/content/dam/develop/external/us/en/documents/tdx-whitepaper-final9-17.pdf</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">White paper</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jayaram et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
K. R. Jayaram, Archit
Verma, Ashish Verma, Gegi Thomas, and
Colin Sutcher-Shepard. 2020.

</span>
<span class="ltx_bibblock">MYSTIKO: Cloud-Mediated, Private, Federated
Gradient Descent. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">2020 IEEE 13th International
Conference on Cloud Computing (CLOUD)</em>. IEEE,
201–210.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz
et al<span id="bib.bib27.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary
Charles, Graham Cormode, Rachel
Cummings, et al<span id="bib.bib27.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.04977</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan (2017)</span>
<span class="ltx_bibblock">
David Kaplan.
2017.

</span>
<span class="ltx_bibblock">Protecting vm register state with sev-es.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">White paper</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan
et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
David Kaplan, Jeremy
Powell, and Tom Woller.
2016.

</span>
<span class="ltx_bibblock">AMD memory encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">White paper</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kata Containers (2021)</span>
<span class="ltx_bibblock">
Kata Containers 2021.

</span>
<span class="ltx_bibblock">The speed of containers, the security of VMs.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://katacontainers.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://katacontainers.io</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koloskova
et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Anastasia Koloskova,
Sebastian Stich, and Martin Jaggi.
2019.

</span>
<span class="ltx_bibblock">Decentralized stochastic optimization and gossip
algorithms with compressed communication. In
<em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.
PMLR, 3478–3487.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lalitha et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Anusha Lalitha,
Osman Cihan Kilinc, Tara Javidi, and
Farinaz Koushanfar. 2019.

</span>
<span class="ltx_bibblock">Peer-to-peer federated learning on graphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.11173</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mengyuan Li, Yinqian
Zhang, and Zhiqiang Lin.
2020.

</span>
<span class="ltx_bibblock">CROSSLINE: Breaking ”Security-by-Crash” based
Memory Isolation in AMD SEV.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.00146</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Mengyuan Li, Yinqian
Zhang, Zhiqiang Lin, and Yan Solihin.
2019.

</span>
<span class="ltx_bibblock">Exploiting unprotected I/O operations in AMD’s
Secure Encrypted Virtualization. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">28th USENIX
Security Symposium</em>. USENIX,
1257–1272.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Changchang Liu, Supriyo
Chakraborty, and Dinesh Verma.
2019.

</span>
<span class="ltx_bibblock">Secure model fusion for distributed learning using
partial homomorphic encryption.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Policy-Based Autonomic Data
Governance</em>. Springer, 154–179.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ludwig et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Heiko Ludwig, Nathalie
Baracaldo, Gegi Thomas, Yi Zhou,
Ali Anwar, Shashank Rajamoni,
Yuya Ong, Jayaram Radhakrishnan,
Ashish Verma, Mathieu Sinn,
Mark Purcell, Ambrish Rawat,
Tran Minh, Naoise Holohan,
Supriyo Chakraborty, Shalisha
Whitherspoon, Dean Steuer, Laura Wynter,
Hifaz Hassan, Sean Laguna,
Mikhail Yurochkin, Mayank Agarwal,
Ebube Chuba, and Annie Abay.
2020.

</span>
<span class="ltx_bibblock">IBM Federated Learning: an Enterprise Framework
White Paper V0.1.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.10987</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McKeen et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Frank McKeen, Ilya
Alexandrovich, Alex Berenzon, Carlos V
Rozas, Hisham Shafi, Vedvyas Shanbhogue,
and Uday R Savagaonkar. 2013.

</span>
<span class="ltx_bibblock">Innovative instructions and software model for
isolated execution.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">The Second Workshop on Hardware and
Architectural Support for Security and Privacy</em> 10,
1 (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2017a)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas.
2017a.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence and Statistics</em>. PMLR,
1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan
et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2017b)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel
Ramage, Kunal Talwar, and Li Zhang.
2017b.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06963</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melis et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng
Song, Emiliano De Cristofaro, and
Vitaly Shmatikov. 2019.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in
collaborative learning. In <em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">2019 IEEE Symposium on
Security and Privacy</em>. IEEE, 691–706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Fan Mo, Hamed Haddadi,
Kleomenis Katevas, Eduard Marin,
Diego Perino, and Nicolas Kourtellis.
2021.

</span>
<span class="ltx_bibblock">PPFL: Privacy-preserving Federated Learning with
Trusted Execution Environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.14380</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohassel and
Zhang (2017)</span>
<span class="ltx_bibblock">
Payman Mohassel and
Yupeng Zhang. 2017.

</span>
<span class="ltx_bibblock">Secureml: A system for scalable privacy-preserving
machine learning. In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on
Security and Privacy</em>. IEEE, 19–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murdock et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Kit Murdock, David
Oswald, Flavio D Garcia, Jo Van Bulck,
Daniel Gruss, and Frank Piessens.
2020.

</span>
<span class="ltx_bibblock">Plundervolt: Software-based fault injection attacks
against Intel SGX. In <em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">2020 IEEE Symposium on
Security and Privacy</em>. IEEE,
1466–1482.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ohrimenko et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Olga Ohrimenko, Felix
Schuster, Cédric Fournet, Aastha
Mehta, Sebastian Nowozin, Kapil Vaswani,
and Manuel Costa. 2016.

</span>
<span class="ltx_bibblock">Oblivious Multi-Party Machine Learning on Trusted
Processors.. In <em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">25th USENIX Security Symposium</em>.
USENIX, 619–636.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paillier (1999)</span>
<span class="ltx_bibblock">
Pascal Paillier.
1999.

</span>
<span class="ltx_bibblock">Public-key cryptosystems based on composite degree
residuosity classes. In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">International conference
on the theory and applications of cryptographic techniques</em>.
Springer, 223–238.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poddar et al<span id="bib.bib46.4.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Rishabh Poddar, Sukrit
Kalra, Avishay Yanai, Ryan Deng,
Raluca Ada Popa, and Joseph M
Hellerstein. 2021.

</span>
<span class="ltx_bibblock">Senate: A Maliciously-Secure <math id="bib.bib46.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib46.1.m1.1a"><mo stretchy="false" id="bib.bib46.1.m1.1.1" xref="bib.bib46.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib46.1.m1.1b"><ci id="bib.bib46.1.m1.1.1.cmml" xref="bib.bib46.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib46.1.m1.1c">\{</annotation></semantics></math>MPC<math id="bib.bib46.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib46.2.m2.1a"><mo stretchy="false" id="bib.bib46.2.m2.1.1" xref="bib.bib46.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib46.2.m2.1b"><ci id="bib.bib46.2.m2.1.1.cmml" xref="bib.bib46.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib46.2.m2.1c">\}</annotation></semantics></math> Platform
for Collaborative Analytics. In <em id="bib.bib46.5.1" class="ltx_emph ltx_font_italic">30th USENIX
Security Symposium</em>. USENIX.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Felix Schuster, Manuel
Costa, Cédric Fournet, Christos
Gkantsidis, Marcus Peinado, Gloria
Mainar-Ruiz, and Mark Russinovich.
2015.

</span>
<span class="ltx_bibblock">VC3: Trustworthy data analytics in the cloud using
SGX. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">2015 IEEE Symposium on Security and
Privacy</em>. IEEE, 38–54.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SEV API (2019)</span>
<span class="ltx_bibblock">
SEV API 2019.

</span>
<span class="ltx_bibblock">Secure Encrypted Virtualization API Version 0.22.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://developer.amd.com/wp-content/resources/55766.PDF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.amd.com/wp-content/resources/55766.PDF</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SEV-SNP (2020)</span>
<span class="ltx_bibblock">
AMD SEV-SNP.
2020.

</span>
<span class="ltx_bibblock">Strengthening VM isolation with integrity
protection and more.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">White Paper</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SEV-Tool (2019)</span>
<span class="ltx_bibblock">
SEV-Tool 2019.

</span>
<span class="ltx_bibblock">SEV-Tool.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/AMDESE/sev-tool" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/AMDESE/sev-tool</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri and
Shmatikov (2015)</span>
<span class="ltx_bibblock">
Reza Shokri and Vitaly
Shmatikov. 2015.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning. In
<em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd ACM SIGSAC conference on
computer and communications security</em>. ACM,
1310–1321.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Christian Szegedy,
Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus.
2013.

</span>
<span class="ltx_bibblock">Intriguing properties of neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1312.6199</em>
(2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tramer and Boneh (2018)</span>
<span class="ltx_bibblock">
Florian Tramer and Dan
Boneh. 2018.

</span>
<span class="ltx_bibblock">Slalom: Fast, Verifiable and Private Execution of
Neural Networks in Trusted Hardware.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.03287</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truex et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Stacey Truex, Nathalie
Baracaldo, Ali Anwar, Thomas Steinke,
Heiko Ludwig, Rui Zhang, and
Yi Zhou. 2019.

</span>
<span class="ltx_bibblock">A hybrid approach to privacy-preserving federated
learning. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM Workshop
on Artificial Intelligence and Security</em>. ACM,
1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Bulck
et al<span id="bib.bib55.4.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jo Van Bulck, Marina
Minkin, Ofir Weisse, Daniel Genkin,
Baris Kasikci, Frank Piessens,
Mark Silberstein, Thomas F Wenisch,
Yuval Yarom, and Raoul Strackx.
2018.

</span>
<span class="ltx_bibblock">Foreshadow: Extracting the keys to the intel
<math id="bib.bib55.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib55.1.m1.1a"><mo stretchy="false" id="bib.bib55.1.m1.1.1" xref="bib.bib55.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib55.1.m1.1b"><ci id="bib.bib55.1.m1.1.1.cmml" xref="bib.bib55.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib55.1.m1.1c">\{</annotation></semantics></math>SGX<math id="bib.bib55.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib55.2.m2.1a"><mo stretchy="false" id="bib.bib55.2.m2.1.1" xref="bib.bib55.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib55.2.m2.1b"><ci id="bib.bib55.2.m2.1.1.cmml" xref="bib.bib55.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib55.2.m2.1c">\}</annotation></semantics></math> kingdom with transient out-of-order execution. In
<em id="bib.bib55.5.1" class="ltx_emph ltx_font_italic">27th USENIX Security Symposium</em>.
USENIX, 991–1008.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Bulck et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jo Van Bulck, Daniel
Moghimi, Michael Schwarz, Moritz Lipp,
Marina Minkin, Daniel Genkin,
Yarom Yuval, Berk Sunar,
Daniel Gruss, and Frank Piessens.
2020.

</span>
<span class="ltx_bibblock">LVI: Hijacking Transient Execution through
Microarchitectural Load Value Injection. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">2020
IEEE Symposium on Security and Privacy</em>. IEEE,
54–72.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van
Schaik et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Stephan van Schaik, Andrew
Kwong, Daniel Genkin, and Yuval
Yarom. 2020a.

</span>
<span class="ltx_bibblock">SGAxe: How SGX fails in practice.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Schaik et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Stephan van Schaik, Marina
Minkin, Andrew Kwong, Daniel Genkin,
and Yuval Yarom. 2020b.

</span>
<span class="ltx_bibblock">CacheOut: Leaking data on Intel CPUs via cache
evictions.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.13353</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanhaesebrouck et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Paul Vanhaesebrouck,
Aurélien Bellet, and Marc Tommasi.
2017.

</span>
<span class="ltx_bibblock">Decentralized collaborative learning of
personalized models over networks. In <em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence and Statistics</em>. PMLR,
509–517.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Werner et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jan Werner, Joshua Mason,
Manos Antonakakis, Michalis
Polychronakis, and Fabian Monrose.
2019.

</span>
<span class="ltx_bibblock">The SEVerESt Of Them All: Inference Attacks Against
Secure Virtual Enclaves. In <em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
2019 ACM Asia Conference on Computer and Communications Security</em>.
ACM, 73–85.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilke et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Luca Wilke, Jan
Wichelmann, Mathias Morbitzer, and
Thomas Eisenbarth. 2020.

</span>
<span class="ltx_bibblock">SEVurity: No Security Without Integrity–Breaking
Integrity-Free Memory Encryption with Minimal Assumptions.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.11071</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin
et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Dong Yin, Yudong Chen,
Ramchandran Kannan, and Peter
Bartlett. 2018.

</span>
<span class="ltx_bibblock">Byzantine-robust distributed learning: Towards
optimal statistical rates. In <em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">International
Conference on Machine Learning</em>. PMLR,
5650–5659.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hongxu Yin, Arun Mallya,
Arash Vahdat, Jose M Alvarez,
Jan Kautz, and Pavlo Molchanov.
2021.

</span>
<span class="ltx_bibblock">See through Gradients: Image Batch Recovery via
GradInversion.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.07586</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Bo Zhao, Konda Reddy
Mopuri, and Hakan Bilen.
2020.

</span>
<span class="ltx_bibblock">iDLG: Improved Deep Leakage from Gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.02610</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Wenting Zheng, Ankur
Dave, Jethro G Beekman, Raluca Ada Popa,
Joseph E Gonzalez, and Ion Stoica.
2017.

</span>
<span class="ltx_bibblock">Opaque: An oblivious and encrypted distributed
analytics platform. In <em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">14th USENIX Symposium on
Networked Systems Design and Implementation</em>. USENIX,
283–298.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Wenting Zheng, Ryan Deng,
Weikeng Chen, Raluca Ada Popa,
Aurojit Panda, and Ion Stoica.
2021.

</span>
<span class="ltx_bibblock">Cerebro: A Platform for Multi-Party Cryptographic
Collaborative Learning. In <em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">30th USENIX Security
Symposium</em>. USENIX.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng
et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Wenting Zheng, Raluca Ada
Popa, Joseph E Gonzalez, and Ion
Stoica. 2019.

</span>
<span class="ltx_bibblock">Helen: Maliciously secure coopetitive learning for
linear models. In <em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">2019 IEEE Symposium on Security
and Privacy</em>. IEEE, 724–738.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu,
and Song Han. 2019.

</span>
<span class="ltx_bibblock">Deep leakage from gradients. In
<em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>. 14774–14784.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A0.F7" class="ltx_figure"><img src="/html/2105.09400/assets/x14.png" id="A0.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="270" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Reconstruction Examples of DLG and iDLG with Model Partitioning and Permutation</figcaption>
</figure>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Tables for DNN Architectures in FL Training</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Here we present the detailed model architectures and hyper-parameters for the three <abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">DNNs</span></abbr> we trained in Section <a href="#S7" title="7. Performance Evaluation ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
The <em id="A1.p1.1.1" class="ltx_emph ltx_font_italic">Layer</em> column shows the layer types,
including convolutional layer (conv), dense layer (dense), flatten layer (flat), max pooling layer (maxpool), batch normalization layer (batchnorm), and dropout layer (dropout).
The <em id="A1.p1.1.2" class="ltx_emph ltx_font_italic">Filter</em> column shows the number of filters in each
convolutional layer. The <em id="A1.p1.1.3" class="ltx_emph ltx_font_italic">Size</em> column is in the format of
<em id="A1.p1.1.4" class="ltx_emph ltx_font_italic">width</em> <math id="A1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.p1.1.m1.1a"><mo id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><times id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\times</annotation></semantics></math> <em id="A1.p1.1.5" class="ltx_emph ltx_font_italic">height</em> to represent filter parameters. The <em id="A1.p1.1.6" class="ltx_emph ltx_font_italic">Activation</em> column shows the type of activation function used.</p>
</div>
<figure id="A1.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span><abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr> Architecture for <em id="A1.T3.2.1" class="ltx_emph ltx_font_italic">MNIST</em></figcaption>
<table id="A1.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T3.3.1.1" class="ltx_tr">
<th id="A1.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Layer</th>
<th id="A1.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Filter</th>
<th id="A1.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Size</th>
<th id="A1.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Activation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T3.3.2.1" class="ltx_tr">
<th id="A1.T3.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1 conv</th>
<td id="A1.T3.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">32</td>
<td id="A1.T3.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">3x3</td>
<td id="A1.T3.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">relu</td>
</tr>
<tr id="A1.T3.3.3.2" class="ltx_tr">
<th id="A1.T3.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2 conv</th>
<td id="A1.T3.3.3.2.2" class="ltx_td ltx_align_center">64</td>
<td id="A1.T3.3.3.2.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T3.3.3.2.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T3.3.4.3" class="ltx_tr">
<th id="A1.T3.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3 maxpool</th>
<td id="A1.T3.3.4.3.2" class="ltx_td"></td>
<td id="A1.T3.3.4.3.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T3.3.4.3.4" class="ltx_td"></td>
</tr>
<tr id="A1.T3.3.5.4" class="ltx_tr">
<th id="A1.T3.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">4 dropout</th>
<td id="A1.T3.3.5.4.2" class="ltx_td ltx_align_center" colspan="2">p = 0.25</td>
<td id="A1.T3.3.5.4.3" class="ltx_td"></td>
</tr>
<tr id="A1.T3.3.6.5" class="ltx_tr">
<th id="A1.T3.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">5 flatten</th>
<td id="A1.T3.3.6.5.2" class="ltx_td"></td>
<td id="A1.T3.3.6.5.3" class="ltx_td"></td>
<td id="A1.T3.3.6.5.4" class="ltx_td"></td>
</tr>
<tr id="A1.T3.3.7.6" class="ltx_tr">
<th id="A1.T3.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">6 dense</th>
<td id="A1.T3.3.7.6.2" class="ltx_td ltx_align_center">128</td>
<td id="A1.T3.3.7.6.3" class="ltx_td"></td>
<td id="A1.T3.3.7.6.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T3.3.8.7" class="ltx_tr">
<th id="A1.T3.3.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">7 dropout</th>
<td id="A1.T3.3.8.7.2" class="ltx_td ltx_align_center" colspan="2">p = 0.50</td>
<td id="A1.T3.3.8.7.3" class="ltx_td"></td>
</tr>
<tr id="A1.T3.3.9.8" class="ltx_tr">
<th id="A1.T3.3.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">8 dense</th>
<td id="A1.T3.3.9.8.2" class="ltx_td ltx_align_center ltx_border_b">10</td>
<td id="A1.T3.3.9.8.3" class="ltx_td ltx_border_b"></td>
<td id="A1.T3.3.9.8.4" class="ltx_td ltx_align_center ltx_border_b">softmax</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span><abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr> Architecture for <em id="A1.T4.2.1" class="ltx_emph ltx_font_italic">CIFAR-10</em></figcaption>
<table id="A1.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T4.3.1.1" class="ltx_tr">
<th id="A1.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Layer</th>
<th id="A1.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Filter</th>
<th id="A1.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Size</th>
<th id="A1.T4.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Activation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T4.3.2.1" class="ltx_tr">
<th id="A1.T4.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1 conv</th>
<td id="A1.T4.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">32</td>
<td id="A1.T4.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">3x3</td>
<td id="A1.T4.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">relu</td>
</tr>
<tr id="A1.T4.3.3.2" class="ltx_tr">
<th id="A1.T4.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2 batchnorm</th>
<td id="A1.T4.3.3.2.2" class="ltx_td"></td>
<td id="A1.T4.3.3.2.3" class="ltx_td"></td>
<td id="A1.T4.3.3.2.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.4.3" class="ltx_tr">
<th id="A1.T4.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3 conv</th>
<td id="A1.T4.3.4.3.2" class="ltx_td ltx_align_center">32</td>
<td id="A1.T4.3.4.3.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T4.3.4.3.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T4.3.5.4" class="ltx_tr">
<th id="A1.T4.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">4 batchnorm</th>
<td id="A1.T4.3.5.4.2" class="ltx_td"></td>
<td id="A1.T4.3.5.4.3" class="ltx_td"></td>
<td id="A1.T4.3.5.4.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.6.5" class="ltx_tr">
<th id="A1.T4.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">5 maxpool</th>
<td id="A1.T4.3.6.5.2" class="ltx_td"></td>
<td id="A1.T4.3.6.5.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T4.3.6.5.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.7.6" class="ltx_tr">
<th id="A1.T4.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">6 dropout</th>
<td id="A1.T4.3.7.6.2" class="ltx_td ltx_align_center" colspan="2">p = 0.20</td>
<td id="A1.T4.3.7.6.3" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.8.7" class="ltx_tr">
<th id="A1.T4.3.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">7 conv</th>
<td id="A1.T4.3.8.7.2" class="ltx_td ltx_align_center">64</td>
<td id="A1.T4.3.8.7.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T4.3.8.7.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T4.3.9.8" class="ltx_tr">
<th id="A1.T4.3.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">8 batchnorm</th>
<td id="A1.T4.3.9.8.2" class="ltx_td"></td>
<td id="A1.T4.3.9.8.3" class="ltx_td"></td>
<td id="A1.T4.3.9.8.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.10.9" class="ltx_tr">
<th id="A1.T4.3.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">9 conv</th>
<td id="A1.T4.3.10.9.2" class="ltx_td ltx_align_center">64</td>
<td id="A1.T4.3.10.9.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T4.3.10.9.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T4.3.11.10" class="ltx_tr">
<th id="A1.T4.3.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">10 batchnorm</th>
<td id="A1.T4.3.11.10.2" class="ltx_td"></td>
<td id="A1.T4.3.11.10.3" class="ltx_td"></td>
<td id="A1.T4.3.11.10.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.12.11" class="ltx_tr">
<th id="A1.T4.3.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">11 maxpool</th>
<td id="A1.T4.3.12.11.2" class="ltx_td"></td>
<td id="A1.T4.3.12.11.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T4.3.12.11.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.13.12" class="ltx_tr">
<th id="A1.T4.3.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">12 dropout</th>
<td id="A1.T4.3.13.12.2" class="ltx_td ltx_align_center" colspan="2">p = 0.30</td>
<td id="A1.T4.3.13.12.3" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.14.13" class="ltx_tr">
<th id="A1.T4.3.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">13 conv</th>
<td id="A1.T4.3.14.13.2" class="ltx_td ltx_align_center">128</td>
<td id="A1.T4.3.14.13.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T4.3.14.13.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T4.3.15.14" class="ltx_tr">
<th id="A1.T4.3.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">14 batchnorm</th>
<td id="A1.T4.3.15.14.2" class="ltx_td"></td>
<td id="A1.T4.3.15.14.3" class="ltx_td"></td>
<td id="A1.T4.3.15.14.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.16.15" class="ltx_tr">
<th id="A1.T4.3.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">15 conv</th>
<td id="A1.T4.3.16.15.2" class="ltx_td ltx_align_center">128</td>
<td id="A1.T4.3.16.15.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T4.3.16.15.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T4.3.17.16" class="ltx_tr">
<th id="A1.T4.3.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">16 batchnorm</th>
<td id="A1.T4.3.17.16.2" class="ltx_td"></td>
<td id="A1.T4.3.17.16.3" class="ltx_td"></td>
<td id="A1.T4.3.17.16.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.18.17" class="ltx_tr">
<th id="A1.T4.3.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">17 maxpool</th>
<td id="A1.T4.3.18.17.2" class="ltx_td"></td>
<td id="A1.T4.3.18.17.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T4.3.18.17.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.19.18" class="ltx_tr">
<th id="A1.T4.3.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">18 dropout</th>
<td id="A1.T4.3.19.18.2" class="ltx_td ltx_align_center" colspan="2">p = 0.40</td>
<td id="A1.T4.3.19.18.3" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.20.19" class="ltx_tr">
<th id="A1.T4.3.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">19 flatten</th>
<td id="A1.T4.3.20.19.2" class="ltx_td"></td>
<td id="A1.T4.3.20.19.3" class="ltx_td"></td>
<td id="A1.T4.3.20.19.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.21.20" class="ltx_tr">
<th id="A1.T4.3.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">20 dense</th>
<td id="A1.T4.3.21.20.2" class="ltx_td ltx_align_center">128</td>
<td id="A1.T4.3.21.20.3" class="ltx_td"></td>
<td id="A1.T4.3.21.20.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T4.3.22.21" class="ltx_tr">
<th id="A1.T4.3.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">21 batchnorm</th>
<td id="A1.T4.3.22.21.2" class="ltx_td"></td>
<td id="A1.T4.3.22.21.3" class="ltx_td"></td>
<td id="A1.T4.3.22.21.4" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.23.22" class="ltx_tr">
<th id="A1.T4.3.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">22 dropout</th>
<td id="A1.T4.3.23.22.2" class="ltx_td ltx_align_center" colspan="2">p = 0.50</td>
<td id="A1.T4.3.23.22.3" class="ltx_td"></td>
</tr>
<tr id="A1.T4.3.24.23" class="ltx_tr">
<th id="A1.T4.3.24.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">23 dense</th>
<td id="A1.T4.3.24.23.2" class="ltx_td ltx_align_center ltx_border_b">10</td>
<td id="A1.T4.3.24.23.3" class="ltx_td ltx_border_b"></td>
<td id="A1.T4.3.24.23.4" class="ltx_td ltx_align_center ltx_border_b">softmax</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A1.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span><abbr title="deep neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DNN</span></abbr> Architecture for <em id="A1.T5.2.1" class="ltx_emph ltx_font_italic">RVL-CDIP</em></figcaption>
<table id="A1.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.3.1.1" class="ltx_tr">
<th id="A1.T5.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Layer</th>
<th id="A1.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Filter</th>
<th id="A1.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Size</th>
<th id="A1.T5.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Activation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.3.2.1" class="ltx_tr">
<th id="A1.T5.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1 conv</th>
<th id="A1.T5.3.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">64</th>
<td id="A1.T5.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">3x3</td>
<td id="A1.T5.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">relu</td>
</tr>
<tr id="A1.T5.3.3.2" class="ltx_tr">
<th id="A1.T5.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2 conv</th>
<th id="A1.T5.3.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">64</th>
<td id="A1.T5.3.3.2.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.3.2.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.4.3" class="ltx_tr">
<th id="A1.T5.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3 maxpool</th>
<th id="A1.T5.3.4.3.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T5.3.4.3.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T5.3.4.3.4" class="ltx_td"></td>
</tr>
<tr id="A1.T5.3.5.4" class="ltx_tr">
<th id="A1.T5.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">4 conv</th>
<th id="A1.T5.3.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">128</th>
<td id="A1.T5.3.5.4.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.5.4.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.6.5" class="ltx_tr">
<th id="A1.T5.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">5 conv</th>
<th id="A1.T5.3.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">128</th>
<td id="A1.T5.3.6.5.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.6.5.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.7.6" class="ltx_tr">
<th id="A1.T5.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">6 maxpool</th>
<th id="A1.T5.3.7.6.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T5.3.7.6.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T5.3.7.6.4" class="ltx_td"></td>
</tr>
<tr id="A1.T5.3.8.7" class="ltx_tr">
<th id="A1.T5.3.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">7 conv</th>
<th id="A1.T5.3.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">256</th>
<td id="A1.T5.3.8.7.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.8.7.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.9.8" class="ltx_tr">
<th id="A1.T5.3.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">8 conv</th>
<th id="A1.T5.3.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">256</th>
<td id="A1.T5.3.9.8.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.9.8.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.10.9" class="ltx_tr">
<th id="A1.T5.3.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">9 conv</th>
<th id="A1.T5.3.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">256</th>
<td id="A1.T5.3.10.9.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.10.9.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.11.10" class="ltx_tr">
<th id="A1.T5.3.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">10 maxpool</th>
<th id="A1.T5.3.11.10.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T5.3.11.10.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T5.3.11.10.4" class="ltx_td"></td>
</tr>
<tr id="A1.T5.3.12.11" class="ltx_tr">
<th id="A1.T5.3.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">11 conv</th>
<th id="A1.T5.3.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">512</th>
<td id="A1.T5.3.12.11.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.12.11.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.13.12" class="ltx_tr">
<th id="A1.T5.3.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">12 conv</th>
<th id="A1.T5.3.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">512</th>
<td id="A1.T5.3.13.12.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.13.12.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.14.13" class="ltx_tr">
<th id="A1.T5.3.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">13 conv</th>
<th id="A1.T5.3.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">512</th>
<td id="A1.T5.3.14.13.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.14.13.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.15.14" class="ltx_tr">
<th id="A1.T5.3.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">14 maxpool</th>
<th id="A1.T5.3.15.14.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T5.3.15.14.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T5.3.15.14.4" class="ltx_td"></td>
</tr>
<tr id="A1.T5.3.16.15" class="ltx_tr">
<th id="A1.T5.3.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">15 conv</th>
<th id="A1.T5.3.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">512</th>
<td id="A1.T5.3.16.15.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.16.15.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.17.16" class="ltx_tr">
<th id="A1.T5.3.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">16 conv</th>
<th id="A1.T5.3.17.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">512</th>
<td id="A1.T5.3.17.16.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.17.16.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.18.17" class="ltx_tr">
<th id="A1.T5.3.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">17 conv</th>
<th id="A1.T5.3.18.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">512</th>
<td id="A1.T5.3.18.17.3" class="ltx_td ltx_align_center">3x3</td>
<td id="A1.T5.3.18.17.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.19.18" class="ltx_tr">
<th id="A1.T5.3.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">18 maxpool</th>
<th id="A1.T5.3.19.18.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T5.3.19.18.3" class="ltx_td ltx_align_center">2x2</td>
<td id="A1.T5.3.19.18.4" class="ltx_td"></td>
</tr>
<tr id="A1.T5.3.20.19" class="ltx_tr">
<th id="A1.T5.3.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">19 flatten</th>
<th id="A1.T5.3.20.19.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T5.3.20.19.3" class="ltx_td"></td>
<td id="A1.T5.3.20.19.4" class="ltx_td"></td>
</tr>
<tr id="A1.T5.3.21.20" class="ltx_tr">
<th id="A1.T5.3.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">20 dense</th>
<th id="A1.T5.3.21.20.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">128</th>
<td id="A1.T5.3.21.20.3" class="ltx_td"></td>
<td id="A1.T5.3.21.20.4" class="ltx_td ltx_align_center">relu</td>
</tr>
<tr id="A1.T5.3.22.21" class="ltx_tr">
<th id="A1.T5.3.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">21 dropout</th>
<th id="A1.T5.3.22.21.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="2">p = 0.50</th>
<td id="A1.T5.3.22.21.3" class="ltx_td"></td>
</tr>
<tr id="A1.T5.3.23.22" class="ltx_tr">
<th id="A1.T5.3.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">22 dense</th>
<th id="A1.T5.3.23.22.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">16</th>
<td id="A1.T5.3.23.22.3" class="ltx_td ltx_border_b"></td>
<td id="A1.T5.3.23.22.4" class="ltx_td ltx_align_center ltx_border_b">softmax</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Reconstruction Examples of DLG/iDLG/IG</h2>

<figure id="A2.F8" class="ltx_figure"><img src="/html/2105.09400/assets/x15.png" id="A2.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="345" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Reconstruction Examples of <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> with Model Partitioning and Permutation</figcaption>
</figure>
<div id="A2.p1" class="ltx_para">
<p id="A2.p1.6" class="ltx_p">In Figure <a href="#A0.F7" title="Figure 7 ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we display the intermediate results of reconstructing an image using <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> and <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> with different combinations of partitioning and permutation parameters. The first row of images shows the reconstruction results with the entire model updates, i.e., <math id="A2.p1.1.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="A2.p1.1.m1.1a"><mrow id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mn id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml">100</mn><mo id="A2.p1.1.m1.1.1.1" xref="A2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><csymbol cd="latexml" id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">100\%</annotation></semantics></math> partition, over <math id="A2.p1.2.m2.1" class="ltx_Math" alttext="300" display="inline"><semantics id="A2.p1.2.m2.1a"><mn id="A2.p1.2.m2.1.1" xref="A2.p1.2.m2.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="A2.p1.2.m2.1b"><cn type="integer" id="A2.p1.2.m2.1.1.cmml" xref="A2.p1.2.m2.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.2.m2.1c">300</annotation></semantics></math> iterations. We use it as the baseline for comparison. The images at iteration <math id="A2.p1.3.m3.1" class="ltx_Math" alttext="0" display="inline"><semantics id="A2.p1.3.m3.1a"><mn id="A2.p1.3.m3.1.1" xref="A2.p1.3.m3.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="A2.p1.3.m3.1b"><cn type="integer" id="A2.p1.3.m3.1.1.cmml" xref="A2.p1.3.m3.1.1">0</cn></annotation-xml></semantics></math> is the randomly initialized input. We observed that with the entire, unperturbed model update, <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> and <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> can reconstruct most of the original image after <math id="A2.p1.4.m4.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A2.p1.4.m4.1a"><mn id="A2.p1.4.m4.1.1" xref="A2.p1.4.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A2.p1.4.m4.1b"><cn type="integer" id="A2.p1.4.m4.1.1.cmml" xref="A2.p1.4.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.4.m4.1c">100</annotation></semantics></math> iterations.
In the second and third row, we randomly kept <math id="A2.p1.5.m5.1" class="ltx_Math" alttext="60\%/20\%" display="inline"><semantics id="A2.p1.5.m5.1a"><mrow id="A2.p1.5.m5.1.1" xref="A2.p1.5.m5.1.1.cmml"><mrow id="A2.p1.5.m5.1.1.2" xref="A2.p1.5.m5.1.1.2.cmml"><mn id="A2.p1.5.m5.1.1.2.2" xref="A2.p1.5.m5.1.1.2.2.cmml">60</mn><mo id="A2.p1.5.m5.1.1.2.1" xref="A2.p1.5.m5.1.1.2.1.cmml">%</mo></mrow><mo id="A2.p1.5.m5.1.1.1" xref="A2.p1.5.m5.1.1.1.cmml">/</mo><mrow id="A2.p1.5.m5.1.1.3" xref="A2.p1.5.m5.1.1.3.cmml"><mn id="A2.p1.5.m5.1.1.3.2" xref="A2.p1.5.m5.1.1.3.2.cmml">20</mn><mo id="A2.p1.5.m5.1.1.3.1" xref="A2.p1.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.5.m5.1b"><apply id="A2.p1.5.m5.1.1.cmml" xref="A2.p1.5.m5.1.1"><divide id="A2.p1.5.m5.1.1.1.cmml" xref="A2.p1.5.m5.1.1.1"></divide><apply id="A2.p1.5.m5.1.1.2.cmml" xref="A2.p1.5.m5.1.1.2"><csymbol cd="latexml" id="A2.p1.5.m5.1.1.2.1.cmml" xref="A2.p1.5.m5.1.1.2.1">percent</csymbol><cn type="integer" id="A2.p1.5.m5.1.1.2.2.cmml" xref="A2.p1.5.m5.1.1.2.2">60</cn></apply><apply id="A2.p1.5.m5.1.1.3.cmml" xref="A2.p1.5.m5.1.1.3"><csymbol cd="latexml" id="A2.p1.5.m5.1.1.3.1.cmml" xref="A2.p1.5.m5.1.1.3.1">percent</csymbol><cn type="integer" id="A2.p1.5.m5.1.1.3.2.cmml" xref="A2.p1.5.m5.1.1.3.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.5.m5.1c">60\%/20\%</annotation></semantics></math> of the model updates and provided them to the <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> and <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> attacks. With only partial information, both attacks were unable to correctly minimize their respective cost function. Thus, the intermediate and final reconstructions do not contain any recognizable information related to the ground truth example.
In the fourth through sixth rows, we randomly permuted the model updates with <math id="A2.p1.6.m6.1" class="ltx_Math" alttext="100\%/60\%/20\%" display="inline"><semantics id="A2.p1.6.m6.1a"><mrow id="A2.p1.6.m6.1.1" xref="A2.p1.6.m6.1.1.cmml"><mrow id="A2.p1.6.m6.1.1.2" xref="A2.p1.6.m6.1.1.2.cmml"><mn id="A2.p1.6.m6.1.1.2.2" xref="A2.p1.6.m6.1.1.2.2.cmml">100</mn><mo id="A2.p1.6.m6.1.1.2.1" xref="A2.p1.6.m6.1.1.2.1.cmml">%</mo></mrow><mo id="A2.p1.6.m6.1.1.1" xref="A2.p1.6.m6.1.1.1.cmml">/</mo><mrow id="A2.p1.6.m6.1.1.3" xref="A2.p1.6.m6.1.1.3.cmml"><mn id="A2.p1.6.m6.1.1.3.2" xref="A2.p1.6.m6.1.1.3.2.cmml">60</mn><mo id="A2.p1.6.m6.1.1.3.1" xref="A2.p1.6.m6.1.1.3.1.cmml">%</mo></mrow><mo id="A2.p1.6.m6.1.1.1a" xref="A2.p1.6.m6.1.1.1.cmml">/</mo><mrow id="A2.p1.6.m6.1.1.4" xref="A2.p1.6.m6.1.1.4.cmml"><mn id="A2.p1.6.m6.1.1.4.2" xref="A2.p1.6.m6.1.1.4.2.cmml">20</mn><mo id="A2.p1.6.m6.1.1.4.1" xref="A2.p1.6.m6.1.1.4.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.6.m6.1b"><apply id="A2.p1.6.m6.1.1.cmml" xref="A2.p1.6.m6.1.1"><divide id="A2.p1.6.m6.1.1.1.cmml" xref="A2.p1.6.m6.1.1.1"></divide><apply id="A2.p1.6.m6.1.1.2.cmml" xref="A2.p1.6.m6.1.1.2"><csymbol cd="latexml" id="A2.p1.6.m6.1.1.2.1.cmml" xref="A2.p1.6.m6.1.1.2.1">percent</csymbol><cn type="integer" id="A2.p1.6.m6.1.1.2.2.cmml" xref="A2.p1.6.m6.1.1.2.2">100</cn></apply><apply id="A2.p1.6.m6.1.1.3.cmml" xref="A2.p1.6.m6.1.1.3"><csymbol cd="latexml" id="A2.p1.6.m6.1.1.3.1.cmml" xref="A2.p1.6.m6.1.1.3.1">percent</csymbol><cn type="integer" id="A2.p1.6.m6.1.1.3.2.cmml" xref="A2.p1.6.m6.1.1.3.2">60</cn></apply><apply id="A2.p1.6.m6.1.1.4.cmml" xref="A2.p1.6.m6.1.1.4"><csymbol cd="latexml" id="A2.p1.6.m6.1.1.4.1.cmml" xref="A2.p1.6.m6.1.1.4.1">percent</csymbol><cn type="integer" id="A2.p1.6.m6.1.1.4.2.cmml" xref="A2.p1.6.m6.1.1.4.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.6.m6.1c">100\%/60\%/20\%</annotation></semantics></math> partitions respectively and ran the <abbr title="Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DLG</span></abbr> and <abbr title="Improved Deep Leakage from Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">iDLG</span></abbr> attacks. Similar to the previous results with partitioning only, the intermediate reconstructions do not contain any recognizable information related to the ground truth example. Of note is the fourth row of images, as they demonstrate that <span id="A2.p1.6.1" class="ltx_text ltx_font_smallcaps">Truda</span>’s permutation mechanism works even when only a single aggregator is present, which is traditionally used in <abbr title="Federated Learning" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FL</span></abbr>.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.2" class="ltx_p">In Figure <a href="#A2.F8" title="Figure 8 ‣ Appendix B Reconstruction Examples of DLG/iDLG/IG ‣ Separation of Powers in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we display five <em id="A2.p2.2.1" class="ltx_emph ltx_font_italic">ImageNet</em> examples used in our <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> reconstruction experiments. The first column shows the original images. The second column presents the reconstructed images (after <math id="A2.p2.1.m1.2" class="ltx_Math" alttext="24,000" display="inline"><semantics id="A2.p2.1.m1.2a"><mrow id="A2.p2.1.m1.2.3.2" xref="A2.p2.1.m1.2.3.1.cmml"><mn id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml">24</mn><mo id="A2.p2.1.m1.2.3.2.1" xref="A2.p2.1.m1.2.3.1.cmml">,</mo><mn id="A2.p2.1.m1.2.2" xref="A2.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.2b"><list id="A2.p2.1.m1.2.3.1.cmml" xref="A2.p2.1.m1.2.3.2"><cn type="integer" id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1">24</cn><cn type="integer" id="A2.p2.1.m1.2.2.cmml" xref="A2.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.2c">24,000</annotation></semantics></math> iterations) when the entire model updates (<math id="A2.p2.2.m2.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="A2.p2.2.m2.1a"><mrow id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml"><mn id="A2.p2.2.m2.1.1.2" xref="A2.p2.2.m2.1.1.2.cmml">100</mn><mo id="A2.p2.2.m2.1.1.1" xref="A2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><apply id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1"><csymbol cd="latexml" id="A2.p2.2.m2.1.1.1.cmml" xref="A2.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="A2.p2.2.m2.1.1.2.cmml" xref="A2.p2.2.m2.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">100\%</annotation></semantics></math> partition) were provided to the <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> attack. These results are used as the baseline for comparison. In the third through seventh columns, we present the reconstructions with different combinations of partitioning and permutation enabled. Compared to the baseline, none of the reconstructions contain recognizable information related to the ground truth examples. Without the complete, unperturbed model updates, <abbr title="Inverting Gradients" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">IG</span></abbr> is unable to minimize its cost function.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2105.09399" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2105.09400" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2105.09400">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2105.09400" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2105.09401" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 05:35:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
