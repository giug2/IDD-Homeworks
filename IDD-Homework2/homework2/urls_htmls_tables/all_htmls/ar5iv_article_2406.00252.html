<article class="ltx_document">
 <h1 class="ltx_title ltx_title_document">
  Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Bowen Jiang
    <sup class="ltx_sup" id="id1.1.id1">
     1
    </sup>
    ,
Yangxinyu Xie
    <sup class="ltx_sup" id="id2.2.id2">
     1, 2
    </sup>
    ,
Xiaomeng Wang
    <sup class="ltx_sup" id="id3.3.id3">
     1
    </sup>
    ,
Weijie J. Su
    <sup class="ltx_sup" id="id4.4.id4">
     1
    </sup>
    ,
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id5.5.id5">
     Camillo J. Taylor
     <sup class="ltx_sup" id="id5.5.id5.1">
      1
     </sup>
     ,
Tanwi Mallick
     <sup class="ltx_sup" id="id5.5.id5.2">
      2
     </sup>
    </span>
    <br class="ltx_break"/>
    University of Pennsylvania
    <sup class="ltx_sup" id="id6.6.id6">
     1
    </sup>
    Argonne National Laboratory
    <sup class="ltx_sup" id="id7.7.id7">
     2
    </sup>
    <br class="ltx_break"/>
    Philadelphia, PA, 19104, USA   Lemont, IL, 60439, USA
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id8.8.id8">
     {bwjiang@seas, xinyux@wharton, xwang1@wharton}.upenn.edu
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id9.9.id9">
     {suw@wharton, cjtaylor@seas}.upenn.edu, tmallick@anl.gov
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id10.id1">
   Rationality is the quality of being guided by reason, characterized by decision-making aligned with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and consistently derived. Despite the advancements of large language models (LLMs) in generating human-like texts with remarkable accuracy, they present limited knowledge space, inconsistency across contexts, and difficulty understanding complex scenarios. Therefore, recent research focuses on building multi-modal and multi-agent systems to achieve considerable progress with enhanced consistency and reliability, instead of relying on a single LLM as the sole planning or decision-making agent. To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at
   <a class="ltx_ref ltx_href" href="https://github.com/bowen-upenn/MMMA_Rationality" target="_blank" title="">
    https://github.com/bowen-upenn/MMMA_Rationality
   </a>
   .
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <div class="ltx_block ltx_align_bottom" id="p1.1">
   <p class="ltx_p" id="p1.1.1">
    <span class="ltx_text ltx_font_bold" id="p1.1.1.1">
     Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
   <p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;">
    <span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
     <span class="ltx_tabular ltx_guessed_headers ltx_align_top" id="p1.1.2.1.1">
      <span class="ltx_thead">
       <span class="ltx_tr" id="p1.1.2.1.1.1.1">
        <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.1.2.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">
          Bowen Jiang
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">
           1
          </sup>
          ,
Yangxinyu Xie
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">
           1, 2
          </sup>
          ,
Xiaomeng Wang
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">
           1
          </sup>
          ,
Weijie J. Su
          <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4">
           1
          </sup>
          ,
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.1.2.1.1.2.2">
        <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.1.2.1.1.2.2.1">
         <span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.1">
          Camillo J. Taylor
          <sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.1">
           1
          </sup>
          ,
Tanwi Mallick
          <sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.2">
           2
          </sup>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.1.2.1.1.3.3">
        <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p1.1.2.1.1.3.3.1">
         University of Pennsylvania
         <sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">
          1
         </sup>
         Argonne National Laboratory
         <sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.2">
          2
         </sup>
        </span>
       </span>
      </span>
      <span class="ltx_tbody">
       <span class="ltx_tr" id="p1.1.2.1.1.4.1">
        <span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.1.1">
         Philadelphia, PA, 19104, USA   Lemont, IL, 60439, USA
        </span>
       </span>
       <span class="ltx_tr" id="p1.1.2.1.1.5.2">
        <span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.2.1">
         <span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.5.2.1.1">
          {bwjiang@seas, xinyux@wharton, xwang1@wharton}.upenn.edu
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.1.2.1.1.6.3">
        <span class="ltx_td ltx_align_center" id="p1.1.2.1.1.6.3.1">
         <span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.6.3.1.1">
          {suw@wharton, cjtaylor@seas}.upenn.edu, tmallick@anl.gov
         </span>
        </span>
       </span>
      </span>
     </span>
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
  </div>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Large language models (LLMs) have demonstrated promising results across a broad spectrum of tasks, particularly in exhibiting capabilities that plausibly mimic human-like reasoning
    <cite class="ltx_cite ltx_citemacro_cite">
     Wei et al. (
     <a class="ltx_ref" href="#bib.bib179" title="">
      2022
     </a>
     ); Yao et al. (
     <a class="ltx_ref" href="#bib.bib197" title="">
      2024
     </a>
     ); Besta et al. (
     <a class="ltx_ref" href="#bib.bib8" title="">
      2024
     </a>
     ); Shinn et al. (
     <a class="ltx_ref" href="#bib.bib145" title="">
      2024
     </a>
     ); Bubeck et al. (
     <a class="ltx_ref" href="#bib.bib12" title="">
      2023
     </a>
     ); Valmeekam et al. (
     <a class="ltx_ref" href="#bib.bib164" title="">
      2023
     </a>
     ); Prasad et al. (
     <a class="ltx_ref" href="#bib.bib121" title="">
      2023
     </a>
     )
    </cite>
    . These models leverage the richness of human language to abstract concepts, elaborate thinking process, comprehend complex user queries, and develop plans and solutions in decision-making scenarios. Despite these advances, recent research has revealed that even state-of-the-art LLMs exhibit various forms of irrational behaviors, such as the framing effect, certainty effect, overweighting bias, and conjunction fallacy
    <cite class="ltx_cite ltx_citemacro_cite">
     Binz and Schulz (
     <a class="ltx_ref" href="#bib.bib9" title="">
      2023
     </a>
     ); Echterhoff et al. (
     <a class="ltx_ref" href="#bib.bib34" title="">
      2024
     </a>
     ); Mukherjee and Chang (
     <a class="ltx_ref" href="#bib.bib110" title="">
      2024
     </a>
     ); Macmillan-Scott and Musolesi (
     <a class="ltx_ref" href="#bib.bib104" title="">
      2024
     </a>
     ); Wang et al. (
     <a class="ltx_ref" href="#bib.bib169" title="">
      2024a
     </a>
     ); Suri et al. (
     <a class="ltx_ref" href="#bib.bib157" title="">
      2024
     </a>
     )
    </cite>
    . Irrationality undermine the practical deployment of LLMs in critical sectors like healthcare, finance, and legal services
    <cite class="ltx_cite ltx_citemacro_cite">
     He et al. (
     <a class="ltx_ref" href="#bib.bib53" title="">
      2023
     </a>
     ); Li et al. (
     <a class="ltx_ref" href="#bib.bib86" title="">
      2023h
     </a>
     ); Kang and Liu (
     <a class="ltx_ref" href="#bib.bib66" title="">
      2023
     </a>
     ); Cheong et al. (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2024
     </a>
     )
    </cite>
    , where reliability and consistency are paramount. The emerging concern about the factual accuracy and trustworthiness of LLMs highlights an urgent need to develop better agents or agent systems
    <cite class="ltx_cite ltx_citemacro_cite">
     Nakajima (
     <a class="ltx_ref" href="#bib.bib111" title="">
      2023
     </a>
     ); Gravitas (
     <a class="ltx_ref" href="#bib.bib45" title="">
      2023
     </a>
     )
    </cite>
    with rational reasoning processes.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    A single LLM agent can fall into irrational behaviors because it cannot go beyond the language model’s inner parametric representations of textual knowledge, lacking the real-world grounding and feedback mechanisms necessary to develop rationality
    <cite class="ltx_cite ltx_citemacro_citep">
     (Bubeck et al.,
     <a class="ltx_ref" href="#bib.bib12" title="">
      2023
     </a>
     ; Sun,
     <a class="ltx_ref" href="#bib.bib155" title="">
      2024
     </a>
     )
    </cite>
    .
In contrast, in real life scenarios, important decisions are rarely made by individuals on their own, and the complexity of problems often requires the collaboration of experts from different fields to ensure rationality
    <cite class="ltx_cite ltx_citemacro_citep">
     (Eisenführ et al.,
     <a class="ltx_ref" href="#bib.bib35" title="">
      2010
     </a>
     )
    </cite>
    . In a similar vein, recent advancements in multi-modal and multi-agent frameworks leverage the expertise of different agents acting together towards a collective goal. Multi-modal foundation models
    <cite class="ltx_cite ltx_citemacro_cite">
     Awadalla et al. (
     <a class="ltx_ref" href="#bib.bib3" title="">
      2023
     </a>
     ); Liu et al. (
     <a class="ltx_ref" href="#bib.bib92" title="">
      2023a
     </a>
     ); Wang et al. (
     <a class="ltx_ref" href="#bib.bib171" title="">
      2023c
     </a>
     ); OpenAI (
     <a class="ltx_ref" href="#bib.bib114" title="">
      2023
     </a>
     ); Reid et al. (
     <a class="ltx_ref" href="#bib.bib127" title="">
      2024
     </a>
     )
    </cite>
    enhance reasoning by grounding decisions in a broader sensory context, akin to how human brains integrate rich sensory inputs to form a more holistic base of knowledge. Meanwhile, multi-agent systems introduce mechanisms such as consensus, debate, and self-consistency
    <cite class="ltx_cite ltx_citemacro_cite">
     Du et al. (
     <a class="ltx_ref" href="#bib.bib30" title="">
      2023
     </a>
     ); Liang et al. (
     <a class="ltx_ref" href="#bib.bib88" title="">
      2023
     </a>
     ); Talebirad and Nadiri (
     <a class="ltx_ref" href="#bib.bib159" title="">
      2023
     </a>
     ); Madaan et al. (
     <a class="ltx_ref" href="#bib.bib105" title="">
      2024
     </a>
     ); Cohen et al. (
     <a class="ltx_ref" href="#bib.bib23" title="">
      2023
     </a>
     ); Shinn et al. (
     <a class="ltx_ref" href="#bib.bib145" title="">
      2024
     </a>
     ); Mohtashami et al. (
     <a class="ltx_ref" href="#bib.bib109" title="">
      2023
     </a>
     )
    </cite>
    to allow for refined and reliable output through collaborative interactions. Such systems can also query external knowledge sources or tools
    <cite class="ltx_cite ltx_citemacro_cite">
     Lewis et al. (
     <a class="ltx_ref" href="#bib.bib75" title="">
      2020
     </a>
     ); Schick et al. (
     <a class="ltx_ref" href="#bib.bib134" title="">
      2024
     </a>
     ); Tang et al. (
     <a class="ltx_ref" href="#bib.bib162" title="">
      2023
     </a>
     ); Pan et al. (
     <a class="ltx_ref" href="#bib.bib117" title="">
      2024
     </a>
     )
    </cite>
    to augment their reasoning capabilities for rational decision making.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    This survey provides a unique lens to reinterpret the underlying motivations behind current multi-modal and/or multi-agent systems by drawing insights from cognitive science. In Section
    <a class="ltx_ref" href="#S2" title="2 Defining Rationality ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    , we outline four essential requirements for rational decision making. Section
    <a class="ltx_ref" href="#S4" title="4 Towards Rationality through Multi-Modal and Multi-Agent Systems ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    then examines how various research areas within the multi-modality and multi-agent literature are advancing towards rationality based on these criteria. We argue that these advancements surpass the limitations of single language-model agents and narrow the gap between the behavior of agent systems and the expectations for rational decision making.
Lastly, Section
    <a class="ltx_ref" href="#S5" title="5 Evaluating Rationality of Agents ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    highlights the lack of sufficient evaluation metrics and benchmarks in the existing literature to adequately measure the rationality of LLMs or agent systems. We hope this survey can inspire further research at the intersection between agent systems and cognitive science.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Defining Rationality
  </h2>
  <figure class="ltx_figure" id="S2.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="495" id="S2.F1.g1" src="/html/2406.00252/assets/tree.png" width="574"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    The evolutionary tree of multi-agent and/or multi-modal systems related to the four axioms of rationality. Many proposed approaches strive to address multiple axioms simultaneously. The
    <span class="ltx_text ltx_font_bold" id="S2.F1.2.1">
     bold
    </span>
    font marks works that involve multi-modalities. This tree also includes some foundational works to provide a clearer reference of time.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    A rational agent should respect the reality of the world in which it operates and avoid reaching contradictory conclusions in decision-making processes. Drawing on foundational works in rational decision-making
    <cite class="ltx_cite ltx_citemacro_citep">
     (Tversky and Kahneman,
     <a class="ltx_ref" href="#bib.bib163" title="">
      1988
     </a>
     ; Hastie and Dawes,
     <a class="ltx_ref" href="#bib.bib52" title="">
      2009
     </a>
     ; Eisenführ et al.,
     <a class="ltx_ref" href="#bib.bib35" title="">
      2010
     </a>
     )
    </cite>
    , this section adopts an axiomatic approach to define rationality, presenting four substantive axioms we expect a rational agent or agent systems to fulfill:
   </p>
  </div>
  <section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Grounding
   </h4>
   <div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">
     The decision of a rational agent is grounded on the physical and factual reality.
For example, a video generation agent should adhere to the laws of physics in a world model and a forecasting assistant ought to estimate likelihoods obeying the law of probability.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Orderability of Preferences
   </h4>
   <div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">
     When comparing alternatives in a decision scenario, a rational agent can rank the options based on the current state and ultimately select the most preferred one based on the expected outcomes. This orderability consists of several key principles, including comparability, transitivity closure, solvability, etc. with detailed defined in Appexdix
     <a class="ltx_ref" href="#A1" title="Appendix A Orderability of Preferences. ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       A
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Independence from irrelevant context
   </h4>
   <div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">
     The agent’s preference should not be influenced by information irrelevant to the decision-making problem at hand.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S2.SS0.SSS0.Px4">
   <h4 class="ltx_title ltx_title_paragraph">
    Invariance
   </h4>
   <div class="ltx_para" id="S2.SS0.SSS0.Px4.p1">
    <p class="ltx_p" id="S2.SS0.SSS0.Px4.p1.1">
     The preference of a rational agent remains invariant across equivalent representations of the decision problem, regardless of specific wordings or modalities.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Scope
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Unlike existing surveys
    <cite class="ltx_cite ltx_citemacro_cite">
     Han et al. (
     <a class="ltx_ref" href="#bib.bib51" title="">
      2024
     </a>
     ); Guo et al. (
     <a class="ltx_ref" href="#bib.bib47" title="">
      2024
     </a>
     ); Xie et al. (
     <a class="ltx_ref" href="#bib.bib186" title="">
      2024a
     </a>
     ); Durante et al. (
     <a class="ltx_ref" href="#bib.bib31" title="">
      2024
     </a>
     ); Cui et al. (
     <a class="ltx_ref" href="#bib.bib25" title="">
      2024
     </a>
     ); Xu et al. (
     <a class="ltx_ref" href="#bib.bib191" title="">
      2024b
     </a>
     ); Zhang et al. (
     <a class="ltx_ref" href="#bib.bib208" title="">
      2024a
     </a>
     ); Cheng et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2024
     </a>
     ); Li et al. (
     <a class="ltx_ref" href="#bib.bib79" title="">
      2024a
     </a>
     )
    </cite>
    that focus on the components, structures, agent profiling, planning, communications, memories, and applications of multi-modal and/or multi-agent systems,
    <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">
     this survey is the first to specifically examine the increasingly important relations between rationality and these multi-modal and multi-agent systems
    </span>
    , exploring how they contribute to enhancing the robustness in decision-making processes.
   </p>
  </div>
  <div class="ltx_para" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    We emphasize that
    <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">
     rationality
    </span>
    , by definition, is not equivalent to
    <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">
     reasoning
    </span>
    <cite class="ltx_cite ltx_citemacro_cite">
     Khardon and Roth (
     <a class="ltx_ref" href="#bib.bib69" title="">
      1997
     </a>
     ); Huang and Chang (
     <a class="ltx_ref" href="#bib.bib60" title="">
      2022
     </a>
     ); Zhang et al. (
     <a class="ltx_ref" href="#bib.bib208" title="">
      2024a
     </a>
     ); Qiao et al. (
     <a class="ltx_ref" href="#bib.bib123" title="">
      2022
     </a>
     )
    </cite>
    , although deeply intertwined. Rationality involves making logically consistent decisions grounded with reality, while reasoning refers to the cognitive process of drawing logical inferences and conclusions from available information, as illustrated in the following thought experiment:
   </p>
   <blockquote class="ltx_quote" id="S3.p2.2">
    <p class="ltx_p" id="S3.p2.2.1">
     <span class="ltx_text ltx_font_italic" id="S3.p2.2.1.1">
      Consider an environment where the input space and the output decision space are finite. A lookup table with consistent mapping from input to output is inherently rational, while no reasoning is necessarily present in the mapping.
     </span>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para" id="S3.p3">
   <p class="ltx_p" id="S3.p3.1">
    Despite this example, it is still crucial to acknowledge that reasoning typically plays a vital role in ensuring rationality, especially in complex and dynamic real-world scenarios where a simple lookup table is insufficient.
Agents must possess the ability to reason through novel situations, adapt to changing circumstances, make plans, and achieve rational decisions based on incomplete or uncertain information.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Towards Rationality through Multi-Modal and Multi-Agent Systems
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    This section surveys recent advancements in multi-modal and multi-agent systems under different research categories as depicted in Figure
    <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2 Defining Rationality ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    .
Each category, such as knowledge retrieval or neuro-symbolic reasoning, addresses one or more fundamental requirements for rational thinking. These rationality requirements are typically
    <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">
     intertwined
    </span>
    : an approach that enhances one aspect of rationality often inherently improves others simultaneously.
Meanwhile, the overall mechanism of current multi-agent system in achieving rationality can categorized into two key concepts:
    <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">
     deliberation
    </span>
    and
    <span class="ltx_text ltx_font_bold" id="S4.p1.1.3">
     abstraction
    </span>
    . Deliberation encourages a slower, iterative reasoning process, while abstraction refers to abstracting the problem into its logical essence.
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    Most existing studies do not explicitly base their frameworks on rationality in their original writings. Our analysis aims to reinterpret these works through the lens of our four axioms of rationality, offering a novel perspective that bridges existing methodologies with rational principles.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Towards Grounding &amp; Invariance
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     Multi-modal approaches aim to improve information grounding across various channels, such as language, vision, and beyond. By incorporating multi-modal agents, multi-agent systems can greatly expand their capabilities, enabling a richer, more accurate, and contextually aware interpretation of the environment.
    </p>
   </div>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Multi-Modal Foundation Models
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">
      Grounding an agent solely based on textual language can be challenging, as information can be represented much more efficiently through other sensory modes. As a picture is worth a thousand words,
recent advances in large vision-language pretraining have enabled LLMs with robust language comprehension capabilities to finally perceive the visual world. Multi-modal foundation models, including but not limited to CLIP
      <cite class="ltx_cite ltx_citemacro_cite">
       Radford et al. (
       <a class="ltx_ref" href="#bib.bib125" title="">
        2021
       </a>
       )
      </cite>
      , VLBERT and ViLBERT
      <cite class="ltx_cite ltx_citemacro_cite">
       Su et al. (
       <a class="ltx_ref" href="#bib.bib151" title="">
        2019
       </a>
       ); Lu et al. (
       <a class="ltx_ref" href="#bib.bib99" title="">
        2019
       </a>
       )
      </cite>
      , BLIP-2
      <cite class="ltx_cite ltx_citemacro_cite">
       Li et al. (
       <a class="ltx_ref" href="#bib.bib82" title="">
        2023d
       </a>
       )
      </cite>
      , (Open) Flamingo
      <cite class="ltx_cite ltx_citemacro_cite">
       Alayrac et al. (
       <a class="ltx_ref" href="#bib.bib2" title="">
        2022
       </a>
       ); Awadalla et al. (
       <a class="ltx_ref" href="#bib.bib3" title="">
        2023
       </a>
       )
      </cite>
      , LLaVA
      <cite class="ltx_cite ltx_citemacro_cite">
       Liu et al. (
       <a class="ltx_ref" href="#bib.bib93" title="">
        2024c
       </a>
       ,
       <a class="ltx_ref" href="#bib.bib92" title="">
        2023a
       </a>
       )
      </cite>
      , CogVLM
      <cite class="ltx_cite ltx_citemacro_cite">
       Wang et al. (
       <a class="ltx_ref" href="#bib.bib171" title="">
        2023c
       </a>
       )
      </cite>
      , MiniGPT-4
      <cite class="ltx_cite ltx_citemacro_cite">
       Zhu et al. (
       <a class="ltx_ref" href="#bib.bib219" title="">
        2023a
       </a>
       )
      </cite>
      , GPT-4 Vision
      <cite class="ltx_cite ltx_citemacro_cite">
       OpenAI (
       <a class="ltx_ref" href="#bib.bib114" title="">
        2023
       </a>
       )
      </cite>
      and GPT-4o
      <cite class="ltx_cite ltx_citemacro_cite">
       OpenAI (
       <a class="ltx_ref" href="#bib.bib115" title="">
        2024
       </a>
       )
      </cite>
      , and Gemini 1.5 Pro
      <cite class="ltx_cite ltx_citemacro_cite">
       Reid et al. (
       <a class="ltx_ref" href="#bib.bib127" title="">
        2024
       </a>
       )
      </cite>
      serve as the cornerstones for multi-modal agent systems to ground knowledge in vision and beyond.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Invariance Across Modalities
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">
      Achieving representation invariance across modalities is critical: given adequate information grounding, agents should make consistent decisions across different modalities that share equivalent underlying logic. Multi-modal foundation models are particularly adept at promoting invariance by processing multi-modal data in an unified representation. Specifically, their large-scale cross-modal pretraining stage seamlessly tokenizes both vision and language inputs into a joint hidden embedding space, learning cross-modal correlations through a data-driven approach. In other words, image tokens are simply regarded as a foreign language
      <cite class="ltx_cite ltx_citemacro_cite">
       Wang et al. (
       <a class="ltx_ref" href="#bib.bib172" title="">
        2022a
       </a>
       )
      </cite>
      . Moreover, the cross-modal validation inherent in multi-modal foundation models allows for reconciliation of data from different modalities, closing their distance in the hidden embedding space
      <cite class="ltx_cite ltx_citemacro_cite">
       Radford et al. (
       <a class="ltx_ref" href="#bib.bib125" title="">
        2021
       </a>
       )
      </cite>
      .
     </p>
    </div>
    <div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
     <p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.1">
      The concept of invariance is the cornerstone of Visual Question Answering (VQA) agents
      <cite class="ltx_cite ltx_citemacro_cite">
       Chen et al. (
       <a class="ltx_ref" href="#bib.bib15" title="">
        2022
       </a>
       ); Jiang et al. (
       <a class="ltx_ref" href="#bib.bib64" title="">
        2024b
       </a>
       ); Wang et al. (
       <a class="ltx_ref" href="#bib.bib175" title="">
        2023d
       </a>
       ); Yi et al. (
       <a class="ltx_ref" href="#bib.bib202" title="">
        2018
       </a>
       ); Wang et al. (
       <a class="ltx_ref" href="#bib.bib172" title="">
        2022a
       </a>
       ); Bao et al. (
       <a class="ltx_ref" href="#bib.bib6" title="">
        2022
       </a>
       ); Zhao and Xu (
       <a class="ltx_ref" href="#bib.bib210" title="">
        2023
       </a>
       )
      </cite>
      . On one hand, these agents must grasp the invariant semantics of any open-ended questions posed about images, maintaining consistency despite variations in wording, syntax, or language. On the other hand, within a multi-agent VQA system, visual agents can provide crucial verification and support for language-based reasoning
      <cite class="ltx_cite ltx_citemacro_cite">
       Wang et al. (
       <a class="ltx_ref" href="#bib.bib175" title="">
        2023d
       </a>
       ); Jiang et al. (
       <a class="ltx_ref" href="#bib.bib64" title="">
        2024b
       </a>
       ); Zhao and Xu (
       <a class="ltx_ref" href="#bib.bib210" title="">
        2023
       </a>
       )
      </cite>
      , while language queries can direct the attention of visual agents, based on a shared and invariant underlying knowledge across vision and language domains.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Information Grounding
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">
      Multi-modalities help enhance the functionality of agent systems through more diverse information grounding.
Web agents are a quintessential example of how multi-modal agents surpass language-only ones. Because HTML code is often lengthy, contains irrelevant information, and may be incomplete
      <cite class="ltx_cite ltx_citemacro_cite">
       Zheng et al. (
       <a class="ltx_ref" href="#bib.bib213" title="">
        2024a
       </a>
       )
      </cite>
      , web navigation grounded on the graphical user interface (GUI) offers higher information density compared to solely HTML codes. As a result, using visual cues from the GUI leads to improved navigation performance
      <cite class="ltx_cite ltx_citemacro_cite">
       Shen et al. (
       <a class="ltx_ref" href="#bib.bib140" title="">
        2024a
       </a>
       ); Yao et al. (
       <a class="ltx_ref" href="#bib.bib196" title="">
        2022a
       </a>
       ); Deng et al. (
       <a class="ltx_ref" href="#bib.bib27" title="">
        2024
       </a>
       ); Gur et al. (
       <a class="ltx_ref" href="#bib.bib50" title="">
        2023
       </a>
       )
      </cite>
      . Multi-modalities also help enhance the functionality of agent systems through more diverse information grounding. For example, RA-CM3
      <cite class="ltx_cite ltx_citemacro_cite">
       Yasunaga et al. (
       <a class="ltx_ref" href="#bib.bib200" title="">
        2022
       </a>
       )
      </cite>
      augments baseline retrieval-augmented LLMs with raw multi-modal documents that include both images and texts, assuming that these two modalities can contextualize each other and make the documents more informative, leading to better generator performance. For other examples, we refer the reader to Appendix
      <a class="ltx_ref" href="#A2" title="Appendix B Information Grounding ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
       <span class="ltx_text ltx_ref_tag">
        B
       </span>
      </a>
      .
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
    <h4 class="ltx_title ltx_title_paragraph">
     Knowledge Retrieval &amp; Tool Usage
    </h4>
    <div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
     <p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.1">
      Bounded Rationality
      <cite class="ltx_cite ltx_citemacro_cite">
       March and Simon (
       <a class="ltx_ref" href="#bib.bib107" title="">
        1958
       </a>
       ); Selten (
       <a class="ltx_ref" href="#bib.bib137" title="">
        1990
       </a>
       )
      </cite>
      is a concept tailored to cognitively limited agents, suggesting that decision-making is limited by the resources available at hand, and any deviations from the optimal are primarily due to insufficient computational capacity and bounded working memory.
In terms of LLMs, the parametric nature of their existing architecture
      <cite class="ltx_cite ltx_citemacro_cite">
       Vaswani et al. (
       <a class="ltx_ref" href="#bib.bib165" title="">
        2017
       </a>
       )
      </cite>
      fundamentally limits how much information they can hold. As a result, in the face of uncertainty, LLMs often hallucinate
      <cite class="ltx_cite ltx_citemacro_cite">
       Bang et al. (
       <a class="ltx_ref" href="#bib.bib5" title="">
        2023
       </a>
       ); Guerreiro et al. (
       <a class="ltx_ref" href="#bib.bib46" title="">
        2023
       </a>
       ); Huang et al. (
       <a class="ltx_ref" href="#bib.bib61" title="">
        2023
       </a>
       )
      </cite>
      , generating outputs that are not supported by the factual reality of the environment.
Retrieval-Augmented Generation (RAG)
      <cite class="ltx_cite ltx_citemacro_cite">
       Lewis et al. (
       <a class="ltx_ref" href="#bib.bib75" title="">
        2020
       </a>
       )
      </cite>
      marks a significant milestone in addressing such an inherent limitation of LLMs.
Broadly speaking, RAG refers to any mechanism that provides external knowledge to the input context of an LLM and helps it deliver responses with up-to-date, factual, and grounded information
, especially in scientific and medical domains. Examples include Chameleon
      <cite class="ltx_cite ltx_citemacro_cite">
       Lu et al. (
       <a class="ltx_ref" href="#bib.bib101" title="">
        2024
       </a>
       )
      </cite>
      , Chain-of-Knowledge
      <cite class="ltx_cite ltx_citemacro_cite">
       Li et al. (
       <a class="ltx_ref" href="#bib.bib84" title="">
        2023f
       </a>
       )
      </cite>
      , WildfireGPT
      <cite class="ltx_cite ltx_citemacro_cite">
       Xie et al. (
       <a class="ltx_ref" href="#bib.bib187" title="">
        2024b
       </a>
       )
      </cite>
      , and Agent Hospital
      <cite class="ltx_cite ltx_citemacro_cite">
       Li et al. (
       <a class="ltx_ref" href="#bib.bib81" title="">
        2024b
       </a>
       )
      </cite>
      . Specifically, Chain-of-Knowledge
      <cite class="ltx_cite ltx_citemacro_cite">
       Li et al. (
       <a class="ltx_ref" href="#bib.bib84" title="">
        2023f
       </a>
       )
      </cite>
      even discovers that integrating multiple knowledge sources enhances performance by
      <math alttext="2.1\%" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.1.m1.1">
       <semantics id="S4.SS1.SSS0.Px4.p1.1.m1.1a">
        <mrow id="S4.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px4.p1.1.m1.1.1.cmml">
         <mn id="S4.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml">
          2.1
         </mn>
         <mo id="S4.SS1.SSS0.Px4.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml">
          %
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px4.p1.1.m1.1b">
         <apply id="S4.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px4.p1.1.m1.1.1">
          <csymbol cd="latexml" id="S4.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px4.p1.1.m1.1.1.1">
           percent
          </csymbol>
          <cn id="S4.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS1.SSS0.Px4.p1.1.m1.1.1.2">
           2.1
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px4.p1.1.m1.1c">
         2.1\%
        </annotation>
       </semantics>
      </math>
      compared to using a single source in its experiments.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS1.SSS0.Px4.p2">
     <p class="ltx_p" id="S4.SS1.SSS0.Px4.p2.3">
      Another line of systems
construct large-scale knowledge graphs (KGs)
      <cite class="ltx_cite ltx_citemacro_cite">
       Hogan et al. (
       <a class="ltx_ref" href="#bib.bib55" title="">
        2021
       </a>
       )
      </cite>
      from real-world sources to effectively expand their working memory and improve their task performance.
Specifically, compared to language-only models, MAVEx
      <cite class="ltx_cite ltx_citemacro_cite">
       Wu et al. (
       <a class="ltx_ref" href="#bib.bib183" title="">
        2022
       </a>
       )
      </cite>
      improves system’s scores by
      <math alttext="9.5\%" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p2.1.m1.1">
       <semantics id="S4.SS1.SSS0.Px4.p2.1.m1.1a">
        <mrow id="S4.SS1.SSS0.Px4.p2.1.m1.1.1" xref="S4.SS1.SSS0.Px4.p2.1.m1.1.1.cmml">
         <mn id="S4.SS1.SSS0.Px4.p2.1.m1.1.1.2" xref="S4.SS1.SSS0.Px4.p2.1.m1.1.1.2.cmml">
          9.5
         </mn>
         <mo id="S4.SS1.SSS0.Px4.p2.1.m1.1.1.1" xref="S4.SS1.SSS0.Px4.p2.1.m1.1.1.1.cmml">
          %
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px4.p2.1.m1.1b">
         <apply id="S4.SS1.SSS0.Px4.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px4.p2.1.m1.1.1">
          <csymbol cd="latexml" id="S4.SS1.SSS0.Px4.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px4.p2.1.m1.1.1.1">
           percent
          </csymbol>
          <cn id="S4.SS1.SSS0.Px4.p2.1.m1.1.1.2.cmml" type="float" xref="S4.SS1.SSS0.Px4.p2.1.m1.1.1.2">
           9.5
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px4.p2.1.m1.1c">
         9.5\%
        </annotation>
       </semantics>
      </math>
      compared to an image-only baseline through the integration of knowledge from ConceptNet
      <cite class="ltx_cite ltx_citemacro_cite">
       Speer et al. (
       <a class="ltx_ref" href="#bib.bib149" title="">
        2017
       </a>
       )
      </cite>
      and Wikipedia
      <cite class="ltx_cite ltx_citemacro_cite">
       Wikipedia contributors (
       <a class="ltx_ref" href="#bib.bib181" title="">
        2004
       </a>
       )
      </cite>
      . It also improves the scores by
      <math alttext="8.3\%" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p2.2.m2.1">
       <semantics id="S4.SS1.SSS0.Px4.p2.2.m2.1a">
        <mrow id="S4.SS1.SSS0.Px4.p2.2.m2.1.1" xref="S4.SS1.SSS0.Px4.p2.2.m2.1.1.cmml">
         <mn id="S4.SS1.SSS0.Px4.p2.2.m2.1.1.2" xref="S4.SS1.SSS0.Px4.p2.2.m2.1.1.2.cmml">
          8.3
         </mn>
         <mo id="S4.SS1.SSS0.Px4.p2.2.m2.1.1.1" xref="S4.SS1.SSS0.Px4.p2.2.m2.1.1.1.cmml">
          %
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px4.p2.2.m2.1b">
         <apply id="S4.SS1.SSS0.Px4.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px4.p2.2.m2.1.1">
          <csymbol cd="latexml" id="S4.SS1.SSS0.Px4.p2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px4.p2.2.m2.1.1.1">
           percent
          </csymbol>
          <cn id="S4.SS1.SSS0.Px4.p2.2.m2.1.1.2.cmml" type="float" xref="S4.SS1.SSS0.Px4.p2.2.m2.1.1.2">
           8.3
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px4.p2.2.m2.1c">
         8.3\%
        </annotation>
       </semantics>
      </math>
      by using the image modality for cross-modal validations with an oracle. Thanks to the external knowledge base, ReAct
      <cite class="ltx_cite ltx_citemacro_cite">
       Yao et al. (
       <a class="ltx_ref" href="#bib.bib198" title="">
        2022b
       </a>
       )
      </cite>
      reduces false positive rates from hallucination by
      <math alttext="8.0\%" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p2.3.m3.1">
       <semantics id="S4.SS1.SSS0.Px4.p2.3.m3.1a">
        <mrow id="S4.SS1.SSS0.Px4.p2.3.m3.1.1" xref="S4.SS1.SSS0.Px4.p2.3.m3.1.1.cmml">
         <mn id="S4.SS1.SSS0.Px4.p2.3.m3.1.1.2" xref="S4.SS1.SSS0.Px4.p2.3.m3.1.1.2.cmml">
          8.0
         </mn>
         <mo id="S4.SS1.SSS0.Px4.p2.3.m3.1.1.1" xref="S4.SS1.SSS0.Px4.p2.3.m3.1.1.1.cmml">
          %
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px4.p2.3.m3.1b">
         <apply id="S4.SS1.SSS0.Px4.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px4.p2.3.m3.1.1">
          <csymbol cd="latexml" id="S4.SS1.SSS0.Px4.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS0.Px4.p2.3.m3.1.1.1">
           percent
          </csymbol>
          <cn id="S4.SS1.SSS0.Px4.p2.3.m3.1.1.2.cmml" type="float" xref="S4.SS1.SSS0.Px4.p2.3.m3.1.1.2">
           8.0
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px4.p2.3.m3.1c">
         8.0\%
        </annotation>
       </semantics>
      </math>
      compared to CoT
      <cite class="ltx_cite ltx_citemacro_cite">
       Wei et al. (
       <a class="ltx_ref" href="#bib.bib179" title="">
        2022
       </a>
       )
      </cite>
      . For more examples, see Appendix
      <a class="ltx_ref" href="#A3" title="Appendix C Knowledge Retrieval &amp; Tool Usage ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
       <span class="ltx_text ltx_ref_tag">
        C
       </span>
      </a>
      .
     </p>
    </div>
    <div class="ltx_para" id="S4.SS1.SSS0.Px4.p3">
     <p class="ltx_p" id="S4.SS1.SSS0.Px4.p3.1">
      Enabling agents to use tools also expands their bounded working memories, akin to retrieving external knowledge. Toolformer
      <cite class="ltx_cite ltx_citemacro_cite">
       Schick et al. (
       <a class="ltx_ref" href="#bib.bib134" title="">
        2024
       </a>
       )
      </cite>
      opens a new era that allows LLMs to use external tools via API calls following predefined syntax, effectively extending their capabilities beyond their intrinsic limitations and enforcing consistent and predictable outputs.
A multi-agent system can coordinate agents understanding when and which tool to use, which modality of information the tool should expect, how to call the corresponding API, and how to incorporate outputs from the API calls, which anchors subsequent reasoning processes with more accurate information beyond their parametric memory. For example, VisProg
      <cite class="ltx_cite ltx_citemacro_cite">
       Gupta and Kembhavi (
       <a class="ltx_ref" href="#bib.bib49" title="">
        2023
       </a>
       )
      </cite>
      , ViperGPT
      <cite class="ltx_cite ltx_citemacro_cite">
       Surís et al. (
       <a class="ltx_ref" href="#bib.bib158" title="">
        2023
       </a>
       )
      </cite>
      , and Parsel
      <cite class="ltx_cite ltx_citemacro_cite">
       Zelikman et al. (
       <a class="ltx_ref" href="#bib.bib206" title="">
        2023
       </a>
       )
      </cite>
      generate Python programs to reliably execute subroutines.
      <cite class="ltx_cite ltx_citemacro_citet">
       Gupta and Kembhavi (
       <a class="ltx_ref" href="#bib.bib49" title="">
        2023
       </a>
       ); Surís et al. (
       <a class="ltx_ref" href="#bib.bib158" title="">
        2023
       </a>
       )
      </cite>
      also invoke off-the-shelf models for multimodal assistance. For more examples, see Appendix
      <a class="ltx_ref" href="#A3" title="Appendix C Knowledge Retrieval &amp; Tool Usage ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
       <span class="ltx_text ltx_ref_tag">
        C
       </span>
      </a>
      .
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Towards Rationality through Deliberation
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Memory is one of the most fundamental cognitive processes that lead to reasoning, creativity, learning, and even self-consciousness in humans
     <cite class="ltx_cite ltx_citemacro_cite">
      Solso and Kagan (
      <a class="ltx_ref" href="#bib.bib147" title="">
       1979
      </a>
      ); Craik and Lockhart (
      <a class="ltx_ref" href="#bib.bib24" title="">
       1972
      </a>
      ); Leydesdorff and Hodgkin (
      <a class="ltx_ref" href="#bib.bib76" title="">
       2017
      </a>
      ); Johnson-Laird (
      <a class="ltx_ref" href="#bib.bib65" title="">
       1983
      </a>
      ); Laird (
      <a class="ltx_ref" href="#bib.bib72" title="">
       2019
      </a>
      ); Sun (
      <a class="ltx_ref" href="#bib.bib154" title="">
       2001
      </a>
      )
     </cite>
     . Any system that lacks the ability to retain information from previous interactions would struggle to make coherent and rational decisions in the long run. In a narrow sense, agent memory includes historical information within the same conversation
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhang et al. (
      <a class="ltx_ref" href="#bib.bib209" title="">
       2024b
      </a>
      )
     </cite>
     . This allows for
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">
      deliberation
     </span>
     , which is the slower, iterative reasoning process to carefully consider information and options in order to arrive at more rational and well-reasoned decisions.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     Although deliberation may increase the likelihood of reaching more rational decisions, there is no inherent guarantee for rationality via this approach. The quality of the decision ultimately depends on the accuracy and relevance of the grounded information, as well as the soundness of the reasoning process. Biases, incomplete information, and flawed logic can still lead to irrational conclusions even with deliberation. Nonetheless, multiple works have shown that the increase in likelihood of rational decisions through deliberation is significant and beneficial. For example, multi-round self-reflection prompting strategies that encourage agents to critically evaluate their previous responses
     <cite class="ltx_cite ltx_citemacro_cite">
      Shinn et al. (
      <a class="ltx_ref" href="#bib.bib145" title="">
       2024
      </a>
      ); Madaan et al. (
      <a class="ltx_ref" href="#bib.bib105" title="">
       2024
      </a>
      ); Wang et al. (
      <a class="ltx_ref" href="#bib.bib174" title="">
       2022b
      </a>
      ); Zhong et al. (
      <a class="ltx_ref" href="#bib.bib218" title="">
       2024
      </a>
      ); Lu et al. (
      <a class="ltx_ref" href="#bib.bib100" title="">
       2023
      </a>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     In a broader context, for multi-agent systems, agent memory expands to include historical information across multiple tasks and agents
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhang et al. (
      <a class="ltx_ref" href="#bib.bib209" title="">
       2024b
      </a>
      )
     </cite>
     . This shared memory enables collective deliberation among agents via collaboration, cross-examination, and debates. By leveraging the collective knowledge and experiences of multiple agents, the system can arrive at more comprehensive and robust solutions to complex problems.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.10">
     LM vs LM
     <cite class="ltx_cite ltx_citemacro_cite">
      Cohen et al. (
      <a class="ltx_ref" href="#bib.bib23" title="">
       2023
      </a>
      )
     </cite>
     , FORD
     <cite class="ltx_cite ltx_citemacro_cite">
      Xiong et al. (
      <a class="ltx_ref" href="#bib.bib188" title="">
       2023
      </a>
      )
     </cite>
     , Multi-Agent Debate
     <cite class="ltx_cite ltx_citemacro_cite">
      Liang et al. (
      <a class="ltx_ref" href="#bib.bib88" title="">
       2023
      </a>
      ); Du et al. (
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023
      </a>
      )
     </cite>
     , DyLAN
     <cite class="ltx_cite ltx_citemacro_cite">
      Liu et al. (
      <a class="ltx_ref" href="#bib.bib98" title="">
       2023c
      </a>
      )
     </cite>
     , and
     <cite class="ltx_cite ltx_citemacro_citet">
      Khan et al. (
      <a class="ltx_ref" href="#bib.bib68" title="">
       2024
      </a>
      )
     </cite>
     highlight the profound impact of
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p4.10.1">
      multi-agent collaboration through cross-examination and debates
     </span>
     .
Specifically, LM vs LM
     <cite class="ltx_cite ltx_citemacro_cite">
      Cohen et al. (
      <a class="ltx_ref" href="#bib.bib23" title="">
       2023
      </a>
      )
     </cite>
     illustrates how its multi-agent framework improves F1 scores by an average of
     <math alttext="15.7\%" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1">
      <semantics id="S4.SS2.p4.1.m1.1a">
       <mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">
        <mn id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml">
         15.7
        </mn>
        <mo id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b">
        <apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">
         <csymbol cd="latexml" id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1">
          percent
         </csymbol>
         <cn id="S4.SS2.p4.1.m1.1.1.2.cmml" type="float" xref="S4.SS2.p4.1.m1.1.1.2">
          15.7
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">
        15.7\%
       </annotation>
      </semantics>
     </math>
     compared to the single-agent baseline
     <cite class="ltx_cite ltx_citemacro_cite">
      Yoshikawa and Okazaki (
      <a class="ltx_ref" href="#bib.bib205" title="">
       2023
      </a>
      )
     </cite>
     . FORD
     <cite class="ltx_cite ltx_citemacro_cite">
      Xiong et al. (
      <a class="ltx_ref" href="#bib.bib188" title="">
       2023
      </a>
      )
     </cite>
     reports an accuracy increase up to
     <math alttext="4.9\%" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2.1">
      <semantics id="S4.SS2.p4.2.m2.1a">
       <mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">
        <mn id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">
         4.9
        </mn>
        <mo id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b">
        <apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">
         <csymbol cd="latexml" id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1">
          percent
         </csymbol>
         <cn id="S4.SS2.p4.2.m2.1.1.2.cmml" type="float" xref="S4.SS2.p4.2.m2.1.1.2">
          4.9
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">
        4.9\%
       </annotation>
      </semantics>
     </math>
     compared to a single LLM.
     <cite class="ltx_cite ltx_citemacro_citet">
      Liang et al. (
      <a class="ltx_ref" href="#bib.bib88" title="">
       2023
      </a>
      )
     </cite>
     indicates significant improvements in accuracy —
     <math alttext="17.0\%" class="ltx_Math" display="inline" id="S4.SS2.p4.3.m3.1">
      <semantics id="S4.SS2.p4.3.m3.1a">
       <mrow id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml">
        <mn id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml">
         17.0
        </mn>
        <mo id="S4.SS2.p4.3.m3.1.1.1" xref="S4.SS2.p4.3.m3.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b">
        <apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">
         <csymbol cd="latexml" id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1.1">
          percent
         </csymbol>
         <cn id="S4.SS2.p4.3.m3.1.1.2.cmml" type="float" xref="S4.SS2.p4.3.m3.1.1.2">
          17.0
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">
        17.0\%
       </annotation>
      </semantics>
     </math>
     for translation tasks and
     <math alttext="16.0\%" class="ltx_Math" display="inline" id="S4.SS2.p4.4.m4.1">
      <semantics id="S4.SS2.p4.4.m4.1a">
       <mrow id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml">
        <mn id="S4.SS2.p4.4.m4.1.1.2" xref="S4.SS2.p4.4.m4.1.1.2.cmml">
         16.0
        </mn>
        <mo id="S4.SS2.p4.4.m4.1.1.1" xref="S4.SS2.p4.4.m4.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b">
        <apply id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1">
         <csymbol cd="latexml" id="S4.SS2.p4.4.m4.1.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1.1">
          percent
         </csymbol>
         <cn id="S4.SS2.p4.4.m4.1.1.2.cmml" type="float" xref="S4.SS2.p4.4.m4.1.1.2">
          16.0
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">
        16.0\%
       </annotation>
      </semantics>
     </math>
     for reasoning tasks — by employing a multi-agent strategy, effectively bridging the performance gap between GPT-3.5 and GPT-4 by harnessing multi-agents.
     <cite class="ltx_cite ltx_citemacro_citet">
      Du et al. (
      <a class="ltx_ref" href="#bib.bib30" title="">
       2023
      </a>
      )
     </cite>
     finds that multi-agent debates not only enhance reasoning performance by
     <math alttext="8.0" class="ltx_Math" display="inline" id="S4.SS2.p4.5.m5.1">
      <semantics id="S4.SS2.p4.5.m5.1a">
       <mn id="S4.SS2.p4.5.m5.1.1" xref="S4.SS2.p4.5.m5.1.1.cmml">
        8.0
       </mn>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.5.m5.1b">
        <cn id="S4.SS2.p4.5.m5.1.1.cmml" type="float" xref="S4.SS2.p4.5.m5.1.1">
         8.0
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.5.m5.1c">
        8.0
       </annotation>
      </semantics>
     </math>
     -
     <math alttext="14.8\%" class="ltx_Math" display="inline" id="S4.SS2.p4.6.m6.1">
      <semantics id="S4.SS2.p4.6.m6.1a">
       <mrow id="S4.SS2.p4.6.m6.1.1" xref="S4.SS2.p4.6.m6.1.1.cmml">
        <mn id="S4.SS2.p4.6.m6.1.1.2" xref="S4.SS2.p4.6.m6.1.1.2.cmml">
         14.8
        </mn>
        <mo id="S4.SS2.p4.6.m6.1.1.1" xref="S4.SS2.p4.6.m6.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.6.m6.1b">
        <apply id="S4.SS2.p4.6.m6.1.1.cmml" xref="S4.SS2.p4.6.m6.1.1">
         <csymbol cd="latexml" id="S4.SS2.p4.6.m6.1.1.1.cmml" xref="S4.SS2.p4.6.m6.1.1.1">
          percent
         </csymbol>
         <cn id="S4.SS2.p4.6.m6.1.1.2.cmml" type="float" xref="S4.SS2.p4.6.m6.1.1.2">
          14.8
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.6.m6.1c">
        14.8\%
       </annotation>
      </semantics>
     </math>
     , but more importantly, increase factual accuracy by
     <math alttext="7.2" class="ltx_Math" display="inline" id="S4.SS2.p4.7.m7.1">
      <semantics id="S4.SS2.p4.7.m7.1a">
       <mn id="S4.SS2.p4.7.m7.1.1" xref="S4.SS2.p4.7.m7.1.1.cmml">
        7.2
       </mn>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.7.m7.1b">
        <cn id="S4.SS2.p4.7.m7.1.1.cmml" type="float" xref="S4.SS2.p4.7.m7.1.1">
         7.2
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.7.m7.1c">
        7.2
       </annotation>
      </semantics>
     </math>
     -
     <math alttext="15.9\%" class="ltx_Math" display="inline" id="S4.SS2.p4.8.m8.1">
      <semantics id="S4.SS2.p4.8.m8.1a">
       <mrow id="S4.SS2.p4.8.m8.1.1" xref="S4.SS2.p4.8.m8.1.1.cmml">
        <mn id="S4.SS2.p4.8.m8.1.1.2" xref="S4.SS2.p4.8.m8.1.1.2.cmml">
         15.9
        </mn>
        <mo id="S4.SS2.p4.8.m8.1.1.1" xref="S4.SS2.p4.8.m8.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.8.m8.1b">
        <apply id="S4.SS2.p4.8.m8.1.1.cmml" xref="S4.SS2.p4.8.m8.1.1">
         <csymbol cd="latexml" id="S4.SS2.p4.8.m8.1.1.1.cmml" xref="S4.SS2.p4.8.m8.1.1.1">
          percent
         </csymbol>
         <cn id="S4.SS2.p4.8.m8.1.1.2.cmml" type="float" xref="S4.SS2.p4.8.m8.1.1.2">
          15.9
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.8.m8.1c">
        15.9\%
       </annotation>
      </semantics>
     </math>
     .
DyLAN
     <cite class="ltx_cite ltx_citemacro_cite">
      Liu et al. (
      <a class="ltx_ref" href="#bib.bib98" title="">
       2023c
      </a>
      )
     </cite>
     observes
     <math alttext="3.5" class="ltx_Math" display="inline" id="S4.SS2.p4.9.m9.1">
      <semantics id="S4.SS2.p4.9.m9.1a">
       <mn id="S4.SS2.p4.9.m9.1.1" xref="S4.SS2.p4.9.m9.1.1.cmml">
        3.5
       </mn>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.9.m9.1b">
        <cn id="S4.SS2.p4.9.m9.1.1.cmml" type="float" xref="S4.SS2.p4.9.m9.1.1">
         3.5
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.9.m9.1c">
        3.5
       </annotation>
      </semantics>
     </math>
     -
     <math alttext="4.1\%" class="ltx_Math" display="inline" id="S4.SS2.p4.10.m10.1">
      <semantics id="S4.SS2.p4.10.m10.1a">
       <mrow id="S4.SS2.p4.10.m10.1.1" xref="S4.SS2.p4.10.m10.1.1.cmml">
        <mn id="S4.SS2.p4.10.m10.1.1.2" xref="S4.SS2.p4.10.m10.1.1.2.cmml">
         4.1
        </mn>
        <mo id="S4.SS2.p4.10.m10.1.1.1" xref="S4.SS2.p4.10.m10.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS2.p4.10.m10.1b">
        <apply id="S4.SS2.p4.10.m10.1.1.cmml" xref="S4.SS2.p4.10.m10.1.1">
         <csymbol cd="latexml" id="S4.SS2.p4.10.m10.1.1.1.cmml" xref="S4.SS2.p4.10.m10.1.1.1">
          percent
         </csymbol>
         <cn id="S4.SS2.p4.10.m10.1.1.2.cmml" type="float" xref="S4.SS2.p4.10.m10.1.1.2">
          4.1
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS2.p4.10.m10.1c">
        4.1\%
       </annotation>
      </semantics>
     </math>
     in accuracy improvements over single-agent execution.
All these approaches enhance the system’s capability to capture initial errors, improve factuality in reasoning, and achieve final consensus with fewer inconsistencies. We discuss more examples in Appendix
     <a class="ltx_ref" href="#A4.SS1" title="D.1 More Examples on Multi-Agent Collaborations ‣ Appendix D Collective Deliberation among Agents ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       D.1
      </span>
     </a>
     . We also talk about collaboration against jailbreaking in Appendix
     <a class="ltx_ref" href="#A4.SS2" title="D.2 Collaboration Againt Jailbreaking ‣ Appendix D Collective Deliberation among Agents ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       D.2
      </span>
     </a>
     and multi-agent evaluation methods in Appendix
     <a class="ltx_ref" href="#A4.SS3" title="D.3 Collaboration on LLM-based Evaluation ‣ Appendix D Collective Deliberation among Agents ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       D.3
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Towards Rationality through Abstraction
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     Independence from irrelevant contexts, invariance, and orderability of preferences can be achieved simultaneously through the use of tools and
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">
      neuro-symbolic reasoning
     </span>
     , because these approaches translate natural language queries into standardized formats like API calls or symbolic representations, which
     <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">
      abstract
     </span>
     away extraneous details, focus only on the underlying logic necessary for the task at hand, and enable consistent and deterministic processing of the input.
    </p>
   </div>
   <section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Independence from Irrelevant Contexts
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
     <p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">
      In most cases, tools require translating natural language queries into API calls with predefined syntax. Once the APIs and their input arguments are determined, the tools will ignore any irrelevant context in the original queries, as long as the queries share the same underlying logic necessary for the inputs. Take Multi-Agent VQA
      <cite class="ltx_cite ltx_citemacro_cite">
       Jiang et al. (
       <a class="ltx_ref" href="#bib.bib64" title="">
        2024b
       </a>
       )
      </cite>
      as an example. In this system, a language model provides only the relevant object names to the Grounded SAM
      <cite class="ltx_cite ltx_citemacro_cite">
       Ren et al. (
       <a class="ltx_ref" href="#bib.bib128" title="">
        2024
       </a>
       )
      </cite>
      component, which functions as an object detector, rather than passing the entire visual question. Other similar examples are discussed in Appendix
      <a class="ltx_ref" href="#A3" title="Appendix C Knowledge Retrieval &amp; Tool Usage ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
       <span class="ltx_text ltx_ref_tag">
        C
       </span>
      </a>
      .
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS0.Px1.p2">
     <p class="ltx_p" id="S4.SS3.SSS0.Px1.p2.1">
      Neuro-symbolic reasoning is an approach that combines neural networks with symbolic systems, such as explicit knowledge representation and logical reasoning. A multi-agent system incorporating symbolic modules can not only understand language queries but also solve them with a level of consistency, providing a faithful and transparent reasoning process based on well-defined rules that adhere to logical principles, which is unachievable by a single language model.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS0.Px1.p3">
     <p class="ltx_p" id="S4.SS3.SSS0.Px1.p3.2">
      Analogous to the external tool utilization, neuro-symbolic modules in a multi-agent system expect standardized input formats
      <cite class="ltx_cite ltx_citemacro_cite">
       Zelikman et al. (
       <a class="ltx_ref" href="#bib.bib206" title="">
        2023
       </a>
       ); Pan et al. (
       <a class="ltx_ref" href="#bib.bib116" title="">
        2023
       </a>
       ); Sclar et al. (
       <a class="ltx_ref" href="#bib.bib136" title="">
        2023b
       </a>
       ); Hsu et al. (
       <a class="ltx_ref" href="#bib.bib58" title="">
        2024
       </a>
       ); Fang et al. (
       <a class="ltx_ref" href="#bib.bib37" title="">
        2024
       </a>
       ); Yang et al. (
       <a class="ltx_ref" href="#bib.bib194" title="">
        2024
       </a>
       ); Subramanian et al. (
       <a class="ltx_ref" href="#bib.bib152" title="">
        2024
       </a>
       )
      </cite>
      . The only relevant factor in this process is the parsed inputs into the predetermined neuro-symbolic programs. For instance, Ada
      <cite class="ltx_cite ltx_citemacro_cite">
       Wong et al. (
       <a class="ltx_ref" href="#bib.bib182" title="">
        2023
       </a>
       )
      </cite>
      introduces symbolic operators to abstract actions, ensuring that lower-level planning models are not compromised by irrelevant information in the queries and observations. Without the symbolic action library, a single LLM would frequently fail at grounding objects or obeying environmental conditions, resulting in a significant accuracy gap of approximately
      <math alttext="59.0" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p3.1.m1.1">
       <semantics id="S4.SS3.SSS0.Px1.p3.1.m1.1a">
        <mn id="S4.SS3.SSS0.Px1.p3.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p3.1.m1.1.1.cmml">
         59.0
        </mn>
        <annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p3.1.m1.1b">
         <cn id="S4.SS3.SSS0.Px1.p3.1.m1.1.1.cmml" type="float" xref="S4.SS3.SSS0.Px1.p3.1.m1.1.1">
          59.0
         </cn>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p3.1.m1.1c">
         59.0
        </annotation>
       </semantics>
      </math>
      -
      <math alttext="89.0\%" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p3.2.m2.1">
       <semantics id="S4.SS3.SSS0.Px1.p3.2.m2.1a">
        <mrow id="S4.SS3.SSS0.Px1.p3.2.m2.1.1" xref="S4.SS3.SSS0.Px1.p3.2.m2.1.1.cmml">
         <mn id="S4.SS3.SSS0.Px1.p3.2.m2.1.1.2" xref="S4.SS3.SSS0.Px1.p3.2.m2.1.1.2.cmml">
          89.0
         </mn>
         <mo id="S4.SS3.SSS0.Px1.p3.2.m2.1.1.1" xref="S4.SS3.SSS0.Px1.p3.2.m2.1.1.1.cmml">
          %
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p3.2.m2.1b">
         <apply id="S4.SS3.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px1.p3.2.m2.1.1">
          <csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p3.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p3.2.m2.1.1.1">
           percent
          </csymbol>
          <cn id="S4.SS3.SSS0.Px1.p3.2.m2.1.1.2.cmml" type="float" xref="S4.SS3.SSS0.Px1.p3.2.m2.1.1.2">
           89.0
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p3.2.m2.1c">
         89.0\%
        </annotation>
       </semantics>
      </math>
      .
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Invariance
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
     <p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">
      The abstraction provided by symbolic representations also allows the multi-agent system to solve language queries with invariance. For example,
Logic-LM
      <cite class="ltx_cite ltx_citemacro_cite">
       Pan et al. (
       <a class="ltx_ref" href="#bib.bib116" title="">
        2023
       </a>
       )
      </cite>
      combines problem formulating, symbolic reasoning, and result interpreting agents, where the symbolic reasoner empowers LLMs with deterministic symbolic solvers to perform inference, ensuring a correct answer is consistently chosen. Its multi-agent framework also encourages self-refinement that modifies logical formulation errors using error messages from the symbolic reasoner as the feedback. For more examples, see Appendix
      <a class="ltx_ref" href="#A5" title="Appendix E Neuro-Symbolic Reasoning ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
       <span class="ltx_text ltx_ref_tag">
        E
       </span>
      </a>
      .
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S4.SS3.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Orderability of Preferences
    </h4>
    <div class="ltx_para" id="S4.SS3.SSS0.Px3.p1">
     <p class="ltx_p" id="S4.SS3.SSS0.Px3.p1.2">
      In explicit scenarios, logical modules can directly compare the order of options represented as variables—such as “left" or “right" in relational logic
      <cite class="ltx_cite ltx_citemacro_cite">
       Hsu et al. (
       <a class="ltx_ref" href="#bib.bib58" title="">
        2024
       </a>
       )
      </cite>
      —rather than relying on a single LLM to generate responses indeterministically within the natural language space. In more complex situations, systems like Binder
      <cite class="ltx_cite ltx_citemacro_cite">
       Cheng et al. (
       <a class="ltx_ref" href="#bib.bib18" title="">
        2022
       </a>
       )
      </cite>
      , Parsel
      <cite class="ltx_cite ltx_citemacro_cite">
       Zelikman et al. (
       <a class="ltx_ref" href="#bib.bib206" title="">
        2023
       </a>
       )
      </cite>
      , LEFT
      <cite class="ltx_cite ltx_citemacro_cite">
       Hsu et al. (
       <a class="ltx_ref" href="#bib.bib58" title="">
        2024
       </a>
       )
      </cite>
      , and
      <cite class="ltx_cite ltx_citemacro_citet">
       Fang et al. (
       <a class="ltx_ref" href="#bib.bib37" title="">
        2024
       </a>
       )
      </cite>
      decompose tasks into planning, parsing, and execution, where the symbolic reasoning agents can help maintain a coherent order of preferences among symbolic options in the system outputs. By skipping the symbolic module, Parsel
      <cite class="ltx_cite ltx_citemacro_cite">
       Zelikman et al. (
       <a class="ltx_ref" href="#bib.bib206" title="">
        2023
       </a>
       )
      </cite>
      observes a substantial performance drop of
      <math alttext="19.5\%" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.1.m1.1">
       <semantics id="S4.SS3.SSS0.Px3.p1.1.m1.1a">
        <mrow id="S4.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px3.p1.1.m1.1.1.cmml">
         <mn id="S4.SS3.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml">
          19.5
         </mn>
         <mo id="S4.SS3.SSS0.Px3.p1.1.m1.1.1.1" xref="S4.SS3.SSS0.Px3.p1.1.m1.1.1.1.cmml">
          %
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px3.p1.1.m1.1b">
         <apply id="S4.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px3.p1.1.m1.1.1">
          <csymbol cd="latexml" id="S4.SS3.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px3.p1.1.m1.1.1.1">
           percent
          </csymbol>
          <cn id="S4.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS3.SSS0.Px3.p1.1.m1.1.1.2">
           19.5
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px3.p1.1.m1.1c">
         19.5\%
        </annotation>
       </semantics>
      </math>
      . LEFT
      <cite class="ltx_cite ltx_citemacro_cite">
       Hsu et al. (
       <a class="ltx_ref" href="#bib.bib58" title="">
        2024
       </a>
       )
      </cite>
      also outperforms end-to-end baselines without symbolic programs by
      <math alttext="3.85\%" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.2.m2.1">
       <semantics id="S4.SS3.SSS0.Px3.p1.2.m2.1a">
        <mrow id="S4.SS3.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS3.SSS0.Px3.p1.2.m2.1.1.cmml">
         <mn id="S4.SS3.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS3.SSS0.Px3.p1.2.m2.1.1.2.cmml">
          3.85
         </mn>
         <mo id="S4.SS3.SSS0.Px3.p1.2.m2.1.1.1" xref="S4.SS3.SSS0.Px3.p1.2.m2.1.1.1.cmml">
          %
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px3.p1.2.m2.1b">
         <apply id="S4.SS3.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px3.p1.2.m2.1.1">
          <csymbol cd="latexml" id="S4.SS3.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px3.p1.2.m2.1.1.1">
           percent
          </csymbol>
          <cn id="S4.SS3.SSS0.Px3.p1.2.m2.1.1.2.cmml" type="float" xref="S4.SS3.SSS0.Px3.p1.2.m2.1.1.2">
           3.85
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px3.p1.2.m2.1c">
         3.85\%
        </annotation>
       </semantics>
      </math>
      on average across multiple experiments.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS3.SSS0.Px3.p2">
     <p class="ltx_p" id="S4.SS3.SSS0.Px3.p2.1">
      Recent work has also explored applying expected utility theory
      <cite class="ltx_cite ltx_citemacro_cite">
       Von Neumann and Morgenstern (
       <a class="ltx_ref" href="#bib.bib166" title="">
        2007
       </a>
       )
      </cite>
      to improve the decision-making capabilities of language models. For example, DeLLMa
      <cite class="ltx_cite ltx_citemacro_cite">
       Liu et al. (
       <a class="ltx_ref" href="#bib.bib96" title="">
        2024e
       </a>
       )
      </cite>
      decomposes complex decision problems into subtasks, assigns utilities to different outcomes, and selects actions that maximize expected utility.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Evaluating Rationality of Agents
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    The amount of studies for testing rationality in multi-modal and multi-agent systems remains scant, despite the growing interest in the field. While there are numerous reasoning benchmarks available
    <cite class="ltx_cite ltx_citemacro_citep">
     (Talmor et al.,
     <a class="ltx_ref" href="#bib.bib160" title="">
      2019
     </a>
     ; Liu et al.,
     <a class="ltx_ref" href="#bib.bib94" title="">
      2021
     </a>
     ; Yang et al.,
     <a class="ltx_ref" href="#bib.bib193" title="">
      2018
     </a>
     ; Hendrycks et al.,
     <a class="ltx_ref" href="#bib.bib54" title="">
      2021
     </a>
     )
    </cite>
    , they do not directly measure rationality. Many of these benchmarks fail to prove whether reasoning is actually used in solving the tasks, leaving no guarantee that these tasks will be solved consistently when generalized to other representations or domains. Issues such as data contamination
    <cite class="ltx_cite ltx_citemacro_cite">
     Magar and Schwartz (
     <a class="ltx_ref" href="#bib.bib106" title="">
      2022
     </a>
     ); Dong et al. (
     <a class="ltx_ref" href="#bib.bib28" title="">
      2024
     </a>
     ); Sainz et al. (
     <a class="ltx_ref" href="#bib.bib132" title="">
      2023
     </a>
     ); Jacovi et al. (
     <a class="ltx_ref" href="#bib.bib62" title="">
      2023
     </a>
     )
    </cite>
    further compound the problem, as some benchmarks may inadvertently include the training data of these LLMs, leading to inflated performance scores. Hence, even though solid reasoning will imply rationality, existing approaches fall short in making the logic click. In this section, we point to several existing ingredients that can constitute the bread-and-butter of future generations of evaluation approaches for rationality.
   </p>
  </div>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Adapting Cognitive Psychology Experiments
   </h4>
   <div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">
     Recent works propose adapting vignette-based experiments borrowed from cognitive psychology to test whether LLMs are susceptible to cognitive biases and fallacies. For instance,
     <cite class="ltx_cite ltx_citemacro_citet">
      Binz and Schulz (
      <a class="ltx_ref" href="#bib.bib9" title="">
       2023
      </a>
      )
     </cite>
     tested GPT-3 on the conjunction fallacy, finding that they exhibit human-like biases. However, many of these approaches are informal and subjective, failing to scale in a way that allows for drawing statistically significant conclusions. Moreover, LLMs may be subject to cognitive biases not existent in humans, such as the hypothetical "algorithmic bias" proposed by
     <cite class="ltx_cite ltx_citemacro_citet">
      Bender et al. (
      <a class="ltx_ref" href="#bib.bib7" title="">
       2021
      </a>
      )
     </cite>
     , which could lead to unintended consequences in decision-making tasks. Further research is needed to uncover and characterize these potential biases.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Testing against Hallucination
   </h4>
   <div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">
     Information grounding is usually evaluated by the level of hallucination
     <cite class="ltx_cite ltx_citemacro_cite">
      Bang et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      ); Guerreiro et al. (
      <a class="ltx_ref" href="#bib.bib46" title="">
       2023
      </a>
      ); Huang et al. (
      <a class="ltx_ref" href="#bib.bib61" title="">
       2023
      </a>
      )
     </cite>
     .
Multiple evaluation benchmarks targeting language-only dialogue have been proposed, such as BEGIN
     <cite class="ltx_cite ltx_citemacro_cite">
      Dziri et al. (
      <a class="ltx_ref" href="#bib.bib33" title="">
       2022b
      </a>
      )
     </cite>
     , HaluEval
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib83" title="">
       2023e
      </a>
      )
     </cite>
     , DialFact
     <cite class="ltx_cite ltx_citemacro_cite">
      Gupta et al. (
      <a class="ltx_ref" href="#bib.bib48" title="">
       2021
      </a>
      )
     </cite>
     , FaithDial
     <cite class="ltx_cite ltx_citemacro_cite">
      Dziri et al. (
      <a class="ltx_ref" href="#bib.bib32" title="">
       2022a
      </a>
      )
     </cite>
     , AIS
     <cite class="ltx_cite ltx_citemacro_cite">
      Rashkin et al. (
      <a class="ltx_ref" href="#bib.bib126" title="">
       2023
      </a>
      )
     </cite>
     , and others
     <cite class="ltx_cite ltx_citemacro_cite">
      Zheng et al. (
      <a class="ltx_ref" href="#bib.bib216" title="">
       2023b
      </a>
      ); Das et al. (
      <a class="ltx_ref" href="#bib.bib26" title="">
       2023
      </a>
      ); Cao et al. (
      <a class="ltx_ref" href="#bib.bib13" title="">
       2021
      </a>
      )
     </cite>
     . In contrast,
     <span class="ltx_text ltx_font_bold" id="S5.SS0.SSS0.Px2.p1.1.1">
      benchmarks on multi-agent frameworks or those involving multi-modalities beyond language dialogue are very limited.
     </span>
     We find that
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu et al. (
      <a class="ltx_ref" href="#bib.bib90" title="">
       2024a
      </a>
      )
     </cite>
     moves beyond conversation to code generation, EureQA
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib77" title="">
       2023a
      </a>
      )
     </cite>
     focuses on reasoning chains, and TofuEval
     <cite class="ltx_cite ltx_citemacro_cite">
      Tang et al. (
      <a class="ltx_ref" href="#bib.bib161" title="">
       2024
      </a>
      )
     </cite>
     evaluates hallucination in multi-domain summarization. Object hallucination
     <cite class="ltx_cite ltx_citemacro_cite">
      Rohrbach et al. (
      <a class="ltx_ref" href="#bib.bib130" title="">
       2018
      </a>
      ); Biten et al. (
      <a class="ltx_ref" href="#bib.bib10" title="">
       2022
      </a>
      )
     </cite>
     , POPE
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib85" title="">
       2023g
      </a>
      )
     </cite>
     , and LLaVA-RLHF
     <cite class="ltx_cite ltx_citemacro_cite">
      Sun et al. (
      <a class="ltx_ref" href="#bib.bib156" title="">
       2023b
      </a>
      )
     </cite>
     are the few examples evaluating multi-modal hallucination.
The community needs more hallucination benchmarks to quantitatively evaluate the extent to which multi-modal and multi-agents reduce hallucinations in comparison with baselines.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Testing the Orderability of Preference
   </h4>
   <div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">
     There are almost no benchmarks for evaluating whether LLMs or agents have a consistent preference in the selection of available options. The Multiple Choice Problem (MCP) serves as a common testing ground.
     <cite class="ltx_cite ltx_citemacro_citet">
      Zheng et al. (
      <a class="ltx_ref" href="#bib.bib214" title="">
       2023a
      </a>
      )
     </cite>
     shows that LLMs are susceptible to changes in the positioning of options. Since the underlying logic remains the same, it also makes LLMs fail to pass the property of invariance. Although there are many MCP benchmarks
     <cite class="ltx_cite ltx_citemacro_cite">
      <a class="ltx_ref" href="#bib.bib118" title="">
       PaperswithcodeMCQA
      </a>
     </cite>
     , they focus on the accuracy of selections and overlook the consistency of preference. However,
     <cite class="ltx_cite ltx_citemacro_citet">
      Robinson et al. (
      <a class="ltx_ref" href="#bib.bib129" title="">
       2023
      </a>
      )
     </cite>
     highlights that the Proportion of Plurality Agreement (PPA) offers a measure of order invariance that does not depend on the model’s ability to perform a task, suggesting a promising direction.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
   <h4 class="ltx_title ltx_title_paragraph">
    Testing the Principle of Invariance
   </h4>
   <div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">
     Recent studies concerning data contamination investigate whether LLMs can generate consistent responses across different, yet inherently equivalent, framing of the same task.
These studies introduce perturbations to the original task descriptions to assess whether LLMs’ responses will change significantly. Perturbation techniques include modifying instruction templates
     <cite class="ltx_cite ltx_citemacro_cite">
      Weber et al. (
      <a class="ltx_ref" href="#bib.bib176" title="">
       2023
      </a>
      )
     </cite>
     , paraphrasing task descriptions
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yang et al.,
      <a class="ltx_ref" href="#bib.bib192" title="">
       2023
      </a>
      ; Ohmer et al.,
      <a class="ltx_ref" href="#bib.bib113" title="">
       2024
      </a>
      )
     </cite>
     , or altering the order of in-context learning exemplars
     <cite class="ltx_cite ltx_citemacro_cite">
      Lu et al. (
      <a class="ltx_ref" href="#bib.bib102" title="">
       2021
      </a>
      ); Pecher et al. (
      <a class="ltx_ref" href="#bib.bib119" title="">
       2024
      </a>
      )
     </cite>
     . For more details on these techniques, we refer the reader to Appendix
     <a class="ltx_ref" href="#A6.SS2" title="F.2 Perturbation Techniques ‣ Appendix F Evaluating Rationality ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       F.2
      </span>
     </a>
     .
It is crucial to recognize that these perturbations are superficial: the altered task descriptions remain syntactically and semantically equivalent to their originals, although linguistic expressions or narratives may vary substantially. Methods that go beyond surface-level perturbations are needed to evaluate the robustness and invariance of LLMs across diverse problem framings and modalities effectively.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
   <h4 class="ltx_title ltx_title_paragraph">
    Testing Independence from Irrelevant Context
   </h4>
   <div class="ltx_para" id="S5.SS0.SSS0.Px5.p1">
    <p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.1">
     Studies such as
     <cite class="ltx_cite ltx_citemacro_citet">
      Shi et al. (
      <a class="ltx_ref" href="#bib.bib142" title="">
       2023
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_citet">
      Wu et al. (
      <a class="ltx_ref" href="#bib.bib185" title="">
       2024
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu et al. (
      <a class="ltx_ref" href="#bib.bib95" title="">
       2024d
      </a>
      )
     </cite>
     , and
     <cite class="ltx_cite ltx_citemacro_citet">
      Yoran et al. (
      <a class="ltx_ref" href="#bib.bib204" title="">
       2023
      </a>
      )
     </cite>
     have explored the phenomenon of “lost-in-context" by introducing random or misleading sentences into original problem statements. While earlier benchmarks like those from
     <cite class="ltx_cite ltx_citemacro_citet">
      Weston et al. (
      <a class="ltx_ref" href="#bib.bib180" title="">
       2015
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_citet">
      Sinha et al. (
      <a class="ltx_ref" href="#bib.bib146" title="">
       2019
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_citet">
      Clark et al. (
      <a class="ltx_ref" href="#bib.bib22" title="">
       2020
      </a>
      )
     </cite>
     , and
     <cite class="ltx_cite ltx_citemacro_citet">
      Webson and Pavlick (
      <a class="ltx_ref" href="#bib.bib177" title="">
       2021
      </a>
      )
     </cite>
     have included irrelevant content, they have been predominantly limited to language modalities and single-agent systems. Recent benchmarks such as MileBench
     <cite class="ltx_cite ltx_citemacro_cite">
      Song et al. (
      <a class="ltx_ref" href="#bib.bib148" title="">
       2024
      </a>
      )
     </cite>
     , Mementos
     <cite class="ltx_cite ltx_citemacro_cite">
      Wang et al. (
      <a class="ltx_ref" href="#bib.bib173" title="">
       2024c
      </a>
      )
     </cite>
     , Seed-bench-2
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib78" title="">
       2023b
      </a>
      )
     </cite>
     , and DEMON
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib80" title="">
       2023c
      </a>
      )
     </cite>
     begin to evaluate multi-modal agents in long context or image sequences, where accurately responding to a specific question requires isolating only the relevant information from the long context window.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Open Problems and Future Directions
  </h2>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Inherent Rationality
   </h4>
   <div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">
     It is important to understand that the notion of multi-modal or multi-agent systems does not
     <span class="ltx_text ltx_font_italic" id="S6.SS0.SSS0.Px1.p1.1.1">
      inherently
     </span>
     imply rationality.
     <span class="ltx_text ltx_font_bold" id="S6.SS0.SSS0.Px1.p1.1.2">
      Current methods are neither sufficient nor necessary, but they serve as instrumental tools that bridge the gap between an LLM’s response and rationality.
     </span>
     These approaches enable multi-agent systems, which are black boxes from the user’s perspective, to more closely mimic rational thinking in their output responses. However, despite these more rational responses elicited from multi-modal and multi-agent systems, the challenge of how to effectively close the loop and bake these enhanced outputs back into foundation models themselves
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhao et al. (
      <a class="ltx_ref" href="#bib.bib212" title="">
       2024
      </a>
      )
     </cite>
     beyond mere fine-tuning remains an open question. In other words, can we leverage these more rational outputs to inherently enhance a single foundation model’s rationality in its initial responses in future applications?
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    More Comprehensive Evaluation on Rationality
   </h4>
   <div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">
     Section
     <a class="ltx_ref" href="#S4" title="4 Towards Rationality through Multi-Modal and Multi-Agent Systems ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     thoroughly compares multi-modal and multi-agent systems over their LLM-based single-agent baselines. However, the choices of evaluation metrics are important
     <cite class="ltx_cite ltx_citemacro_cite">
      Schaeffer et al. (
      <a class="ltx_ref" href="#bib.bib133" title="">
       2024
      </a>
      )
     </cite>
     ; these examples predominantly focus on the accuracy of their final performance, ignoring the most interesting intermediate reasoning steps and the concept of rationality. Section
     <a class="ltx_ref" href="#S5" title="5 Evaluating Rationality of Agents ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     furthermore acknowledges that while there have been some efforts to assess the rationality of agent systems, the field still lacks comprehensive and rigorous evaluation metrics. Moreover,
     <span class="ltx_text ltx_font_bold" id="S6.SS0.SSS0.Px2.p1.1.1">
      most existing benchmarks on rationality provide limited comparisons between multi-agent frameworks and single-agent baselines
     </span>
     , thus failing to fully elucidate the advantages multi-agent frameworks can offer.
    </p>
   </div>
   <div class="ltx_para" id="S6.SS0.SSS0.Px2.p2">
    <p class="ltx_p" id="S6.SS0.SSS0.Px2.p2.1">
     Future research should prioritize the development of more robust and scalable methods for evaluating rationality, taking into account unique challenges and biases
     <cite class="ltx_cite ltx_citemacro_cite">
      Jiang et al. (
      <a class="ltx_ref" href="#bib.bib63" title="">
       2024a
      </a>
      )
     </cite>
     posed by agents.
     <span class="ltx_text ltx_font_bold" id="S6.SS0.SSS0.Px2.p2.1.1">
      A promising direction is to create methods specifically tailored to assess rationality, going beyond existing ones on accuracy.
     </span>
     These new methods should avoid data contamination and emphasize tasks that demand consistent reasoning across diverse representations and domains. There is a need for more rigorous and large-scale studies on the principles of invariance and orderability of preference, together with their applications to testing rationality in agent systems. This would involve developing more sophisticated perturbation methods that probe the consistency of reasoning at a deeper level, as well as designing experiments that yield statistically significant results.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Encouraging More Multi-Modal Agents in Multi-Agent Systems
   </h4>
   <div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">
     Research into the integration of multi-modality within multi-agent systems would be promising. Fields such as multi-agent debate, collaboration, and neuro-symbolic reasoning, as shown in Figure
     <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2 Defining Rationality ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , currently under-utilize the potential of multi-modal sensory inputs. We believe that expanding the role of multi-modalities, including but not limited to vision, sounds, and structured data could significantly enhance the capabilities and rationality of multi-agent systems.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Conclusions
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    This survey builds connections between multi-modal and multi-agent systems with rationality, guided by dual-process theories and the four axioms we expect a rational agent or agent systems should satisfy:
    <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">
     grounding, orderability of preference, independence from irrelevant context, and invariance
    </span>
    . Our findings suggest that the grounding can usually be enhanced by multi-modalities, knowledge retrieval, and tool utilization. The remaining three axioms are typically intertwined, and often simultaneously improved via deliberation (slow, iterative thinking process) and abstraction (distilling the logical essence).
   </p>
  </div>
  <div class="ltx_para" id="S7.p2">
   <p class="ltx_p" id="S7.p2.1">
    Collaboration between the AI research community and cognitive psychologists could be particularly fruitful. We need better evaluation benchmarks on the rationality of agents, more exploration to mitigate cognitive biases in multi-modal and multi-agent systems, and deeper understanding of how these biases arise and how they can be mitigated, ultimately enhancing rationality in decision-making processes.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S8">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    8
   </span>
   Limitations
  </h2>
  <div class="ltx_para" id="S8.p1">
   <p class="ltx_p" id="S8.p1.1">
    The fields of multi-modal and multi-agent systems are rapidly evolving. Despite our best efforts, it is inherently impossible to encompass all related works within the scope of this survey. Our discussion also possesses limited mention of the reasoning capabilities, theory of mind in machine psychology, and cognitive architectures, all of which lie beyond the scope of this survey but are crucial for a deeper understanding of LLMs and agent systems. Furthermore, the concept of rationality in human cognitive science may encompass more principles and axioms than those defined in our survey.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Aghajanyan et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Cm3: A causal masked multimodal model of the internet.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      arXiv preprint arXiv:2201.07520
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Alayrac et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Flamingo: a visual language model for few-shot learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      Advances in neural information processing systems
     </em>
     , 35:23716–23736.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Awadalla et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Openflamingo: An open-source framework for training large autoregressive vision-language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      arXiv preprint arXiv:2308.01390
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bai et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Benchmarking foundation models with language-model-as-an-examiner.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2302.04023
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. 2022.
    </span>
    <span class="ltx_bibblock">
     Vlmo: Unified vision-language pre-training with mixture-of-modality-experts.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 35:32897–32912.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bender et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.
    </span>
    <span class="ltx_bibblock">
     On the dangers of stochastic parrots: Can language models be too big?
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
     </em>
     , pages 610–623.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Besta et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Graph of thoughts: Solving elaborate problems with large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      Proceedings of the AAAI Conference on Artificial Intelligence
     </em>
     , volume 38, pages 17682–17690.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Binz and Schulz (2023)
    </span>
    <span class="ltx_bibblock">
     Marcel Binz and Eric Schulz. 2023.
    </span>
    <span class="ltx_bibblock">
     Using cognitive psychology to understand gpt-3.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      Proceedings of the National Academy of Sciences
     </em>
     , 120(6):e2218523120.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Biten et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Ali Furkan Biten, Lluís Gómez, and Dimosthenis Karatzas. 2022.
    </span>
    <span class="ltx_bibblock">
     Let there be a clock on the beach: Reducing object hallucination in image captioning.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
     </em>
     , pages 1381–1390.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brooks et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openai.com/research/video-generation-models-as-world-simulators" target="_blank" title="">
      Video generation models as world simulators
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bubeck et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Sparks of artificial general intelligence: Early experiments with gpt-4.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      arXiv preprint arXiv:2303.12712
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cao et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2021.
    </span>
    <span class="ltx_bibblock">
     Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      arXiv preprint arXiv:2109.09784
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     Chateval: Towards better llm-based evaluators through multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      arXiv preprint arXiv:2308.07201
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Pali: A jointly-scaled multilingual language-image model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2209.06794
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023.
    </span>
    <span class="ltx_bibblock">
     Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      arXiv preprint arXiv:2304.00723
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cheng et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Exploring large language model based intelligent agents: Definitions, methods, and prospects.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      arXiv preprint arXiv:2401.03428
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cheng et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Binding language models in symbolic languages.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      arXiv preprint arXiv:2210.02875
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cheong et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Inyoung Cheong, King Xia, KJ Feng, Quan Ze Chen, and Amy X Zhang. 2024.
    </span>
    <span class="ltx_bibblock">
     (a) i am not a lawyer, but…: Engaging legal experts towards responsible llm policies for legal advice.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      arXiv preprint arXiv:2402.01864
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chern et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Steffi Chern, Zhen Fan, and Andy Liu. 2024.
    </span>
    <span class="ltx_bibblock">
     Combating adversarial attacks with multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      arXiv preprint arXiv:2401.05998
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chiang and Lee (2023)
    </span>
    <span class="ltx_bibblock">
     Cheng-Han Chiang and Hung-yi Lee. 2023.
    </span>
    <span class="ltx_bibblock">
     A closer look into automatic evaluation using large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      arXiv preprint arXiv:2310.05657
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Clark et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.
    </span>
    <span class="ltx_bibblock">
     Transformers as soft reasoners over language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      arXiv preprint arXiv:2002.05867
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cohen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023.
    </span>
    <span class="ltx_bibblock">
     Lm vs lm: Detecting factual errors via cross examination.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2305.13281
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Craik and Lockhart (1972)
    </span>
    <span class="ltx_bibblock">
     Fergus IM Craik and Robert S Lockhart. 1972.
    </span>
    <span class="ltx_bibblock">
     Levels of processing: A framework for memory research.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      Journal of verbal learning and verbal behavior
     </em>
     , 11(6):671–684.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cui et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     A survey on multimodal large language models for autonomous driving.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision
     </em>
     , pages 958–979.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Das et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Souvik Das, Sougata Saha, and Rohini K Srihari. 2023.
    </span>
    <span class="ltx_bibblock">
     Diving deep into modes of fact hallucinations in dialogue systems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      arXiv preprint arXiv:2301.04449
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Deng et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2024.
    </span>
    <span class="ltx_bibblock">
     Mind2web: Towards a generalist agent for the web.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dong et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, and Ge Li. 2024.
    </span>
    <span class="ltx_bibblock">
     Generalization or memorization: Data contamination and trustworthy evaluation for large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:2402.15938
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Du et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Wei Du, Yichun Zhao, Boqun Li, Gongshen Liu, and Shilin Wang. 2022.
    </span>
    <span class="ltx_bibblock">
     Ppt: Backdoor attacks on pre-trained models via poisoned prompt tuning.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      IJCAI
     </em>
     , pages 680–686.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Du et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023.
    </span>
    <span class="ltx_bibblock">
     Improving factuality and reasoning in language models through multiagent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      arXiv preprint arXiv:2305.14325
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Durante et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Agent ai: Surveying the horizons of multimodal interaction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      arXiv preprint arXiv:2401.03568
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dziri et al. (2022a)
    </span>
    <span class="ltx_bibblock">
     Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M Ponti, and Siva Reddy. 2022a.
    </span>
    <span class="ltx_bibblock">
     Faithdial: A faithful benchmark for information-seeking dialogue.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      Transactions of the Association for Computational Linguistics
     </em>
     , 10:1473–1490.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dziri et al. (2022b)
    </span>
    <span class="ltx_bibblock">
     Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022b.
    </span>
    <span class="ltx_bibblock">
     Evaluating attribution in dialogue systems: The begin benchmark.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      Transactions of the Association for Computational Linguistics
     </em>
     , 10:1066–1083.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Echterhoff et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, and Zexue He. 2024.
    </span>
    <span class="ltx_bibblock">
     Cognitive bias in high-stakes decision-making with llms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      arXiv preprint arXiv:2403.00811
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Eisenführ et al. (2010)
    </span>
    <span class="ltx_bibblock">
     Franz Eisenführ, Martin Weber, and Thomas Langer. 2010.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      Rational decision making
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Springer.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fan et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.
    </span>
    <span class="ltx_bibblock">
     Minedojo: Building open-ended embodied agents with internet-scale knowledge.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 35:18343–18362.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fang et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Meng Fang, Shilong Deng, Yudi Zhang, Zijing Shi, Ling Chen, Mykola Pechenizkiy, and Jun Wang. 2024.
    </span>
    <span class="ltx_bibblock">
     Large language models are neurosymbolic reasoners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      arXiv preprint arXiv:2401.09334
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     Gptscore: Evaluate as you desire.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      arXiv preprint arXiv:2302.04166
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Furuta et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. 2023.
    </span>
    <span class="ltx_bibblock">
     Multimodal web navigation with instruction-finetuned foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      arXiv preprint arXiv:2305.11854
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ganguli et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      arXiv preprint arXiv:2209.07858
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and Yong Li. 2023a.
    </span>
    <span class="ltx_bibblock">
     S3: Social-network simulation system with large language model-empowered agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      arXiv preprint arXiv:2307.14984
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. 2023b.
    </span>
    <span class="ltx_bibblock">
     Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      arXiv preprint arXiv:2306.08640
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. (2023c)
    </span>
    <span class="ltx_bibblock">
     Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023c.
    </span>
    <span class="ltx_bibblock">
     Human-like summarization evaluation with chatgpt.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      arXiv preprint arXiv:2304.02554
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gehman et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020.
    </span>
    <span class="ltx_bibblock">
     Realtoxicityprompts: Evaluating neural toxic degeneration in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      arXiv preprint arXiv:2009.11462
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gravitas (2023)
    </span>
    <span class="ltx_bibblock">
     Significant Gravitas. 2023.
    </span>
    <span class="ltx_bibblock">
     Autogpt.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      Python. https://github.com/Significant-Gravitas/ Auto-GPT
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guerreiro et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Nuno M Guerreiro, Duarte M Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André FT Martins. 2023.
    </span>
    <span class="ltx_bibblock">
     Hallucinations in large multilingual translation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      Transactions of the Association for Computational Linguistics
     </em>
     , 11:1500–1517.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024.
    </span>
    <span class="ltx_bibblock">
     Large language model based multi-agents: A survey of progress and challenges.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      arXiv preprint arXiv:2402.01680
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gupta et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021.
    </span>
    <span class="ltx_bibblock">
     Dialfact: A benchmark for fact-checking in dialogue.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      arXiv preprint arXiv:2110.08222
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gupta and Kembhavi (2023)
    </span>
    <span class="ltx_bibblock">
     Tanmay Gupta and Aniruddha Kembhavi. 2023.
    </span>
    <span class="ltx_bibblock">
     Visual programming: Compositional visual reasoning without training.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     , pages 14953–14962.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gur et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2023.
    </span>
    <span class="ltx_bibblock">
     A real-world webagent with planning, long context understanding, and program synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      arXiv preprint arXiv:2307.12856
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Han et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, and Chaoyang He. 2024.
    </span>
    <span class="ltx_bibblock">
     Llm multi-agent systems: Challenges and open problems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      arXiv preprint arXiv:2402.03578
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hastie and Dawes (2009)
    </span>
    <span class="ltx_bibblock">
     Reid Hastie and Robyn M Dawes. 2009.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">
      Rational choice in an uncertain world: The psychology of judgment and decision making
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Sage Publications.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     He et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. 2023.
    </span>
    <span class="ltx_bibblock">
     A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">
      arXiv preprint arXiv:2310.05694
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hendrycks et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.
    </span>
    <span class="ltx_bibblock">
     Measuring mathematical problem solving with the math dataset.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">
      Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hogan et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d’Amato, Gerard De Melo, Claudio Gutierrez, Sabrina Kirrane, José Emilio Labra Gayo, Roberto Navigli, Sebastian Neumaier, et al. 2021.
    </span>
    <span class="ltx_bibblock">
     Knowledge graphs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">
      ACM Computing Surveys (Csur)
     </em>
     , 54(4):1–37.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hong et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023a.
    </span>
    <span class="ltx_bibblock">
     Metagpt: Meta programming for multi-agent collaborative framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">
      arXiv preprint arXiv:2308.00352
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hong et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2023b.
    </span>
    <span class="ltx_bibblock">
     Cogagent: A visual language model for gui agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">
      arXiv preprint arXiv:2312.08914
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hsu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Joy Hsu, Jiayuan Mao, Josh Tenenbaum, and Jiajun Wu. 2024.
    </span>
    <span class="ltx_bibblock">
     What’s left? concept grounding with logic-enhanced foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib59">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David Ross, Cordelia Schmid, and Alireza Fathi. 2024.
    </span>
    <span class="ltx_bibblock">
     Avis: Autonomous visual information seeking with large language model agent.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib60">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang and Chang (2022)
    </span>
    <span class="ltx_bibblock">
     Jie Huang and Kevin Chen-Chuan Chang. 2022.
    </span>
    <span class="ltx_bibblock">
     Towards reasoning in large language models: A survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">
      arXiv preprint arXiv:2212.10403
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib61">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">
      arXiv preprint arXiv:2311.05232
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib62">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jacovi et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. 2023.
    </span>
    <span class="ltx_bibblock">
     Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">
      arXiv preprint arXiv:2305.10160
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib63">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, and Dan Roth. 2024a.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2406.11050" target="_blank" title="">
      A peek into token bias: Large language models are not yet genuine reasoners
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">
      Preprint
     </em>
     , arXiv:2406.11050.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib64">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Bowen Jiang, Zhijun Zhuang, Shreyas S Shivakumar, Dan Roth, and Camillo J Taylor. 2024b.
    </span>
    <span class="ltx_bibblock">
     Multi-agent vqa: Exploring multi-agent foundation models in zero-shot visual question answering.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">
      arXiv preprint arXiv:2403.14783
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib65">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Johnson-Laird (1983)
    </span>
    <span class="ltx_bibblock">
     Philip Nicholas Johnson-Laird. 1983.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">
      Mental models: Towards a cognitive science of language, inference, and consciousness
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     6. Harvard University Press.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib66">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kang and Liu (2023)
    </span>
    <span class="ltx_bibblock">
     Haoqiang Kang and Xiao-Yang Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     Deficiency of large language models in finance: An empirical examination of hallucination.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">
      arXiv preprint arXiv:2311.15548
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib67">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ke et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Yu He Ke, Rui Yang, Sui An Lie, Taylor Xin Yi Lim, Hairil Rizal Abdullah, Daniel Shu Wei Ting, and Nan Liu. 2024.
    </span>
    <span class="ltx_bibblock">
     Enhancing diagnostic accuracy through multi-agent conversations: Using large language models to mitigate cognitive bias.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">
      arXiv preprint arXiv:2401.14589
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib68">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Khan et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rocktäschel, and Ethan Perez. 2024.
    </span>
    <span class="ltx_bibblock">
     Debating with more persuasive llms leads to more truthful answers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">
      arXiv preprint arXiv:2402.06782
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib69">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Khardon and Roth (1997)
    </span>
    <span class="ltx_bibblock">
     Roni Khardon and Dan Roth. 1997.
    </span>
    <span class="ltx_bibblock">
     Learning to reason.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">
      Journal of the ACM (JACM)
     </em>
     , 44(5):697–725.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib70">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kirillov et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. 2023.
    </span>
    <span class="ltx_bibblock">
     Segment anything.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">
      arXiv:2304.02643
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib71">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Koo et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023.
    </span>
    <span class="ltx_bibblock">
     Benchmarking cognitive biases in large language models as evaluators.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">
      arXiv preprint arXiv:2309.17012
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib72">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Laird (2019)
    </span>
    <span class="ltx_bibblock">
     John E Laird. 2019.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">
      The Soar cognitive architecture
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     MIT press.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib73">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     LeCun (2022)
    </span>
    <span class="ltx_bibblock">
     Yann LeCun. 2022.
    </span>
    <span class="ltx_bibblock">
     A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">
      Open Review
     </em>
     , 62(1).
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib74">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     LeCun (2024)
    </span>
    <span class="ltx_bibblock">
     Yann LeCun. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf" target="_blank" title="">
      Objective-driven ai: Towards ai systems that can learn, remember, reason, plan, have common sense, yet are steerable and safe
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     University of Washington, Department of Electrical &amp; Computer Engineering.
    </span>
    <span class="ltx_bibblock">
     Slide presentation retrieved from University of Washington.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib75">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lewis et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020.
    </span>
    <span class="ltx_bibblock">
     Retrieval-augmented generation for knowledge-intensive nlp tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 33:9459–9474.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib76">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Leydesdorff and Hodgkin (2017)
    </span>
    <span class="ltx_bibblock">
     Selma Leydesdorff and Katharine Hodgkin. 2017.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">
      Memory cultures: Memory, subjectivity and recognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Routledge.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib77">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, and Muhao Chen. 2023a.
    </span>
    <span class="ltx_bibblock">
     Deceiving semantic shortcuts on reasoning chains: How far can models go without hallucination?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">
      arXiv preprint arXiv:2311.09702
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib78">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2023b.
    </span>
    <span class="ltx_bibblock">
     Seed-bench-2: Benchmarking multimodal large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">
      arXiv preprint arXiv:2311.17092
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib79">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. 2024a.
    </span>
    <span class="ltx_bibblock">
     Multimodal foundation models: From specialists to general-purpose assistants.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">
      Foundations and Trends® in Computer Graphics and Vision
     </em>
     , 16(1-2):1–214.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib80">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023c)
    </span>
    <span class="ltx_bibblock">
     Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. 2023c.
    </span>
    <span class="ltx_bibblock">
     Fine-tuning multimodal llms to follow zero-shot demonstrative instructions.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib81">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. 2024b.
    </span>
    <span class="ltx_bibblock">
     Agent hospital: A simulacrum of hospital with evolvable medical agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">
      arXiv preprint arXiv:2405.02957
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib82">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023d)
    </span>
    <span class="ltx_bibblock">
     Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023d.
    </span>
    <span class="ltx_bibblock">
     Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">
      International conference on machine learning
     </em>
     , pages 19730–19742. PMLR.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib83">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023e)
    </span>
    <span class="ltx_bibblock">
     Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023e.
    </span>
    <span class="ltx_bibblock">
     Halueval: A large-scale hallucination evaluation benchmark for large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">
      Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 6449–6464.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib84">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023f)
    </span>
    <span class="ltx_bibblock">
     Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Soujanya Poria. 2023f.
    </span>
    <span class="ltx_bibblock">
     Chain of knowledge: A framework for grounding large language models with structured knowledge bases.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">
      arXiv preprint arXiv:2305.13269
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib85">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023g)
    </span>
    <span class="ltx_bibblock">
     Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023g.
    </span>
    <span class="ltx_bibblock">
     Evaluating object hallucination in large vision-language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">
      arXiv preprint arXiv:2305.10355
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib86">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023h)
    </span>
    <span class="ltx_bibblock">
     Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023h.
    </span>
    <span class="ltx_bibblock">
     Large language models in finance: A survey.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">
      Proceedings of the Fourth ACM International Conference on AI in Finance
     </em>
     , pages 374–382.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib87">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023i)
    </span>
    <span class="ltx_bibblock">
     Yuan Li, Yixuan Zhang, and Lichao Sun. 2023i.
    </span>
    <span class="ltx_bibblock">
     Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">
      arXiv preprint arXiv:2310.06500
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib88">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023.
    </span>
    <span class="ltx_bibblock">
     Encouraging divergent thinking in large language models through multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">
      arXiv preprint arXiv:2305.19118
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib89">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 2018.
    </span>
    <span class="ltx_bibblock">
     Reinforcement learning on web interfaces using workflow-guided exploration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">
      arXiv preprint arXiv:1802.08802
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib90">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and Li Zhang. 2024a.
    </span>
    <span class="ltx_bibblock">
     Exploring and evaluating hallucinations in llm-powered code generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">
      arXiv preprint arXiv:2404.00971
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib91">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024b.
    </span>
    <span class="ltx_bibblock">
     World model on million-length video and language with ringattention.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">
      arXiv preprint arXiv:2402.08268
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib92">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.
    </span>
    <span class="ltx_bibblock">
     Improved baselines with visual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">
      arXiv preprint arXiv:2310.03744
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib93">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2024c)
    </span>
    <span class="ltx_bibblock">
     Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024c.
    </span>
    <span class="ltx_bibblock">
     Visual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">
      Advances in neural information processing systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib94">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2021.
    </span>
    <span class="ltx_bibblock">
     Logiqa: a challenge dataset for machine reading comprehension with logical reasoning.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">
      Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence
     </em>
     , pages 3622–3628.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib95">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2024d)
    </span>
    <span class="ltx_bibblock">
     Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024d.
    </span>
    <span class="ltx_bibblock">
     Lost in the middle: How language models use long contexts.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">
      Transactions of the Association for Computational Linguistics
     </em>
     , 12:157–173.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib96">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2024e)
    </span>
    <span class="ltx_bibblock">
     Ollie Liu, Deqing Fu, Dani Yogatama, and Willie Neiswanger. 2024e.
    </span>
    <span class="ltx_bibblock">
     Dellma: A framework for decision making under uncertainty with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">
      arXiv preprint arXiv:2402.02392
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib97">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b.
    </span>
    <span class="ltx_bibblock">
     Gpteval: Nlg evaluation using gpt-4 with better human alignment.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">
      arXiv preprint arXiv:2303.16634
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib98">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023c)
    </span>
    <span class="ltx_bibblock">
     Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. 2023c.
    </span>
    <span class="ltx_bibblock">
     Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">
      arXiv preprint arXiv:2310.02170
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib99">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.
    </span>
    <span class="ltx_bibblock">
     Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">
      Advances in neural information processing systems
     </em>
     , 32.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib100">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. 2023.
    </span>
    <span class="ltx_bibblock">
     Memochat: Tuning llms to use memos for consistent long-range open-domain conversation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">
      arXiv preprint arXiv:2308.08239
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib101">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2024.
    </span>
    <span class="ltx_bibblock">
     Chameleon: Plug-and-play compositional reasoning with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib102">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021.
    </span>
    <span class="ltx_bibblock">
     Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">
      arXiv preprint arXiv:2104.08786
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib103">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Luo et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023.
    </span>
    <span class="ltx_bibblock">
     Chatgpt as a factual inconsistency evaluator for abstractive text summarization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">
      arXiv preprint arXiv:2303.15621
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib104">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Macmillan-Scott and Musolesi (2024)
    </span>
    <span class="ltx_bibblock">
     Olivia Macmillan-Scott and Mirco Musolesi. 2024.
    </span>
    <span class="ltx_bibblock">
     (ir) rationality and cognitive biases in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">
      arXiv preprint arXiv:2402.09193
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib105">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Madaan et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Self-refine: Iterative refinement with self-feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib106">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Magar and Schwartz (2022)
    </span>
    <span class="ltx_bibblock">
     Inbal Magar and Roy Schwartz. 2022.
    </span>
    <span class="ltx_bibblock">
     Data contamination: From memorization to exploitation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">
      arXiv preprint arXiv:2203.08242
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib107">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     March and Simon (1958)
    </span>
    <span class="ltx_bibblock">
     James G March and Herbert A Simon. 1958.
    </span>
    <span class="ltx_bibblock">
     Organizations.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">
      University of Illinois at Urbana-Champaign’s Academy for Entrepreneurial Leadership Historical Research Reference in Entrepreneurship
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib108">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Marino et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. 2021.
    </span>
    <span class="ltx_bibblock">
     Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
     </em>
     , pages 14111–14121.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib109">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mohtashami et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Amirkeivan Mohtashami, Florian Hartmann, Sian Gooding, Lukas Zilka, Matt Sharifi, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Social learning: Towards collaborative learning with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">
      arXiv preprint arXiv:2312.11441
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib110">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Mukherjee and Chang (2024)
    </span>
    <span class="ltx_bibblock">
     Anirban Mukherjee and Hannah Hanwen Chang. 2024.
    </span>
    <span class="ltx_bibblock">
     Heuristic reasoning in ai: Instrumental use and mimetic absorption.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">
      arXiv preprint arXiv:2403.09404
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib111">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakajima (2023)
    </span>
    <span class="ltx_bibblock">
     Yohei Nakajima. 2023.
    </span>
    <span class="ltx_bibblock">
     Babyagi.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">
      Python. https://github. com/yoheinakajima/babyagi
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib112">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ohmer et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xenia Ohmer, Elia Bruni, and Dieuwke Hupkes. 2023.
    </span>
    <span class="ltx_bibblock">
     Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">
      CoRR, abs/2305.11662
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib113">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ohmer et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Xenia Ohmer, Elia Bruni, and Dieuwke Hupkes. 2024.
    </span>
    <span class="ltx_bibblock">
     From form (s) to meaning: Probing the semantic depths of language models using multisense consistency.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">
      arXiv preprint arXiv:2404.12145
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib114">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023)
    </span>
    <span class="ltx_bibblock">
     OpenAI. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:263218031" target="_blank" title="">
      Gpt-4v(ision) system card
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib115">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2024)
    </span>
    <span class="ltx_bibblock">
     OpenAI. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openai.com/index/hello-gpt-4o/" target="_blank" title="">
      Gpt-4o
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Software available from OpenAI.
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-05-20.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib116">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023.
    </span>
    <span class="ltx_bibblock">
     Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">
      arXiv preprint arXiv:2305.12295
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib117">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pan et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu. 2024.
    </span>
    <span class="ltx_bibblock">
     Chain-of-action: Faithful and multimodal question answering through large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">
      arXiv preprint arXiv:2403.17359
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib118">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     (118)
    </span>
    <span class="ltx_bibblock">
     PaperswithcodeMCQA.
    </span>
    <span class="ltx_bibblock">
     Multiple choice qa.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://paperswithcode.com/task/multiple-choice-qa/latest" target="_blank" title="">
      https://paperswithcode.com/task/multiple-choice-qa/latest
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Accessed: 2024-05-28.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib119">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pecher et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Branislav Pecher, Ivan Srba, and Maria Bielikova. 2024.
    </span>
    <span class="ltx_bibblock">
     On sensitivity of learning with limited labelled data to the effects of randomness: Impact of interactions and systematic choices.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">
      arXiv preprint arXiv:2402.12817
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib120">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Perez et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022.
    </span>
    <span class="ltx_bibblock">
     Red teaming language models with language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">
      arXiv preprint arXiv:2202.03286
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib121">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Prasad et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2023.
    </span>
    <span class="ltx_bibblock">
     Adapt: As-needed decomposition and planning with language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">
      arXiv preprint arXiv:2311.05772
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib122">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023.
    </span>
    <span class="ltx_bibblock">
     Communicative agents for software development.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">
      arXiv preprint arXiv:2307.07924
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib123">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qiao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022.
    </span>
    <span class="ltx_bibblock">
     Reasoning with language model prompting: A survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">
      arXiv preprint arXiv:2212.09597
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib124">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qiao et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024.
    </span>
    <span class="ltx_bibblock">
     Autoact: Automatic agent learning from scratch via self-planning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">
      arXiv preprint arXiv:2401.05268
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib125">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.
    </span>
    <span class="ltx_bibblock">
     Learning transferable visual models from natural language supervision.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">
      International conference on machine learning
     </em>
     , pages 8748–8763. PMLR.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib126">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rashkin et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023.
    </span>
    <span class="ltx_bibblock">
     Measuring attribution in natural language generation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">
      Computational Linguistics
     </em>
     , 49(4):777–840.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib127">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Reid et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">
      arXiv preprint arXiv:2403.05530
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib128">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ren et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Grounded sam: Assembling open-world models for diverse visual tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">
      arXiv preprint arXiv:2401.14159
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib129">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Robinson et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Raquel B Robinson, Karin Johansson, James Collin Fey, Elena Márquez Segura, Jon Back, Annika Waern, Sarah Lynne Bowman, and Katherine Isbister. 2023.
    </span>
    <span class="ltx_bibblock">
     Leveraging large language models for multiple choice question answering.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">
      Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems
     </em>
     , pages 1–5.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib130">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rohrbach et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018.
    </span>
    <span class="ltx_bibblock">
     Object hallucination in image captioning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">
      arXiv preprint arXiv:1809.02156
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib131">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rombach et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.
    </span>
    <span class="ltx_bibblock">
     High-resolution image synthesis with latent diffusion models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">
      Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
     </em>
     , pages 10684–10695.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib132">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sainz et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023.
    </span>
    <span class="ltx_bibblock">
     Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">
      arXiv preprint arXiv:2310.18018
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib133">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schaeffer et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. 2024.
    </span>
    <span class="ltx_bibblock">
     Are emergent abilities of large language models a mirage?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib134">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib135">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sclar et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023a.
    </span>
    <span class="ltx_bibblock">
     Quantifying language models’ sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">
      arXiv preprint arXiv:2310.11324
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib136">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sclar et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. 2023b.
    </span>
    <span class="ltx_bibblock">
     Minding language models’(lack of) theory of mind: A plug-and-play multi-character belief tracker.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">
      arXiv preprint arXiv:2306.00924
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib137">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Selten (1990)
    </span>
    <span class="ltx_bibblock">
     Reinhard Selten. 1990.
    </span>
    <span class="ltx_bibblock">
     Bounded rationality.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">
      Journal of Institutional and Theoretical Economics (JITE)/Zeitschrift für die gesamte Staatswissenschaft
     </em>
     , 146(4):649–658.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib138">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shaw et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina N Toutanova. 2024.
    </span>
    <span class="ltx_bibblock">
     From pixels to ui actions: Learning to follow instructions via graphical user interfaces.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib139">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. 2023.
    </span>
    <span class="ltx_bibblock">
     Large language models are not yet human-level evaluators for abstractive summarization.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">
      Findings of the Association for Computational Linguistics: EMNLP 2023
     </em>
     , pages 4215–4233.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib140">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. 2024a.
    </span>
    <span class="ltx_bibblock">
     Small llms are weak tool learners: A multi-llm agent.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">
      arXiv preprint arXiv:2401.07324
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib141">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2024b.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib142">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shi et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023.
    </span>
    <span class="ltx_bibblock">
     Large language models can be easily distracted by irrelevant context.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">
      International Conference on Machine Learning
     </em>
     , pages 31210–31227. PMLR.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib143">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shi et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. 2017.
    </span>
    <span class="ltx_bibblock">
     World of bits: An open-domain platform for web-based agents.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">
      International Conference on Machine Learning
     </em>
     , pages 3135–3144. PMLR.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib144">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shi et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, and Zhaochun Ren. 2024.
    </span>
    <span class="ltx_bibblock">
     Learning to use tools via cooperative and interactive agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">
      arXiv preprint arXiv:2403.03031
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib145">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024.
    </span>
    <span class="ltx_bibblock">
     Reflexion: Language agents with verbal reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib146">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sinha et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. 2019.
    </span>
    <span class="ltx_bibblock">
     Clutrr: A diagnostic benchmark for inductive reasoning from text.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">
      arXiv preprint arXiv:1908.06177
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib147">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Solso and Kagan (1979)
    </span>
    <span class="ltx_bibblock">
     Robert L Solso and Jerome Kagan. 1979.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">
      Cognitive psychology
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Houghton Mifflin Harcourt P.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib148">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Song et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. 2024.
    </span>
    <span class="ltx_bibblock">
     Milebench: Benchmarking mllms in long context.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">
      arXiv preprint arXiv:2404.18532
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib149">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Speer et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Robyn Speer, Joshua Chin, and Catherine Havasi. 2017.
    </span>
    <span class="ltx_bibblock">
     Conceptnet 5.5: An open multilingual graph of general knowledge.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">
      Proceedings of the AAAI conference on artificial intelligence
     </em>
     , volume 31.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib150">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stureborg et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. 2024.
    </span>
    <span class="ltx_bibblock">
     Large language models are inconsistent and biased evaluators.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">
      arXiv preprint arXiv:2405.01724
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib151">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Su et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2019.
    </span>
    <span class="ltx_bibblock">
     Vl-bert: Pre-training of generic visual-linguistic representations.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">
      arXiv preprint arXiv:1908.08530
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib152">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Subramanian et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Chitra Subramanian, Miao Liu, Naweed Khan, Jonathan Lenchner, Aporva Amarnath, Sarathkrishna Swaminathan, Ryan Riegel, and Alexander Gray. 2024.
    </span>
    <span class="ltx_bibblock">
     A neuro-symbolic approach to multi-agent rl for interpretability and probabilistic decision making.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">
      arXiv preprint arXiv:2402.13440
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib153">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, and Lingpeng Kong. 2023a.
    </span>
    <span class="ltx_bibblock">
     Corex: Pushing the boundaries of complex reasoning through multi-model collaboration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">
      arXiv preprint arXiv:2310.00280
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib154">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun (2001)
    </span>
    <span class="ltx_bibblock">
     Ron Sun. 2001.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">
      Duality of the mind: A bottom-up approach toward cognition
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Psychology Press.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib155">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun (2024)
    </span>
    <span class="ltx_bibblock">
     Ron Sun. 2024.
    </span>
    <span class="ltx_bibblock">
     Can a cognitive architecture fundamentally enhance llms? or vice versa?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">
      arXiv preprint arXiv:2401.10444
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib156">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. 2023b.
    </span>
    <span class="ltx_bibblock">
     Aligning large multimodal models with factually augmented rlhf.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">
      arXiv preprint arXiv:2309.14525
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib157">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Suri et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Gaurav Suri, Lily R Slater, Ali Ziaee, and Morgan Nguyen. 2024.
    </span>
    <span class="ltx_bibblock">
     Do large language models show decision heuristics similar to humans? a case study using gpt-3.5.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">
      Journal of Experimental Psychology: General
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib158">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Surís et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Dídac Surís, Sachit Menon, and Carl Vondrick. 2023.
    </span>
    <span class="ltx_bibblock">
     Vipergpt: Visual inference via python execution for reasoning.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">
      Proceedings of the IEEE/CVF International Conference on Computer Vision
     </em>
     , pages 11888–11898.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib159">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Talebirad and Nadiri (2023)
    </span>
    <span class="ltx_bibblock">
     Yashar Talebirad and Amirhossein Nadiri. 2023.
    </span>
    <span class="ltx_bibblock">
     Multi-agent collaboration: Harnessing the power of intelligent llm agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">
      arXiv preprint arXiv:2306.03314
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib160">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Talmor et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.
    </span>
    <span class="ltx_bibblock">
     Commonsenseqa: A question answering challenge targeting commonsense knowledge.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">
      Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)
     </em>
     , pages 4149–4158.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib161">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tang et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W Vincent, Yu’an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, et al. 2024.
    </span>
    <span class="ltx_bibblock">
     Tofueval: Evaluating hallucinations of llms on topic-focused dialogue summarization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">
      arXiv preprint arXiv:2402.13249
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib162">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023.
    </span>
    <span class="ltx_bibblock">
     Toolalpaca: Generalized tool learning for language models with 3000 simulated cases.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">
      arXiv preprint arXiv:2306.05301
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib163">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tversky and Kahneman (1988)
    </span>
    <span class="ltx_bibblock">
     Amos Tversky and Daniel Kahneman. 1988.
    </span>
    <span class="ltx_bibblock">
     Rational choice and the framing of decisions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">
      Decision making: Descriptive, normative, and prescriptive interactions
     </em>
     , pages 167–192.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib164">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Valmeekam et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023.
    </span>
    <span class="ltx_bibblock">
     On the planning abilities of large language models-a critical investigation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36:75993–76005.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib165">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vaswani et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.
    </span>
    <span class="ltx_bibblock">
     Attention is all you need.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">
      Advances in neural information processing systems
     </em>
     , 30.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib166">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Von Neumann and Morgenstern (2007)
    </span>
    <span class="ltx_bibblock">
     John Von Neumann and Oskar Morgenstern. 2007.
    </span>
    <span class="ltx_bibblock">
     Theory of games and economic behavior: 60th anniversary commemorative edition.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">
      Theory of games and economic behavior
     </em>
     . Princeton university press.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib167">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a.
    </span>
    <span class="ltx_bibblock">
     Voyager: An open-ended embodied agent with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">
      arXiv preprint arXiv:2305.16291
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib168">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023b.
    </span>
    <span class="ltx_bibblock">
     Is chatgpt a good nlg evaluator? a preliminary study.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib168.1.1">
      arXiv preprint arXiv:2303.04048
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib169">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Pengda Wang, Zilin Xiao, Hanjie Chen, and Frederick L Oswald. 2024a.
    </span>
    <span class="ltx_bibblock">
     Will the real linda please stand up… to large language models? examining the representativeness heuristic in llms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">
      arXiv preprint arXiv:2404.01461
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib170">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, and Xuanjing Huang. 2024b.
    </span>
    <span class="ltx_bibblock">
     Benchmark self-evolving: A multi-agent framework for dynamic llm evaluation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">
      arXiv preprint arXiv:2402.11443
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib171">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023c)
    </span>
    <span class="ltx_bibblock">
     Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023c.
    </span>
    <span class="ltx_bibblock">
     Cogvlm: Visual expert for pretrained language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">
      arXiv preprint arXiv:2311.03079
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib172">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2022a)
    </span>
    <span class="ltx_bibblock">
     Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. 2022a.
    </span>
    <span class="ltx_bibblock">
     Image as a foreign language: Beit pretraining for all vision and vision-language tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">
      arXiv preprint arXiv:2208.10442
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib173">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2024c)
    </span>
    <span class="ltx_bibblock">
     Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. 2024c.
    </span>
    <span class="ltx_bibblock">
     Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">
      arXiv preprint arXiv:2401.10529
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib174">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2022b)
    </span>
    <span class="ltx_bibblock">
     Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022b.
    </span>
    <span class="ltx_bibblock">
     Self-consistency improves chain of thought reasoning in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">
      arXiv preprint arXiv:2203.11171
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib175">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023d)
    </span>
    <span class="ltx_bibblock">
     Zeqing Wang, Wentao Wan, Runmeng Chen, Qiqing Lao, Minjie Lang, and Keze Wang. 2023d.
    </span>
    <span class="ltx_bibblock">
     Towards top-down reasoning: An explainable multi-agent approach for visual question answering.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">
      arXiv preprint arXiv:2311.17331
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib176">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Weber et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lucas Weber, Elia Bruni, and Dieuwke Hupkes. 2023.
    </span>
    <span class="ltx_bibblock">
     Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">
      arXiv preprint arXiv:2310.13486
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib177">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Webson and Pavlick (2021)
    </span>
    <span class="ltx_bibblock">
     Albert Webson and Ellie Pavlick. 2021.
    </span>
    <span class="ltx_bibblock">
     Do prompt-based models really understand the meaning of their prompts?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">
      arXiv preprint arXiv:2109.01247
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib178">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024.
    </span>
    <span class="ltx_bibblock">
     Jailbroken: How does llm safety training fail?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib179">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">
      Advances in neural information processing systems
     </em>
     , 35:24824–24837.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib180">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Weston et al. (2015)
    </span>
    <span class="ltx_bibblock">
     Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015.
    </span>
    <span class="ltx_bibblock">
     Towards ai-complete question answering: A set of prerequisite toy tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">
      arXiv preprint arXiv:1502.05698
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib181">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wikipedia contributors (2004)
    </span>
    <span class="ltx_bibblock">
     Wikipedia contributors. 2004.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://en.wikipedia.org/w/index.php?title=Plagiarism&amp;oldid=5139350" target="_blank" title="">
      Plagiarism — Wikipedia, the free encyclopedia
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     [Online; accessed 22-July-2004].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib182">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wong et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lionel Wong, Jiayuan Mao, Pratyusha Sharma, Zachary S Siegel, Jiahai Feng, Noa Korneev, Joshua B Tenenbaum, and Jacob Andreas. 2023.
    </span>
    <span class="ltx_bibblock">
     Learning adaptive planning representations with natural language guidance.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">
      arXiv preprint arXiv:2312.08566
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib183">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. 2022.
    </span>
    <span class="ltx_bibblock">
     Multi-modal answer validation for knowledge-based vqa.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">
      Proceedings of the AAAI conference on artificial intelligence
     </em>
     , volume 36, pages 2712–2721.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib184">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023.
    </span>
    <span class="ltx_bibblock">
     Autogen: Enabling next-gen llm applications via multi-agent conversation framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">
      arXiv preprint arXiv:2308.08155
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib185">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. 2024.
    </span>
    <span class="ltx_bibblock">
     How easily do irrelevant inputs skew the responses of large language models?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">
      arXiv preprint arXiv:2404.03302
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib186">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xie et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. 2024a.
    </span>
    <span class="ltx_bibblock">
     Large multimodal agents: A survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">
      arXiv preprint arXiv:2402.15116
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib187">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xie et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Yangxinyu Xie, Tanwi Mallick, Joshua David Bergerson, John K Hutchison, Duane R Verner, Jordan Branham, M Ross Alexander, Robert B Ross, Yan Feng, Leslie-Anne Levy, et al. 2024b.
    </span>
    <span class="ltx_bibblock">
     Wildfiregpt: Tailored large language model for wildfire analysis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">
      arXiv preprint arXiv:2402.07877
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib188">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xiong et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023.
    </span>
    <span class="ltx_bibblock">
     Diving into the inter-consistency of large language models: An insightful analysis through debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">
      arXiv preprint arXiv:2305.11595
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib189">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, and Deyi Xiong. 2024a.
    </span>
    <span class="ltx_bibblock">
     Exploring multilingual human value concepts in large language models: Is value alignment consistent, transferable and controllable across languages?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">
      arXiv preprint arXiv:2402.18120
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib190">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-seng Chua. 2023.
    </span>
    <span class="ltx_bibblock">
     Search-in-the-chain: Towards the accurate, credible and traceable content generation for complex knowledge-intensive tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">
      arXiv preprint arXiv:2304.14732
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib191">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Xinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and Börje F Karlsson. 2024b.
    </span>
    <span class="ltx_bibblock">
     A survey on game playing agents and large models: Methods, applications, and challenges.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">
      arXiv preprint arXiv:2403.10249
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib192">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, and Ion Stoica. 2023.
    </span>
    <span class="ltx_bibblock">
     Rethinking benchmark and contamination for language models with rephrased samples.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">
      arXiv preprint arXiv:2311.04850
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib193">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018.
    </span>
    <span class="ltx_bibblock">
     Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">
      Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 2369–2380.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib194">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, and Yi Yang. 2024.
    </span>
    <span class="ltx_bibblock">
     Doraemongpt: Toward understanding dynamic scenes with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">
      arXiv preprint arXiv:2401.08392
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib195">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang and Zhu (2024)
    </span>
    <span class="ltx_bibblock">
     Zukang Yang and Zixuan Zhu. 2024.
    </span>
    <span class="ltx_bibblock">
     Curiousllm: Elevating multi-document qa with reasoning-infused knowledge graph prompting.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">
      arXiv preprint arXiv:2404.09077
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib196">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022a)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022a.
    </span>
    <span class="ltx_bibblock">
     Webshop: Towards scalable real-world web interaction with grounded language agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 35:20744–20757.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib197">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024.
    </span>
    <span class="ltx_bibblock">
     Tree of thoughts: Deliberate problem solving with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib198">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022b)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022b.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib198.1.1">
      arXiv preprint arXiv:2210.03629
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib199">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Retroformer: Retrospective large language agents with policy gradient optimization.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">
      arXiv preprint arXiv:2308.02151
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib200">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yasunaga et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2022.
    </span>
    <span class="ltx_bibblock">
     Retrieval-augmented multimodal language modeling.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">
      arXiv preprint arXiv:2211.12561
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib201">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ye et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hongbin Ye, Honghao Gui, Aijia Zhang, Tong Liu, Wei Hua, and Weiqiang Jia. 2023.
    </span>
    <span class="ltx_bibblock">
     Beyond isolation: Multi-agent synergy for improving knowledge graph construction.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib201.1.1">
      arXiv preprint arXiv:2312.03022
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib202">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yi et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. 2018.
    </span>
    <span class="ltx_bibblock">
     Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">
      Advances in neural information processing systems
     </em>
     , 31.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib203">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yin et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. 2023.
    </span>
    <span class="ltx_bibblock">
     Lumos: Learning agents with unified data, modular design, and open-source llms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">
      arXiv preprint arXiv:2311.05657
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib204">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yoran et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.
    </span>
    <span class="ltx_bibblock">
     Making retrieval-augmented language models robust to irrelevant context.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">
      arXiv preprint arXiv:2310.01558
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib205">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yoshikawa and Okazaki (2023)
    </span>
    <span class="ltx_bibblock">
     Hiyori Yoshikawa and Naoaki Okazaki. 2023.
    </span>
    <span class="ltx_bibblock">
     Selective-lama: Selective prediction for confidence-aware evaluation of language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib205.1.1">
      Findings of the Association for Computational Linguistics: EACL 2023
     </em>
     , pages 2017–2028.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib206">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zelikman et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman, and Nick Haber. 2023.
    </span>
    <span class="ltx_bibblock">
     Parsel: Algorithmic reasoning with language models by composing decompositions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36:31466–31523.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib207">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian McAuley, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023.
    </span>
    <span class="ltx_bibblock">
     Agentcf: Collaborative learning with autonomous language agents for recommender systems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">
      arXiv preprint arXiv:2310.09233
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib208">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. 2024a.
    </span>
    <span class="ltx_bibblock">
     Llm as a mastermind: A survey of strategic reasoning with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib208.1.1">
      arXiv preprint arXiv:2404.01230
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib209">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024b.
    </span>
    <span class="ltx_bibblock">
     A survey on the memory mechanism of large language model based agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib209.1.1">
      arXiv preprint arXiv:2404.13501
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib210">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao and Xu (2023)
    </span>
    <span class="ltx_bibblock">
     Shu Zhao and Huijuan Xu. 2023.
    </span>
    <span class="ltx_bibblock">
     Less is more: Toward zero-shot local scene graph generation via foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib210.1.1">
      arXiv preprint arXiv:2310.01356
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib211">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. 2023.
    </span>
    <span class="ltx_bibblock">
     Bubogpt: Enabling visual grounding in multi-modal llms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib211.1.1">
      arXiv preprint arXiv:2307.08581
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib212">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zhonghan Zhao, Ke Ma, Wenhao Chai, Xuan Wang, Kewei Chen, Dongxu Guo, Yanting Zhang, Hongwei Wang, and Gaoang Wang. 2024.
    </span>
    <span class="ltx_bibblock">
     Do we really need a complex agent system? distill embodied agent into a single model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib212.1.1">
      arXiv preprint arXiv:2404.04619
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib213">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024a.
    </span>
    <span class="ltx_bibblock">
     Gpt-4v (ision) is a generalist web agent, if grounded.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib213.1.1">
      arXiv preprint arXiv:2401.01614
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib214">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023a.
    </span>
    <span class="ltx_bibblock">
     Large language models are not robust multiple choice selectors.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib214.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib215">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024b.
    </span>
    <span class="ltx_bibblock">
     Judging llm-as-a-judge with mt-bench and chatbot arena.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 36.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib216">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023b.
    </span>
    <span class="ltx_bibblock">
     Why does chatgpt fall short in providing truthful answers.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib216.1.1">
      ArXiv preprint, abs/2304.10513
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib217">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhong et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022.
    </span>
    <span class="ltx_bibblock">
     Towards a unified multi-dimensional evaluator for text generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib217.1.1">
      arXiv preprint arXiv:2210.07197
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib218">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhong et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024.
    </span>
    <span class="ltx_bibblock">
     Memorybank: Enhancing large language models with long-term memory.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">
      Proceedings of the AAAI Conference on Artificial Intelligence
     </em>
     , volume 38, pages 19724–19731.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib219">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023a.
    </span>
    <span class="ltx_bibblock">
     Minigpt-4: Enhancing vision-language understanding with advanced large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib219.1.1">
      arXiv preprint arXiv:2304.10592
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib220">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. 2023b.
    </span>
    <span class="ltx_bibblock">
     Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib220.1.1">
      arXiv preprint arXiv:2305.17144
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib221">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zong et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, and Timothy Hospedales. 2023.
    </span>
    <span class="ltx_bibblock">
     Fool your (vision and) language model with embarrassingly simple permutations.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib221.1.1">
      arXiv preprint arXiv:2310.01651
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib222">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zou et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023.
    </span>
    <span class="ltx_bibblock">
     Universal and transferable adversarial attacks on aligned language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib222.1.1">
      arXiv preprint arXiv:2307.15043
     </em>
     .
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Orderability of Preferences.
  </h2>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Comparability
   </h4>
   <div class="ltx_para" id="A1.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.2">
     When faced with any two alternatives A and B, the agent should have at least a weak preference, i.e.,
     <math alttext="A\succeq B" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.1.m1.1">
      <semantics id="A1.SS0.SSS0.Px1.p1.1.m1.1a">
       <mrow id="A1.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">
        <mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">
         A
        </mi>
        <mo id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">
         ⪰
        </mo>
        <mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">
         B
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.1.m1.1b">
        <apply id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1">
         <csymbol cd="latexml" id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.1">
          succeeds-or-equals
         </csymbol>
         <ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.2">
          𝐴
         </ci>
         <ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.3">
          𝐵
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.1.m1.1c">
        A\succeq B
       </annotation>
      </semantics>
     </math>
     or
     <math alttext="B\succeq A" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.2.m2.1">
      <semantics id="A1.SS0.SSS0.Px1.p1.2.m2.1a">
       <mrow id="A1.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">
        <mi id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">
         B
        </mi>
        <mo id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">
         ⪰
        </mo>
        <mi id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">
         A
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.2.m2.1b">
        <apply id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1">
         <csymbol cd="latexml" id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.1">
          succeeds-or-equals
         </csymbol>
         <ci id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.2">
          𝐵
         </ci>
         <ci id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.3">
          𝐴
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.2.m2.1c">
        B\succeq A
       </annotation>
      </semantics>
     </math>
     . This means that the agent can compare any pair of alternatives and determine which one is preferred or if they are equally preferred.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Transitivity
   </h4>
   <div class="ltx_para" id="A1.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1">
     If the agent prefers A to B and B to C, then the agent must prefer A to C. This ensures that the agent’s preferences are consistent and logical across multiple comparisons.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Closure
   </h4>
   <div class="ltx_para" id="A1.SS0.SSS0.Px3.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px3.p1.1">
     If A and B are in the alternative set S, then any probabilistic combination of A and B (denoted as ApB) should also be in S. This principle ensures that the set of alternatives is closed under probability mixtures.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px4">
   <h4 class="ltx_title ltx_title_paragraph">
    Distribution of probabilities across alternatives
   </h4>
   <div class="ltx_para" id="A1.SS0.SSS0.Px4.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px4.p1.1">
     If A and B are in S, then the probability mixture of (ApB) and B, denoted as [(ApB)qB], should be indifferent to the probability mixture of A and B, denoted as (ApqB). This principle ensures consistency in the agent’s preferences when dealing with probability mixtures of alternatives.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="A1.SS0.SSS0.Px5">
   <h4 class="ltx_title ltx_title_paragraph">
    Solvability
   </h4>
   <div class="ltx_para" id="A1.SS0.SSS0.Px5.p1">
    <p class="ltx_p" id="A1.SS0.SSS0.Px5.p1.1">
     When faced with three alternatives A, B, and C, with the preference order
     <math alttext="A\succeq B\succeq C" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px5.p1.1.m1.1">
      <semantics id="A1.SS0.SSS0.Px5.p1.1.m1.1a">
       <mrow id="A1.SS0.SSS0.Px5.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">
        <mi id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.2" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml">
         A
        </mi>
        <mo id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.3" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml">
         ⪰
        </mo>
        <mi id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.4" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.4.cmml">
         B
        </mi>
        <mo id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.5" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.5.cmml">
         ⪰
        </mo>
        <mi id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.6" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.6.cmml">
         C
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px5.p1.1.m1.1b">
        <apply id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1">
         <and id="A1.SS0.SSS0.Px5.p1.1.m1.1.1a.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1">
         </and>
         <apply id="A1.SS0.SSS0.Px5.p1.1.m1.1.1b.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1">
          <csymbol cd="latexml" id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.3">
           succeeds-or-equals
          </csymbol>
          <ci id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.2">
           𝐴
          </ci>
          <ci id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.4.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.4">
           𝐵
          </ci>
         </apply>
         <apply id="A1.SS0.SSS0.Px5.p1.1.m1.1.1c.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1">
          <csymbol cd="latexml" id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.5.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.5">
           succeeds-or-equals
          </csymbol>
          <share href="#A1.SS0.SSS0.Px5.p1.1.m1.1.1.4.cmml" id="A1.SS0.SSS0.Px5.p1.1.m1.1.1d.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1">
          </share>
          <ci id="A1.SS0.SSS0.Px5.p1.1.m1.1.1.6.cmml" xref="A1.SS0.SSS0.Px5.p1.1.m1.1.1.6">
           𝐶
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px5.p1.1.m1.1c">
        A\succeq B\succeq C
       </annotation>
      </semantics>
     </math>
     , there should be some probabilistic way of combining A and C such that the agent is indifferent between choosing B or this combination. In other words, the agent should be able to find a solution to the decision problem by making trade-offs between alternatives.
     <br class="ltx_break"/>
     <br class="ltx_break"/>
     One consequence of the orderability is the concept of
     <span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px5.p1.1.1">
      dominance
     </span>
     : If alternative A is better than alternative B in terms of one attribute and at least as good in terms of all other attributes, the dominant option A should be chosen. An example of a fallacy that violates dominance is the sunk cost fallacy, where an agent continues to invest in a suboptimal alternative due to past investments, despite the availability of better options based on future outcomes.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Information Grounding
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    Web agents are a quintessential example of how multi-modal agents surpass language-only ones. In agents like Pix2Act
    <cite class="ltx_cite ltx_citemacro_cite">
     Shaw et al. (
     <a class="ltx_ref" href="#bib.bib138" title="">
      2024
     </a>
     )
    </cite>
    , WebGUM
    <cite class="ltx_cite ltx_citemacro_cite">
     Furuta et al. (
     <a class="ltx_ref" href="#bib.bib39" title="">
      2023
     </a>
     )
    </cite>
    , CogAgent
    <cite class="ltx_cite ltx_citemacro_cite">
     Hong et al. (
     <a class="ltx_ref" href="#bib.bib57" title="">
      2023b
     </a>
     )
    </cite>
    , and SeeAct
    <cite class="ltx_cite ltx_citemacro_cite">
     Zheng et al. (
     <a class="ltx_ref" href="#bib.bib213" title="">
      2024a
     </a>
     )
    </cite>
    , web navigation is grounded on graphical user interface (GUI) rather than solely on HTML texts
    <cite class="ltx_cite ltx_citemacro_cite">
     Shen et al. (
     <a class="ltx_ref" href="#bib.bib140" title="">
      2024a
     </a>
     ); Yao et al. (
     <a class="ltx_ref" href="#bib.bib196" title="">
      2022a
     </a>
     ); Deng et al. (
     <a class="ltx_ref" href="#bib.bib27" title="">
      2024
     </a>
     ); Gur et al. (
     <a class="ltx_ref" href="#bib.bib50" title="">
      2023
     </a>
     )
    </cite>
    . This method of visual grounding offers higher information density compared to HTML codes that are usually lengthy, noisy, and sometimes even incomplete
    <cite class="ltx_cite ltx_citemacro_cite">
     Zheng et al. (
     <a class="ltx_ref" href="#bib.bib213" title="">
      2024a
     </a>
     )
    </cite>
    . Supporting the importance of vision, ablation studies in WebGUM
    <cite class="ltx_cite ltx_citemacro_cite">
     Furuta et al. (
     <a class="ltx_ref" href="#bib.bib39" title="">
      2023
     </a>
     )
    </cite>
    also reports
    <math alttext="5.5\%" class="ltx_Math" display="inline" id="A2.p1.1.m1.1">
     <semantics id="A2.p1.1.m1.1a">
      <mrow id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">
       <mn id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml">
        5.5
       </mn>
       <mo id="A2.p1.1.m1.1.1.1" xref="A2.p1.1.m1.1.1.1.cmml">
        %
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b">
       <apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1">
        <csymbol cd="latexml" id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1.1">
         percent
        </csymbol>
        <cn id="A2.p1.1.m1.1.1.2.cmml" type="float" xref="A2.p1.1.m1.1.1.2">
         5.5
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">
       5.5\%
      </annotation>
     </semantics>
    </math>
    success rate improvement on the MiniWoB++ dataset
    <cite class="ltx_cite ltx_citemacro_cite">
     Shi et al. (
     <a class="ltx_ref" href="#bib.bib143" title="">
      2017
     </a>
     ); Liu et al. (
     <a class="ltx_ref" href="#bib.bib89" title="">
      2018
     </a>
     )
    </cite>
    by simply adding the image modality.
   </p>
  </div>
  <div class="ltx_para" id="A2.p2">
   <p class="ltx_p" id="A2.p2.1">
    Multi-modalities also help enhance the functionality of agent systems through more diverse information grounding. For example, Chain-of-Action
    <cite class="ltx_cite ltx_citemacro_cite">
     Pan et al. (
     <a class="ltx_ref" href="#bib.bib117" title="">
      2024
     </a>
     )
    </cite>
    advances the single-modal Search-in-the-Chain
    <cite class="ltx_cite ltx_citemacro_cite">
     Xu et al. (
     <a class="ltx_ref" href="#bib.bib190" title="">
      2023
     </a>
     )
    </cite>
    by supporting multi-modal data retrieval for faithful question answering. DoraemonGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Yang et al. (
     <a class="ltx_ref" href="#bib.bib194" title="">
      2024
     </a>
     )
    </cite>
    decomposes complex tasks into simpler ones toward understanding dynamic scenes, where multi-modal understanding is necessary for spatial-temporal videos analysis. RA-CM3
    <cite class="ltx_cite ltx_citemacro_cite">
     Yasunaga et al. (
     <a class="ltx_ref" href="#bib.bib200" title="">
      2022
     </a>
     )
    </cite>
    augments baseline retrieval-augmented LLMs with raw multi-modal documents that include both images and texts, assuming that these two modalities can contextualize each other and make the documents more informative, leading to better generator performance. The multi-modal capabilities also allow HuggingGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Shen et al. (
     <a class="ltx_ref" href="#bib.bib141" title="">
      2024b
     </a>
     )
    </cite>
    , Agent LUMOS
    <cite class="ltx_cite ltx_citemacro_cite">
     Yin et al. (
     <a class="ltx_ref" href="#bib.bib203" title="">
      2023
     </a>
     )
    </cite>
    , ToolAlpaca
    <cite class="ltx_cite ltx_citemacro_cite">
     Tang et al. (
     <a class="ltx_ref" href="#bib.bib162" title="">
      2023
     </a>
     )
    </cite>
    , and AssistGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Gao et al. (
     <a class="ltx_ref" href="#bib.bib42" title="">
      2023b
     </a>
     )
    </cite>
    to expand the scope of tasks they can address, including cooperation among specialized agents or tools capable of handling different information modalities.
   </p>
  </div>
  <div class="ltx_para" id="A2.p3">
   <p class="ltx_p" id="A2.p3.1">
    Large world models is an emerging and promising direction to reduce multi-modal hallucinations. The notion is also mentioned in “Objective-driven AI"
    <cite class="ltx_cite ltx_citemacro_cite">
     LeCun (
     <a class="ltx_ref" href="#bib.bib74" title="">
      2024
     </a>
     )
    </cite>
    , where agents have behavior driven by fulfilling objectives and they understand how the world works with common sense knowledge, beyond an auto-regressive generation. For example, Large World Model (LWM)
    <cite class="ltx_cite ltx_citemacro_cite">
     Liu et al. (
     <a class="ltx_ref" href="#bib.bib91" title="">
      2024b
     </a>
     )
    </cite>
    and Sora
    <cite class="ltx_cite ltx_citemacro_cite">
     Brooks et al. (
     <a class="ltx_ref" href="#bib.bib11" title="">
      2024
     </a>
     )
    </cite>
    develop insights from both textual knowledge and the world through video sequences. Although these models both advance toward general-purpose simulators of the world, they still lack reliable physical engines for guaranteed grounding in real-world dynamics. Ghost-in-the-Minecraft
    <cite class="ltx_cite ltx_citemacro_cite">
     Zhu et al. (
     <a class="ltx_ref" href="#bib.bib220" title="">
      2023b
     </a>
     )
    </cite>
    and Voyager
    <cite class="ltx_cite ltx_citemacro_cite">
     Wang et al. (
     <a class="ltx_ref" href="#bib.bib167" title="">
      2023a
     </a>
     )
    </cite>
    have agents living in a well-defined game-world environment. JEPA
    <cite class="ltx_cite ltx_citemacro_cite">
     LeCun (
     <a class="ltx_ref" href="#bib.bib73" title="">
      2022
     </a>
     )
    </cite>
    creates a recurrent world model in an abstract representation space.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Knowledge Retrieval &amp; Tool Usage
  </h2>
  <div class="ltx_para" id="A3.p1">
   <p class="ltx_p" id="A3.p1.4">
    CuriousLLM
    <cite class="ltx_cite ltx_citemacro_cite">
     Yang and Zhu (
     <a class="ltx_ref" href="#bib.bib195" title="">
      2024
     </a>
     )
    </cite>
    presents ablation studies showing the effectiveness of KGs on improving reasoning within the search process. MineDojo
    <cite class="ltx_cite ltx_citemacro_cite">
     Fan et al. (
     <a class="ltx_ref" href="#bib.bib36" title="">
      2022
     </a>
     )
    </cite>
    observes that internet-scale multi-modal knowledge allows models to significantly outperform all creative task baselines. Equipped with world knowledge, RA-CM3
    <cite class="ltx_cite ltx_citemacro_cite">
     Yasunaga et al. (
     <a class="ltx_ref" href="#bib.bib200" title="">
      2022
     </a>
     )
    </cite>
    can finally generate faithful images from captions compared to CM3
    <cite class="ltx_cite ltx_citemacro_cite">
     Aghajanyan et al. (
     <a class="ltx_ref" href="#bib.bib1" title="">
      2022
     </a>
     )
    </cite>
    and Stable Diffusion
    <cite class="ltx_cite ltx_citemacro_cite">
     Rombach et al. (
     <a class="ltx_ref" href="#bib.bib131" title="">
      2022
     </a>
     )
    </cite>
    . CooperKGC
    <cite class="ltx_cite ltx_citemacro_cite">
     Ye et al. (
     <a class="ltx_ref" href="#bib.bib201" title="">
      2023
     </a>
     )
    </cite>
    enables multi-agent collaborations, leveraging knowledge bases of different experts. It finds that the incorporation of KGs improves F1 scores by
    <math alttext="10.0" class="ltx_Math" display="inline" id="A3.p1.1.m1.1">
     <semantics id="A3.p1.1.m1.1a">
      <mn id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">
       10.0
      </mn>
      <annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b">
       <cn id="A3.p1.1.m1.1.1.cmml" type="float" xref="A3.p1.1.m1.1.1">
        10.0
       </cn>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">
       10.0
      </annotation>
     </semantics>
    </math>
    -
    <math alttext="33.6\%" class="ltx_Math" display="inline" id="A3.p1.2.m2.1">
     <semantics id="A3.p1.2.m2.1a">
      <mrow id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">
       <mn id="A3.p1.2.m2.1.1.2" xref="A3.p1.2.m2.1.1.2.cmml">
        33.6
       </mn>
       <mo id="A3.p1.2.m2.1.1.1" xref="A3.p1.2.m2.1.1.1.cmml">
        %
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b">
       <apply id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1">
        <csymbol cd="latexml" id="A3.p1.2.m2.1.1.1.cmml" xref="A3.p1.2.m2.1.1.1">
         percent
        </csymbol>
        <cn id="A3.p1.2.m2.1.1.2.cmml" type="float" xref="A3.p1.2.m2.1.1.2">
         33.6
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">
       33.6\%
      </annotation>
     </semantics>
    </math>
    across different backgrounds, and adding more collaboration rounds also enhance performance by about
    <math alttext="10.0" class="ltx_Math" display="inline" id="A3.p1.3.m3.1">
     <semantics id="A3.p1.3.m3.1a">
      <mn id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml">
       10.0
      </mn>
      <annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b">
       <cn id="A3.p1.3.m3.1.1.cmml" type="float" xref="A3.p1.3.m3.1.1">
        10.0
       </cn>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">
       10.0
      </annotation>
     </semantics>
    </math>
    -
    <math alttext="30.0\%" class="ltx_Math" display="inline" id="A3.p1.4.m4.1">
     <semantics id="A3.p1.4.m4.1a">
      <mrow id="A3.p1.4.m4.1.1" xref="A3.p1.4.m4.1.1.cmml">
       <mn id="A3.p1.4.m4.1.1.2" xref="A3.p1.4.m4.1.1.2.cmml">
        30.0
       </mn>
       <mo id="A3.p1.4.m4.1.1.1" xref="A3.p1.4.m4.1.1.1.cmml">
        %
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A3.p1.4.m4.1b">
       <apply id="A3.p1.4.m4.1.1.cmml" xref="A3.p1.4.m4.1.1">
        <csymbol cd="latexml" id="A3.p1.4.m4.1.1.1.cmml" xref="A3.p1.4.m4.1.1.1">
         percent
        </csymbol>
        <cn id="A3.p1.4.m4.1.1.2.cmml" type="float" xref="A3.p1.4.m4.1.1.2">
         30.0
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A3.p1.4.m4.1c">
       30.0\%
      </annotation>
     </semantics>
    </math>
    . DoraemonGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Yang et al. (
     <a class="ltx_ref" href="#bib.bib194" title="">
      2024
     </a>
     )
    </cite>
    supports knowledge tools to assist the understanding of specialized video contents. SIRI
    <cite class="ltx_cite ltx_citemacro_cite">
     Wang et al. (
     <a class="ltx_ref" href="#bib.bib175" title="">
      2023d
     </a>
     )
    </cite>
    builds a multi-view knowledge base to increase the explainability of visual question answering.
   </p>
  </div>
  <div class="ltx_para" id="A3.p2">
   <p class="ltx_p" id="A3.p2.1">
    A multi-agent system can coordinate agents understanding when and which tool to use, which modality of information the tool should expect, how to call the corresponding API, and how to incorporate outputs from the API calls, which anchors subsequent reasoning processes with more accurate information beyond their parametric memory. For example, VisProg
    <cite class="ltx_cite ltx_citemacro_cite">
     Gupta and Kembhavi (
     <a class="ltx_ref" href="#bib.bib49" title="">
      2023
     </a>
     )
    </cite>
    , ViperGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Surís et al. (
     <a class="ltx_ref" href="#bib.bib158" title="">
      2023
     </a>
     )
    </cite>
    , and Parsel
    <cite class="ltx_cite ltx_citemacro_cite">
     Zelikman et al. (
     <a class="ltx_ref" href="#bib.bib206" title="">
      2023
     </a>
     )
    </cite>
    generate Python programs to reliably execute subroutines.
    <cite class="ltx_cite ltx_citemacro_citet">
     Gupta and Kembhavi (
     <a class="ltx_ref" href="#bib.bib49" title="">
      2023
     </a>
     ); Surís et al. (
     <a class="ltx_ref" href="#bib.bib158" title="">
      2023
     </a>
     )
    </cite>
    also invoke off-the-shelf models for multimodal assistance.
   </p>
  </div>
  <div class="ltx_para" id="A3.p3">
   <p class="ltx_p" id="A3.p3.1">
    Foundation models are not specifically trained for object detection or segmentation, so BuboGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Zhao et al. (
     <a class="ltx_ref" href="#bib.bib211" title="">
      2023
     </a>
     )
    </cite>
    and Multi-Agent VQA
    <cite class="ltx_cite ltx_citemacro_cite">
     Jiang et al. (
     <a class="ltx_ref" href="#bib.bib64" title="">
      2024b
     </a>
     )
    </cite>
    call SAM
    <cite class="ltx_cite ltx_citemacro_cite">
     Kirillov et al. (
     <a class="ltx_ref" href="#bib.bib70" title="">
      2023
     </a>
     ); Ren et al. (
     <a class="ltx_ref" href="#bib.bib128" title="">
      2024
     </a>
     )
    </cite>
    as the tool.
Besides, BabyAGI
    <cite class="ltx_cite ltx_citemacro_cite">
     Nakajima (
     <a class="ltx_ref" href="#bib.bib111" title="">
      2023
     </a>
     )
    </cite>
    , Chamelon
    <cite class="ltx_cite ltx_citemacro_cite">
     Lu et al. (
     <a class="ltx_ref" href="#bib.bib101" title="">
      2024
     </a>
     )
    </cite>
    , AssistGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Gao et al. (
     <a class="ltx_ref" href="#bib.bib42" title="">
      2023b
     </a>
     )
    </cite>
    , Avis
    <cite class="ltx_cite ltx_citemacro_cite">
     Hu et al. (
     <a class="ltx_ref" href="#bib.bib59" title="">
      2024
     </a>
     )
    </cite>
    , ToolAlpaca
    <cite class="ltx_cite ltx_citemacro_cite">
     Tang et al. (
     <a class="ltx_ref" href="#bib.bib162" title="">
      2023
     </a>
     )
    </cite>
    , MetaGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Hong et al. (
     <a class="ltx_ref" href="#bib.bib56" title="">
      2023a
     </a>
     )
    </cite>
    , Agent LUMOS
    <cite class="ltx_cite ltx_citemacro_cite">
     Yin et al. (
     <a class="ltx_ref" href="#bib.bib203" title="">
      2023
     </a>
     )
    </cite>
    , AutoAct
    <cite class="ltx_cite ltx_citemacro_cite">
     Qiao et al. (
     <a class="ltx_ref" href="#bib.bib124" title="">
      2024
     </a>
     )
    </cite>
    ,
    <math alttext="\alpha" class="ltx_Math" display="inline" id="A3.p3.1.m1.1">
     <semantics id="A3.p3.1.m1.1a">
      <mi id="A3.p3.1.m1.1.1" xref="A3.p3.1.m1.1.1.cmml">
       α
      </mi>
      <annotation-xml encoding="MathML-Content" id="A3.p3.1.m1.1b">
       <ci id="A3.p3.1.m1.1.1.cmml" xref="A3.p3.1.m1.1.1">
        𝛼
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A3.p3.1.m1.1c">
       \alpha
      </annotation>
     </semantics>
    </math>
    -UMi
    <cite class="ltx_cite ltx_citemacro_cite">
     Shen et al. (
     <a class="ltx_ref" href="#bib.bib140" title="">
      2024a
     </a>
     )
    </cite>
    , and ConAgents
    <cite class="ltx_cite ltx_citemacro_cite">
     Shi et al. (
     <a class="ltx_ref" href="#bib.bib144" title="">
      2024
     </a>
     )
    </cite>
    harness compositional reasoning to enable generalized multi-agent systems with planning and modular tool-using capabilities in real-world scenarios.
   </p>
  </div>
  <div class="ltx_para" id="A3.p4">
   <p class="ltx_p" id="A3.p4.1">
    In most cases, tools require translating natural language queries into API calls with predefined syntax. Once the APIs and their input arguments are determined, the tools will ignore any irrelevant context in the original queries, as long as the queries share the same underlying logic necessary for the inputs. Take Multi-Agent VQA
    <cite class="ltx_cite ltx_citemacro_cite">
     Jiang et al. (
     <a class="ltx_ref" href="#bib.bib64" title="">
      2024b
     </a>
     )
    </cite>
    as an example. In this system, a language model provides only the relevant object names to the Grounded SAM
    <cite class="ltx_cite ltx_citemacro_cite">
     Ren et al. (
     <a class="ltx_ref" href="#bib.bib128" title="">
      2024
     </a>
     )
    </cite>
    component, which functions as an object detector, rather than passing the entire visual question. Similarly, the image editing tools in VisProg
    <cite class="ltx_cite ltx_citemacro_cite">
     Gupta and Kembhavi (
     <a class="ltx_ref" href="#bib.bib49" title="">
      2023
     </a>
     )
    </cite>
    only receive a fixed set of arguments translated from user queries to perform deterministic code executions. SeeAct
    <cite class="ltx_cite ltx_citemacro_cite">
     Zheng et al. (
     <a class="ltx_ref" href="#bib.bib213" title="">
      2024a
     </a>
     )
    </cite>
    as a Web agent explores vision-language models, ranking models, and a bounding box annotation tool to improve Web elements grounding from lengthy and noisy HTML codes.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Collective Deliberation among Agents
  </h2>
  <section class="ltx_subsection" id="A4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.1
    </span>
    More Examples on Multi-Agent Collaborations
   </h3>
   <div class="ltx_para" id="A4.SS1.p1">
    <p class="ltx_p" id="A4.SS1.p1.7">
     Corex
     <cite class="ltx_cite ltx_citemacro_cite">
      Sun et al. (
      <a class="ltx_ref" href="#bib.bib153" title="">
       2023a
      </a>
      )
     </cite>
     finds that orchestrating multiple agents to work together yields better complex reasoning results, exceeding strong single-agent baselines
     <cite class="ltx_cite ltx_citemacro_cite">
      Wang et al. (
      <a class="ltx_ref" href="#bib.bib174" title="">
       2022b
      </a>
      )
     </cite>
     by an average of
     <math alttext="1.1" class="ltx_Math" display="inline" id="A4.SS1.p1.1.m1.1">
      <semantics id="A4.SS1.p1.1.m1.1a">
       <mn id="A4.SS1.p1.1.m1.1.1" xref="A4.SS1.p1.1.m1.1.1.cmml">
        1.1
       </mn>
       <annotation-xml encoding="MathML-Content" id="A4.SS1.p1.1.m1.1b">
        <cn id="A4.SS1.p1.1.m1.1.1.cmml" type="float" xref="A4.SS1.p1.1.m1.1.1">
         1.1
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS1.p1.1.m1.1c">
        1.1
       </annotation>
      </semantics>
     </math>
     -
     <math alttext="10.6\%" class="ltx_Math" display="inline" id="A4.SS1.p1.2.m2.1">
      <semantics id="A4.SS1.p1.2.m2.1a">
       <mrow id="A4.SS1.p1.2.m2.1.1" xref="A4.SS1.p1.2.m2.1.1.cmml">
        <mn id="A4.SS1.p1.2.m2.1.1.2" xref="A4.SS1.p1.2.m2.1.1.2.cmml">
         10.6
        </mn>
        <mo id="A4.SS1.p1.2.m2.1.1.1" xref="A4.SS1.p1.2.m2.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.SS1.p1.2.m2.1b">
        <apply id="A4.SS1.p1.2.m2.1.1.cmml" xref="A4.SS1.p1.2.m2.1.1">
         <csymbol cd="latexml" id="A4.SS1.p1.2.m2.1.1.1.cmml" xref="A4.SS1.p1.2.m2.1.1.1">
          percent
         </csymbol>
         <cn id="A4.SS1.p1.2.m2.1.1.2.cmml" type="float" xref="A4.SS1.p1.2.m2.1.1.2">
          10.6
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS1.p1.2.m2.1c">
        10.6\%
       </annotation>
      </semantics>
     </math>
     .
Retroformer
     <cite class="ltx_cite ltx_citemacro_cite">
      Yao et al. (
      <a class="ltx_ref" href="#bib.bib199" title="">
       2023
      </a>
      )
     </cite>
     equips the single-agent Reflexion
     <cite class="ltx_cite ltx_citemacro_cite">
      Shinn et al. (
      <a class="ltx_ref" href="#bib.bib145" title="">
       2024
      </a>
      )
     </cite>
     algorithm with an additional LLM to generate verbal reinforcement cues and assist its self-improvement, enhancing accuracy by
     <math alttext="1.0" class="ltx_Math" display="inline" id="A4.SS1.p1.3.m3.1">
      <semantics id="A4.SS1.p1.3.m3.1a">
       <mn id="A4.SS1.p1.3.m3.1.1" xref="A4.SS1.p1.3.m3.1.1.cmml">
        1.0
       </mn>
       <annotation-xml encoding="MathML-Content" id="A4.SS1.p1.3.m3.1b">
        <cn id="A4.SS1.p1.3.m3.1.1.cmml" type="float" xref="A4.SS1.p1.3.m3.1.1">
         1.0
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS1.p1.3.m3.1c">
        1.0
       </annotation>
      </semantics>
     </math>
     -
     <math alttext="20.9\%" class="ltx_Math" display="inline" id="A4.SS1.p1.4.m4.1">
      <semantics id="A4.SS1.p1.4.m4.1a">
       <mrow id="A4.SS1.p1.4.m4.1.1" xref="A4.SS1.p1.4.m4.1.1.cmml">
        <mn id="A4.SS1.p1.4.m4.1.1.2" xref="A4.SS1.p1.4.m4.1.1.2.cmml">
         20.9
        </mn>
        <mo id="A4.SS1.p1.4.m4.1.1.1" xref="A4.SS1.p1.4.m4.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.SS1.p1.4.m4.1b">
        <apply id="A4.SS1.p1.4.m4.1.1.cmml" xref="A4.SS1.p1.4.m4.1.1">
         <csymbol cd="latexml" id="A4.SS1.p1.4.m4.1.1.1.cmml" xref="A4.SS1.p1.4.m4.1.1.1">
          percent
         </csymbol>
         <cn id="A4.SS1.p1.4.m4.1.1.2.cmml" type="float" xref="A4.SS1.p1.4.m4.1.1.2">
          20.9
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS1.p1.4.m4.1c">
        20.9\%
       </annotation>
      </semantics>
     </math>
     .
MetaAgents
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib87" title="">
       2023i
      </a>
      )
     </cite>
     effectively coordinate agents within task-oriented social contexts to achieve consistent behavior patterns, and the implementation of agent reflection in this system leads to a
     <math alttext="21.0\%" class="ltx_Math" display="inline" id="A4.SS1.p1.5.m5.1">
      <semantics id="A4.SS1.p1.5.m5.1a">
       <mrow id="A4.SS1.p1.5.m5.1.1" xref="A4.SS1.p1.5.m5.1.1.cmml">
        <mn id="A4.SS1.p1.5.m5.1.1.2" xref="A4.SS1.p1.5.m5.1.1.2.cmml">
         21.0
        </mn>
        <mo id="A4.SS1.p1.5.m5.1.1.1" xref="A4.SS1.p1.5.m5.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.SS1.p1.5.m5.1b">
        <apply id="A4.SS1.p1.5.m5.1.1.cmml" xref="A4.SS1.p1.5.m5.1.1">
         <csymbol cd="latexml" id="A4.SS1.p1.5.m5.1.1.1.cmml" xref="A4.SS1.p1.5.m5.1.1.1">
          percent
         </csymbol>
         <cn id="A4.SS1.p1.5.m5.1.1.2.cmml" type="float" xref="A4.SS1.p1.5.m5.1.1.2">
          21.0
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS1.p1.5.m5.1c">
        21.0\%
       </annotation>
      </semantics>
     </math>
     improvement in success rates. Multi-agent debating in
     <cite class="ltx_cite ltx_citemacro_citet">
      Khan et al. (
      <a class="ltx_ref" href="#bib.bib68" title="">
       2024
      </a>
      )
     </cite>
     also leads to more truthful answers, boosting single-agent baselines by
     <math alttext="28.0\%" class="ltx_Math" display="inline" id="A4.SS1.p1.6.m6.1">
      <semantics id="A4.SS1.p1.6.m6.1a">
       <mrow id="A4.SS1.p1.6.m6.1.1" xref="A4.SS1.p1.6.m6.1.1.cmml">
        <mn id="A4.SS1.p1.6.m6.1.1.2" xref="A4.SS1.p1.6.m6.1.1.2.cmml">
         28.0
        </mn>
        <mo id="A4.SS1.p1.6.m6.1.1.1" xref="A4.SS1.p1.6.m6.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.SS1.p1.6.m6.1b">
        <apply id="A4.SS1.p1.6.m6.1.1.cmml" xref="A4.SS1.p1.6.m6.1.1">
         <csymbol cd="latexml" id="A4.SS1.p1.6.m6.1.1.1.cmml" xref="A4.SS1.p1.6.m6.1.1.1">
          percent
         </csymbol>
         <cn id="A4.SS1.p1.6.m6.1.1.2.cmml" type="float" xref="A4.SS1.p1.6.m6.1.1.2">
          28.0
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS1.p1.6.m6.1c">
        28.0\%
       </annotation>
      </semantics>
     </math>
     .
Multi-Agent Collaboration
     <cite class="ltx_cite ltx_citemacro_cite">
      Talebirad and Nadiri (
      <a class="ltx_ref" href="#bib.bib159" title="">
       2023
      </a>
      )
     </cite>
     , ChatDev
     <cite class="ltx_cite ltx_citemacro_cite">
      Qian et al. (
      <a class="ltx_ref" href="#bib.bib122" title="">
       2023
      </a>
      )
     </cite>
     , AgentCF
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhang et al. (
      <a class="ltx_ref" href="#bib.bib207" title="">
       2023
      </a>
      )
     </cite>
     , AutoGen
     <cite class="ltx_cite ltx_citemacro_cite">
      Wu et al. (
      <a class="ltx_ref" href="#bib.bib184" title="">
       2023
      </a>
      )
     </cite>
     , Social Learning
     <cite class="ltx_cite ltx_citemacro_cite">
      Mohtashami et al. (
      <a class="ltx_ref" href="#bib.bib109" title="">
       2023
      </a>
      )
     </cite>
     , S
     <sup class="ltx_sup" id="A4.SS1.p1.7.1">
      <span class="ltx_text ltx_font_italic" id="A4.SS1.p1.7.1.1">
       3
      </span>
     </sup>
     <cite class="ltx_cite ltx_citemacro_cite">
      Gao et al. (
      <a class="ltx_ref" href="#bib.bib41" title="">
       2023a
      </a>
      )
     </cite>
     ,
     <cite class="ltx_cite ltx_citemacro_citet">
      Ke et al. (
      <a class="ltx_ref" href="#bib.bib67" title="">
       2024
      </a>
      )
     </cite>
     , and
     <cite class="ltx_cite ltx_citemacro_citet">
      Chern et al. (
      <a class="ltx_ref" href="#bib.bib20" title="">
       2024
      </a>
      )
     </cite>
     continue to push the frontier of a multi-agent system’s applications beyond daily conversation to a versatile set of real-world task completions.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.2
    </span>
    Collaboration Againt Jailbreaking
   </h3>
   <div class="ltx_para" id="A4.SS2.p1">
    <p class="ltx_p" id="A4.SS2.p1.1">
     LLMs are also sensitive to prompt perturbations due to token bias and noises
     <cite class="ltx_cite ltx_citemacro_cite">
      Sclar et al. (
      <a class="ltx_ref" href="#bib.bib135" title="">
       2023a
      </a>
      )
     </cite>
     . One of the most worrying examples are adversarial attacks
     <cite class="ltx_cite ltx_citemacro_cite">
      Gehman et al. (
      <a class="ltx_ref" href="#bib.bib44" title="">
       2020
      </a>
      ); Ganguli et al. (
      <a class="ltx_ref" href="#bib.bib40" title="">
       2022
      </a>
      ); Du et al. (
      <a class="ltx_ref" href="#bib.bib29" title="">
       2022
      </a>
      ); Wei et al. (
      <a class="ltx_ref" href="#bib.bib178" title="">
       2024
      </a>
      ); Perez et al. (
      <a class="ltx_ref" href="#bib.bib120" title="">
       2022
      </a>
      ); Zou et al. (
      <a class="ltx_ref" href="#bib.bib222" title="">
       2023
      </a>
      )
     </cite>
     through malicious prompt engineering. These attacks, also known as the Red Team Task, also named the Red Team Task, involve malicious prompt engineering designed to exploit vulnerabilities in the model. To combat this issue,
     <cite class="ltx_cite ltx_citemacro_citet">
      Chern et al. (
      <a class="ltx_ref" href="#bib.bib20" title="">
       2024
      </a>
      )
     </cite>
     propose a multi-agent debating approach involving agents with harmless, neutral, or harmful intentions. The authors demonstrate that engaging these agents in multi-round, multi-agent debate is more effective in improving the model’s robustness against adversarial prompt variations and perturbations compared to a single-agent with self-reflection prompts.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.3
    </span>
    Collaboration on LLM-based Evaluation
   </h3>
   <div class="ltx_para" id="A4.SS3.p1">
    <p class="ltx_p" id="A4.SS3.p1.4">
     LLM-based evaluation methods are popular in assessing open-ended language responses.
     <cite class="ltx_cite ltx_citemacro_citet">
      Stureborg et al. (
      <a class="ltx_ref" href="#bib.bib150" title="">
       2024
      </a>
      ); Koo et al. (
      <a class="ltx_ref" href="#bib.bib71" title="">
       2023
      </a>
      )
     </cite>
     point out LLMs often present cognitive biases in their evaluations, favoring certain types of responses over others regardless of the actual quality or relevance of the respective responses. To establish a more coherent preference orderability aligned with human preference.
ChatEval
     <cite class="ltx_cite ltx_citemacro_cite">
      Chan et al. (
      <a class="ltx_ref" href="#bib.bib14" title="">
       2023
      </a>
      )
     </cite>
     introduces a multi-agent debate framework to mimic human annotators collaborating in robust answer evaluations. Its multi-agent approach achieves greater alignment with human preferences compared to single-agent evaluations, enhancing accuracy by
     <math alttext="6.2\%" class="ltx_Math" display="inline" id="A4.SS3.p1.1.m1.1">
      <semantics id="A4.SS3.p1.1.m1.1a">
       <mrow id="A4.SS3.p1.1.m1.1.1" xref="A4.SS3.p1.1.m1.1.1.cmml">
        <mn id="A4.SS3.p1.1.m1.1.1.2" xref="A4.SS3.p1.1.m1.1.1.2.cmml">
         6.2
        </mn>
        <mo id="A4.SS3.p1.1.m1.1.1.1" xref="A4.SS3.p1.1.m1.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.SS3.p1.1.m1.1b">
        <apply id="A4.SS3.p1.1.m1.1.1.cmml" xref="A4.SS3.p1.1.m1.1.1">
         <csymbol cd="latexml" id="A4.SS3.p1.1.m1.1.1.1.cmml" xref="A4.SS3.p1.1.m1.1.1.1">
          percent
         </csymbol>
         <cn id="A4.SS3.p1.1.m1.1.1.2.cmml" type="float" xref="A4.SS3.p1.1.m1.1.1.2">
          6.2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS3.p1.1.m1.1c">
        6.2\%
       </annotation>
      </semantics>
     </math>
     for GPT-3.5 and
     <math alttext="2.5\%" class="ltx_Math" display="inline" id="A4.SS3.p1.2.m2.1">
      <semantics id="A4.SS3.p1.2.m2.1a">
       <mrow id="A4.SS3.p1.2.m2.1.1" xref="A4.SS3.p1.2.m2.1.1.cmml">
        <mn id="A4.SS3.p1.2.m2.1.1.2" xref="A4.SS3.p1.2.m2.1.1.2.cmml">
         2.5
        </mn>
        <mo id="A4.SS3.p1.2.m2.1.1.1" xref="A4.SS3.p1.2.m2.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.SS3.p1.2.m2.1b">
        <apply id="A4.SS3.p1.2.m2.1.1.cmml" xref="A4.SS3.p1.2.m2.1.1">
         <csymbol cd="latexml" id="A4.SS3.p1.2.m2.1.1.1.cmml" xref="A4.SS3.p1.2.m2.1.1.1">
          percent
         </csymbol>
         <cn id="A4.SS3.p1.2.m2.1.1.2.cmml" type="float" xref="A4.SS3.p1.2.m2.1.1.2">
          2.5
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS3.p1.2.m2.1c">
        2.5\%
       </annotation>
      </semantics>
     </math>
     for GPT-4, and an increase of
     <math alttext="16.3\%" class="ltx_Math" display="inline" id="A4.SS3.p1.3.m3.1">
      <semantics id="A4.SS3.p1.3.m3.1a">
       <mrow id="A4.SS3.p1.3.m3.1.1" xref="A4.SS3.p1.3.m3.1.1.cmml">
        <mn id="A4.SS3.p1.3.m3.1.1.2" xref="A4.SS3.p1.3.m3.1.1.2.cmml">
         16.3
        </mn>
        <mo id="A4.SS3.p1.3.m3.1.1.1" xref="A4.SS3.p1.3.m3.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.SS3.p1.3.m3.1b">
        <apply id="A4.SS3.p1.3.m3.1.1.cmml" xref="A4.SS3.p1.3.m3.1.1">
         <csymbol cd="latexml" id="A4.SS3.p1.3.m3.1.1.1.cmml" xref="A4.SS3.p1.3.m3.1.1.1">
          percent
         </csymbol>
         <cn id="A4.SS3.p1.3.m3.1.1.2.cmml" type="float" xref="A4.SS3.p1.3.m3.1.1.2">
          16.3
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS3.p1.3.m3.1c">
        16.3\%
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="10.0\%" class="ltx_Math" display="inline" id="A4.SS3.p1.4.m4.1">
      <semantics id="A4.SS3.p1.4.m4.1a">
       <mrow id="A4.SS3.p1.4.m4.1.1" xref="A4.SS3.p1.4.m4.1.1.cmml">
        <mn id="A4.SS3.p1.4.m4.1.1.2" xref="A4.SS3.p1.4.m4.1.1.2.cmml">
         10.0
        </mn>
        <mo id="A4.SS3.p1.4.m4.1.1.1" xref="A4.SS3.p1.4.m4.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.SS3.p1.4.m4.1b">
        <apply id="A4.SS3.p1.4.m4.1.1.cmml" xref="A4.SS3.p1.4.m4.1.1">
         <csymbol cd="latexml" id="A4.SS3.p1.4.m4.1.1.1.cmml" xref="A4.SS3.p1.4.m4.1.1.1">
          percent
         </csymbol>
         <cn id="A4.SS3.p1.4.m4.1.1.2.cmml" type="float" xref="A4.SS3.p1.4.m4.1.1.2">
          10.0
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.SS3.p1.4.m4.1c">
        10.0\%
       </annotation>
      </semantics>
     </math>
     in average Spearman and Kendall-Tau correlations
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhong et al. (
      <a class="ltx_ref" href="#bib.bib217" title="">
       2022
      </a>
      )
     </cite>
     with human judgements in GPT-4.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     D.4
    </span>
    The Orderability of Preferences Matters for LLM-based Evaluations
   </h3>
   <div class="ltx_para" id="A4.SS4.p1">
    <p class="ltx_p" id="A4.SS4.p1.1">
     This section talks about LLM-based evaluation rather than evaluating the rationality of LLMs discussed in Section
     <a class="ltx_ref" href="#S5" title="5 Evaluating Rationality of Agents ‣ Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     . Recent research underscores a critical need for more rational LLM-based evaluation methods, particularly for assessing open-ended language responses. CoBBLEr
     <cite class="ltx_cite ltx_citemacro_cite">
      Koo et al. (
      <a class="ltx_ref" href="#bib.bib71" title="">
       2023
      </a>
      )
     </cite>
     provides a cognitive bias benchmark for evaluating LLMs as evaluators, revealing a preference for their own outputs over those from other LLMs.
     <cite class="ltx_cite ltx_citemacro_citet">
      Stureborg et al. (
      <a class="ltx_ref" href="#bib.bib150" title="">
       2024
      </a>
      )
     </cite>
     argues that LLMs are biased evaluators towards more familiar tokens and previous predictions, and exhibit strong self-inconsistency in the score distribution.
     <cite class="ltx_cite ltx_citemacro_citet">
      Luo et al. (
      <a class="ltx_ref" href="#bib.bib103" title="">
       2023
      </a>
      ); Shen et al. (
      <a class="ltx_ref" href="#bib.bib139" title="">
       2023
      </a>
      ); Gao et al. (
      <a class="ltx_ref" href="#bib.bib43" title="">
       2023c
      </a>
      ); Wang et al. (
      <a class="ltx_ref" href="#bib.bib168" title="">
       2023b
      </a>
      ); Chen et al. (
      <a class="ltx_ref" href="#bib.bib16" title="">
       2023
      </a>
      ); Chiang and Lee (
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023
      </a>
      ); Zheng et al. (
      <a class="ltx_ref" href="#bib.bib215" title="">
       2024b
      </a>
      ); Fu et al. (
      <a class="ltx_ref" href="#bib.bib38" title="">
       2023
      </a>
      ); Liu et al. (
      <a class="ltx_ref" href="#bib.bib97" title="">
       2023b
      </a>
      )
     </cite>
     also point out the problem with a single LLM as the evaluator, with concerns over factual and rating inconsistencies, a high dependency on prompt design, a low correlation with human evaluations, and struggles with the comparison. As a result, having a coherent orderability of preferences aligned with human preference becomes increasingly important.
    </p>
   </div>
   <div class="ltx_para" id="A4.SS4.p2">
    <p class="ltx_p" id="A4.SS4.p2.1">
     Multi-agent systems might be a possible remedy. By involving multiple evaluative agents from diverse perspectives, it becomes possible to achieve a more balanced and consistent orderability of preferences. For instance, ChatEval
     <cite class="ltx_cite ltx_citemacro_cite">
      Chan et al. (
      <a class="ltx_ref" href="#bib.bib14" title="">
       2023
      </a>
      )
     </cite>
     posits that a multi-agent debate evaluation usually offers judgments that are better aligned with human annotators compared to single-agent ones.
     <cite class="ltx_cite ltx_citemacro_citet">
      Bai et al. (
      <a class="ltx_ref" href="#bib.bib4" title="">
       2024
      </a>
      )
     </cite>
     also finds decentralized methods yield fairer evaluation results.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A5">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix E
   </span>
   Neuro-Symbolic Reasoning
  </h2>
  <div class="ltx_para" id="A5.p1">
   <p class="ltx_p" id="A5.p1.1">
    Logic-LM
    <cite class="ltx_cite ltx_citemacro_cite">
     Pan et al. (
     <a class="ltx_ref" href="#bib.bib116" title="">
      2023
     </a>
     )
    </cite>
    combines problem formulating, symbolic reasoning, and result interpreting agents, where the symbolic reasoner empowers LLMs with deterministic symbolic solvers to perform inference, ensuring a correct answer is consistently chosen. Its multi-agent framework also encourages self-refinement that modifies logical formulation errors using error messages from the symbolic reasoner as the feedback.
Besides, SymbolicToM
    <cite class="ltx_cite ltx_citemacro_cite">
     Sclar et al. (
     <a class="ltx_ref" href="#bib.bib136" title="">
      2023b
     </a>
     )
    </cite>
    and KRISP
    <cite class="ltx_cite ltx_citemacro_cite">
     Marino et al. (
     <a class="ltx_ref" href="#bib.bib108" title="">
      2021
     </a>
     )
    </cite>
    construct explicit symbolic graphs and answer questions by retrieving nodes in the graph.
Binder
    <cite class="ltx_cite ltx_citemacro_cite">
     Cheng et al. (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2022
     </a>
     )
    </cite>
    , Parsel
    <cite class="ltx_cite ltx_citemacro_cite">
     Zelikman et al. (
     <a class="ltx_ref" href="#bib.bib206" title="">
      2023
     </a>
     )
    </cite>
    , LEFT
    <cite class="ltx_cite ltx_citemacro_cite">
     Hsu et al. (
     <a class="ltx_ref" href="#bib.bib58" title="">
      2024
     </a>
     )
    </cite>
    , and
    <cite class="ltx_cite ltx_citemacro_citet">
     Fang et al. (
     <a class="ltx_ref" href="#bib.bib37" title="">
      2024
     </a>
     )
    </cite>
    decompose tasks into planning, parsing, and execution, where the symbolic reasoning agents can help maintain a coherent order of preferences among symbolic options in the system outputs.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A6">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix F
   </span>
   Evaluating Rationality
  </h2>
  <section class="ltx_subsection" id="A6.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     F.1
    </span>
    Benchmarks for Hallucination
   </h3>
   <div class="ltx_para" id="A6.SS1.p1">
    <p class="ltx_p" id="A6.SS1.p1.1">
     Multiple evaluation benchmarks targeting language-only dialogue have been proposed, such as BEGIN
     <cite class="ltx_cite ltx_citemacro_cite">
      Dziri et al. (
      <a class="ltx_ref" href="#bib.bib33" title="">
       2022b
      </a>
      )
     </cite>
     , HaluEval
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib83" title="">
       2023e
      </a>
      )
     </cite>
     , DialFact
     <cite class="ltx_cite ltx_citemacro_cite">
      Gupta et al. (
      <a class="ltx_ref" href="#bib.bib48" title="">
       2021
      </a>
      )
     </cite>
     , FaithDial
     <cite class="ltx_cite ltx_citemacro_cite">
      Dziri et al. (
      <a class="ltx_ref" href="#bib.bib32" title="">
       2022a
      </a>
      )
     </cite>
     , AIS
     <cite class="ltx_cite ltx_citemacro_cite">
      Rashkin et al. (
      <a class="ltx_ref" href="#bib.bib126" title="">
       2023
      </a>
      )
     </cite>
     , and others
     <cite class="ltx_cite ltx_citemacro_cite">
      Zheng et al. (
      <a class="ltx_ref" href="#bib.bib216" title="">
       2023b
      </a>
      ); Das et al. (
      <a class="ltx_ref" href="#bib.bib26" title="">
       2023
      </a>
      ); Cao et al. (
      <a class="ltx_ref" href="#bib.bib13" title="">
       2021
      </a>
      )
     </cite>
     . In contrast,
     <span class="ltx_text ltx_font_italic" id="A6.SS1.p1.1.1">
      benchmarks on multi-agent frameworks beyond language dialogue or those involving multi-modalities are very limited.
     </span>
     <cite class="ltx_cite ltx_citemacro_citet">
      Liu et al. (
      <a class="ltx_ref" href="#bib.bib90" title="">
       2024a
      </a>
      )
     </cite>
     moves beyond conversation to code generation; EureQA
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib77" title="">
       2023a
      </a>
      )
     </cite>
     focuses on reasoning chains; and TofuEval
     <cite class="ltx_cite ltx_citemacro_cite">
      Tang et al. (
      <a class="ltx_ref" href="#bib.bib161" title="">
       2024
      </a>
      )
     </cite>
     evaluates hallucination in multi-domain summarization. Object hallucination
     <cite class="ltx_cite ltx_citemacro_cite">
      Rohrbach et al. (
      <a class="ltx_ref" href="#bib.bib130" title="">
       2018
      </a>
      ); Biten et al. (
      <a class="ltx_ref" href="#bib.bib10" title="">
       2022
      </a>
      )
     </cite>
     , POPE
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib85" title="">
       2023g
      </a>
      )
     </cite>
     , and LLaVA-RLHF
     <cite class="ltx_cite ltx_citemacro_cite">
      Sun et al. (
      <a class="ltx_ref" href="#bib.bib156" title="">
       2023b
      </a>
      )
     </cite>
     are the few examples evaluating multi-modal hallucination.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A6.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     F.2
    </span>
    Perturbation Techniques
   </h3>
   <div class="ltx_para" id="A6.SS2.p1">
    <p class="ltx_p" id="A6.SS2.p1.1">
     Perturbation techniques typically involve some versions of paraphrasing or permutation. Paraphrasing includes changing the instruction templates
     <cite class="ltx_cite ltx_citemacro_cite">
      Weber et al. (
      <a class="ltx_ref" href="#bib.bib176" title="">
       2023
      </a>
      )
     </cite>
     , rewording task descriptions
     <cite class="ltx_cite ltx_citemacro_citep">
      (Yang et al.,
      <a class="ltx_ref" href="#bib.bib192" title="">
       2023
      </a>
      ; Ohmer et al.,
      <a class="ltx_ref" href="#bib.bib113" title="">
       2024
      </a>
      ; Wang et al.,
      <a class="ltx_ref" href="#bib.bib170" title="">
       2024b
      </a>
      )
     </cite>
     , translating the prompts into a different language
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ohmer et al.,
      <a class="ltx_ref" href="#bib.bib112" title="">
       2023
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib113" title="">
       2024
      </a>
      ; Xu et al.,
      <a class="ltx_ref" href="#bib.bib189" title="">
       2024a
      </a>
      )
     </cite>
     and then back to the original language
     <cite class="ltx_cite ltx_citemacro_cite">
      Yang et al. (
      <a class="ltx_ref" href="#bib.bib192" title="">
       2023
      </a>
      )
     </cite>
     , and making subtle changes to entities in task descriptions without affecting the logical structure, like altering names of the characters, numerical values in math problems, or locations of the events
     <cite class="ltx_cite ltx_citemacro_cite">
      Wang et al. (
      <a class="ltx_ref" href="#bib.bib170" title="">
       2024b
      </a>
      )
     </cite>
     . Permutation also includes reordering in-context learning examples
     <cite class="ltx_cite ltx_citemacro_cite">
      Lu et al. (
      <a class="ltx_ref" href="#bib.bib102" title="">
       2021
      </a>
      ); Pecher et al. (
      <a class="ltx_ref" href="#bib.bib119" title="">
       2024
      </a>
      )
     </cite>
     and, in the case of multiple-choice questions, rearranging the options
     <cite class="ltx_cite ltx_citemacro_citep">
      (Zong et al.,
      <a class="ltx_ref" href="#bib.bib221" title="">
       2023
      </a>
      ; Zheng et al.,
      <a class="ltx_ref" href="#bib.bib214" title="">
       2023a
      </a>
      )
     </cite>
     .
    </p>
   </div>
  </section>
 </section>
</article>
