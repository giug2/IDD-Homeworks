<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</title>
<!--Generated on Mon Sep 30 16:14:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.03843v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S1" title="In POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Related works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S1.SS1" title="In 1 Related works ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Prompt Engineering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S1.SS2" title="In 1 Related works ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Multimodal Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S1.SS3" title="In 1 Related works ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Visual Analytics for model understanding and steering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S2" title="In POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Design Requirements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3" title="In POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>System &amp; Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS1" title="In 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>System Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS2" title="In 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Dataset and Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS3" title="In 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Multimodal Rationale Understanding</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS3.SSS1" title="In 3.3 Multimodal Rationale Understanding ‣ 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Modality Interaction Characterization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS3.SSS2" title="In 3.3 Multimodal Rationale Understanding ‣ 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Multimodal Reasoning Pattern Mining</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS4" title="In 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Prompt Iteration Strategy Recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS4.SSS1" title="In 3.4 Prompt Iteration Strategy Recommendation ‣ 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>K-shot Example Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS4.SSS2" title="In 3.4 Prompt Iteration Strategy Recommendation ‣ 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Instructional Principle Generation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S4" title="In POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Interface Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S4.SS1" title="In 4 Interface Design ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Prompt Panel</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S4.SS2" title="In 4 Interface Design ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Reasoning Panel</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S4.SS3" title="In 4 Interface Design ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation Panel</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5" title="In POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.SS1" title="In 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.SS2" title="In 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Case Two: Enhancing Multimodal User Intention Understanding with WTaG dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.SS3" title="In 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Expert Interviews</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S6" title="In POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S7" title="In POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\authorfooter</span>
</div>
<h1 class="ltx_title ltx_title_document">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jianben He
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Xingbo Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Shiyi Liu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Guande Wu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Claudio Silva
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> and Huamin Qu
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="1.1">Large language models (LLMs) have exhibited impressive abilities for multimodal content comprehension and reasoning with proper prompting in zero- or few-shot settings.
Despite the proliferation of interactive systems developed to support prompt engineering for LLMs across various tasks, most have primarily focused on textual or visual inputs, thus neglecting the complex interplay between modalities within multimodal inputs.
This oversight hinders the development of effective prompts that guide models’ multimodal reasoning processes by fully exploiting the rich context provided by multiple modalities.
In this paper, we present <span class="ltx_ERROR undefined" id="1.1.1">\name</span>, a visual analytics system to facilitate efficient prompt engineering for <span class="ltx_ERROR undefined" id="1.1.2">\jianben</span>steering the multimodal reasoning performance of LLMs.
The system enables users to explore the interaction patterns across modalities at varying levels of detail for a comprehensive understanding of the multimodal knowledge elicited by various prompts.
Through diverse recommendations of demonstration examples and instructional principles, <span class="ltx_ERROR undefined" id="1.1.3">\name</span>supports users in iteratively crafting and refining prompts to better align and enhance model knowledge with human insights.
The effectiveness and efficiency of our system are validated through two case studies and interviews with experts.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>prompt engineering, multimodal reasoning, multimodal large language models
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\teaser</span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="p2.g1" src="extracted/5886160/figs/teaser.png" width="598"/>
<p class="ltx_p ltx_align_center" id="p2.2"><span class="ltx_text ltx_caption" id="p2.2.1">
The <span class="ltx_ERROR undefined" id="p2.2.1.1">\name</span>interface consists of three major panels. The <span class="ltx_text ltx_font_italic" id="p2.2.1.2">Prompt Panel</span> (A) offers versatile operations for users to efficiently craft and edit prompt content, such as importing various principles and demonstration examples, to support an effortless prompt engineering experience.
The <span class="ltx_text ltx_font_italic" id="p2.2.1.3">Reasoning Panel</span> (B) facilitates a comprehensive multi-level investigation of the model’s multimodal reasoning performance, ranging from the global modality interaction level to the local instance level.
The <span class="ltx_text ltx_font_italic" id="p2.2.1.4">Evaluation Panel</span> (C) supports both global and local evaluation of prompts, coupled with detailed documentation of modifications during prompt iterations for continuous monitoring and comparison.
</span>
</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">Introduction
Large Language Models (LLMs), pre-trained on massive data with billions of parameters, have become a cornerstone for natural language processing.
They encode extensive knowledge about the world in their parameter space, exhibiting impressive capabilities in text understanding, reasoning, and generation across various downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib62" title="">62</a>]</cite>.
Building on the strength of LLMs, there are an increasing number of works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib55" title="">55</a>]</cite>
exploring their applications in a wide spectrum of multimodal tasks (e.g., multimodal scene understanding and question answering).
By using text as the universal representation,
these works aim to leverage LLMs to integrate and analyze knowledge distilled from diverse modalities (e.g., audio and images) in text format and provide a holistic understanding of multimodal content.
These models are also known as multimodal LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib49" title="">49</a>]</cite>.
Comprehending multimodal content necessitates extensive multimodal knowledge, where models not only need to understand the information presented in each individual modality but also have to correctly infer how the information combines to inform accurate reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib63" title="">63</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">Recently, prompting has emerged as a data-efficient and user-friendly paradigm for steering and <span class="ltx_ERROR undefined" id="p4.1.1">\jianben</span>improving LLM’s performance on complex reasoning tasks. Relying on extensive knowledge acquired during pre-training, the models can instantly adapt to new downstream tasks in the few-shot or even zero-shot settings without the need for model retraining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib33" title="">33</a>]</cite>.
Moreover, the LLMs can be prompted to generate <span class="ltx_ERROR undefined" id="p4.1.2">\jianben</span>free-text rationales emulating human thought processes in the chain-of-thought (CoT) manner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib5" title="">5</a>]</cite>.
For example, they can provide step-by-step derivations that lead to the final answer of a math problem or substantiate their analysis process with supported evidence such as emotive words for sentiment prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib73" title="">73</a>]</cite>.
These human-readable rationales enhance the accuracy and transparency of the model’s reasoning process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib42" title="">42</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">While multimodal LLMs exhibit remarkable performance in various tasks with prompting, their reasoning performance is notably sensitive to prompt variations. Moreover, inadequate or ill-designed prompts may elicit erroneous knowledge, resulting in biased and unreliable reasoning.
Devising well-performing prompts that can guide and <span class="ltx_ERROR undefined" id="p5.1.1">\jianben</span>improve the multimodal reasoning performance of LLMs remains a persistent challenge.
Prompting is inherently a process requiring expertise and trial-and-error, where users need to meticulously craft prompts, scrutinize the outputs to identify flaws requiring improvement, and iteratively refine the prompts to reach the intended outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib12" title="">12</a>]</cite>.
During the process, <span class="ltx_text ltx_font_italic" id="p5.1.2">users first face the challenge of systematically understanding and examining the multimodal reasoning performance of different prompts. </span>
Manually inspecting each instance is not only time-consuming but also fails to provide a holistic understanding. Therefore, summarizing and presenting generated rationales at varying detail levels is non-trivial for users to fully verify outputs and pinpoint problematic aspects. However, in the multimodal context, the complex interplay among different modalities, coupled with the unstructured and generative nature of free-text <span class="ltx_ERROR undefined" id="p5.1.3">\jianben</span>rationales, makes interpreting multimodal LLMs’ reasoning process particularly challenging.
Furthermore, <span class="ltx_text ltx_font_italic" id="p5.1.4">users also struggle to revise their prompts in a way that effectively incorporates and elicits the desired multimodal reasoning knowledge from the model</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib21" title="">21</a>]</cite>. Well-clarified task instructions (e.g., format, phrasing, and content) and informative demonstration examples are both imperative for enabling LLMs to grasp the intended input-output relationships and generate consistent outputs with correct rationales <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib43" title="">43</a>]</cite>.
Considering the huge space of possible task instructions and the difficulty of selecting and annotating demonstration examples from high-dimensional and information-complex multimodal data, it is important yet challenging to facilitate users to craft and refine prompts in an efficient manner.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">To tackle the above challenges, we present <span class="ltx_ERROR undefined" id="p6.1.1">\name</span>, a visual analytics approach designed to streamline the process of prompt engineering for <span class="ltx_ERROR undefined" id="p6.1.2">\jianben</span>model practitioners, including model developers and model users, to systematically probe and steer the multimodal reasoning performance of LLMs for targeted downstream tasks.
<span class="ltx_text" id="p6.1.3" style="color:#000000;"> To build a comprehensive understanding of LLMs’ knowledge and reasoning on multimodal tasks, we develop computational methods to decompose and summarize cross-modal interactions captured by LLMs in various levels of detail.
At the modality level, we adopt a three-layer augmented Sankey diagram to contextualize model performance with complement and conflict interactions between modalities.
Then, drilling down into specific interactions, we distill and summarize the linguistic and visual evidence from individual instances to reflect model reasoning patterns.
These visualizations help align the model’s knowledge and reasoning processes with the human understanding at scale.</span>
<span class="ltx_text" id="p6.1.4" style="color:#000000;"> Based on the multi-level model understanding, <span class="ltx_ERROR undefined" id="p6.1.4.1">\name</span> allows users to conduct both top-down and bottom-up approaches to build and refine prompts that guide LLM’s multimodal reasoning.</span>
Specifically, we employ an effective sampling strategy for demonstration examples, ensuring a balance between relevance and diversity to provide varied and informative input-output mappings for inductive model learning.
On the other side, drawing on human innate capabilities for summarization and generalization, we incorporate an LLM-assisted module that distills principles at both instance-specific and agnostic levels. This approach facilitates users to precisely articulate and apply their domain-specific knowledge and expertise to guide the model deductively.</p>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p" id="p7.1">Our contributions are summarized as follows:
</p>
<ul class="ltx_itemize" id="S0.I1">
<li class="ltx_item" id="S0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i1.p1">
<p class="ltx_p" id="S0.I1.i1.p1.1">We propose an effective human-in-the-loop workflow that facilitates systematic investigation and guidance of the multimodal reasoning performance of LLMs.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i2.p1">
<p class="ltx_p" id="S0.I1.i2.p1.1">We develop a visual analytics system <span class="ltx_ERROR undefined" id="S0.I1.i2.p1.1.1">\name</span>, equipped with carefully designed visualizations and interactions to support efficient prompt engineering for multimodal reasoning tasks.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i3.p1">
<p class="ltx_p" id="S0.I1.i3.p1.1">We conduct two case studies and expert interviews to demonstrate the usefulness and efficiency of <span class="ltx_ERROR undefined" id="S0.I1.i3.p1.1.1">\name</span>.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Related works</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The research studies related to the design of <span class="ltx_ERROR undefined" id="S1.p1.1.1">\name</span> include prompt engineering, multimodal reasoning, and visual analytics for model understanding and steering.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Prompt Engineering</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Equipped with extensive knowledge acquired during pre-training, LLMs (e.g., GPT-<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib8" title="">8</a>]</cite> and LLaMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib51" title="">51</a>]</cite> series models)
exhibit remarkable adaptability to specialized downstream tasks such as question answering, content retrieval, and complex reasoning, given precise instructions and proper demonstration examples.
This emerging paradigm, known as prompting or prompt engineering, offers a user-friendly and data-efficient way for non-expert users to interact and steer large models.
Prompting generally includes instruction-based and example-based prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib4" title="">4</a>]</cite>. Instruction-based prompts include system prompts that provide general guidelines and task prompts that deliver direct and task-specific instructions. Example-based prompts utilize a small set of examples to showcase the desired input-output patterns for models to follow.
Numerous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib59" title="">59</a>]</cite> have highlighted two major challenges in prompting: the formulation of effective prompts, and the assessment of prompt efficacy alongside strategies for enhancement.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">Many studies have been conducted to address the prompting challenges.
Strobelt <span class="ltx_text ltx_font_italic" id="S1.SS1.p2.1.1">et al.</span>introduced PromptIDE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib48" title="">48</a>]</cite> as a tool for rapid exploration and assessment of variations in prompt templates.
KnowledgeVis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib10" title="">10</a>]</cite> compared multiple fill-in-the-blank prompts to probe the input-output associations in BERT-based models.
Beyond expediting the wording and phrase structure refinement, ScatterShot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib59" title="">59</a>]</cite> proposed a slice-based sampling strategy to identify the most informative data patterns for human annotation.
PromptAid <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib43" title="">43</a>]</cite> combined multiple prompt perturbation strategies to find satisfactory prompts for text classification tasks.
In addition, PromptChainer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib60" title="">60</a>]</cite> and AI Chains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib61" title="">61</a>]</cite> have been developed to support more sophisticated tasks by decomposing them into manageable sub-tasks, and supported prompt chain prototyping and authoring to enhance controllability.
Kim<span class="ltx_text ltx_font_italic" id="S1.SS1.p2.1.2">et al.</span>proposed EvaLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib22" title="">22</a>]</cite> for iterative prompt evaluation according to user-defined criteria, while ContitutionMaker <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib44" title="">44</a>]</cite> converted users’ natural language feedback into a principle for chatbot behavior customization.
Besides text-to-text generative tasks, several works facilitated prompt refinement for text-to-image generation by keywords <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib13" title="">13</a>]</cite> and style description recommendation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib7" title="">7</a>]</cite>, structured search of visual concepts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib35" title="">35</a>]</cite>, and rubric-based adjustment for precise emotion expression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib54" title="">54</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">However, existing interactive prompt engineering systems are limited to text-to-text or text-to-image generation tasks, failing to deal with the complexity of multimodal inputs for more sophisticated reasoning tasks.
In this paper, we develop <span class="ltx_ERROR undefined" id="S1.SS1.p3.1.1">\name</span>to optimize the prompt engineering process for adapting and steering the multimodal reasoning performance of LLMs.
<span class="ltx_ERROR undefined" id="S1.SS1.p3.1.2">\name</span>facilitates a comprehensive investigation of prompt effects and provides diverse support for users to iterate prompts with reduced cognitive burden and increased efficiency.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Multimodal Reasoning</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">Reasoning generally refers to the process of drawing on evidence to make logical inferences based on existing knowledge for prediction and decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib49" title="">49</a>]</cite>.
In the multimodal context, it is imperative for models to not only grasp evidence derived from single modalities but also to comprehend how evidence from different modalities relates to each other.
This comprehension could lead to the generation of new insights that the models must capture to achieve accurate reasoning.
Recently, LLMs have demonstrated the capability to generate coherent <span class="ltx_ERROR undefined" id="S1.SS2.p1.1.1">\jianben</span>rationales through Chain of Thought (CoT) prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib57" title="">57</a>]</cite>, <span class="ltx_ERROR undefined" id="S1.SS2.p1.1.2">\jianben</span>where LLMs provide the intermediate reasoning steps in natural language that lead to the final answer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib73" title="">73</a>]</cite>.
These generated <span class="ltx_ERROR undefined" id="S1.SS2.p1.1.3">\jianben</span>free-text rationales have been increasingly explored for model <span class="ltx_ERROR undefined" id="S1.SS2.p1.1.4">\jianben</span>interpretability, as they provide an explicit and transparent way to communicate the decision-making process of models to end-users in a human-like manner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib42" title="">42</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1">A growing number of benchmark datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib14" title="">14</a>]</cite> have been proposed to evaluate the capabilities of LLMs in multimodal reasoning tasks, with a primary focus on visual content understanding like Visual Question Answering (<span class="ltx_ERROR undefined" id="S1.SS2.p2.1.1">\jianben</span>i.e., answering text questions based solely on visual content). However, the comprehension of how LLMs integrate and coordinate information from various modalities (visual + language, or additional modalities) in the given context for question answering and reasoning remains under-explored. This includes tasks that necessitate a nuanced understanding of multimodal contexts, such as multimodal scene comprehension and multimodal sentiment analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib63" title="">63</a>]</cite>.
Moreover, the metrics on these benchmarks fail to capture the detailed reasoning process of models for in-depth model understanding and diagnosis.
Besides evaluation, many works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib55" title="">55</a>]</cite> have tried to steer multimodal LLMs’ reasoning abilities.
Compared with the labor-intensive fine-tuning approaches involving curating specific datasets with additional reasoning chain annotation, the training-free prompting-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib71" title="">71</a>]</cite> have become prevalent.
However, these automatic techniques fall short of providing fine-grained prompt evaluation and flexible prompt refinement.
Instead, we present a human-in-the-loop approach where users can interactively examine, evaluate, and refine prompts to guide and <span class="ltx_ERROR undefined" id="S1.SS2.p2.1.2">\jianben</span>steer model performance in a more interpretable and controllable manner.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Visual Analytics for model understanding and steering</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">Visual Analytics has proved to be an effective approach to help users understand and steer machine learning models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib62" title="">62</a>]</cite>.
Prior works aimed to disclose the functionalities of neurons and layers of diverse neural network models like RNNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib17" title="">17</a>]</cite>.
<span class="ltx_ERROR undefined" id="S1.SS3.p1.1.1">\jianben</span>Recently, many works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib34" title="">34</a>]</cite> have sought to elucidate the attention mechanism to understand the inner workings of transformer-based models in reasoning and decision-making process.
Beyond visualizing model internals, numerous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib26" title="">26</a>]</cite> have tried to probe model knowledge through analyzing post-hoc model behaviors with input variations.
For example,
<span class="ltx_ERROR undefined" id="S1.SS3.p1.1.2">\jianben</span>M2Lens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib52" title="">52</a>]</cite> and MultiViz <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib31" title="">31</a>]</cite> characterized intra- and inter-modal interactions with aggregated feature importance for multimodal model diagnosis.
The What-If Tool <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib58" title="">58</a>]</cite> and SliceTeller <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib72" title="">72</a>]</cite> identified specific data slices to understand model failures.
Integrated tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib26" title="">26</a>]</cite> have also been developed for unified language model evaluation.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p" id="S1.SS3.p2.1">Beyond mere understanding, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib53" title="">53</a>]</cite> have progressed to align model behavior with human knowledge, thereby adapting and steering models to generate desired outcomes for specific tasks. SharedInterest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib6" title="">6</a>]</cite> designed quantitative metrics using saliency methods to compare human and model reasoning for identifying recurring model behavior patterns.
Hoque<span class="ltx_text ltx_font_italic" id="S1.SS3.p2.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib19" title="">19</a>]</cite> and He<span class="ltx_text ltx_font_italic" id="S1.SS3.p2.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib16" title="">16</a>]</cite> employed data programming concepts to inject human knowledge at scale for model improvement.
CommonsenseVis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib53" title="">53</a>]</cite> constructed knowledge graphs with external knowledge bases to contextualize model <span class="ltx_ERROR undefined" id="S1.SS3.p2.1.3">\jianben</span>reasoning behaviors and allow interactive model editing to enhance specific knowledge for poorly behaved areas.
<span class="ltx_ERROR undefined" id="S1.SS3.p2.1.4">\jianben</span>Our work expands on these ideas to examine post-hoc model behaviors with varied prompt inputs for comprehending how different prompts affect model performance. It further enables model practitioners to provide feedback and align model performance with their knowledge and expertise through iterative prompting.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Design Requirements</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="197" id="S2.F1.g1" src="extracted/5886160/figs/system_workflow_v3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.4.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.5.2" style="font-size:90%;">The <span class="ltx_ERROR undefined" id="S2.F1.5.2.1">\name</span>system framework comprises four primary modules. (A) The visual and language modality information from the multimodal video dataset is processed by expert models, which are then fused and fed into multimodal LLMs. (B) The multimodal reasoning understanding module summarized the nuanced modality interactions and patterns at global and group levels. (C) The prompt iteration strategy recommendation panel provides diverse support for prompt refinement with semi-automatic k-shot example construction and instructional principle generation. (D) The <span class="ltx_ERROR undefined" id="S2.F1.5.2.2">\name</span>interface facilitates efficient prompt performance examination, prompt refinement assistance, and prompt monitoring and comparison.
sys </span></figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our goal is to develop a visual analytics approach that <span class="ltx_ERROR undefined" id="S2.p1.1.1">\jianben</span>streamlines prompt engineering, empowering model practitioners to <span class="ltx_ERROR undefined" id="S2.p1.1.2">\jianben</span>efficiently adapt and steer the multimodal reasoning performance of LLMs for targeted downstream tasks.
<span class="ltx_ERROR undefined" id="S2.p1.1.3">\jianben</span>By systematically understanding how models integrate multimodal information for reasoning, users can evaluate and enhance knowledge in underperforming areas through proper prompt design <span class="ltx_ERROR undefined" id="S2.p1.1.4">\jianben</span>informed by domain expertise.
To better understand users’ requirements for system design, we worked closely with four experts in NLP and multimodal machine learning (<span class="ltx_text ltx_font_bold" id="S2.p1.1.5">E1-E4</span>, <span class="ltx_text ltx_font_bold" id="S2.p1.1.6">E1</span> is the coauthor).
<span class="ltx_text ltx_font_bold" id="S2.p1.1.7">E1</span> is a researcher specializing in developing interactive systems for NLP and multimodal model analysis.
<span class="ltx_text ltx_font_bold" id="S2.p1.1.8">E2</span> is an industry researcher responsible for applying and developing multimodal models for real-world applications.
<span class="ltx_text ltx_font_bold" id="S2.p1.1.9">E3</span> and <span class="ltx_text ltx_font_bold" id="S2.p1.1.10">E4</span> are Ph.D. candidates with multiple top conference publications in the areas of multimodal machine learning and multimodal LLMs.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">All experts concurred that there is a lack of tools for systematically analyzing the multimodal reasoning performance of LLMs.
The current practice typically begins with observing the model’s overall performance metrics, followed by randomly sampling instances to examine reasoning correctness.
While few datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib27" title="">27</a>]</cite> provide expert-written rationales as ground truth, the intricate interplay across modalities and extensive variability in free-text expressions makes it challenging to systematically understand the knowledge models use for reasoning and pinpoint their weaknesses.
Moreover, crafting and refining prompts to effectively elicit the desired knowledge <span class="ltx_ERROR undefined" id="S2.p2.1.1">\jianben</span>from models for specific tasks often require labor-intensive and tedious prompt iterations.
Consequently, an integrated tool is desired to facilitate systematic investigation of model behaviors at various levels and support well-informed prompt iterations with less cognitive effort.
The design requirements are summarized as follows:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.1.1.1">R1</span></span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Summarize the impact of prompts on multimodal reasoning performance across varying levels of detail</span>
<span class="ltx_ERROR undefined" id="S2.I1.i1.p1.1.2">\jianben</span>When evaluating the reasoning performance of different prompts, users focus not only on overall statistics but also on how well the model’s reasoning aligns with established knowledge at group and instance level.
Therefore, it is crucial for the system to support multi-level and multi-faceted investigation of the model’s multimodal reasoning performance.
Initially, the system should present a global overview of model performance. <span class="ltx_ERROR undefined" id="S2.I1.i1.p1.1.3">\jianben</span>As <span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.4">E1</span> noted, “understanding how different modalities interact is crucial for interpreting the model’s behavior in the context of multimodal reasoning.”
Users need to recognize the modalities the model relies on for its decisions and how the model behaves when different modalities present complementary or contradictory information.
After <span class="ltx_ERROR undefined" id="S2.I1.i1.p1.1.5">\jianben</span>gaining a global understanding, users also need <span class="ltx_ERROR undefined" id="S2.I1.i1.p1.1.6">\jianben</span>insights into how evidence from distinct modalities and their combinations influence the model. For example, <span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.7">E3</span> expressed interest in identifying which types of visual cues or spoken words the model interprets as key indicators during reasoning. Besides, users need to inspect the model’s output at the instance level to intuitively understand and <span class="ltx_ERROR undefined" id="S2.I1.i1.p1.1.8">\jianben</span>verify the alignment of rationales with the original data.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.1.1.1">R2</span></span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Provide comparative analysis of different prompt performance</span>
Multiple aspects of prompts influence model reasoning performance, including the structure and content of <span class="ltx_ERROR undefined" id="S2.I1.i2.p1.1.2">\jianben</span>task-specific instructions, as well as the choice and order of demonstration examples.
<span class="ltx_ERROR undefined" id="S2.I1.i2.p1.1.3">\jianben</span>Navigating and exploring the evolving dynamics of prompts is necessary for users to “identify influential factors for improvement”, as <span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.4">E2</span> commented.
Therefore, it is imperative for the system to document prompt alternations, support streamlined prompt testing, and assist users in tracking and comparing the effects of diverse prompt modifications throughout the refinement process.
This process facilitates an understanding of how different modifications impact model reasoning performance, thereby offering valuable insights for users to provide appropriate feedback and make informed decisions regarding subsequent iterations.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.1.1.1">R3</span></span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Facilitate effective prompt refinement in diverse and efficient manner</span>
After pinpointing areas of underperformance, users can align and elicit model knowledge through refining prompts.
This refinement <span class="ltx_ERROR undefined" id="S2.I1.i3.p1.1.2">\jianben</span>includes providing more precise <span class="ltx_ERROR undefined" id="S2.I1.i3.p1.1.3">\jianben</span>task and scenario descriptions, clear outlining principles for the model to follow, and supplementing with informative <span class="ltx_ERROR undefined" id="S2.I1.i3.p1.1.4">\jianben</span>demonstration examples that help the model grasp the intended relationships.
However, given the vast range of potential feedback options,
it imposes a huge cognitive burden on users to manually revise task articulation, formulate principles from scratch, and source the most informative examples for learning.
Furthermore, since the feedback users intend to provide often stems from their intuition and expertise, encompassing both inductive and deductive reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib44" title="">44</a>]</cite>, the system ought to assist in translating these intuitive insights into concrete prompt content in an efficient and user-friendly manner.
For instance, <span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.5">E4</span> suggested providing diverse prompt templates for easy selection. <span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.6">E1</span> emphasized the need for a feature
that converts users’ fragmented feedback into systematic principles for the model to follow, while <span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.7">E3</span> highlighted the importance of automatically sourcing informative examples to provide high-quality rationales for knowledge alignment.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>System &amp; Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We designed <span class="ltx_ERROR undefined" id="S3.p1.1.1">\name</span>based on the distilled design requirements in  <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S2" title="2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2</span></a>. In this section, we first introduce the overall system framework. Then we illustrate the methods for data processing, multimodal rationale understanding, and prompt iteration strategy recommendation.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>System Framework</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="In 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> demonstrates the overarching workflow of the system. The multimodal video dataset, <span class="ltx_ERROR undefined" id="S3.SS1.p1.1.1">\jianben</span>processed into image frames (visual modality) with spoken narratives (language modality), along with the prompt, serves as input for the multimodal LLM.
The multimodal LLM then performs reasoning and generates free-text answers for each input instance. (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="In 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>A). Subsequently, the Multimodal Reasoning Understanding module (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="In 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>B) provides a multi-level analysis of the generated free-text answers for a systematic understanding of the model’s reasoning behavior.
Initially, it characterizes different interaction types between modalities.
Then, a multimodal reasoning pattern mining algorithm is employed to identify intricate and fine-grained reasoning patterns.
Concurrently, the Prompt Iteration Strategy Recommendation module (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="In 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>C) offers varied support, including bottom-up k-shot example recommendations that balance similarity and diversity, and top-down instructional principle summarization at both instance-specific and agnostics levels aided by an auxiliary LLM.
This module is designed to facilitate efficient prompt refinement, aiming to elicit and enhance specific knowledge to guide and improve model performance</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In the <span class="ltx_ERROR undefined" id="S3.SS1.p2.1.1">\name</span>interface (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="In 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>D), users have the option to either input their own prompts or choose from available templates in the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">Prompt Panel</span>. Subsequently, they can inspect the model’s multimodal reasoning performance from different levels of detail. Specifically, at a global level, users can inspect the model’s overall performance in the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">Evaluation Panel</span> and the interaction between and within modalities in the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.4">Reasoning Panel</span>. At the group level, users are able to scrutinize the model’s reasoning patterns concerning different concepts spanning across modalities. At the instance level, users can examine individual instances in detail for verification.
Users can then revise and incorporate principles and/or k-shot examples into prompts based on automatic recommendations and insights obtained from the current model and prompt performance examinations. The refined prompt can then be sent to the model for evaluation in the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.5">Prompt Panel</span>.
In the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.6">Evaluation Panel</span>, users can evaluate and compare the effect of each prompt iteration on both global model performance and individual instances. Additionally, they can monitor and track detailed changes across various prompt versions and iteratively refine the prompt to achieve satisfactory multimodal reasoning performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset and Model</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We demonstrate the effectiveness of our system on two different datasets for multimodal content comprehension tasks: CMU-MOSEI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib2" title="">2</a>]</cite> for multimodal sentiment analysis, and WTaG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib3" title="">3</a>]</cite> for user intent understanding.
The CMU-MOSEI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib2" title="">2</a>]</cite> dataset consists of monologue video clips in which speakers express their sentiments about a specific topic.
The WTaG dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib3" title="">3</a>]</cite> comprises egocentric video clips of users performing cooking tasks under the guidance of an instructor within an augmented reality setting.
The videos within both datasets contain information from two primary modalities: the language modality, represented by spoken content, and the visual modality, characterized by the scenes and user behaviors depicted in the videos.
<span class="ltx_ERROR undefined" id="S3.SS2.p1.1.1">\jianben</span>Both datasets include ground-truth labels for evaluation.
Following the practice in prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib59" title="">59</a>]</cite>, we split each dataset into three subsets: a validation set, a demonstration example set, and a test set.
In the splitting process, we ensure that the label distribution remains consistent across these subsets.
The validation set serves the purpose of prompt iteration evaluation. The demonstration example set facilitates the construction of k-shot examples, and the test set provides additional instances beyond the validation set for a more comprehensive assessment of prompt efficacy.
The size of the validation set needs to be moderate so that users can get timely feedback during the prompt iteration while also covering diverse data patterns for comprehensive model reasoning performance diagnosis. Based on our preliminary experiment, we maintain a distribution ratio of 1:2:1 for the validation, demonstration, and test sets, respectively.
We also implemented batch processing to improve the system’s response speed.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<span class="ltx_ERROR undefined" id="S3.SS2.p2.1">\jianben</span>
<p class="ltx_p" id="S3.SS2.p2.2">Regarding the model setting, we employ the LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib32" title="">32</a>]</cite> and GPT-4V(ision) <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://openai.com/index/gpt-4v-system-card/</span></span></span> model to perform multimodal reasoning considering their strong reasoning and instruction-following abilities. We specifically utilize the “llava-v1.5-13b” and “gpt-4-vision-preview” version.
It’s important to note that our approach is designed to be model-agnostic, meaning other multimodal LLMs that support multimodal content reasoning, such as Gemini <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://deepmind.google/technologies/gemini/</span></span></span> and LLaMA series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib51" title="">51</a>]</cite>, can be easily integrated into the system.
<span class="ltx_ERROR undefined" id="S3.SS2.p2.2.1">\jianben</span>For each video clip, we followed the commonly adopted practice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib63" title="">63</a>]</cite>, sampling frames per second to compose an image sequence from the visual modality, which is then combined with the corresponding spoken content from the language modality as input of the multimodal LLM.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<span class="ltx_ERROR undefined" id="S3.SS2.p3.10">\jianben</span>
<p class="ltx_p" id="S3.SS2.p3.9">The input prompt for the multimodal LLM reasoning follows the general prompt structure <math alttext="(I,\left\{x_{i},y_{i}\right\}_{i=1}^{k},x_{t})" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.3"><semantics id="S3.SS2.p3.1.m1.3a"><mrow id="S3.SS2.p3.1.m1.3.3.2" xref="S3.SS2.p3.1.m1.3.3.3.cmml"><mo id="S3.SS2.p3.1.m1.3.3.2.3" stretchy="false" xref="S3.SS2.p3.1.m1.3.3.3.cmml">(</mo><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">I</mi><mo id="S3.SS2.p3.1.m1.3.3.2.4" xref="S3.SS2.p3.1.m1.3.3.3.cmml">,</mo><msubsup id="S3.SS2.p3.1.m1.2.2.1.1" xref="S3.SS2.p3.1.m1.2.2.1.1.cmml"><mrow id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.3.cmml"><mo id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.3" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.3.cmml">{</mo><msub id="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.4" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.3.cmml">,</mo><msub id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.2.cmml">y</mi><mi id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.3" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.5" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.3.cmml">}</mo></mrow><mrow id="S3.SS2.p3.1.m1.2.2.1.1.2.4" xref="S3.SS2.p3.1.m1.2.2.1.1.2.4.cmml"><mi id="S3.SS2.p3.1.m1.2.2.1.1.2.4.2" xref="S3.SS2.p3.1.m1.2.2.1.1.2.4.2.cmml">i</mi><mo id="S3.SS2.p3.1.m1.2.2.1.1.2.4.1" xref="S3.SS2.p3.1.m1.2.2.1.1.2.4.1.cmml">=</mo><mn id="S3.SS2.p3.1.m1.2.2.1.1.2.4.3" xref="S3.SS2.p3.1.m1.2.2.1.1.2.4.3.cmml">1</mn></mrow><mi id="S3.SS2.p3.1.m1.2.2.1.1.4" xref="S3.SS2.p3.1.m1.2.2.1.1.4.cmml">k</mi></msubsup><mo id="S3.SS2.p3.1.m1.3.3.2.5" xref="S3.SS2.p3.1.m1.3.3.3.cmml">,</mo><msub id="S3.SS2.p3.1.m1.3.3.2.2" xref="S3.SS2.p3.1.m1.3.3.2.2.cmml"><mi id="S3.SS2.p3.1.m1.3.3.2.2.2" xref="S3.SS2.p3.1.m1.3.3.2.2.2.cmml">x</mi><mi id="S3.SS2.p3.1.m1.3.3.2.2.3" xref="S3.SS2.p3.1.m1.3.3.2.2.3.cmml">t</mi></msub><mo id="S3.SS2.p3.1.m1.3.3.2.6" stretchy="false" xref="S3.SS2.p3.1.m1.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.3b"><vector id="S3.SS2.p3.1.m1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.3.3.2"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝐼</ci><apply id="S3.SS2.p3.1.m1.2.2.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.1.1.3.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1">superscript</csymbol><apply id="S3.SS2.p3.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.1.1.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1">subscript</csymbol><set id="S3.SS2.p3.1.m1.2.2.1.1.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.2"><apply id="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.2">𝑦</ci><ci id="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.2.2.2.2.3">𝑖</ci></apply></set><apply id="S3.SS2.p3.1.m1.2.2.1.1.2.4.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.2.4"><eq id="S3.SS2.p3.1.m1.2.2.1.1.2.4.1.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.2.4.1"></eq><ci id="S3.SS2.p3.1.m1.2.2.1.1.2.4.2.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.2.4.2">𝑖</ci><cn id="S3.SS2.p3.1.m1.2.2.1.1.2.4.3.cmml" type="integer" xref="S3.SS2.p3.1.m1.2.2.1.1.2.4.3">1</cn></apply></apply><ci id="S3.SS2.p3.1.m1.2.2.1.1.4.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.4">𝑘</ci></apply><apply id="S3.SS2.p3.1.m1.3.3.2.2.cmml" xref="S3.SS2.p3.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.3.3.2.2.1.cmml" xref="S3.SS2.p3.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p3.1.m1.3.3.2.2.2">𝑥</ci><ci id="S3.SS2.p3.1.m1.3.3.2.2.3.cmml" xref="S3.SS2.p3.1.m1.3.3.2.2.3">𝑡</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.3c">(I,\left\{x_{i},y_{i}\right\}_{i=1}^{k},x_{t})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.3d">( italic_I , { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib60" title="">60</a>]</cite>. 
Here, <math alttext="I" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">I</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_I</annotation></semantics></math> is the task-specific instructions elaborating on the targeted scenarios, tasks to be finished, and expected output structure (e.g., return your answer in a JSON object).
To propel the LLM to perform CoT reasoning, The prompt could include instructions like “Please provide a step-by-step analysis”.
<math alttext="\left\{x_{i},y_{i}\right\}_{i=1}^{k}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.2"><semantics id="S3.SS2.p3.3.m3.2a"><msubsup id="S3.SS2.p3.3.m3.2.2" xref="S3.SS2.p3.3.m3.2.2.cmml"><mrow id="S3.SS2.p3.3.m3.2.2.2.2.2" xref="S3.SS2.p3.3.m3.2.2.2.2.3.cmml"><mo id="S3.SS2.p3.3.m3.2.2.2.2.2.3" xref="S3.SS2.p3.3.m3.2.2.2.2.3.cmml">{</mo><msub id="S3.SS2.p3.3.m3.1.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p3.3.m3.2.2.2.2.2.4" xref="S3.SS2.p3.3.m3.2.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p3.3.m3.2.2.2.2.2.2" xref="S3.SS2.p3.3.m3.2.2.2.2.2.2.cmml"><mi id="S3.SS2.p3.3.m3.2.2.2.2.2.2.2" xref="S3.SS2.p3.3.m3.2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS2.p3.3.m3.2.2.2.2.2.2.3" xref="S3.SS2.p3.3.m3.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS2.p3.3.m3.2.2.2.2.2.5" xref="S3.SS2.p3.3.m3.2.2.2.2.3.cmml">}</mo></mrow><mrow id="S3.SS2.p3.3.m3.2.2.2.4" xref="S3.SS2.p3.3.m3.2.2.2.4.cmml"><mi id="S3.SS2.p3.3.m3.2.2.2.4.2" xref="S3.SS2.p3.3.m3.2.2.2.4.2.cmml">i</mi><mo id="S3.SS2.p3.3.m3.2.2.2.4.1" xref="S3.SS2.p3.3.m3.2.2.2.4.1.cmml">=</mo><mn id="S3.SS2.p3.3.m3.2.2.2.4.3" xref="S3.SS2.p3.3.m3.2.2.2.4.3.cmml">1</mn></mrow><mi id="S3.SS2.p3.3.m3.2.2.4" xref="S3.SS2.p3.3.m3.2.2.4.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.2b"><apply id="S3.SS2.p3.3.m3.2.2.cmml" xref="S3.SS2.p3.3.m3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.2.2.3.cmml" xref="S3.SS2.p3.3.m3.2.2">superscript</csymbol><apply id="S3.SS2.p3.3.m3.2.2.2.cmml" xref="S3.SS2.p3.3.m3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.2.2.2.3.cmml" xref="S3.SS2.p3.3.m3.2.2">subscript</csymbol><set id="S3.SS2.p3.3.m3.2.2.2.2.3.cmml" xref="S3.SS2.p3.3.m3.2.2.2.2.2"><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p3.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.3.m3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.2.2.2.2.2.2.1.cmml" xref="S3.SS2.p3.3.m3.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.3.m3.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.SS2.p3.3.m3.2.2.2.2.2.2.3.cmml" xref="S3.SS2.p3.3.m3.2.2.2.2.2.2.3">𝑖</ci></apply></set><apply id="S3.SS2.p3.3.m3.2.2.2.4.cmml" xref="S3.SS2.p3.3.m3.2.2.2.4"><eq id="S3.SS2.p3.3.m3.2.2.2.4.1.cmml" xref="S3.SS2.p3.3.m3.2.2.2.4.1"></eq><ci id="S3.SS2.p3.3.m3.2.2.2.4.2.cmml" xref="S3.SS2.p3.3.m3.2.2.2.4.2">𝑖</ci><cn id="S3.SS2.p3.3.m3.2.2.2.4.3.cmml" type="integer" xref="S3.SS2.p3.3.m3.2.2.2.4.3">1</cn></apply></apply><ci id="S3.SS2.p3.3.m3.2.2.4.cmml" xref="S3.SS2.p3.3.m3.2.2.4">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.2c">\left\{x_{i},y_{i}\right\}_{i=1}^{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.2d">{ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> is the demonstration example set. Each demonstration example includes input <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">x</mi><mi id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">𝑥</ci><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and output <math alttext="y_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">y</mi><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">𝑦</ci><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <span class="ltx_ERROR undefined" id="S3.SS2.p3.9.1">\jianben</span>where <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.1"><semantics id="S3.SS2.p3.6.m6.1a"><msub id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">x</mi><mi id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2">𝑥</ci><ci id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> follows the same format as the validation set and <math alttext="y_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m7.1"><semantics id="S3.SS2.p3.7.m7.1a"><msub id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.2" xref="S3.SS2.p3.7.m7.1.1.2.cmml">y</mi><mi id="S3.SS2.p3.7.m7.1.1.3" xref="S3.SS2.p3.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2">𝑦</ci><ci id="S3.SS2.p3.7.m7.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.7.m7.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> includes the correct rational and final answer as provided by the ground-truth labels.
We also support the zero-shot setting where demonstration examples are not provided.
Finally, for each test input <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS2.p3.8.m8.1"><semantics id="S3.SS2.p3.8.m8.1a"><msub id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">x</mi><mi id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">𝑥</ci><ci id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.8.m8.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, the LLM is expected to generate the output <math alttext="y_{t}" class="ltx_Math" display="inline" id="S3.SS2.p3.9.m9.1"><semantics id="S3.SS2.p3.9.m9.1a"><msub id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml"><mi id="S3.SS2.p3.9.m9.1.1.2" xref="S3.SS2.p3.9.m9.1.1.2.cmml">y</mi><mi id="S3.SS2.p3.9.m9.1.1.3" xref="S3.SS2.p3.9.m9.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><apply id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p3.9.m9.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.2">𝑦</ci><ci id="S3.SS2.p3.9.m9.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">y_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.9.m9.1d">italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, containing a free-text <span class="ltx_ERROR undefined" id="S3.SS2.p3.9.2">\jianben</span>rationale and a <span class="ltx_ERROR undefined" id="S3.SS2.p3.9.3">\jianben</span>final answer.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multimodal Rationale Understanding</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Modality Interaction Characterization</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<span class="ltx_ERROR undefined" id="S3.SS3.SSS1.p1.1">\jianben</span>
<p class="ltx_p" id="S3.SS3.SSS1.p1.2">Understanding how multimodal models utilize information from distinct modalities and integrate it to make cross-modal inferences is crucial for gaining insight into the model’s reasoning performance.
Several works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib31" title="">31</a>]</cite> have tried to characterize the interaction between different modalities based on aggregated feature attribution values <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib38" title="">38</a>]</cite>. There are also works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib66" title="">66</a>]</cite> trying to quantify the degree of interactions between modalities with a partial information decomposition framework.
Building on the foundation of these works, we characterize the modality interaction in the context of our targeted multimodal <span class="ltx_ERROR undefined" id="S3.SS3.SSS1.p1.2.1">\jianben</span>reasoning tasks as follows:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.19">Considering the labeled multimodal dataset with two modality <math alttext="\mathcal{X}_{1}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.1.m1.1"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><msub id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.1.m1.1.1.2" xref="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml">𝒳</mi><mn id="S3.SS3.SSS1.p2.1.m1.1.1.3" xref="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><apply id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1.2">𝒳</ci><cn id="S3.SS3.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">\mathcal{X}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.1.m1.1d">caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{X}_{2}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.2.m2.1"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><msub id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.2.m2.1.1.2" xref="S3.SS3.SSS1.p2.2.m2.1.1.2.cmml">𝒳</mi><mn id="S3.SS3.SSS1.p2.2.m2.1.1.3" xref="S3.SS3.SSS1.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><apply id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1.2">𝒳</ci><cn id="S3.SS3.SSS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">\mathcal{X}_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.2.m2.1d">caligraphic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, the unimodal data <math alttext="\mathcal{D}_{i}=\left\{\left(x_{i},y\right):\mathcal{X}_{i}\times\mathcal{Y}\right\}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.3.m3.3"><semantics id="S3.SS3.SSS1.p2.3.m3.3a"><mrow id="S3.SS3.SSS1.p2.3.m3.3.3" xref="S3.SS3.SSS1.p2.3.m3.3.3.cmml"><msub id="S3.SS3.SSS1.p2.3.m3.3.3.4" xref="S3.SS3.SSS1.p2.3.m3.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.3.m3.3.3.4.2" xref="S3.SS3.SSS1.p2.3.m3.3.3.4.2.cmml">𝒟</mi><mi id="S3.SS3.SSS1.p2.3.m3.3.3.4.3" xref="S3.SS3.SSS1.p2.3.m3.3.3.4.3.cmml">i</mi></msub><mo id="S3.SS3.SSS1.p2.3.m3.3.3.3" xref="S3.SS3.SSS1.p2.3.m3.3.3.3.cmml">=</mo><mrow id="S3.SS3.SSS1.p2.3.m3.3.3.2.2" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.3.cmml"><mo id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.3" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.3.1.cmml">{</mo><mrow id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.2.cmml"><mo id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.2" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.2.cmml">(</mo><msub id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.3" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.3" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.2.cmml">,</mo><mi id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml">y</mi><mo id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.4" rspace="0.278em" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.2.cmml">)</mo></mrow><mo id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.4" rspace="0.278em" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.3.1.cmml">:</mo><mrow id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.cmml"><msub id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.2" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.2.cmml">𝒳</mi><mi id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.3" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.3" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.3.cmml">𝒴</mi></mrow><mo id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.5" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.3b"><apply id="S3.SS3.SSS1.p2.3.m3.3.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3"><eq id="S3.SS3.SSS1.p2.3.m3.3.3.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.3"></eq><apply id="S3.SS3.SSS1.p2.3.m3.3.3.4.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.4"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.3.m3.3.3.4.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.4">subscript</csymbol><ci id="S3.SS3.SSS1.p2.3.m3.3.3.4.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.4.2">𝒟</ci><ci id="S3.SS3.SSS1.p2.3.m3.3.3.4.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.4.3">𝑖</ci></apply><apply id="S3.SS3.SSS1.p2.3.m3.3.3.2.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2"><csymbol cd="latexml" id="S3.SS3.SSS1.p2.3.m3.3.3.2.3.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.3">conditional-set</csymbol><interval closure="open" id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1"><apply id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.2.2.1.1.1.1.1.3">𝑖</ci></apply><ci id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1">𝑦</ci></interval><apply id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2"><times id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.1"></times><apply id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.2">𝒳</ci><ci id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.2.3">𝑖</ci></apply><ci id="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.3.3.2.2.2.3">𝒴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.3c">\mathcal{D}_{i}=\left\{\left(x_{i},y\right):\mathcal{X}_{i}\times\mathcal{Y}\right\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.3.m3.3d">caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y ) : caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT × caligraphic_Y }</annotation></semantics></math> where <math alttext="i\in\{1,2\}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.4.m4.2"><semantics id="S3.SS3.SSS1.p2.4.m4.2a"><mrow id="S3.SS3.SSS1.p2.4.m4.2.3" xref="S3.SS3.SSS1.p2.4.m4.2.3.cmml"><mi id="S3.SS3.SSS1.p2.4.m4.2.3.2" xref="S3.SS3.SSS1.p2.4.m4.2.3.2.cmml">i</mi><mo id="S3.SS3.SSS1.p2.4.m4.2.3.1" xref="S3.SS3.SSS1.p2.4.m4.2.3.1.cmml">∈</mo><mrow id="S3.SS3.SSS1.p2.4.m4.2.3.3.2" xref="S3.SS3.SSS1.p2.4.m4.2.3.3.1.cmml"><mo id="S3.SS3.SSS1.p2.4.m4.2.3.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p2.4.m4.2.3.3.1.cmml">{</mo><mn id="S3.SS3.SSS1.p2.4.m4.1.1" xref="S3.SS3.SSS1.p2.4.m4.1.1.cmml">1</mn><mo id="S3.SS3.SSS1.p2.4.m4.2.3.3.2.2" xref="S3.SS3.SSS1.p2.4.m4.2.3.3.1.cmml">,</mo><mn id="S3.SS3.SSS1.p2.4.m4.2.2" xref="S3.SS3.SSS1.p2.4.m4.2.2.cmml">2</mn><mo id="S3.SS3.SSS1.p2.4.m4.2.3.3.2.3" stretchy="false" xref="S3.SS3.SSS1.p2.4.m4.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.4.m4.2b"><apply id="S3.SS3.SSS1.p2.4.m4.2.3.cmml" xref="S3.SS3.SSS1.p2.4.m4.2.3"><in id="S3.SS3.SSS1.p2.4.m4.2.3.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.2.3.1"></in><ci id="S3.SS3.SSS1.p2.4.m4.2.3.2.cmml" xref="S3.SS3.SSS1.p2.4.m4.2.3.2">𝑖</ci><set id="S3.SS3.SSS1.p2.4.m4.2.3.3.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.2.3.3.2"><cn id="S3.SS3.SSS1.p2.4.m4.1.1.cmml" type="integer" xref="S3.SS3.SSS1.p2.4.m4.1.1">1</cn><cn id="S3.SS3.SSS1.p2.4.m4.2.2.cmml" type="integer" xref="S3.SS3.SSS1.p2.4.m4.2.2">2</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.4.m4.2c">i\in\{1,2\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.4.m4.2d">italic_i ∈ { 1 , 2 }</annotation></semantics></math>
, and the multimodal data <math alttext="\mathcal{D}_{M}=\left\{\left(x_{1},x_{2},y\right):\mathcal{X}_{1}\times%
\mathcal{X}_{2}\times\mathcal{Y}\right\}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.5.m5.3"><semantics id="S3.SS3.SSS1.p2.5.m5.3a"><mrow id="S3.SS3.SSS1.p2.5.m5.3.3" xref="S3.SS3.SSS1.p2.5.m5.3.3.cmml"><msub id="S3.SS3.SSS1.p2.5.m5.3.3.4" xref="S3.SS3.SSS1.p2.5.m5.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.5.m5.3.3.4.2" xref="S3.SS3.SSS1.p2.5.m5.3.3.4.2.cmml">𝒟</mi><mi id="S3.SS3.SSS1.p2.5.m5.3.3.4.3" xref="S3.SS3.SSS1.p2.5.m5.3.3.4.3.cmml">M</mi></msub><mo id="S3.SS3.SSS1.p2.5.m5.3.3.3" xref="S3.SS3.SSS1.p2.5.m5.3.3.3.cmml">=</mo><mrow id="S3.SS3.SSS1.p2.5.m5.3.3.2.2" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.3.cmml"><mo id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.3" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.3.1.cmml">{</mo><mrow id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.3.cmml"><mo id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.3" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.3.cmml">(</mo><msub id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2.cmml">x</mi><mn id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.3" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.4" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.3.cmml">,</mo><msub id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.cmml"><mi id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.2" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.2.cmml">x</mi><mn id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.3" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.5" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.3.cmml">,</mo><mi id="S3.SS3.SSS1.p2.5.m5.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.cmml">y</mi><mo id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.6" rspace="0.278em" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.4" rspace="0.278em" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.3.1.cmml">:</mo><mrow id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.cmml"><msub id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.2" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.2.cmml">𝒳</mi><mn id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.3" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.3.cmml">1</mn></msub><mo id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.1.cmml">×</mo><msub id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.2" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.2.cmml">𝒳</mi><mn id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.3" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.3.cmml">2</mn></msub><mo id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.4" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.4.cmml">𝒴</mi></mrow><mo id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.5" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.5.m5.3b"><apply id="S3.SS3.SSS1.p2.5.m5.3.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3"><eq id="S3.SS3.SSS1.p2.5.m5.3.3.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.3"></eq><apply id="S3.SS3.SSS1.p2.5.m5.3.3.4.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.4"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.3.3.4.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.4">subscript</csymbol><ci id="S3.SS3.SSS1.p2.5.m5.3.3.4.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.4.2">𝒟</ci><ci id="S3.SS3.SSS1.p2.5.m5.3.3.4.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.4.3">𝑀</ci></apply><apply id="S3.SS3.SSS1.p2.5.m5.3.3.2.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2"><csymbol cd="latexml" id="S3.SS3.SSS1.p2.5.m5.3.3.2.3.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.3">conditional-set</csymbol><vector id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2"><apply id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.2">𝑥</ci><cn id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.2">𝑥</ci><cn id="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.5.m5.2.2.1.1.1.2.2.3">2</cn></apply><ci id="S3.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1">𝑦</ci></vector><apply id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2"><times id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.1"></times><apply id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.2">𝒳</ci><cn id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.2.3">1</cn></apply><apply id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3">subscript</csymbol><ci id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.2.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.2">𝒳</ci><cn id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.3.3">2</cn></apply><ci id="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.4.cmml" xref="S3.SS3.SSS1.p2.5.m5.3.3.2.2.2.4">𝒴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.5.m5.3c">\mathcal{D}_{M}=\left\{\left(x_{1},x_{2},y\right):\mathcal{X}_{1}\times%
\mathcal{X}_{2}\times\mathcal{Y}\right\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.5.m5.3d">caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT = { ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_y ) : caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × caligraphic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT × caligraphic_Y }</annotation></semantics></math>.
<span class="ltx_ERROR undefined" id="S3.SS3.SSS1.p2.19.1">\jianben</span>When performing chain-of-thought reasoning, for each input data point, the output of the multimodal LLM includes a free-text rationale and a final answer.
<span class="ltx_ERROR undefined" id="S3.SS3.SSS1.p2.19.2">\jianben</span>Here, we denote the sample space where the multimodal LLM performs reasoning using information from a single modality as <math alttext="f_{i}:\mathcal{X}_{i}\rightarrow\Delta\mathcal{Y}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.6.m6.1"><semantics id="S3.SS3.SSS1.p2.6.m6.1a"><mrow id="S3.SS3.SSS1.p2.6.m6.1.1" xref="S3.SS3.SSS1.p2.6.m6.1.1.cmml"><msub id="S3.SS3.SSS1.p2.6.m6.1.1.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.cmml"><mi id="S3.SS3.SSS1.p2.6.m6.1.1.2.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.6.m6.1.1.2.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.SSS1.p2.6.m6.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.SS3.SSS1.p2.6.m6.1.1.1.cmml">:</mo><mrow id="S3.SS3.SSS1.p2.6.m6.1.1.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.cmml"><msub id="S3.SS3.SSS1.p2.6.m6.1.1.3.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.6.m6.1.1.3.2.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2.2.cmml">𝒳</mi><mi id="S3.SS3.SSS1.p2.6.m6.1.1.3.2.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.SS3.SSS1.p2.6.m6.1.1.3.1" stretchy="false" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.1.cmml">→</mo><mrow id="S3.SS3.SSS1.p2.6.m6.1.1.3.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.cmml"><mi id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.2" mathvariant="normal" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.2.cmml">Δ</mi><mo id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.1" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.1.cmml">⁢</mo><msub id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.2" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.2.cmml">𝒴</mi><mi id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.3" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.6.m6.1b"><apply id="S3.SS3.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1"><ci id="S3.SS3.SSS1.p2.6.m6.1.1.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.1">:</ci><apply id="S3.SS3.SSS1.p2.6.m6.1.1.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.6.m6.1.1.2.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.6.m6.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.6.m6.1.1.2.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.SSS1.p2.6.m6.1.1.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3"><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.1">→</ci><apply id="S3.SS3.SSS1.p2.6.m6.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.6.m6.1.1.3.2.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.2.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2.2">𝒳</ci><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.2.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.2.3">𝑖</ci></apply><apply id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3"><times id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.1"></times><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.2">Δ</ci><apply id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.1.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.2.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.2">𝒴</ci><ci id="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.3.cmml" xref="S3.SS3.SSS1.p2.6.m6.1.1.3.3.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.6.m6.1c">f_{i}:\mathcal{X}_{i}\rightarrow\Delta\mathcal{Y}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.6.m6.1d">italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT → roman_Δ caligraphic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the sample space where the multimodal LLM performs reasoning with information from both modalities as <math alttext="f_{M}:\mathcal{X}_{1}\times\mathcal{X}_{2}\rightarrow\Delta\mathcal{Y}_{M}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.7.m7.1"><semantics id="S3.SS3.SSS1.p2.7.m7.1a"><mrow id="S3.SS3.SSS1.p2.7.m7.1.1" xref="S3.SS3.SSS1.p2.7.m7.1.1.cmml"><msub id="S3.SS3.SSS1.p2.7.m7.1.1.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.cmml"><mi id="S3.SS3.SSS1.p2.7.m7.1.1.2.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.7.m7.1.1.2.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.3.cmml">M</mi></msub><mo id="S3.SS3.SSS1.p2.7.m7.1.1.1" lspace="0.278em" rspace="0.278em" xref="S3.SS3.SSS1.p2.7.m7.1.1.1.cmml">:</mo><mrow id="S3.SS3.SSS1.p2.7.m7.1.1.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.cmml"><mrow id="S3.SS3.SSS1.p2.7.m7.1.1.3.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.cmml"><msub id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.2.cmml">𝒳</mi><mn id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.3.cmml">1</mn></msub><mo id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.1.cmml">×</mo><msub id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.2.cmml">𝒳</mi><mn id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.3.cmml">2</mn></msub></mrow><mo id="S3.SS3.SSS1.p2.7.m7.1.1.3.1" stretchy="false" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.1.cmml">→</mo><mrow id="S3.SS3.SSS1.p2.7.m7.1.1.3.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.cmml"><mi id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.2" mathvariant="normal" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.2.cmml">Δ</mi><mo id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.1" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.1.cmml">⁢</mo><msub id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.2" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.2.cmml">𝒴</mi><mi id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.3" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.3.cmml">M</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.7.m7.1b"><apply id="S3.SS3.SSS1.p2.7.m7.1.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1"><ci id="S3.SS3.SSS1.p2.7.m7.1.1.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.1">:</ci><apply id="S3.SS3.SSS1.p2.7.m7.1.1.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.7.m7.1.1.2.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.7.m7.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.7.m7.1.1.2.3.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.2.3">𝑀</ci></apply><apply id="S3.SS3.SSS1.p2.7.m7.1.1.3.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3"><ci id="S3.SS3.SSS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.1">→</ci><apply id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2"><times id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.1"></times><apply id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.2">𝒳</ci><cn id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.2.3">1</cn></apply><apply id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3">subscript</csymbol><ci id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.2">𝒳</ci><cn id="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.2.3.3">2</cn></apply></apply><apply id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3"><times id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.1"></times><ci id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.2">Δ</ci><apply id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.1.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.2.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.2">𝒴</ci><ci id="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.3.cmml" xref="S3.SS3.SSS1.p2.7.m7.1.1.3.3.3.3">𝑀</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.7.m7.1c">f_{M}:\mathcal{X}_{1}\times\mathcal{X}_{2}\rightarrow\Delta\mathcal{Y}_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.7.m7.1d">italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT : caligraphic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × caligraphic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT → roman_Δ caligraphic_Y start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math>. where <math alttext="\Delta\mathcal{Y}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.8.m8.1"><semantics id="S3.SS3.SSS1.p2.8.m8.1a"><mrow id="S3.SS3.SSS1.p2.8.m8.1.1" xref="S3.SS3.SSS1.p2.8.m8.1.1.cmml"><mi id="S3.SS3.SSS1.p2.8.m8.1.1.2" mathvariant="normal" xref="S3.SS3.SSS1.p2.8.m8.1.1.2.cmml">Δ</mi><mo id="S3.SS3.SSS1.p2.8.m8.1.1.1" xref="S3.SS3.SSS1.p2.8.m8.1.1.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p2.8.m8.1.1.3" xref="S3.SS3.SSS1.p2.8.m8.1.1.3.cmml">𝒴</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.8.m8.1b"><apply id="S3.SS3.SSS1.p2.8.m8.1.1.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1"><times id="S3.SS3.SSS1.p2.8.m8.1.1.1.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.1"></times><ci id="S3.SS3.SSS1.p2.8.m8.1.1.2.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.2">Δ</ci><ci id="S3.SS3.SSS1.p2.8.m8.1.1.3.cmml" xref="S3.SS3.SSS1.p2.8.m8.1.1.3">𝒴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.8.m8.1c">\Delta\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.8.m8.1d">roman_Δ caligraphic_Y</annotation></semantics></math> denotes the probability simplex of final output answer.
<span class="ltx_ERROR undefined" id="S3.SS3.SSS1.p2.19.3">\jianben</span>For <math alttext="f_{a}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.9.m9.1"><semantics id="S3.SS3.SSS1.p2.9.m9.1a"><msub id="S3.SS3.SSS1.p2.9.m9.1.1" xref="S3.SS3.SSS1.p2.9.m9.1.1.cmml"><mi id="S3.SS3.SSS1.p2.9.m9.1.1.2" xref="S3.SS3.SSS1.p2.9.m9.1.1.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.9.m9.1.1.3" xref="S3.SS3.SSS1.p2.9.m9.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.9.m9.1b"><apply id="S3.SS3.SSS1.p2.9.m9.1.1.cmml" xref="S3.SS3.SSS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.9.m9.1.1.1.cmml" xref="S3.SS3.SSS1.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.9.m9.1.1.2.cmml" xref="S3.SS3.SSS1.p2.9.m9.1.1.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.9.m9.1.1.3.cmml" xref="S3.SS3.SSS1.p2.9.m9.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.9.m9.1c">f_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.9.m9.1d">italic_f start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{b}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.10.m10.1"><semantics id="S3.SS3.SSS1.p2.10.m10.1a"><msub id="S3.SS3.SSS1.p2.10.m10.1.1" xref="S3.SS3.SSS1.p2.10.m10.1.1.cmml"><mi id="S3.SS3.SSS1.p2.10.m10.1.1.2" xref="S3.SS3.SSS1.p2.10.m10.1.1.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.10.m10.1.1.3" xref="S3.SS3.SSS1.p2.10.m10.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.10.m10.1b"><apply id="S3.SS3.SSS1.p2.10.m10.1.1.cmml" xref="S3.SS3.SSS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.10.m10.1.1.1.cmml" xref="S3.SS3.SSS1.p2.10.m10.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.10.m10.1.1.2.cmml" xref="S3.SS3.SSS1.p2.10.m10.1.1.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.10.m10.1.1.3.cmml" xref="S3.SS3.SSS1.p2.10.m10.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.10.m10.1c">f_{b}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.10.m10.1d">italic_f start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> where <math alttext="a,b\in\{1,2,M\}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.11.m11.5"><semantics id="S3.SS3.SSS1.p2.11.m11.5a"><mrow id="S3.SS3.SSS1.p2.11.m11.5.6" xref="S3.SS3.SSS1.p2.11.m11.5.6.cmml"><mrow id="S3.SS3.SSS1.p2.11.m11.5.6.2.2" xref="S3.SS3.SSS1.p2.11.m11.5.6.2.1.cmml"><mi id="S3.SS3.SSS1.p2.11.m11.4.4" xref="S3.SS3.SSS1.p2.11.m11.4.4.cmml">a</mi><mo id="S3.SS3.SSS1.p2.11.m11.5.6.2.2.1" xref="S3.SS3.SSS1.p2.11.m11.5.6.2.1.cmml">,</mo><mi id="S3.SS3.SSS1.p2.11.m11.5.5" xref="S3.SS3.SSS1.p2.11.m11.5.5.cmml">b</mi></mrow><mo id="S3.SS3.SSS1.p2.11.m11.5.6.1" xref="S3.SS3.SSS1.p2.11.m11.5.6.1.cmml">∈</mo><mrow id="S3.SS3.SSS1.p2.11.m11.5.6.3.2" xref="S3.SS3.SSS1.p2.11.m11.5.6.3.1.cmml"><mo id="S3.SS3.SSS1.p2.11.m11.5.6.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p2.11.m11.5.6.3.1.cmml">{</mo><mn id="S3.SS3.SSS1.p2.11.m11.1.1" xref="S3.SS3.SSS1.p2.11.m11.1.1.cmml">1</mn><mo id="S3.SS3.SSS1.p2.11.m11.5.6.3.2.2" xref="S3.SS3.SSS1.p2.11.m11.5.6.3.1.cmml">,</mo><mn id="S3.SS3.SSS1.p2.11.m11.2.2" xref="S3.SS3.SSS1.p2.11.m11.2.2.cmml">2</mn><mo id="S3.SS3.SSS1.p2.11.m11.5.6.3.2.3" xref="S3.SS3.SSS1.p2.11.m11.5.6.3.1.cmml">,</mo><mi id="S3.SS3.SSS1.p2.11.m11.3.3" xref="S3.SS3.SSS1.p2.11.m11.3.3.cmml">M</mi><mo id="S3.SS3.SSS1.p2.11.m11.5.6.3.2.4" stretchy="false" xref="S3.SS3.SSS1.p2.11.m11.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.11.m11.5b"><apply id="S3.SS3.SSS1.p2.11.m11.5.6.cmml" xref="S3.SS3.SSS1.p2.11.m11.5.6"><in id="S3.SS3.SSS1.p2.11.m11.5.6.1.cmml" xref="S3.SS3.SSS1.p2.11.m11.5.6.1"></in><list id="S3.SS3.SSS1.p2.11.m11.5.6.2.1.cmml" xref="S3.SS3.SSS1.p2.11.m11.5.6.2.2"><ci id="S3.SS3.SSS1.p2.11.m11.4.4.cmml" xref="S3.SS3.SSS1.p2.11.m11.4.4">𝑎</ci><ci id="S3.SS3.SSS1.p2.11.m11.5.5.cmml" xref="S3.SS3.SSS1.p2.11.m11.5.5">𝑏</ci></list><set id="S3.SS3.SSS1.p2.11.m11.5.6.3.1.cmml" xref="S3.SS3.SSS1.p2.11.m11.5.6.3.2"><cn id="S3.SS3.SSS1.p2.11.m11.1.1.cmml" type="integer" xref="S3.SS3.SSS1.p2.11.m11.1.1">1</cn><cn id="S3.SS3.SSS1.p2.11.m11.2.2.cmml" type="integer" xref="S3.SS3.SSS1.p2.11.m11.2.2">2</cn><ci id="S3.SS3.SSS1.p2.11.m11.3.3.cmml" xref="S3.SS3.SSS1.p2.11.m11.3.3">𝑀</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.11.m11.5c">a,b\in\{1,2,M\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.11.m11.5d">italic_a , italic_b ∈ { 1 , 2 , italic_M }</annotation></semantics></math>,
The distance function can be defined as <math alttext="d\left(f_{a},f_{b}\right)=\left\|\Delta_{a}-\Delta_{b}\right\|" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.12.m12.3"><semantics id="S3.SS3.SSS1.p2.12.m12.3a"><mrow id="S3.SS3.SSS1.p2.12.m12.3.3" xref="S3.SS3.SSS1.p2.12.m12.3.3.cmml"><mrow id="S3.SS3.SSS1.p2.12.m12.2.2.2" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.cmml"><mi id="S3.SS3.SSS1.p2.12.m12.2.2.2.4" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.4.cmml">d</mi><mo id="S3.SS3.SSS1.p2.12.m12.2.2.2.3" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.3.cmml"><mo id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.3" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.3.cmml">(</mo><msub id="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1" xref="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.3.cmml">a</mi></msub><mo id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.4" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.3.cmml">,</mo><msub id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.cmml"><mi id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.2" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.3" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.3.cmml">b</mi></msub><mo id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.5" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p2.12.m12.3.3.4" xref="S3.SS3.SSS1.p2.12.m12.3.3.4.cmml">=</mo><mrow id="S3.SS3.SSS1.p2.12.m12.3.3.3.1" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.2.cmml"><mo id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.2" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.2.1.cmml">‖</mo><mrow id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.cmml"><msub id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.cmml"><mi id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.2" mathvariant="normal" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.2.cmml">Δ</mi><mi id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.3" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.3.cmml">a</mi></msub><mo id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.1" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.1.cmml">−</mo><msub id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.cmml"><mi id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.2" mathvariant="normal" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.2.cmml">Δ</mi><mi id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.3" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.3.cmml">b</mi></msub></mrow><mo id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.3" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.2.1.cmml">‖</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.12.m12.3b"><apply id="S3.SS3.SSS1.p2.12.m12.3.3.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3"><eq id="S3.SS3.SSS1.p2.12.m12.3.3.4.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.4"></eq><apply id="S3.SS3.SSS1.p2.12.m12.2.2.2.cmml" xref="S3.SS3.SSS1.p2.12.m12.2.2.2"><times id="S3.SS3.SSS1.p2.12.m12.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.3"></times><ci id="S3.SS3.SSS1.p2.12.m12.2.2.2.4.cmml" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.4">𝑑</ci><interval closure="open" id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2"><apply id="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS1.p2.12.m12.1.1.1.1.1.1.3">𝑎</ci></apply><apply id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.12.m12.2.2.2.2.2.2.3">𝑏</ci></apply></interval></apply><apply id="S3.SS3.SSS1.p2.12.m12.3.3.3.2.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1"><csymbol cd="latexml" id="S3.SS3.SSS1.p2.12.m12.3.3.3.2.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.2">norm</csymbol><apply id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1"><minus id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.1"></minus><apply id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.2.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.2">Δ</ci><ci id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.3.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.2.3">𝑎</ci></apply><apply id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.1.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3">subscript</csymbol><ci id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.2.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.2">Δ</ci><ci id="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.3.cmml" xref="S3.SS3.SSS1.p2.12.m12.3.3.3.1.1.3.3">𝑏</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.12.m12.3c">d\left(f_{a},f_{b}\right)=\left\|\Delta_{a}-\Delta_{b}\right\|</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.12.m12.3d">italic_d ( italic_f start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) = ∥ roman_Δ start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT - roman_Δ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ∥</annotation></semantics></math> to measure the distance between <math alttext="f_{a}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.13.m13.1"><semantics id="S3.SS3.SSS1.p2.13.m13.1a"><msub id="S3.SS3.SSS1.p2.13.m13.1.1" xref="S3.SS3.SSS1.p2.13.m13.1.1.cmml"><mi id="S3.SS3.SSS1.p2.13.m13.1.1.2" xref="S3.SS3.SSS1.p2.13.m13.1.1.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.13.m13.1.1.3" xref="S3.SS3.SSS1.p2.13.m13.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.13.m13.1b"><apply id="S3.SS3.SSS1.p2.13.m13.1.1.cmml" xref="S3.SS3.SSS1.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.13.m13.1.1.1.cmml" xref="S3.SS3.SSS1.p2.13.m13.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.13.m13.1.1.2.cmml" xref="S3.SS3.SSS1.p2.13.m13.1.1.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.13.m13.1.1.3.cmml" xref="S3.SS3.SSS1.p2.13.m13.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.13.m13.1c">f_{a}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.13.m13.1d">italic_f start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{b}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.14.m14.1"><semantics id="S3.SS3.SSS1.p2.14.m14.1a"><msub id="S3.SS3.SSS1.p2.14.m14.1.1" xref="S3.SS3.SSS1.p2.14.m14.1.1.cmml"><mi id="S3.SS3.SSS1.p2.14.m14.1.1.2" xref="S3.SS3.SSS1.p2.14.m14.1.1.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.14.m14.1.1.3" xref="S3.SS3.SSS1.p2.14.m14.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.14.m14.1b"><apply id="S3.SS3.SSS1.p2.14.m14.1.1.cmml" xref="S3.SS3.SSS1.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.14.m14.1.1.1.cmml" xref="S3.SS3.SSS1.p2.14.m14.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.14.m14.1.1.2.cmml" xref="S3.SS3.SSS1.p2.14.m14.1.1.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.14.m14.1.1.3.cmml" xref="S3.SS3.SSS1.p2.14.m14.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.14.m14.1c">f_{b}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.14.m14.1d">italic_f start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>.
Based on the distance function, we can define two basic interaction types between two modalities.
When <math alttext="d\left(f_{1},f_{2}\right)&lt;\theta" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.15.m15.2"><semantics id="S3.SS3.SSS1.p2.15.m15.2a"><mrow id="S3.SS3.SSS1.p2.15.m15.2.2" xref="S3.SS3.SSS1.p2.15.m15.2.2.cmml"><mrow id="S3.SS3.SSS1.p2.15.m15.2.2.2" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.cmml"><mi id="S3.SS3.SSS1.p2.15.m15.2.2.2.4" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.4.cmml">d</mi><mo id="S3.SS3.SSS1.p2.15.m15.2.2.2.3" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.3.cmml"><mo id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.3" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.3.cmml">(</mo><msub id="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1" xref="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.4" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.3.cmml">,</mo><msub id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.cmml"><mi id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.2" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.2.cmml">f</mi><mn id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.3" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.5" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p2.15.m15.2.2.3" xref="S3.SS3.SSS1.p2.15.m15.2.2.3.cmml">&lt;</mo><mi id="S3.SS3.SSS1.p2.15.m15.2.2.4" xref="S3.SS3.SSS1.p2.15.m15.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.15.m15.2b"><apply id="S3.SS3.SSS1.p2.15.m15.2.2.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2"><lt id="S3.SS3.SSS1.p2.15.m15.2.2.3.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.3"></lt><apply id="S3.SS3.SSS1.p2.15.m15.2.2.2.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.2"><times id="S3.SS3.SSS1.p2.15.m15.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.3"></times><ci id="S3.SS3.SSS1.p2.15.m15.2.2.2.4.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.4">𝑑</ci><interval closure="open" id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2"><apply id="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.15.m15.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.2">𝑓</ci><cn id="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.15.m15.2.2.2.2.2.2.3">2</cn></apply></interval></apply><ci id="S3.SS3.SSS1.p2.15.m15.2.2.4.cmml" xref="S3.SS3.SSS1.p2.15.m15.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.15.m15.2c">d\left(f_{1},f_{2}\right)&lt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.15.m15.2d">italic_d ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) &lt; italic_θ</annotation></semantics></math>, where <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.16.m16.1"><semantics id="S3.SS3.SSS1.p2.16.m16.1a"><mi id="S3.SS3.SSS1.p2.16.m16.1.1" xref="S3.SS3.SSS1.p2.16.m16.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.16.m16.1b"><ci id="S3.SS3.SSS1.p2.16.m16.1.1.cmml" xref="S3.SS3.SSS1.p2.16.m16.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.16.m16.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.16.m16.1d">italic_θ</annotation></semantics></math> is a pre-defined threshold, the interaction type is <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.19.4">complement</span>, indicating these two modalities
contribute to the final answer in the same direction.
Conversely, when <math alttext="d\left(f_{1},f_{2}\right)&gt;\theta" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.17.m17.2"><semantics id="S3.SS3.SSS1.p2.17.m17.2a"><mrow id="S3.SS3.SSS1.p2.17.m17.2.2" xref="S3.SS3.SSS1.p2.17.m17.2.2.cmml"><mrow id="S3.SS3.SSS1.p2.17.m17.2.2.2" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.cmml"><mi id="S3.SS3.SSS1.p2.17.m17.2.2.2.4" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.4.cmml">d</mi><mo id="S3.SS3.SSS1.p2.17.m17.2.2.2.3" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.3.cmml"><mo id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.3" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.3.cmml">(</mo><msub id="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1" xref="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.3" xref="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.4" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.3.cmml">,</mo><msub id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.cmml"><mi id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.2" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.2.cmml">f</mi><mn id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.3" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.5" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p2.17.m17.2.2.3" xref="S3.SS3.SSS1.p2.17.m17.2.2.3.cmml">&gt;</mo><mi id="S3.SS3.SSS1.p2.17.m17.2.2.4" xref="S3.SS3.SSS1.p2.17.m17.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.17.m17.2b"><apply id="S3.SS3.SSS1.p2.17.m17.2.2.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2"><gt id="S3.SS3.SSS1.p2.17.m17.2.2.3.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.3"></gt><apply id="S3.SS3.SSS1.p2.17.m17.2.2.2.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.2"><times id="S3.SS3.SSS1.p2.17.m17.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.3"></times><ci id="S3.SS3.SSS1.p2.17.m17.2.2.2.4.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.4">𝑑</ci><interval closure="open" id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2"><apply id="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.17.m17.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.2">𝑓</ci><cn id="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.17.m17.2.2.2.2.2.2.3">2</cn></apply></interval></apply><ci id="S3.SS3.SSS1.p2.17.m17.2.2.4.cmml" xref="S3.SS3.SSS1.p2.17.m17.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.17.m17.2c">d\left(f_{1},f_{2}\right)&gt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.17.m17.2d">italic_d ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) &gt; italic_θ</annotation></semantics></math>, the interaction type is <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.19.5">conflict</span>, indicating these two modalities provide discrepant information for reasoning.
By further considering how the final answer will change when analyzing information from each modality independently, and combining information from two modalities jointly for reasoning, i.e., the distance function <math alttext="d\left(f_{1},f_{M}\right)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.18.m18.2"><semantics id="S3.SS3.SSS1.p2.18.m18.2a"><mrow id="S3.SS3.SSS1.p2.18.m18.2.2" xref="S3.SS3.SSS1.p2.18.m18.2.2.cmml"><mi id="S3.SS3.SSS1.p2.18.m18.2.2.4" xref="S3.SS3.SSS1.p2.18.m18.2.2.4.cmml">d</mi><mo id="S3.SS3.SSS1.p2.18.m18.2.2.3" xref="S3.SS3.SSS1.p2.18.m18.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p2.18.m18.2.2.2.2" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.3.cmml"><mo id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.3" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.3.cmml">(</mo><msub id="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1" xref="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.2.cmml">f</mi><mn id="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.3" xref="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.4" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.3.cmml">,</mo><msub id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.cmml"><mi id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.2" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.3" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.5" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.18.m18.2b"><apply id="S3.SS3.SSS1.p2.18.m18.2.2.cmml" xref="S3.SS3.SSS1.p2.18.m18.2.2"><times id="S3.SS3.SSS1.p2.18.m18.2.2.3.cmml" xref="S3.SS3.SSS1.p2.18.m18.2.2.3"></times><ci id="S3.SS3.SSS1.p2.18.m18.2.2.4.cmml" xref="S3.SS3.SSS1.p2.18.m18.2.2.4">𝑑</ci><interval closure="open" id="S3.SS3.SSS1.p2.18.m18.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.2"><apply id="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.2">𝑓</ci><cn id="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.18.m18.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.18.m18.2.2.2.2.2.3">𝑀</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.18.m18.2c">d\left(f_{1},f_{M}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.18.m18.2d">italic_d ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT )</annotation></semantics></math> and <math alttext="d\left(f_{2},f_{M}\right)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.19.m19.2"><semantics id="S3.SS3.SSS1.p2.19.m19.2a"><mrow id="S3.SS3.SSS1.p2.19.m19.2.2" xref="S3.SS3.SSS1.p2.19.m19.2.2.cmml"><mi id="S3.SS3.SSS1.p2.19.m19.2.2.4" xref="S3.SS3.SSS1.p2.19.m19.2.2.4.cmml">d</mi><mo id="S3.SS3.SSS1.p2.19.m19.2.2.3" xref="S3.SS3.SSS1.p2.19.m19.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p2.19.m19.2.2.2.2" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.3.cmml"><mo id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.3" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.3.cmml">(</mo><msub id="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1" xref="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.2" xref="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.2.cmml">f</mi><mn id="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.3" xref="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.4" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.3.cmml">,</mo><msub id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.cmml"><mi id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.2" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.2.cmml">f</mi><mi id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.3" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.5" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.19.m19.2b"><apply id="S3.SS3.SSS1.p2.19.m19.2.2.cmml" xref="S3.SS3.SSS1.p2.19.m19.2.2"><times id="S3.SS3.SSS1.p2.19.m19.2.2.3.cmml" xref="S3.SS3.SSS1.p2.19.m19.2.2.3"></times><ci id="S3.SS3.SSS1.p2.19.m19.2.2.4.cmml" xref="S3.SS3.SSS1.p2.19.m19.2.2.4">𝑑</ci><interval closure="open" id="S3.SS3.SSS1.p2.19.m19.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.2"><apply id="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.2">𝑓</ci><cn id="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p2.19.m19.1.1.1.1.1.3">2</cn></apply><apply id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.2">𝑓</ci><ci id="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS1.p2.19.m19.2.2.2.2.2.3">𝑀</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.19.m19.2c">d\left(f_{2},f_{M}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.19.m19.2d">italic_d ( italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT )</annotation></semantics></math>, we can define subdivided interaction type <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib30" title="">30</a>]</cite> as shown in  <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="Figure 1 ‣ 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>B:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.5"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.5.1">Complement-Redundant</span>: when <math alttext="f_{M}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><msub id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">f_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> aligns with <math alttext="f_{1}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><msub id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.2.cmml">f</mi><mn id="S3.I1.i1.p1.2.m2.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><apply id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.2.m2.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.2.m2.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.2">𝑓</ci><cn id="S3.I1.i1.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I1.i1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">f_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{2}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.3.m3.1"><semantics id="S3.I1.i1.p1.3.m3.1a"><msub id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml"><mi id="S3.I1.i1.p1.3.m3.1.1.2" xref="S3.I1.i1.p1.3.m3.1.1.2.cmml">f</mi><mn id="S3.I1.i1.p1.3.m3.1.1.3" xref="S3.I1.i1.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><apply id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.3.m3.1.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.3.m3.1.1.2.cmml" xref="S3.I1.i1.p1.3.m3.1.1.2">𝑓</ci><cn id="S3.I1.i1.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.I1.i1.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">f_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="d\left(f_{1},f_{M}\right)&lt;\theta" class="ltx_Math" display="inline" id="S3.I1.i1.p1.4.m4.2"><semantics id="S3.I1.i1.p1.4.m4.2a"><mrow id="S3.I1.i1.p1.4.m4.2.2" xref="S3.I1.i1.p1.4.m4.2.2.cmml"><mrow id="S3.I1.i1.p1.4.m4.2.2.2" xref="S3.I1.i1.p1.4.m4.2.2.2.cmml"><mi id="S3.I1.i1.p1.4.m4.2.2.2.4" xref="S3.I1.i1.p1.4.m4.2.2.2.4.cmml">d</mi><mo id="S3.I1.i1.p1.4.m4.2.2.2.3" xref="S3.I1.i1.p1.4.m4.2.2.2.3.cmml">⁢</mo><mrow id="S3.I1.i1.p1.4.m4.2.2.2.2.2" xref="S3.I1.i1.p1.4.m4.2.2.2.2.3.cmml"><mo id="S3.I1.i1.p1.4.m4.2.2.2.2.2.3" xref="S3.I1.i1.p1.4.m4.2.2.2.2.3.cmml">(</mo><msub id="S3.I1.i1.p1.4.m4.1.1.1.1.1.1" xref="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.2" xref="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.3" xref="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.I1.i1.p1.4.m4.2.2.2.2.2.4" xref="S3.I1.i1.p1.4.m4.2.2.2.2.3.cmml">,</mo><msub id="S3.I1.i1.p1.4.m4.2.2.2.2.2.2" xref="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.cmml"><mi id="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.2" xref="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.3" xref="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.I1.i1.p1.4.m4.2.2.2.2.2.5" xref="S3.I1.i1.p1.4.m4.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.I1.i1.p1.4.m4.2.2.3" xref="S3.I1.i1.p1.4.m4.2.2.3.cmml">&lt;</mo><mi id="S3.I1.i1.p1.4.m4.2.2.4" xref="S3.I1.i1.p1.4.m4.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.4.m4.2b"><apply id="S3.I1.i1.p1.4.m4.2.2.cmml" xref="S3.I1.i1.p1.4.m4.2.2"><lt id="S3.I1.i1.p1.4.m4.2.2.3.cmml" xref="S3.I1.i1.p1.4.m4.2.2.3"></lt><apply id="S3.I1.i1.p1.4.m4.2.2.2.cmml" xref="S3.I1.i1.p1.4.m4.2.2.2"><times id="S3.I1.i1.p1.4.m4.2.2.2.3.cmml" xref="S3.I1.i1.p1.4.m4.2.2.2.3"></times><ci id="S3.I1.i1.p1.4.m4.2.2.2.4.cmml" xref="S3.I1.i1.p1.4.m4.2.2.2.4">𝑑</ci><interval closure="open" id="S3.I1.i1.p1.4.m4.2.2.2.2.3.cmml" xref="S3.I1.i1.p1.4.m4.2.2.2.2.2"><apply id="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.I1.i1.p1.4.m4.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.1.cmml" xref="S3.I1.i1.p1.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.3.cmml" xref="S3.I1.i1.p1.4.m4.2.2.2.2.2.2.3">𝑀</ci></apply></interval></apply><ci id="S3.I1.i1.p1.4.m4.2.2.4.cmml" xref="S3.I1.i1.p1.4.m4.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.4.m4.2c">d\left(f_{1},f_{M}\right)&lt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.4.m4.2d">italic_d ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) &lt; italic_θ</annotation></semantics></math>, and <math alttext="d\left(f_{2},f_{M}\right)&lt;\theta" class="ltx_Math" display="inline" id="S3.I1.i1.p1.5.m5.2"><semantics id="S3.I1.i1.p1.5.m5.2a"><mrow id="S3.I1.i1.p1.5.m5.2.2" xref="S3.I1.i1.p1.5.m5.2.2.cmml"><mrow id="S3.I1.i1.p1.5.m5.2.2.2" xref="S3.I1.i1.p1.5.m5.2.2.2.cmml"><mi id="S3.I1.i1.p1.5.m5.2.2.2.4" xref="S3.I1.i1.p1.5.m5.2.2.2.4.cmml">d</mi><mo id="S3.I1.i1.p1.5.m5.2.2.2.3" xref="S3.I1.i1.p1.5.m5.2.2.2.3.cmml">⁢</mo><mrow id="S3.I1.i1.p1.5.m5.2.2.2.2.2" xref="S3.I1.i1.p1.5.m5.2.2.2.2.3.cmml"><mo id="S3.I1.i1.p1.5.m5.2.2.2.2.2.3" xref="S3.I1.i1.p1.5.m5.2.2.2.2.3.cmml">(</mo><msub id="S3.I1.i1.p1.5.m5.1.1.1.1.1.1" xref="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.2" xref="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.3" xref="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S3.I1.i1.p1.5.m5.2.2.2.2.2.4" xref="S3.I1.i1.p1.5.m5.2.2.2.2.3.cmml">,</mo><msub id="S3.I1.i1.p1.5.m5.2.2.2.2.2.2" xref="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.cmml"><mi id="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.2" xref="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.3" xref="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.I1.i1.p1.5.m5.2.2.2.2.2.5" xref="S3.I1.i1.p1.5.m5.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.I1.i1.p1.5.m5.2.2.3" xref="S3.I1.i1.p1.5.m5.2.2.3.cmml">&lt;</mo><mi id="S3.I1.i1.p1.5.m5.2.2.4" xref="S3.I1.i1.p1.5.m5.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.5.m5.2b"><apply id="S3.I1.i1.p1.5.m5.2.2.cmml" xref="S3.I1.i1.p1.5.m5.2.2"><lt id="S3.I1.i1.p1.5.m5.2.2.3.cmml" xref="S3.I1.i1.p1.5.m5.2.2.3"></lt><apply id="S3.I1.i1.p1.5.m5.2.2.2.cmml" xref="S3.I1.i1.p1.5.m5.2.2.2"><times id="S3.I1.i1.p1.5.m5.2.2.2.3.cmml" xref="S3.I1.i1.p1.5.m5.2.2.2.3"></times><ci id="S3.I1.i1.p1.5.m5.2.2.2.4.cmml" xref="S3.I1.i1.p1.5.m5.2.2.2.4">𝑑</ci><interval closure="open" id="S3.I1.i1.p1.5.m5.2.2.2.2.3.cmml" xref="S3.I1.i1.p1.5.m5.2.2.2.2.2"><apply id="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.5.m5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.I1.i1.p1.5.m5.1.1.1.1.1.1.3">2</cn></apply><apply id="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.5.m5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.I1.i1.p1.5.m5.2.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.3.cmml" xref="S3.I1.i1.p1.5.m5.2.2.2.2.2.2.3">𝑀</ci></apply></interval></apply><ci id="S3.I1.i1.p1.5.m5.2.2.4.cmml" xref="S3.I1.i1.p1.5.m5.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.5.m5.2c">d\left(f_{2},f_{M}\right)&lt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.5.m5.2d">italic_d ( italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) &lt; italic_θ</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.5"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.5.1">Complement-Distinct</span>: when <math alttext="f_{M}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><msub id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">f_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> distinct from <math alttext="f_{1}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.1"><semantics id="S3.I1.i2.p1.2.m2.1a"><msub id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml"><mi id="S3.I1.i2.p1.2.m2.1.1.2" xref="S3.I1.i2.p1.2.m2.1.1.2.cmml">f</mi><mn id="S3.I1.i2.p1.2.m2.1.1.3" xref="S3.I1.i2.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><apply id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.2.m2.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.1.1.2">𝑓</ci><cn id="S3.I1.i2.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I1.i2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">f_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{2}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.1"><semantics id="S3.I1.i2.p1.3.m3.1a"><msub id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><mi id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml">f</mi><mn id="S3.I1.i2.p1.3.m3.1.1.3" xref="S3.I1.i2.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">𝑓</ci><cn id="S3.I1.i2.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.I1.i2.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">f_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="d\left(f_{1},f_{M}\right)&gt;\theta" class="ltx_Math" display="inline" id="S3.I1.i2.p1.4.m4.2"><semantics id="S3.I1.i2.p1.4.m4.2a"><mrow id="S3.I1.i2.p1.4.m4.2.2" xref="S3.I1.i2.p1.4.m4.2.2.cmml"><mrow id="S3.I1.i2.p1.4.m4.2.2.2" xref="S3.I1.i2.p1.4.m4.2.2.2.cmml"><mi id="S3.I1.i2.p1.4.m4.2.2.2.4" xref="S3.I1.i2.p1.4.m4.2.2.2.4.cmml">d</mi><mo id="S3.I1.i2.p1.4.m4.2.2.2.3" xref="S3.I1.i2.p1.4.m4.2.2.2.3.cmml">⁢</mo><mrow id="S3.I1.i2.p1.4.m4.2.2.2.2.2" xref="S3.I1.i2.p1.4.m4.2.2.2.2.3.cmml"><mo id="S3.I1.i2.p1.4.m4.2.2.2.2.2.3" xref="S3.I1.i2.p1.4.m4.2.2.2.2.3.cmml">(</mo><msub id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.2" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.3" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.I1.i2.p1.4.m4.2.2.2.2.2.4" xref="S3.I1.i2.p1.4.m4.2.2.2.2.3.cmml">,</mo><msub id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.cmml"><mi id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.2" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.3" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.I1.i2.p1.4.m4.2.2.2.2.2.5" xref="S3.I1.i2.p1.4.m4.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.I1.i2.p1.4.m4.2.2.3" xref="S3.I1.i2.p1.4.m4.2.2.3.cmml">&gt;</mo><mi id="S3.I1.i2.p1.4.m4.2.2.4" xref="S3.I1.i2.p1.4.m4.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.2b"><apply id="S3.I1.i2.p1.4.m4.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2"><gt id="S3.I1.i2.p1.4.m4.2.2.3.cmml" xref="S3.I1.i2.p1.4.m4.2.2.3"></gt><apply id="S3.I1.i2.p1.4.m4.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2"><times id="S3.I1.i2.p1.4.m4.2.2.2.3.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2.3"></times><ci id="S3.I1.i2.p1.4.m4.2.2.2.4.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2.4">𝑑</ci><interval closure="open" id="S3.I1.i2.p1.4.m4.2.2.2.2.3.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2"><apply id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.1.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.3.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.3">𝑀</ci></apply></interval></apply><ci id="S3.I1.i2.p1.4.m4.2.2.4.cmml" xref="S3.I1.i2.p1.4.m4.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.2c">d\left(f_{1},f_{M}\right)&gt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.4.m4.2d">italic_d ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) &gt; italic_θ</annotation></semantics></math>, and <math alttext="d\left(f_{2},f_{M}\right)&gt;\theta" class="ltx_Math" display="inline" id="S3.I1.i2.p1.5.m5.2"><semantics id="S3.I1.i2.p1.5.m5.2a"><mrow id="S3.I1.i2.p1.5.m5.2.2" xref="S3.I1.i2.p1.5.m5.2.2.cmml"><mrow id="S3.I1.i2.p1.5.m5.2.2.2" xref="S3.I1.i2.p1.5.m5.2.2.2.cmml"><mi id="S3.I1.i2.p1.5.m5.2.2.2.4" xref="S3.I1.i2.p1.5.m5.2.2.2.4.cmml">d</mi><mo id="S3.I1.i2.p1.5.m5.2.2.2.3" xref="S3.I1.i2.p1.5.m5.2.2.2.3.cmml">⁢</mo><mrow id="S3.I1.i2.p1.5.m5.2.2.2.2.2" xref="S3.I1.i2.p1.5.m5.2.2.2.2.3.cmml"><mo id="S3.I1.i2.p1.5.m5.2.2.2.2.2.3" xref="S3.I1.i2.p1.5.m5.2.2.2.2.3.cmml">(</mo><msub id="S3.I1.i2.p1.5.m5.1.1.1.1.1.1" xref="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.2" xref="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.3" xref="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S3.I1.i2.p1.5.m5.2.2.2.2.2.4" xref="S3.I1.i2.p1.5.m5.2.2.2.2.3.cmml">,</mo><msub id="S3.I1.i2.p1.5.m5.2.2.2.2.2.2" xref="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.cmml"><mi id="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.2" xref="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.3" xref="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.I1.i2.p1.5.m5.2.2.2.2.2.5" xref="S3.I1.i2.p1.5.m5.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.I1.i2.p1.5.m5.2.2.3" xref="S3.I1.i2.p1.5.m5.2.2.3.cmml">&gt;</mo><mi id="S3.I1.i2.p1.5.m5.2.2.4" xref="S3.I1.i2.p1.5.m5.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.5.m5.2b"><apply id="S3.I1.i2.p1.5.m5.2.2.cmml" xref="S3.I1.i2.p1.5.m5.2.2"><gt id="S3.I1.i2.p1.5.m5.2.2.3.cmml" xref="S3.I1.i2.p1.5.m5.2.2.3"></gt><apply id="S3.I1.i2.p1.5.m5.2.2.2.cmml" xref="S3.I1.i2.p1.5.m5.2.2.2"><times id="S3.I1.i2.p1.5.m5.2.2.2.3.cmml" xref="S3.I1.i2.p1.5.m5.2.2.2.3"></times><ci id="S3.I1.i2.p1.5.m5.2.2.2.4.cmml" xref="S3.I1.i2.p1.5.m5.2.2.2.4">𝑑</ci><interval closure="open" id="S3.I1.i2.p1.5.m5.2.2.2.2.3.cmml" xref="S3.I1.i2.p1.5.m5.2.2.2.2.2"><apply id="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.I1.i2.p1.5.m5.1.1.1.1.1.1.3">2</cn></apply><apply id="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.I1.i2.p1.5.m5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.I1.i2.p1.5.m5.2.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.2.cmml" xref="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.3.cmml" xref="S3.I1.i2.p1.5.m5.2.2.2.2.2.2.3">𝑀</ci></apply></interval></apply><ci id="S3.I1.i2.p1.5.m5.2.2.4.cmml" xref="S3.I1.i2.p1.5.m5.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.5.m5.2c">d\left(f_{2},f_{M}\right)&gt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.5.m5.2d">italic_d ( italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) &gt; italic_θ</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.7"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.7.1">Conflict-Dominant</span>: when <math alttext="f_{M}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><msub id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">f_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> aligns with <math alttext="f_{1}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.2.m2.1"><semantics id="S3.I1.i3.p1.2.m2.1a"><msub id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml"><mi id="S3.I1.i3.p1.2.m2.1.1.2" xref="S3.I1.i3.p1.2.m2.1.1.2.cmml">f</mi><mn id="S3.I1.i3.p1.2.m2.1.1.3" xref="S3.I1.i3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.2.m2.1.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.2.m2.1.1.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.2">𝑓</ci><cn id="S3.I1.i3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I1.i3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">f_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> or <math alttext="f_{2}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.3.m3.1"><semantics id="S3.I1.i3.p1.3.m3.1a"><msub id="S3.I1.i3.p1.3.m3.1.1" xref="S3.I1.i3.p1.3.m3.1.1.cmml"><mi id="S3.I1.i3.p1.3.m3.1.1.2" xref="S3.I1.i3.p1.3.m3.1.1.2.cmml">f</mi><mn id="S3.I1.i3.p1.3.m3.1.1.3" xref="S3.I1.i3.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><apply id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.3.m3.1.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.3.m3.1.1.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2">𝑓</ci><cn id="S3.I1.i3.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.I1.i3.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">f_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="d\left(f_{1},f_{M}\right)&lt;\theta" class="ltx_Math" display="inline" id="S3.I1.i3.p1.4.m4.2"><semantics id="S3.I1.i3.p1.4.m4.2a"><mrow id="S3.I1.i3.p1.4.m4.2.2" xref="S3.I1.i3.p1.4.m4.2.2.cmml"><mrow id="S3.I1.i3.p1.4.m4.2.2.2" xref="S3.I1.i3.p1.4.m4.2.2.2.cmml"><mi id="S3.I1.i3.p1.4.m4.2.2.2.4" xref="S3.I1.i3.p1.4.m4.2.2.2.4.cmml">d</mi><mo id="S3.I1.i3.p1.4.m4.2.2.2.3" xref="S3.I1.i3.p1.4.m4.2.2.2.3.cmml">⁢</mo><mrow id="S3.I1.i3.p1.4.m4.2.2.2.2.2" xref="S3.I1.i3.p1.4.m4.2.2.2.2.3.cmml"><mo id="S3.I1.i3.p1.4.m4.2.2.2.2.2.3" xref="S3.I1.i3.p1.4.m4.2.2.2.2.3.cmml">(</mo><msub id="S3.I1.i3.p1.4.m4.1.1.1.1.1.1" xref="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.2" xref="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.3" xref="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.I1.i3.p1.4.m4.2.2.2.2.2.4" xref="S3.I1.i3.p1.4.m4.2.2.2.2.3.cmml">,</mo><msub id="S3.I1.i3.p1.4.m4.2.2.2.2.2.2" xref="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.cmml"><mi id="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.2" xref="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.3" xref="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.I1.i3.p1.4.m4.2.2.2.2.2.5" xref="S3.I1.i3.p1.4.m4.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.I1.i3.p1.4.m4.2.2.3" xref="S3.I1.i3.p1.4.m4.2.2.3.cmml">&lt;</mo><mi id="S3.I1.i3.p1.4.m4.2.2.4" xref="S3.I1.i3.p1.4.m4.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.2b"><apply id="S3.I1.i3.p1.4.m4.2.2.cmml" xref="S3.I1.i3.p1.4.m4.2.2"><lt id="S3.I1.i3.p1.4.m4.2.2.3.cmml" xref="S3.I1.i3.p1.4.m4.2.2.3"></lt><apply id="S3.I1.i3.p1.4.m4.2.2.2.cmml" xref="S3.I1.i3.p1.4.m4.2.2.2"><times id="S3.I1.i3.p1.4.m4.2.2.2.3.cmml" xref="S3.I1.i3.p1.4.m4.2.2.2.3"></times><ci id="S3.I1.i3.p1.4.m4.2.2.2.4.cmml" xref="S3.I1.i3.p1.4.m4.2.2.2.4">𝑑</ci><interval closure="open" id="S3.I1.i3.p1.4.m4.2.2.2.2.3.cmml" xref="S3.I1.i3.p1.4.m4.2.2.2.2.2"><apply id="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.I1.i3.p1.4.m4.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.cmml" xref="S3.I1.i3.p1.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.1.cmml" xref="S3.I1.i3.p1.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.2.cmml" xref="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.3.cmml" xref="S3.I1.i3.p1.4.m4.2.2.2.2.2.2.3">𝑀</ci></apply></interval></apply><ci id="S3.I1.i3.p1.4.m4.2.2.4.cmml" xref="S3.I1.i3.p1.4.m4.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.2c">d\left(f_{1},f_{M}\right)&lt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.4.m4.2d">italic_d ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) &lt; italic_θ</annotation></semantics></math> and <math alttext="d\left(f_{2},f_{M}\right)&gt;\theta" class="ltx_Math" display="inline" id="S3.I1.i3.p1.5.m5.2"><semantics id="S3.I1.i3.p1.5.m5.2a"><mrow id="S3.I1.i3.p1.5.m5.2.2" xref="S3.I1.i3.p1.5.m5.2.2.cmml"><mrow id="S3.I1.i3.p1.5.m5.2.2.2" xref="S3.I1.i3.p1.5.m5.2.2.2.cmml"><mi id="S3.I1.i3.p1.5.m5.2.2.2.4" xref="S3.I1.i3.p1.5.m5.2.2.2.4.cmml">d</mi><mo id="S3.I1.i3.p1.5.m5.2.2.2.3" xref="S3.I1.i3.p1.5.m5.2.2.2.3.cmml">⁢</mo><mrow id="S3.I1.i3.p1.5.m5.2.2.2.2.2" xref="S3.I1.i3.p1.5.m5.2.2.2.2.3.cmml"><mo id="S3.I1.i3.p1.5.m5.2.2.2.2.2.3" xref="S3.I1.i3.p1.5.m5.2.2.2.2.3.cmml">(</mo><msub id="S3.I1.i3.p1.5.m5.1.1.1.1.1.1" xref="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.2" xref="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.3" xref="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S3.I1.i3.p1.5.m5.2.2.2.2.2.4" xref="S3.I1.i3.p1.5.m5.2.2.2.2.3.cmml">,</mo><msub id="S3.I1.i3.p1.5.m5.2.2.2.2.2.2" xref="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.cmml"><mi id="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.2" xref="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.3" xref="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.I1.i3.p1.5.m5.2.2.2.2.2.5" xref="S3.I1.i3.p1.5.m5.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.I1.i3.p1.5.m5.2.2.3" xref="S3.I1.i3.p1.5.m5.2.2.3.cmml">&gt;</mo><mi id="S3.I1.i3.p1.5.m5.2.2.4" xref="S3.I1.i3.p1.5.m5.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.5.m5.2b"><apply id="S3.I1.i3.p1.5.m5.2.2.cmml" xref="S3.I1.i3.p1.5.m5.2.2"><gt id="S3.I1.i3.p1.5.m5.2.2.3.cmml" xref="S3.I1.i3.p1.5.m5.2.2.3"></gt><apply id="S3.I1.i3.p1.5.m5.2.2.2.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2"><times id="S3.I1.i3.p1.5.m5.2.2.2.3.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.3"></times><ci id="S3.I1.i3.p1.5.m5.2.2.2.4.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.4">𝑑</ci><interval closure="open" id="S3.I1.i3.p1.5.m5.2.2.2.2.3.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.2.2"><apply id="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.I1.i3.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.I1.i3.p1.5.m5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.I1.i3.p1.5.m5.1.1.1.1.1.1.3">2</cn></apply><apply id="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.2.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.3.cmml" xref="S3.I1.i3.p1.5.m5.2.2.2.2.2.2.3">𝑀</ci></apply></interval></apply><ci id="S3.I1.i3.p1.5.m5.2.2.4.cmml" xref="S3.I1.i3.p1.5.m5.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.5.m5.2c">d\left(f_{2},f_{M}\right)&gt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.5.m5.2d">italic_d ( italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) &gt; italic_θ</annotation></semantics></math> or switch the <math alttext="f_{1}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.6.m6.1"><semantics id="S3.I1.i3.p1.6.m6.1a"><msub id="S3.I1.i3.p1.6.m6.1.1" xref="S3.I1.i3.p1.6.m6.1.1.cmml"><mi id="S3.I1.i3.p1.6.m6.1.1.2" xref="S3.I1.i3.p1.6.m6.1.1.2.cmml">f</mi><mn id="S3.I1.i3.p1.6.m6.1.1.3" xref="S3.I1.i3.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.6.m6.1b"><apply id="S3.I1.i3.p1.6.m6.1.1.cmml" xref="S3.I1.i3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.6.m6.1.1.1.cmml" xref="S3.I1.i3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.6.m6.1.1.2.cmml" xref="S3.I1.i3.p1.6.m6.1.1.2">𝑓</ci><cn id="S3.I1.i3.p1.6.m6.1.1.3.cmml" type="integer" xref="S3.I1.i3.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.6.m6.1c">f_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.6.m6.1d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{2}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.7.m7.1"><semantics id="S3.I1.i3.p1.7.m7.1a"><msub id="S3.I1.i3.p1.7.m7.1.1" xref="S3.I1.i3.p1.7.m7.1.1.cmml"><mi id="S3.I1.i3.p1.7.m7.1.1.2" xref="S3.I1.i3.p1.7.m7.1.1.2.cmml">f</mi><mn id="S3.I1.i3.p1.7.m7.1.1.3" xref="S3.I1.i3.p1.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.7.m7.1b"><apply id="S3.I1.i3.p1.7.m7.1.1.cmml" xref="S3.I1.i3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.7.m7.1.1.1.cmml" xref="S3.I1.i3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.7.m7.1.1.2.cmml" xref="S3.I1.i3.p1.7.m7.1.1.2">𝑓</ci><cn id="S3.I1.i3.p1.7.m7.1.1.3.cmml" type="integer" xref="S3.I1.i3.p1.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.7.m7.1c">f_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.7.m7.1d">italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.5"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.5.1">Conflict-Distinct</span>: when <math alttext="f_{M}" class="ltx_Math" display="inline" id="S3.I1.i4.p1.1.m1.1"><semantics id="S3.I1.i4.p1.1.m1.1a"><msub id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml"><mi id="S3.I1.i4.p1.1.m1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.I1.i4.p1.1.m1.1.1.3" xref="S3.I1.i4.p1.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><apply id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i4.p1.1.m1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.I1.i4.p1.1.m1.1.1.3.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">f_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> distinct from <math alttext="f_{1}" class="ltx_Math" display="inline" id="S3.I1.i4.p1.2.m2.1"><semantics id="S3.I1.i4.p1.2.m2.1a"><msub id="S3.I1.i4.p1.2.m2.1.1" xref="S3.I1.i4.p1.2.m2.1.1.cmml"><mi id="S3.I1.i4.p1.2.m2.1.1.2" xref="S3.I1.i4.p1.2.m2.1.1.2.cmml">f</mi><mn id="S3.I1.i4.p1.2.m2.1.1.3" xref="S3.I1.i4.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.2.m2.1b"><apply id="S3.I1.i4.p1.2.m2.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i4.p1.2.m2.1.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i4.p1.2.m2.1.1.2.cmml" xref="S3.I1.i4.p1.2.m2.1.1.2">𝑓</ci><cn id="S3.I1.i4.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I1.i4.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.2.m2.1c">f_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.2.m2.1d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> or <math alttext="f_{2}" class="ltx_Math" display="inline" id="S3.I1.i4.p1.3.m3.1"><semantics id="S3.I1.i4.p1.3.m3.1a"><msub id="S3.I1.i4.p1.3.m3.1.1" xref="S3.I1.i4.p1.3.m3.1.1.cmml"><mi id="S3.I1.i4.p1.3.m3.1.1.2" xref="S3.I1.i4.p1.3.m3.1.1.2.cmml">f</mi><mn id="S3.I1.i4.p1.3.m3.1.1.3" xref="S3.I1.i4.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.3.m3.1b"><apply id="S3.I1.i4.p1.3.m3.1.1.cmml" xref="S3.I1.i4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i4.p1.3.m3.1.1.1.cmml" xref="S3.I1.i4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I1.i4.p1.3.m3.1.1.2.cmml" xref="S3.I1.i4.p1.3.m3.1.1.2">𝑓</ci><cn id="S3.I1.i4.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.I1.i4.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.3.m3.1c">f_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="d\left(f_{1},f_{M}\right)&lt;\theta" class="ltx_Math" display="inline" id="S3.I1.i4.p1.4.m4.2"><semantics id="S3.I1.i4.p1.4.m4.2a"><mrow id="S3.I1.i4.p1.4.m4.2.2" xref="S3.I1.i4.p1.4.m4.2.2.cmml"><mrow id="S3.I1.i4.p1.4.m4.2.2.2" xref="S3.I1.i4.p1.4.m4.2.2.2.cmml"><mi id="S3.I1.i4.p1.4.m4.2.2.2.4" xref="S3.I1.i4.p1.4.m4.2.2.2.4.cmml">d</mi><mo id="S3.I1.i4.p1.4.m4.2.2.2.3" xref="S3.I1.i4.p1.4.m4.2.2.2.3.cmml">⁢</mo><mrow id="S3.I1.i4.p1.4.m4.2.2.2.2.2" xref="S3.I1.i4.p1.4.m4.2.2.2.2.3.cmml"><mo id="S3.I1.i4.p1.4.m4.2.2.2.2.2.3" xref="S3.I1.i4.p1.4.m4.2.2.2.2.3.cmml">(</mo><msub id="S3.I1.i4.p1.4.m4.1.1.1.1.1.1" xref="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.2" xref="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.3" xref="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.I1.i4.p1.4.m4.2.2.2.2.2.4" xref="S3.I1.i4.p1.4.m4.2.2.2.2.3.cmml">,</mo><msub id="S3.I1.i4.p1.4.m4.2.2.2.2.2.2" xref="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.cmml"><mi id="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.2" xref="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.3" xref="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.I1.i4.p1.4.m4.2.2.2.2.2.5" xref="S3.I1.i4.p1.4.m4.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.I1.i4.p1.4.m4.2.2.3" xref="S3.I1.i4.p1.4.m4.2.2.3.cmml">&lt;</mo><mi id="S3.I1.i4.p1.4.m4.2.2.4" xref="S3.I1.i4.p1.4.m4.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.4.m4.2b"><apply id="S3.I1.i4.p1.4.m4.2.2.cmml" xref="S3.I1.i4.p1.4.m4.2.2"><lt id="S3.I1.i4.p1.4.m4.2.2.3.cmml" xref="S3.I1.i4.p1.4.m4.2.2.3"></lt><apply id="S3.I1.i4.p1.4.m4.2.2.2.cmml" xref="S3.I1.i4.p1.4.m4.2.2.2"><times id="S3.I1.i4.p1.4.m4.2.2.2.3.cmml" xref="S3.I1.i4.p1.4.m4.2.2.2.3"></times><ci id="S3.I1.i4.p1.4.m4.2.2.2.4.cmml" xref="S3.I1.i4.p1.4.m4.2.2.2.4">𝑑</ci><interval closure="open" id="S3.I1.i4.p1.4.m4.2.2.2.2.3.cmml" xref="S3.I1.i4.p1.4.m4.2.2.2.2.2"><apply id="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.I1.i4.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.I1.i4.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.I1.i4.p1.4.m4.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.cmml" xref="S3.I1.i4.p1.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.1.cmml" xref="S3.I1.i4.p1.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.2.cmml" xref="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.3.cmml" xref="S3.I1.i4.p1.4.m4.2.2.2.2.2.2.3">𝑀</ci></apply></interval></apply><ci id="S3.I1.i4.p1.4.m4.2.2.4.cmml" xref="S3.I1.i4.p1.4.m4.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.4.m4.2c">d\left(f_{1},f_{M}\right)&lt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.4.m4.2d">italic_d ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) &lt; italic_θ</annotation></semantics></math>, and <math alttext="d\left(f_{2},f_{M}\right)&lt;\theta" class="ltx_Math" display="inline" id="S3.I1.i4.p1.5.m5.2"><semantics id="S3.I1.i4.p1.5.m5.2a"><mrow id="S3.I1.i4.p1.5.m5.2.2" xref="S3.I1.i4.p1.5.m5.2.2.cmml"><mrow id="S3.I1.i4.p1.5.m5.2.2.2" xref="S3.I1.i4.p1.5.m5.2.2.2.cmml"><mi id="S3.I1.i4.p1.5.m5.2.2.2.4" xref="S3.I1.i4.p1.5.m5.2.2.2.4.cmml">d</mi><mo id="S3.I1.i4.p1.5.m5.2.2.2.3" xref="S3.I1.i4.p1.5.m5.2.2.2.3.cmml">⁢</mo><mrow id="S3.I1.i4.p1.5.m5.2.2.2.2.2" xref="S3.I1.i4.p1.5.m5.2.2.2.2.3.cmml"><mo id="S3.I1.i4.p1.5.m5.2.2.2.2.2.3" xref="S3.I1.i4.p1.5.m5.2.2.2.2.3.cmml">(</mo><msub id="S3.I1.i4.p1.5.m5.1.1.1.1.1.1" xref="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.2" xref="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.2.cmml">f</mi><mn id="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.3" xref="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S3.I1.i4.p1.5.m5.2.2.2.2.2.4" xref="S3.I1.i4.p1.5.m5.2.2.2.2.3.cmml">,</mo><msub id="S3.I1.i4.p1.5.m5.2.2.2.2.2.2" xref="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.cmml"><mi id="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.2" xref="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.2.cmml">f</mi><mi id="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.3" xref="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.3.cmml">M</mi></msub><mo id="S3.I1.i4.p1.5.m5.2.2.2.2.2.5" xref="S3.I1.i4.p1.5.m5.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.I1.i4.p1.5.m5.2.2.3" xref="S3.I1.i4.p1.5.m5.2.2.3.cmml">&lt;</mo><mi id="S3.I1.i4.p1.5.m5.2.2.4" xref="S3.I1.i4.p1.5.m5.2.2.4.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.5.m5.2b"><apply id="S3.I1.i4.p1.5.m5.2.2.cmml" xref="S3.I1.i4.p1.5.m5.2.2"><lt id="S3.I1.i4.p1.5.m5.2.2.3.cmml" xref="S3.I1.i4.p1.5.m5.2.2.3"></lt><apply id="S3.I1.i4.p1.5.m5.2.2.2.cmml" xref="S3.I1.i4.p1.5.m5.2.2.2"><times id="S3.I1.i4.p1.5.m5.2.2.2.3.cmml" xref="S3.I1.i4.p1.5.m5.2.2.2.3"></times><ci id="S3.I1.i4.p1.5.m5.2.2.2.4.cmml" xref="S3.I1.i4.p1.5.m5.2.2.2.4">𝑑</ci><interval closure="open" id="S3.I1.i4.p1.5.m5.2.2.2.2.3.cmml" xref="S3.I1.i4.p1.5.m5.2.2.2.2.2"><apply id="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.I1.i4.p1.5.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.I1.i4.p1.5.m5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.2">𝑓</ci><cn id="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.I1.i4.p1.5.m5.1.1.1.1.1.1.3">2</cn></apply><apply id="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.I1.i4.p1.5.m5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.I1.i4.p1.5.m5.2.2.2.2.2.2">subscript</csymbol><ci id="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.2.cmml" xref="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.2">𝑓</ci><ci id="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.3.cmml" xref="S3.I1.i4.p1.5.m5.2.2.2.2.2.2.3">𝑀</ci></apply></interval></apply><ci id="S3.I1.i4.p1.5.m5.2.2.4.cmml" xref="S3.I1.i4.p1.5.m5.2.2.4">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.5.m5.2c">d\left(f_{2},f_{M}\right)&lt;\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.p1.5.m5.2d">italic_d ( italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) &lt; italic_θ</annotation></semantics></math></p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS3.SSS1.p2.20">In this paper, we primarily focus on the visual and language modalities, which are the main subjects of investigation in current multimodal LLM research. For more modalities, the interaction characterization framework can be extended by pairwise comparison.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Multimodal Reasoning Pattern Mining</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Upon gaining insight into the model’s reasoning process at the modality interaction level, it becomes crucial to identify the specific concepts or their combinations within and across individual modalities the model utilizes for reasoning.
<span class="ltx_ERROR undefined" id="S3.SS3.SSS2.p1.1.1">\jianben</span>As shown in <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="In 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>B,
we parse the generated <span class="ltx_ERROR undefined" id="S3.SS3.SSS2.p1.1.2">\jianben</span>rationale into a list of intermediate evidence along with their associated inferences that contribute to the final answer. For example, within a free-text <span class="ltx_ERROR undefined" id="S3.SS3.SSS2.p1.1.3">\jianben</span>rationale generated by the LLM, <span class="ltx_ERROR undefined" id="S3.SS3.SSS2.p1.1.4">\jianben</span><span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.5">“The serious expression suggests a neutral sentiment, while in the spoken content, the phrase ’incredible command’ conveys a positive sentiment.</span> The visual evidence “serious expression” infers “neutral” sentiment and the language evidence “incredible command” implies “positive” sentiment.
Given the LLM’s generative characteristics resulting in the variability of evidence across different <span class="ltx_ERROR undefined" id="S3.SS3.SSS2.p1.1.6">\jianben</span>rationales, we employed the <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.7">text-embedding-3-small<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote3.1.1.1">3</span></span><span class="ltx_text ltx_font_upright" id="footnote3.9">https://platform.openai.com/docs/guides/embeddings</span></span></span></span></span> model to calculate embeddings for all extracted evidence (e.g., ”serious expression” and ”incredible command” et al). Subsequently, we utilized the HDBSCAN algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib40" title="">40</a>]</cite> to cluster visual and language evidence respectively. We identified the evidence located closest to the cluster’s centroid as the representative concept for each cluster. Subsequently, we utilized the Apriori <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib1" title="">1</a>]</cite> algorithm to identify frequent patterns of concept co-occurrence within and across different modalities in the generated rationales <span class="ltx_ERROR undefined" id="S3.SS3.SSS2.p1.1.8">\jianben</span>for validation set.
This approach enables users to conduct a more structured and <span class="ltx_ERROR undefined" id="S3.SS3.SSS2.p1.1.9">\jianben</span>comprehensive analysis of the patterns within the generated rationales, allowing them to identify potential recurring biases or errors made by the model.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Prompt Iteration Strategy Recommendation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">As mentioned in  <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S1" title="1 Related works ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">1</span></a>, the content and phrasing of task instructions, along with the choice of demonstration examples, can greatly influence the model’s reasoning performance.
Our preliminary experiment and expert interview results suggested that the instruction content (e.g., task specifications) and the choice of demonstration examples exert a more pronounced effect on model performance than the precise wording used for our targeted multimodal reasoning tasks.
Therefore, in this paper, we mainly focus on facilitating users in instruction content refinement and demonstration example construction.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>K-shot Example Recommendation</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">Few-shot prompting has been a data-efficient strategy to adapt LLMs for specific downstream tasks using merely a handful of illustrative input-output pairs.
However, the effectiveness heavily relies on the choice of examples to inform the model about the desired mapping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib21" title="">21</a>]</cite>.
Identifying informative examples for effectively guiding the model can be challenging for users.
Moreover, beyond simply pairing inputs with final answers, reasoning necessitates providing a rationale for each example, which is equally difficult for users to craft on their own.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.3">To enhance the efficiency of sourcing the demonstration example set, we first employed the k-nearest neighbors algorithm to sample the candidate k-shot example set considering both relevancy and diversity.
<span class="ltx_ERROR undefined" id="S3.SS4.SSS1.p2.3.1">\jianben</span>For each instance in both the validation set and demonstration example sets, we computed embeddings for the visual (images) and language (text transcript) modalities separately and then concatenated these embeddings to represent each instance.
Specifically, we utilized the pre-trained CLIP<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/sentence-transformers/clip-ViT-B-16</span></span></span> model, which maps text and images to a shared vector space for embedding computation. Further details are provided in the supplementary material.
<span class="ltx_ERROR undefined" id="S3.SS4.SSS1.p2.3.2">\jianben</span>For each validation instance, we identified its k-nearest neighbors as potential candidates based on their embedding cosine similarity.
These candidates were then ranked in descending order of similarity.
To select the final k-shot examples, we prioritized both ranking and label diversity, ensuring the inclusion of all possible labels in the final set to prevent model bias.
To streamline the process of crafting rationales for users, we integrated the <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS1.p2.3.3">gpt-4-turbo model</span> <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4</span></span></span> to automatically generate structured rationales for each demonstration example based on its ground truth labels.
This approach offers users a preliminary basis for refinement, sparing them the need to begin from scratch.
Furthermore, we utilized the refinements operated by users to iteratively enhance the quality of the generated rationales.
These demonstration examples are then combined into the sequence<math alttext="\left\{x_{i},y_{i}\right\}_{i=1}^{k}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.1.m1.2"><semantics id="S3.SS4.SSS1.p2.1.m1.2a"><msubsup id="S3.SS4.SSS1.p2.1.m1.2.2" xref="S3.SS4.SSS1.p2.1.m1.2.2.cmml"><mrow id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.3.cmml"><mo id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.3" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.3.cmml">{</mo><msub id="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1" xref="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.3" xref="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.4" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.cmml"><mi id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.2" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.2.cmml">y</mi><mi id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.3" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.5" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.3.cmml">}</mo></mrow><mrow id="S3.SS4.SSS1.p2.1.m1.2.2.2.4" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.4.cmml"><mi id="S3.SS4.SSS1.p2.1.m1.2.2.2.4.2" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.4.2.cmml">i</mi><mo id="S3.SS4.SSS1.p2.1.m1.2.2.2.4.1" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.4.1.cmml">=</mo><mn id="S3.SS4.SSS1.p2.1.m1.2.2.2.4.3" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.4.3.cmml">1</mn></mrow><mi id="S3.SS4.SSS1.p2.1.m1.2.2.4" xref="S3.SS4.SSS1.p2.1.m1.2.2.4.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.1.m1.2b"><apply id="S3.SS4.SSS1.p2.1.m1.2.2.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.1.m1.2.2.3.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2">superscript</csymbol><apply id="S3.SS4.SSS1.p2.1.m1.2.2.2.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.1.m1.2.2.2.3.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2">subscript</csymbol><set id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.3.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2"><apply id="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS1.p2.1.m1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.2.2.2.3">𝑖</ci></apply></set><apply id="S3.SS4.SSS1.p2.1.m1.2.2.2.4.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.4"><eq id="S3.SS4.SSS1.p2.1.m1.2.2.2.4.1.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.4.1"></eq><ci id="S3.SS4.SSS1.p2.1.m1.2.2.2.4.2.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.4.2">𝑖</ci><cn id="S3.SS4.SSS1.p2.1.m1.2.2.2.4.3.cmml" type="integer" xref="S3.SS4.SSS1.p2.1.m1.2.2.2.4.3">1</cn></apply></apply><ci id="S3.SS4.SSS1.p2.1.m1.2.2.4.cmml" xref="S3.SS4.SSS1.p2.1.m1.2.2.4">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.1.m1.2c">\left\{x_{i},y_{i}\right\}_{i=1}^{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS1.p2.1.m1.2d">{ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math> for inclusion in the prompt, where <math alttext="y_{i}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.2.m2.1"><semantics id="S3.SS4.SSS1.p2.2.m2.1a"><msub id="S3.SS4.SSS1.p2.2.m2.1.1" xref="S3.SS4.SSS1.p2.2.m2.1.1.cmml"><mi id="S3.SS4.SSS1.p2.2.m2.1.1.2" xref="S3.SS4.SSS1.p2.2.m2.1.1.2.cmml">y</mi><mi id="S3.SS4.SSS1.p2.2.m2.1.1.3" xref="S3.SS4.SSS1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.2.m2.1b"><apply id="S3.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS4.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS4.SSS1.p2.2.m2.1.1.2">𝑦</ci><ci id="S3.SS4.SSS1.p2.2.m2.1.1.3.cmml" xref="S3.SS4.SSS1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.2.m2.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS1.p2.2.m2.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the rationale and final answer provided by users for <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.3.m3.1"><semantics id="S3.SS4.SSS1.p2.3.m3.1a"><msub id="S3.SS4.SSS1.p2.3.m3.1.1" xref="S3.SS4.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS4.SSS1.p2.3.m3.1.1.2" xref="S3.SS4.SSS1.p2.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS4.SSS1.p2.3.m3.1.1.3" xref="S3.SS4.SSS1.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p2.3.m3.1b"><apply id="S3.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS4.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS4.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS4.SSS1.p2.3.m3.1.1.2">𝑥</ci><ci id="S3.SS4.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS4.SSS1.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p2.3.m3.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS1.p2.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="Figure 1 ‣ 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>C).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Instructional Principle Generation</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">While k-shot examples aim to inductively teach the model the correct mappings between input-output pairs, providing explicit principles regarding proper practices or clarifying potential errors has also proven to be an effective strategy for drawing out desired knowledge and guiding model performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib44" title="">44</a>]</cite>.
Humans generally formulate principles in two ways. One involves directly leveraging their existing knowledge. For example, the principle for identifying sarcasm could be to “pay attention to the inconsistency between a word’s literal interpretation and its contextual meaning.”
The other is that individuals derive lessons from specific instances and subsequently aggregate these instance-level insights into higher-level principles in a bottom-up manner.
However, users may find it difficult to immediately generate principles from scratch, derive insights by manually examining instances one at a time, and fully articulate their principles considering the complexity of multimodal reasoning.
<span class="ltx_ERROR undefined" id="S3.SS4.SSS2.p1.1.1">\jianben</span>For this purpose, we employed an auxiliary LLM to facilitate the summarization and recommendation of principles. We selected the <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p1.1.2">gpt-4-turbo model</span> for its strong capabilities in text understanding and summarization, and it can be replaced by more advanced models in the future.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">Specifically, we instructed the <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p2.1.1">gpt-4-turbo model</span> to produce principles at both instance-specific and instance-agnostic levels (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.03843v3#S2.F1" title="Figure 1 ‣ 2 Design Requirements ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>C).
At the instance-specific level, the model is tasked with analyzing discrepancies between generated reasoning and ground truth answers for each instance, summarizing potential error causes, and further deriving principles to avoid similar mistakes.
At the instance-agnostic level, We instruct the model to condense the generated instance-specific principles into more generic principles tailored to the specific targeted task.
It is important to acknowledge that the generated principles may not always be accurate and should not be treated as golden rules. Their primary purpose is to provoke thought and inspire users to conceive new ideas or enhance existing ones rather than initiate from zero.
Thus, users are empowered to either input and create their principles or choose to amend and revise principles that have already been generated according to their preferences.
<span class="ltx_ERROR undefined" id="S3.SS4.SSS2.p2.1.2">\jianben</span>Details regarding the prompt used for principle generation are provided in the supplementary material.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Interface Design</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The <span class="ltx_ERROR undefined" id="S4.p1.1.1">\name</span>interface (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>) consists of three coordinated views to assist users in seamlessly evaluating the impact of different prompts, refining prompts through semi-automatic suggestions, and conducting iterative testing of prompts. In this section, we introduce the design of each view and the interactions that connect them in detail.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Prompt Panel</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">Prompt Panel</span> (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>A) provides flexible prompt operations to support smooth prompt engineering experience (<span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">R3</span>).
Upon selecting the dataset and model, users can craft the prompt on their own or initiate by selecting from a list of prompt templates collected from state-of-the-art benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib14" title="">14</a>]</cite> in the <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.3">Prompt Editor</span> (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>A-1).
The prompt is organized into distinct sections, as introduced in  <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS2" title="3.2 Dataset and Model ‣ 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, to facilitate a clear and straightforward editing experience. Users can also switch to the plain text editing mode for editing and format checking before submission.
The <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.4">Principle Recommendation</span> view (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>A-2) displays an organized summary of principles for user validation.
The generated instance-specific and agnostic principles are differentiated by background colors: gray for instance-specific principles and green for instance-agnostic principles.
Newly generated principles are marked with red dots at the top right for highlighting.
Users can modify any existing principle by utilizing the editing function or articulate their principles via the principle input box. Furthermore, users are allowed to delete any principles deemed inappropriate or redundant. Subsequently, upon selecting the desired principles, users can integrate them into the current prompt within the prompt editor by clicking the “Import <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="12" id="S4.SS1.p1.1.g1" src="extracted/5886160/figs/import-button.png" width="9"/> ” button.
The <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.5">K-shot Example List</span> (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>A-3) below provides a concise summary of K-shot examples with user-annotated rationales, waiting for further editing or inclusion into the prompt.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Reasoning Panel</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Reasoning Panel</span> (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>B) facilitates a thorough investigation of the model’s multimodal reasoning behaviors, from global and sub-group patterns down to specific individual instances (<span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">R1</span>).</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">A three-layer Sankey diagram-based design (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>B-1) is adopted to portray interactions among modalities at the global level.
The first layer demonstrates the overall distribution of prediction classes and errors with two vertically stacked barcode charts. The horizontal length encodes the number of instances, and the color encodes the corresponding class and error. Instances belonging to the same class are positioned close together to enable easy exploration both within and between classes.
The second intermediate layer summarizes the conflict and complement relationship between visual and language modalities and adopts the same encoding as the first layer.
The third layer delves into the fine-grained four types of modality interactions. While retaining the same visual encoding for the prediction class and error distribution in each interaction type, this layer introduces two additional barcode charts to delineate the prediction result of the single visual and language modality, thus illustrating the detailed distributions of the two modalities across various types of interactions.
Besides, two adjacent layers are interconnected through flows, the width of which is proportional to the number of instances they encompass. Hovering over the flows will highlight the related instances across all three layers.
Users can also brush the barcode chart in each layer to select an interested group of instances for further investigation. <span class="ltx_ERROR undefined" id="S4.SS2.p2.1.1">\jianben</span>The selected instances will be highlighted with a grey background. <span class="ltx_ERROR undefined" id="S4.SS2.p2.1.2">\jianben</span>The corresponding mined patterns and instances will be displayed on the right and below respectively.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">After selecting the interested group of instances, the multimodal reasoning pattern mining algorithm in  <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS3" title="3.3 Multimodal Rationale Understanding ‣ 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a> is applied, with the extracted patterns displayed in the table on the right (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>B-2). Each row exhibits one distinct pattern with its representative visual and language concepts, support (i.e., contained instance numbers), and error statistics. They can sort and filter the patterns based on these statistics by clicking on the corresponding column. The representative language and visual concepts are shown for intuitive pattern understanding by users. Adjacent to each concept, a stacked bar chart presents the distribution of its associated class.
Users can expand each row to view the detailed distribution of evidence in a word cloud, where each phrase’s size represents its frequency of occurrence and its color denotes the proportion of associated classes.
Users can select patterns or evidence of interest by clicking, and the corresponding instances will be displayed in the instance view below.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.3">The instance view below (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>B-3) is designed to expedite the examination and verification of individual instances by showcasing the original multimodal video content along with its detailed reasoning. The raw data column exhibits the video’s keyframe image sequence and spoken content to enable quick visual and language content digestion and <span class="ltx_ERROR undefined" id="S4.SS2.p4.3.1">\jianben</span>validation. Users can hover over these frames for an enlarged view and playback the original video for rapid verification.
Subsequent columns present the ground truth labels, the model’s predictions, and the generated free-text rationales. To enhance readability and quick text comprehension, evidence is highlighted with the corresponding color of its associated class. The ground truth and prediction columns are also colored for easy comparison.
Users can select instances for principle generation by clicking the “Generate <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="11" id="S4.SS2.p4.1.g1" src="x1.png" width="12"/> ” button, after which the generated principles will be listed in the <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.3.2">Principle Recommendation</span> view.
In addition to displaying selected validation instances for review, users can toggle to the <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.3.3">K-shot Example Mode</span>( <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.03843v3#S5.F5" title="Figure 5 ‣ 5.2 Case Two: Enhancing Multimodal User Intention Understanding with WTaG dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>A). Within this mode, the interface presents the ranked list of k-shot examples recommended by the proposed sampling strategy in  <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S3.SS4" title="3.4 Prompt Iteration Strategy Recommendation ‣ 3 System &amp; Methods ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>. For each example, the interface details its raw data (i.e., keyframe sequence and spoken narratives in the raw video), its ground truth, and the rationales. Users can modify the content in the corresponding column directly to provide high-quality rationales. Moreover, users can source more k-shot examples by clicking the “Retrieve <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="S4.SS2.p4.2.g2" src="x2.png" width="12"/> ” button and save the selected ones to the <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.3.4">K-shot Example List</span> with the “Save <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="S4.SS2.p4.3.g3" src="x3.png" width="12"/> ” button.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation Panel</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Evaluation Panel</span> (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>C) offers comprehensive insights into both global and local performance of prompts, along with the prompt iteration history for efficient monitoring and comparison of prompt performance (<span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.2">R2</span>).</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2">The <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.2.1">Prompt History</span> view (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>C-1) archives previous prompts regarding their content and performance. Each row represents a prompt version with its accuracy and modifications are summarized using intuitive icons. This design enables users to easily compare performance and trace alterations in different sections of the prompts.
Users can expand and collapse each row for a hierarchical examination of modifications within each prompt section.
Detailed additions and deletions in the content are distinctly marked and highlighted through varied colors and line styles.
<span class="ltx_ERROR undefined" id="S4.SS3.p2.2.2">\jianben</span>The line chart below shows the model accuracy change.
The <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.2.3">Overall Model Performance</span> view (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>C-2) records the global performance statistics of each prompt iteration.
Users can expand each row to inspect the detailed confusion matrix.
The <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.2.4">Instance Test</span> view (<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>C-3) exhibits the performance of prompts on individual instances that are of particular interest to users.
Users can select instances from <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.2.5">Reasoning Panel</span> and save them to observe their performance change during prompt iterations with the “Save <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="S4.SS3.p2.1.g1" src="x4.png" width="12"/> ” button.
They can also source additional unseen test instances with the “Retrieve <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="12" id="S4.SS3.p2.2.g2" src="x5.png" width="12"/> ” function.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we showcase the efficacy and efficiency of <span class="ltx_ERROR undefined" id="S5.p1.1.1">\name</span>via two case studies and feedback gathered from expert interviews.
The primary objective of the two case studies is to help users obtain well-performing prompts utilizing their domain expertise and knowledge to guide LLM’s multimodal reasoning performance with minimal effort.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">E5</span>, a sentiment analysis expert, seeks to generate effective prompts for steering LLM’s multimodal sentiment reasoning performance with the CMU-MOSEI dataset. The LLM is tasked with interpreting the speakers’ verbal and visual signals to determine their sentiment as “positive”, “negative”, or “neutral”.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">After loading the dataset and model, <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">E5</span> initially selected and submitted a provided prompt template in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.2">Prompt Panel</span> to evaluate its performance (<span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.3">R2</span>), <span class="ltx_ERROR undefined" id="S5.SS1.p2.1.4">\jianben</span>which yielded an accuracy of 70%.
To gain an overview of the interactions between the visual and language modalities in the LLM’s reasoning process (<span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.5">R1</span>), <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.6">E5</span> began by examining the Sankey diagram in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.7">Reasoning Panel</span>.
<span class="ltx_ERROR undefined" id="S5.SS1.p2.1.8">\jianben</span>Through observing the length and error distribution of the barcode charts in the first and second layers, <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.9">E5</span> noticed that the model tended to interpret sentiments as “neutral” or “positive” rather than “negative”.
Furthermore, in a large proportion of instances, the visual and language modalities provided complementary information, while in others, they presented conflicting information with increased errors.
<span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.10">E5</span> was particularly interested in how the LLM reasoned in scenarios where the two modalities <span class="ltx_ERROR undefined" id="S5.SS1.p2.1.11">\jianben</span>presented conflicting information and how errors occurred. So, she <span class="ltx_ERROR undefined" id="S5.SS1.p2.1.12">\jianben</span>explored the third layer for a more fine-grained examination.
At the third layer, she <span class="ltx_ERROR undefined" id="S5.SS1.p2.1.13">\jianben</span>identified a dense <span class="ltx_ERROR undefined" id="S5.SS1.p2.1.14">\jianben</span>cluster of errors within the <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.15">conflict-dominant</span> relationship (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F2" title="In 5.1 Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>A), where the visual modality implied a positive influence, while the language modality suggested a negative one. The ultimate combined effect was positive, indicating that the visual modality <span class="ltx_ERROR undefined" id="S5.SS1.p2.1.16">\jianben</span>dominated the reasoning process.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Following this, <span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">E5</span> brushed this group of instances to further inspect their contained reasoning patterns in the table on the right.
When going through the patterns <span class="ltx_ERROR undefined" id="S5.SS1.p3.1.2">\jianben</span>sorted in descending order of error rate, <span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.3">E5</span> discovered the combination of the language concept “didn’t like” with the visual concept “smile” yielded high error rates (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F2" title="In 5.1 Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>B).
The adjacent bar charts, predominantly <span class="ltx_ERROR undefined" id="S5.SS1.p3.1.4">\jianben</span>colored in blue for “didn’t like” and red for “smile”, indicated that the LLM consistently interpreted language evidence concluded with “didn’t like” as a negative signal and visual evidence <span class="ltx_ERROR undefined" id="S5.SS1.p3.1.5">\jianben</span>featuring “smile” as positive during the reasoning process.
She further explored this pattern <span class="ltx_ERROR undefined" id="S5.SS1.p3.1.6">\jianben</span>by unfolding the row, where evidence under the language concept “didn’t like” included phrases like “arduous”, “boring”, and “hate” highlighted in blue (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F2" title="In 5.1 Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>B-1), while the visual concept “smile” comprised instances such as “small smile”, “slight smile”, and “smiling” marked in red (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F2" title="In 5.1 Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>B-2).
<span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.7">E5</span> thought these inferences for each modality reasonable but wondered how the correctly deduced evidence led to the final error. Therefore, she proceeded to inspect the detailed reasonings of individual instances exhibiting this pattern in the instance view below (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F2" title="In 5.1 Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>C).
Upon examining the raw data and the rationales generated by the LLM, <span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.8">E5</span> figured out that the model correctly reasoned about individual modality, as in these instances, the speakers had explicitly stated their negative opinions verbally while <span class="ltx_ERROR undefined" id="S5.SS1.p3.1.9">\jianben</span>showing mild positive facial expressions like gentle smiles.
However, the LLM was biased by the positive visual cues, allowing them to overshadow and dominate its reasoning, despite the explicit negative sentiment conveyed through language.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="614" id="S5.F2.g1" src="extracted/5886160/figs/case_one-first.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S5.F2.4.2" style="font-size:90%;">(A) Identified dense error areas in <span class="ltx_text ltx_font_italic" id="S5.F2.4.2.1">conflict-dominant</span> modality interaction. (B) The multimodal pattern “didn’t like” and “smile” and their associated evidence group. (C) The error cases where “smile” predominated and biased the reasoning process.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Following this discovery, <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">E5</span> decided to derive principles from these erroneous cases to guide the LLM toward correct reasoning in this situation (<span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.2">R3</span>). Therefore, <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.3">E5</span> selected these instances and clicked the “Generate <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="11" id="S5.SS1.p4.1.g1" src="x6.png" width="12"/> ” button to generate principles. She also saved these instances of interest to the right <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.4">test panel</span> for further validation.
In the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.5">Principle Recommendation View</span>, <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.6">E5</span> reviewed the generated principles and identified well-articulated general principles that underscored the importance of interpreting visual cues alongside the corresponding verbal content with careful consideration of specific context (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F3" title="In 5.1 Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>A).
To ensure generalizability and avoid introducing new bias, <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.7">E5</span> <span class="ltx_ERROR undefined" id="S5.SS1.p4.1.8">\jianben</span>revised the last sentence as “<span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.9">It is crucial to avoid overemphasizing one modality over another when the latter carries clear indications of opinions or explicit expressions of sentiment.</span>”
Then, <span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.10">E5</span> imported this principle into the prompt editor and submitted it for testing. In the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.11">Model Performance View</span>, she found a slight improvement in the overall accuracy from <span class="ltx_ERROR undefined" id="S5.SS1.p4.1.12">\jianben</span>70% to <span class="ltx_ERROR undefined" id="S5.SS1.p4.1.13">\jianben</span>74%. Meanwhile, in the <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.14">Test Panel View</span>, she checked the performance of the new prompt on previously saved instances, the majority of which were now correctly reasoned (<span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</span></span>C).
This indicated that the incorporated principle had <span class="ltx_ERROR undefined" id="S5.SS1.p4.1.15">\jianben</span>effectively guided the LLM to use the correct knowledge for reasoning in this scenario.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">Subsequently, <span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.1">E5</span> sought to enhance the model’s reasoning stability and its ability to recognize varied patterns in sentiment analysis by incorporating some k-shot examples (<span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.2">R3</span>).
Thus, she switched to <span class="ltx_text ltx_font_italic" id="S5.SS1.p5.1.3">K-shot Example Mode</span>, where recommended K-shot examples with reasonings crafted by the auxiliary LLM were listed. <span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.4">E5</span> selected the top three instances spanning distinct classes and refined the provided reasoning leveraging his knowledge and expertise.
Upon completing the rationale annotations, <span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.5">E5</span> appended these examples to the K-shot example list on the left side and imported them into the prompt.
After running the test, the overall accuracy increased to 82% (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F3" title="In 5.1 Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>B).</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S5.F3.g1" src="extracted/5886160/figs/case_one-second.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S5.F3.3.2" style="font-size:90%;">(A) The recommended principles for alleviating errors in case one. (B) The recorded prompt iteration history in case one.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Case Two: Enhancing Multimodal User Intention Understanding with WTaG dataset</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">E6</span> is an engineer tasked with building an intelligent virtual assistant to help users perform complex tasks within augmented reality environments.
Building such an assistant necessitates comprehending user intentions.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.2">E6</span> thus wanted to steer the GPT-4V(ision) model using <span class="ltx_ERROR undefined" id="S5.SS2.p1.1.3">\name</span>to finish this task.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.4">E6</span> experimented with the WTaG dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib3" title="">3</a>]</cite>, <span class="ltx_ERROR undefined" id="S5.SS2.p1.1.5">\jianben</span>where the video clips were recorded from the user’s egocentric perspective.
These clips included user-instructor dialogues captured by microphones and visual context encompassing the scene and user behaviors from head-mounted cameras.
The multimodal LLM needs to deduce the user’s intention based on this multimodal context and categorize it into one of five classes: “Question”, “Answer”, “Confirmation”, “Hesitation” and “Self Description”.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">After initializing the dataset and model, <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">E6</span> first chose to use the prompt <span class="ltx_ERROR undefined" id="S5.SS2.p2.1.2">\jianben</span>provided in the original dataset repository for validation (<span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.3">R2</span>). The <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.4">Evaluation Panel</span> revealed that this prompt <span class="ltx_ERROR undefined" id="S5.SS2.p2.1.5">\jianben</span>achieved only 53% accuracy in a zero-shot setting. While this result is higher than what was reported in the paper using <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.6">gpt-3.5-turbo model</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib3" title="">3</a>]</cite>, it is still insufficient for the task.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.7">E6</span> next examined the confusion matrix and noticed that the model’s predictions were heavily biased towards the “ Confirmation” and “Answer” classes ( <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.03843v3#S5.F2" title="Figure 2 ‣ 5.1 Case One: Improving multimodal sentiment reasoning with CMU-MOSEI dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>A).
Upon randomly inspecting the model-generated rationales alongside the raw data incorrectly classified in the <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.8">Reasoning Panel</span>, <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.9">E6</span> observed that while the model could adequately describe and analyze both visual and spoken content, it struggled to comprehend the meaning of designated prediction classes, especially “Self Description.” This resulted in scarce predictions for this class and a bias towards more familiar classes such as “Confirmation” and “Answer”.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="208" id="S5.F4.g1" src="extracted/5886160/figs/case_two-first.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">(A) The confusion matrix showing the model’s prediction bias towards the “Confirmation” and “Answer” classes. (B) The recorded prompt iteration history in case two.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">To address this problem, <span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">E6</span> decided to include more explicit <span class="ltx_ERROR undefined" id="S5.SS2.p3.1.2">\jianben</span>rationales of each prediction class within the prompt instructions <span class="ltx_ERROR undefined" id="S5.SS2.p3.1.3">\jianben</span>to guide the model. (<span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.4">R3</span>).
Thus, he revised the prompt to add the clarification such as <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.5">“Self Description refers to scenarios where the user narrates or explains what they are doing, intend to do, or their thought process regarding the task at hand.”</span>
While submitting this prompt for testing, <span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.6">E6</span> also thought that, besides <span class="ltx_ERROR undefined" id="S5.SS2.p3.1.7">\jianben</span>providing explicit <span class="ltx_ERROR undefined" id="S5.SS2.p3.1.8">\jianben</span>rationales, he could also <span class="ltx_ERROR undefined" id="S5.SS2.p3.1.9">\jianben</span>include concrete k-shot examples to help the model learn (<span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.10">R3</span>).
He navigated to the <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.11">K-shot Example Mode</span> in the <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.12">Reasoning Panel</span> and selected five K-shot examples, each representing a distinct class from the top recommended ones (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F5" title="In 5.2 Case Two: Enhancing Multimodal User Intention Understanding with WTaG dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>A).
<span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.13">E6</span> also noticed that the rationales generated by the more advanced auxiliary LLM also contained errors for the “Self Description” class, indicating that this category might be challenging for LLMs to grasp and reason about, underscoring the need for providing additional guidance in the prompt.
Following the refinement of rationales for the k-shot examples, <span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.14">E6</span> imported these annotated examples and submitted this prompt version for testing.
<span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.15">E6</span> then examined the updated test outcomes in the <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.16">Evaluation Panel</span> (<span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.17">R2</span>).
The increased <span class="ltx_ERROR undefined" id="S5.SS2.p3.1.18">\jianben</span>performance statistics proved that providing either explicit explanations or k-shot examples can help improve the LLM’s reasoning performance ( <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.03843v3#S5.F4" title="Figure 4 ‣ 5.2 Case Two: Enhancing Multimodal User Intention Understanding with WTaG dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="297" id="S5.F5.g1" src="extracted/5886160/figs/case_two-second.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.3.2" style="font-size:90%;">(A) The selected and annotated k-shot examples from distinct classes. (B) The “uh” pattern influenced the “Hesitation” class reasoning. (C) The recommended principles to guide “Hesitation” class reasoning. (D) The test results of added out-of-distribution instances.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">E6</span> further explored the performance specifics of the latest prompt version (enhanced with k-shot examples) in the <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.2">Reasoning Panel</span> (<span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.3">R1</span>). <span class="ltx_ERROR undefined" id="S5.SS2.p4.1.4">\jianben</span>He identified a cluster of errors <span class="ltx_ERROR undefined" id="S5.SS2.p4.1.5">\jianben</span>in the first layer of the Sankey diagram associated with the predicted “Hesitation” class.
The consistent yellow color of the language modality and the overall prediction suggested that language modality predominated the reasoning process, and all these instances were misclassified as the “Hesitation” class.
In the pattern table on the right, he identified a frequent language pattern, “uh”, associated with a high error rate, with its bar chart fully colored in yellow (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F5" title="In 5.2 Case Two: Enhancing Multimodal User Intention Understanding with WTaG dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>B).
<span class="ltx_ERROR undefined" id="S5.SS2.p4.1.6">\jianben</span>Upon expanding the row, he found it contained evidence like “uh” and “oh” that indicated “Hesitation”.
Therefore, <span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.7">E6</span> clicked the row to examine the specific instances it included. He found that whenever the spoken content contained modal words like “uh” and “oh”, the model interpreted these as indicators of unwillingness to continue, thereby predicting the user’s intention as “Hesitation” without considering any other factors.
Consequently, <span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.8">E6</span> selected these instances for the auxiliary LLM to summarize principles for avoiding such error (<span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.9">R3</span>).
He then refined and incorporated these principles (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F5" title="In 5.2 Case Two: Enhancing Multimodal User Intention Understanding with WTaG dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>C) into the prompt and saved these instances in the <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.10">Instance Test</span> view.
Additionally, he added multiple instances from his project into the <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.11">Instance Test</span> view to evaluate the prompt robustness (<span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.12">R2</span>).
The test results showed that the accuracy reached 77%, with the added test instances correctly predicted (<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#S5.F5" title="In 5.2 Case Two: Enhancing Multimodal User Intention Understanding with WTaG dataset ‣ 5 Evaluation ‣ POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>D).
<span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.13">E6</span> was satisfied with this result and planned to use the prompt for his project.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Expert Interviews</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We further conducted semi-structured interviews with two academic researchers and one industry research scientist (<span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">P1-P3</span>) to verify the effectiveness and usability of <span class="ltx_ERROR undefined" id="S5.SS3.p1.1.2">\name</span>.
All participants had experience in prompt engineering and the training or adaption of multimodal LLMs for downstream tasks, while none had previously tried the <span class="ltx_ERROR undefined" id="S5.SS3.p1.1.3">\name</span>before the interviews.
Each interview began with the research background introduction, followed by the system workflow and function demonstration with examples.
Experts were then invited to freely explore the system using real datasets, voicing their thoughts in a think-aloud manner. <span class="ltx_ERROR undefined" id="S5.SS3.p1.1.4">\jianben</span>We also collected feedback from <span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.5">E5</span> and <span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.6">E6</span> during case studies.
The gathered feedback is summarized below:</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">System workflow</span>
All experts concurred that the workflow of <span class="ltx_ERROR undefined" id="S5.SS3.p2.1.2">\name</span>is thoughtfully designed, significantly improving the efficiency of prompt iteration compared to their current practices, <span class="ltx_ERROR undefined" id="S5.SS3.p2.1.3">\jianben</span>which rely solely on performance statistics for evaluating prompt effects and laborious manual experiments to search for better-performing prompts.
As <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.4">P2</span> noted, “I think <span class="ltx_ERROR undefined" id="S5.SS3.p2.1.5">\name</span>offers a more systematic and comprehensive way to analyze the model’s complex reasoning behaviors.”
<span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.6">P1</span> highlighted that the varied strategies and streamlined process provided by <span class="ltx_ERROR undefined" id="S5.SS3.p2.1.7">\name</span>notably “reduce the pain for prompt writing and testing ” which are challenging tasks for them.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.8">E5</span> commented that the recommended principles and K-shot examples “serve as good starting points to bring new perspectives and inspire thoughts”.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">System designs and interactions</span>
All experts remarked that the visual and interaction design of <span class="ltx_ERROR undefined" id="S5.SS3.p3.1.2">\name</span>is intuitive and easy to learn and use.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.3">P3</span> expressed particular favor for the <span class="ltx_text ltx_font_italic" id="S5.SS3.p3.1.4">Prompt History</span> design, which makes it effortless to track every detail of changes, “as I usually get lost after several rounds of prompt iteration. Now I can start with any version at ease.”
<span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.5">P1</span> valued the convenient <span class="ltx_ERROR undefined" id="S5.SS3.p3.1.6">\jianben</span>one-click generate and import function, which saves tons of time in manually editing and formatting the prompts.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.7">E5</span> appreciated the ability to examine and evaluate at the instance level with reference to raw data, stating, “Since hallucinations can happen inevitably, having access to instance-specific details for validation significantly increased my trust for the system and confidence in the prompts I developed”.
Meanwhile, experts <span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.8">E6</span> and <span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.9">P2</span> mentioned that it took some time to understand and proficiently use the Sankey diagram, yet they acknowledged that the complexity of multimodal reasoning performance necessitates such a design.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Suggestions for improvement</span>
<span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.2">P1</span> proposed that the generated instance-specific principles can be visually linked to their originating instances to offer a more intuitive and comprehensible reference.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.3">P2</span> expressed a desire for a feature that allows the system to recommend instances based on users’ high-level input criteria for further evaluation or demonstration.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.4">E6</span> also thought it would be beneficial if the system could help summarize users’ annotated rationales to identify potential ambiguities and conflicts.
<span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.5">P3</span> thought it would be interesting and useful to enable comparisons across multiple LLMs.
Besides, step-by-step guides are wanted during real-time exploration to reduce learning curve.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we discuss the <span class="ltx_ERROR undefined" id="S6.p1.1.1">\name</span>regarding knowledge alignment with principle, system generalizability, and scalability. We also pointed out current limitations and potential directions for future work.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Human-AI knowledge alignment through principle</span>
Given the emerging prompting paradigm that allows users to interact with LLMs through natural language, there is a growing interest in harnessing explicitly stated principles for evaluating and guiding model performance in downstream applications.
While previous studies have explored the assessment of models using human-input criteria <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib22" title="">22</a>]</cite> and the alignment of chatbot behaviors with user preferences through converting feedback into principles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib44" title="">44</a>]</cite>, our research pioneers the use of data-derived principles to direct and <span class="ltx_ERROR undefined" id="S6.p2.1.2">\jianben</span>improve model multimodal reasoning performance.
Drawing on the innate human capacity for both inductive and deductive reasoning, <span class="ltx_ERROR undefined" id="S6.p2.1.3">\name</span>proposed an LLM-assisted module condensing both instance-specific and agnostic principles to encourage users to efficiently express and externalize their domain-specific knowledge and expertise for model steering.
Despite the exhibited great potential for eliciting desired knowledge, exploring how to design, manage, and apply principles more effectively across varied tasks and contexts remains a fertile area for research.
As pointed out by prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib71" title="">71</a>]</cite>, there is no one-size-fits-all principle granularity, as the effectiveness varies with task complexity, dataset diversity, and principle quality. In our work, we provide both specific and universal principles for balancing both uniqueness and generability. Identifying and crafting an effective set of principles with suitable granularity for different tasks remains an open question. Moreover, current users can only articulate principles in natural language where more diverse interactions (e.g., clicking in SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib23" title="">23</a>]</cite>) can be integrated to enable users to provide more nuanced and precise feedback.
Meanwhile, managing the accumulated principles is non-trivial due to conflict and forgetting issues. Users may also struggle to grasp the influence of varying principles on model performance. Utilizing LLMs to condense and differentiate the patterns and impacts of principles could serve as a potential solution.
On the other side, while principles are most effective for large models possessing robust instruction-following capacities, they can also benefit smaller models by guiding dataset retrieval and generation for model fine-tuning.
Furthermore, as demonstration examples and principles represent two distinct approaches of injecting and eliciting knowledge for reasoning in a bottom-up and top-down manner respectively, how to collocate k-shot examples with principles to maximize information gain in prompt engineering remains a compelling question.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">System generalizability and scalability</span>
In this paper, we mainly focus on the interaction between the two most-studied visual and language modalities. However, our system can be extended to investigate the interactions between multiple modalities by pair-wise comparison.
Besides the analysis tasks evaluated in this paper, the proposed framework is readily to be utilized for other multimodal content comprehension and reasoning tasks such as multimodal hate or sarcasm recognition, and multimodal context question answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.03843v3#bib.bib63" title="">63</a>]</cite>, where the modality interaction relationship persistently exist.
This prompting-based system can also serve as a testing tool to uncover weaknesses in model multimodal reasoning performance and identify example types and principles to inform larger-scale data collection for model fine-tuning.
Moreover, the design of the system can be extended for other applications. For example, the <span class="ltx_text ltx_font_italic" id="S6.p3.1.2">Reasoning Panel</span> design can be used for other tasks that necessitate summarizing relationships across various information channels at multiple levels.
The highlighted difference design in <span class="ltx_text ltx_font_italic" id="S6.p3.1.3">Prompt History</span> view can also help text summarization and comparison tasks in a structured and intuitive way.
The system scalability is rooted in the algorithm and visual design. The bottleneck of the algorithm part is the time cost of processing the video dataset and LLM’s generation speed. Currently, we have implemented batch processing to expedite the data processing and generation process for a smooth prompting experience. However, this approach may not suffice for handling the data scale of thousands of instances, necessitating the exploration of strategies like parallel computing and data sampling to ensure instant feedback.
For the visual design, The Sankey diagram design in <span class="ltx_text ltx_font_italic" id="S6.p3.1.4">Reasoning Panel</span> may become visually cluttered when dealing with a large number of prediction classes or complex modalities. For this situation, we can consider adopting a hierarchical visualization design coupled with interaction techniques to enhance visual scalability.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Limitations &amp; Future Work</span>
Current LLMs exhibit deficiencies in producing hallucinated and inconsistent responses. Our system has tried to mitigate this issue by fixing hyperparameters and providing a multi-level systematic analysis of outputs regarding different prompts, allowing users to easily examine and identify outlying responses. Future efforts can be directed towards developing techniques for reducing hallucination occurrence in model outputs.
Moreover, considering the potential information loss or inaccuracies introduced by expert models across different modalities, we plan to integrate more advanced expert models and visualize potential uncertainties to increase user trust.
In the future, we consider enabling comparison across multiple LLMs to further investigate effective prompt engineering strategies for different models and tasks. Additionally, we plan to extend our work to study interaction involving more modalities in increasingly complex scenarios and applications.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we introduce <span class="ltx_ERROR undefined" id="S7.p1.1.1">\name</span>, a novel visual analytics tool designed to facilitate prompt engineering for enhancing multimodal reasoning of LLMs with human insight and expertise. The system allows users to thoroughly assess prompt effectiveness through well-summarized multimodal reasoning patterns and offers varied strategies for prompt revision, enabling users to apply their knowledge for efficient prompt iteration. The system’s efficacy and efficiency are validated through two case studies and positive feedback from experts.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
R. Agrawal, R. Srikant, et al.

</span>
<span class="ltx_bibblock">Fast algorithms for mining association rules.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Proc. VLDB</span>, vol. 1215, pp. 487–499. Santiago, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Bagher Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency.

</span>
<span class="ltx_bibblock">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proc. ACL (Volume 1: Long Papers)</span>, pp. 2236–2246. ACL, 2018. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-1208" title="">doi: 10 . 18653/v1/P18-1208</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y. Bao, K. Yu, Y. Zhang, S. Storks, I. Bar-Yossef, A. de la Iglesia, M. Su, X. Zheng, and J. Chai.

</span>
<span class="ltx_bibblock">Can foundation models watch, talk and guide you step by step to make a cake?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Findings of EMNLP</span>, pp. 12325–12341. ACL, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.824" title="">doi: 10 . 18653/v1/2023 . findings-emnlp . 824</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. Bhattacharjya, J. Lee, D. J. Agravante, B. Ganesan, and R. Marinescu.

</span>
<span class="ltx_bibblock">Foundation model sherpas: Guiding foundation models through knowledge and reasoning, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2402.01602" title="">doi: 10 . 48550/arXiv . 2402 . 01602</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Bhattacharyya, Y. K. Singla, B. Krishnamurthy, R. R. Shah, and C. Chen.

</span>
<span class="ltx_bibblock">A video is worth 4096 tokens: Verbalize videos to understand them in zero shot.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proc. EMNLP</span>, pp. 9822–9839. ACL, Singapore, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.608" title="">doi: 10 . 18653/v1/2023 . emnlp-main . 608</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Boggust, B. Hoover, A. Satyanarayan, and H. Strobelt.

</span>
<span class="ltx_bibblock">Shared interest: Measuring human-ai alignment to identify recurring patterns in model behavior.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proc. CHI</span>, article no. 10, 17 pages. ACM, New York, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3491102.3501965" title="">doi: 10 . 1145/3491102 . 3501965</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Brade, B. Wang, M. Sousa, S. Oore, and T. Grossman.

</span>
<span class="ltx_bibblock">Promptify: Text-to-image generation through interactive prompt exploration with large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proc. UIST</span>, article no. 96, 14 pages. ACM, New York, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3586183.3606725" title="">doi: 10 . 1145/3586183 . 3606725</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, and et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proc. NeurIPS</span>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. A. Cabrera, E. Fu, D. Bertucci, K. Holstein, A. Talwalkar, J. I. Hong, and A. Perer.

</span>
<span class="ltx_bibblock">Zeno: An interactive framework for behavioral evaluation of machine learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proc. CHI</span>, article no. 419, 14 pages. ACM, New York, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3544548.3581268" title="">doi: 10 . 1145/3544548 . 3581268</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Coscia and A. Endert.

</span>
<span class="ltx_bibblock">Knowledgevis: Interpreting language models by comparing fill-in-the-blank prompts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, pp. 1–13, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2023.3346713" title="">doi: 10 . 1109/TVCG . 2023 . 3346713</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. F. DeRose, J. Wang, and M. Berger.

</span>
<span class="ltx_bibblock">Attention flows: Analyzing and comparing attention mechanisms in language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 27(2):1160–1170, 2021. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2020.3028976" title="">doi: 10 . 1109/TVCG . 2020 . 3028976</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui.

</span>
<span class="ltx_bibblock">A survey for in-context learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv</span>, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2301.00234" title="">doi: 10 . 48550/arXiv . 2301 . 00234</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Feng, X. Wang, K. K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang, and W. Chen.

</span>
<span class="ltx_bibblock">Promptmagician: Interactive prompt engineering for text-to-image creation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 30(1):295–305, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2023.3327168" title="">doi: 10 . 1109/TVCG . 2023 . 3327168</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, et al.

</span>
<span class="ltx_bibblock">Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2405.21075</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
L. Hanu, A. L. Verő, and J. Thewlis.

</span>
<span class="ltx_bibblock">Language as the medium: Multimodal video classification through text only.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2309.10783" title="">doi: 10 . 48550/arXiv . 2309 . 10783</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. He, X. Wang, K. K. Wong, X. Huang, C. Chen, Z. Chen, F. Wang, M. Zhu, and H. Qu.

</span>
<span class="ltx_bibblock">Videopro: A visual analytics approach for interactive video programming.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 30(1):87–97, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2023.3326586" title="">doi: 10 . 1109/TVCG . 2023 . 3326586</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
F. Hohman, H. Park, C. Robinson, and D. H. Polo Chau.

</span>
<span class="ltx_bibblock">Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 26(1):1096–1106, 2020. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2019.2934659" title="">doi: 10 . 1109/TVCG . 2019 . 2934659</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
B. Hoover, H. Strobelt, and S. Gehrmann.

</span>
<span class="ltx_bibblock">exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proc. ACL: System Demonstrations</span>, pp. 187–196. ACL, Online, 2020. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-demos.22" title="">doi: 10 . 18653/v1/2020 . acl-demos . 22</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. N. Hoque, W. He, A. K. Shekar, L. Gou, and L. Ren.

</span>
<span class="ltx_bibblock">Visual concept programming: A visual analytics approach to injecting human intelligence at scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 29(1):74–83, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2022.3209466" title="">doi: 10 . 1109/TVCG . 2022 . 3209466</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T. Jaunet, C. Kervadec, R. Vuillemot, G. Antipov, M. Baccouche, and C. Wolf.

</span>
<span class="ltx_bibblock">Visqa: X-raying vision and language reasoning in transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 28(1):976–986, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2021.3114683" title="">doi: 10 . 1109/TVCG . 2021 . 3114683</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
E. Jiang, K. Olson, E. Toh, A. Molina, A. Donsbach, M. Terry, and C. J. Cai.

</span>
<span class="ltx_bibblock">Promptmaker: Prompt-based prototyping with large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proc. CHI: Extended Abstracts</span>, article no. 35, 8 pages. ACM, New York, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3491101.3503564" title="">doi: 10 . 1145/3491101 . 3503564</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. S. Kim, Y. Lee, J. Shin, Y.-H. Kim, and J. Kim.

</span>
<span class="ltx_bibblock">Evallm: Interactive evaluation of large language model prompts on user-defined criteria.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proc. CHI</span>, article no. 306, 21 pages. ACM, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proc. ICCV</span>, pp. 4015–4026. IEEE Computer Society, Los Alamitos, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Li, D. Li, S. Savarese, and S. Hoi.

</span>
<span class="ltx_bibblock">BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Proc. ICML</span>, vol. 202, pp. 19730–19742. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
R. Li, W. Xiao, L. Wang, H. Jang, and G. Carenini.

</span>
<span class="ltx_bibblock">T3-vis: visual analytic for training and fine-tuning transformers in NLP.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proc. EMNLP: System Demonstrations</span>, pp. 220–230. ACL, Singapore, 2021. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-demo.26" title="">doi: 10 . 18653/v1/2021 . emnlp-demo . 26</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Li, X. Wang, W. Yang, J. Wu, Z. Zhang, Z. Liu, M. Sun, H. Zhang, and S. Liu.

</span>
<span class="ltx_bibblock">A unified understanding of deep nlp models for text classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 28(12):4980–4994, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2022.3184186" title="">doi: 10 . 1109/TVCG . 2022 . 3184186</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Z. Lian, L. Sun, M. Xu, H. Sun, K. Xu, Z. Wen, S. Chen, B. Liu, and J. Tao.

</span>
<span class="ltx_bibblock">Explainable multimodal emotion reasoning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">arXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2306.15401" title="">doi: 10 . 48550/arXiv . 2306 . 15401</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
P. P. Liang, Y. Cheng, X. Fan, C. K. Ling, S. Nie, R. Chen, Z. Deng, N. Allen, R. Auerbach, F. Mahmood, R. R. Salakhutdinov, and L.-P. Morency.

</span>
<span class="ltx_bibblock">Quantifying and modeling multimodal interactions: An information decomposition framework.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">NeurIPS</span>, vol. 36, pp. 27351–27393, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
P. P. Liang, Y. Cheng, R. Salakhutdinov, and L.-P. Morency.

</span>
<span class="ltx_bibblock">Multimodal fusion interactions: A study of human and automatic quantification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proc. ICMI</span>, 11 pages, pp. 425––435. ACM, New York, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3577190.3614151" title="">doi: 10 . 1145/3577190 . 3614151</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
P. P. Liang, C. K. Ling, Y. Cheng, A. Obolenskiy, Y. Liu, R. Pandey, A. Wilf, L.-P. Morency, and R. Salakhutdinov.

</span>
<span class="ltx_bibblock">Quantifying interactions in semi-supervised multimodal learning: Guarantees and applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">ICLR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
P. P. Liang, Y. Lyu, G. Chhablani, N. Jain, Z. Deng, X. Wang, L.-P. Morency, and R. Salakhutdinov.

</span>
<span class="ltx_bibblock">Multiviz: Towards visualizing and understanding multimodal models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">ICLR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
H. Liu, C. Li, Q. Wu, and Y. J. Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proc. NeurIPS</span>, vol. 36, pp. 34892–34916, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">ACM Computing Surveys</span>, 55(9), article no. 195, 35 pages, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3560815" title="">doi: 10 . 1145/3560815</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Liu, Z. Li, T. Li, V. Srikumar, V. Pascucci, and P.-T. Bremer.

</span>
<span class="ltx_bibblock">Nlize: A perturbation-driven visual interrogation tool for analyzing and interpreting natural language inference models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 25(1):651–660, 2019. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2018.2865230" title="">doi: 10 . 1109/TVCG . 2018 . 2865230</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
V. Liu, H. Qiao, and L. Chilton.

</span>
<span class="ltx_bibblock">Opal: Multimodal image generation for news illustration.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Proc. UIST</span>, article no. 73, 17 pages. ACM, New York, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3526113.3545621" title="">doi: 10 . 1145/3526113 . 3545621</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan.

</span>
<span class="ltx_bibblock">Learn to explain: Multimodal reasoning via thought chains for science question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proc. NeurIPS</span>, vol. 35, pp. 2507–2521, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu, and J. Gao.

</span>
<span class="ltx_bibblock">Chameleon: Plug-and-play compositional reasoning with large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proc. NeurIPS</span>, vol. 36, pp. 43447–43478, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S. M. Lundberg and S.-I. Lee.

</span>
<span class="ltx_bibblock">A unified approach to interpreting model predictions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Proc. NeurIPS</span>, 10 pages, p. 4768–4777, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A. Madsen, S. Reddy, and S. Chandar.

</span>
<span class="ltx_bibblock">Post-hoc interpretability for neural nlp: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">ACM Computing Surveys</span>, 55(8), article no. 155, 42 pages, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3546577" title="">doi: 10 . 1145/3546577</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
L. McInnes, J. Healy, S. Astels, et al.

</span>
<span class="ltx_bibblock">hdbscan: Hierarchical density based clustering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">J. Open Source Softw.</span>, 2(11):205, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Y. Ming, S. Cao, R. Zhang, Z. Li, Y. Chen, Y. Song, and H. Qu.

</span>
<span class="ltx_bibblock">Understanding hidden memories of recurrent neural networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">IEEE Conference on Visual Analytics Science and Technology (VAST)</span>, pp. 13–24, 2017. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/VAST.2017.8585721" title="">doi: 10 . 1109/VAST . 2017 . 8585721</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. Mishra, S. Rahman, H. Kim, K. Mitra, and E. Hruschka.

</span>
<span class="ltx_bibblock">Characterizing large language models as rationalizers of knowledge-intensive tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">arXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2311.05085" title="">doi: 10 . 48550/arXiv . 2311 . 05085</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A. Mishra, U. Soni, A. Arunkumar, J. Huang, B. C. Kwon, and C. Bryan.

</span>
<span class="ltx_bibblock">Promptaid: Prompt exploration, perturbation, testing and iteration using visual analytics for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">arXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.01964" title="">doi: 10 . 48550/arXiv . 2304 . 01964</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
S. Petridis, B. D. Wedin, J. Wexler, M. Pushkarna, A. Donsbach, N. Goyal, C. J. Cai, and M. Terry.

</span>
<span class="ltx_bibblock">Constitutionmaker: Interactively critiquing large language models by converting feedback into principles.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Proc. IUI</span>, 16 pages, p. 853–868. ACM, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3640543.3645144" title="">doi: 10 . 1145/3640543 . 3645144</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
M. Ribeiro, S. Singh, and C. Guestrin.

</span>
<span class="ltx_bibblock">“why should I trust you?”: Explaining the predictions of any classifier.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proc. ACM SIGKDD</span>, p. 1135–1144. ACM, New York, 2016. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/2939672.2939778" title="">doi: 10 . 1145/2939672 . 2939778</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Z. Shao, Z. Yu, M. Wang, and J. Yu.

</span>
<span class="ltx_bibblock">Prompting large language models with answer heuristics for knowledge-based visual question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Proc. CVPR</span>, pp. 14974–14983, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush.

</span>
<span class="ltx_bibblock">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 24(1):667–676, 2018. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2017.2744158" title="">doi: 10 . 1109/TVCG . 2017 . 2744158</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
H. Strobelt, A. Webson, V. Sanh, B. Hoover, J. Beyer, H. Pfister, and A. M. Rush.

</span>
<span class="ltx_bibblock">Interactive and visual prompt engineering for ad-hoc task adaptation with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 29(1):1146–1156, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2022.3209479" title="">doi: 10 . 1109/TVCG . 2022 . 3209479</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Sun, C. Zheng, E. Xie, Z. Liu, R. Chu, J. Qiu, J. Xu, M. Ding, H. Li, M. Geng, et al.

</span>
<span class="ltx_bibblock">A survey of reasoning with foundation models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">arXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2312.11562" title="">doi: 10 . 48550/arXiv . 2312 . 11562</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
I. Tenney, J. Wexler, J. Bastings, T. Bolukbasi, A. Coenen, S. Gehrmann, E. Jiang, M. Pushkarna, C. Radebaugh, E. Reif, and A. Yuan.

</span>
<span class="ltx_bibblock">The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Proc. EMNLP: System Demonstrations</span>, pp. 107–118. ACL, Online, 2020. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-demos.15" title="">doi: 10 . 18653/v1/2020 . emnlp-demos . 15</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">arXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.13971" title="">doi: 10 . 48550/arXiv . 2302 . 13971</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
X. Wang, J. He, Z. Jin, M. Yang, Y. Wang, and H. Qu.

</span>
<span class="ltx_bibblock">M2lens: Visualizing and explaining multimodal models for sentiment analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 28(1):802–812, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2021.3114794" title="">doi: 10 . 1109/TVCG . 2021 . 3114794</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
X. Wang, R. Huang, Z. Jin, T. Fang, and H. Qu.

</span>
<span class="ltx_bibblock">Commonsensevis: Visualizing and understanding commonsense reasoning capabilities of natural language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 30(01):273–283, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2023.3327153" title="">doi: 10 . 1109/TVCG . 2023 . 3327153</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Y. Wang, S. Shen, and B. Y. Lim.

</span>
<span class="ltx_bibblock">Reprompt: Automatic prompt editing to refine ai-generative art towards precise expressions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Proc. CHI</span>, article no. 22, p. 29. ACM, New York, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3544548.3581402" title="">doi: 10 . 1145/3544548 . 3581402</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Z. Wang, M. Li, R. Xu, L. Zhou, J. Lei, X. Lin, S. Wang, Z. Yang, C. Zhu, D. Hoiem, S.-F. Chang, M. Bansal, and H. Ji.

</span>
<span class="ltx_bibblock">Language models with image descriptors are strong few-shot video-language learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proc. NeurIPS</span>, vol. 35, pp. 8483–8497, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Z. J. Wang, R. Turko, and D. H. Chau.

</span>
<span class="ltx_bibblock">Dodrio: Exploring transformer models with interactive visualization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Proc.ACL: System Demonstrations</span>, pp. 132–141. ACL, Online, 2021. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-demo.16" title="">doi: 10 . 18653/v1/2021 . acl-demo . 16</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">Proc. NeurIPS</span>, vol. 35, pp. 24824–24837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
J. Wexler, M. Pushkarna, T. Bolukbasi, M. Wattenberg, F. Viégas, and J. Wilson.

</span>
<span class="ltx_bibblock">The what-if tool: Interactive probing of machine learning models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 26(1):56–65, 2020. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2019.2934619" title="">doi: 10 . 1109/TVCG . 2019 . 2934619</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
S. Wu, H. Shen, D. S. Weld, J. Heer, and M. T. Ribeiro.

</span>
<span class="ltx_bibblock">Scattershot: Interactive in-context example curation for text transformation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Proc. UIST</span>, p. 353–367. ACM, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3581641.3584059" title="">doi: 10 . 1145/3581641 . 3584059</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J. Cai.

</span>
<span class="ltx_bibblock">Promptchainer: Chaining large language model prompts through visual programming.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">Proc. CHI: Extended Abstracts</span>, article no. 359, 10 pages. ACM, New York, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
T. Wu, M. Terry, and C. J. Cai.

</span>
<span class="ltx_bibblock">Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Proc. CHI</span>, p. 385. ACM, New York, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3491102.3517582" title="">doi: 10 . 1145/3491102 . 3517582</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
W. Yang, M. Liu, Z. Wang, and S. Liu.

</span>
<span class="ltx_bibblock">Foundation models meet visualizations: Challenges and opportunities.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">Comp.Visual Media</span>, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s41095-023-0393-x" title="">doi: 10 . 1007/s41095-023-0393-x</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
X. Yang, W. Wu, S. Feng, M. Wang, D. Wang, Y. Li, Q. Sun, Y. Zhang, X. Fu, and S. Poria.

</span>
<span class="ltx_bibblock">Mm-bigbench: Evaluating multimodal models on multimodal content comprehension tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">arXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2310.09036" title="">doi: 10 . 48550/arXiv . 2310 . 09036</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
C. Yeh, Y. Chen, A. Wu, C. Chen, F. Viegas, and M. Wattenberg.

</span>
<span class="ltx_bibblock">Attentionviz: A global view of transformer attention.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 30(01):262–272, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2023.3327163" title="">doi: 10 . 1109/TVCG . 2023 . 3327163</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen.

</span>
<span class="ltx_bibblock">A survey on multimodal large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">arXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2306.13549" title="">doi: 10 . 48550/arXiv . 2306 . 13549</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
H. Yu, P. P. Liang, R. Salakhutdinov, and L.-P. Morency.

</span>
<span class="ltx_bibblock">Mixture of multimodal interaction experts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">UniReps: the First Workshop on Unifying Representations in Neural Models</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J. Yuan, C. Chen, W. Yang, M. Liu, J. Xia, and S. Liu.

</span>
<span class="ltx_bibblock">A survey of visual analytics techniques for machine learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">Computational Visual Media</span>, 7(1):2, 2021. <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s41095-020-0191-7" title="">doi: 10 . 1007/s41095-020-0191-7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al.

</span>
<span class="ltx_bibblock">Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">Proc. CVPR</span>, pp. 9556–9567, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
J. Zamfirescu-Pereira, R. Y. Wong, B. Hartmann, and Q. Yang.

</span>
<span class="ltx_bibblock">Why johnny can’t prompt: How non-ai experts try (and fail) to design llm prompts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">Proc. CHI</span>, article no. 437. ACM, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3544548.3581388" title="">doi: 10 . 1145/3544548 . 3581388</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
A. Zeng, M. Attarian, brian ichter, K. M. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, M. S. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, and P. Florence.

</span>
<span class="ltx_bibblock">Socratic models: Composing zero-shot multimodal reasoning with language.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">ICLR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
T. Zhang, A. Madaan, L. Gao, S. Zhang, S. Mishra, Y. Yang, N. Tandon, and U. Alon.

</span>
<span class="ltx_bibblock">In-context principle learning from mistakes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">ICML 2024 Workshop on In-Context Learning</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
X. Zhang, J. P. Ono, H. Song, L. Gou, K.-L. Ma, and L. Ren.

</span>
<span class="ltx_bibblock">Sliceteller: A data slice-driven approach for machine learning model validation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 29(1):842–852, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2022.3209465" title="">doi: 10 . 1109/TVCG . 2022 . 3209465</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin, and M. Du.

</span>
<span class="ltx_bibblock">Explainability for large language models: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">ACM Transactions on Intelligent Systems and Technology</span>, 15(2):1–38, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3639372" title="">doi: 10 . 1145/3639372</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
G. Zheng, B. Yang, J. Tang, H.-Y. Zhou, and S. Yang.

</span>
<span class="ltx_bibblock">Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">Proc. NeurIPS</span>, vol. 36, pp. 5168–5191, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 16:14:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
