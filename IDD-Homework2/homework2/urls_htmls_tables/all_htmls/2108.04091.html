<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2108.04091] ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA</title><meta property="og:description" content="We present a network architecture which compares RGB images and untextured 3D models by the similarity of the represented shape. Our system is optimised for zero-shot retrieval, meaning it can recognise shapes never sh…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2108.04091">

<!--Generated on Sat Mar  2 04:10:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\TPMargin</span>
<p id="p1.2" class="ltx_p">5pt</p>
</div>
<h1 class="ltx_title ltx_title_document">ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">We present a network architecture which compares RGB images and untextured 3D models by the similarity of the represented shape. Our system is optimised for zero-shot retrieval, meaning it can recognise shapes never shown in training. We use a view-based shape descriptor and a siamese network to learn object geometry from pairs of 3D models and 2D images. Due to scarcity of datasets with exact photograph-mesh correspondences, we train our network with only synthetic data. Our experiments investigate the effect of different qualities and quantities of training data on retrieval accuracy and present insights from bridging the domain gap. We show that increasing the variety of synthetic data improves retrieval accuracy and that our system’s performance in zero-shot mode can match that of the instance-aware mode, as far as narrowing down the search to the top 10% of objects.</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p2.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
3D model retrieval, zero-shot, siamese networks, synthetic data, domain gap</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">{textblock}</span>
<p id="p3.2" class="ltx_p">0.83(0.09,0.92) <span id="p3.2.1" class="ltx_ERROR undefined">\tekstblokkulur</span>white<span id="p3.2.2" class="ltx_text" style="font-size:70%;">©  2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Reasoning about real objects in the context of computer vision is easier if we are in possession of their 3D models. Unlike 2D images, 3D models are not spoiled by factors such as viewpoint, occlusion, or lighting, and therefore provide a pure and accurate reference to the real object. In particular, we are interested in rigid objects which can be reliably recognised based solely on their geometry.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we learn a metric (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) which measures shape similarity of objects in RGB images and as 3D models. The metric is implemented as a Siamese CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which extracts shape features from images and models and, for identical shapes, maps them close together in the latent space.
We represent 3D models by a set of textureless, shaded greyscale views, rendered from viewing positions on the corners of an icosahedron enclosing the object.
Features computed from every view are merged into a compact representation by max-pooling, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Similarity was learned by exposing the CNN to same and different image-model pairs, computing cosine distance between their extracted features and applying the contrastive loss function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2108.04091/assets/images/metric.png" id="S1.F1.g1" class="ltx_graphics ltx_img_square" width="598" height="513" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Our network retrieves an unknown 3D model from an RGB image on the sole basis of object geometry.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In our work, we focus on zero-shot retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, that is, on optimizing the network to recognise shapes different from those shown in training. This ability to work with arbitrary shapes makes our network very practical, since making changes (such as adding new objects) to the retrieval set does not require any retraining of the model.
To this end, we expose the CNN to a sufficient number of versatile image-to-model correspondences such that it learns a generic 2D-to-3D shape similarity function.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Due to scarcity of existing datasets featuring real images and 3D models of the exact same objects, we train our network using only synthetic data. We generate the data using Domain Randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and leverage the siamese architecture of the CNN to reduce the domain gap between the synthetic training data and the real test data.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In summary, our contributions are three-fold: 1) we design a network which finds a common embedding of two heterogeneous object representations 2) we demonstrate the network’s potential for recognising unknown objects (zero-shot) 3) we present insights from training the network with only synthetic data and from bridging the domain gap.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Image-Based 3D Model Retrieval</h3>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">In order to match images with 3D models we must first design suitable embeddings of these media. Compared to image descriptors, shape descriptors are still relatively unexplored. While shape descriptors can be computed from native model formats, such as a polygon mesh or a point cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, most of the existing approaches instead employ intermediate representations of models, such as grayscale renderings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, depth maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, or location fields <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Such descriptors were shown to perform better in both model-to-model and model-to-image comparison tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Besides, operating on 2D projections, rather than on native 3D formats, is computationally more efficient and allows leveraging existing image descriptors pretrained on large datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">Another question is how to jointly embed shapes and images in a meaningful way.
Our work is most similar to the one of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> who also aggregate greyscale views of each model into a single vector, and map them together with RGB images with a siamese network. Different from us, they train their network using real images and corresponding shape instances from 40 different classes. As their training pairs use the closest available shape, rather than the exact same shape, their system effectively works on a class-level, rather than on the instance-level.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p">Grabner et al. propose to match images and shapes by computing location field descriptors from both media <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The authors train their network for instance-level recognition and achieve state of the art results on the Pix3D dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. However, the small size of the dataset restricts them to learning only a small number of shape classes. In contrast, in our work we extend the recognition to unknown classes. For this we employ (unlimited in samples) synthetic data, which shifts a portion of our work towards bridging the synthetic-to-real domain gap.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Synthetic Data</h3>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">In recent years, visual deep learning saw a growing trend for using synthetic images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, driven by the scarcity of available real data.
Image synthesis allows to quickly generate fully annotated images in potentially unlimited quantities. Recently emerged tools such as BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and NDDS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> provide ready-made dataset generation pipelines and example maps for rendering custom objects. On the downside, synthesized images have different characteristics from the real ones. This results in a ’domain gap’ – systems trained with synthetic data to show worse results on real data than if they had been trained with real data.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Two of the common approaches to mitigating this problem are Domain Randomization (DR) and Domain Adaptation (DA). DR, introduced by Tobin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for training an object detector, proposes to augment the simulated scene in a large number of ways, such as by randomising object poses, textures, camera poses, and lighting, intending that the real scene would appear to the network just as one of the many synthetic variations.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p3.1" class="ltx_p">Although most works use synthetic data only for pretraining or complementing real data, Tremblay et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> attempted to bridge the domain gap without using any real data. Their training set featured 50% non-photorealistic DR renderings and 50% photorealistic renderings. The authors showed that mixing these two types of data results in better performance then using either of the data types alone. Their all-synthetic dataset was used for 6D pose estimation of 21 textured objects, and it has shown competitive results against detectors trained with both real and synthetic data.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p4" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p4.1" class="ltx_p">While DR is applied to the generation of data, DA aims to make up for different distributions of computed image features, usually in the direction from real to synthetic. For example, Massa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> add to their system an adaptation layer pretrained to map synthetic images of certain objects to their real counterparts. Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> adopt a similar approach, but their adaptation function is learned together with the rest of the CNN in an end-to-end manner.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p5" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p5.1" class="ltx_p">Our approach follows the DR principle, such that it varies all parameters of the simulated scene except for the 3D shape of interest rendered in the foreground. This prompts the CNN to extract the pure shape information from complex real images. We follow Tremblay et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> in using only synthetic training data, as well as making our training set half-photorealistic and half-randomised. Unlike them, we do not restrict ourselves to classifying a small set of textured shapes, but aim for learning a general idea of shape. On top of that we introduce a novel adaptation strategy of splitting the siamese networks in the few beginning layers allowing the CNN to extract the shape information differently from different types of data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Network Architecture</h3>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ Network Architecture ‣ 3 Method ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents our network architecture. In each training or evaluation step the network compares one untextured 3D model with one colour image. We use ResNet-34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> to compute the image descriptors, and reduce the number output features from 1000 to 128. Shape features are computed from twelve all-around views. As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ Network Architecture ‣ 3 Method ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the same <span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">CNN-view</span> computes features for all views, which are then merged into a single shape representation via max-pooling. In parallel, the <span id="S3.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">CNN-img</span> computes features of the colour image (either synthetic or real). <span id="S3.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">CNN-view</span> and <span id="S3.SS0.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_italic">CNN-img</span> share the parameters from the ResNet’s 7th layer on; this allows to better adjust to pixel-level differences, while still jointly extracting the relevant shape information in the later layers. Finally the shape vector and the image vector are L2-normalized and the cosine distance computes their similarity score. Based on the pair label and the similarity score, we compute contrastive loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, whose margin we set to 1.0.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2108.04091/assets/images/net_arch.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="303" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>Network Architecture</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2108.04091/assets/images/view_pool.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="282" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>Shape Descriptor</figcaption>
</figure>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p2.1" class="ltx_p">The training sets were designed such that each object was represented by a roughly equal number of colour images. While the <span id="S3.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">CNN-view</span> processed 12 views of an object, the <span id="S3.SS0.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_italic">CNN-img</span> processed 6 positive and 6 negative images. On average, in one epoch every colour image was fed to the network twice, once as a positive and once as a negative example. In addition, the colour images were augmented with vertical and horizontal flip, random translation, rotation, and scale, as well as with varying contrast and brightness.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Data Generation</h3>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">Experimental data was made up of 3 types of images, all depicting a single object: 1) plain, greyscale renderings, 2) colour renderings, and 3) real photographs. Greyscale renderings, or views, (part of Figure <a href="#S3.F3" title="Figure 3 ‣ Network Architecture ‣ 3 Method ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) were generated using Pyrender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Each mesh was given the default metallic material parameters and was placed at the centre of an icosahedron. The camera was placed at each of the 12 vertices and oriented at the object centre. Each captured view was cropped such that the object’s longer dimension occupied 70% of the image. In addition, we tried representing 3D objects with sets of 20 and 42 views, but noted no advantage compared to using just 12.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p2.1" class="ltx_p">Colour renderings served to imitate real images and were generated using BlenderProc and NDDS. One example of each is shown in Figure <a href="#S3.F4" title="Figure 4 ‣ Data Generation ‣ 3 Method ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In BlenderProc, we focused on photorealism of the images. The objects were rendered inside a cube, whose inner walls were assigned varying textures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and where the illumination was frequently randomised. Objects were also assigned varying textures and placed on the floor in randomly sampled poses. Renderings were captured from an upper half sphere centred on the object, with a varying radius.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p3.1" class="ltx_p">In NDDS, the objects were captured in a fully-randomised scene, by rendering objects in varying poses on many different backgrounds. In this case, the realistic look was traded for more variation in object textures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, backgrounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and lighting, the simplicity of the scene and the speed of rendering.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p4" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p4.1" class="ltx_p">The last type of images are photographs which serve as test data only. The test set features 325 images picked from two datasets: Pix3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and Toyota-Light (a subset of BOP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>), and in small part found on web search engines. Each image is annotated with the name of the object it depicts. In order to assure that the objects are well visible, the images were cropped accordingly. We retrieve from a set of 50 models of distinct shapes representing furniture and other household objects. A link to the test set is given in the supplementary material.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2108.04091/assets/images/renderings.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text ltx_font_bold">Fig. 4</span>: </span>Photorealistic renderings (BlenderProc, left) draw a direct link with the real world, while non-realistic ones (NDDS, right) provide more background and texture variation.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The model was trained in two different modes, which will be referred to as instance-aware and zero-shot. In the first mode the same 50 object instances were used in training and in evaluation. Effectively, the network’s task was to transfer knowledge about a set of shapes from synthetic to real data in the most efficient manner.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In zero-shot mode, none of the test objects were shown to the network in training. Instead, the model learned from data rendered from several hundreds of different 3D models, in pursuit of generalising to recognition of any shape.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The network was implemented in Pytorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. After experimenting with AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, VGG-16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and different depth-versions of ResNet, we chose ResNet-34 as the best backbone for our task. We used Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, set the learning rate to 5e-5, weight decay to 1e-5 and the batch size to 12. After every epoch we evaluated the model on the real test set and reported results from the checkpoint with the highest retrieval rate of the single correct object (Top-1 accuracy). Depending on the dataset used this proceeding required training for 10 to 25 epochs.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The initial experiments were conducted in instance-aware mode. Firstly, we investigated how well different types of synthetic data serve learning our function. For this, we rendered two training sets of 6k images, featuring 50 object instances: one using BlenderProc, with the focus on photorealism and one using NDDS, with the focus on more randomisation. Examples of these images are shown in Figure <a href="#S3.F4" title="Figure 4 ‣ Data Generation ‣ 3 Method ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The network was trained thrice: first with only realistic data, then with only randomised data, finally with the union of the datasets. The top accuracy for all trainings are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We see that the photo-realistic data alone yielded better results than the randomised data alone. Mixing the datasets resulted in the best accuracies of all, which speaks for increasing the variety of synthetic data to train with.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:133.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(97.9pt,-30.1pt) scale(1.82303609516165,1.82303609516165) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><svg version="1.1" height="19.07" width="67.64" overflow="visible"><g transform="translate(0,19.07) scale(1,-1)"><path d="M 0,19.07 67.64,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="26.52" height="9.46" overflow="visible">
<span id="S4.T1.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.T1.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.T1.1.1.1.1.pic1.1.1.1.1" class="ltx_p">Acc.</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(33.82,9.46)"><g transform="translate(0,9.61) scale(1, -1)"><foreignObject width="33.82" height="9.61" overflow="visible">
<span id="S4.T1.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.T1.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.T1.1.1.1.1.pic1.2.1.1.1" class="ltx_p">Mode</span>
</span>
</span></foreignObject></g></g></g></svg></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Photorealistic</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Randomised</th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Mixed</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Top-1</td>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.50</td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.46</td>
<td id="S4.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.59</td>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Top-2</td>
<td id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.65</td>
<td id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.61</td>
<td id="S4.T1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.73</td>
</tr>
<tr id="S4.T1.1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Top-5</td>
<td id="S4.T1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.86</td>
<td id="S4.T1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.80</td>
<td id="S4.T1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.87</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Comparison of different synthetic training data types.</figcaption>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Secondly, we looked into the effect of siamese parameter sharing between the network’s arms. The model was trained once more in instance-aware mode, this time using more training samples (20k) including renderings from two extra UE4 maps, in order to further increase the retrieval rates through more varied data. Following that, the model was retrained with the same data, this time with <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">CNN-view</span> and <span id="S4.p5.1.2" class="ltx_text ltx_font_italic">CNN-img</span> learning separately. The retrieval rates shown in Table <a href="#S4.T2" title="Table 2 ‣ 4 Experiments ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrate a clear advantage from sharing weights.</p>
</div>
<div id="S4.p6" class="ltx_para">
<span id="S4.p6.1" class="ltx_ERROR undefined">\newcolumntype</span>
<p id="S4.p6.2" class="ltx_p">P[1]¿<span id="S4.p6.2.1" class="ltx_ERROR ltx_centering undefined">\arraybackslash</span>p#1</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:141.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(104.4pt,-34.0pt) scale(1.92867313948364,1.92867313948364) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><svg version="1.1" height="19.07" width="67.64" overflow="visible"><g transform="translate(0,19.07) scale(1,-1)"><path d="M 0,19.07 67.64,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="26.52" height="9.46" overflow="visible">
<span id="S4.T2.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.T2.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.T2.1.1.1.1.pic1.1.1.1.1" class="ltx_p">Acc.</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(33.82,9.46)"><g transform="translate(0,9.61) scale(1, -1)"><foreignObject width="33.82" height="9.61" overflow="visible">
<span id="S4.T2.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.T2.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.T2.1.1.1.1.pic1.2.1.1.1" class="ltx_p">Mode</span>
</span>
</span></foreignObject></g></g></g></svg></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Separate Params</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Shared Params</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<th id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Top-1</th>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.55</td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.62</td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<th id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Top-2</th>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.71</td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.75</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<th id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Top-5</th>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.87</td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.90</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>The effect of siamese weight sharing.</figcaption>
</figure>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">The final experiments were conducted in zero-shot mode, as we looked into how increasing the number of training objects affects the retrieval rates. For this the model was trained five times, using sets of 150, 300, 450, 600, and 1800 different meshes sampled from ShapeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The best accuracy is shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4 Experiments ‣ ZERO IN ON SHAPE: A GENERIC 2D-3D INSTANCE SIMILARITY METRIC LEARNED FROM SYNTHETIC DATA" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, together with the best accuracy from the instance-aware mode (all models were evaluated on the same test set).</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2108.04091/assets/images/zero_shot.png" id="S4.F5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="389" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text ltx_font_bold">Fig. 5</span>: </span>Deriving a sufficient number of objects to learn generic shape similarity. For comparison we also include the retrieval rates of the best instance-aware model.</figcaption>
</figure>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">The graph shows an increase in accuracy caused by raising the number of objects, until using 600 unique objects. Further increase in this number does not show a clear advantage.
If we couple these results with those of Experiment 1, we see that for zero-shot object recognition in the wild it is more vital to increase the versatility of synthetic training data than to increase the number of samples beyond a certain point.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">A comparison of the best instance-aware model and the zero-shot model shows that although the former’s Top-1 and Top-2 accuracy are clearly better, zero-shot matches the other’s performance with the Top-5 accuracy. This enables the use of our metric in applications that only require narrowing down the search to a few candidates.</p>
</div>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">In a qualitative analysis of retrieval cases we noted that the network tends to have trouble classifying images with strong patterns present in the background as well as those with non-uniform texturing of the object itself.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We presented a novel approach to matching RGB images and 3D models of arbitrary objects. We showed that our network, trained with only 600 unique objects, can extend its recognition ability from seen to unseen shapes as far as the top five retrieved results are concerned. This zero-shot functionality has practical use in situations where we need to recognise obscure shapes, for which little training data exists or wherever we frequently add new objects to the retrieval set.
Besides, we showed the advantage of siamese weight sharing for learning cross-dimensional similarity of shape, and of increasing the variety of synthetic training data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In the future, we plan to explore different shape descriptors, such as MeshNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, as well as to backtrack network activations in order to highlight image pixels most responsible for the extraction of similar features from the two inputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work is supported by the German Federal Ministry of Economic Affairs and Energy
(DigitalTWIN, grant no. 01MD18008B) and the German Federal Ministry of Education and Research (Single Sensor 3D++, grant no. 03ZZ0457).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore,
E. Säckinger, and R. Shah,

</span>
<span class="ltx_bibblock">“Signature verification using a ”siamese” time delay neural
network,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">International Journal of Pattern Recognition and Artificial
Intelligence (IJPRAI)</span>, vol. 7, no. 4, pp. 669–688, August 1993.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller,

</span>
<span class="ltx_bibblock">“Multi-view convolutional neural networks for 3d shape
recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proc. ICCV</span>, 2015, p. 945–953.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. Hadsell, S. Chopra, and Y. LeCun,

</span>
<span class="ltx_bibblock">“Dimensionality reduction by learning an invariant mapping,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2006, p. 1735–1742.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Larochelle, D. Erhan, and Y. Bengio,

</span>
<span class="ltx_bibblock">“Zero-data learning of new tasks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proc. AAAI</span>, 2008, p. 646–651.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Tobin, R. H. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,

</span>
<span class="ltx_bibblock">“Domain randomization for transferring deep neural networks from
simulation to the real world,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proc. IROS</span>, 2017, pp. 23–30.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Feng, Y. Feng, H. You, X. Zhao, and Y. Gao,

</span>
<span class="ltx_bibblock">“Meshnet: Mesh neural network for 3d shape representation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proc. ECAI</span>, 2019, pp. 8279–8286.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Q.-H. Pham, M. A. Uy, B.-S. Hua, D. T. Nguyen, G. Roig, and S.-K. Yeung,

</span>
<span class="ltx_bibblock">“Lcd: Learned cross-domain descriptors for 2d-3d matching,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proc. AAAI</span>, 2020, pp. 11856–11864.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Grabner, P. M. Roth, and V. Lepetit,

</span>
<span class="ltx_bibblock">“3d pose estimation and 3d model retrieval for objects in the
wild,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2018, pp. 3022–3031.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Grabner, P. M. Roth, and V. Lepetit,

</span>
<span class="ltx_bibblock">“Location field descriptors: Single image 3d model retrieval in the
wild,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. 3DV</span>, 2019, pp. 583–593.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,

</span>
<span class="ltx_bibblock">“Imagenet: A large-scale hierarchical image database,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. Lee, Y.-L. Lin, H. Chiang, M.-W. Chiu, W. Hsu, and P. Huang,

</span>
<span class="ltx_bibblock">“Cross-domain image-based 3d shape retrieval by view sequence
learning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. 3DV</span>, 2018, pp. 258–266.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Sun, J. Wu, X. Zhang, Z. Zhang, C. Zhang, T. Xue, J. B. Tenenbaum, and W. T.
Freeman,

</span>
<span class="ltx_bibblock">“Pix3d: Dataset and methods for single-image 3d shape modeling,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2018, pp. 2974–2983.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Tremblay, A. Prakash, D. Acuna, et al.,

</span>
<span class="ltx_bibblock">“Training deep networks with synthetic data: Bridging the reality
gap by domain randomization,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2018, pp. 1082–10828.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox, and S. Birchfield,

</span>
<span class="ltx_bibblock">“Deep object pose estimation for semantic robotic grasping of
household objects,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. CoRL</span>, 2018, pp. 306–316.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. Hinterstoißer, O. Pauly, H. Heibel, M. Martina, and M. Bokeloh,

</span>
<span class="ltx_bibblock">“An annotation saved is an annotation earned: Using fully synthetic
training for object detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proc. ICCV</span>, 2019, pp. 2787–2796.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Denninger, M. Sundermeyer, D. Winkelbauer, Y. Zidan, D. Olefir,
M. Elbadrawy, A. Lodhi, and H. Katam,

</span>
<span class="ltx_bibblock">“Blenderproc: Reducing the reality gap with photorealistic
rendering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proc. RSS</span>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T. To, J. Tremblay, D. McKay, Y. Yamaguchi, K. Leung, A. Balanon, J. Cheng,
W. Hodge, and S. Birchfield,

</span>
<span class="ltx_bibblock">“NDDS: NVIDIA deep learning dataset synthesizer,”
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/NVIDIA/Dataset_Synthesizer</span>, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
F. Massa, B. C. Russell, and M. Aubry,

</span>
<span class="ltx_bibblock">“Deep exemplar 2d-3d detection by adapting from real to rendered
views,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2016, pp. 6024–6033.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun,

</span>
<span class="ltx_bibblock">“Deep residual learning for image recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Matl,

</span>
<span class="ltx_bibblock">“Pyrender,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/mmatl/pyrender</span>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L. Demes,

</span>
<span class="ltx_bibblock">“Cc textures,” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://cc0textures.com</span>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi,

</span>
<span class="ltx_bibblock">“Describing textures in the wild,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2014, p. 3606–3613.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. Hodosh, P. Young, and J. Hockenmaier,

</span>
<span class="ltx_bibblock">“Framing image description as a ranking task: Data, models and
evaluation metrics,”

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Journal of Artificial Intelligence Research (JAIR)</span>, vol. 47,
no. 1, pp. 853–899, May 2013.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
T. Hodan, F. Michel, E. Brachmann, et al.,

</span>
<span class="ltx_bibblock">“Bop: Benchmark for 6d object pose estimation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proc. ECCV</span>, 2018, pp. 19–35.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. Paszke, S. Gross, F. Massa, et al.,

</span>
<span class="ltx_bibblock">“Pytorch: An imperative style, high-performance deep learning
library,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proc. NIPS</span>, 2019, pp. 8026–8037.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton,

</span>
<span class="ltx_bibblock">“Imagenet classification with deep convolutional neural networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proc. NIPS</span>, 2012, p. 1097–1105.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman,

</span>
<span class="ltx_bibblock">“Very deep convolutional networks for large-scale image
recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proc. CoRR</span>, 2014.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba,

</span>
<span class="ltx_bibblock">“Adam: A method for stochastic optimization,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proc. ICLR</span>, 2015.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Savva, A. X. Chang, and P. Hanrahan,

</span>
<span class="ltx_bibblock">“Semantically-enriched 3d models for common-sense knowledge,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proc. CVPR</span>, 2015, pp. 24–31.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
C. Seibold, A. Hilsmann, and P. Eisert,

</span>
<span class="ltx_bibblock">“Focused lrp: Explainable ai for face morphing attack detection,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">WACV Workshops</span>, 2021, pp. 88–96.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2108.04090" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2108.04091" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2108.04091">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2108.04091" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2108.04093" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 04:10:10 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
