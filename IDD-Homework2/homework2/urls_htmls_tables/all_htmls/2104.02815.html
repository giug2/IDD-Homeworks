<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.02815] On the Applicability of Synthetic Data for Face Recognition</title><meta property="og:description" content="Face verification has come into increasing focus in various applications including the European Entry/Exit System, which integrates face recognition mechanisms. At the same time, the rapid advancement of biometric auth‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On the Applicability of Synthetic Data for Face Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="On the Applicability of Synthetic Data for Face Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.02815">

<!--Generated on Sun Mar 17 01:45:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Synthetic Face Image Generation,  Face Image Quality Assessment,  Face Recognition
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">On the Applicability of Synthetic Data for Face Recognition 
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haoyu Zhang, Marcel Grimmer, Raghavendra Ramachandra, Kiran Raja, Christoph Busch 
<br class="ltx_break">Norwegian Biometrics Laboratory, Norwegian University of Science and Technology (NTNU), Norway
<br class="ltx_break">{<span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">haoyu.zhang; marceg; raghavendra.ramachandra;kiran.raja;christoph.busch} @ntnu.no
<br class="ltx_break"></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Face verification has come into increasing focus in various applications including the European Entry/Exit System, which integrates face recognition mechanisms. At the same time, the rapid advancement of biometric authentication requires extensive performance tests in order to inhibit the discriminatory treatment of travellers due to their demographic background. However, the use of face images collected as part of border controls is restricted by the European General Data Protection Law to be processed for no other reason than its original purpose. Therefore, this paper investigates the suitability of synthetic face images generated with StyleGAN and StyleGAN2 to compensate for the urgent lack of publicly available large-scale test data. Specifically, two deep learning-based (SER-FIQ, FaceQnet v1) and one standard-based (ISO/IEC TR 29794-5) face image quality assessment algorithm is utilized to compare the applicability of synthetic face images compared to real face images extracted from the FRGC dataset. Finally, based on the analysis of impostor score distributions and utility score distributions, our experiments reveal negligible differences between StyleGAN vs. StyleGAN2, and further also minor discrepancies compared to real face images.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Synthetic Face Image Generation, Face Image Quality Assessment, Face Recognition

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Biometric verification refers to the automated recognition of individuals based on their biological and behavioural characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Among multiple biometric characteristics, face has gained popularity in various application scenarios, such as border control systems like the European Entry-Exit System (EES) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, passport issuance and civilian ID management. Human face as a biometric modality has proven to be sufficiently unique to allow individual recognition with reasonable inter-class distance. Driven by the factors such as user convenience and good biometric performance (i.e., both identification and verification), the Smart Borders program under EES initiative has identified face as a mandatory biometric data. The EES system will be deployed as a central system for collecting and querying traveller data including face data to the Schengen area at all border crossing points<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The deployment of biometric recognition at the European borders requires the biometric performance to comply with high standards defined in the best practices for automated border control of the European Border and Coast Guard Agency (Frontex) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. While the value of real and sufficiently representative biometric data for biometric performance tests is obvious, it is becoming more and more important that the business can reasonably consider also alternative options such as the
generation and use of synthetic biometric data samples, ensuring comparable characteristics and representativeness to real data. However, generating synthetic face images with similar properties to real face images captured
at border control scenarios (e.g. frontal head poses without
face occlusions) continues to be a technical challenge.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2104.02815/assets/Figures/qs-examples.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example face images from different datasets annotated with three different FQAAs. (a) FRGC (b) StyleGAN (c) StyleGAN2</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In an attempt to answer this question, this work analyses the differences between synthetically generated face images and real face images from the biometric perspective. Although biometric performance reporting can be a way to assess the applicability of the synthetically generated data, the random generation procedure often produces limited true-mated pairs. Reporting the verification performance on synthetically generated dataset is therefore not realistic. Among other approaches for conducting such an analysis, we choose to assess the quality of the face images reflecting regular biometric systems where quality is first determined before using it for any biometric purpose. Face Quality Assessment Algorithms (FQAA) quantify the biometric quality of a sample (for instance, face image) by translating it to a quality score between [0, 100] <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. A high quality score indicates that the corresponding biometric sample is well suited for biometric recognition and a low quality score leads to inaccurate results due to the low quality of the input image. This understanding of biometric quality complies with the terminology of ISO/IEC 29794-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which defines the utility of a biometric sample as the prediction of the biometric recognition accuracy. In general, FQAAs are either aiming to predict the utility of face images for a specific face recognition model or alternatively try to establish a general utility predictor that can be used for an arbitrary face recognition system. In this context, Terh√∂rst et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> introduced a model-specific FQAA, based on the stochastic embedding robustness (‚ÄùSER-FIQ‚Äù) of quality features extracted with ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The comparison of the authors against previous state-of-the-art FQAA approaches showed that SER-FIQ significantly outperformed alternative methods. Recently, Hernandez-Ortega <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> presented FaceQnet v1, a general FQAA, the performance of which was benchmarked in the ongoing quality assessment evaluation of the National Institute of Standards and Quality (NIST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, despite state-of-the-art performances of SER-FIQ and FaceQnet v1, the utility predictions of both algorithms are not explainable. For this reason, face image quality features based on a technical report of international standardisation bodies, such as ISO/IEC TR 29794-5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, can be utilized to manually construct a more explainable FQAA, as proposed by Wasnik et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> (see FFigure <a href="#S4.F7" title="Figure 7 ‚Ä£ IV-B Comparison between Synthetic and Real Data ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">By measuring the face quality metrics of synthetically generated face samples, we can establish if they can be used for biometric algorithm training and testing purposes. In order to assert and evaluate our hypothesis, we conduct an extensive analysis of three different quality metrics such as ISO/IEC TR 29794-5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and two deep learning based quality metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> on face images generated using StyleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> respectively. StyleGAN2 emphasizes the capability of the new architecture to generate higher quality face images by removing water droplet-like artefacts caused by StyleGAN. However, the question of whether StyleGAN2 images are more suitable for face recognition compared to StyleGAN images has not yet been addressed. Therefore, this paper also includes a comparison between StyleGAN and StyleGAN2 based on recent face image quality assessment algorithms (FQAAs). Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the quality scores obtained on face images from different datasets. As noted from Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, various FQAAs indicate different face image quality score. Noting that they are inconsistent in providing similar scores, we evaluate the synthetically generated face images using all three FQAAs to assess the usability of such images in biometrics from different complementary perspectives.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contribution of this paper is in investigating and reporting to which extent the synthetic samples could replace the need for real images for conducting biometric performance tests of a pre-trained face recognition system. More specifically, the goal is to obtain synthetic face images that are similar to real face images not only from the human perception point, but also in the context of face recognition. In this paper, the differences between synthetic and real face images in terms of their utility are evaluated by measuring the quality with two deep learning based FQAAs: FaceQnet v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and SER-FIQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Additionally, based on the work of Wasnik et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, a random forest regressor is trained on manually extracted quality features defined by ISO/IEC TR 29794-5:2010 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Synthetic Face Image Generation and Dataset</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Karras et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> presented a style-based generator architecture for generative adversarial networks (StyleGAN), capable of generating synthetic images with high-resolution (1024x1024) and realistic appearances. StyleGAN applies latent space mapping and adaptive instance normalization (AdaIN) to control the image synthesis process by styles, which finally leads to the unsupervised separation of high-level attributes. In addition to their proposed GAN architecture, the authors webcrawled high-quality human face images from a social media platform (Flickr) in order to create a new dataset (FFHQ), which covers a wide variation of soft biometrics (e.g. ethnicity, gender). Extending the work of StyleGAN, Karras et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> further published the state-of-the-art model StyleGAN2, where they improved the architectural design and fixed the characteristic artefacts occurring in the synthetic images generated by StyleGAN.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">In order to assert our hypothesis, we employ a synthetically generated face image dataset. Our synthetic data is generated using StyleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> models which are pre-trained on the FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> dataset. These generated images are of resolution <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">√ó</mo><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">1024</cn><cn type="integer" id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">1024\times 1024</annotation></semantics></math> pixels with high visual quality.
The synthetic image generation by sampling latent codes may result in images choosing extreme regions of the latent space causing unwanted variation. To avoid such an adverse effect and mitigate images of unwanted visual appearance, we apply a truncation factor on the latent space of both StyleGAN and StyleGAN2. The truncated latent code <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="w^{\prime}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msup id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">w</mi><mo id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">‚Ä≤</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">ùë§</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">‚Ä≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">w^{\prime}</annotation></semantics></math> can be represented as <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="w^{\prime}=\bar{w}+\psi(w-\bar{w})" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><msup id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.3.2.cmml">w</mi><mo id="S2.SS1.p1.3.m3.1.1.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.cmml">‚Ä≤</mo></msup><mo id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">=</mo><mrow id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.cmml"><mover accent="true" id="S2.SS1.p1.3.m3.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.1.3.2.cmml">w</mi><mo id="S2.SS1.p1.3.m3.1.1.1.3.1" xref="S2.SS1.p1.3.m3.1.1.1.3.1.cmml">¬Ø</mo></mover><mo id="S2.SS1.p1.3.m3.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.2.cmml">+</mo><mrow id="S2.SS1.p1.3.m3.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.3.cmml">œà</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.SS1.p1.3.m3.1.1.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.3.m3.1.1.1.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.2.cmml">w</mi><mo id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mover accent="true" id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.2.cmml">w</mi><mo id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.1.cmml">¬Ø</mo></mover></mrow><mo stretchy="false" id="S2.SS1.p1.3.m3.1.1.1.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><eq id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2"></eq><apply id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2">ùë§</ci><ci id="S2.SS1.p1.3.m3.1.1.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3">‚Ä≤</ci></apply><apply id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"><plus id="S2.SS1.p1.3.m3.1.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.1.2"></plus><apply id="S2.SS1.p1.3.m3.1.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.1.3"><ci id="S2.SS1.p1.3.m3.1.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.3.1">¬Ø</ci><ci id="S2.SS1.p1.3.m3.1.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.1.3.2">ùë§</ci></apply><apply id="S2.SS1.p1.3.m3.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1"><times id="S2.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.2"></times><ci id="S2.SS1.p1.3.m3.1.1.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.3">ùúì</ci><apply id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1"><minus id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.1"></minus><ci id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.2">ùë§</ci><apply id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3"><ci id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.1">¬Ø</ci><ci id="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.1.3.2">ùë§</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">w^{\prime}=\bar{w}+\psi(w-\bar{w})</annotation></semantics></math>
and is generated by scaling the deviation between the original latent code <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">ùë§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">w</annotation></semantics></math> and <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="\bar{w}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mover accent="true" id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">w</mi><mo id="S2.SS1.p1.5.m5.1.1.1" xref="S2.SS1.p1.5.m5.1.1.1.cmml">¬Ø</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><ci id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1.1">¬Ø</ci><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">ùë§</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\bar{w}</annotation></semantics></math>, the center of mass of the latent space, with a truncation factor <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="\psi\leq 1" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">œà</mi><mo id="S2.SS1.p1.6.m6.1.1.1" xref="S2.SS1.p1.6.m6.1.1.1.cmml">‚â§</mo><mn id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><leq id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1.1"></leq><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">ùúì</ci><cn type="integer" id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\psi\leq 1</annotation></semantics></math>. The truncation factor therefore stabilizes the image generation by avoiding sampling latent codes in extreme regions of the latent space but will also cause some loss of variation as a trade-off. To provide an insight on the impact of the truncation factor on generated biometric quality, we also include sub-datasets generated with truncation factors varying from 0.25, 0.5 and 0.75 where each of the configurations has 50,000 random samples.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To compare the synthetic data with real face data, we chose a subset containing 24,025 images from the FRGC-V2 face database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> as our representative dataset due to relatively high-quality images and constrained conditions that resemble the image quality in EES cases.
To ensure that our synthetic datasets have comparable conditions, we also discard some unsatisfying images by having pre-selection criteria such as minimum inter-eye distance (IED), illumination metrics and predicted head poses to further improve the consistency between our assessment and the facial quality requirements proposed by ISO/IEC TR 29794-5:2010 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, ICAO 9303 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The key motivation is also to mimic automatic border control, one of the most common and important real-life scenarios of FRS. The statistical distribution information of our datasets is presented in Figure¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ II-A Dataset ‚Ä£ II Synthetic Face Image Generation and Dataset ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2104.02815/assets/Figures/dataset.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Constitution and origin of our datasets</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Face Quality Assessment Algorithms</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For the convenience of the reader, we provide a brief background of various FQAA. Our rationale for choosing them for the evaluations is to cover a standardized algorithm together with deep learning algorithms in the emerging direction. The range of the output scores in these algorithms is originally [0,1] but has been scaled to [0,100] following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">ISO/IEC TR 29794-5:2010 Implementation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">ISO/IEC 29794-5:2010 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> Implementation uses the hand-crafted quality metrics and is based on the work of Wasnik <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. For each image under assessment, various hand-crafted quality metrics for facial images following the technical report ISO/IEC TR 29794-5 are extracted as a feature vector. Given the feature vector as an input, then a pre-trained Random Forest Regressor is applied to predict a quality score. As against other deep learning models, this approach ensures a better explainability basing the results on various hand-crafted features.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">FaceQnet v1</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">FaceQnet v1 is a deep learning based FQAA proposed by Hernandez-Ortega et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and aims to predict the general utility of a face image, independent from a specific face recognition system. For the quality score prediction, a pre-trained network of ResNet-50 is fine-tuned on a small subset of VGGFace2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> including 300 data subjects. All images contained in VGGFace2 represent webcrawled celebrities with a large variation in pose, age, illumination, etc. Since ResNet-50 is already trained for face recognition, the authors assume that the same network weights can be exploited for assessing the quality of a face image.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">FaceQnet v1 follows a supervised learning approach, which means that the ground truth quality scores are required for fine-tuning the model. Finding representative quality scores that accurately reflect general utility criteria is a challenging task. Therefore, the authors assume if an ICAO 9303 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> compliant image A represents perfect image quality, the mated comparison score between image A (ICAO compliant) and image B (unknown quality) measures the utility of image B. If the mated comparison score is low, this must be due to image B, since image A fulfills the ICAO quality criteria. On the other hand, if the comparison score is high, it can be assumed that also image B is of good quality.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The authors employed the BioLab-ICAO framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to select high-quality images per subject, which are used as a reference for computing the ground truth quality score for the remaining training images. The ground truth quality scores are obtained by calculating and fusing the comparison scores of mated samples, which represents the utility of the non-ICAO compliant sample.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">SER-FIQ</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Recently, Terh√∂rst et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> introduced an estimation method for predicting the face image quality based on stochastic embedding robustness (SER-FIQ). SER-FIQ is an unsupervised technique that is not dependent on previously extracted ground truths in order to train the prediction model. Compared to FaceQnet v1, which outputs the general utility of a face image, SER-FIQ focuses on predicting the utility of a specific face recognition system. More precisely, the quality scores are based on the variations of face embeddings stemming from random subnetworks of a face recognition model. The authors argue that a high variation between the embeddings of the same sample functions as a robustness indication, which is assumed to be synonymous with image quality.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Since the face embeddings extracted with face recognition systems are deterministic, the main idea of SER-FIQ is to add dropout layers as additional components to create random subnetworks for each prediction of the same sample. Once a fixed number of stochastic embeddings are extracted, the sigmoid of the negative mean euclidean distances between all embedding pairs is computed and outputs a quality score. However, the computational complexity of SER-FIQ increases quadratically with the number of random subnetworks, which leads to a trade-off between the efficiency of the algorithm and the expected accuracy of the quality predictions. Following the recommendation of the authors, all experiments in this report are conducted with a number of N=100 subnetworks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, the experimental details for assessing the quality of synthetic data are introduced and the corresponding quantitative and qualitative results are discussed. We first provide the differences between synthetic datasets generated with StyleGAN and StyleGAN2, taking into account various truncation factors. Based on these results, we also present the analysis on the distinction between synthetic and real face images.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In order to compare the utility of face images from different datasets, impostor distributions of comparison scores are created to evaluate differences in the similarity among non-mated face images. The impostor distribution can show the diversity of identity in each dataset and also can evaluate its verification performance on FRS, in the circumstance that our synthetic data is randomly generated without any mated samples. In the impostor distribution, 5,000 random pairs are randomly sampled from the dataset and the cosine similarities between their embeddings extracted with Arcface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> are calculated. Then, comparison scores of each dataset are converted to histograms and fitted to a Gaussian distribution using kernel density estimation. As a reference, the threshold=0.25 of the applied Arcface model is set at False Match Rate (FMR) = 0.1% on LFW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> following the guidelines of Frontex <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. In addition to the impostor scores, we apply two deep learning-based and one standard FQAA to predict the utility of the face images. Once the quality scores are extracted, the differences between histograms of different datasets are evaluated using Kullback-Leibler divergence which measures how one distribution is different from another.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Comparison between StyleGAN and StyleGAN2</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As introduced in Section <a href="#S1" title="I Introduction ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, Karras et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> have stated that StyleGAN2 has achieved an improvement in image quality. However, it is still necessary to further evaluate the difference of generated data between StyleGAN and StyleGAN2 from the perspective of biometric data instead of general images.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ IV-A Comparison between StyleGAN and StyleGAN2 ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents impostor distributions of StyleGAN and StyleGAN2. It is shown that the dataset generated with StyleGAN2 using a truncation factor of <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\psi=0.25" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">œà</mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">0.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">ùúì</ci><cn type="float" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">0.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\psi=0.25</annotation></semantics></math> has a higher mean value of comparison score than StyleGAN. However, this difference is reducing with the increase of <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\psi" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">œà</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">ùúì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\psi</annotation></semantics></math> and vanishes as the truncation factor increases to 0.75. This also fits our original understanding of the truncation that it shrinks the sampling region of the latent space so the generated facial images will have less variation of conditions. However, this may also raise the concern that the generated samples will lack a diversity of identity information. As shown in Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ IV-A Comparison between StyleGAN and StyleGAN2 ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, it is obvious that reducing the truncation factor will lead to an increase in the mean value of comparison scores, which means that the non-mated samples tend to have more similar identity information.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">To further validate the findings based on the impostor scores, the distributions of the face quality scores from different FQAAs are shown in Figure <a href="#S4.F4" title="Figure 4 ‚Ä£ IV-A Comparison between StyleGAN and StyleGAN2 ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, limited to a single truncation factor of <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\psi=0.75" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">œà</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><eq id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">ùúì</ci><cn type="float" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\psi=0.75</annotation></semantics></math>. Similar to the impostor scores, the distributions between StyleGAN and StyleGAN2 are close to identical across all FQAAs, thus reinforcing the conclusion that StyleGAN and StyleGAN2 face images are equally suitable for biometric recognition.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel">
<div id="S4.F3.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:134.4pt;">
<img src="/html/2104.02815/assets/Figures/imposter_distribution_1vs2_025.jpg" id="S4.F3.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="210" height="157" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F3.sf1.4.2" class="ltx_text" style="font-size:80%;">KL-D: 0.702</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel">
<div id="S4.F3.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:134.4pt;">
<img src="/html/2104.02815/assets/Figures/imposter_distribution_1vs2_050.jpg" id="S4.F3.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="210" height="157" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F3.sf2.4.2" class="ltx_text" style="font-size:80%;">KL-D: 0.107</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel">
<div id="S4.F3.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:134.4pt;">
<img src="/html/2104.02815/assets/Figures/imposter_distribution_1vs2_075.jpg" id="S4.F3.sf3.1.g1" class="ltx_graphics ltx_img_landscape" width="210" height="157" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S4.F3.sf3.4.2" class="ltx_text" style="font-size:80%;">KL-D: 0.007</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of impostor distributions between StyleGAN and StyleGAN2 using Arcface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (threshold=0.25 @ FMR=0.1% on LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> dataset). (a) FaceQnet v1 (b) Random Forest Regressor (ISO/IEC TR 29794-5) (c) SER-FIQ </figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel">
<div id="S4.F4.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:134.4pt;">
<img src="/html/2104.02815/assets/Figures/faceqnet_1vs2_075.png" id="S4.F4.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="220" height="165" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F4.sf1.4.2" class="ltx_text" style="font-size:80%;">KL-D: 0.008</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel">
<div id="S4.F4.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:134.4pt;">
<img src="/html/2104.02815/assets/Figures/iso_1vs2_075.png" id="S4.F4.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="220" height="165" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F4.sf2.4.2" class="ltx_text" style="font-size:80%;">KL-D: 0.003</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel">
<div id="S4.F4.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:134.4pt;">
<img src="/html/2104.02815/assets/Figures/serfiq_1vs2_075.png" id="S4.F4.sf3.1.g1" class="ltx_graphics ltx_img_landscape" width="220" height="165" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S4.F4.sf3.4.2" class="ltx_text" style="font-size:80%;">KL-D: 0.116</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparing the quality score distributions from various face quality algorithms between StyleGAN and StyleGAN2. (a) FaceQnet v1 (b) Random Forest Regressor (ISO/IEC TR 29794-5) (c) SER-FIQ </figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Comparison between Synthetic and Real Data</span>
</h3>

<figure id="S4.F5" class="ltx_figure"><img src="/html/2104.02815/assets/Figures/imposter_realvssyn.jpg" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="347" height="261" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Impostor comparison score distributions of randomly selected StyleGAN and FRGC image pairs using Arcface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (threshold=0.25 @ FMR=0.1% on LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> dataset). KL-D: 0.184.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel">
<div id="S4.F6.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:121.4pt;">
<img src="/html/2104.02815/assets/Figures/qs_realvssyn/faceqnet_fvs2.png" id="S4.F6.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="210" height="157" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S4.F6.sf1.4.2" class="ltx_text" style="font-size:80%;">KL-D: 0.111</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel">
<div id="S4.F6.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:121.4pt;">
<img src="/html/2104.02815/assets/Figures/qs_realvssyn/ISO_fvs2.png" id="S4.F6.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="210" height="157" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S4.F6.sf2.4.2" class="ltx_text" style="font-size:80%;">KL-D: 0.444</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel">
<div id="S4.F6.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:121.4pt;">
<img src="/html/2104.02815/assets/Figures/qs_realvssyn/serfiq_fvs2.png" id="S4.F6.sf3.1.g1" class="ltx_graphics ltx_img_landscape" width="210" height="157" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S4.F6.sf3.4.2" class="ltx_text" style="font-size:80%;">KL-D: 1.172</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparing the quality score distributions from various face quality algorithms between FRGC and StyleGAN2. (a) FaceQnet v1 (b) Random Forest Regressor (ISO/IEC TR 29794-5) (c) SER-FIQ </figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.1" class="ltx_figure ltx_figure_panel">
<div id="S4.F7.1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:177.8pt;">
<img src="/html/2104.02815/assets/Figures/qs_realvssyn/iso_features_realvs2.png" id="S4.F7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="260" height="195" alt="Refer to caption">
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.2" class="ltx_figure ltx_figure_panel">
<div id="S4.F7.2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:177.8pt;">
<img src="/html/2104.02815/assets/Figures/qs_realvssyn/bluriness_realvs2.png" id="S4.F7.2.1.g1" class="ltx_graphics ltx_img_landscape" width="279" height="210" alt="Refer to caption">
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparing the quality features from ISO/IEC TR 29794-5:2010 implementation between FRGC and StyleGAN2 (<math id="S4.F7.4.m1.1" class="ltx_Math" alttext="\psi=0.75" display="inline"><semantics id="S4.F7.4.m1.1b"><mrow id="S4.F7.4.m1.1.1" xref="S4.F7.4.m1.1.1.cmml"><mi id="S4.F7.4.m1.1.1.2" xref="S4.F7.4.m1.1.1.2.cmml">œà</mi><mo id="S4.F7.4.m1.1.1.1" xref="S4.F7.4.m1.1.1.1.cmml">=</mo><mn id="S4.F7.4.m1.1.1.3" xref="S4.F7.4.m1.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.4.m1.1c"><apply id="S4.F7.4.m1.1.1.cmml" xref="S4.F7.4.m1.1.1"><eq id="S4.F7.4.m1.1.1.1.cmml" xref="S4.F7.4.m1.1.1.1"></eq><ci id="S4.F7.4.m1.1.1.2.cmml" xref="S4.F7.4.m1.1.1.2">ùúì</ci><cn type="float" id="S4.F7.4.m1.1.1.3.cmml" xref="S4.F7.4.m1.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.4.m1.1d">\psi=0.75</annotation></semantics></math>). (a) Kullback-Leibler Divergences between each quality features (b) Comparison of bluriness score distributions</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In the previous section, the comparison between StyleGAN and StyleGAN2 has revealed that as the truncation factor increases, the differences between the generated images vanish. Meanwhile, it has been shown that the variation of identity information of generated images is limited when the truncation factor is small. Based on this observation, this section focuses on the comparison of the utility between synthetic and real face images. More precisely, the synthetic images are generated with StyleGAN2 (<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\psi=0.75" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">œà</mi><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">ùúì</ci><cn type="float" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\psi=0.75</annotation></semantics></math>) and compared to real face images from the publicly available FRGC dataset (see section <a href="#S2.SS1" title="II-A Dataset ‚Ä£ II Synthetic Face Image Generation and Dataset ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Figure <a href="#S2.F2" title="Figure 2 ‚Ä£ II-A Dataset ‚Ä£ II Synthetic Face Image Generation and Dataset ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the proportion of filtered out images: While only 58% of the StyleGAN2 images passed the filtering pipeline, more FRGC images could be retained with a rate of 75%. The huge difference can be explained by the way the data samples were acquired: StyleGAN2 is trained on the FFHQ dataset, which has been webcrawled from a social media platform (Flickr). On the other hand, the images contained in FRGC were physically captured to use them for face recognition testing. Therefore, the variability of the StyleGAN2 images is much higher compared to those included in FRGC. In the same context, the number of filtered out images due to extreme pose rotations turns out to be much lower for FRGC in contrast to StyleGAN2 images.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Analogous to the last subsection, Figure <a href="#S4.F5" title="Figure 5 ‚Ä£ IV-B Comparison between Synthetic and Real Data ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares the impostor score distributions between StyleGAN2 and FRGC non-mated pairs. Since we are investigating whether real face images can be replaced by synthetic ones, the degree of overlapping areas is of particular interest. In this context, the inspection of Figure <a href="#S4.F5" title="Figure 5 ‚Ä£ IV-B Comparison between Synthetic and Real Data ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that both distributions have a similar gaussian-curved shape with huge overlapping areas. However, it is also visible that the right tail of the StyleGAN2 distribution is heavier compared to the FRGC dataset. In other words, the similarity scores of non-mated comparisons are slightly higher compared to those of FRGC, which potentially causes higher False-Match-Rates. Nevertheless, the minor differences might also be due to random effects rooted in the data acquisition process.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">To further evaluate the discrepancies between StyleGAN2 and FRGC, Figure <a href="#S4.F6" title="Figure 6 ‚Ä£ IV-B Comparison between Synthetic and Real Data ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> depicts the quality score distributions for different FQAAs. Looking at the FaceQnet v1 distributions, both areas are nearly identical with a very low Kullback Leibler Divergence of 0.111. However, steering the focus on the SER-FIQ distributions, a clear shift in the peaks is notable, which reveals that the estimated utility of the synthetic images is lower compared to FRGC images. This behavior corresponds to our expectations as the variety of pose rotations is much higher within the StyleGAN2 images and the authors highlight the sensitivity of their approach to this kind of deviation.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Finally, Figure <a href="#S4.F7" title="Figure 7 ‚Ä£ IV-B Comparison between Synthetic and Real Data ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the Kullback-Leibler divergences between distributions of the individual ISO/IEC TR 29794-5:2010 quality features. At first glance, the blurriness score stands out, which indicates a significant difference between the real and synthetic datasets. A deeper look at the two distributions in Figure <a href="#S4.F7" title="Figure 7 ‚Ä£ IV-B Comparison between Synthetic and Real Data ‚Ä£ IV Experiments and Results ‚Ä£ On the Applicability of Synthetic Data for Face Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(b) reveals that several FRGC images are annotated with high blurriness scores, which manifests itself in a bimodal distribution. The reason for this observation is due to the capturing process, where the images of multiple subjects were captured in motion.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To investigate the suitability of synthetically generated face images for biometric recognition, we evaluated the face quality of synthetic face images generated by StyleGAN and StyleGAN2 and also compared to real face images from the FRGC dataset which represents the quality of images in border control. The comparison of the utility between different datasets is based on histograms of non-mated comparison scores with Arcface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and histograms of quality scores with 3 different representative face quality algorithms, including the state-of-the-art SER-FIQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> algorithm.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The first part of our experiments focuses on the comparison between different synthetic face datasets. The evaluation result of comparing synthetic face data generated by StyleGAN and StyleGAN2 shows that their utility for biometric recognition is similar. In this context, both the impostor comparison scores and the quality scores are consistent in their results and indicate that as the truncation factor increases, the differences vanish. By comparing various truncation factors, it is also demonstrated that while the image quality increases with smaller truncation values, the variety of the generated face images decreases at the same time. In other words, a low truncation factor leads to face images with similar identities and is therefore not suitable for biometric performance in border control scenarios, such as the EES.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The second part of the experiments is focusing on the comparison between the synthetic and real face images. The impostor distribution of synthetic data is slightly offset to the right, which means that the similarity between non-mated samples is slightly higher compared to real face images. The analysis of the FaceQnet v1 quality scores has not revealed any major differences between images stemming from StyleGAN2 or FRGC. However, the comparison of the SER-FIQ quality scores revealed a minor drop in quality for synthetic face images, which can be explained by the wider range of pose rotations. Further, the assessment of the ISO/IEC TR 29794-5:2010 features has shown a high blurriness score for FRGC images, caused by capturing some face images in motion. Finally, we conclude that StyleGAN2 and FRGC images have shown the minor differences in face quality, which means the evaluated synthetic data can achieve a similar quality as biometric samples in EES cases and allows us to exploit both domains for realistic biometric performance tests.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
International Organization for Standardization, ‚ÄúISO/IEC 2382-37:2017
Information technology ‚Äî Vocabulary ‚Äî Part 37: Biometrics,‚Äù 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Council of European Union, ‚ÄúCouncil regulation (EU) no 2226/2017:
Establishing an Entry/Exit System (EES),‚Äù 2017,

<br class="ltx_break">https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32017R2226.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Frontex, ‚ÄúBest practice technical guidelines for Automated Border Control
(ABC) systems,‚Äù 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
International Organization for Standardization, ‚ÄúISO/IEC 29794-1:2016
Information technology ‚Äî Biometric sample quality ‚Äî Part 1: Framework,‚Äù
2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P.¬†Terhorst, J.¬†N. Kolf, N.¬†Damer, F.¬†Kirchbuchner, and A.¬†Kuijper, ‚ÄúSer-fiq:
Unsupervised estimation of face image quality based on stochastic embedding
robustness,‚Äù in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>, 2020, pp. 5651‚Äì5660.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.¬†Deng, J.¬†Guo, N.¬†Xue, and S.¬†Zafeiriou, ‚ÄúArcface: Additive angular margin
loss for deep face recognition,‚Äù in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition</em>, 2019, pp. 4690‚Äì4699.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J.¬†Hernandez-Ortega, J.¬†Galbally, J.¬†Fierrez, and L.¬†Beslay, ‚ÄúBiometric
Quality: Review and Application to Face Recognition with
FaceQnet,‚Äù <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv:2006.03298</em>, June 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P.¬†Grother, A.¬†Hom, M.¬†Ngan, and K.¬†Hanaoka, ‚ÄúOngoing face recognition vendor
test (frvt) - part 5: Face image quality asssessment,‚Äù <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">NIST draft IR</em>,
2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
International Organization for Standardization, ‚ÄúISO/IEC TR 29794-5:2010
Information technology ‚Äî Biometric sample quality ‚Äî Part 5: Face image
data,‚Äù 2010.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
P.¬†Wasnik, K.¬†B. Raja, R.¬†Ramachandra, and C.¬†Busch, ‚ÄúAssessing face image
quality for smartphone based face recognition system,‚Äù in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2017 5th
International Workshop on Biometrics and Forensics (IWBF)</em>.¬†¬†¬†IEEE, 2017, pp. 1‚Äì6.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T.¬†Karras, S.¬†Laine, and T.¬†Aila, ‚ÄúA style-based generator architecture for
generative adversarial networks,‚Äù in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 4401‚Äì4410.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
T.¬†Karras, S.¬†Laine, M.¬†Aittala, J.¬†Hellsten, J.¬†Lehtinen, and T.¬†Aila,
‚ÄúAnalyzing and improving the image quality of stylegan,‚Äù in
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2020, pp. 8110‚Äì8119.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P.¬†J. Phillips, P.¬†J. Flynn, T.¬†Scruggs, K.¬†W. Bowyer, Jin Chang,
K.¬†Hoffman, J.¬†Marques, Jaesik Min, and W.¬†Worek, ‚ÄúOverview of the
face recognition grand challenge,‚Äù in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2005 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition (CVPR‚Äô05)</em>, June 2005,
pp. 947‚Äì954 vol. 1.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
International Civil Aviation Organization, ‚ÄúMachine readable passports ‚Äì
part 9 ‚Äì deployment of biometric identification and electronic storage of
data in emrtds,‚Äù International Civil Aviation Organization (ICAO), 2015,

<br class="ltx_break">https://www.icao.int/publications/pages/publication.aspx?docnum=9303.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J.¬†Hernandez-Ortega, J.¬†Galbally, J.¬†Fierrez, and L.¬†Beslay, ‚ÄúBiometric
quality: Review and application to face recognition with faceqnet,‚Äù
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.03298</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Q.¬†Cao, L.¬†Shen, W.¬†Xie, O.¬†M. Parkhi, and A.¬†Zisserman, ‚ÄúVggface2: A dataset
for recognising faces across pose and age,‚Äù in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2018 13th IEEE
international conference on automatic face &amp; gesture recognition (FG
2018)</em>.¬†¬†¬†IEEE, 2018, pp. 67‚Äì74.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M.¬†Ferrara, A.¬†Franco, D.¬†Maio, and D.¬†Maltoni, ‚ÄúFace image conformance to
iso/icao standards in machine readable travel documents,‚Äù <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Information Forensics and Security</em>, vol.¬†7, no.¬†4, pp.
1204‚Äì1213, 2012.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G.¬†B. Huang, M.¬†Mattar, T.¬†Berg, and E.¬†Learned-Miller, ‚ÄúLabeled faces in the
wild: A database for studying face recognition in unconstrained
environments,‚Äù in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Workshop on faces in‚ÄôReal-Life‚ÄôImages: detection,
alignment, and recognition</em>, 2008.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Frontex, ‚ÄúBest Practice Technical Guidelines for Automated Border Control
(ABC) Systems,‚Äù Frontex ‚Äì Research and Development Unit, 2015.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.02813" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.02815" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.02815">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.02815" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.02816" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 01:45:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
