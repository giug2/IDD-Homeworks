<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title>
<!--Generated on Thu Sep 12 17:37:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.08239v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S1" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S2" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Synthetic Data Generation using LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Teaching LLMs to Use Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Teaching LLMs to use SQL</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.SS1" title="In 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.SS1.SSS0.Px1" title="In 3.1 Dataset Generation ‣ 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data source selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.SS1.SSS0.Px2" title="In 3.1 Dataset Generation ‣ 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Seed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.SS1.SSS0.Px3" title="In 3.1 Dataset Generation ‣ 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Dataset construction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.SS2" title="In 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Dataset Curation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.SS2.SSS0.Px1" title="In 3.2 Dataset Curation ‣ 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.SS2.SSS0.Px2" title="In 3.2 Dataset Curation ‣ 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data Imputation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.SS3" title="In 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Model fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Applying <span class="ltx_text ltx_font_italic">Source2Synth</span> to Special Cases</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS1" title="In 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Multi-hop question answering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS1.SSS1" title="In 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Dataset Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS1.SSS1.Px1" title="In 4.1.1 Dataset Generation ‣ 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data source selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS1.SSS1.Px2" title="In 4.1.1 Dataset Generation ‣ 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Seed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS1.SSS1.Px3" title="In 4.1.1 Dataset Generation ‣ 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Dataset construction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS1.SSS2" title="In 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Dataset Curation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS1.SSS2.Px1" title="In 4.1.2 Dataset Curation ‣ 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS1.SSS2.Px2" title="In 4.1.2 Dataset Curation ‣ 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data Imputation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS2" title="In 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Tabular question answering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS2.SSS0.Px1" title="In 4.2 Tabular question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data source selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS2.SSS0.Px2" title="In 4.2 Tabular question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Seed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS2.SSS1" title="In 4.2 Tabular question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Dataset Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS2.SSS2" title="In 4.2 Tabular question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Dataset Curation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.SS2.SSS2.Px1" title="In 4.2.2 Dataset Curation ‣ 4.2 Tabular question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data filtering</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS1" title="In 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Multi-Hop QA Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS1.SSS0.Px1" title="In 5.1 Multi-Hop QA Setup ‣ 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS1.SSS0.Px2" title="In 5.1 Multi-Hop QA Setup ‣ 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS1.SSS0.Px3" title="In 5.1 Multi-Hop QA Setup ‣ 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS1.SSS0.Px4" title="In 5.1 Multi-Hop QA Setup ‣ 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS2" title="In 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Tabular QA Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS2.SSS0.Px1" title="In 5.2 Tabular QA Setup ‣ 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS2.SSS0.Px2" title="In 5.2 Tabular QA Setup ‣ 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS2.SSS0.Px3" title="In 5.2 Tabular QA Setup ‣ 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S5.SS2.SSS0.Px4" title="In 5.2 Tabular QA Setup ‣ 5 Experimental Setup ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S6" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S6.SS1" title="In 6 Results ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Multi-Hop question answering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S6.SS1.SSS0.Px1" title="In 6.1 Multi-Hop question answering ‣ 6 Results ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Overall performance of <span class="ltx_text ltx_font_italic">Source2Synth</span> on MHQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S6.SS1.SSS0.Px2" title="In 6.1 Multi-Hop question answering ‣ 6 Results ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title">Scaling performance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S6.SS2" title="In 6 Results ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Tabular question answering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S7" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S8" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#A1" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>More Results on Prompt Engineering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#A2" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>SQL non-executable code filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#A3" title="In Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Prompts used in our experiments</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Source2Synth:  Synthetic Data Generation and Curation 
<br class="ltx_break"/>Grounded in Real Data Sources</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Alisia Lupidi<sup class="ltx_sup" id="id1.1.id1.1">1,2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.id2">Carlos Gemmell<sup class="ltx_sup" id="id2.2.id2.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id3.3.id3">Nicola Cancedda <sup class="ltx_sup" id="id3.3.id3.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id4.4.id4">Jane Dwivedi-Yu <sup class="ltx_sup" id="id4.4.id4.1">1</sup></span>,

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.5.id5">Jason Weston <sup class="ltx_sup" id="id5.5.id5.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id6.6.id6">Jakob Foerster <sup class="ltx_sup" id="id6.6.id6.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id7.7.id7">Roberta Raileanu<sup class="ltx_sup" id="id7.7.id7.1">1,3</sup></span>,
<span class="ltx_text ltx_font_bold" id="id8.8.id8">Maria Lomeli<sup class="ltx_sup" id="id8.8.id8.1">1</sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id9.9.id9">1</sup>Meta,
<sup class="ltx_sup" id="id10.10.id10">2</sup>Oxford University,
<sup class="ltx_sup" id="id11.11.id11">3</sup>University College London
</span></span>
</div>
<h1 class="ltx_title ltx_title_document">Source2Synth:  Synthetic Data Generation and Curation 
<br class="ltx_break"/>Grounded in Real Data Sources</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Alisia Lupidi<sup class="ltx_sup" id="id1.1.id1.1">1,2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.id2">Carlos Gemmell<sup class="ltx_sup" id="id2.2.id2.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id3.3.id3">Nicola Cancedda <sup class="ltx_sup" id="id3.3.id3.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id4.4.id4">Jane Dwivedi-Yu <sup class="ltx_sup" id="id4.4.id4.1">1</sup></span>,

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.5.id5">Jason Weston <sup class="ltx_sup" id="id5.5.id5.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id6.6.id6">Jakob Foerster <sup class="ltx_sup" id="id6.6.id6.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id7.7.id7">Roberta Raileanu<sup class="ltx_sup" id="id7.7.id7.1">1,3</sup></span>,
<span class="ltx_text ltx_font_bold" id="id8.8.id8">Maria Lomeli<sup class="ltx_sup" id="id8.8.id8.1">1</sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id9.9.id9">1</sup>Meta,
<sup class="ltx_sup" id="id10.10.id10">2</sup>Oxford University,
<sup class="ltx_sup" id="id11.11.id11">3</sup>University College London
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose  <span class="ltx_text ltx_font_italic" id="id12.id1.1">Source2Synth</span>: a new method that can be used for teaching LLMs new skills without
relying on costly human annotations. <span class="ltx_text ltx_font_italic" id="id12.id1.2">Source2Synth</span> takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources.
<span class="ltx_text ltx_font_italic" id="id12.id1.3">Source2Synth</span> improves the dataset quality by discarding low-quality generations based on their answerability.
We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA).
Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA
compared to the fine-tuned baselines.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Source2Synth:  Synthetic Data Generation and Curation 
<br class="ltx_break"/>Grounded in Real Data Sources</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">
Alisia Lupidi<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">1,2</sup>,
Carlos Gemmell<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">1</sup>,
Nicola Cancedda <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">1</sup>,
Jane Dwivedi-Yu <sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4">1</sup>,</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.1">Jason Weston <sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.2">Jakob Foerster <sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.2.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.3">Roberta Raileanu<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.3.1">1,3</sup></span>,
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.4">Maria Lomeli<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.4.1">1</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">1</sup>Meta,
<sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.2">2</sup>Oxford University,
<sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.3">3</sup>University College London</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib4" title="">2019</a>; Chowdhery et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib3" title="">2022</a>; Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib1" title="">2020</a>; Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib31" title="">2017</a>)</cite> have risen to popularity due to their remarkable ability to digest and generate human-like text <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib23" title="">2018</a>)</cite>.
However, LLMs still struggle with more complex tasks such as multi-step reasoning, tool use and manipulating or processing structured data.
For many of these tasks there exists source data, such as existing structured data on the web, but little data of how to use it to solve a task.In principle, one can achieve performance improvements during fine-tuning by collecting human annotated data of such tasks.
However, this is an expensive and time-consuming process <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib30" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, we propose <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Source2Synth</span>, a general approach to generate <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">synthetic data grounded in external real-world sources</span>. Grounding the data generation process in real-world sources steers the examples to be more realistic, diverse, and factually correct.
We showcase our method on two challenging tasks: multi-hop questions based on sources from the web, and
tabular question answering using SQL as a tool.
In both cases it achieves improved performance without relying on human annotations, resulting in a scalable data generation method for complex tasks.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_italic" id="S1.p3.1.1">Source2Synth</span> consists of three stages: <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">Dataset Generation</span> and <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">Dataset Curation</span>, followed by <span class="ltx_text ltx_font_italic" id="S1.p3.1.4">Model Finetuning</span>, see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>. At the <span class="ltx_text ltx_font_italic" id="S1.p3.1.5">Data Generation</span> stage, we start by selecting a data source (such as tables on the web, or related Wikipedia articles) to <span class="ltx_text ltx_font_italic" id="S1.p3.1.6">ground</span> our synthetic data generation in realistic information for a specific task. Then, to generate a given example, our method first selects a seed topic to condition the generation - for example a specific entity in a Wikipedia article or a factual statement about a table.
Given the seed topic, the method then generates the full example: the instruction (e.g., question), the reasoning chain to arrive at the answer (e.g., the steps of multi-hop question answering, or tool use) and the answer itself.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">At the <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Data Curation</span> stage, the constructed synthetic dataset is split into two slices: the first slice is used to fine-tune the LLM, resulting in an intermediate fine-tuned model. We use this model to curate the second slice of data via imputation and the use of a filtering step by rejection sampling. For imputation, we blank some parts of a given
example and accept the example if the model can fill in the blanks. For filtering, we reject examples that cannot produce the correct answer in <math alttext="k" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mi id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">italic_k</annotation></semantics></math> trials. This provides a higher quality curated dataset for the final fine-tuning stage on the second slice, resulting in a better performing model on a given task.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To demonstrate the generality of our approach, we apply it to two different domains:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">answering <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">tabular-based questions</span> by learning how to use SQL as a tool;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">answering <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">multi-hop questions</span> by performing multi-step reasoning and information extraction.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To summarize, our key contributions are:</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We introduce a new method for generating synthetic examples aligned with the target task, given a real-world data source as context.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We introduce a curation method based on filtering and imputation which yields higher quality data and improved task performance.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="444" id="S1.F1.g1" src="x1.png" width="790"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.8.1">Overall <span class="ltx_text ltx_font_italic" id="S1.F1.8.1.1">Source2Synth</span> Method.</span> In the <span class="ltx_text ltx_font_italic" id="S1.F1.9.2">Dataset Generation</span> step we first choose a data source to build our dataset from. For each example we select a seed topic to condition the generation on, and use the data source and seed together to construct the example. The resulting synthetic dataset is sliced in two: slice 0 is used to fine-tune an intermediate version of the LLM (<span class="ltx_text ltx_font_italic" id="S1.F1.10.3">LLMSynth</span>), and we use <span class="ltx_text ltx_font_italic" id="S1.F1.11.4">LLMSynth</span> to curate slice 1 through filtering and/or imputation during the <span class="ltx_text ltx_font_italic" id="S1.F1.12.5">Dataset Curation</span> step. The resulting curated dataset is of higher quality and aligned with the user’s design. At the <span class="ltx_text ltx_font_italic" id="S1.F1.13.6">Model Finetuning</span> stage, the final LLM (<span class="ltx_text ltx_font_italic" id="S1.F1.14.7">LLMCurated</span>) is trained on the curated synthetic dataset, which can then be used to provide good performance on the task of interest.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Synthetic Data Generation using LLMs</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">A number of works propose different strategies to generate synthetic datasets leveraging pre-trained language models. Some of these works rely on probing the knowledge contained in the LLM by first providing a prompt and letting the model either generate the continuation of a prefix or predict missing words in a close-style template <cite class="ltx_cite ltx_citemacro_citep">(Schick and Schütze, <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib25" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib27" title="">2021</a>; Petroni et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib21" title="">2019</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib9" title="">2019</a>)</cite>.
Other works introduce a variety of ways to improve the quality of synthetic data by using model-based or human filtering <cite class="ltx_cite ltx_citemacro_citep">(Schick and Schütze, <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib26" title="">2021</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib13" title="">2022</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib12" title="">2024</a>; Thoppilan et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib29" title="">2022</a>)</cite>. Our method however does not rely on human annotations, and we improve the quality of the synthetic data by leveraging the LLM itself. Furthermore, our selection of the seed topic is automated and we use real data as a starting point. We note that some recent work also leverages real-world data for specific cases, such as a corpus from the web to construct high-quality synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib17" title="">2024</a>)</cite> or open-source code snippets to generate diverse instruction data for code generation <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib33" title="">2024</a>; Dubey et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib5" title="">2024</a>)</cite>. In our case, we do not require a back-translation approach or an initial finetuning to generate the seed to digest the data. Our work proposes a general framework which can be applied across tasks.
See <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib14" title="">2024</a>)</cite> for a thorough overview of synthetic data research and references therein.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Teaching LLMs to Use Tools</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Enabling LLMs to use different tools can augment their abilities beyond text generation and towards manipulating structured data, retrieving information from external sources, or interacting with APIs. Even though the goal of our work is not specifically to teach models to use tools, but to develop a general synthetic data generation approach, we consider this to be a by-product. For example, we demonstrate how our method can be used to make LLMs use SQL - which is an example of a tool.
Tool usage is a very active area of research.
Various works augment LLMs with general tools or API calls <cite class="ltx_cite ltx_citemacro_citep">(Parisi et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib19" title="">2022</a>; Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib24" title="">2023</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib28" title="">2023</a>)</cite>, while some propose to interweave intermediate reasoning steps with API calls <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib6" title="">2023</a>; Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib2" title="">2024</a>; Paranjape et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib18" title="">2023</a>)</cite> which improves performance on more complex tasks.
Finally, handling unseen tools at test time has also been tackled <cite class="ltx_cite ltx_citemacro_citep">(Paranjape et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib18" title="">2023</a>; Mekala et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib15" title="">2024</a>)</cite>.
See <cite class="ltx_cite ltx_citemacro_citet">Mialon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib16" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib22" title="">2023</a>)</cite> for an in-depth literature review of augmented language models research and references therein.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Teaching LLMs to use SQL</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">The above approaches usually only enable tool usage for inputs that are strings or numbers. However, using structured data during post-training can be useful to enhance the LLM’s capabilities in complex tasks, such as tables or relational data like graphs. A particular tool of interest is SQL since it enables aggregating information from tabular data. There exist a variety of benchmarks that have been proposed to assess LLMs abilities to generate SQL as well as their performance on tabular-based question answering leveraging SQL tasks <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib10" title="">2023a</a>; Zhong et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib36" title="">2017</a>)</cite>.
Alternatively, handling tabular data directly by LLMs has also been tried <cite class="ltx_cite ltx_citemacro_citep">(Herzig et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib8" title="">2020</a>; Gemmell and Dalton, <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib7" title="">2023</a>)</cite>, and tabular question answering benchmarks have been proposed  <cite class="ltx_cite ltx_citemacro_citep">(Pasupat and Liang, <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib20" title="">2015</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.p1.1.1">Source2Synth</span> produces high-quality synthetic examples grounded in external real-world data sources, and this resulting synthetic data is provided as step-by-step examples to the LLM for fine-tuning.
<span class="ltx_text ltx_font_italic" id="S3.p1.1.2">Source2Synth</span> is composed of three stages: <span class="ltx_text ltx_font_italic" id="S3.p1.1.3">Dataset Generation</span>, <span class="ltx_text ltx_font_italic" id="S3.p1.1.4">Dataset Curation</span>, and <span class="ltx_text ltx_font_italic" id="S3.p1.1.5">Model fine-tuning</span>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset Generation</h3>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data source selection</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">The generation process begins by selecting a data source. This can be an already existing dataset re-purposed for a given task, a collection of existing data points that we would like to leverage to construct a new dataset, or structured information (e.g. graphs, tables).
There is no need for human annotations on the entries, as <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.1">Source2Synth</span> will enrich it with extra instructions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Seed</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">In order to create a given example of our new synthetic dataset, we first generate
a <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px2.p1.1.1">seed</span> topic as the initial trigger for the generation process, which is chosen conditioned on a randomly selected portion of the source data. The seed inspires the creation of the entry and dictates how the source data will be used.
In addition, the randomness of the seed ensures variety in the generated data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Dataset construction</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">In order to tackle complex tasks, LLMs can leverage a step-by-step approach <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib32" title="">2022</a>)</cite> that divides reasoning into smaller sub-tasks plus instructions on how to merge back each step into the final one. In <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px3.p1.1.1">Source2Synth</span>, we leverage the seed to build synthetic data step-by-step, decomposing into such intermediate steps in order to arrive at an answer for a given question. This reasoning chain can then be used as supervision by providing it as the target in the synthetically generated training examples.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F2.1" style="width:505.2pt;height:279.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(50.5pt,-27.9pt) scale(1.25,1.25) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="429" id="S3.F2.1.g1" src="x2.png" width="763"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.F2.24.1">Source2Synth<span class="ltx_text ltx_font_upright" id="S3.F2.24.1.1"> synthetic data generation process for multi-hop question answering</span></span>. The method first randomly picks one article <math alttext="D_{1}" class="ltx_Math" display="inline" id="S3.F2.12.m1.1"><semantics id="S3.F2.12.m1.1b"><msub id="S3.F2.12.m1.1.1" xref="S3.F2.12.m1.1.1.cmml"><mi id="S3.F2.12.m1.1.1.2" xref="S3.F2.12.m1.1.1.2.cmml">D</mi><mn id="S3.F2.12.m1.1.1.3" xref="S3.F2.12.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.12.m1.1c"><apply id="S3.F2.12.m1.1.1.cmml" xref="S3.F2.12.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.12.m1.1.1.1.cmml" xref="S3.F2.12.m1.1.1">subscript</csymbol><ci id="S3.F2.12.m1.1.1.2.cmml" xref="S3.F2.12.m1.1.1.2">𝐷</ci><cn id="S3.F2.12.m1.1.1.3.cmml" type="integer" xref="S3.F2.12.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.12.m1.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.12.m1.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, in this case with title "The Moon". At the <span class="ltx_text ltx_font_italic" id="S3.F2.25.2">Seed</span> stage, an entity E is selected from <math alttext="D_{1}" class="ltx_Math" display="inline" id="S3.F2.13.m2.1"><semantics id="S3.F2.13.m2.1b"><msub id="S3.F2.13.m2.1.1" xref="S3.F2.13.m2.1.1.cmml"><mi id="S3.F2.13.m2.1.1.2" xref="S3.F2.13.m2.1.1.2.cmml">D</mi><mn id="S3.F2.13.m2.1.1.3" xref="S3.F2.13.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.13.m2.1c"><apply id="S3.F2.13.m2.1.1.cmml" xref="S3.F2.13.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.13.m2.1.1.1.cmml" xref="S3.F2.13.m2.1.1">subscript</csymbol><ci id="S3.F2.13.m2.1.1.2.cmml" xref="S3.F2.13.m2.1.1.2">𝐷</ci><cn id="S3.F2.13.m2.1.1.3.cmml" type="integer" xref="S3.F2.13.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.13.m2.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.13.m2.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>’s pool of entities, “Apollo 11”. Then, documents are sampled from the related documents pool of <math alttext="D_{1}" class="ltx_Math" display="inline" id="S3.F2.14.m3.1"><semantics id="S3.F2.14.m3.1b"><msub id="S3.F2.14.m3.1.1" xref="S3.F2.14.m3.1.1.cmml"><mi id="S3.F2.14.m3.1.1.2" xref="S3.F2.14.m3.1.1.2.cmml">D</mi><mn id="S3.F2.14.m3.1.1.3" xref="S3.F2.14.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.14.m3.1c"><apply id="S3.F2.14.m3.1.1.cmml" xref="S3.F2.14.m3.1.1"><csymbol cd="ambiguous" id="S3.F2.14.m3.1.1.1.cmml" xref="S3.F2.14.m3.1.1">subscript</csymbol><ci id="S3.F2.14.m3.1.1.2.cmml" xref="S3.F2.14.m3.1.1.2">𝐷</ci><cn id="S3.F2.14.m3.1.1.3.cmml" type="integer" xref="S3.F2.14.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.14.m3.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.14.m3.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> such that <math alttext="E" class="ltx_Math" display="inline" id="S3.F2.15.m4.1"><semantics id="S3.F2.15.m4.1b"><mi id="S3.F2.15.m4.1.1" xref="S3.F2.15.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.F2.15.m4.1c"><ci id="S3.F2.15.m4.1.1.cmml" xref="S3.F2.15.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.15.m4.1d">E</annotation><annotation encoding="application/x-llamapun" id="S3.F2.15.m4.1e">italic_E</annotation></semantics></math> is present,
and <math alttext="D_{2}" class="ltx_Math" display="inline" id="S3.F2.16.m5.1"><semantics id="S3.F2.16.m5.1b"><msub id="S3.F2.16.m5.1.1" xref="S3.F2.16.m5.1.1.cmml"><mi id="S3.F2.16.m5.1.1.2" xref="S3.F2.16.m5.1.1.2.cmml">D</mi><mn id="S3.F2.16.m5.1.1.3" xref="S3.F2.16.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.16.m5.1c"><apply id="S3.F2.16.m5.1.1.cmml" xref="S3.F2.16.m5.1.1"><csymbol cd="ambiguous" id="S3.F2.16.m5.1.1.1.cmml" xref="S3.F2.16.m5.1.1">subscript</csymbol><ci id="S3.F2.16.m5.1.1.2.cmml" xref="S3.F2.16.m5.1.1.2">𝐷</ci><cn id="S3.F2.16.m5.1.1.3.cmml" type="integer" xref="S3.F2.16.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.16.m5.1d">D_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.16.m5.1e">italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, “Neil Armstrong”, is selected. A question <math alttext="Q_{1}" class="ltx_Math" display="inline" id="S3.F2.17.m6.1"><semantics id="S3.F2.17.m6.1b"><msub id="S3.F2.17.m6.1.1" xref="S3.F2.17.m6.1.1.cmml"><mi id="S3.F2.17.m6.1.1.2" xref="S3.F2.17.m6.1.1.2.cmml">Q</mi><mn id="S3.F2.17.m6.1.1.3" xref="S3.F2.17.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.17.m6.1c"><apply id="S3.F2.17.m6.1.1.cmml" xref="S3.F2.17.m6.1.1"><csymbol cd="ambiguous" id="S3.F2.17.m6.1.1.1.cmml" xref="S3.F2.17.m6.1.1">subscript</csymbol><ci id="S3.F2.17.m6.1.1.2.cmml" xref="S3.F2.17.m6.1.1.2">𝑄</ci><cn id="S3.F2.17.m6.1.1.3.cmml" type="integer" xref="S3.F2.17.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.17.m6.1d">Q_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.17.m6.1e">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is then generated from <math alttext="D_{1}" class="ltx_Math" display="inline" id="S3.F2.18.m7.1"><semantics id="S3.F2.18.m7.1b"><msub id="S3.F2.18.m7.1.1" xref="S3.F2.18.m7.1.1.cmml"><mi id="S3.F2.18.m7.1.1.2" xref="S3.F2.18.m7.1.1.2.cmml">D</mi><mn id="S3.F2.18.m7.1.1.3" xref="S3.F2.18.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.18.m7.1c"><apply id="S3.F2.18.m7.1.1.cmml" xref="S3.F2.18.m7.1.1"><csymbol cd="ambiguous" id="S3.F2.18.m7.1.1.1.cmml" xref="S3.F2.18.m7.1.1">subscript</csymbol><ci id="S3.F2.18.m7.1.1.2.cmml" xref="S3.F2.18.m7.1.1.2">𝐷</ci><cn id="S3.F2.18.m7.1.1.3.cmml" type="integer" xref="S3.F2.18.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.18.m7.1d">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.18.m7.1e">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> with the constraint that the answer <math alttext="A_{1}" class="ltx_Math" display="inline" id="S3.F2.19.m8.1"><semantics id="S3.F2.19.m8.1b"><msub id="S3.F2.19.m8.1.1" xref="S3.F2.19.m8.1.1.cmml"><mi id="S3.F2.19.m8.1.1.2" xref="S3.F2.19.m8.1.1.2.cmml">A</mi><mn id="S3.F2.19.m8.1.1.3" xref="S3.F2.19.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.19.m8.1c"><apply id="S3.F2.19.m8.1.1.cmml" xref="S3.F2.19.m8.1.1"><csymbol cd="ambiguous" id="S3.F2.19.m8.1.1.1.cmml" xref="S3.F2.19.m8.1.1">subscript</csymbol><ci id="S3.F2.19.m8.1.1.2.cmml" xref="S3.F2.19.m8.1.1.2">𝐴</ci><cn id="S3.F2.19.m8.1.1.3.cmml" type="integer" xref="S3.F2.19.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.19.m8.1d">A_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.19.m8.1e">italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is the entity itself. A second question <math alttext="Q_{2}" class="ltx_Math" display="inline" id="S3.F2.20.m9.1"><semantics id="S3.F2.20.m9.1b"><msub id="S3.F2.20.m9.1.1" xref="S3.F2.20.m9.1.1.cmml"><mi id="S3.F2.20.m9.1.1.2" xref="S3.F2.20.m9.1.1.2.cmml">Q</mi><mn id="S3.F2.20.m9.1.1.3" xref="S3.F2.20.m9.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.20.m9.1c"><apply id="S3.F2.20.m9.1.1.cmml" xref="S3.F2.20.m9.1.1"><csymbol cd="ambiguous" id="S3.F2.20.m9.1.1.1.cmml" xref="S3.F2.20.m9.1.1">subscript</csymbol><ci id="S3.F2.20.m9.1.1.2.cmml" xref="S3.F2.20.m9.1.1.2">𝑄</ci><cn id="S3.F2.20.m9.1.1.3.cmml" type="integer" xref="S3.F2.20.m9.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.20.m9.1d">Q_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.20.m9.1e">italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is then generated from <math alttext="D_{2}" class="ltx_Math" display="inline" id="S3.F2.21.m10.1"><semantics id="S3.F2.21.m10.1b"><msub id="S3.F2.21.m10.1.1" xref="S3.F2.21.m10.1.1.cmml"><mi id="S3.F2.21.m10.1.1.2" xref="S3.F2.21.m10.1.1.2.cmml">D</mi><mn id="S3.F2.21.m10.1.1.3" xref="S3.F2.21.m10.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.F2.21.m10.1c"><apply id="S3.F2.21.m10.1.1.cmml" xref="S3.F2.21.m10.1.1"><csymbol cd="ambiguous" id="S3.F2.21.m10.1.1.1.cmml" xref="S3.F2.21.m10.1.1">subscript</csymbol><ci id="S3.F2.21.m10.1.1.2.cmml" xref="S3.F2.21.m10.1.1.2">𝐷</ci><cn id="S3.F2.21.m10.1.1.3.cmml" type="integer" xref="S3.F2.21.m10.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.21.m10.1d">D_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.21.m10.1e">italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> with the constraint that its main topic is the entity. We then prompt an LLM to merge the two questions based on the link/entity they have in common to produce the final question, reasoning chain and answer that comprise the training example.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset Curation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Dataset Generation</span> process yields an augmented dataset grounded in real data. At the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">Dataset Curation</span> step, we employ a model-based approach to automatically refine the dataset to enhance its quality, while avoiding the need for human supervision. In particular, we prune the newly-built dataset of all the entries that have been incorrectly crafted or that are deemed low quality. This is achieved by slicing the dataset in two and using one slice to fine-tune the LLM (<span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.3">LLMSynth</span>). During curation, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.4">LLMSynth</span> is then used to improve the quality of the second slice of the dataset using imputation plus a filtering step. After these steps, we obtain the final curated dataset (shown in purple in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>).</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data filtering</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">During filtering, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.1">LLMSynth</span> is used to predict the output of the given synthetic example using <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> tries.
If the output cannot be predicted at least once, it is assumed the example is low quality and is not included in the final curated dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Data Imputation</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">We also consider an imputation process, which involves blanking parts of the augmented data points and using the LLM to fill in the blanks, to replace those fields. This is to provide cleaner data which is less unnatural.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model fine-tuning</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">At this stage, we fine-tune on the curated synthetic dataset, initializing from a base version or instruction-tuned version of the LLM. We use our dataset for supervised training of both the reasoning chain and the final answer given an input.
The resulting model <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">LLMCurated</span> is then ready to perform the desired task.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Applying <span class="ltx_text ltx_font_italic" id="S4.1.1">Source2Synth</span> to Special Cases</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The general pipeline described above can be used to produce examples for the task at hand and to teach LLMs new skills. To demonstrate the impact of <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Source2Synth</span>, we apply it to two challenging tasks where LLMs struggle, which are both areas of great interest for the community: multi-hop question answering and tabular question answering.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Multi-hop question answering</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In multi-hop question answering (MHQA),
we generate a dataset of multi-hop question-answer pairs, in addition to the reasoning chain that is used to answer the question, consisting of question decomposition into subquestions with answers, plus the entity that links them.
See Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.F2" title="Figure 2 ‣ Dataset construction ‣ 3.1 Dataset Generation ‣ 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">2</span></a> for an overview of the procedure and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S4.F3" title="Figure 3 ‣ Dataset construction ‣ 4.1.1 Dataset Generation ‣ 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">3</span></a> for an example response from the <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">Source2Synth</span> model.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Dataset Generation</h4>
<section class="ltx_paragraph" id="S4.SS1.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data source selection</h5>
<div class="ltx_para" id="S4.SS1.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.Px1.p1.3">For multi-hop question answering, we pick English Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Wikipedia contributors, <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib34" title="">2004</a>)</cite> as the data source, since it contains articles in natural language as well as additional meta-information like links to related articles.
The data generation process starts by randomly selecting an initial article, denoted as <math alttext="D_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.Px1.p1.1.m1.1a"><msub id="S4.SS1.SSS1.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2">𝐷</ci><cn id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px1.p1.1.m1.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px1.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, among all available Wikipedia articles. For each <math alttext="D_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px1.p1.2.m2.1"><semantics id="S4.SS1.SSS1.Px1.p1.2.m2.1a"><msub id="S4.SS1.SSS1.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px1.p1.2.m2.1b"><apply id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2">𝐷</ci><cn id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px1.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px1.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> we collect <math alttext="n\geq 2" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px1.p1.3.m3.1"><semantics id="S4.SS1.SSS1.Px1.p1.3.m3.1a"><mrow id="S4.SS1.SSS1.Px1.p1.3.m3.1.1" xref="S4.SS1.SSS1.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS1.Px1.p1.3.m3.1.1.2" xref="S4.SS1.SSS1.Px1.p1.3.m3.1.1.2.cmml">n</mi><mo id="S4.SS1.SSS1.Px1.p1.3.m3.1.1.1" xref="S4.SS1.SSS1.Px1.p1.3.m3.1.1.1.cmml">≥</mo><mn id="S4.SS1.SSS1.Px1.p1.3.m3.1.1.3" xref="S4.SS1.SSS1.Px1.p1.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px1.p1.3.m3.1b"><apply id="S4.SS1.SSS1.Px1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.3.m3.1.1"><geq id="S4.SS1.SSS1.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.3.m3.1.1.1"></geq><ci id="S4.SS1.SSS1.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.Px1.p1.3.m3.1.1.2">𝑛</ci><cn id="S4.SS1.SSS1.Px1.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px1.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px1.p1.3.m3.1c">n\geq 2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px1.p1.3.m3.1d">italic_n ≥ 2</annotation></semantics></math> related articles.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Seed</h5>
<div class="ltx_para" id="S4.SS1.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS1.Px2.p1.9">An MHQA seed topic corresponds to an entity <math alttext="E" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.1.m1.1"><semantics id="S4.SS1.SSS1.Px2.p1.1.m1.1a"><mi id="S4.SS1.SSS1.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS1.Px2.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.1.m1.1b"><ci id="S4.SS1.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.1.m1.1d">italic_E</annotation></semantics></math> retrieved from <math alttext="D_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.2.m2.1"><semantics id="S4.SS1.SSS1.Px2.p1.2.m2.1a"><msub id="S4.SS1.SSS1.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS1.Px2.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS1.Px2.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.Px2.p1.2.m2.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS1.Px2.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.Px2.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.2.m2.1b"><apply id="S4.SS1.SSS1.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.Px2.p1.2.m2.1.1.2">𝐷</ci><cn id="S4.SS1.SSS1.Px2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. The seed in MHQA doubles also as the “hop” in the multi-hop question <math alttext="Q" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.3.m3.1"><semantics id="S4.SS1.SSS1.Px2.p1.3.m3.1a"><mi id="S4.SS1.SSS1.Px2.p1.3.m3.1.1" xref="S4.SS1.SSS1.Px2.p1.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.3.m3.1b"><ci id="S4.SS1.SSS1.Px2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.3.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.3.m3.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.3.m3.1d">italic_Q</annotation></semantics></math> that we aim to generate, since <math alttext="E" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.4.m4.1"><semantics id="S4.SS1.SSS1.Px2.p1.4.m4.1a"><mi id="S4.SS1.SSS1.Px2.p1.4.m4.1.1" xref="S4.SS1.SSS1.Px2.p1.4.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.4.m4.1b"><ci id="S4.SS1.SSS1.Px2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.4.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.4.m4.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.4.m4.1d">italic_E</annotation></semantics></math> links the <math alttext="n=2" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.5.m5.1"><semantics id="S4.SS1.SSS1.Px2.p1.5.m5.1a"><mrow id="S4.SS1.SSS1.Px2.p1.5.m5.1.1" xref="S4.SS1.SSS1.Px2.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS1.Px2.p1.5.m5.1.1.2" xref="S4.SS1.SSS1.Px2.p1.5.m5.1.1.2.cmml">n</mi><mo id="S4.SS1.SSS1.Px2.p1.5.m5.1.1.1" xref="S4.SS1.SSS1.Px2.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS1.Px2.p1.5.m5.1.1.3" xref="S4.SS1.SSS1.Px2.p1.5.m5.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.5.m5.1b"><apply id="S4.SS1.SSS1.Px2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.5.m5.1.1"><eq id="S4.SS1.SSS1.Px2.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.5.m5.1.1.1"></eq><ci id="S4.SS1.SSS1.Px2.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.Px2.p1.5.m5.1.1.2">𝑛</ci><cn id="S4.SS1.SSS1.Px2.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px2.p1.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.5.m5.1c">n=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.5.m5.1d">italic_n = 2</annotation></semantics></math> subquestions that compose <math alttext="Q" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.6.m6.1"><semantics id="S4.SS1.SSS1.Px2.p1.6.m6.1a"><mi id="S4.SS1.SSS1.Px2.p1.6.m6.1.1" xref="S4.SS1.SSS1.Px2.p1.6.m6.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.6.m6.1b"><ci id="S4.SS1.SSS1.Px2.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.6.m6.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.6.m6.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.6.m6.1d">italic_Q</annotation></semantics></math>. For example, in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.F2" title="Figure 2 ‣ Dataset construction ‣ 3.1 Dataset Generation ‣ 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">2</span></a>, we sample "The Moon" article at random, denoted by <math alttext="D_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.7.m7.1"><semantics id="S4.SS1.SSS1.Px2.p1.7.m7.1a"><msub id="S4.SS1.SSS1.Px2.p1.7.m7.1.1" xref="S4.SS1.SSS1.Px2.p1.7.m7.1.1.cmml"><mi id="S4.SS1.SSS1.Px2.p1.7.m7.1.1.2" xref="S4.SS1.SSS1.Px2.p1.7.m7.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS1.Px2.p1.7.m7.1.1.3" xref="S4.SS1.SSS1.Px2.p1.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.7.m7.1b"><apply id="S4.SS1.SSS1.Px2.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px2.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px2.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS1.Px2.p1.7.m7.1.1.2">𝐷</ci><cn id="S4.SS1.SSS1.Px2.p1.7.m7.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px2.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.7.m7.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.7.m7.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and the corresponding entity, denoted by <math alttext="E" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.8.m8.1"><semantics id="S4.SS1.SSS1.Px2.p1.8.m8.1a"><mi id="S4.SS1.SSS1.Px2.p1.8.m8.1.1" xref="S4.SS1.SSS1.Px2.p1.8.m8.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.8.m8.1b"><ci id="S4.SS1.SSS1.Px2.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.8.m8.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.8.m8.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.8.m8.1d">italic_E</annotation></semantics></math>, is "Apollo 11" (displayed in blue). Then, we pick "Neil Armstrong" as <math alttext="D_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px2.p1.9.m9.1"><semantics id="S4.SS1.SSS1.Px2.p1.9.m9.1a"><msub id="S4.SS1.SSS1.Px2.p1.9.m9.1.1" xref="S4.SS1.SSS1.Px2.p1.9.m9.1.1.cmml"><mi id="S4.SS1.SSS1.Px2.p1.9.m9.1.1.2" xref="S4.SS1.SSS1.Px2.p1.9.m9.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS1.Px2.p1.9.m9.1.1.3" xref="S4.SS1.SSS1.Px2.p1.9.m9.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px2.p1.9.m9.1b"><apply id="S4.SS1.SSS1.Px2.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px2.p1.9.m9.1.1.1.cmml" xref="S4.SS1.SSS1.Px2.p1.9.m9.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px2.p1.9.m9.1.1.2.cmml" xref="S4.SS1.SSS1.Px2.p1.9.m9.1.1.2">𝐷</ci><cn id="S4.SS1.SSS1.Px2.p1.9.m9.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px2.p1.9.m9.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px2.p1.9.m9.1c">D_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px2.p1.9.m9.1d">italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> from the pool of related articles, since it contains a paragraph where the entity "Apollo 11" is included.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Dataset construction</h5>
<div class="ltx_para" id="S4.SS1.SSS1.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS1.Px3.p1.10">We prompt an instruction-tuned language model to generate two questions: a question <math alttext="Q_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.1.m1.1"><semantics id="S4.SS1.SSS1.Px3.p1.1.m1.1a"><msub id="S4.SS1.SSS1.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2.cmml">Q</mi><mn id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.1.m1.1b"><apply id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2">𝑄</ci><cn id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.1.m1.1c">Q_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.1.m1.1d">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> based on <math alttext="D_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.2.m2.1"><semantics id="S4.SS1.SSS1.Px3.p1.2.m2.1a"><msub id="S4.SS1.SSS1.Px3.p1.2.m2.1.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.2.m2.1b"><apply id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2">𝐷</ci><cn id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.2.m2.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and whose answer is the selected entity <math alttext="E" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.3.m3.1"><semantics id="S4.SS1.SSS1.Px3.p1.3.m3.1a"><mi id="S4.SS1.SSS1.Px3.p1.3.m3.1.1" xref="S4.SS1.SSS1.Px3.p1.3.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.3.m3.1b"><ci id="S4.SS1.SSS1.Px3.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.3.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.3.m3.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.3.m3.1d">italic_E</annotation></semantics></math>, and a second question <math alttext="Q_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.4.m4.1"><semantics id="S4.SS1.SSS1.Px3.p1.4.m4.1a"><msub id="S4.SS1.SSS1.Px3.p1.4.m4.1.1" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2.cmml">Q</mi><mn id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.4.m4.1b"><apply id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2">𝑄</ci><cn id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.4.m4.1c">Q_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.4.m4.1d">italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> based on <math alttext="D_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.5.m5.1"><semantics id="S4.SS1.SSS1.Px3.p1.5.m5.1a"><msub id="S4.SS1.SSS1.Px3.p1.5.m5.1.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.5.m5.1b"><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2">𝐷</ci><cn id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.5.m5.1c">D_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> such that its main topic is <math alttext="E" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.6.m6.1"><semantics id="S4.SS1.SSS1.Px3.p1.6.m6.1a"><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.6.m6.1b"><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.6.m6.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.6.m6.1d">italic_E</annotation></semantics></math>.
See Figures  <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#A3.F16" title="Figure 16 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">16</span></a> and  <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#A3.F17" title="Figure 17 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">17</span></a> for the exact prompts.
For example, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S3.F2" title="Figure 2 ‣ Dataset construction ‣ 3.1 Dataset Generation ‣ 3 Method ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">2</span></a>, <math alttext="Q_{1}=" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.7.m7.1"><semantics id="S4.SS1.SSS1.Px3.p1.7.m7.1a"><mrow id="S4.SS1.SSS1.Px3.p1.7.m7.1.1" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.cmml"><msub id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.cmml"><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.2" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.2.cmml">Q</mi><mn id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.3" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1.cmml">=</mo><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.7.m7.1b"><apply id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1"><eq id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1"></eq><apply id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.2.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.2">𝑄</ci><cn id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.3.cmml" type="integer" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.7.m7.1c">Q_{1}=</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.7.m7.1d">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT =</annotation></semantics></math> "What was the spaceflight that first landed humans on the Moon?", the hop is <math alttext="E=" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.8.m8.1"><semantics id="S4.SS1.SSS1.Px3.p1.8.m8.1a"><mrow id="S4.SS1.SSS1.Px3.p1.8.m8.1.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.2.cmml">E</mi><mo id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.1.cmml">=</mo><mi id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.3" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.8.m8.1b"><apply id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1"><eq id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.1"></eq><ci id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.2">𝐸</ci><csymbol cd="latexml" id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.8.m8.1c">E=</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.8.m8.1d">italic_E =</annotation></semantics></math> "Apollo" and <math alttext="Q_{2}=" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.9.m9.1"><semantics id="S4.SS1.SSS1.Px3.p1.9.m9.1a"><mrow id="S4.SS1.SSS1.Px3.p1.9.m9.1.1" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.cmml"><msub id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.cmml"><mi id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.2" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.2.cmml">Q</mi><mn id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.3" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1.cmml">=</mo><mi id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.9.m9.1b"><apply id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1"><eq id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1"></eq><apply id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.1.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.2.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.2">𝑄</ci><cn id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.3.cmml" type="integer" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.3">2</cn></apply><csymbol cd="latexml" id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.9.m9.1c">Q_{2}=</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.9.m9.1d">italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT =</annotation></semantics></math> "Who was the commander of Apollo 11?".
We then prompt the LLM to merge
the two questions, in order to generate the final two-hop question <math alttext="Q" class="ltx_Math" display="inline" id="S4.SS1.SSS1.Px3.p1.10.m10.1"><semantics id="S4.SS1.SSS1.Px3.p1.10.m10.1a"><mi id="S4.SS1.SSS1.Px3.p1.10.m10.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.10.m10.1b"><ci id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.10.m10.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.Px3.p1.10.m10.1d">italic_Q</annotation></semantics></math> by using the entity as a conceptual link (hop). The exact prompt is given in
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#A3.F15" title="Figure 15 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 15</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F3.1" style="width:481.8pt;height:270.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(131.4pt,-73.7pt) scale(2.2,2.2) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="236" id="S4.F3.1.g1" src="x3.png" width="419"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S4.F3.11.1">Example <span class="ltx_text ltx_font_italic" id="S4.F3.11.1.1">Source2Synth</span> Response on MHQA</span> (closed book inference). We show the model’s response (reasoning steps and answer) to a multi-hop input question (yellow). The colours highlight the generation of the corresponding augmented entries: the decomposition into sub questions <math alttext="Q_{1}" class="ltx_Math" display="inline" id="S4.F3.6.m1.1"><semantics id="S4.F3.6.m1.1b"><msub id="S4.F3.6.m1.1.1" xref="S4.F3.6.m1.1.1.cmml"><mi id="S4.F3.6.m1.1.1.2" xref="S4.F3.6.m1.1.1.2.cmml">Q</mi><mn id="S4.F3.6.m1.1.1.3" xref="S4.F3.6.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.F3.6.m1.1c"><apply id="S4.F3.6.m1.1.1.cmml" xref="S4.F3.6.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.6.m1.1.1.1.cmml" xref="S4.F3.6.m1.1.1">subscript</csymbol><ci id="S4.F3.6.m1.1.1.2.cmml" xref="S4.F3.6.m1.1.1.2">𝑄</ci><cn id="S4.F3.6.m1.1.1.3.cmml" type="integer" xref="S4.F3.6.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m1.1d">Q_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.F3.6.m1.1e">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="Q_{2}" class="ltx_Math" display="inline" id="S4.F3.7.m2.1"><semantics id="S4.F3.7.m2.1b"><msub id="S4.F3.7.m2.1.1" xref="S4.F3.7.m2.1.1.cmml"><mi id="S4.F3.7.m2.1.1.2" xref="S4.F3.7.m2.1.1.2.cmml">Q</mi><mn id="S4.F3.7.m2.1.1.3" xref="S4.F3.7.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.F3.7.m2.1c"><apply id="S4.F3.7.m2.1.1.cmml" xref="S4.F3.7.m2.1.1"><csymbol cd="ambiguous" id="S4.F3.7.m2.1.1.1.cmml" xref="S4.F3.7.m2.1.1">subscript</csymbol><ci id="S4.F3.7.m2.1.1.2.cmml" xref="S4.F3.7.m2.1.1.2">𝑄</ci><cn id="S4.F3.7.m2.1.1.3.cmml" type="integer" xref="S4.F3.7.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.7.m2.1d">Q_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.F3.7.m2.1e">italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> in green, the seed <math alttext="A_{1}" class="ltx_Math" display="inline" id="S4.F3.8.m3.1"><semantics id="S4.F3.8.m3.1b"><msub id="S4.F3.8.m3.1.1" xref="S4.F3.8.m3.1.1.cmml"><mi id="S4.F3.8.m3.1.1.2" xref="S4.F3.8.m3.1.1.2.cmml">A</mi><mn id="S4.F3.8.m3.1.1.3" xref="S4.F3.8.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.F3.8.m3.1c"><apply id="S4.F3.8.m3.1.1.cmml" xref="S4.F3.8.m3.1.1"><csymbol cd="ambiguous" id="S4.F3.8.m3.1.1.1.cmml" xref="S4.F3.8.m3.1.1">subscript</csymbol><ci id="S4.F3.8.m3.1.1.2.cmml" xref="S4.F3.8.m3.1.1.2">𝐴</ci><cn id="S4.F3.8.m3.1.1.3.cmml" type="integer" xref="S4.F3.8.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.8.m3.1d">A_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.F3.8.m3.1e">italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> in blue, and the final answer <math alttext="A_{2}" class="ltx_Math" display="inline" id="S4.F3.9.m4.1"><semantics id="S4.F3.9.m4.1b"><msub id="S4.F3.9.m4.1.1" xref="S4.F3.9.m4.1.1.cmml"><mi id="S4.F3.9.m4.1.1.2" xref="S4.F3.9.m4.1.1.2.cmml">A</mi><mn id="S4.F3.9.m4.1.1.3" xref="S4.F3.9.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.F3.9.m4.1c"><apply id="S4.F3.9.m4.1.1.cmml" xref="S4.F3.9.m4.1.1"><csymbol cd="ambiguous" id="S4.F3.9.m4.1.1.1.cmml" xref="S4.F3.9.m4.1.1">subscript</csymbol><ci id="S4.F3.9.m4.1.1.2.cmml" xref="S4.F3.9.m4.1.1.2">𝐴</ci><cn id="S4.F3.9.m4.1.1.3.cmml" type="integer" xref="S4.F3.9.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.9.m4.1d">A_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.F3.9.m4.1e">italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> in red.

</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Dataset Curation</h4>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data filtering</h5>
<div class="ltx_para" id="S4.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px1.p1.1">We check if the predicted answer matches the answer in the synthetically generated example, and if after <math alttext="k" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS2.Px1.p1.1.m1.1a"><mi id="S4.SS1.SSS2.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS2.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px1.p1.1.m1.1b"><ci id="S4.SS1.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> tries the LLM has not supplied the correct answer we filter out the entry entirely.
See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#S4.F3" title="Figure 3 ‣ Dataset construction ‣ 4.1.1 Dataset Generation ‣ 4.1 Multi-hop question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> for an example of model inference.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Data Imputation</h5>
<div class="ltx_para" id="S4.SS1.SSS2.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.Px2.p1.11">For MHQA, we blank <math alttext="Q_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.Px2.p1.1.m1.1a"><msub id="S4.SS1.SSS2.Px2.p1.1.m1.1.1" xref="S4.SS1.SSS2.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.Px2.p1.1.m1.1.1.2" xref="S4.SS1.SSS2.Px2.p1.1.m1.1.1.2.cmml">Q</mi><mn id="S4.SS1.SSS2.Px2.p1.1.m1.1.1.3" xref="S4.SS1.SSS2.Px2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.1.m1.1b"><apply id="S4.SS1.SSS2.Px2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.Px2.p1.1.m1.1.1.2">𝑄</ci><cn id="S4.SS1.SSS2.Px2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.Px2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.1.m1.1c">Q_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.1.m1.1d">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and provide the LLM with <math alttext="Q" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.Px2.p1.2.m2.1a"><mi id="S4.SS1.SSS2.Px2.p1.2.m2.1.1" xref="S4.SS1.SSS2.Px2.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.2.m2.1b"><ci id="S4.SS1.SSS2.Px2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.2.m2.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.2.m2.1d">italic_Q</annotation></semantics></math>, <math alttext="Q_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.3.m3.1"><semantics id="S4.SS1.SSS2.Px2.p1.3.m3.1a"><msub id="S4.SS1.SSS2.Px2.p1.3.m3.1.1" xref="S4.SS1.SSS2.Px2.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS2.Px2.p1.3.m3.1.1.2" xref="S4.SS1.SSS2.Px2.p1.3.m3.1.1.2.cmml">Q</mi><mn id="S4.SS1.SSS2.Px2.p1.3.m3.1.1.3" xref="S4.SS1.SSS2.Px2.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.3.m3.1b"><apply id="S4.SS1.SSS2.Px2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.Px2.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.Px2.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.Px2.p1.3.m3.1.1.2">𝑄</ci><cn id="S4.SS1.SSS2.Px2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.Px2.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.3.m3.1c">Q_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.3.m3.1d">italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="E" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.4.m4.1"><semantics id="S4.SS1.SSS2.Px2.p1.4.m4.1a"><mi id="S4.SS1.SSS2.Px2.p1.4.m4.1.1" xref="S4.SS1.SSS2.Px2.p1.4.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.4.m4.1b"><ci id="S4.SS1.SSS2.Px2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.4.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.4.m4.1c">E</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.4.m4.1d">italic_E</annotation></semantics></math>, and the relative doc sample <math alttext="D_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.5.m5.1"><semantics id="S4.SS1.SSS2.Px2.p1.5.m5.1a"><msub id="S4.SS1.SSS2.Px2.p1.5.m5.1.1" xref="S4.SS1.SSS2.Px2.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS2.Px2.p1.5.m5.1.1.2" xref="S4.SS1.SSS2.Px2.p1.5.m5.1.1.2.cmml">D</mi><mn id="S4.SS1.SSS2.Px2.p1.5.m5.1.1.3" xref="S4.SS1.SSS2.Px2.p1.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.5.m5.1b"><apply id="S4.SS1.SSS2.Px2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.Px2.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.Px2.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS2.Px2.p1.5.m5.1.1.2">𝐷</ci><cn id="S4.SS1.SSS2.Px2.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.Px2.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.5.m5.1c">D_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> as context when asking it to reconstruct <math alttext="Q_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.6.m6.1"><semantics id="S4.SS1.SSS2.Px2.p1.6.m6.1a"><msub id="S4.SS1.SSS2.Px2.p1.6.m6.1.1" xref="S4.SS1.SSS2.Px2.p1.6.m6.1.1.cmml"><mi id="S4.SS1.SSS2.Px2.p1.6.m6.1.1.2" xref="S4.SS1.SSS2.Px2.p1.6.m6.1.1.2.cmml">Q</mi><mn id="S4.SS1.SSS2.Px2.p1.6.m6.1.1.3" xref="S4.SS1.SSS2.Px2.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.6.m6.1b"><apply id="S4.SS1.SSS2.Px2.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.Px2.p1.6.m6.1.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.Px2.p1.6.m6.1.1.2.cmml" xref="S4.SS1.SSS2.Px2.p1.6.m6.1.1.2">𝑄</ci><cn id="S4.SS1.SSS2.Px2.p1.6.m6.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.Px2.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.6.m6.1c">Q_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.6.m6.1d">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. The new candidate for <math alttext="Q_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.7.m7.1"><semantics id="S4.SS1.SSS2.Px2.p1.7.m7.1a"><msub id="S4.SS1.SSS2.Px2.p1.7.m7.1.1" xref="S4.SS1.SSS2.Px2.p1.7.m7.1.1.cmml"><mi id="S4.SS1.SSS2.Px2.p1.7.m7.1.1.2" xref="S4.SS1.SSS2.Px2.p1.7.m7.1.1.2.cmml">Q</mi><mn id="S4.SS1.SSS2.Px2.p1.7.m7.1.1.3" xref="S4.SS1.SSS2.Px2.p1.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.7.m7.1b"><apply id="S4.SS1.SSS2.Px2.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.Px2.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.Px2.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS2.Px2.p1.7.m7.1.1.2">𝑄</ci><cn id="S4.SS1.SSS2.Px2.p1.7.m7.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.Px2.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.7.m7.1c">Q_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.7.m7.1d">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is then assessed: if <math alttext="A^{\prime}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.8.m8.1"><semantics id="S4.SS1.SSS2.Px2.p1.8.m8.1a"><msup id="S4.SS1.SSS2.Px2.p1.8.m8.1.1" xref="S4.SS1.SSS2.Px2.p1.8.m8.1.1.cmml"><mi id="S4.SS1.SSS2.Px2.p1.8.m8.1.1.2" xref="S4.SS1.SSS2.Px2.p1.8.m8.1.1.2.cmml">A</mi><mo id="S4.SS1.SSS2.Px2.p1.8.m8.1.1.3" xref="S4.SS1.SSS2.Px2.p1.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.8.m8.1b"><apply id="S4.SS1.SSS2.Px2.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.Px2.p1.8.m8.1.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.8.m8.1.1">superscript</csymbol><ci id="S4.SS1.SSS2.Px2.p1.8.m8.1.1.2.cmml" xref="S4.SS1.SSS2.Px2.p1.8.m8.1.1.2">𝐴</ci><ci id="S4.SS1.SSS2.Px2.p1.8.m8.1.1.3.cmml" xref="S4.SS1.SSS2.Px2.p1.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.8.m8.1c">A^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.8.m8.1d">italic_A start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> (the answer to this new multi-hop question <math alttext="Q^{\prime}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.9.m9.1"><semantics id="S4.SS1.SSS2.Px2.p1.9.m9.1a"><msup id="S4.SS1.SSS2.Px2.p1.9.m9.1.1" xref="S4.SS1.SSS2.Px2.p1.9.m9.1.1.cmml"><mi id="S4.SS1.SSS2.Px2.p1.9.m9.1.1.2" xref="S4.SS1.SSS2.Px2.p1.9.m9.1.1.2.cmml">Q</mi><mo id="S4.SS1.SSS2.Px2.p1.9.m9.1.1.3" xref="S4.SS1.SSS2.Px2.p1.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.9.m9.1b"><apply id="S4.SS1.SSS2.Px2.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.Px2.p1.9.m9.1.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.9.m9.1.1">superscript</csymbol><ci id="S4.SS1.SSS2.Px2.p1.9.m9.1.1.2.cmml" xref="S4.SS1.SSS2.Px2.p1.9.m9.1.1.2">𝑄</ci><ci id="S4.SS1.SSS2.Px2.p1.9.m9.1.1.3.cmml" xref="S4.SS1.SSS2.Px2.p1.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.9.m9.1c">Q^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.9.m9.1d">italic_Q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>) matches <math alttext="A" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.10.m10.1"><semantics id="S4.SS1.SSS2.Px2.p1.10.m10.1a"><mi id="S4.SS1.SSS2.Px2.p1.10.m10.1.1" xref="S4.SS1.SSS2.Px2.p1.10.m10.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.10.m10.1b"><ci id="S4.SS1.SSS2.Px2.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.10.m10.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.10.m10.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.10.m10.1d">italic_A</annotation></semantics></math> (the original answer to <math alttext="Q" class="ltx_Math" display="inline" id="S4.SS1.SSS2.Px2.p1.11.m11.1"><semantics id="S4.SS1.SSS2.Px2.p1.11.m11.1a"><mi id="S4.SS1.SSS2.Px2.p1.11.m11.1.1" xref="S4.SS1.SSS2.Px2.p1.11.m11.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.Px2.p1.11.m11.1b"><ci id="S4.SS1.SSS2.Px2.p1.11.m11.1.1.cmml" xref="S4.SS1.SSS2.Px2.p1.11.m11.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.Px2.p1.11.m11.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.Px2.p1.11.m11.1d">italic_Q</annotation></semantics></math>) then we keep the example. We find that asking the model to reconstruct parts of the multi-hop question in-context
results in a more natural and cohesive question, thus removing some of the unnaturalness of the text that can occur from automatically generated and merged examples.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.F4.1" style="width:279.9pt;height:157.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-120.2pt,67.6pt) scale(0.538,0.538) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="562" id="S4.F4.1.g1" src="x4.png" width="1000"/>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S4.F4.6.1">
Source2Synth synthetic data generation process for
Tabular question answering.</span> The method first generates the seed, which is a fact based on the table (shown in blue). Given the seed and table, an <span class="ltx_text ltx_font_italic" id="S4.F4.7.2">SQL query</span> is then generated (in green) as well as its translation into natural language (the question
<span class="ltx_text ltx_font_italic" id="S4.F4.8.3">Q</span>). Then the SQL is executed on the table to obtain the answer <span class="ltx_text ltx_font_bold" id="S4.F4.9.4">A</span>.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Tabular question answering</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In Tabular question answering (TQA) we generate a question-answer dataset where each question is based on a given (real) table from the data source. Generated training examples are hence
enriched with tables and annotations which are built from automatically-generated interesting facts retrieved the table.</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data source selection</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">In the TQA case, we use unlabeled tables in the train split of the WikiSQL dataset <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib36" title="">2017</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Seed</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We then prompt an instruction-tuned language model to generate a statement based on the table. This statement corresponds to our seed topic for the generation and is a pertinent interesting fact or set of observations in natural language that can be derived from the table. The prompt is given in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#A3.F13" title="Figure 13 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 13</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Dataset Construction</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We next generate an SQL-statement by zero-shot prompting the LLM: we provide the table and the seed (factual statement) as context, see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#A3.F13" title="Figure 13 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 13</span></a> for the exact prompt. Given the produced SQL statement, it is then executed using the Python library <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p1.1.1">sqlite3<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote1.1.1.1">1</span></span><span class="ltx_text ltx_font_upright" id="footnote1.9">https://www.sqlite.org</span></span></span></span></span> to obtain an SQL answer formatted as a table. If the generated statement is invalid, we discard it and re-generate.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Dataset Curation</h4>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data filtering</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px1.p1.1">We check if the predicted answer of <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.Px1.p1.1.1">LLMSynth</span> fine-tuned on slice 0 matches the answer in the synthetically generated example, and if after <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.SSS2.Px1.p1.1.m1.1"><semantics id="S4.SS2.SSS2.Px1.p1.1.m1.1a"><mi id="S4.SS2.SSS2.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p1.1.m1.1b"><ci id="S4.SS2.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> tries the model has not supplied the correct answer we filter out the entry entirely.
See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#S4.F5" title="Figure 5 ‣ Data filtering ‣ 4.2.2 Dataset Curation ‣ 4.2 Tabular question answering ‣ 4 Applying Source2Synth to Special Cases ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 5</span></a> for an example of model inference.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F5.1" style="width:949.2pt;height:475.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(276.8pt,-138.6pt) scale(2.4,2.4) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="282" id="S4.F5.1.g1" src="x5.png" width="538"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S4.F5.3.1">Example <span class="ltx_text ltx_font_italic" id="S4.F5.3.1.1">Source2Synth</span> Response on TQA.</span> We show the model’s response (SQL call and final answer) to the tabular input question (yellow). The coloured parts highlight the generation of the corresponding augmented entries: SQL in green, and the final answer in red.

</figcaption>
</figure>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We test our method on two domains: <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">tabular question answering</span> and <span class="ltx_text ltx_font_italic" id="S5.p1.1.2">multi-hop question answering</span>.
For each, we use <span class="ltx_text ltx_font_italic" id="S5.p1.1.3">Source2Synth</span> to generate and curate a high quality dataset suitable for fine-tuning, and compare our method to a number of baselines.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Multi-Hop QA Setup</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">To evaluate multi-hop question answering abilities, we evaluate our <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p1.1.1">Source2Synth</span> method on HotPotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib35" title="">2018</a>)</cite>. HotPotQA is a benchmark based on Wikipedia containing 113,000 examples of multi-hop question-answer pairs and is split in train, test, and validation sets.
Each entry in HotPotQA is constructed such that:
1) each question requires finding and reasoning over multiple supporting documents in order to answer; 2) each entry provides sentence-level supporting facts for strong supervision and explainability of the prediction; 3) each question can be classified as either a comparison or bridge question.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p2.2">A comparison question entails comparing the same concept between <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p2.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p2.1.m1.1a"><mi id="S5.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p2.1.m1.1d">italic_n</annotation></semantics></math> objects (e.g. "Who is the tallest student in class?"), while a bridge question builds on a logical and/or causal link and requires deriving one statement to get to the answer (e.g. "What is the height of the student that topped the entry exam?" - this requires first identifying the student that topped the exam. The hop length is the number of comparison objects for comparison questions or the number of links for bridge questions. In our case, we chose <math alttext="n=2" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p2.2.m2.1"><semantics id="S5.SS1.SSS0.Px1.p2.2.m2.1a"><mrow id="S5.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.2" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.2.cmml">n</mi><mo id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.1" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.3" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.2.m2.1b"><apply id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1"><eq id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.1"></eq><ci id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.2">𝑛</ci><cn id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.2.m2.1c">n=2</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p2.2.m2.1d">italic_n = 2</annotation></semantics></math> to be consistent with HotPotQA. The test set consists of 7,405 entries, split evenly between bridge and comparison questions. We only generate synthetic data for bridge questions, since they pose a bigger challenge to current LLMs. In order to counterbalance this disparity, we include 500 comparison questions from HotPotQA’s training dataset in our fine-tuning dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Metrics</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">We measure the performance using soft exact match (soft-EM) as the metric. Soft-EM is 1 if the generated output contains the golden answer and 0 otherwise.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Model</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">The base model used in the experiments in MHQA is Llama-2 70B-Chat. We hence fine-tune <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px3.p1.1.1">Source2Synth</span> and various other baseline methods initializing from this model.
<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px3.p1.1.2">Source2Synth</span> is trained with 1250 synthetic examples, unless noted otherwise, in addition to the 500 HotPotQA examples described above.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Baselines</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px4.p1.1">We compare <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px4.p1.1.1">Source2Synth</span> to the following baselines:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Instruction-tuned LLM: using LLama 2 70B-Chat for the task in a zero-shot manner.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Fine-tuned LLM (HotPotQA only): fine-tuning from the base model on 500 HPQA examples from the training split.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i3.p1.1.1">LLMSynth</span> (Synthetic dataset only): training our model with 1250 synthetic examples from Slice 1 (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>), <span class="ltx_text ltx_font_italic" id="S5.I1.i3.p1.1.2">without</span> the data curation step.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I1.i4.p1.1.1">LLMSynth</span> (Synthetic and HotPotQA): training with the uncurated synthetic data in addition to the 500 HPQA examples.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px4.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px4.p2.1">For all the models listed, we tested them using two prompting methods: a zero-shot and a three-shot CoT prompt, see the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#A3" title="Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">C</span></a> for details.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Tabular QA Setup</h3>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">We conduct evaluations with the WikiSQL <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib36" title="">2017</a>)</cite> dataset validation split. The WikiSQL dataset consists of a corpus of 80,654 hand-annotated examples of natural language questions, SQL queries, and SQL tables created from 24,241 tables extracted from Wikipedia. The validation split contains 7,857 examples after removing non-executable SQL tables, see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#A2" title="Appendix B SQL non-executable code filtering ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">B</span></a> for more details.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Metrics</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">We measure performance using the exact match (EM) and the soft-EM metrics. The EM metric equals 1 if the golden answer is equal to the generated answer and 0 otherwise.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Model</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1">For TQA, we use the Starchat-beta language model <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib11" title="">2023b</a>)</cite> from Huggingface as the initial language model (batch size 32, 100 steps, lr 0.0001, linear warm-up). The Starchat model is an instruction-tuned LLM with 16 billion parameters trained to act as a helpful coding assistant. This model is a fine-tuned version of StarCoder <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#bib.bib11" title="">2023b</a>)</cite>, a LLM which was pre-trained and then fine-tuned on a large code corpus, which contains SQL statements, and successively fine-tuned on 35B Python tokens.
For our <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px3.p1.1.1">Source2Synth</span> generated data, the initial number of synthetic examples per slice is 8k (so 16k in total). After curation, we keep 2160 of the examples in slice 2 (27%).</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Baselines</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p1.1">We compare the performance of our <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px4.p1.1.1">Source2Synth</span> method against a variety of baselines. The baselines consist of prompting the Starchat-beta instruction-tuned language model as follows:</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I2.i1.p1.1.1">Zero-shot Table QA</span>: prompt with the task instruction, the table and the question in a zero-shot fashion. See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#A3.F7" title="Figure 7 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 7</span></a> for the prompt.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I2.i2.p1.1.1">One-Shot No Context QA</span>: prompt with the task instruction and a one-shot example containing a question and answer, together with the actual question for the model to answer. See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#A3.F8" title="Figure 8 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 8</span></a> for the prompt.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I2.i3.p1.1.1">One-Shot Table QA</span>: prompt that includes the table for both the one-shot example and the question to be answered. We use one-shot due to LLM context length and the typically large size of the tables.
See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#A3.F9" title="Figure 9 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 9</span></a> for the prompt.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I2.i4.p1.1.1">One-shot Table+SQL QA</span>: the prompt includes an example containing the table and question, and an instruction suggesting that the model can leverage an SQL tool. We then execute the predicted SQL to obtain the answer.
See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#A3.F10" title="Figure 10 ‣ Appendix C Prompts used in our experiments ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Figure 10</span></a> for the prompt.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i5.p1">
<p class="ltx_p" id="S5.I2.i5.p1.1"><span class="ltx_text ltx_font_italic" id="S5.I2.i5.p1.1.1">LLMSynth</span>: Finetune the model with synthetic data but <span class="ltx_text ltx_font_italic" id="S5.I2.i5.p1.1.2">without</span> applying the data curation step.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Multi-Hop question answering</h3>
<figure class="ltx_table" id="S6.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T1.1.1.1.2.1">0-shot</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T1.1.1.1.3.1">3-shot CoT prompt</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T1.1.2.1.1">Instruction-tuned LLM (LLama 2 70B-Chat)</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S6.T1.1.2.1.2">40.45%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T1.1.2.1.3">44.13%</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T1.1.3.2.1">fine-tuned LLM (HotPotQA only)</th>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T1.1.3.2.2">53.22%</td>
<td class="ltx_td ltx_align_right" id="S6.T1.1.3.2.3">58.40%</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T1.1.4.3.1">
<span class="ltx_text ltx_font_italic" id="S6.T1.1.4.3.1.1">LLMSynth</span> (Synthetic dataset only)</th>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T1.1.4.3.2">52.31%</td>
<td class="ltx_td ltx_align_right" id="S6.T1.1.4.3.3">56.70%</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T1.1.5.4.1">
<span class="ltx_text ltx_font_italic" id="S6.T1.1.5.4.1.1">LLMSynth</span> (Synthetic and HotPotQA)</th>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T1.1.5.4.2">57.46%</td>
<td class="ltx_td ltx_align_right" id="S6.T1.1.5.4.3">62.73%</td>
</tr>
<tr class="ltx_tr" id="S6.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T1.1.6.5.1">
<span class="ltx_text ltx_font_italic" id="S6.T1.1.6.5.1.1">LLMCurated</span> (Synthetic and HotPotQA)</th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S6.T1.1.6.5.2">65.23%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T1.1.6.5.3">66.05%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S6.T1.5.1">Evaluation of <span class="ltx_text ltx_font_italic" id="S6.T1.5.1.1">Source2Synth</span> on Multi-hop question answering.</span> The models shown are fine-tuned with 500 entries from HotPotQA (‘HotPotQA”) and/or 1250 entries from the <span class="ltx_text ltx_font_italic" id="S6.T1.6.2">Source2Synth</span> Synthetic Dataset (“Synthetic Dataset”). Using <span class="ltx_text ltx_font_italic" id="S6.T1.7.3">Source2Synth</span> curated synthetic data in combination with HotPotQA (last row) works best.
</figcaption>
</figure>
<figure class="ltx_table" id="S6.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S6.T2.1.1.1.2"><span class="ltx_text ltx_font_italic" id="S6.T2.1.1.1.2.1">Bridge</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S6.T2.1.1.1.3"><span class="ltx_text ltx_font_italic" id="S6.T2.1.1.1.3.1">Comparison</span></th>
</tr>
<tr class="ltx_tr" id="S6.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S6.T2.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.2.2.1.1">Model</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S6.T2.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.2.2.2.1">Hard</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S6.T2.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S6.T2.1.2.2.3.1">Medium</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S6.T2.1.2.2.4">
<span class="ltx_text ltx_font_bold" id="S6.T2.1.2.2.4.1">Easy</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column" id="S6.T2.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S6.T2.1.2.2.5.1">Hard</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S6.T2.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S6.T2.1.2.2.6.1">Medium</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S6.T2.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S6.T2.1.2.2.7.1">Easy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.1.3.1.1">Llama2-70B-Chat</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.3.1.2">5.3%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.3.1.3">11.3%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.3.1.4">27.1%</td>
<td class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" id="S6.T2.1.3.1.5">13.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.3.1.6">17.7%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.1.3.1.7">26.4%</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.1.4.2.1">fine-tuned LLM (HotPotQA only)</th>
<td class="ltx_td ltx_align_right" id="S6.T2.1.4.2.2">14.6%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.4.2.3">16.9%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.4.2.4">39.0%</td>
<td class="ltx_td ltx_nopad_l ltx_align_right" id="S6.T2.1.4.2.5">23.5%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.4.2.6">31.4%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.1.4.2.7">35.6%</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T2.1.5.3.1">LLMCurated-1250</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.5.3.2">25.3%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.5.3.3">30.7%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.5.3.4">41.8%</td>
<td class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" id="S6.T2.1.5.3.5">23.3%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.5.3.6">32.3%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.1.5.3.7">36.9%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S6.T2.4.1">Analysis of MHQA bridge and comparison questions with respect to level of difficulty.</span> We evaluate models on 1k entries for each question type. <span class="ltx_text ltx_font_italic" id="S6.T2.5.2">Source2Synth</span> was used to generated bridge question type data, hence
LLM-Curated-1250 outperforms other models particularly for bridge questions.
</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.F6.1" style="width:420.6pt;height:428.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.5pt,6.6pt) scale(0.97,0.97) ;">
<div class="ltx_inline-block ltx_transformed_outer" id="S6.F6.1.1" style="width:433.6pt;height:440.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(73.1pt,-74.3pt) scale(1.50900751165831,1.50900751165831) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="560" id="S6.F6.1.1.g1" src="x6.png" width="544"/>
</span></div>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S6.F6.6.1">Synthetic Data scaling performance.</span> We show how the performance of
<span class="ltx_text ltx_font_italic" id="S6.F6.7.2">Source2Synth</span> changes with respect to MHQA data mix size, both before and after curation.
During the curation step, the following percentages of samples were removed: 7% for 500, 8% for 750, 11% for 1250. <span class="ltx_text ltx_font_italic" id="S6.F6.8.3">LLMSynth</span> (before curation) performs worse than <span class="ltx_text ltx_font_italic" id="S6.F6.9.4">LLMCurated</span> (after curation) despite having more samples – but both approaches improve with more data.

</figcaption>
</figure>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Overall performance of <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.1.1">Source2Synth</span> on MHQA</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p1.1">We report the experimental results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S6.T1" title="Table 1 ‣ 6.1 Multi-Hop question answering ‣ 6 Results ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">1</span></a>. We include the baselines
of the vanilla instruction-tuned LLM, a
<span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p1.1.1">fine-tuned LLM</span> using only the HPQA 500 examples from the train split (second row), and  <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p1.1.2">LLMSynth</span> which only uses the uncurated synthetic data for fine-tuning (third row).
All fine-tuned methods outperform the instruction-tuned model (first row).
Using only synthetic data or only HotPotQA data for fine-tuning demonstrates worse performance than when combined, whether the synthetic data is curated (fifth row) or not as in  <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p1.1.3">LLMSynth</span> (fourth row). Once we use the full <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p1.1.4">Source2Synth</span> pipeline to obtain the curated synthetic dataset for fine-tuning we see further performance improvements <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p1.1.5">LLMCurated</span> (fifth row) over not curating the data (fourth row).</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS0.Px1.p2.1.1">Analysis of performance on different question types and levels of difficulty</span>
We study the capabilities of our model by analysing the performance of LLM-Curated-1250 with particular focus on the type and difficulty of the questions – namely hard/medium/easy bridge and comparison questions.
We compare the performance of the base model, the model fine-tuned on HotPotQA, and  <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p2.1.2">Source2Synth</span> according to the difficulty level, as provided by the HotPotQA dataset. We also subdivide the results according to the type of question (bridge vs. comparison).
Results are given in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.08239v1#S6.T2" title="Table 2 ‣ 6.1 Multi-Hop question answering ‣ 6 Results ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p3.1">In almost all cases, we observe that <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p3.1.1">Source2Synth</span> performs better, particularly on bridge questions, with an overall gain of 17.44% compared to the base model and a 14.56% gain compared to the LLM fine-tuned on HotPotQA. For the comparison questions, we see modest gains because our synthetic dataset only includes the harder task type (bridge). However, it is interesting to see some (small) improvement despite not explicitly targeting comparison-type questions in our dataset. Hence, <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px1.p3.1.2">Source2Synth</span> tackled a more demanding task (bridge vs. comparison), and achieves 25.3% on hard-bridge questions compared to 14.6% of the Instruction-tuned LLM fine-tuned on HotPotQA.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S6.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.2.1">Exact Match</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.3.1">     Soft-EM</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.1.2.1.1">One-Shot No Context QA (Starchat-beta LLM)</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S6.T3.1.2.1.2">0.25%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T3.1.2.1.3">16.22%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T3.1.3.2.1">Zero-shot Table QA (Starchat-beta LLM)</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.3.2.2">1.83%</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.3.2.3">20.07%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T3.1.4.3.1">One-Shot Table QA (Starchat-beta LLM)</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.4.3.2">2.03%</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.4.3.3">31.06%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T3.1.5.4.1">One-shot Table+SQL QA (Starchat-beta LLM)</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.5.4.2">12.30%</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.5.4.3">34.13%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T3.1.6.5.1">
<span class="ltx_text ltx_font_italic" id="S6.T3.1.6.5.1.1">LLMSynth</span> (Synthetic dataset only)</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.6.5.2">23.86%</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.6.5.3">34.21%</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S6.T3.1.7.6.1">
<span class="ltx_text ltx_font_italic" id="S6.T3.1.7.6.1.1">LLMCurated</span> (Synthetic dataset only)</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S6.T3.1.7.6.2">34.50%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.7.6.3">42.80%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S6.T3.3.1">Tabular question answering.</span> Performance comparison on the WikiSQL evaluation dataset.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Scaling performance</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px2.p1.1">We also report scaling performance
in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S6.F6" title="Figure 6 ‣ 6.1 Multi-Hop question answering ‣ 6 Results ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">6</span></a>. We study how performance evolves when adding more synthetic data in the fine-tuning data mix - that already includes 500 samples from the HPQA train split. We perform the analysis on <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px2.p1.1.1">LLMSynth</span> and <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px2.p1.1.2">LLMCurated</span> to show the impact of the curation technique.
In both cases and in all data mixes, we see that applying the <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px2.p1.1.3">Source2Synth</span> pipeline results in a stronger model on the task.
For the <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px2.p1.1.4">LLMSynth</span> model fine-tuned on uncurated samples we see that providing more synthetic examples leads to a steady improvement in performance across all data sizes, for both zero-shot and three-shot prompting variants.
<span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px2.p1.1.5">LLMCurated</span> follows a similar trend, but consistently outperforms the uncurated version of the model, for all training set sizes.
Overall, we observe that using our synthetic data generation pipeline to construct more data brings further performance gains in the task.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Tabular question answering</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We report the experimental results for Tabular question answering in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08239v1#S6.T3" title="Table 3 ‣ Overall performance of Source2Synth on MHQA ‣ 6.1 Multi-Hop question answering ‣ 6 Results ‣ Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources"><span class="ltx_text ltx_ref_tag">3</span></a>.
Firstly, they indicate that providing no context about the table when prompting the instruction-tuned StarChat language model has very poor performance (first row), with an EM metric of 0.25%. This is expected since the WikiSQL benchmark questions require information contained in the table, and the model does not have any other information to answer the question except for the general knowledge stored in its parameters. However, even if we pass the table as part of the prompt, the performance does not improve much. For example, passing in a zero-shot fashion (second row) only has an EM metric of 1.83%. This may be challenging for the model as the information in the table is presented in a form that is not easy for the LLM to naturally handle (i.e. a table rather than natural language). While passing an example of table usage in a one-shot fashion (third row) improves the soft-EM metric, the EM metric is still very low (2.03%). Hence, this is still very challenging for the model.
Thirdly, the performance increases once we provide a one-shot example containing the relevant table and SQL query (fourth row), with an EM of 12.3%. The ability to use the SQL tool improves the performance markedly.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">We obtain a significant increase in performance when we fine-tune the StarChat model using the <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.1">Source2Synth</span> curated data (last row), with an EM of 34.5%. Our full method performs significantly better than fine-tuning the StarChat language model using synthetic data without curation, <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.2">LLMSynth</span> (second to last row) which has an EM of 23.86%, although that still outperforms the other baselines by a large margin as well, indicating the utility of our <span class="ltx_text ltx_font_italic" id="S6.SS2.p2.1.3">Source2Synth</span> synthetic data generation scheme.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we introduce <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">Source2Synth</span>, a new method for generating and curating high-quality synthetic data grounded in real data sources. We demonstrate its utility on two tasks that pose significant challenges for LLMs: multi-hop reasoning and tabular question answering with SQL. We believe our work could also be beneficial in other low-data regimes, and future work could explore our approach on other tasks and in diverse fields, for example, in to biology, chemistry and medicine.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this paper, our applications use a single seed to derive two-hop questions in the case of MHQA or SQL on a single table in TQA.
However, <span class="ltx_text ltx_font_italic" id="S8.p1.1.1">Source2Synth</span> can be extended to more complex questions e.g. with more hops,
and even more complex tool-use, e.g. the use of multiple tables. This could be done
by looping the dataset generation steps and feeding the result of the previous step as input to the next one.
Our approach provides a way to sample the related articles graph and corresponding entities within the articles based on simple rejection sampling but we believe that our method could be improved with more clever sampling techniques. We consider this to be an interesting avenue of future research.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2005.14165</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2024)</span>
<span class="ltx_bibblock">
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.17126" title="">Large language models as tool makers</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Preprint</em>, arXiv:2305.17126.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2204.02311" title="">Palm: Scaling language modeling with pathways</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Preprint</em>, arXiv:2204.02311.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1810.04805" title="">Bert: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Preprint</em>, arXiv:1810.04805.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier
Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu,
Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James
Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini
Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,
Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2407.21783" title="">The llama 3 herd of models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Preprint</em>, arXiv:2407.21783.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2211.10435" title="">Pal: Program-aided language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Preprint</em>, arXiv:2211.10435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemmell and Dalton (2023)</span>
<span class="ltx_bibblock">
Carlos Gemmell and Jeffrey Dalton. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.10138" title="">Generate, transform, answer: Question specific tool synthesis for tabular data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Preprint</em>, arXiv:2303.10138.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herzig et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Eisenschlos. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.398" title="">TaPas: Weakly supervised table parsing via pre-training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4320–4333, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2019)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank F. Xu, J. Araki, and Graham Neubig. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:208513249" title="">How can we know what language models know?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Transactions of the Association for Computational Linguistics</em>, 8:423–438.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.03111" title="">Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Preprint</em>, arXiv:2305.03111.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von
Werra, and Harm de Vries. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.06161" title="">Starcoder: may the source be with you!</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Preprint</em>, arXiv:2305.06161.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2308.06259" title="">Self-alignment with instruction backtranslation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Preprint</em>, arXiv:2308.06259.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-emnlp.508" title="">WANLI: Worker and AI collaboration for natural language inference dataset creation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 6826–6847, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, and Andrew M. Dai. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2404.07503" title="">Best practices and lessons learned on synthetic data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Preprint</em>, arXiv:2404.07503.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mekala et al. (2024)</span>
<span class="ltx_bibblock">
Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, and Jane Dwivedi-Yu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.14158" title="">Toolverifier: Generalization to new tools via self-verification</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Preprint</em>, arXiv:2402.14158.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mialon et al. (2023)</span>
<span class="ltx_bibblock">
Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=jh7wH2AzKK" title="">Augmented language models: a survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Transactions on Machine Learning Research</em>.

</span>
<span class="ltx_bibblock">Survey Certification.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2024)</span>
<span class="ltx_bibblock">
Thao Nguyen, Jeffrey Li, Sewoong Oh, Ludwig Schmidt, Jason Weston, Luke Zettlemoyer, and Xian Li. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2408.04614" title="">Better alignment with instruction back-and-forth translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Preprint</em>, arXiv:2408.04614.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paranjape et al. (2023)</span>
<span class="ltx_bibblock">
Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.09014" title="">Art: Automatic multi-step reasoning and tool-use for large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Preprint</em>, arXiv:2303.09014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parisi et al. (2022)</span>
<span class="ltx_bibblock">
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2205.12255" title="">Talm: Tool augmented language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Preprint</em>, arXiv:2205.12255.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat and Liang (2015)</span>
<span class="ltx_bibblock">
Panupong Pasupat and Percy Liang. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P15-1142" title="">Compositional semantic parsing on semi-structured tables</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 1470–1480, Beijing, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2019)</span>
<span class="ltx_bibblock">
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:202539551" title="">Language models as knowledge bases?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023)</span>
<span class="ltx_bibblock">
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.08354" title="">Tool learning with foundation models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Preprint</em>, arXiv:2304.08354.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding with unsupervised learning.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Yacmpz84TH" title="">Toolformer: Language models can teach themselves to use tools</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick and Schütze (2020)</span>
<span class="ltx_bibblock">
Timo Schick and Hinrich Schütze. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:238260199" title="">Few-shot text generation with natural language instructions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick and Schütze (2021)</span>
<span class="ltx_bibblock">
Timo Schick and Hinrich Schütze. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:233241169" title="">Generating datasets with pretrained language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ArXiv</em>, abs/2104.07540.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick and Schütze (2021)</span>
<span class="ltx_bibblock">
Timo Schick and Hinrich Schütze. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2001.07676" title="">Exploiting cloze questions for few shot text classification and natural language inference</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Preprint</em>, arXiv:2001.07676.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:259108190" title="">Toolalpaca: Generalized tool learning for language models with 3000 simulated cases</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">ArXiv</em>, abs/2306.05301.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et al. (2022)</span>
<span class="ltx_bibblock">
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2201.08239" title="">Lamda: Language models for dialog applications</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Preprint</em>, arXiv:2201.08239.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Preprint</em>, arXiv:2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:13756489" title="">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2201.11903" title="">Chain of thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, abs/2201.11903.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2024)</span>
<span class="ltx_bibblock">
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2312.02120" title="">Magicoder: Empowering code generation with oss-instruct</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Preprint</em>, arXiv:2312.02120.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wikipedia contributors (2004)</span>
<span class="ltx_bibblock">
Wikipedia contributors. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://en.wikipedia.org/w/index.php?title=Plagiarism&amp;oldid=5139350" title="">Plagiarism — Wikipedia, the free encyclopedia</a>.

</span>
<span class="ltx_bibblock">[Online; accessed 22-July-2004].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:52822214" title="">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2017)</span>
<span class="ltx_bibblock">
Victor Zhong, Caiming Xiong, and Richard Socher. 2017.

</span>
<span class="ltx_bibblock">Seq2sql: Generating structured queries from natural language using reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">CoRR</em>, abs/1709.00103.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>More Results on Prompt Engineering</h2>
<figure class="ltx_table" id="A1.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Prompt Type</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">Model Accuracy (soft-EM)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.2.1.1">0-shot</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T4.1.2.1.2">40.45%</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.3.2.1">Role</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.3.2.2">22.34%</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.4.3.1">1-shot</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.4.3.2">26.65%</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.5.4.1">Few-shots (5-shots)</th>
<td class="ltx_td ltx_align_right" id="A1.T4.1.5.4.2">21.83%</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.1">Role (1-shot)</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T4.1.6.5.2">28.29%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="A1.T4.2.1">MHQA prompts sweep.</span> Overview of the model’s accuracy across different prompt strategies. <span class="ltx_text ltx_font_italic" id="A1.T4.3.2">Role</span> "You are a QA-robot. Answer the following question:". Model used: Llama-2-70B-Chat, Dataset: HotPotQA test.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>SQL non-executable code filtering</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We discard incorrect SQL statements - i.e. whose execution with <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">sqlite3<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote2.1.1.1">2</span></span><span class="ltx_text ltx_font_upright" id="footnote2.9">https://www.sqlite.org</span></span></span></span></span> leads to an error. Discarded proportion: out of 50 tables, we generate 800 seed statements and the number of valid (executable) SQL statements was 658.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Prompts used in our experiments</h2>
<figure class="ltx_figure" id="A3.F7"><svg class="ltx_picture" height="114.21" id="A3.F7.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,114.21) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 108.3 C 0 111.56 2.64 114.21 5.91 114.21 L 594.09 114.21 C 597.36 114.21 600 111.56 600 108.3 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 97.97 L 598.03 97.97 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 99.94)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F7.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F7.pic1.1.1.1.1.1.1">Zero-shot Table QA prompt.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="80.25" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F7.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F7.pic1.2.2.2.1.1.1">Answer the following question using the table below. You may leverage an SQL tool.</span>
<span class="ltx_p" id="A3.F7.pic1.2.2.2.1.1.2">{table}</span>
<span class="ltx_p" id="A3.F7.pic1.2.2.2.1.1.3">Q: {question}</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Zero-shot Table QA prompt for the TQA task.

</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F8"><svg class="ltx_picture" height="130.81" id="A3.F8.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,130.81) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 124.91 C 0 128.17 2.64 130.81 5.91 130.81 L 594.09 130.81 C 597.36 130.81 600 128.17 600 124.91 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 114.58 L 598.03 114.58 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 116.54)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F8.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F8.pic1.1.1.1.1.1.1">One-Shot No context QA prompt.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="96.86" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F8.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F8.pic1.2.2.2.1.1.1">– Example –</span>
<span class="ltx_p" id="A3.F8.pic1.2.2.2.1.1.2">Q: What was the last year where this team was part of the US A-league?</span>
<span class="ltx_p" id="A3.F8.pic1.2.2.2.1.1.3">A: 2004</span>
<span class="ltx_p" id="A3.F8.pic1.2.2.2.1.1.4">Now do the same for the following question.</span>
<span class="ltx_p" id="A3.F8.pic1.2.2.2.1.1.5">Q: {question}</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>One-Shot No context QA prompt for the TQA task.

</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F9"><svg class="ltx_picture" height="313.46" id="A3.F9.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,313.46) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 307.55 C 0 310.82 2.64 313.46 5.91 313.46 L 594.09 313.46 C 597.36 313.46 600 310.82 600 307.55 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 297.22 L 598.03 297.22 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 299.19)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F9.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F9.pic1.1.1.1.1.1.1">One-shot Table QA prompt.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="279.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F9.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F9.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A3.F9.pic1.2.2.2.1.1.1.1">--</span> Example <span class="ltx_text ltx_font_typewriter" id="A3.F9.pic1.2.2.2.1.1.1.2">--
<br class="ltx_break"/></span>Answer the following question using the table below.</span>
<span class="ltx_p" id="A3.F9.pic1.2.2.2.1.1.2">Your answer should be short and concise.</span><pre class="ltx_verbatim ltx_font_typewriter" id="A3.F9.pic1.2.2.2.1.1.3">
Season | Team        | League_apps | Goals
1923   |Swindon Town | 55          | 3
1922   |Swindon Town | 14          | 4
1921   |Swindon Town | 24          | 11
1920   |Swindon Town | 26          | 16
1919   |Swindon Town | 20          | 10
1914   |Swindon Town | 23          | 12
1913   |Swindon Town | 24          | 18
1912   |Swindon Town | 12          | 9
1911   |Swindon Town | 20          | 16
1910   |Swindon Town | 30          | 19
1909   |Swindon Town | 33          | 19
1908   |Swindon Town | 34          | 28
1907   |Swindon Town | 30          | 17
</pre>
<span class="ltx_p" id="A3.F9.pic1.2.2.2.1.1.4">Q: How many league appearances were there between 1907 and 1909 (inclusive)?</span>
<span class="ltx_p" id="A3.F9.pic1.2.2.2.1.1.5">A: 97</span>
<span class="ltx_p" id="A3.F9.pic1.2.2.2.1.1.6">Now do the same for the following table and question.</span>
<span class="ltx_p" id="A3.F9.pic1.2.2.2.1.1.7">{table}</span>
<span class="ltx_p" id="A3.F9.pic1.2.2.2.1.1.8">Q: {question}</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>One-shot Table QA prompt for the TQA task.

</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F10"><svg class="ltx_picture" height="280.25" id="A3.F10.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,280.25) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 274.35 C 0 277.61 2.64 280.25 5.91 280.25 L 594.09 280.25 C 597.36 280.25 600 277.61 600 274.35 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 264.02 L 598.03 264.02 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 265.98)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F10.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F10.pic1.1.1.1.1.1.1">One-shot Table+SQL QA prompt.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="246.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F10.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F10.pic1.2.2.2.1.1.1"><span class="ltx_text ltx_font_typewriter" id="A3.F10.pic1.2.2.2.1.1.1.1">--</span> Example <span class="ltx_text ltx_font_typewriter" id="A3.F10.pic1.2.2.2.1.1.1.2">--
<br class="ltx_break"/></span>Answer the following question using the table below.</span>
<span class="ltx_p" id="A3.F10.pic1.2.2.2.1.1.2">You may leverage an SQL tool.</span>
<span class="ltx_p" id="A3.F10.pic1.2.2.2.1.1.3">The table is stored in a variable ‘sql_table’ and has the following schema:</span><pre class="ltx_verbatim ltx_font_typewriter" id="A3.F10.pic1.2.2.2.1.1.4">
Season | Team        | League_apps | Goals
1923   |Swindon Town | 55          | 3
1922   |Swindon Town | 14          | 4
</pre>
<span class="ltx_p" id="A3.F10.pic1.2.2.2.1.1.5">Q: How many league appearances were there between 1907 and 1909 (inclusive)?</span><pre class="ltx_verbatim ltx_font_typewriter" id="A3.F10.pic1.2.2.2.1.1.6">
SQL: SELECT SUM(League_apps) FROM sql_table WHERE Season BETWEEN 1907 AND 1909
</pre>
<pre class="ltx_verbatim ltx_font_typewriter" id="A3.F10.pic1.2.2.2.1.1.7">
       | Result
result | 97
</pre>
<span class="ltx_p" id="A3.F10.pic1.2.2.2.1.1.8">Now do the same for the following table and question.</span>
<span class="ltx_p" id="A3.F10.pic1.2.2.2.1.1.9">{table}</span>
<span class="ltx_p" id="A3.F10.pic1.2.2.2.1.1.10">Q: {question}</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>One-shot Table+SQL QA prompt for the TQA task.

</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_figure_panel" height="112.67" id="A3.F13.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,112.67) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 106.76 C 0 110.03 2.64 112.67 5.91 112.67 L 594.09 112.67 C 597.36 112.67 600 110.03 600 106.76 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 96.43 L 598.03 96.43 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 98.4)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F13.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F13.pic1.1.1.1.1.1.1">Generating a seed in TQA.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F13.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F13.pic1.2.2.2.1.1.1">Please generate an interesting statement about this table. The statement is a fact about one of the columns in the following table.</span>
<span class="ltx_p" id="A3.F13.pic1.2.2.2.1.1.2">{table}</span>
<span class="ltx_p" id="A3.F13.pic1.2.2.2.1.1.3">An interesting statement as a result of this is:</span>
</span></foreignobject></g></g></svg></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Prompt used to induce a pertinent and interesting seed topic in TQA. This is done zero-shot.

</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_figure_panel" height="145.88" id="A3.F13.pic2" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,145.88) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 139.97 C 0 143.24 2.64 145.88 5.91 145.88 L 594.09 145.88 C 597.36 145.88 600 143.24 600 139.97 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 129.64 L 598.03 129.64 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 131.61)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F13.pic2.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F13.pic2.1.1.1.1.1.1">Generating meaningful SQL in TQA.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="111.93" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F13.pic2.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F13.pic2.2.2.2.1.1.1">Please generate SQL statements for the following table:</span>
<span class="ltx_p" id="A3.F13.pic2.2.2.2.1.1.2">{table}</span>
<span class="ltx_p" id="A3.F13.pic2.2.2.2.1.1.3">Seed: {seed}</span>
<span class="ltx_p" id="A3.F13.pic2.2.2.2.1.1.4">An interesting SQL statement as a result of this is</span>
</span></foreignobject></g></g></svg></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Prompt used to induce a meaningful SQL statement given the table and seed for the TQA task. This is done zero-shot.

</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><svg class="ltx_picture ltx_figure_panel" height="145.73" id="A3.F13.pic3" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,145.73) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 139.82 C 0 143.08 2.64 145.73 5.91 145.73 L 594.09 145.73 C 597.36 145.73 600 143.08 600 139.82 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 129.64 L 598.03 129.64 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 131.61)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F13.pic3.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F13.pic3.1.1.1.1.1.1">Generating a question in TQA.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="111.93" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F13.pic3.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F13.pic3.2.2.2.1.1.1">I want to convert an SQL statement into a question.</span>
<span class="ltx_p" id="A3.F13.pic3.2.2.2.1.1.2">Here is the original table:</span>
<span class="ltx_p" id="A3.F13.pic3.2.2.2.1.1.3">{table}</span>
<span class="ltx_p" id="A3.F13.pic3.2.2.2.1.1.4">SQL: {SQL}</span>
<span class="ltx_p" id="A3.F13.pic3.2.2.2.1.1.5">What is the question that this SQL statement would be the answer to?</span>
</span></foreignobject></g></g></svg></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Prompt used to induce a meaningful question using the table and generated SQL query for the TQA task. This is done zero-shot.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F14"><svg class="ltx_picture" height="1027.45" id="A3.F14.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1027.45) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 1021.54 C 0 1024.81 2.64 1027.45 5.91 1027.45 L 594.09 1027.45 C 597.36 1027.45 600 1024.81 600 1021.54 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 1011.21 L 598.03 1011.21 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 1013.18)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F14.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F14.pic1.1.1.1.1.1.1">Three-shot CoT prompt used at evaluation time on MHQA.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="993.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F14.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.1">Answer the following multi-hop question ‘Q’ by decomposing it into ‘Q1’ and ‘Q2’ and solving them step-by-step. Learn from the following 3 examples. As shown in the following example:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.2"><span class="ltx_text ltx_font_typewriter" id="A3.F14.pic1.2.2.2.1.1.2.1">--</span> Example #1 <span class="ltx_text ltx_font_typewriter" id="A3.F14.pic1.2.2.2.1.1.2.2">--
<br class="ltx_break"/></span>‘Q’ = ‘Who was the commander of the spaceflight that first landed humans on the Moon?’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.3">1. Splitting ‘Q’ into ‘Q1’ and ‘Q2’:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.4">‘Q1’ : ‘What was the spaceflight that first landed humans on the Moon?’;</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.5">‘Q2’ : ‘Who was the commander of [A1]?’;</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.6">2. Answering Q1:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.7">The answer ‘A1’ to ‘Q1’ : ‘What was the spaceflight that first landed humans on the Moon?’ is ‘Apollo
11’. ‘A1’ = ‘Apollo 11’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.8">3. Substituting A1 to Q2:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.9">‘Q2’ : ‘Who was the commander of Apollo 11?’,</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.10">4. Answers Q2:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.11">The answer ‘A2’ to Q2’ : ‘Who was the commander of Apollo 11?’ is ‘Neil Armstrong’.</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.12">‘A2’ = ‘A’ = ‘Neil Armstrong’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.13"><span class="ltx_text ltx_font_typewriter" id="A3.F14.pic1.2.2.2.1.1.13.1">--</span> Example #2 <span class="ltx_text ltx_font_typewriter" id="A3.F14.pic1.2.2.2.1.1.13.2">--
<br class="ltx_break"/></span>‘Q’ = ‘What is the main ingredient in the flagship product of Ferrero?’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.14">1. Splitting ‘Q’ into ‘Q1’ and ‘Q2’:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.15">‘Q1’: ‘What is the flagship product of Ferrero?’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.16">‘Q2’: ‘What is the main ingredient in [A1]?’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.17">2. Answering Q1:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.18">The answer ‘A1’ to ‘Q1’ : ‘What is the flagship product of Ferrero?’ is Nutella’.‘A1’ = Nutella’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.19">3. Substituting A1 to Q2:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.20">‘Q2’ : ‘What is the main ingredient in Nutella?’,</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.21">4. Answers Q2:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.22">The answer ‘A2’ to Q2’ : ‘What is the main ingredient in Nutella?’.</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.23">‘A2’ = ‘A’ = ‘Hazelnuts</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.24"><span class="ltx_text ltx_font_typewriter" id="A3.F14.pic1.2.2.2.1.1.24.1">--</span>Example #3 <span class="ltx_text ltx_font_typewriter" id="A3.F14.pic1.2.2.2.1.1.24.2">--
<br class="ltx_break"/>
<br class="ltx_break"/></span>‘Q’ = ‘Who was the Roman Emperor when Jesus was born?’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.25">1. Splitting ‘Q’ into ‘Q1’ and ‘Q2’:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.26">‘Q1’: ‘When was Jesus born? ‘</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.27">‘Q2’: ‘Who was the Roman Emperor in [A1]?’</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.28">2. Answering Q1:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.29">The answer ‘A1’ to ‘Q1’ : ‘When was Jesus born?’ is 1 BCE. ‘A1’ = 1 BCE</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.30">3. Substituting A1 to Q2:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.31">‘Q2’ : ‘Who was the Roman Emperor in 1 BCE?’,</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.32">4. Answers Q2:</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.33">The answer ‘A2’ to Q2’ : ‘Who was the Roman Emperor in 1 BCE?’.</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.34">‘A2‘ = ‘A‘ = ‘Caesar Augustus‘</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.35">You MUST apply this structure when asked to answer a multi-hop question ‘Q’.
Now answer the multi-hop question ‘Q‘ as shown in the examples above.</span>
<span class="ltx_p" id="A3.F14.pic1.2.2.2.1.1.36">Q: {question}</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span><span class="ltx_text ltx_font_italic" id="A3.F14.2.1">Three-shot CoT prompt</span> used at evaluation time in MHQA.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F15"><svg class="ltx_picture" height="1010.85" id="A3.F15.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1010.85) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 1004.94 C 0 1008.2 2.64 1010.85 5.91 1010.85 L 594.09 1010.85 C 597.36 1010.85 600 1008.2 600 1004.94 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 994.61 L 598.03 994.61 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 996.58)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F15.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F15.pic1.1.1.1.1.1.1">Prompt used to merge Q1 and Q2 in MHQA.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="976.89" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F15.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.1">Merge ‘Q1‘ and ‘Q2’ into a single multi-hop bridge question ‘Q’.</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.2">Learn from the following 3 examples. As shown in the following example:</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.3"><span class="ltx_text ltx_font_typewriter" id="A3.F15.pic1.2.2.2.1.1.3.1">--</span> Example #1 <span class="ltx_text ltx_font_typewriter" id="A3.F15.pic1.2.2.2.1.1.3.2">--
<br class="ltx_break"/>
<br class="ltx_break"/></span>‘Q1’ : "What was the spaceflight that first landed humans on the Moon?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.4">‘Q2’: "Who was the commander of Apollo 11?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.5">Solution:</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.6">1. Answer Q1; ‘A1’ is "Apollo 11”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.7">2. If ‘A1’ is in ‘Q2’ print(A1); ‘A1’ = Apollo 11 is in ‘Q2’ so I print "Apollo 11”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.8">3. Since you found ‘A1’ in ‘Q2’, rewrite ‘Q2’ so that you delete ‘A1’ and substitute ‘Q1’ there;</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.9">Rewriting Q2. Original ‘Q2’: "Who was the commander of Apollo 11?”. Since ‘A1’ is in ‘Q2’, I
delete it and write ‘Q1’ there. Rewritten ‘Q2’: "Who was the commander of the spaceflight
that first landed humans on the Moon?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.10">The single multi-hop question is therefore the rewritten ‘Q2’.</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.11">‘Q2‘ = ‘Q‘ = "Who was the commander of the spaceflight that first landed humans on the Moon?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.12"><span class="ltx_text ltx_font_typewriter" id="A3.F15.pic1.2.2.2.1.1.12.1">--</span> Example #2 <span class="ltx_text ltx_font_typewriter" id="A3.F15.pic1.2.2.2.1.1.12.2">--
<br class="ltx_break"/>
<br class="ltx_break"/></span>‘Q1’: What is the flagship product of Ferrero?</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.13">‘Q2’: What is the main ingredient in Nutella?</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.14">Solution:</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.15">1. Answer Q1; ‘A1’ is "Nutella”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.16">2. If ‘A1’ is in ‘Q2’ print(A1); ‘A1’ = "Nutella” is in ‘Q2’ so I print "Nutella”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.17">3. Since you found ‘A1’ in ‘Q2’, rewrite ‘Q2’ so that you delete ‘A1’ and substitute ‘Q1’ there;</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.18">Rewriting Q2. Original ‘Q2’: "What is the main ingredient in Nutella?”.</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.19">Since ‘A1’ is in ‘Q2’, I delete it and write ‘Q1’ there.</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.20">Rewritten ‘Q2’: "What is the main ingredient in the flagship product of Ferrero?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.21">The single multi-hop question is therefore the rewritten ‘Q2’.
‘Q2’ = ‘Q’ = "What is the main ingredient in the flagship product of Ferrero?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.22"><span class="ltx_text ltx_font_typewriter" id="A3.F15.pic1.2.2.2.1.1.22.1">--</span> Example #3 <span class="ltx_text ltx_font_typewriter" id="A3.F15.pic1.2.2.2.1.1.22.2">--
<br class="ltx_break"/>
<br class="ltx_break"/></span>‘Q1’: "When was Jesus born?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.23">‘Q2’: "Who was the Roman Emperor in 1 BCE?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.24">Solution:</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.25">1. Answer Q1; ‘A1’ is "1 BCE”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.26">2. If ‘A1’ is in ‘Q2’ print(A1); ‘A1’ = 1 BCE is in ‘Q2’ so I print “1 BCE”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.27">3. Since you found ‘A1’ in ‘Q2’, rewrite ‘Q2’ so that you delete ‘A1’ and substitute ‘Q1’ there;</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.28">Rewriting Q2. Original ‘Q2’: "Who was the Roman Emperor in 1 BCE?”. Since ‘A1’ is in ‘Q2’,
I delete it and write ‘Q1’ there. Rewritten ‘Q2’: "Who was the Roman Emperor when Jesus was
born?"</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.29">The single multi-hop question is therefore the rewritten ‘Q2’.</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.30">‘Q2’ = ‘Q’ = "Who was the Roman Emperor when Jesus was born?”</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.31">You MUST apply this structure when asked to merge ‘Q1’ and ‘Q2’.</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.32">Now merge ‘Q1’ and ‘Q2’ into a single multi-hop bridge question ‘Q’.</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.33">‘Q2’ : {question1}</span>
<span class="ltx_p" id="A3.F15.pic1.2.2.2.1.1.34">‘Q2’ : {question2}</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Prompt used to merge Q1 and Q2 in MHQA.

</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F16"><svg class="ltx_picture" height="114.05" id="A3.F16.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,114.05) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 108.15 C 0 111.41 2.64 114.05 5.91 114.05 L 594.09 114.05 C 597.36 114.05 600 111.41 600 108.15 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 97.97 L 598.03 97.97 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 99.94)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F16.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F16.pic1.1.1.1.1.1.1">Generating Q1 in MHQA.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="80.25" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F16.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F16.pic1.2.2.2.1.1.1">Identify one entity in the following text. Come up with a question so that the answer to this question is the entity chosen earlier. The question must be based on the following text. Write your results as ’Question:’ and then the question and ’Entity:’ and then the entity.</span>
<span class="ltx_p" id="A3.F16.pic1.2.2.2.1.1.2">Text: {document_one}</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Prompt used to generate <math alttext="Q_{1}" class="ltx_Math" display="inline" id="A3.F16.5.m1.1"><semantics id="A3.F16.5.m1.1b"><msub id="A3.F16.5.m1.1.1" xref="A3.F16.5.m1.1.1.cmml"><mi id="A3.F16.5.m1.1.1.2" xref="A3.F16.5.m1.1.1.2.cmml">Q</mi><mn id="A3.F16.5.m1.1.1.3" xref="A3.F16.5.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A3.F16.5.m1.1c"><apply id="A3.F16.5.m1.1.1.cmml" xref="A3.F16.5.m1.1.1"><csymbol cd="ambiguous" id="A3.F16.5.m1.1.1.1.cmml" xref="A3.F16.5.m1.1.1">subscript</csymbol><ci id="A3.F16.5.m1.1.1.2.cmml" xref="A3.F16.5.m1.1.1.2">𝑄</ci><cn id="A3.F16.5.m1.1.1.3.cmml" type="integer" xref="A3.F16.5.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F16.5.m1.1d">Q_{1}</annotation><annotation encoding="application/x-llamapun" id="A3.F16.5.m1.1e">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="Q_{1}" class="ltx_Math" display="inline" id="A3.F16.6.m2.1"><semantics id="A3.F16.6.m2.1b"><msub id="A3.F16.6.m2.1.1" xref="A3.F16.6.m2.1.1.cmml"><mi id="A3.F16.6.m2.1.1.2" xref="A3.F16.6.m2.1.1.2.cmml">Q</mi><mn id="A3.F16.6.m2.1.1.3" xref="A3.F16.6.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A3.F16.6.m2.1c"><apply id="A3.F16.6.m2.1.1.cmml" xref="A3.F16.6.m2.1.1"><csymbol cd="ambiguous" id="A3.F16.6.m2.1.1.1.cmml" xref="A3.F16.6.m2.1.1">subscript</csymbol><ci id="A3.F16.6.m2.1.1.2.cmml" xref="A3.F16.6.m2.1.1.2">𝑄</ci><cn id="A3.F16.6.m2.1.1.3.cmml" type="integer" xref="A3.F16.6.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F16.6.m2.1d">Q_{1}</annotation><annotation encoding="application/x-llamapun" id="A3.F16.6.m2.1e">italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> is generated such that its answer <math alttext="A1=E" class="ltx_Math" display="inline" id="A3.F16.7.m3.1"><semantics id="A3.F16.7.m3.1b"><mrow id="A3.F16.7.m3.1.1" xref="A3.F16.7.m3.1.1.cmml"><mrow id="A3.F16.7.m3.1.1.2" xref="A3.F16.7.m3.1.1.2.cmml"><mi id="A3.F16.7.m3.1.1.2.2" xref="A3.F16.7.m3.1.1.2.2.cmml">A</mi><mo id="A3.F16.7.m3.1.1.2.1" xref="A3.F16.7.m3.1.1.2.1.cmml">⁢</mo><mn id="A3.F16.7.m3.1.1.2.3" xref="A3.F16.7.m3.1.1.2.3.cmml">1</mn></mrow><mo id="A3.F16.7.m3.1.1.1" xref="A3.F16.7.m3.1.1.1.cmml">=</mo><mi id="A3.F16.7.m3.1.1.3" xref="A3.F16.7.m3.1.1.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="A3.F16.7.m3.1c"><apply id="A3.F16.7.m3.1.1.cmml" xref="A3.F16.7.m3.1.1"><eq id="A3.F16.7.m3.1.1.1.cmml" xref="A3.F16.7.m3.1.1.1"></eq><apply id="A3.F16.7.m3.1.1.2.cmml" xref="A3.F16.7.m3.1.1.2"><times id="A3.F16.7.m3.1.1.2.1.cmml" xref="A3.F16.7.m3.1.1.2.1"></times><ci id="A3.F16.7.m3.1.1.2.2.cmml" xref="A3.F16.7.m3.1.1.2.2">𝐴</ci><cn id="A3.F16.7.m3.1.1.2.3.cmml" type="integer" xref="A3.F16.7.m3.1.1.2.3">1</cn></apply><ci id="A3.F16.7.m3.1.1.3.cmml" xref="A3.F16.7.m3.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F16.7.m3.1d">A1=E</annotation><annotation encoding="application/x-llamapun" id="A3.F16.7.m3.1e">italic_A 1 = italic_E</annotation></semantics></math> where <math alttext="E" class="ltx_Math" display="inline" id="A3.F16.8.m4.1"><semantics id="A3.F16.8.m4.1b"><mi id="A3.F16.8.m4.1.1" xref="A3.F16.8.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="A3.F16.8.m4.1c"><ci id="A3.F16.8.m4.1.1.cmml" xref="A3.F16.8.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F16.8.m4.1d">E</annotation><annotation encoding="application/x-llamapun" id="A3.F16.8.m4.1e">italic_E</annotation></semantics></math> is the entity retrieved.
</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F17"><svg class="ltx_picture" height="97.45" id="A3.F17.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,97.45) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 91.54 C 0 94.81 2.64 97.45 5.91 97.45 L 594.09 97.45 C 597.36 97.45 600 94.81 600 91.54 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 81.37 L 598.03 81.37 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 83.34)"><foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F17.pic1.1.1.1.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F17.pic1.1.1.1.1.1.1">Generating Q2 in MHQA.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 17.72 9.84)"><foreignobject color="#000000" height="63.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="564.57">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.F17.pic1.2.2.2.1.1" style="width:408.0pt;">
<span class="ltx_p" id="A3.F17.pic1.2.2.2.1.1.1">Come up with a question based on the following text that contains the word:</span>
<span class="ltx_p" id="A3.F17.pic1.2.2.2.1.1.2">{entity}</span>
<span class="ltx_p" id="A3.F17.pic1.2.2.2.1.1.3">Text: {document_two}</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Prompt used to generate <math alttext="Q_{2}" class="ltx_Math" display="inline" id="A3.F17.5.m1.1"><semantics id="A3.F17.5.m1.1b"><msub id="A3.F17.5.m1.1.1" xref="A3.F17.5.m1.1.1.cmml"><mi id="A3.F17.5.m1.1.1.2" xref="A3.F17.5.m1.1.1.2.cmml">Q</mi><mn id="A3.F17.5.m1.1.1.3" xref="A3.F17.5.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.F17.5.m1.1c"><apply id="A3.F17.5.m1.1.1.cmml" xref="A3.F17.5.m1.1.1"><csymbol cd="ambiguous" id="A3.F17.5.m1.1.1.1.cmml" xref="A3.F17.5.m1.1.1">subscript</csymbol><ci id="A3.F17.5.m1.1.1.2.cmml" xref="A3.F17.5.m1.1.1.2">𝑄</ci><cn id="A3.F17.5.m1.1.1.3.cmml" type="integer" xref="A3.F17.5.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F17.5.m1.1d">Q_{2}</annotation><annotation encoding="application/x-llamapun" id="A3.F17.5.m1.1e">italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="Q_{2}" class="ltx_Math" display="inline" id="A3.F17.6.m2.1"><semantics id="A3.F17.6.m2.1b"><msub id="A3.F17.6.m2.1.1" xref="A3.F17.6.m2.1.1.cmml"><mi id="A3.F17.6.m2.1.1.2" xref="A3.F17.6.m2.1.1.2.cmml">Q</mi><mn id="A3.F17.6.m2.1.1.3" xref="A3.F17.6.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.F17.6.m2.1c"><apply id="A3.F17.6.m2.1.1.cmml" xref="A3.F17.6.m2.1.1"><csymbol cd="ambiguous" id="A3.F17.6.m2.1.1.1.cmml" xref="A3.F17.6.m2.1.1">subscript</csymbol><ci id="A3.F17.6.m2.1.1.2.cmml" xref="A3.F17.6.m2.1.1.2">𝑄</ci><cn id="A3.F17.6.m2.1.1.3.cmml" type="integer" xref="A3.F17.6.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.F17.6.m2.1d">Q_{2}</annotation><annotation encoding="application/x-llamapun" id="A3.F17.6.m2.1e">italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is generated such that its main topicis <math alttext="E" class="ltx_Math" display="inline" id="A3.F17.7.m3.1"><semantics id="A3.F17.7.m3.1b"><mi id="A3.F17.7.m3.1.1" xref="A3.F17.7.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="A3.F17.7.m3.1c"><ci id="A3.F17.7.m3.1.1.cmml" xref="A3.F17.7.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F17.7.m3.1d">E</annotation><annotation encoding="application/x-llamapun" id="A3.F17.7.m3.1e">italic_E</annotation></semantics></math> where <math alttext="E" class="ltx_Math" display="inline" id="A3.F17.8.m4.1"><semantics id="A3.F17.8.m4.1b"><mi id="A3.F17.8.m4.1.1" xref="A3.F17.8.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="A3.F17.8.m4.1c"><ci id="A3.F17.8.m4.1.1.cmml" xref="A3.F17.8.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F17.8.m4.1d">E</annotation><annotation encoding="application/x-llamapun" id="A3.F17.8.m4.1e">italic_E</annotation></semantics></math> is the entity retrieved. </figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 17:37:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
