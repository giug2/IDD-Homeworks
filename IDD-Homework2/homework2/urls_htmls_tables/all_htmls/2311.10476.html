<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.10476] FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data</title><meta property="og:description" content="Despite the widespread adoption of face recognition technology around the world, and its remarkable performance on current benchmarks, there are still several challenges that must be covered in more detail. This paper â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.10476">

<!--Generated on Tue Feb 27 18:37:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FRCSyn Challenge at WACV 2024:
<br class="ltx_break">Face Recognition Challenge in the Era of Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pietro Melzi<sup id="id70.2.id1" class="ltx_sup"><span id="id70.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruben Tolosana<sup id="id71.2.id1" class="ltx_sup"><span id="id71.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruben Vera-Rodriguez<sup id="id72.2.id1" class="ltx_sup"><span id="id72.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minchul Kim<sup id="id73.2.id1" class="ltx_sup"><span id="id73.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christian Rathgeb<sup id="id74.2.id1" class="ltx_sup"><span id="id74.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoming Liu<sup id="id75.2.id1" class="ltx_sup"><span id="id75.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ivan DeAndres-Tame<sup id="id76.2.id1" class="ltx_sup"><span id="id76.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aythami Morales<sup id="id77.2.id1" class="ltx_sup"><span id="id77.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julian Fierrez<sup id="id78.2.id1" class="ltx_sup"><span id="id78.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Javier Ortega-Garcia<sup id="id79.2.id1" class="ltx_sup"><span id="id79.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weisong Zhao<sup id="id80.2.id1" class="ltx_sup"><span id="id80.2.id1.1" class="ltx_text ltx_font_italic">4,5</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiangyu Zhu<sup id="id81.2.id1" class="ltx_sup"><span id="id81.2.id1.1" class="ltx_text ltx_font_italic">6,7</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zheyu Yan<sup id="id82.2.id1" class="ltx_sup"><span id="id82.2.id1.1" class="ltx_text ltx_font_italic">6</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiao-Yu Zhang<sup id="id83.2.id1" class="ltx_sup"><span id="id83.2.id1.1" class="ltx_text ltx_font_italic">4,5</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinlin Wu<sup id="id84.2.id1" class="ltx_sup"><span id="id84.2.id1.1" class="ltx_text ltx_font_italic">8</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhen Lei<sup id="id85.2.id1" class="ltx_sup"><span id="id85.2.id1.1" class="ltx_text ltx_font_italic">6,7,8</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Suvidha Tripathi<sup id="id86.2.id1" class="ltx_sup"><span id="id86.2.id1.1" class="ltx_text ltx_font_italic">9</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mahak Kothari<sup id="id87.2.id1" class="ltx_sup"><span id="id87.2.id1.1" class="ltx_text ltx_font_italic">9</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md Haider Zama<sup id="id88.2.id1" class="ltx_sup"><span id="id88.2.id1.1" class="ltx_text ltx_font_italic">9</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Debayan Deb<sup id="id89.2.id1" class="ltx_sup"><span id="id89.2.id1.1" class="ltx_text ltx_font_italic">9</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bernardo Biesseck<sup id="id90.2.id1" class="ltx_sup"><span id="id90.2.id1.1" class="ltx_text ltx_font_italic">10,11</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pedro Vidal<sup id="id91.2.id1" class="ltx_sup"><span id="id91.2.id1.1" class="ltx_text ltx_font_italic">10</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roger Granada<sup id="id92.2.id1" class="ltx_sup"><span id="id92.2.id1.1" class="ltx_text ltx_font_italic">12</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guilherme Fickel<sup id="id93.2.id1" class="ltx_sup"><span id="id93.2.id1.1" class="ltx_text ltx_font_italic">12</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gustavo FÃ¼hr<sup id="id94.2.id1" class="ltx_sup"><span id="id94.2.id1.1" class="ltx_text ltx_font_italic">12</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Menotti<sup id="id95.2.id1" class="ltx_sup"><span id="id95.2.id1.1" class="ltx_text ltx_font_italic">10</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander Unnervik<sup id="id96.2.id1" class="ltx_sup"><span id="id96.2.id1.1" class="ltx_text ltx_font_italic">13,14</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anjith George<sup id="id97.2.id1" class="ltx_sup"><span id="id97.2.id1.1" class="ltx_text ltx_font_italic">13</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christophe Ecabert<sup id="id98.2.id1" class="ltx_sup"><span id="id98.2.id1.1" class="ltx_text ltx_font_italic">13</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hatef Otroshi Shahreza<sup id="id99.2.id1" class="ltx_sup"><span id="id99.2.id1.1" class="ltx_text ltx_font_italic">13,14</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Parsa Rahimi<sup id="id100.2.id1" class="ltx_sup"><span id="id100.2.id1.1" class="ltx_text ltx_font_italic">13,14</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">SÃ©bastien Marcel<sup id="id101.2.id1" class="ltx_sup"><span id="id101.2.id1.1" class="ltx_text ltx_font_italic">13,15</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ioannis Sarridis<sup id="id102.2.id1" class="ltx_sup"><span id="id102.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christos Koutlis<sup id="id103.2.id1" class="ltx_sup"><span id="id103.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Georgia Baltsou<sup id="id104.2.id1" class="ltx_sup"><span id="id104.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Symeon Papadopoulos<sup id="id105.2.id1" class="ltx_sup"><span id="id105.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christos Diou<sup id="id106.2.id1" class="ltx_sup"><span id="id106.2.id1.1" class="ltx_text ltx_font_italic">17</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">NicolÃ² Di Domenico<sup id="id107.2.id1" class="ltx_sup"><span id="id107.2.id1.1" class="ltx_text ltx_font_italic">18</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guido Borghi<sup id="id108.2.id1" class="ltx_sup"><span id="id108.2.id1.1" class="ltx_text ltx_font_italic">18</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lorenzo Pellegrini<sup id="id109.2.id1" class="ltx_sup"><span id="id109.2.id1.1" class="ltx_text ltx_font_italic">18</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Enrique Mas-Candela<sup id="id110.2.id1" class="ltx_sup"><span id="id110.2.id1.1" class="ltx_text ltx_font_italic">19</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ãngela SÃ¡nchez-PÃ©rez<sup id="id111.2.id1" class="ltx_sup"><span id="id111.2.id1.1" class="ltx_text ltx_font_italic">19</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrea Atzori<sup id="id112.2.id1" class="ltx_sup"><span id="id112.2.id1.1" class="ltx_text ltx_font_italic">20</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fadi Boutros<sup id="id113.2.id1" class="ltx_sup"><span id="id113.2.id1.1" class="ltx_text ltx_font_italic">21,22</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Naser Damer<sup id="id114.2.id1" class="ltx_sup"><span id="id114.2.id1.1" class="ltx_text ltx_font_italic">21,22</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gianni Fenu<sup id="id115.2.id1" class="ltx_sup"><span id="id115.2.id1.1" class="ltx_text ltx_font_italic">20</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mirko Marras<sup id="id116.2.id1" class="ltx_sup"><span id="id116.2.id1.1" class="ltx_text ltx_font_italic">20</span></sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><sup id="id117.23.id1" class="ltx_sup"><span id="id117.23.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Universidad Autonoma de Madrid, Spain
<sup id="id118.24.id2" class="ltx_sup"><span id="id118.24.id2.1" class="ltx_text ltx_font_italic">2</span></sup>Michigan State University, US
<sup id="id119.25.id3" class="ltx_sup"><span id="id119.25.id3.1" class="ltx_text ltx_font_italic">3</span></sup>Hochschule Darmstadt, Germany
<br class="ltx_break"><sup id="id120.26.id4" class="ltx_sup"><span id="id120.26.id4.1" class="ltx_text ltx_font_italic">4</span></sup>IIE, CAS, China
<sup id="id121.27.id5" class="ltx_sup"><span id="id121.27.id5.1" class="ltx_text ltx_font_italic">5</span></sup>School of Cyber Security, UCAS, China
<sup id="id122.28.id6" class="ltx_sup"><span id="id122.28.id6.1" class="ltx_text ltx_font_italic">6</span></sup>MAIS, CASIA, China
<br class="ltx_break"><sup id="id123.29.id7" class="ltx_sup"><span id="id123.29.id7.1" class="ltx_text ltx_font_italic">7</span></sup>School of Artificial Intelligence, UCAS, China
<sup id="id124.30.id8" class="ltx_sup"><span id="id124.30.id8.1" class="ltx_text ltx_font_italic">8</span></sup>CAIR, HKISI, CAS, China
<sup id="id125.31.id9" class="ltx_sup"><span id="id125.31.id9.1" class="ltx_text ltx_font_italic">9</span></sup>LENS, Inc., USA
<br class="ltx_break"><sup id="id126.32.id10" class="ltx_sup"><span id="id126.32.id10.1" class="ltx_text ltx_font_italic">10</span></sup>Federal University of ParanÃ¡, Curitiba, PR, Brazil
<sup id="id127.33.id11" class="ltx_sup"><span id="id127.33.id11.1" class="ltx_text ltx_font_italic">11</span></sup>Federal Institute of Mato Grosso, Pontes e Lacerda, Brazil
<br class="ltx_break"><sup id="id128.34.id12" class="ltx_sup"><span id="id128.34.id12.1" class="ltx_text ltx_font_italic">12</span></sup>unico - idTech, Brazil
<sup id="id129.35.id13" class="ltx_sup"><span id="id129.35.id13.1" class="ltx_text ltx_font_italic">13</span></sup>Idiap Research Institute, Switzerland
<sup id="id130.36.id14" class="ltx_sup"><span id="id130.36.id14.1" class="ltx_text ltx_font_italic">14</span></sup>Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne, Switzerland
<br class="ltx_break"><sup id="id131.37.id15" class="ltx_sup"><span id="id131.37.id15.1" class="ltx_text ltx_font_italic">15</span></sup>UniversitÃ© de Lausanne, Switzerland
<sup id="id132.38.id16" class="ltx_sup"><span id="id132.38.id16.1" class="ltx_text ltx_font_italic">16</span></sup>Centre for Research and Technology Hellas, Greece
<br class="ltx_break"><sup id="id133.39.id17" class="ltx_sup"><span id="id133.39.id17.1" class="ltx_text ltx_font_italic">17</span></sup>Harokopio University of Athens, Greece
<sup id="id134.40.id18" class="ltx_sup"><span id="id134.40.id18.1" class="ltx_text ltx_font_italic">18</span></sup>University of Bologna, Cesena Campus, Italy
<sup id="id135.41.id19" class="ltx_sup"><span id="id135.41.id19.1" class="ltx_text ltx_font_italic">19</span></sup>Facephi, Spain
<br class="ltx_break"><sup id="id136.42.id20" class="ltx_sup"><span id="id136.42.id20.1" class="ltx_text ltx_font_italic">20</span></sup>University of Cagliari, Italy
<sup id="id137.43.id21" class="ltx_sup"><span id="id137.43.id21.1" class="ltx_text ltx_font_italic">21</span></sup>Fraunhofer IGD, Germany
<sup id="id138.44.id22" class="ltx_sup"><span id="id138.44.id22.1" class="ltx_text ltx_font_italic">22</span></sup>TU Darmstadt, Germany
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id139.id1" class="ltx_p">Despite the widespread adoption of face recognition technology around the world, and its remarkable performance on current benchmarks, there are still several challenges that must be covered in more detail. This paper offers an overview of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) organized at WACV 2024. This is the first international challenge aiming to explore the use of synthetic data in face recognition to address existing limitations in the technology. Specifically, the FRCSyn Challenge targets concerns related to data privacy issues, demographic biases, generalization to unseen scenarios, and performance limitations in challenging scenarios, including significant age disparities between enrollment and testing, pose variations, and occlusions. The results achieved in the FRCSyn Challenge, together with the proposed benchmark, contribute significantly to the application of synthetic data to improve face recognition technology.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Facial images represent the most popular data for biometric recognition nowadays, finding extensive applications in surveillance, government offices, and smartphone authentication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, among others. Numerous studies in the literature have contributed to the development of state-of-the-art (SOTA) Face Recognition (FR) technologies, demonstrating exceptional performance on standard benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The success of these technologies is attributed to the advent of Deep Learning (DL) and the formulation of highly effective loss functions based on margin loss, capable of generating highly discriminative features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. As a result, FR systems have significantly advanced, achieving astonishing results on well-recognized databases, such as LFW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.10476/assets/x1.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="264" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2311.10476/assets/x2.png" id="S1.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="264" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Examples of synthetic identities (one for each row) and intra-class variations for different demographic groups.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, FR still encounters numerous challenges due to factors such as variations in facial images concerning pose, aging, expressions, and occlusions, giving rise to significant issues in the field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The application of DL introduces additional concerns, including limited training data, noisy labeling, imbalanced data related to different identities and demographic groups, and low resolution, among other issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Deploying FR systems that remain resilient to these challenges and generalize well to unseen conditions is a difficult task. For instance, training data often exhibit significant imbalances across demographic groups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and may fail to represent the full spectrum of possible occlusions in real-world scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Various limitations associated with established databases and benchmarks are discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In recent years, several approaches have been presented in the literature for the generation of face synthetic content <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> for different applications such as FR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and digital face manipulations, a.k.a. DeepFakes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. These synthetic data offer several advantages over real-world databases. Firstly, synthetic databases provide a promising solution to address privacy concerns associated with real data, often collected from individuals without their knowledge or consent through various online sources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Secondly, synthetic face generators have the potential to produce large amounts of data, especially valuable following the discontinuation of established databases due to privacy concerns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and the enforcement of regulations like the EU-GDPR, which requires informed consent for collecting and using personal data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Finally, when the synthesis process is controllable, it becomes relatively straightforward to create databases with the desired characteristics (<em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> demographic groups, age, pose, etc.) and their corresponding labels, without additional human efforts. This contrasts with real-world databases, which may not adequately represent diverse demographic groups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, among many other aspects.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">These advantages have motivated an initial exploration of the application of face synthetic data to current FR systems. Innovative generative frameworks have been introduced to synthesize databases suitable for training FR systems, including Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and 3D models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. While these synthetic databases advance in the field, some have limitations that impact FR systems performance compared to those trained with real data. Specifically, databases synthesized with GANs provide limited representations of intra-class variations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and those synthesized with 3D models lack realism. Recently, Diffusion models have been employed to generate synthetic databases with enhanced intra-class variations, effectively mitigating some limitations observed in prior synthetic databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. This is supported by various recent works involving Diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To evaluate the effectiveness of novel synthetic databases generated using Diffusion models for training FR systems, this paper analyzes the results achieved in the â€œFace Recognition Challenge in the Era of Synthetic Data (FRCSyn)â€ organized at WACV 2024<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://frcsyn.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://frcsyn.github.io/</a></span></span></span>. This challenge is designed to comprehensively analyze the following research questions:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Can synthetic data effectively replace real data for training FR systems, and what are the limits of FR technology exclusively trained with synthetic data?</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Can the utilization of synthetic data be beneficial in addressing and mitigating the existing limitations within FR technology?</p>
</div>
</li>
</ol>
<p id="S1.p5.2" class="ltx_p">In the proposed FRCSyn Challenge, we have designed specific tasks and sub-tasks to address these questions. In addition, we have released to the participants two novel synthetic databases created using two state-of-the-art Diffusion methods: DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. These databases have been generated with a particular focus on tackling common challenges in FR, including imbalanced demographic distributions, pose variation, expression diversity, and the presence of occlusion (see Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The proposed FRCSyn Challenge provides valuable insights for the future of FR and the utilization of synthetic data, with a specific emphasis on quantifying the performance gap between training FR systems with real and synthetic data. In addition, the FRCSyn Challenge proposes standard benchmarks that are easily reproducible for the research community. The reminder of the paper is organized as follows. Section <a href="#S2" title="2 FRCSyn Challenge: Databases â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides details about the databases considered in the FRCSyn Challenge. In Section <a href="#S3" title="3 FRCSyn Challenge: Setup â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we outline the proposed tasks and sub-tasks, the experimental protocol, and metrics used in the challenge. In Section <a href="#S4" title="4 FRCSyn Challenge: Description of Systems â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we provide a description of the top-5 FR systems proposed in the FRCSyn Challenge for each sub-task. Section <a href="#S5" title="5 FRCSyn Challenge: Results â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the results achieved in the different tasks and sub-tasks of the challenge. Finally, in Section <a href="#S6" title="6 Conclusion â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we draw the conclusions from the FRCSyn Challenge and highlight potential future research directions in the field.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>FRCSyn Challenge: Databases</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 â€£ 2 FRCSyn Challenge: Databases â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides details of the public databases considered in the FRCSyn Challenge. Participants were instructed to download all necessary databases for the FRCSyn Challenge upon registration. Permission for redistributing these databases was obtained from the owners.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:91.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-90.4pt,35.3pt) scale(0.563704927392101,0.563704927392101) ;">
<table id="S2.T1.2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.2.1.1.1" class="ltx_tr">
<td id="S2.T1.2.1.1.1.1" class="ltx_td ltx_align_left"><span id="S2.T1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Database</span></td>
<td id="S2.T1.2.1.1.1.2" class="ltx_td ltx_align_left"><span id="S2.T1.2.1.1.1.2.1" class="ltx_text ltx_font_bold">Framework</span></td>
<td id="S2.T1.2.1.1.1.3" class="ltx_td ltx_align_left"><span id="S2.T1.2.1.1.1.3.1" class="ltx_text ltx_font_bold">Use</span></td>
<td id="S2.T1.2.1.1.1.4" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.1.1.4.1" class="ltx_text ltx_font_bold"># Id</span></td>
<td id="S2.T1.2.1.1.1.5" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.1.1.5.1" class="ltx_text ltx_font_bold"># Img/Id</span></td>
</tr>
<tr id="S2.T1.2.1.2.2" class="ltx_tr">
<td id="S2.T1.2.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S2.T1.2.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">DCFace</td>
<td id="S2.T1.2.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">Train</td>
<td id="S2.T1.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">10K</td>
<td id="S2.T1.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">50</td>
</tr>
<tr id="S2.T1.2.1.3.3" class="ltx_tr">
<td id="S2.T1.2.1.3.3.1" class="ltx_td ltx_align_left">GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S2.T1.2.1.3.3.2" class="ltx_td ltx_align_left">GANDiffFace</td>
<td id="S2.T1.2.1.3.3.3" class="ltx_td ltx_align_left">Train</td>
<td id="S2.T1.2.1.3.3.4" class="ltx_td ltx_align_center">10K</td>
<td id="S2.T1.2.1.3.3.5" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="S2.T1.2.1.4.4" class="ltx_tr">
<td id="S2.T1.2.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t">CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S2.T1.2.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t">Real-world</td>
<td id="S2.T1.2.1.4.4.3" class="ltx_td ltx_align_left ltx_border_t">Train</td>
<td id="S2.T1.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">10.5K</td>
<td id="S2.T1.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">47</td>
</tr>
<tr id="S2.T1.2.1.5.5" class="ltx_tr">
<td id="S2.T1.2.1.5.5.1" class="ltx_td ltx_align_left">FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S2.T1.2.1.5.5.2" class="ltx_td ltx_align_left">Real-world</td>
<td id="S2.T1.2.1.5.5.3" class="ltx_td ltx_align_left">Train</td>
<td id="S2.T1.2.1.5.5.4" class="ltx_td ltx_align_center">70K</td>
<td id="S2.T1.2.1.5.5.5" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="S2.T1.2.1.6.6" class="ltx_tr">
<td id="S2.T1.2.1.6.6.1" class="ltx_td ltx_align_left ltx_border_t">BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S2.T1.2.1.6.6.2" class="ltx_td ltx_align_left ltx_border_t">Real-world</td>
<td id="S2.T1.2.1.6.6.3" class="ltx_td ltx_align_left ltx_border_t">Eval</td>
<td id="S2.T1.2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">24K</td>
<td id="S2.T1.2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">45</td>
</tr>
<tr id="S2.T1.2.1.7.7" class="ltx_tr">
<td id="S2.T1.2.1.7.7.1" class="ltx_td ltx_align_left">AgeDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S2.T1.2.1.7.7.2" class="ltx_td ltx_align_left">Real-world</td>
<td id="S2.T1.2.1.7.7.3" class="ltx_td ltx_align_left">Eval</td>
<td id="S2.T1.2.1.7.7.4" class="ltx_td ltx_align_center">570</td>
<td id="S2.T1.2.1.7.7.5" class="ltx_td ltx_align_center">29</td>
</tr>
<tr id="S2.T1.2.1.8.8" class="ltx_tr">
<td id="S2.T1.2.1.8.8.1" class="ltx_td ltx_align_left">CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S2.T1.2.1.8.8.2" class="ltx_td ltx_align_left">Real-world</td>
<td id="S2.T1.2.1.8.8.3" class="ltx_td ltx_align_left">Eval</td>
<td id="S2.T1.2.1.8.8.4" class="ltx_td ltx_align_center">500</td>
<td id="S2.T1.2.1.8.8.5" class="ltx_td ltx_align_center">14</td>
</tr>
<tr id="S2.T1.2.1.9.9" class="ltx_tr">
<td id="S2.T1.2.1.9.9.1" class="ltx_td ltx_align_left ltx_border_b">ROF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S2.T1.2.1.9.9.2" class="ltx_td ltx_align_left ltx_border_b">Real-world</td>
<td id="S2.T1.2.1.9.9.3" class="ltx_td ltx_align_left ltx_border_b">Eval</td>
<td id="S2.T1.2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b">180</td>
<td id="S2.T1.2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_b">31</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">Details of the databases considered in the FRCSyn Challenge. Id = Identities, Img = Images.</span></figcaption>
</figure>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic Databases:</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">For the training of the proposed FR systems, we provide access to two synthetic databases generated using recent frameworks based on Diffusion models:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.4" class="ltx_p"><span id="S2.I1.i1.p1.4.1" class="ltx_text ltx_font_bold">DCFace</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. This framework comprises: <em id="S2.I1.i1.p1.4.2" class="ltx_emph ltx_font_italic">i)</em> a sampling stage for generating synthetic identities <math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="X_{ID}" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><msub id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.2" xref="S2.I1.i1.p1.1.m1.1.1.2.cmml">X</mi><mrow id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.3.2" xref="S2.I1.i1.p1.1.m1.1.1.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.1.m1.1.1.3.1" xref="S2.I1.i1.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.I1.i1.p1.1.m1.1.1.3.3" xref="S2.I1.i1.p1.1.m1.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2">ğ‘‹</ci><apply id="S2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3"><times id="S2.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.1"></times><ci id="S2.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.2">ğ¼</ci><ci id="S2.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">X_{ID}</annotation></semantics></math>, and <em id="S2.I1.i1.p1.4.3" class="ltx_emph ltx_font_italic">ii)</em> a mixing stage for generating images <math id="S2.I1.i1.p1.2.m2.2" class="ltx_Math" alttext="X_{ID,sty}" display="inline"><semantics id="S2.I1.i1.p1.2.m2.2a"><msub id="S2.I1.i1.p1.2.m2.2.3" xref="S2.I1.i1.p1.2.m2.2.3.cmml"><mi id="S2.I1.i1.p1.2.m2.2.3.2" xref="S2.I1.i1.p1.2.m2.2.3.2.cmml">X</mi><mrow id="S2.I1.i1.p1.2.m2.2.2.2.2" xref="S2.I1.i1.p1.2.m2.2.2.2.3.cmml"><mrow id="S2.I1.i1.p1.2.m2.1.1.1.1.1" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.2" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.2.m2.1.1.1.1.1.1" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.3" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.3.cmml">D</mi></mrow><mo id="S2.I1.i1.p1.2.m2.2.2.2.2.3" xref="S2.I1.i1.p1.2.m2.2.2.2.3.cmml">,</mo><mrow id="S2.I1.i1.p1.2.m2.2.2.2.2.2" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.cmml"><mi id="S2.I1.i1.p1.2.m2.2.2.2.2.2.2" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.2.m2.2.2.2.2.2.1" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S2.I1.i1.p1.2.m2.2.2.2.2.2.3" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.2.m2.2.2.2.2.2.1a" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.1.cmml">â€‹</mo><mi id="S2.I1.i1.p1.2.m2.2.2.2.2.2.4" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.4.cmml">y</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.2b"><apply id="S2.I1.i1.p1.2.m2.2.3.cmml" xref="S2.I1.i1.p1.2.m2.2.3"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.2.3.1.cmml" xref="S2.I1.i1.p1.2.m2.2.3">subscript</csymbol><ci id="S2.I1.i1.p1.2.m2.2.3.2.cmml" xref="S2.I1.i1.p1.2.m2.2.3.2">ğ‘‹</ci><list id="S2.I1.i1.p1.2.m2.2.2.2.3.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2"><apply id="S2.I1.i1.p1.2.m2.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1"><times id="S2.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.1"></times><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.2">ğ¼</ci><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.3">ğ·</ci></apply><apply id="S2.I1.i1.p1.2.m2.2.2.2.2.2.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2"><times id="S2.I1.i1.p1.2.m2.2.2.2.2.2.1.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.1"></times><ci id="S2.I1.i1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.2">ğ‘ </ci><ci id="S2.I1.i1.p1.2.m2.2.2.2.2.2.3.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.3">ğ‘¡</ci><ci id="S2.I1.i1.p1.2.m2.2.2.2.2.2.4.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.4">ğ‘¦</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.2c">X_{ID,sty}</annotation></semantics></math> with the same identities <math id="S2.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="X_{ID}" display="inline"><semantics id="S2.I1.i1.p1.3.m3.1a"><msub id="S2.I1.i1.p1.3.m3.1.1" xref="S2.I1.i1.p1.3.m3.1.1.cmml"><mi id="S2.I1.i1.p1.3.m3.1.1.2" xref="S2.I1.i1.p1.3.m3.1.1.2.cmml">X</mi><mrow id="S2.I1.i1.p1.3.m3.1.1.3" xref="S2.I1.i1.p1.3.m3.1.1.3.cmml"><mi id="S2.I1.i1.p1.3.m3.1.1.3.2" xref="S2.I1.i1.p1.3.m3.1.1.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.3.m3.1.1.3.1" xref="S2.I1.i1.p1.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.I1.i1.p1.3.m3.1.1.3.3" xref="S2.I1.i1.p1.3.m3.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.3.m3.1b"><apply id="S2.I1.i1.p1.3.m3.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.3.m3.1.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.3.m3.1.1.2.cmml" xref="S2.I1.i1.p1.3.m3.1.1.2">ğ‘‹</ci><apply id="S2.I1.i1.p1.3.m3.1.1.3.cmml" xref="S2.I1.i1.p1.3.m3.1.1.3"><times id="S2.I1.i1.p1.3.m3.1.1.3.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1.3.1"></times><ci id="S2.I1.i1.p1.3.m3.1.1.3.2.cmml" xref="S2.I1.i1.p1.3.m3.1.1.3.2">ğ¼</ci><ci id="S2.I1.i1.p1.3.m3.1.1.3.3.cmml" xref="S2.I1.i1.p1.3.m3.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.3.m3.1c">X_{ID}</annotation></semantics></math> from the sampling stage and styles selected from a â€œstyle bankâ€ of images <math id="S2.I1.i1.p1.4.m4.1" class="ltx_Math" alttext="X_{sty}" display="inline"><semantics id="S2.I1.i1.p1.4.m4.1a"><msub id="S2.I1.i1.p1.4.m4.1.1" xref="S2.I1.i1.p1.4.m4.1.1.cmml"><mi id="S2.I1.i1.p1.4.m4.1.1.2" xref="S2.I1.i1.p1.4.m4.1.1.2.cmml">X</mi><mrow id="S2.I1.i1.p1.4.m4.1.1.3" xref="S2.I1.i1.p1.4.m4.1.1.3.cmml"><mi id="S2.I1.i1.p1.4.m4.1.1.3.2" xref="S2.I1.i1.p1.4.m4.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.4.m4.1.1.3.1" xref="S2.I1.i1.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.I1.i1.p1.4.m4.1.1.3.3" xref="S2.I1.i1.p1.4.m4.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.I1.i1.p1.4.m4.1.1.3.1a" xref="S2.I1.i1.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.I1.i1.p1.4.m4.1.1.3.4" xref="S2.I1.i1.p1.4.m4.1.1.3.4.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.4.m4.1b"><apply id="S2.I1.i1.p1.4.m4.1.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.4.m4.1.1.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.4.m4.1.1.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2">ğ‘‹</ci><apply id="S2.I1.i1.p1.4.m4.1.1.3.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3"><times id="S2.I1.i1.p1.4.m4.1.1.3.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.1"></times><ci id="S2.I1.i1.p1.4.m4.1.1.3.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.2">ğ‘ </ci><ci id="S2.I1.i1.p1.4.m4.1.1.3.3.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.3">ğ‘¡</ci><ci id="S2.I1.i1.p1.4.m4.1.1.3.4.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.4">ğ‘¦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.4.m4.1c">X_{sty}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">GANDiffFace</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. This framework combines GANs and Diffusion models to generate fully-synthetic FR databases with desired properties such as human face realism, controllable demographic distributions, and realistic intra-class variations.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides examples of the synthetic face images created using DCFace and GANDiffFace approaches. These synthetic databases represent a diverse range of demographic groups, including variations in ethnicity, gender, and age. The synthesis process considers typical variations in FR, including pose, facial expression, illumination, and occlusions. In the FRCSyn Challenge, synthetic data are exclusively utilized in the training stage, replicating realistic operational scenarios.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Real Databases:</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.3" class="ltx_p">For the training of FR systems (depending on the sub-task, please see Section <a href="#S3" title="3 FRCSyn Challenge: Setup â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), participants are allowed to use two real databases: <span id="S2.SS0.SSS0.Px2.p1.3.1" class="ltx_text ltx_font_italic">i)</span> <span id="S2.SS0.SSS0.Px2.p1.3.2" class="ltx_text ltx_font_bold">CASIA-WebFace</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, a database containing <math id="S2.SS0.SSS0.Px2.p1.1.m1.2" class="ltx_Math" alttext="494,414" display="inline"><semantics id="S2.SS0.SSS0.Px2.p1.1.m1.2a"><mrow id="S2.SS0.SSS0.Px2.p1.1.m1.2.3.2" xref="S2.SS0.SSS0.Px2.p1.1.m1.2.3.1.cmml"><mn id="S2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">494</mn><mo id="S2.SS0.SSS0.Px2.p1.1.m1.2.3.2.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS0.SSS0.Px2.p1.1.m1.2.2" xref="S2.SS0.SSS0.Px2.p1.1.m1.2.2.cmml">414</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.1.m1.2b"><list id="S2.SS0.SSS0.Px2.p1.1.m1.2.3.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.2.3.2"><cn type="integer" id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1">494</cn><cn type="integer" id="S2.SS0.SSS0.Px2.p1.1.m1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.2.2">414</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.1.m1.2c">494,414</annotation></semantics></math> face images of <math id="S2.SS0.SSS0.Px2.p1.2.m2.2" class="ltx_Math" alttext="10,575" display="inline"><semantics id="S2.SS0.SSS0.Px2.p1.2.m2.2a"><mrow id="S2.SS0.SSS0.Px2.p1.2.m2.2.3.2" xref="S2.SS0.SSS0.Px2.p1.2.m2.2.3.1.cmml"><mn id="S2.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">10</mn><mo id="S2.SS0.SSS0.Px2.p1.2.m2.2.3.2.1" xref="S2.SS0.SSS0.Px2.p1.2.m2.2.3.1.cmml">,</mo><mn id="S2.SS0.SSS0.Px2.p1.2.m2.2.2" xref="S2.SS0.SSS0.Px2.p1.2.m2.2.2.cmml">575</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.2.m2.2b"><list id="S2.SS0.SSS0.Px2.p1.2.m2.2.3.1.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.2.3.2"><cn type="integer" id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1">10</cn><cn type="integer" id="S2.SS0.SSS0.Px2.p1.2.m2.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.2.2">575</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.2.m2.2c">10,575</annotation></semantics></math> real identities collected from the web, and <span id="S2.SS0.SSS0.Px2.p1.3.3" class="ltx_text ltx_font_italic">ii)</span> <span id="S2.SS0.SSS0.Px2.p1.3.4" class="ltx_text ltx_font_bold">FFHQ</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, a database designed for face applications, containing <math id="S2.SS0.SSS0.Px2.p1.3.m3.2" class="ltx_Math" alttext="70,000" display="inline"><semantics id="S2.SS0.SSS0.Px2.p1.3.m3.2a"><mrow id="S2.SS0.SSS0.Px2.p1.3.m3.2.3.2" xref="S2.SS0.SSS0.Px2.p1.3.m3.2.3.1.cmml"><mn id="S2.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">70</mn><mo id="S2.SS0.SSS0.Px2.p1.3.m3.2.3.2.1" xref="S2.SS0.SSS0.Px2.p1.3.m3.2.3.1.cmml">,</mo><mn id="S2.SS0.SSS0.Px2.p1.3.m3.2.2" xref="S2.SS0.SSS0.Px2.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.3.m3.2b"><list id="S2.SS0.SSS0.Px2.p1.3.m3.2.3.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.2.3.2"><cn type="integer" id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1">70</cn><cn type="integer" id="S2.SS0.SSS0.Px2.p1.3.m3.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.3.m3.2c">70,000</annotation></semantics></math> high-quality face images with considerable variation in terms of age, ethnicity and image background. These real databases are chosen as they are used to train the generative frameworks of DCFace and GANDiffFace, respectively. This strategy enables a direct comparison between the traditional approach of training FR systems using only real data and the novel approach explored in this challenge, using synthetic data. Despite not being specifically designed for face recognition, the FFHQ database can be considered in the proposed challenge for various purposes, such as training a model for feature extraction and applying domain adaptation, among other possibilities.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">For the final evaluation of the proposed FR systems, we consider four real databases: <span id="S2.SS0.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">i)</span> <span id="S2.SS0.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_bold">BUPT-BalancedFace</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, <span id="S2.SS0.SSS0.Px2.p2.1.3" class="ltx_text ltx_font_italic">ii)</span> <span id="S2.SS0.SSS0.Px2.p2.1.4" class="ltx_text ltx_font_bold">AgeDB</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, <span id="S2.SS0.SSS0.Px2.p2.1.5" class="ltx_text ltx_font_italic">iii)</span> <span id="S2.SS0.SSS0.Px2.p2.1.6" class="ltx_text ltx_font_bold">CFP-FP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and <span id="S2.SS0.SSS0.Px2.p2.1.7" class="ltx_text ltx_font_italic">iv)</span> <span id="S2.SS0.SSS0.Px2.p2.1.8" class="ltx_text ltx_font_bold">ROF</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> is designed to address performance disparities across different ethnic groups. We relabel it according to the FairFace classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, which provides labels for ethnicity and gender. We then consider the eight demographic groups obtained from all possible combinations of four ethnic groups (Asian, Black, Indian, and White) and two genders (Female and Male). We recognize that these groups do not comprehensively represent the entire spectrum of real world ethnic diversity. The selection of these categories, while imperfect, is primarily driven by the need to align with the demographic categorizations used in BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> for facilitating easier and more consistent evaluation. The other three databases, <span id="S2.SS0.SSS0.Px2.p2.1.9" class="ltx_text ltx_font_italic">i.e.,</span> AgeDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and ROF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, are real-world databases widely employed to benchmark FR systems in terms of age variations, pose variations, and presence of occlusions. It is important to highlight that, as different real databases are considered for training and evaluation, we also intend to analyse the generalization ability of the proposed FR systems.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>FRCSyn Challenge: Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tasks</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The FRCSyn Challenge has been hosted on Codalab<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://codalab.lisn.upsaclay.fr/competitions/15485" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://codalab.lisn.upsaclay.fr/competitions/15485</a></span></span></span>, an open-source framework for running scientific competitions and benchmarks. It aims to explore the application of synthetic data into the training of FR systems, with a specific focus on addressing two critical aspects in current FR technology: <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i)</em> mitigating demographic bias, and <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">ii)</em> enhancing overall performance under challenging conditions that include variations in age and pose, the presence of occlusions, and diverse demographic groups. To investigate these two areas, the FRCSyn Challenge considers two distinct tasks, each comprising two sub-tasks. Sub-tasks have been designed to consider different approaches for training FR systems: <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">i)</span> utilizing solely synthetic data, and <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">ii)</span> involving a combination of real and synthetic data. Consequently, the FRCSyn Challenge comprises a total of four sub-tasks. A summary is provided in Table <a href="#S3.T2" title="Table 2 â€£ 3.1 Tasks â€£ 3 FRCSyn Challenge: Setup â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For each sub-task, we specify the databases allowed for training FR systems. Nevertheless, participants have the flexibility to decide whether and how to utilize each database in the training process.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:132.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-200.1pt,113.7pt) scale(0.368493537515962,0.368493537515962) ;">
<table id="S3.T2.2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.1.1.1" class="ltx_tr">
<td id="S3.T2.2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Task 1:</span> synthetic data for <span id="S3.T2.2.1.1.1.1.2" class="ltx_text ltx_font_bold">demographic bias mitigation</span>
</td>
</tr>
<tr id="S3.T2.2.1.2.2" class="ltx_tr">
<td id="S3.T2.2.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒBaseline: training only with CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>;</td>
</tr>
<tr id="S3.T2.2.1.3.3" class="ltx_tr">
<td id="S3.T2.2.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒMetrics: accuracy (for each demographic group);</td>
</tr>
<tr id="S3.T2.2.1.4.4" class="ltx_tr">
<td id="S3.T2.2.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒRanking: average vs SD of accuracy, see Section <a href="#S3.SS3" title="3.3 Metrics â€£ 3 FRCSyn Challenge: Setup â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> for more details.</td>
</tr>
<tr id="S3.T2.2.1.5.5" class="ltx_tr">
<td id="S3.T2.2.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.2.1.5.5.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 1.1:</span> training exclusively with <span id="S3.T2.2.1.5.5.1.2" class="ltx_text ltx_font_bold">synthetic</span> databases</td>
</tr>
<tr id="S3.T2.2.1.6.6" class="ltx_tr">
<td id="S3.T2.2.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒTrain: DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>;</td>
</tr>
<tr id="S3.T2.2.1.7.7" class="ltx_tr">
<td id="S3.T2.2.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒEval: BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</td>
</tr>
<tr id="S3.T2.2.1.8.8" class="ltx_tr">
<td id="S3.T2.2.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.2.1.8.8.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 1.2:</span> training with <span id="S3.T2.2.1.8.8.1.2" class="ltx_text ltx_font_bold">real and synthetic</span> databases</td>
</tr>
<tr id="S3.T2.2.1.9.9" class="ltx_tr">
<td id="S3.T2.2.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒTrain: CASIA-WebFace, FFHQ, DCFace, and GANDiffFace;</td>
</tr>
<tr id="S3.T2.2.1.10.10" class="ltx_tr">
<td id="S3.T2.2.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒEval: BUPT-BalancedFace.</td>
</tr>
<tr id="S3.T2.2.1.11.11" class="ltx_tr">
<td id="S3.T2.2.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">
<span id="S3.T2.2.1.11.11.1.1" class="ltx_text ltx_font_bold">Task 2:</span> synthetic data for <span id="S3.T2.2.1.11.11.1.2" class="ltx_text ltx_font_bold">overall performance improvement</span>
</td>
</tr>
<tr id="S3.T2.2.1.12.12" class="ltx_tr">
<td id="S3.T2.2.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒBaseline: training only with CASIA-WebFace and FFHQ;</td>
</tr>
<tr id="S3.T2.2.1.13.13" class="ltx_tr">
<td id="S3.T2.2.1.13.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒMetrics: accuracy (for each evaluation database);</td>
</tr>
<tr id="S3.T2.2.1.14.14" class="ltx_tr">
<td id="S3.T2.2.1.14.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒRanking: average accuracy.</td>
</tr>
<tr id="S3.T2.2.1.15.15" class="ltx_tr">
<td id="S3.T2.2.1.15.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.2.1.15.15.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 2.1:</span> training exclusively with <span id="S3.T2.2.1.15.15.1.2" class="ltx_text ltx_font_bold">synthetic</span> databases</td>
</tr>
<tr id="S3.T2.2.1.16.16" class="ltx_tr">
<td id="S3.T2.2.1.16.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒTrain: DCFace and GANDiffFace;</td>
</tr>
<tr id="S3.T2.2.1.17.17" class="ltx_tr">
<td id="S3.T2.2.1.17.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒEval: BUPT-BalancedFace, AgeDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and ROF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</td>
</tr>
<tr id="S3.T2.2.1.18.18" class="ltx_tr">
<td id="S3.T2.2.1.18.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.2.1.18.18.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 2.2:</span> training with <span id="S3.T2.2.1.18.18.1.2" class="ltx_text ltx_font_bold">real and synthetic</span> databases</td>
</tr>
<tr id="S3.T2.2.1.19.19" class="ltx_tr">
<td id="S3.T2.2.1.19.19.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Â Â â€ƒTrain: CASIA-WebFace, FFHQ, DCFace, and GANDiffFace;</td>
</tr>
<tr id="S3.T2.2.1.20.20" class="ltx_tr">
<td id="S3.T2.2.1.20.20.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Â Â â€ƒEval: BUPT-BalancedFace, AgeDB, CFP-FP, and ROF.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.4.2" class="ltx_text" style="font-size:90%;">Tasks and sub-tasks proposed in FRCSyn Challenge with their respective metrics and databases. SD = Standard Deviation. </span></figcaption>
</figure>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Task 1:</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">The first proposed task explores the use of synthetic data to address demographic biases in FR systems. To evaluate the proposed systems, we create lists of mated and non-mated comparisons derived from individuals in the BUPT-BalancedFace database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. We consider the eight demographic groups described in Section <a href="#S2" title="2 FRCSyn Challenge: Databases â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, obtained from the combination of four ethnic groups with two genders. For non-mated comparisons, we exclusively focus on pairs of individuals belonging to the same demographic group, as these are more relevant than non-mated comparisons between individuals of different demographic groups.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Task 2:</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">The second proposed task explores the application of synthetic data to enhance overall performance in FR under challenging conditions. To assess the proposed systems, we use lists of mated and non-mated comparisons derived from individuals included in the four databases indicated in Section <a href="#S2" title="2 FRCSyn Challenge: Databases â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, namely BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, AgeDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and ROF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Each database allows the evaluation of specific challenging conditions for FR, including diverse demographic groups, aging, pose variations, and presence of occlusions.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental protocol</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training:</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">The four sub-tasks proposed in the FRCSyn Challenge are mutually independent. This means that participants have the freedom to participate in any number of sub-tasks of their choice. For each selected sub-task, participants are expected to propose a FR system and train it twice: <span id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">i)</span> using authorized real databases only, <span id="S3.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">i.e.,</span> CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and <span id="S3.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">ii)</span> in accordance with the specific requirements of the chosen sub-task, as summarized in Table <a href="#S3.T2" title="Table 2 â€£ 3.1 Tasks â€£ 3 FRCSyn Challenge: Setup â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. According to this protocol, participants provide both the <span id="S3.SS2.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_italic">baseline system</span> and the <span id="S3.SS2.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_italic">proposed system</span> for the specific sub-task. The baseline system plays a critical role in evaluating the impact of synthetic data on training and serves as a reference point for comparing against the conventional practice of training solely with real databases. To maintain consistency, the baseline FR system, trained exclusively with real data, and the proposed FR system, trained according to the specifications of the selected sub-task, must have the same architecture.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation:</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">In each sub-task, participants are provided with comparison files containing both mated and non-mated comparisons, which are used to evaluate the performance of their proposed FR system. In Task 1 there is a single comparison file containing balanced comparisons of different demographic groups, while in Task 2 there are four comparison files, one for each real database considered. The evaluation process occurs twice for each sub-task to assess: <span id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">i)</span> the baseline system trained exclusively with real databases, and <span id="S3.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">ii)</span> the proposed system trained in accordance with the sub-task specifications. For the evaluation of each sub-task, participants must submit through Codalab platform two files per database (one for the baseline and one for the proposed system), including the score and the binary decision (mated/non-mated) for each comparison listed in the comparison files. The organizers retain the right to disqualify participants to uphold the integrity of the evaluation process if anomalous results are detected or if participants fail to adhere to the challengeâ€™s rules.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Restrictions:</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Participants have the freedom to choose the FR system for each task, provided that the systemâ€™s number of Floating Point Operations Per Second (FLOPs) does not exceed 25 GFLOPs. This threshold has been established to facilitate the exploration of innovative architectures and encourage the use of diverse models while preventing the dominance of excessively large models. Participants are also free to utilize their preferred training modality, with the requirement that only the specified databases are used for training. This means that no additional databases can be employed during the training phase, such as to establish verification thresholds. Generative models cannot be utilized to generate supplementary data. Participants are allowed to use non-face databases for pre-training purposes and employ traditional data augmentation techniques using the authorized training databases.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Metrics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">We evaluate FR systems using a protocol based on lists of mated and non-mated comparisons for each sub-task and database. From the binary decisions provided by participants, we calculate verification accuracy. This approach is straightforward and allows participants to choose the preferred threshold for their systems. Additionally, we calculate the gap to real (GAP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> as follows: <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\text{GAP}=\left(\text{REAL}-\text{SYN}\right)/\text{SYN}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mtext id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3a.cmml">GAP</mtext><mo id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml"><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml"><mo id="S3.SS3.p1.1.m1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml"><mtext id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2a.cmml">REAL</mtext><mo id="S3.SS3.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mtext id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3a.cmml">SYN</mtext></mrow><mo id="S3.SS3.p1.1.m1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS3.p1.1.m1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.2.cmml">/</mo><mtext id="S3.SS3.p1.1.m1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.3a.cmml">SYN</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><eq id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2"></eq><ci id="S3.SS3.p1.1.m1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><mtext id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">GAP</mtext></ci><apply id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"><divide id="S3.SS3.p1.1.m1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.2"></divide><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1"><minus id="S3.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.1"></minus><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2a.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2"><mtext id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2">REAL</mtext></ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3"><mtext id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3">SYN</mtext></ci></apply><ci id="S3.SS3.p1.1.m1.1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3"><mtext id="S3.SS3.p1.1.m1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3">SYN</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\text{GAP}=\left(\text{REAL}-\text{SYN}\right)/\text{SYN}</annotation></semantics></math>, with <span id="S3.SS3.p1.3.1" class="ltx_text ltx_markedasmath">REAL</span> representing the verification accuracy of the baseline system and <span id="S3.SS3.p1.3.2" class="ltx_text ltx_markedasmath">SYN</span> the verification accuracy of the proposed system, trained with synthetic (or real + synthetic) data.
Other metrics such as False Non-Match Rate (FNMR) at different operational points, which are very popular for the analysis of FR systems in real-world applications, can be computed from the scores provided by participants. Comprehensive evaluations of the proposed systems will be conducted in subsequent studies, including FNMRs and metrics for each demographic group and database used for evaluation. Next, we explain how participants are ranked in the different tasks.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Task 1:</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">To rank participants and determine the winners of Sub-Tasks 1.1 and 1.2, we closely examine the trade-off between the average (AVG) and standard deviation (SD) of the verification accuracy across the eight demographic groups defined in Section <a href="#S2" title="2 FRCSyn Challenge: Databases â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We define the trade-off metric (TO) as follows: <math id="S3.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\text{TO}=\text{AVG}-\text{SD}" display="inline"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mtext id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2a.cmml">TO</mtext><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mtext id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2a.cmml">AVG</mtext><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">âˆ’</mo><mtext id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3a.cmml">SD</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1"><eq id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2a.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2"><mtext id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2">TO</mtext></ci><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3"><minus id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.1"></minus><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2a.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2"><mtext id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.2">AVG</mtext></ci><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3a.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3"><mtext id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.3">SD</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">\text{TO}=\text{AVG}-\text{SD}</annotation></semantics></math>. This metric corresponds to plotting the average accuracy on the x-axis and the standard deviation on the y-axis in 2D space. We draw multiple 45-degree parallel lines to find the winning team whose performance falls to the far right side of these lines. With this proposed metric, we reward FR systems that achieve good levels of performance and fairness simultaneously, unlike common benchmarks based only on recognition performance. The standard deviation of verification accuracy across demographic groups is a common metric for assessing bias and should be reported by any work addressing demographic bias mitigation.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Task 2:</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">To rank participants and determine the winners of Sub-Tasks 2.1 and 2.2, we consider the average verification accuracy across the four databases used for evaluation, described in Section <a href="#S2" title="2 FRCSyn Challenge: Databases â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This approach allows us to evaluate four challenging aspects of FR simultaneously: <span id="S3.SS3.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">i)</span> pose variations, <span id="S3.SS3.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">ii)</span> aging, <span id="S3.SS3.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">iii)</span> presence of occlusions, and <span id="S3.SS3.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_italic">iv)</span> diverse demographic groups, providing a comprehensive evaluation of FR systems in real operational scenarios.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:154.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.7pt,15.7pt) scale(0.831123589897961,0.831123589897961) ;">
<table id="S3.T3.2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.2.1.1.1" class="ltx_tr">
<td id="S3.T3.2.1.1.1.1" class="ltx_td ltx_align_left"><span id="S3.T3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Team</span></td>
<td id="S3.T3.2.1.1.1.2" class="ltx_td ltx_align_center"><span id="S3.T3.2.1.1.1.2.1" class="ltx_text ltx_font_bold">Affiliations</span></td>
<td id="S3.T3.2.1.1.1.3" class="ltx_td ltx_align_left"><span id="S3.T3.2.1.1.1.3.1" class="ltx_text ltx_font_bold">Country</span></td>
<td id="S3.T3.2.1.1.1.4" class="ltx_td ltx_align_center"><span id="S3.T3.2.1.1.1.4.1" class="ltx_text ltx_font_bold">Sub-Tasks</span></td>
</tr>
<tr id="S3.T3.2.1.2.2" class="ltx_tr">
<td id="S3.T3.2.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">CBSR</td>
<td id="S3.T3.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">4-8</td>
<td id="S3.T3.2.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">China</td>
<td id="S3.T3.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">1.2 - 2.2</td>
</tr>
<tr id="S3.T3.2.1.3.3" class="ltx_tr">
<td id="S3.T3.2.1.3.3.1" class="ltx_td ltx_align_left">LENS</td>
<td id="S3.T3.2.1.3.3.2" class="ltx_td ltx_align_center">9</td>
<td id="S3.T3.2.1.3.3.3" class="ltx_td ltx_align_left">USA</td>
<td id="S3.T3.2.1.3.3.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S3.T3.2.1.4.4" class="ltx_tr">
<td id="S3.T3.2.1.4.4.1" class="ltx_td ltx_align_left">BOVIFOCR-UFPR</td>
<td id="S3.T3.2.1.4.4.2" class="ltx_td ltx_align_center">10-12</td>
<td id="S3.T3.2.1.4.4.3" class="ltx_td ltx_align_left">Brazil</td>
<td id="S3.T3.2.1.4.4.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S3.T3.2.1.5.5" class="ltx_tr">
<td id="S3.T3.2.1.5.5.1" class="ltx_td ltx_align_left">Idiap</td>
<td id="S3.T3.2.1.5.5.2" class="ltx_td ltx_align_center">13-15</td>
<td id="S3.T3.2.1.5.5.3" class="ltx_td ltx_align_left">Switzerland</td>
<td id="S3.T3.2.1.5.5.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S3.T3.2.1.6.6" class="ltx_tr">
<td id="S3.T3.2.1.6.6.1" class="ltx_td ltx_align_left">MeVer</td>
<td id="S3.T3.2.1.6.6.2" class="ltx_td ltx_align_center">16,17</td>
<td id="S3.T3.2.1.6.6.3" class="ltx_td ltx_align_left">Greece</td>
<td id="S3.T3.2.1.6.6.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S3.T3.2.1.7.7" class="ltx_tr">
<td id="S3.T3.2.1.7.7.1" class="ltx_td ltx_align_left">BioLab</td>
<td id="S3.T3.2.1.7.7.2" class="ltx_td ltx_align_center">18</td>
<td id="S3.T3.2.1.7.7.3" class="ltx_td ltx_align_left">Italy</td>
<td id="S3.T3.2.1.7.7.4" class="ltx_td ltx_align_center">2.1</td>
</tr>
<tr id="S3.T3.2.1.8.8" class="ltx_tr">
<td id="S3.T3.2.1.8.8.1" class="ltx_td ltx_align_left">Aphi</td>
<td id="S3.T3.2.1.8.8.2" class="ltx_td ltx_align_center">19</td>
<td id="S3.T3.2.1.8.8.3" class="ltx_td ltx_align_left">Spain</td>
<td id="S3.T3.2.1.8.8.4" class="ltx_td ltx_align_center">1.1 - 2.1</td>
</tr>
<tr id="S3.T3.2.1.9.9" class="ltx_tr">
<td id="S3.T3.2.1.9.9.1" class="ltx_td ltx_align_left ltx_border_b">
<table id="S3.T3.2.1.9.9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.2.1.9.9.1.1.1" class="ltx_tr">
<td id="S3.T3.2.1.9.9.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">UNICA-FRAUN-</td>
</tr>
<tr id="S3.T3.2.1.9.9.1.1.2" class="ltx_tr">
<td id="S3.T3.2.1.9.9.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">HOFER IGD</td>
</tr>
</table>
</td>
<td id="S3.T3.2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b">20-22</td>
<td id="S3.T3.2.1.9.9.3" class="ltx_td ltx_align_left ltx_border_b">
<table id="S3.T3.2.1.9.9.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.2.1.9.9.3.1.1" class="ltx_tr">
<td id="S3.T3.2.1.9.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Italy,</td>
</tr>
<tr id="S3.T3.2.1.9.9.3.1.2" class="ltx_tr">
<td id="S3.T3.2.1.9.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Germany</td>
</tr>
</table>
</td>
<td id="S3.T3.2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b">1.2 - 2.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.4.2" class="ltx_text" style="font-size:90%;">Description of the top-5 best teams ordered by the affiliation number. The numbers reported in the column â€˜affiliationsâ€™ refer to the ones provided in the title page.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>FRCSyn Challenge: Description of Systems</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">The FRCSyn Challenge received significant interest, with <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="67" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">67</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="integer" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">67</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">67</annotation></semantics></math> international teams correctly registered, comprising research groups from both industry and academia. These teams work in various domains, including FR, generative AI, and other aspects of computer vision, such as demographic fairness and domain adaptation. Finally, we received submissions from <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="integer" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">15</annotation></semantics></math> teams, receiving all sub-tasks high attention. The submitting teams are geographically distributed, with six teams from Europe, five teams from Asia, and four teams from America. Table <a href="#S3.T3" title="Table 3 â€£ Task 2: â€£ 3.3 Metrics â€£ 3 FRCSyn Challenge: Setup â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a general overview of the top-5 best teams, including the sub-tasks they participated. Next, we describe briefly the FR systems proposed for each team.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CBSR (Sub-Tasks 1.2 and 2.2):</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.4" class="ltx_p">They first trained a recognition model using CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. They extracted features for images in FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and clustered them using the DBSCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> for pseudo labels. Then, they removed the samples in FFHQ that are similar to CASIA-WebFace with a cosine similarity threshold of <math id="S4.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="0.6" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.m1.1b"><cn type="float" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.m1.1c">0.6</annotation></semantics></math> and merged the two to train a new model <math id="S4.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="S4.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.2.m2.1c">F</annotation></semantics></math>. They utilized <math id="S4.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S4.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.3.m3.1c">F</annotation></semantics></math> to de-overlap DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> from CASIA-WebFace and FFHQ. Subsequently, they conducted the intra-class clustering for all databases using DBSCAN (similarity threshold of <math id="S4.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.4.m4.1b"><cn type="float" id="S4.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.4.m4.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.4.m4.1c">0.3</annotation></semantics></math>) and removed the samples that were separate from the class center. They merged the cleansed databases and trained IResNet-100 with mask and sunglasses augmentation and AdaFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. They trained two recognition models using occlusion augmentation with 10% and 30% probability, respectively. They finally submitted the average similarity prediction of the two models. The threshold was determined by the 10-fold optimal threshold in the validation set.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.3" class="ltx_p">They constructed different validation sets for different evaluation tasks. For AgeDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, they randomly sampled pairs from the training databases. For CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, they added randomly positioned vertical bar masks to the images to simulate the self-occlusion due to pose. For ROF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, they detected face landmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and added mask and sunglasses to images. For BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, they randomly sampled pairs from DCFace with GANDiffFace because they have balanced demographic groups. All validation sets consisted of <math id="S4.SS0.SSS0.Px1.p2.1.m1.2" class="ltx_Math" alttext="12,000" display="inline"><semantics id="S4.SS0.SSS0.Px1.p2.1.m1.2a"><mrow id="S4.SS0.SSS0.Px1.p2.1.m1.2.3.2" xref="S4.SS0.SSS0.Px1.p2.1.m1.2.3.1.cmml"><mn id="S4.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">12</mn><mo id="S4.SS0.SSS0.Px1.p2.1.m1.2.3.2.1" xref="S4.SS0.SSS0.Px1.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px1.p2.1.m1.2.2" xref="S4.SS0.SSS0.Px1.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p2.1.m1.2b"><list id="S4.SS0.SSS0.Px1.p2.1.m1.2.3.1.cmml" xref="S4.SS0.SSS0.Px1.p2.1.m1.2.3.2"><cn type="integer" id="S4.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p2.1.m1.1.1">12</cn><cn type="integer" id="S4.SS0.SSS0.Px1.p2.1.m1.2.2.cmml" xref="S4.SS0.SSS0.Px1.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p2.1.m1.2c">12,000</annotation></semantics></math> image pairs containing <math id="S4.SS0.SSS0.Px1.p2.2.m2.2" class="ltx_Math" alttext="6,000" display="inline"><semantics id="S4.SS0.SSS0.Px1.p2.2.m2.2a"><mrow id="S4.SS0.SSS0.Px1.p2.2.m2.2.3.2" xref="S4.SS0.SSS0.Px1.p2.2.m2.2.3.1.cmml"><mn id="S4.SS0.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS0.SSS0.Px1.p2.2.m2.1.1.cmml">6</mn><mo id="S4.SS0.SSS0.Px1.p2.2.m2.2.3.2.1" xref="S4.SS0.SSS0.Px1.p2.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px1.p2.2.m2.2.2" xref="S4.SS0.SSS0.Px1.p2.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p2.2.m2.2b"><list id="S4.SS0.SSS0.Px1.p2.2.m2.2.3.1.cmml" xref="S4.SS0.SSS0.Px1.p2.2.m2.2.3.2"><cn type="integer" id="S4.SS0.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p2.2.m2.1.1">6</cn><cn type="integer" id="S4.SS0.SSS0.Px1.p2.2.m2.2.2.cmml" xref="S4.SS0.SSS0.Px1.p2.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p2.2.m2.2c">6,000</annotation></semantics></math> positive pairs and <math id="S4.SS0.SSS0.Px1.p2.3.m3.2" class="ltx_Math" alttext="6,000" display="inline"><semantics id="S4.SS0.SSS0.Px1.p2.3.m3.2a"><mrow id="S4.SS0.SSS0.Px1.p2.3.m3.2.3.2" xref="S4.SS0.SSS0.Px1.p2.3.m3.2.3.1.cmml"><mn id="S4.SS0.SSS0.Px1.p2.3.m3.1.1" xref="S4.SS0.SSS0.Px1.p2.3.m3.1.1.cmml">6</mn><mo id="S4.SS0.SSS0.Px1.p2.3.m3.2.3.2.1" xref="S4.SS0.SSS0.Px1.p2.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px1.p2.3.m3.2.2" xref="S4.SS0.SSS0.Px1.p2.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p2.3.m3.2b"><list id="S4.SS0.SSS0.Px1.p2.3.m3.2.3.1.cmml" xref="S4.SS0.SSS0.Px1.p2.3.m3.2.3.2"><cn type="integer" id="S4.SS0.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px1.p2.3.m3.1.1">6</cn><cn type="integer" id="S4.SS0.SSS0.Px1.p2.3.m3.2.2.cmml" xref="S4.SS0.SSS0.Px1.p2.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p2.3.m3.2c">6,000</annotation></semantics></math> negative pairs.
Code available<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/zws98/wacv_frcsyn" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/zws98/wacv_frcsyn</a></span></span></span>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LENS (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">For sub-tasks using only synthetic data (<span id="S4.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">i.e.,</span> 1.1 and 2.1), they observed that since the evaluation data are real databases, they needed an approach that makes the architecture robust to domain shifts between synthetic training data and real test data. For the same, they utilized the augmentations and AdaFace loss introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The augmentations like Crop, Photometric jittering, and Low-res scaling from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> helped to create more robust images similar to the real domain, effectively improving performance. They further enhanced the features by using an ensemble of two models, with different styles of augmenting databases like randomly selecting four from set of Identity, Spatial transformations, Brightness, Color, Contrast, Sharpness, Posterize, Solarize, AutoContrast, Equalize, Grayscale, ResizedCrop augmentations in each iteration, inspired from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The features of the two models were then combined to create a feature set of length <math id="S4.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><cn type="integer" id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">1024</annotation></semantics></math>. The same method was repeated for Sub-Tasks 1.2 and 2.2.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">After cropping and alignment, they divided their total data in the ratio <math id="S4.SS0.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="80:20" display="inline"><semantics id="S4.SS0.SSS0.Px2.p2.1.m1.1a"><mrow id="S4.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px2.p2.1.m1.1.1.2" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml">80</mn><mo lspace="0.278em" rspace="0.278em" id="S4.SS0.SSS0.Px2.p2.1.m1.1.1.1" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml">:</mo><mn id="S4.SS0.SSS0.Px2.p2.1.m1.1.1.3" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p2.1.m1.1b"><apply id="S4.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1"><ci id="S4.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1.1">:</ci><cn type="integer" id="S4.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1.2">80</cn><cn type="integer" id="S4.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p2.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p2.1.m1.1c">80:20</annotation></semantics></math> for training and validation, respectively. For training the baseline model and Sub-Tasks 1.2 and 2.2, they utilized CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> for the real database and skipped FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. They adopted the architecture of ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> (R50) backbone for all the sub-tasks for its lesser number of parameters and suitability when the size of the databases is not huge. They used AdaFace loss from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BOVIFOCR-UFPR (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.15" class="ltx_p">Inspired by Zhang <span id="S4.SS0.SSS0.Px3.p1.15.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, they reduced bias in Sub-Task 1.1 by creating a multi-task collaborative model composed of two backbones <math id="S4.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="B(x)" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.2.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.1.m1.1.2.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.2.3.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.1.m1.1.2.3.2.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2.cmml">(</mo><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.1.m1.1.2.3.2.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2"><times id="S4.SS0.SSS0.Px3.p1.1.m1.1.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2.1"></times><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.2.2">ğµ</ci><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.m1.1c">B(x)</annotation></semantics></math> and <math id="S4.SS0.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="R(e)" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px3.p1.2.m2.1.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.2.m2.1.2.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.2.m2.1.2.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S4.SS0.SSS0.Px3.p1.2.m2.1.2.3.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.2.m2.1.2.3.2.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2.cmml">(</mo><mi id="S4.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">e</mi><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.2.m2.1.2.3.2.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px3.p1.2.m2.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2"><times id="S4.SS0.SSS0.Px3.p1.2.m2.1.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2.1"></times><ci id="S4.SS0.SSS0.Px3.p1.2.m2.1.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.2.2">ğ‘…</ci><ci id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m2.1c">R(e)</annotation></semantics></math>, which produced the embeddings <math id="S4.SS0.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="e\in R^{512}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="S4.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">e</mi><mo id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">R</mi><mn id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml">512</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1"><in id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1"></in><ci id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2">ğ‘’</ci><apply id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.2">ğ‘…</ci><cn type="integer" id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.3">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.3.m3.1c">e\in R^{512}</annotation></semantics></math> and <math id="S4.SS0.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="g\in R^{256}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px3.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml">g</mi><mo id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.2" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.2.cmml">R</mi><mn id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.3" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.3.cmml">256</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1"><in id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.1"></in><ci id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.2">ğ‘”</ci><apply id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.2">ğ‘…</ci><cn type="integer" id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.3">256</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.4.m4.1c">g\in R^{256}</annotation></semantics></math>, respectively. This schema forced <math id="S4.SS0.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="B(x)" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.5.m5.1a"><mrow id="S4.SS0.SSS0.Px3.p1.5.m5.1.2" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.5.m5.1.2.2" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.5.m5.1.2.1" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2.1.cmml">â€‹</mo><mrow id="S4.SS0.SSS0.Px3.p1.5.m5.1.2.3.2" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2.cmml"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.5.m5.1.2.3.2.1" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2.cmml">(</mo><mi id="S4.SS0.SSS0.Px3.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.1.cmml">x</mi><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.5.m5.1.2.3.2.2" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.5.m5.1b"><apply id="S4.SS0.SSS0.Px3.p1.5.m5.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2"><times id="S4.SS0.SSS0.Px3.p1.5.m5.1.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2.1"></times><ci id="S4.SS0.SSS0.Px3.p1.5.m5.1.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.2.2">ğµ</ci><ci id="S4.SS0.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.1">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.5.m5.1c">B(x)</annotation></semantics></math> to learn less biased features across different ethnic groups. ResNet100 and ResNet18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> architectures were used as <math id="S4.SS0.SSS0.Px3.p1.6.m6.1" class="ltx_Math" alttext="B(x)" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.6.m6.1a"><mrow id="S4.SS0.SSS0.Px3.p1.6.m6.1.2" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.6.m6.1.2.2" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.6.m6.1.2.1" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2.1.cmml">â€‹</mo><mrow id="S4.SS0.SSS0.Px3.p1.6.m6.1.2.3.2" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2.cmml"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.6.m6.1.2.3.2.1" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2.cmml">(</mo><mi id="S4.SS0.SSS0.Px3.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.1.cmml">x</mi><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.6.m6.1.2.3.2.2" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.6.m6.1b"><apply id="S4.SS0.SSS0.Px3.p1.6.m6.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2"><times id="S4.SS0.SSS0.Px3.p1.6.m6.1.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2.1"></times><ci id="S4.SS0.SSS0.Px3.p1.6.m6.1.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.2.2">ğµ</ci><ci id="S4.SS0.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.1">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.6.m6.1c">B(x)</annotation></semantics></math> and <math id="S4.SS0.SSS0.Px3.p1.7.m7.1" class="ltx_Math" alttext="R(e)" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.7.m7.1a"><mrow id="S4.SS0.SSS0.Px3.p1.7.m7.1.2" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.7.m7.1.2.2" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.7.m7.1.2.1" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2.1.cmml">â€‹</mo><mrow id="S4.SS0.SSS0.Px3.p1.7.m7.1.2.3.2" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2.cmml"><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.7.m7.1.2.3.2.1" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2.cmml">(</mo><mi id="S4.SS0.SSS0.Px3.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.1.cmml">e</mi><mo stretchy="false" id="S4.SS0.SSS0.Px3.p1.7.m7.1.2.3.2.2" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.7.m7.1b"><apply id="S4.SS0.SSS0.Px3.p1.7.m7.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2"><times id="S4.SS0.SSS0.Px3.p1.7.m7.1.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2.1"></times><ci id="S4.SS0.SSS0.Px3.p1.7.m7.1.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.2.2">ğ‘…</ci><ci id="S4.SS0.SSS0.Px3.p1.7.m7.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.7.m7.1.1">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.7.m7.1c">R(e)</annotation></semantics></math>. Each training sample <math id="S4.SS0.SSS0.Px3.p1.8.m8.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.8.m8.1a"><msub id="S4.SS0.SSS0.Px3.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px3.p1.8.m8.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.8.m8.1.1.2" xref="S4.SS0.SSS0.Px3.p1.8.m8.1.1.2.cmml">x</mi><mi id="S4.SS0.SSS0.Px3.p1.8.m8.1.1.3" xref="S4.SS0.SSS0.Px3.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.8.m8.1b"><apply id="S4.SS0.SSS0.Px3.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.8.m8.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.8.m8.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.8.m8.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.8.m8.1.1.2">ğ‘¥</ci><ci id="S4.SS0.SSS0.Px3.p1.8.m8.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.8.m8.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.8.m8.1c">x_{i}</annotation></semantics></math> contained two labels <math id="S4.SS0.SSS0.Px3.p1.9.m9.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.9.m9.1a"><msub id="S4.SS0.SSS0.Px3.p1.9.m9.1.1" xref="S4.SS0.SSS0.Px3.p1.9.m9.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.9.m9.1.1.2" xref="S4.SS0.SSS0.Px3.p1.9.m9.1.1.2.cmml">y</mi><mi id="S4.SS0.SSS0.Px3.p1.9.m9.1.1.3" xref="S4.SS0.SSS0.Px3.p1.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.9.m9.1b"><apply id="S4.SS0.SSS0.Px3.p1.9.m9.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.9.m9.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.9.m9.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.9.m9.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.9.m9.1.1.2">ğ‘¦</ci><ci id="S4.SS0.SSS0.Px3.p1.9.m9.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.9.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.9.m9.1c">y_{i}</annotation></semantics></math> (to compute the subject loss <math id="S4.SS0.SSS0.Px3.p1.10.m10.1" class="ltx_Math" alttext="L_{S}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.10.m10.1a"><msub id="S4.SS0.SSS0.Px3.p1.10.m10.1.1" xref="S4.SS0.SSS0.Px3.p1.10.m10.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.10.m10.1.1.2" xref="S4.SS0.SSS0.Px3.p1.10.m10.1.1.2.cmml">L</mi><mi id="S4.SS0.SSS0.Px3.p1.10.m10.1.1.3" xref="S4.SS0.SSS0.Px3.p1.10.m10.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.10.m10.1b"><apply id="S4.SS0.SSS0.Px3.p1.10.m10.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.10.m10.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.10.m10.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.10.m10.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.10.m10.1.1.2">ğ¿</ci><ci id="S4.SS0.SSS0.Px3.p1.10.m10.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.10.m10.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.10.m10.1c">L_{S}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>), and <math id="S4.SS0.SSS0.Px3.p1.11.m11.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.11.m11.1a"><msub id="S4.SS0.SSS0.Px3.p1.11.m11.1.1" xref="S4.SS0.SSS0.Px3.p1.11.m11.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.11.m11.1.1.2" xref="S4.SS0.SSS0.Px3.p1.11.m11.1.1.2.cmml">w</mi><mi id="S4.SS0.SSS0.Px3.p1.11.m11.1.1.3" xref="S4.SS0.SSS0.Px3.p1.11.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.11.m11.1b"><apply id="S4.SS0.SSS0.Px3.p1.11.m11.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.11.m11.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.11.m11.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.11.m11.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.11.m11.1.1.2">ğ‘¤</ci><ci id="S4.SS0.SSS0.Px3.p1.11.m11.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.11.m11.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.11.m11.1c">w_{i}</annotation></semantics></math>, (to compute the ethnic group loss <math id="S4.SS0.SSS0.Px3.p1.12.m12.1" class="ltx_Math" alttext="L_{E}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.12.m12.1a"><msub id="S4.SS0.SSS0.Px3.p1.12.m12.1.1" xref="S4.SS0.SSS0.Px3.p1.12.m12.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.12.m12.1.1.2" xref="S4.SS0.SSS0.Px3.p1.12.m12.1.1.2.cmml">L</mi><mi id="S4.SS0.SSS0.Px3.p1.12.m12.1.1.3" xref="S4.SS0.SSS0.Px3.p1.12.m12.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.12.m12.1b"><apply id="S4.SS0.SSS0.Px3.p1.12.m12.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.12.m12.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.12.m12.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.12.m12.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.12.m12.1.1.2">ğ¿</ci><ci id="S4.SS0.SSS0.Px3.p1.12.m12.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.12.m12.1.1.3">ğ¸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.12.m12.1c">L_{E}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>). Their total loss was <math id="S4.SS0.SSS0.Px3.p1.13.m13.1" class="ltx_Math" alttext="L_{T}=\lambda_{S}L_{S}+\lambda_{E}L_{E}" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.13.m13.1a"><mrow id="S4.SS0.SSS0.Px3.p1.13.m13.1.1" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.cmml"><msub id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.2.cmml">L</mi><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.3.cmml">T</mi></msub><mo id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.1" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.1.cmml">=</mo><mrow id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.cmml"><mrow id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.cmml"><msub id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.2.cmml">Î»</mi><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.3.cmml">S</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.1" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.1.cmml">â€‹</mo><msub id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.cmml"><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.2.cmml">L</mi><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.3.cmml">S</mi></msub></mrow><mo id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.1" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.1.cmml">+</mo><mrow id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.cmml"><msub id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.2.cmml">Î»</mi><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.3.cmml">E</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.1" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.1.cmml">â€‹</mo><msub id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.cmml"><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.2" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.2.cmml">L</mi><mi id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.3" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.3.cmml">E</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.13.m13.1b"><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1"><eq id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.1"></eq><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.2">ğ¿</ci><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.2.3">ğ‘‡</ci></apply><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3"><plus id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.1"></plus><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2"><times id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.1"></times><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.2">ğœ†</ci><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.2.3">ğ‘†</ci></apply><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.2">ğ¿</ci><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.2.3.3">ğ‘†</ci></apply></apply><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3"><times id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.1"></times><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.2">ğœ†</ci><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.2.3">ğ¸</ci></apply><apply id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.2">ğ¿</ci><ci id="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.13.m13.1.1.3.3.3.3">ğ¸</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.13.m13.1c">L_{T}=\lambda_{S}L_{S}+\lambda_{E}L_{E}</annotation></semantics></math>. In Sub-Task 2.1 they employed ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as their loss function and Resnet100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> as the backbone, which is one of the top-performing models for deep FR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. They trained the network using the InsightFace library for <math id="S4.SS0.SSS0.Px3.p1.14.m14.1" class="ltx_Math" alttext="26" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.14.m14.1a"><mn id="S4.SS0.SSS0.Px3.p1.14.m14.1.1" xref="S4.SS0.SSS0.Px3.p1.14.m14.1.1.cmml">26</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.14.m14.1b"><cn type="integer" id="S4.SS0.SSS0.Px3.p1.14.m14.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.14.m14.1.1">26</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.14.m14.1c">26</annotation></semantics></math> epochs. The images used for training were augmented using Random Flip with a probability of <math id="S4.SS0.SSS0.Px3.p1.15.m15.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.15.m15.1a"><mn id="S4.SS0.SSS0.Px3.p1.15.m15.1.1" xref="S4.SS0.SSS0.Px3.p1.15.m15.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.15.m15.1b"><cn type="float" id="S4.SS0.SSS0.Px3.p1.15.m15.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.15.m15.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.15.m15.1c">0.5</annotation></semantics></math>. They used DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> as the training database in this sub-task, which provided the most accurate feature vectors on the validation set.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Idiap (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.2" class="ltx_p">The primary strategy for all tasks and sub-tasks was the fusion of features from two models, chosen for its potential to enhance accuracy and reduce bias. These models compute a mean feature vector via a feature fusion approach and undergo independent training to maximize the differences between them, to improve fusion results.
For preprocessing, RetinaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> was used to detect facial landmarks across all evaluation sets, and a similarity transform aligned five key facial points to a standard template before cropping and resizing images to <math id="S4.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="112\times 112" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">112</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml">112</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.2">112</cn><cn type="integer" id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.3">112</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.1.m1.1c">112\times 112</annotation></semantics></math> pixels, with pixel values normalized between <math id="S4.SS0.SSS0.Px4.p1.2.m2.2" class="ltx_Math" alttext="\left[-1,1\right]" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.2.m2.2a"><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.2.cmml"><mo id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.2.cmml">[</mo><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.cmml"><mo id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1a" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.cmml">âˆ’</mo><mn id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.2.cmml">1</mn></mrow><mo id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.3" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.2.cmml">,</mo><mn id="S4.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.cmml">1</mn><mo id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.4" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.2.m2.2b"><interval closure="closed" id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.2.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1"><apply id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1"><minus id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1"></minus><cn type="integer" id="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.2">1</cn></apply><cn type="integer" id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.2.m2.2c">\left[-1,1\right]</annotation></semantics></math>.</p>
</div>
<div id="S4.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p2.2" class="ltx_p">The models were based on iResNet-50 and iResNet-101 architectures. Training utilized specific databases for each track, with the iResNet-101 leveraging CosFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and the iResNet-50 using AdaFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Training ran for approximately <math id="S4.SS0.SSS0.Px4.p2.1.m1.2" class="ltx_Math" alttext="60,000" display="inline"><semantics id="S4.SS0.SSS0.Px4.p2.1.m1.2a"><mrow id="S4.SS0.SSS0.Px4.p2.1.m1.2.3.2" xref="S4.SS0.SSS0.Px4.p2.1.m1.2.3.1.cmml"><mn id="S4.SS0.SSS0.Px4.p2.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p2.1.m1.1.1.cmml">60</mn><mo id="S4.SS0.SSS0.Px4.p2.1.m1.2.3.2.1" xref="S4.SS0.SSS0.Px4.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px4.p2.1.m1.2.2" xref="S4.SS0.SSS0.Px4.p2.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p2.1.m1.2b"><list id="S4.SS0.SSS0.Px4.p2.1.m1.2.3.1.cmml" xref="S4.SS0.SSS0.Px4.p2.1.m1.2.3.2"><cn type="integer" id="S4.SS0.SSS0.Px4.p2.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p2.1.m1.1.1">60</cn><cn type="integer" id="S4.SS0.SSS0.Px4.p2.1.m1.2.2.cmml" xref="S4.SS0.SSS0.Px4.p2.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p2.1.m1.2c">60,000</annotation></semantics></math> batches of size <math id="S4.SS0.SSS0.Px4.p2.2.m2.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S4.SS0.SSS0.Px4.p2.2.m2.1a"><mn id="S4.SS0.SSS0.Px4.p2.2.m2.1.1" xref="S4.SS0.SSS0.Px4.p2.2.m2.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p2.2.m2.1b"><cn type="integer" id="S4.SS0.SSS0.Px4.p2.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px4.p2.2.m2.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p2.2.m2.1c">256</annotation></semantics></math>, with learning rate adjustments at set intervals. Training data underwent further preprocessing, including random cropping and augmentations in resolution, brightness, contrast, and saturation.
The final model checkpoint was taken after the last training step. A subset of the training data was used to determine the optimal threshold for maximizing verification accuracy, using a 10-fold cross-validation approach based on a random selection of identities and comparison pairs.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">MeVer (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p1.20" class="ltx_p">Their proposed system utilized the sub-center ArcFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to mitigate noise, which occurs in synthetic training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Comprising three CNNs, the proposed system adapted various margins within the ArcFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, aligning with relevant literature, indicating different demographic groups require different margin considerations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Final embeddings were obtained by combining the outputs of three ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> models each trained with <math id="S4.SS0.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.1.m1.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.1.m1.1c">4</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.2.m2.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.2.m2.1c">5</annotation></semantics></math>, and <math id="S4.SS0.SSS0.Px5.p1.3.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px5.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px5.p1.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.3.m3.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.3.m3.1c">5</annotation></semantics></math> subcenters and margins of <math id="S4.SS0.SSS0.Px5.p1.4.m4.1" class="ltx_Math" alttext="0.45" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px5.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px5.p1.4.m4.1.1.cmml">0.45</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.4.m4.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.4.m4.1.1">0.45</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.4.m4.1c">0.45</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px5.p1.5.m5.1" class="ltx_Math" alttext="0.47" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px5.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px5.p1.5.m5.1.1.cmml">0.47</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.5.m5.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.5.m5.1.1">0.47</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.5.m5.1c">0.47</annotation></semantics></math>, and <math id="S4.SS0.SSS0.Px5.p1.6.m6.1" class="ltx_Math" alttext="0.50" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px5.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px5.p1.6.m6.1.1.cmml">0.50</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.6.m6.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.6.m6.1.1">0.50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.6.m6.1c">0.50</annotation></semantics></math>. Prediction involved computing the Euclidean distance between feature vectors, utilizing thresholds of <math id="S4.SS0.SSS0.Px5.p1.7.m7.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px5.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px5.p1.7.m7.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.7.m7.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.7.m7.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.7.m7.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.7.m7.1c">1.5</annotation></semantics></math> and <math id="S4.SS0.SSS0.Px5.p1.8.m8.1" class="ltx_Math" alttext="1.35" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.8.m8.1a"><mn id="S4.SS0.SSS0.Px5.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px5.p1.8.m8.1.1.cmml">1.35</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.8.m8.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.8.m8.1.1">1.35</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.8.m8.1c">1.35</annotation></semantics></math> for tasks involving synthetic-only and mixed synthetic-real training data, respectively. The training procedure involved a batch size of <math id="S4.SS0.SSS0.Px5.p1.9.m9.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.9.m9.1a"><mn id="S4.SS0.SSS0.Px5.p1.9.m9.1.1" xref="S4.SS0.SSS0.Px5.p1.9.m9.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.9.m9.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.9.m9.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.9.m9.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.9.m9.1c">256</annotation></semantics></math>, an initial learning rate of <math id="S4.SS0.SSS0.Px5.p1.10.m10.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.10.m10.1a"><mn id="S4.SS0.SSS0.Px5.p1.10.m10.1.1" xref="S4.SS0.SSS0.Px5.p1.10.m10.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.10.m10.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.10.m10.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.10.m10.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.10.m10.1c">0.1</annotation></semantics></math> that decayed by a factor of <math id="S4.SS0.SSS0.Px5.p1.11.m11.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.11.m11.1a"><mn id="S4.SS0.SSS0.Px5.p1.11.m11.1.1" xref="S4.SS0.SSS0.Px5.p1.11.m11.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.11.m11.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.11.m11.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.11.m11.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.11.m11.1c">10</annotation></semantics></math> at steps <math id="S4.SS0.SSS0.Px5.p1.12.m12.1" class="ltx_Math" alttext="75" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.12.m12.1a"><mn id="S4.SS0.SSS0.Px5.p1.12.m12.1.1" xref="S4.SS0.SSS0.Px5.p1.12.m12.1.1.cmml">75</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.12.m12.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.12.m12.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.12.m12.1.1">75</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.12.m12.1c">75</annotation></semantics></math>k, <math id="S4.SS0.SSS0.Px5.p1.13.m13.1" class="ltx_Math" alttext="127.5" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.13.m13.1a"><mn id="S4.SS0.SSS0.Px5.p1.13.m13.1.1" xref="S4.SS0.SSS0.Px5.p1.13.m13.1.1.cmml">127.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.13.m13.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.13.m13.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.13.m13.1.1">127.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.13.m13.1c">127.5</annotation></semantics></math>k, and <math id="S4.SS0.SSS0.Px5.p1.14.m14.1" class="ltx_Math" alttext="165" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.14.m14.1a"><mn id="S4.SS0.SSS0.Px5.p1.14.m14.1.1" xref="S4.SS0.SSS0.Px5.p1.14.m14.1.1.cmml">165</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.14.m14.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.14.m14.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.14.m14.1.1">165</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.14.m14.1c">165</annotation></semantics></math>k over <math id="S4.SS0.SSS0.Px5.p1.15.m15.1" class="ltx_Math" alttext="180" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.15.m15.1a"><mn id="S4.SS0.SSS0.Px5.p1.15.m15.1.1" xref="S4.SS0.SSS0.Px5.p1.15.m15.1.1.cmml">180</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.15.m15.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.15.m15.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.15.m15.1.1">180</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.15.m15.1c">180</annotation></semantics></math>k total training steps. Optimizing with stochastic gradient descent (SGD), momentum was set at <math id="S4.SS0.SSS0.Px5.p1.16.m16.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.16.m16.1a"><mn id="S4.SS0.SSS0.Px5.p1.16.m16.1.1" xref="S4.SS0.SSS0.Px5.p1.16.m16.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.16.m16.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.16.m16.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.16.m16.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.16.m16.1c">0.9</annotation></semantics></math>, and weight decay at <math id="S4.SS0.SSS0.Px5.p1.17.m17.1" class="ltx_Math" alttext="0.0005" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.17.m17.1a"><mn id="S4.SS0.SSS0.Px5.p1.17.m17.1.1" xref="S4.SS0.SSS0.Px5.p1.17.m17.1.1.cmml">0.0005</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.17.m17.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.17.m17.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.17.m17.1.1">0.0005</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.17.m17.1c">0.0005</annotation></semantics></math>. Data preprocessing involved an MTCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, resizing all data to <math id="S4.SS0.SSS0.Px5.p1.18.m18.1" class="ltx_Math" alttext="112\times 112" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.18.m18.1a"><mrow id="S4.SS0.SSS0.Px5.p1.18.m18.1.1" xref="S4.SS0.SSS0.Px5.p1.18.m18.1.1.cmml"><mn id="S4.SS0.SSS0.Px5.p1.18.m18.1.1.2" xref="S4.SS0.SSS0.Px5.p1.18.m18.1.1.2.cmml">112</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS0.SSS0.Px5.p1.18.m18.1.1.1" xref="S4.SS0.SSS0.Px5.p1.18.m18.1.1.1.cmml">Ã—</mo><mn id="S4.SS0.SSS0.Px5.p1.18.m18.1.1.3" xref="S4.SS0.SSS0.Px5.p1.18.m18.1.1.3.cmml">112</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.18.m18.1b"><apply id="S4.SS0.SSS0.Px5.p1.18.m18.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.18.m18.1.1"><times id="S4.SS0.SSS0.Px5.p1.18.m18.1.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.18.m18.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px5.p1.18.m18.1.1.2.cmml" xref="S4.SS0.SSS0.Px5.p1.18.m18.1.1.2">112</cn><cn type="integer" id="S4.SS0.SSS0.Px5.p1.18.m18.1.1.3.cmml" xref="S4.SS0.SSS0.Px5.p1.18.m18.1.1.3">112</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.18.m18.1c">112\times 112</annotation></semantics></math>, and employing color jittering and random horizontal flip augmentations. Task-wise, both synthetic databases were utilized, while the CASIA-WebFace database was specific to Sub-Tasks 1.2 and 2.2. Validation included <math id="S4.SS0.SSS0.Px5.p1.19.m19.1" class="ltx_Math" alttext="800" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.19.m19.1a"><mn id="S4.SS0.SSS0.Px5.p1.19.m19.1.1" xref="S4.SS0.SSS0.Px5.p1.19.m19.1.1.cmml">800</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.19.m19.1b"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.19.m19.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.19.m19.1.1">800</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.19.m19.1c">800</annotation></semantics></math> synthetic identities and <math id="S4.SS0.SSS0.Px5.p1.20.m20.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.20.m20.2a"><mrow id="S4.SS0.SSS0.Px5.p1.20.m20.2.3.2" xref="S4.SS0.SSS0.Px5.p1.20.m20.2.3.1.cmml"><mn id="S4.SS0.SSS0.Px5.p1.20.m20.1.1" xref="S4.SS0.SSS0.Px5.p1.20.m20.1.1.cmml">1</mn><mo id="S4.SS0.SSS0.Px5.p1.20.m20.2.3.2.1" xref="S4.SS0.SSS0.Px5.p1.20.m20.2.3.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px5.p1.20.m20.2.2" xref="S4.SS0.SSS0.Px5.p1.20.m20.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.20.m20.2b"><list id="S4.SS0.SSS0.Px5.p1.20.m20.2.3.1.cmml" xref="S4.SS0.SSS0.Px5.p1.20.m20.2.3.2"><cn type="integer" id="S4.SS0.SSS0.Px5.p1.20.m20.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.20.m20.1.1">1</cn><cn type="integer" id="S4.SS0.SSS0.Px5.p1.20.m20.2.2.cmml" xref="S4.SS0.SSS0.Px5.p1.20.m20.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.20.m20.2c">1,000</annotation></semantics></math> identities from CASIA-WebFace for the tasks involving synthetic-only and mixed synthetic-real databases, respectively. Code available<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/gsarridis/fair-face-verification-with-synthetic-data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/gsarridis/fair-face-verification-with-synthetic-data</a></span></span></span>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BioLab (Sub-Task 2.1):</h4>

<div id="S4.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px6.p1.2" class="ltx_p">The model selected for the Sub-Task 2.1 is a customized ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which had been trained using the margin-based AdaFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, whose advantage is its resilience when training data contain low-quality images with unrecognizable faces. According to their assumption, this ensured that the modelâ€™s performance remained unaffected when exposed to GAN-related visual glitches and artifacts. Their baseline model was trained employing the CASIA-WebFace database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Differently, the proposed model employed both DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. In both cases they built the validation set by generating couples from the first classes of the training sets, which were excluded from training.
They applied data augmentation on the training set. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, the pipeline consisted of random horizontal flips, random crop-and-resize, and random color jittering on saturation and value channels. Each transformation had a probability of <math id="S4.SS0.SSS0.Px6.p1.1.m1.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.SS0.SSS0.Px6.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px6.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px6.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml">20</mn><mo id="S4.SS0.SSS0.Px6.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px6.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px6.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px6.p1.1.m1.1c">20\%</annotation></semantics></math> of being applied. Finally, the model was optimized with cross entropy loss and SGD with an initial learning rate of <math id="S4.SS0.SSS0.Px6.p1.2.m2.1" class="ltx_Math" alttext="0.05" display="inline"><semantics id="S4.SS0.SSS0.Px6.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px6.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px6.p1.2.m2.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px6.p1.2.m2.1b"><cn type="float" id="S4.SS0.SSS0.Px6.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px6.p1.2.m2.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px6.p1.2.m2.1c">0.05</annotation></semantics></math>. Learning rate scheduling was employed to improve training stability.
For face verification, the dissimilarity between embeddings was measured employing the cosine distance. Its threshold was computed to maximize the accuracy on the validation set (<span id="S4.SS0.SSS0.Px6.p1.2.1" class="ltx_text ltx_font_italic">i.e.,</span> using a non-overlapping partition of the training databases), following the same idea described in the LFW protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Code available<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/ndido98/frcsyn" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ndido98/frcsyn</a></span></span></span>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Aphi (Sub-Tasks 1.1 and 2.1):</h4>

<div id="S4.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px7.p1.12" class="ltx_p">In their approach, they used an EfficientNetV2-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> architecture to produce a <math id="S4.SS0.SSS0.Px7.p1.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px7.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.1.m1.1b"><cn type="integer" id="S4.SS0.SSS0.Px7.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.1.m1.1c">512</annotation></semantics></math>-D deep embedding trained with ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> loss function. They modified the backbone network by reducing the first layerâ€™s stride from <math id="S4.SS0.SSS0.Px7.p1.2.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px7.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.2.m2.1b"><cn type="integer" id="S4.SS0.SSS0.Px7.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.2.m2.1c">2</annotation></semantics></math> to <math id="S4.SS0.SSS0.Px7.p1.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px7.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.3.m3.1b"><cn type="integer" id="S4.SS0.SSS0.Px7.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.3.m3.1c">1</annotation></semantics></math> to enhance the preservation of spatial features. The output of the backbone network was projected with a <math id="S4.SS0.SSS0.Px7.p1.4.m4.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px7.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.1.cmml">Ã—</mo><mn id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.3" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1"><times id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.2">1</cn><cn type="integer" id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.4.m4.1c">1\times 1</annotation></semantics></math> convolutional layer and normalized with batch normalization. These features were flattened and fed into a fully connected layer which produces the deep embedding. The weights of the model were optimized through the SGD algorithm with a momentum of <math id="S4.SS0.SSS0.Px7.p1.5.m5.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px7.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.5.m5.1b"><cn type="float" id="S4.SS0.SSS0.Px7.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.5.m5.1c">0.9</annotation></semantics></math> and a weight decay of <math id="S4.SS0.SSS0.Px7.p1.6.m6.1" class="ltx_Math" alttext="1e^{-4}" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.6.m6.1a"><mrow id="S4.SS0.SSS0.Px7.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.2" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.1" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.1.cmml">â€‹</mo><msup id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.2" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.2.cmml">e</mi><mrow id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3.cmml"><mo id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3a" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3.cmml">âˆ’</mo><mn id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3.2" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.6.m6.1b"><apply id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1"><times id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.2">1</cn><apply id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3">superscript</csymbol><ci id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.2">ğ‘’</ci><apply id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3"><minus id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3.1.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3"></minus><cn type="integer" id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3.2.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.6.m6.1c">1e^{-4}</annotation></semantics></math> during <math id="S4.SS0.SSS0.Px7.p1.7.m7.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px7.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px7.p1.7.m7.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.7.m7.1b"><cn type="integer" id="S4.SS0.SSS0.Px7.p1.7.m7.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.7.m7.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.7.m7.1c">20</annotation></semantics></math> epochs and a learning rate starting at <math id="S4.SS0.SSS0.Px7.p1.8.m8.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.8.m8.1a"><mn id="S4.SS0.SSS0.Px7.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px7.p1.8.m8.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.8.m8.1b"><cn type="float" id="S4.SS0.SSS0.Px7.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.8.m8.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.8.m8.1c">0.1</annotation></semantics></math> and decayed through a polynomial scheduler.
The model was trained with the images aligned using a proprietary algorithm, resized to <math id="S4.SS0.SSS0.Px7.p1.9.m9.1" class="ltx_Math" alttext="112\times 112" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.9.m9.1a"><mrow id="S4.SS0.SSS0.Px7.p1.9.m9.1.1" xref="S4.SS0.SSS0.Px7.p1.9.m9.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.9.m9.1.1.2" xref="S4.SS0.SSS0.Px7.p1.9.m9.1.1.2.cmml">112</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS0.SSS0.Px7.p1.9.m9.1.1.1" xref="S4.SS0.SSS0.Px7.p1.9.m9.1.1.1.cmml">Ã—</mo><mn id="S4.SS0.SSS0.Px7.p1.9.m9.1.1.3" xref="S4.SS0.SSS0.Px7.p1.9.m9.1.1.3.cmml">112</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.9.m9.1b"><apply id="S4.SS0.SSS0.Px7.p1.9.m9.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.9.m9.1.1"><times id="S4.SS0.SSS0.Px7.p1.9.m9.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.9.m9.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px7.p1.9.m9.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.9.m9.1.1.2">112</cn><cn type="integer" id="S4.SS0.SSS0.Px7.p1.9.m9.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.9.m9.1.1.3">112</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.9.m9.1c">112\times 112</annotation></semantics></math>, and normalized in the range of <math id="S4.SS0.SSS0.Px7.p1.10.m10.1" class="ltx_Math" alttext="-1" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.10.m10.1a"><mrow id="S4.SS0.SSS0.Px7.p1.10.m10.1.1" xref="S4.SS0.SSS0.Px7.p1.10.m10.1.1.cmml"><mo id="S4.SS0.SSS0.Px7.p1.10.m10.1.1a" xref="S4.SS0.SSS0.Px7.p1.10.m10.1.1.cmml">âˆ’</mo><mn id="S4.SS0.SSS0.Px7.p1.10.m10.1.1.2" xref="S4.SS0.SSS0.Px7.p1.10.m10.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.10.m10.1b"><apply id="S4.SS0.SSS0.Px7.p1.10.m10.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.10.m10.1.1"><minus id="S4.SS0.SSS0.Px7.p1.10.m10.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.10.m10.1.1"></minus><cn type="integer" id="S4.SS0.SSS0.Px7.p1.10.m10.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.10.m10.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.10.m10.1c">-1</annotation></semantics></math> to <math id="S4.SS0.SSS0.Px7.p1.11.m11.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.11.m11.1a"><mn id="S4.SS0.SSS0.Px7.p1.11.m11.1.1" xref="S4.SS0.SSS0.Px7.p1.11.m11.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.11.m11.1b"><cn type="integer" id="S4.SS0.SSS0.Px7.p1.11.m11.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.11.m11.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.11.m11.1c">1</annotation></semantics></math>. To prevent overfitting, they applied data augmentation techniques during training, including Gaussian Blur, Random Scale, Hue-Saturation adjustments, and Horizontal Flip transformations as well as dropout with a rate of <math id="S4.SS0.SSS0.Px7.p1.12.m12.1" class="ltx_Math" alttext="0.2" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.12.m12.1a"><mn id="S4.SS0.SSS0.Px7.p1.12.m12.1.1" xref="S4.SS0.SSS0.Px7.p1.12.m12.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.12.m12.1b"><cn type="float" id="S4.SS0.SSS0.Px7.p1.12.m12.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.12.m12.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.12.m12.1c">0.2</annotation></semantics></math> before the deep embedding projection.
To train the baseline model, they made use of CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and for their proposed model, they employed the synthetic database DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px8" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">UNICA-FRAUNHOFER IGD (Sub-Tasks 1.2 and 2.2):</h4>

<div id="S4.SS0.SSS0.Px8.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px8.p1.13" class="ltx_p">The presented solution utilized ResNet100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> as network architecture as it is one of the most widely used architectures in state-of-the-art FR approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Training and validation images were aligned and cropped to <math id="S4.SS0.SSS0.Px8.p1.1.m1.1" class="ltx_Math" alttext="112\times 112" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px8.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px8.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1.2.cmml">112</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS0.SSS0.Px8.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS0.SSS0.Px8.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1.3.cmml">112</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px8.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px8.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px8.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1.2">112</cn><cn type="integer" id="S4.SS0.SSS0.Px8.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1.3">112</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.1.m1.1c">112\times 112</annotation></semantics></math> using five-points landmarks extracted with MTCNN. The networkâ€™s outputs were <math id="S4.SS0.SSS0.Px8.p1.2.m2.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px8.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px8.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.2.m2.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.2.m2.1c">512</annotation></semantics></math>-D feature representations.
The presented solution, submitted to Sub-Tasks 1.2 and 2.2, relies on training the ResNet100 network with CosFace as a loss function with a margin penalty value of <math id="S4.SS0.SSS0.Px8.p1.3.m3.1" class="ltx_Math" alttext="0.35" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px8.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px8.p1.3.m3.1.1.cmml">0.35</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.3.m3.1b"><cn type="float" id="S4.SS0.SSS0.Px8.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.3.m3.1.1">0.35</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.3.m3.1c">0.35</annotation></semantics></math> and a scale parameter of <math id="S4.SS0.SSS0.Px8.p1.4.m4.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px8.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px8.p1.4.m4.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.4.m4.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.4.m4.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.4.m4.1c">64</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. The model was trained for <math id="S4.SS0.SSS0.Px8.p1.5.m5.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px8.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px8.p1.5.m5.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.5.m5.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.5.m5.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.5.m5.1c">40</annotation></semantics></math> epochs with a batch size of <math id="S4.SS0.SSS0.Px8.p1.6.m6.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px8.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px8.p1.6.m6.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.6.m6.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.6.m6.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.6.m6.1c">512</annotation></semantics></math> and an initial learning rate of <math id="S4.SS0.SSS0.Px8.p1.7.m7.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px8.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px8.p1.7.m7.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.7.m7.1b"><cn type="float" id="S4.SS0.SSS0.Px8.p1.7.m7.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.7.m7.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.7.m7.1c">0.1</annotation></semantics></math>. The learning rate was divided by <math id="S4.SS0.SSS0.Px8.p1.8.m8.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.8.m8.1a"><mn id="S4.SS0.SSS0.Px8.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px8.p1.8.m8.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.8.m8.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.8.m8.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.8.m8.1c">10</annotation></semantics></math> after <math id="S4.SS0.SSS0.Px8.p1.9.m9.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.9.m9.1a"><mn id="S4.SS0.SSS0.Px8.p1.9.m9.1.1" xref="S4.SS0.SSS0.Px8.p1.9.m9.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.9.m9.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.9.m9.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.9.m9.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.9.m9.1c">10</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px8.p1.10.m10.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.10.m10.1a"><mn id="S4.SS0.SSS0.Px8.p1.10.m10.1.1" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.10.m10.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.10.m10.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.10.m10.1c">22</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px8.p1.11.m11.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.11.m11.1a"><mn id="S4.SS0.SSS0.Px8.p1.11.m11.1.1" xref="S4.SS0.SSS0.Px8.p1.11.m11.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.11.m11.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.11.m11.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.11.m11.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.11.m11.1c">30</annotation></semantics></math>, and <math id="S4.SS0.SSS0.Px8.p1.12.m12.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.12.m12.1a"><mn id="S4.SS0.SSS0.Px8.p1.12.m12.1.1" xref="S4.SS0.SSS0.Px8.p1.12.m12.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.12.m12.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.12.m12.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.12.m12.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.12.m12.1c">40</annotation></semantics></math> epochs. During the training phase the training databases, CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, provided by the competition organizers, were merged into one database with a total number of <math id="S4.SS0.SSS0.Px8.p1.13.m13.1" class="ltx_Math" alttext="20.572" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.13.m13.1a"><mn id="S4.SS0.SSS0.Px8.p1.13.m13.1.1" xref="S4.SS0.SSS0.Px8.p1.13.m13.1.1.cmml">20.572</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.13.m13.1b"><cn type="float" id="S4.SS0.SSS0.Px8.p1.13.m13.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.13.m13.1.1">20.572</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.13.m13.1c">20.572</annotation></semantics></math> identities. During the training phase, an extensive set of data augmentation operations based on RandAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> was applied only to the synthetic samples. The real samples were only augmented with horizontal flipping. Code available<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/atzoriandrea/FRCSyn" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/atzoriandrea/FRCSyn</a></span></span></span>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T4.2" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:120pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.9pt,3.0pt) scale(0.952117246517242,0.952117246517242) ;">
<table id="S4.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6"><span id="S4.T4.2.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 1.1 (Bias Mitigation): Synthetic Data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.2.1" class="ltx_tr">
<th id="S4.T4.2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.2.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T4.2.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.2.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T4.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TO [%]</span></td>
<td id="S4.T4.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T4.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SD [%]</span></td>
<td id="S4.T4.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T4.2.1.3.2" class="ltx_tr">
<th id="S4.T4.2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.2.1.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1</span></th>
<th id="S4.T4.2.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.2.1.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LENS</span></th>
<td id="S4.T4.2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">92.25</span></td>
<td id="S4.T4.2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">93.54</span></td>
<td id="S4.T4.2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.3.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1.28</span></td>
<td id="S4.T4.2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.1.3.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">-0.74</span></td>
</tr>
<tr id="S4.T4.2.1.4.3" class="ltx_tr">
<th id="S4.T4.2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.2.1.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T4.2.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.2.1.4.3.2.1" class="ltx_text" style="font-size:80%;">Idiap</span></th>
<td id="S4.T4.2.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.4.3.3.1" class="ltx_text" style="font-size:80%;">91.88</span></td>
<td id="S4.T4.2.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.4.3.4.1" class="ltx_text" style="font-size:80%;">93.41</span></td>
<td id="S4.T4.2.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.4.3.5.1" class="ltx_text" style="font-size:80%;">1.53</span></td>
<td id="S4.T4.2.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.4.3.6.1" class="ltx_text" style="font-size:80%;">-3.80</span></td>
</tr>
<tr id="S4.T4.2.1.5.4" class="ltx_tr">
<th id="S4.T4.2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.2.1.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T4.2.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.2.1.5.4.2.1" class="ltx_text" style="font-size:80%;">BOVIFOCR</span></th>
<td id="S4.T4.2.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.3.1" class="ltx_text" style="font-size:80%;">90.51</span></td>
<td id="S4.T4.2.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.4.1" class="ltx_text" style="font-size:80%;">92.35</span></td>
<td id="S4.T4.2.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.5.1" class="ltx_text" style="font-size:80%;">1.84</span></td>
<td id="S4.T4.2.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.5.4.6.1" class="ltx_text" style="font-size:80%;">4.23</span></td>
</tr>
<tr id="S4.T4.2.1.6.5" class="ltx_tr">
<th id="S4.T4.2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.2.1.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T4.2.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.2.1.6.5.2.1" class="ltx_text" style="font-size:80%;">MeVer</span></th>
<td id="S4.T4.2.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.6.5.3.1" class="ltx_text" style="font-size:80%;">87.51</span></td>
<td id="S4.T4.2.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.6.5.4.1" class="ltx_text" style="font-size:80%;">89.62</span></td>
<td id="S4.T4.2.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.6.5.5.1" class="ltx_text" style="font-size:80%;">2.11</span></td>
<td id="S4.T4.2.1.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T4.2.1.6.5.6.1" class="ltx_text" style="font-size:80%;">5.68</span></td>
</tr>
<tr id="S4.T4.2.1.7.6" class="ltx_tr">
<th id="S4.T4.2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.2.1.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T4.2.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.2.1.7.6.2.1" class="ltx_text" style="font-size:80%;">Aphi</span></th>
<td id="S4.T4.2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.2.1.7.6.3.1" class="ltx_text" style="font-size:80%;">82.24</span></td>
<td id="S4.T4.2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.2.1.7.6.4.1" class="ltx_text" style="font-size:80%;">86.01</span></td>
<td id="S4.T4.2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.2.1.7.6.5.1" class="ltx_text" style="font-size:80%;">3.77</span></td>
<td id="S4.T4.2.1.7.6.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.2.1.7.6.6.1" class="ltx_text" style="font-size:80%;">0.84</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T4.3" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:120pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.9pt,3.0pt) scale(0.952117246517242,0.952117246517242) ;">
<table id="S4.T4.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.1.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6"><span id="S4.T4.3.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 1.2 (Bias Mitigation): Synthetic + Real Data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.1.2.1" class="ltx_tr">
<th id="S4.T4.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.3.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T4.3.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.3.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T4.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TO [%]</span></td>
<td id="S4.T4.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T4.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SD [%]</span></td>
<td id="S4.T4.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T4.3.1.3.2" class="ltx_tr">
<th id="S4.T4.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.3.1.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1</span></th>
<th id="S4.T4.3.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.3.1.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">CBSR</span></th>
<td id="S4.T4.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">95.25</span></td>
<td id="S4.T4.3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">96.45</span></td>
<td id="S4.T4.3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.3.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1.20</span></td>
<td id="S4.T4.3.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.3.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">-2.10</span></td>
</tr>
<tr id="S4.T4.3.1.4.3" class="ltx_tr">
<th id="S4.T4.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.3.1.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T4.3.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.3.1.4.3.2.1" class="ltx_text" style="font-size:80%;">LENS</span></th>
<td id="S4.T4.3.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.3.3.1" class="ltx_text" style="font-size:80%;">95.24</span></td>
<td id="S4.T4.3.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.3.4.1" class="ltx_text" style="font-size:80%;">96.35</span></td>
<td id="S4.T4.3.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.3.5.1" class="ltx_text" style="font-size:80%;">1.11</span></td>
<td id="S4.T4.3.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.4.3.6.1" class="ltx_text" style="font-size:80%;">-5.67</span></td>
</tr>
<tr id="S4.T4.3.1.5.4" class="ltx_tr">
<th id="S4.T4.3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.3.1.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T4.3.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.3.1.5.4.2.1" class="ltx_text" style="font-size:80%;">MeVer</span></th>
<td id="S4.T4.3.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.5.4.3.1" class="ltx_text" style="font-size:80%;">93.87</span></td>
<td id="S4.T4.3.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.5.4.4.1" class="ltx_text" style="font-size:80%;">95.44</span></td>
<td id="S4.T4.3.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.5.4.5.1" class="ltx_text" style="font-size:80%;">1.56</span></td>
<td id="S4.T4.3.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.5.4.6.1" class="ltx_text" style="font-size:80%;">-0.78</span></td>
</tr>
<tr id="S4.T4.3.1.6.5" class="ltx_tr">
<th id="S4.T4.3.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.3.1.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T4.3.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.3.1.6.5.2.1" class="ltx_text" style="font-size:80%;">BOVIFOCR</span></th>
<td id="S4.T4.3.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.6.5.3.1" class="ltx_text" style="font-size:80%;">93.15</span></td>
<td id="S4.T4.3.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.6.5.4.1" class="ltx_text" style="font-size:80%;">95.04</span></td>
<td id="S4.T4.3.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.6.5.5.1" class="ltx_text" style="font-size:80%;">1.89</span></td>
<td id="S4.T4.3.1.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T4.3.1.6.5.6.1" class="ltx_text" style="font-size:80%;">1.28</span></td>
</tr>
<tr id="S4.T4.3.1.7.6" class="ltx_tr">
<th id="S4.T4.3.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.3.1.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T4.3.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.3.1.7.6.2.1" class="ltx_text" style="font-size:80%;">UNICA</span></th>
<td id="S4.T4.3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.3.1.7.6.3.1" class="ltx_text" style="font-size:80%;">91.03</span></td>
<td id="S4.T4.3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.3.1.7.6.4.1" class="ltx_text" style="font-size:80%;">94.06</span></td>
<td id="S4.T4.3.1.7.6.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.3.1.7.6.5.1" class="ltx_text" style="font-size:80%;">3.03</span></td>
<td id="S4.T4.3.1.7.6.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.3.1.7.6.6.1" class="ltx_text" style="font-size:80%;">-10.62</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T4.4" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.4.1.1" class="ltx_tr">
<th id="S4.T4.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="4"><span id="S4.T4.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 2.1 (Overall Improvement): Synthetic Data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.4.2.1" class="ltx_tr">
<th id="S4.T4.4.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.4.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T4.4.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.4.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T4.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T4.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T4.4.3.2" class="ltx_tr">
<th id="S4.T4.4.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.4.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1</span></th>
<th id="S4.T4.4.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.4.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">BOVIFOCR</span></th>
<td id="S4.T4.4.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">90.50</span></td>
<td id="S4.T4.4.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">2.66</span></td>
</tr>
<tr id="S4.T4.4.4.3" class="ltx_tr">
<th id="S4.T4.4.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.4.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T4.4.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.4.4.3.2.1" class="ltx_text" style="font-size:80%;">LENS</span></th>
<td id="S4.T4.4.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.4.3.3.1" class="ltx_text" style="font-size:80%;">88.18</span></td>
<td id="S4.T4.4.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.4.3.4.1" class="ltx_text" style="font-size:80%;">3.75</span></td>
</tr>
<tr id="S4.T4.4.5.4" class="ltx_tr">
<th id="S4.T4.4.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.4.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T4.4.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.4.5.4.2.1" class="ltx_text" style="font-size:80%;">Idiap</span></th>
<td id="S4.T4.4.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.5.4.3.1" class="ltx_text" style="font-size:80%;">86.39</span></td>
<td id="S4.T4.4.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.5.4.4.1" class="ltx_text" style="font-size:80%;">6.39</span></td>
</tr>
<tr id="S4.T4.4.6.5" class="ltx_tr">
<th id="S4.T4.4.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.4.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T4.4.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.4.6.5.2.1" class="ltx_text" style="font-size:80%;">BioLab</span></th>
<td id="S4.T4.4.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.4.6.5.3.1" class="ltx_text" style="font-size:80%;">83.93</span></td>
<td id="S4.T4.4.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.4.6.5.4.1" class="ltx_text" style="font-size:80%;">6.88</span></td>
</tr>
<tr id="S4.T4.4.7.6" class="ltx_tr">
<th id="S4.T4.4.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.4.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T4.4.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.4.7.6.2.1" class="ltx_text" style="font-size:80%;">MeVer</span></th>
<td id="S4.T4.4.7.6.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.7.6.3.1" class="ltx_text" style="font-size:80%;">83.45</span></td>
<td id="S4.T4.4.7.6.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.4.7.6.4.1" class="ltx_text" style="font-size:80%;">3.20</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T4.5" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.5.1.1" class="ltx_tr">
<th id="S4.T4.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="4"><span id="S4.T4.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 2.2 (Overall Improvement): Synthetic + Real Data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.5.2.1" class="ltx_tr">
<th id="S4.T4.5.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.5.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T4.5.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.5.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T4.5.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T4.5.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T4.5.3.2" class="ltx_tr">
<th id="S4.T4.5.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.5.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1</span></th>
<th id="S4.T4.5.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.5.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">CBSR</span></th>
<td id="S4.T4.5.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">94.95</span></td>
<td id="S4.T4.5.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.5.3.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">-3.69</span></td>
</tr>
<tr id="S4.T4.5.4.3" class="ltx_tr">
<th id="S4.T4.5.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.5.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T4.5.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.5.4.3.2.1" class="ltx_text" style="font-size:80%;">LENS</span></th>
<td id="S4.T4.5.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.4.3.3.1" class="ltx_text" style="font-size:80%;">92.40</span></td>
<td id="S4.T4.5.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.4.3.4.1" class="ltx_text" style="font-size:80%;">-1.63</span></td>
</tr>
<tr id="S4.T4.5.5.4" class="ltx_tr">
<th id="S4.T4.5.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.5.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T4.5.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.5.5.4.2.1" class="ltx_text" style="font-size:80%;">Idiap</span></th>
<td id="S4.T4.5.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.5.4.3.1" class="ltx_text" style="font-size:80%;">91.74</span></td>
<td id="S4.T4.5.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.5.4.4.1" class="ltx_text" style="font-size:80%;">0.00</span></td>
</tr>
<tr id="S4.T4.5.6.5" class="ltx_tr">
<th id="S4.T4.5.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.5.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T4.5.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.5.6.5.2.1" class="ltx_text" style="font-size:80%;">BOVIFOCR</span></th>
<td id="S4.T4.5.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.5.6.5.3.1" class="ltx_text" style="font-size:80%;">91.34</span></td>
<td id="S4.T4.5.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.5.6.5.4.1" class="ltx_text" style="font-size:80%;">1.77</span></td>
</tr>
<tr id="S4.T4.5.7.6" class="ltx_tr">
<th id="S4.T4.5.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.5.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T4.5.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.5.7.6.2.1" class="ltx_text" style="font-size:80%;">MeVer</span></th>
<td id="S4.T4.5.7.6.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.5.7.6.3.1" class="ltx_text" style="font-size:80%;">87.60</span></td>
<td id="S4.T4.5.7.6.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.5.7.6.4.1" class="ltx_text" style="font-size:80%;">-1.57</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.9.1.1" class="ltx_text" style="font-size:113%;">Table 4</span>: </span><span id="S4.T4.10.2" class="ltx_text" style="font-size:113%;">Ranking for the four sub-tasks, according to the metrics described in Section <a href="#S3.SS3" title="3.3 Metrics â€£ 3 FRCSyn Challenge: Setup â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. TO = Trade-Off, AVG = Average accuracy, SD = Standard Deviation of accuracy, GAP = Gap to Real.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>FRCSyn Challenge: Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 â€£ UNICA-FRAUNHOFER IGD (Sub-Tasks 1.2 and 2.2): â€£ 4 FRCSyn Challenge: Description of Systems â€£ FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the rankings for the different sub-tasks considered in the FRCSyn Challenge. In general, the rankings for Sub-Tasks 1.1 and 1.2 (bias mitigation), corresponding to the descending order of TO, closely align with the ascending order of SD (<span id="S5.p1.1.1" class="ltx_text ltx_font_italic">i.e.,</span> from less to more biased FR systems). Notably, in Sub-Task 1.1, the top two classified teams, LENS (92.25% TO) and Idiap (91.88% TO), exhibit negative GAP values (-0.74% and -3.80%, respectively), indicating higher accuracy when training the FR system with synthetic data compared to real data. These results highlight the potential of DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> synthetic data to reduce bias in current FR technology. The inclusion of real data in the training process (<span id="S5.p1.1.2" class="ltx_text ltx_font_italic">i.e.,</span> Sub-Task 1.2) results in general in a simultaneous increase in AVG and reduction in SD, being the CBSR team the winner with a 95.25% TO (<span id="S5.p1.1.3" class="ltx_text ltx_font_italic">i.e.,</span> 3% TO general improvement between Sub-Tasks 1.1 and 1.2). In addition, and as it happens in Sub-Task 1.1, we can observe in Sub-Task 1.2 negative GAP values for the top teams (<span id="S5.p1.1.4" class="ltx_text ltx_font_italic">e.g.,</span> -2.10% and -5.67% for the CBSR and LENS teams, respectively), evidencing that the combination of synthetic and real data (proposed system) outperforms FR systems trained only with real data (baseline system).</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">For Task 2, it is evident that the average accuracy across databases in Sub-Tasks 2.1 and 2.2 is lower than the accuracy achieved for BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> in Sub-Tasks 1.1 and 1.2, emphasizing the additional challenges introduced by the other real databases considered for evaluation. Also, although good results are achieved in Sub-Task 2.1 when training only with synthetic data (90.50% AVG for BOVIFOCR-UFPR), the positive GAP values provided by the top-5 teams indicate that synthetic data alone currently struggles to completely replace real data for training FR systems in challenging conditions. Nevertheless, the negative GAP values provided by the top-2 teams in Sub-Task 2.2 also suggest that synthetic data combining with real data can mitigate existing limitations within FR technology.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Finally, analyzing the contributions of all the eight top teams, a notable trend emerges, showing the prevalence of well-established methodologies. ResNet backbones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> were chosen by seven teams, except for Aphi, which opted for EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> loss functions were widely used, featuring in the approaches of CBSR, LENS, Idiap, and BioLab for the former, and BOVIFOCR-UFPR, MeVer, and Aphi for the latter. Idiap and UNICA-FRAUNHOFER IGD also considered the CosFace loss function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Most of the teams integrated multiple networks into their proposed architectures for different objectives, <span id="S5.p3.1.1" class="ltx_text ltx_font_italic">e.g.</span>, CBSR and LENS trained different networks with distinct augmentation techniques, while BOVIFOCR-UFPR and Idiap combined different loss functions.
Some teams also addressed the challenges of domain shift between synthetic and real data, <span id="S5.p3.1.2" class="ltx_text ltx_font_italic">e.g.</span>, LENS proposed solutions robust to domain shifts with consistent data augmentation, while CBSR implemented a range of strategies, including advanced data augmentation, identity clustering, and distinct thresholds for different databases. Notably, CBSR utilized all available databases for training, including FFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, unlike other teams. Excluding BOVIFOCR-UFPR, Aphi, and UNICA-FRAUNHOFER IGD, which exclusively used DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, the majority of teams employed both DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, demonstrating the suitability of both generative frameworks.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) has provided a comprehensive analysis for the application of synthetic data to FR, addressing current limitations in the field. Within this challenge numerous approaches from different research groups have been proposed. These approaches can be compared across a variety of sub-tasks, with many being reproducible thanks to the materials made available by the participating teams.
Future works will be oriented to a more detailed analysed of the results, including additional metrics and graphical representations. Furthermore, we are considering transforming the CodaLab platform into an ongoing competition, where new tasks and sub-tasks might be introduced.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text" style="font-size:80%;">Special thanks to Mei Wang and Stylianos Moschoglou for authorizing the distribution of their databases.
This study has received funding from the European Unionâ€™s Horizon 2020 TReSPAsS-ETN (No 860813) and is supported by INTER-ACTION (PID2021- 126521OB-I00 MICINN/FEDER) and R&amp;D Agreement DGGC/UAM/FUAM for Biometrics and Cybersecurity.
It is also supported by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.
BioLab acknowledge Andrea Pilzer from the NVIDIA AI Technology Center, EMEA, for his support.
MeVer was supported by the EU Horizon Europe project MAMMOth (Grant Agreement 101070285).</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Insaf Adjabi, Abdeldjalil Ouahabi, Amir Benzaoui, and Abdelmalik Taleb-Ahmed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Past, present, and future of face recognition: A review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Electronics</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 9(8):1188, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Waqar Ali, Wenhong Tian, SalahÂ Ud Din, Desire Iradukunda, and AbdullahÂ Aman
Khan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Classical and modern face recognition approaches: a complete review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Multimedia tools and applications</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 80:4825â€“4880, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Gwangbin Bae, Martin de LaÂ Gorce, Tadas BaltruÅ¡aitis, Charlie Hewitt,
Dong Chen, Julien Valentin, Roberto Cipolla, and Jingjing Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">DigiFace-1M: 1 Million Digital Face Images for Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Fadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">ElasticFace: Elastic Margin Loss for Deep Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Fadi Boutros, JonasÂ Henry Grebe, Arjan Kuijper, and Naser Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">IDiff-Face: Synthetic-based Face Recognition through Fizzy
Identity-Conditioned Diffusion Model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Fadi Boutros, Marco Huber, Patrick Siebke, Tim Rieber, and Naser Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Sface: Privacy-friendly and accurate face recognition using synthetic
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Joint Conference on
Biometrics</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Fadi Boutros, Marcel Klemt, Meiling Fang, Arjan Kuijper, and Naser Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Unsupervised face recognition using unlabeled synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Automatic Face
and Gesture Recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Fadi Boutros, Vitomir Struc, Julian Fierrez, and Naser Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Synthetic data for face recognition: Current state and future
prospects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Image and Vision Computing</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, page 104688, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Learning with bounded instance and label-dependent label noise.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine
Learning</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
EkinÂ D Cubuk, Barret Zoph, Jonathon Shlens, and QuocÂ V Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">RandAugment: Practical automated data augmentation with a reduced
search space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Jiankang Deng, Jia Guo, Xiang An, Zheng Zhu, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Masked face recognition challenge: The insightface track report.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Jiankang Deng, Jia Guo, Tongliang Liu, Mingming Gong, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Sub-center ArcFace: Boosting Face Recognition by Large-scale Noisy
Web Faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos
Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">RetinaFace: Single-shot Multi-level Face Localisation in the Wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Arcface: Additive angular margin loss for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Disentangled and controllable face image generation via 3d
imitative-contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Hang Du, Hailin Shi, Dan Zeng, Xiao-Ping Zhang, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">The elements of end-to-end deep face recognition: A survey of recent
advances.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Computing Surveys (CSUR)</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 54(10s):1â€“42, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">
MustafaÂ Ekrem Erak</span><math id="bib.bib17.1.m1.1" class="ltx_Math" alttext="\iota" display="inline"><semantics id="bib.bib17.1.m1.1a"><mi mathsize="90%" id="bib.bib17.1.m1.1.1" xref="bib.bib17.1.m1.1.1.cmml">Î¹</mi><annotation-xml encoding="MathML-Content" id="bib.bib17.1.m1.1b"><ci id="bib.bib17.1.m1.1.1.cmml" xref="bib.bib17.1.m1.1.1">ğœ„</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib17.1.m1.1c">\iota</annotation></semantics></math><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">n, UÄŸur Demir, and Haz</span><math id="bib.bib17.2.m2.1" class="ltx_Math" alttext="\iota" display="inline"><semantics id="bib.bib17.2.m2.1a"><mi mathsize="90%" id="bib.bib17.2.m2.1.1" xref="bib.bib17.2.m2.1.1.cmml">Î¹</mi><annotation-xml encoding="MathML-Content" id="bib.bib17.2.m2.1b"><ci id="bib.bib17.2.m2.1.1.cmml" xref="bib.bib17.2.m2.1.1">ğœ„</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib17.2.m2.1c">\iota</annotation></semantics></math><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">mÂ Kemal Ekenel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">On Recognizing Occluded Faces in the Wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.8.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference of the Biometrics
Special Interest Group</span><span id="bib.bib17.9.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Martin Ester, Hans-Peter Kriegel, JÃ¶rg Sander, Xiaowei Xu, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">A Density-Based Algorithm for Discovering Clusters in Large Spatial
Databases with Noise.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Second International Conference on
Knowledge Discovery and Data Mining</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 96, pages 226â€“231, 1996.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Jules. Harvey, Adam.Â LaPlace.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Exposing.ai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">https://exposing.ai, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
GaryÂ B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Labeled Faces in the Wild: A Database for Studying Face Recognition
in Unconstrained Environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Workshop on Faces in â€˜Real-Lifeâ€™ Images:
Detection, Alignment, and Recognition</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Manuel Kansy, Anton RaÃ«l, Graziana Mignone, Jacek Naruniec, Christopher
Schroers, Markus Gross, and RomannÂ M Weber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Controllable Inversion of Black-Box Face Recognition Models via
Diffusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Kimmo Karkkainen and Jungseock Joo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">FairFace: Face attribute dataset for balanced race, gender, and age
for bias measurement and mitigation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Samuli Laine, and Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">A Style-Based Generator Architecture for Generative Adversarial
Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Minchul Kim, AnilÂ K. Jain, and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">AdaFace: Quality Adaptive Margin for Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">DCFace: Synthetic Face Generation with Dual Condition Diffusion
Model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Dominik
Lawatsch, Florian Domin, and Maxim Schaubert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">GANDiffFace: Controllable Generation of Synthetic Datasets for Face
Recognition with Realistic Variations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision Workshops</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami
Morales, Dominik Lawatsch, Florian Domin, and Maxim Schaubert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Synthetic Data for the Mitigation of Demographic Biases in Face
Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Joint Conference on
Biometrics</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Shervin Minaee, Amirali Abdolrashidi, Hang Su, Mohammed Bennamoun, and David
Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Biometrics recognition using deep learning: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Artificial Intelligence Review</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, pages 1â€“49, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez, and Ruben Tolosana.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">SensitiveNets: Learning agnostic representations with application to
face images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">,
43(6):2158â€“2164, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng,
Irene Kotsia, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">AgeDB: The First Manually Collected, In-The-Wild Age Database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Madhumita Murgia and Max Harlow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Whoâ€™s using your face? The ugly truth about facial recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Financial Times</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 19, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
JoaoÂ C Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo
ProenÃ§a, and Julian Fierrez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">GANprintR: Improved Fakes and Evaluation of the State of the Art in
Face Manipulation Detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Journal of Selected Topics in Signal Processing</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">,
14(5):1038â€“1048, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Haibo Qiu, Baosheng Yu, Dihong Gong, Zhifeng Li, Wei Liu, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">SynFace: Face Recognition With Synthetic Data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, and Christoph Busch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Handbook of digital face manipulation and detection: from
DeepFakes to morphing attacks</span><span id="bib.bib35.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">Springer Nature, 2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, VishalÂ M Patel, Rama
Chellappa, and DavidÂ W Jacobs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Frontal to profile face verification in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Winter conference on Applications of
Computer Vision</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan and Quoc Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">EfficientNetV2: Smaller Models and Faster Training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine
Learning</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, and
Javier Ortega-Garcia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Deepfakes and beyond: A survey of face manipulation and fake
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Information Fusion</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 64:131â€“148, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Paul Voigt and Axel VonÂ dem Bussche.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">The EU General Data Protection Regulation (GDPR).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">A Practical Guide, 1st Ed., Cham: Springer International
Publishing</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 10(3152676):10â€“5555, 2017.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng
Li, and Wei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">CosFace: Large margin cosine loss for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Jun Wang, Yinglu Liu, Yibo Hu, Hailin Shi, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">FaceX-Zoo: A PyTorch Toolbox for Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 29th ACM International Conference on
Multimedia</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Mei Wang and Weihong Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Mitigating bias in face recognition using skewness-aware
reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Mei Wang and Weihong Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Deep face recognition: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neurocomputing</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 429:215â€“244, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Mei Wang, Yaobin Zhang, and Weihong Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Meta balanced network for fair face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">,
44(11):8433â€“8448, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
David Wanyonyi and Turgay Celik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Open-source face recognition frameworks: A review of the landscape.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 10:50601â€“50623, 2022.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Dong Yi, Zhen Lei, Shengcai Liao, and StanÂ Z Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Learning face representation from scratch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1411.7923</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Dan Zeng, Raymond Veldhuis, and Luuk Spreeuwers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">A survey of face recognition techniques under occlusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IET Biometrics</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 10(6):581â€“606, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
BrianÂ Hu Zhang, Blake Lemoine, and Margaret Mitchell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Mitigating unwanted biases with adversarial learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI/ACM Conference on AI, Ethics, and
Society</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Cheng Zhang, Xuanbai Chen, Siqi Chai, ChenÂ Henry Wu, Dmitry Lagun, Thabo
Beeler, and Fernando DeÂ la Torre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">ITI-GEN: Inclusive Text-to-Image Generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Joint face detection and alignment using multitask cascaded
convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Letters</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 23(10):1499â€“1503, 2016.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.10475" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.10476" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.10476">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.10476" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.10477" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 18:37:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
