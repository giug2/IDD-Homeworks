<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models</title>
<!--Generated on Wed May  8 13:53:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.05060v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S1" title="In Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S2" title="In Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S3" title="In Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S3.SS1" title="In 3 Architecture ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Decision Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S3.SS2" title="In 3 Architecture ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>LLMs for Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4" title="In Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4.SS1" title="In 4 Evaluation ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4.SS2" title="In 4 Evaluation ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Additional Analysis for Interpretability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S5" title="In Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S6" title="In Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Ethical Considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S7" title="In Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\useunder</span>
<p class="ltx_p" id="p1.2"><span class="ltx_text ltx_ulem_uline" id="p1.2.1"></span><span class="ltx_ERROR undefined" id="p1.2.2">\ul</span>
</p>
</div>
<h1 class="ltx_title ltx_title_document">Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aylin Gunal 
<br class="ltx_break"/><span class="ltx_text" id="id1.1.id1" style="font-size:90%;"> University of Michigan 
<br class="ltx_break"/><span class="ltx_text" id="id1.1.id1.1" style="font-size:90%;"> Ann Arbor, MI 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1.1.1">gunala@umich.edu</span>
<span class="ltx_ERROR undefined" id="id1.1.id1.1.2">\And</span>Baihan Lin 
<br class="ltx_break"/><span class="ltx_text" id="id1.1.id1.1.3" style="font-size:90%;"> Icahn School of Medicine at Mount Sinai 
<br class="ltx_break"/><span class="ltx_text" id="id1.1.id1.1.3.1" style="font-size:90%;"> New York, NY 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1.1.3.1.1">baihan.lin@mssm.edu</span> <span class="ltx_ERROR undefined" id="id1.1.id1.1.3.1.2">\And</span>Djallel Bouneffouf 
<br class="ltx_break"/><span class="ltx_text" id="id1.1.id1.1.3.1.3" style="font-size:90%;"> IBM Research 
<br class="ltx_break"/><span class="ltx_text" id="id1.1.id1.1.3.1.3.1" style="font-size:90%;"> Yorktown Heights, NY
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1.1.3.1.3.1.1">djallel.bouneffouf@ibm.com</span>
<br class="ltx_break"/></span></span></span></span></span></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Given the increasing demand for mental health assistance, artificial intelligence (AI), particularly large language models (LLMs), may be valuable for integration into automated clinical support systems. In this work, we leverage a decision transformer architecture for topic recommendation in counseling conversations between patients and mental health professionals. The architecture is utilized for offline reinforcement learning, and we extract states (dialogue turn embeddings), actions (conversation topics), and rewards (scores measuring the alignment between patient and therapist) from previous turns within a conversation to train a decision transformer model. We demonstrate an improvement over baseline reinforcement learning methods, and propose a novel system of utilizing our model’s output as synthetic labels for fine-tuning a large language model for the same task. Although our implementation based on LLaMA-2 7B has mixed results, future work can undoubtedly build on the design.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, there has been a notable uptick in the number of people seeking professional help for mental health concerns, but the available pool of mental health professionals remains small in comparison. To address this need, automated AI-based tools and methods for counseling have been explored and engineered, ranging from systems for training junior mental health counselors <cite class="ltx_cite ltx_citemacro_cite">Min et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib19" title="">2022</a>); Demasi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib3" title="">2019</a>)</cite> to AI-in-the-loop chatbots <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib22" title="">2022</a>)</cite>.
With the dramatic rise in popularity and accessibility of large language models (LLMs), it’s expected that LLMs will play a significant role in the intersection of computing and mental health research, as well.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In our prior work <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib16" title="">2023b</a>)</cite>, we introduced the SupervisorBot, a reinforcement learning (RL)-based topic recommendation system in counseling conversations.
This proves to be a useful tool for clinicians during their psychotherapy sessions, where the system recommends what topics to discuss next given what has been discussed so far, as well as what works best in the past in terms of the patient outcomes.
In this work, we improve upon this meaningful task by introducing the Decision Transformer <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib2" title="">2021</a>)</cite>, a transformer model designed for reinforcement learning (RL), into the recommendation pipeline, demonstrating better performance than other RL methods. We also explore the potential combination of Decision Transformer with LLMs, by generating labels for unseen transcript data using the pre-trained Decision Transformer model, and feeding the synthetically annotated data to fine-tuning a LLM.
Our primary contribution is demonstrating that in the task of topic recommendation, Decision Transformer outperforms baseline RL methods; if such a system were to go through the process of user testing, the Decision Transformer––or models building on or improving Decision Transformer––can be utilized as the backbone for the recommendation module.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We first describe how we implement the pre-processing of the therapy conversation dataset, and how this is fed into the Decision Transformer model. We then describe how we use a portion of the dataset to train the Decision Transformer model, and that trained model’s predicted labels are used as input to a large language model to train for the same task of topic recommendation.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Decision Transformer was introduced as a transformer-based architecture to abstract the process of offline reinforcement learning, and has been used successfully in various NLP tasks including natural language understanding <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib26" title="">2022</a>); Bucker et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib1" title="">2023</a>)</cite>, navigating text-based games <cite class="ltx_cite ltx_citemacro_cite">Putterman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib20" title="">2021</a>)</cite>, and generative language modeling <cite class="ltx_cite ltx_citemacro_cite">Memisevic et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib17" title="">2022</a>)</cite>. The Decision Transformer architecture has also been effectively applied to the clinical domain to generate treatment recommendations based on patient history <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib10" title="">2023</a>)</cite>.
In this work, we effectively apply the Decision Transformer architecture to the mental health domain in a dialogue recommendation task and improve on performances with older reinforcement learning methods.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The improvement of AI-in-the-loop tools to support humans in tasks has typically focused on human feedback, although more recent work has explored the potential for AI tools to improve themselves through a number of methods. <cite class="ltx_cite ltx_citemacro_cite">Saunders et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib21" title="">2022</a>)</cite> demonstrates that a generative language model can improve its own outputs through fine-tuning on its own generations, and that the improvements are more significant as the model size increases. Generative models also have the advantage of being able to generate improvements to their own outputs <cite class="ltx_cite ltx_citemacro_cite">Zelikman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib25" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">In addition to models improving themselves, ensemble methods in which one model serves some intermediary purpose within the pipeline––e.g. data generation or filtering for input to another model––can be used for conversational modeling tasks as well <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib9" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Stiennon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib23" title="">2020</a>)</cite> uses an intermediary model’s output as a reward function for another model, outperforming sole supervised learning from the source dataset. In this work, we explore a potential pipeline in which one model’s output is used as synthetic data to train a language model for the task of topic recommendation. We consider the idea of AI supplementing a typical reinforcement learning with human feedback (RLHF) process by experimenting with how AI may be able to augment feedback, which can have significant implications given the lack of publicly available mental health dialogue data, let alone annotated data.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Architecture</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In the following sections, we describe the architecture of our system in detail (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S3.F1" title="Figure 1 ‣ 3 Architecture ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S3.F1.g1" src="extracted/5584765/figures/fig1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of the proposed LLM integration, demonstrating how both gold-standard labels from the dataset as well as synthetic annotations from Decision Transformer output can be used to fine-tune the LLM.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Decision Transformer</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We re-implement the recommendation system pipeline as described in our original paper, <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib16" title="">2023b</a>)</cite>.
This system is designed to provide real-time feedback in the form of next-topic recommendation for mental health counselors in session with patients, using reinforcement learning methods to learn and to recommend the next topic (the <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">action</span> taken by the counselor) to move on from the current segment of dialogue (the current <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">state</span>). <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">Rewards</span> are calculated using working alliance inventory (WAI) <cite class="ltx_cite ltx_citemacro_cite">Horvath and Greenberg (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib7" title="">1989</a>)</cite>, a score from a survey of questions to determine how aligned a counselor is with their patient within a session. WAI is determined by computing similarity between inventory items and segments of dialogue <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib15" title="">2023a</a>)</cite>, and inventory items fall under three different categories: Task, Bond, and Goal. We include an aggregate WAI score, referred to as Full.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The original SupervisorBot paper evaluates the system’s performance on three baseline RL algorithms: DDPG <cite class="ltx_cite ltx_citemacro_cite">Lillicrap et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib11" title="">2015</a>)</cite>, TD3 <cite class="ltx_cite ltx_citemacro_cite">Fujimoto et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib5" title="">2018</a>)</cite>, and BCQ <cite class="ltx_cite ltx_citemacro_cite">Fujimoto et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib6" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">We use the Alex Street dataset <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://alexanderstreet.com/products/counseling-and-psychotherapy-transcripts-series</span></span></span>, a dataset composed of counseling session transcripts for patients suffering from depression, anxiety, suicidal thoughts, and schizophrenia.
The Alex Street dataset is preprocessed and segmented into turn-pairs, which are then embedded using Word2Vec <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib18" title="">2013</a>)</cite>. We use embedded topic modeling <cite class="ltx_cite ltx_citemacro_cite">Dieng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib4" title="">2020</a>)</cite> to extract 8 topics from the corpus––as determined optimal by the motivating paper––and label each turn-pair with the topic it best represents. The WAI scores are computed for each turn-pair. As the original system design is done, turn-pair embeddings represent states, topic labels represent actions, and associated WAI scores represent rewards. These items are fed as input into the Decision Transformer in the form of tuples of <math alttext="(r_{t},s_{t},a_{t})" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.3"><semantics id="S3.SS1.p3.1.m1.3a"><mrow id="S3.SS1.p3.1.m1.3.3.3" xref="S3.SS1.p3.1.m1.3.3.4.cmml"><mo id="S3.SS1.p3.1.m1.3.3.3.4" stretchy="false" xref="S3.SS1.p3.1.m1.3.3.4.cmml">(</mo><msub id="S3.SS1.p3.1.m1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.2.cmml">r</mi><mi id="S3.SS1.p3.1.m1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.p3.1.m1.3.3.3.5" xref="S3.SS1.p3.1.m1.3.3.4.cmml">,</mo><msub id="S3.SS1.p3.1.m1.2.2.2.2" xref="S3.SS1.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS1.p3.1.m1.2.2.2.2.2" xref="S3.SS1.p3.1.m1.2.2.2.2.2.cmml">s</mi><mi id="S3.SS1.p3.1.m1.2.2.2.2.3" xref="S3.SS1.p3.1.m1.2.2.2.2.3.cmml">t</mi></msub><mo id="S3.SS1.p3.1.m1.3.3.3.6" xref="S3.SS1.p3.1.m1.3.3.4.cmml">,</mo><msub id="S3.SS1.p3.1.m1.3.3.3.3" xref="S3.SS1.p3.1.m1.3.3.3.3.cmml"><mi id="S3.SS1.p3.1.m1.3.3.3.3.2" xref="S3.SS1.p3.1.m1.3.3.3.3.2.cmml">a</mi><mi id="S3.SS1.p3.1.m1.3.3.3.3.3" xref="S3.SS1.p3.1.m1.3.3.3.3.3.cmml">t</mi></msub><mo id="S3.SS1.p3.1.m1.3.3.3.7" stretchy="false" xref="S3.SS1.p3.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.3b"><vector id="S3.SS1.p3.1.m1.3.3.4.cmml" xref="S3.SS1.p3.1.m1.3.3.3"><apply id="S3.SS1.p3.1.m1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.2">𝑟</ci><ci id="S3.SS1.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.SS1.p3.1.m1.2.2.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.2">𝑠</ci><ci id="S3.SS1.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p3.1.m1.2.2.2.2.3">𝑡</ci></apply><apply id="S3.SS1.p3.1.m1.3.3.3.3.cmml" xref="S3.SS1.p3.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.3.3.3.3.1.cmml" xref="S3.SS1.p3.1.m1.3.3.3.3">subscript</csymbol><ci id="S3.SS1.p3.1.m1.3.3.3.3.2.cmml" xref="S3.SS1.p3.1.m1.3.3.3.3.2">𝑎</ci><ci id="S3.SS1.p3.1.m1.3.3.3.3.3.cmml" xref="S3.SS1.p3.1.m1.3.3.3.3.3">𝑡</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.3c">(r_{t},s_{t},a_{t})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.3d">( italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">We defer to the original Decision Transformer paper for architecture details. Our model contains a single-head, 3-layer attention mechanism, and we use a context window of 20 for the baseline results.
Pearson’s correlation between the model predicted actions and real actions taken is used for evaluation for all experiments. We run experiments 5 times using a 95%/5% train-test split, and take the average result.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>LLMs for Recommendation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">An attractive property of LLMs is flexibility in usage; at their core they simply model language probability distributions, making their outputs malleable to various tasks in NLP. In this section, we explore LLMs’ ability for dialogue classification into labels of different psychiatric conditions to demonstrate their usefulness in various components of RLHF, similar to a diagnostic scenario in clinical setting as in <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib14" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib13" title="">2024</a>)</cite>. We primarily experiment with LLaMA-2 with 7B parameters <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib24" title="">2023</a>)</cite>. We fine-tune LLaMA-2 model for sequence classification and test using the same preprocessing steps and train-test split described in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S3.SS1" title="3.1 Decision Transformer ‣ 3 Architecture ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_tag">3.1</span></a>. During the fine-tuning process, we do not use a validation dataset to avoid data leakage into the test set.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We experiment with treating Decision Transformer predictions as synthetic gold standard annotations for the LLM to learn from. We split the full dataset in a 40%/40%/20% split; the first 40% of the Alex Street dataset is used to train Decision Transformer, then Decision Transformer outputs predictions for another 40% of the dataset which the LLM is then fine-tuned with, and ultimately the LLM is evaluated on the final 20% of the dataset. Due to computational constraints, we apply low-rank adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib8" title="">2021</a>)</cite>, a parameter efficient fine-tuning method, in order to optimize the fine-tuning process. The LoRA configuration includes an alpha of 16 and dropout rate of .05, and we fine-tune for 1 epoch.
We target the LLaMA-2 model’s attention layers during training and save the final layer’s weights to avoid those scores being randomly initialized for inference.
To provide some baseline for comparison, we additionally fine-tune the LLaMA-2 model on the original gold-standard labels.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T1.1" style="width:433.6pt;height:127.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(33.3pt,-9.8pt) scale(1.18125574983502,1.18125574983502) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T1.1.1.1.1.1">
<span class="ltx_ERROR undefined" id="S4.T1.1.1.1.1.1.1">\ul</span><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1.2">Decision Transformer</span>
</th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.1.2"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.1.3"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.1.4"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.1.5"></th>
<th class="ltx_td ltx_th ltx_th_column" id="S4.T1.1.1.1.1.6"></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="S4.T1.1.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.2.2.2">Depression</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.2.2.3">Anxiety</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.2.2.4">Schizophrenia</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.2.2.5">Suicidal</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.2.2.6">All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.3.1.1">Full</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.2">.176</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.3">.233</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.4">.246</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.5">.213</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.6">.361</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.1.4.2.1">Task</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.2.2.1">.291</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.2.3.1">.320</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.4">.247</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.5">.231</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.6">.323</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.1.5.3.1">Bond</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.2">.270</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.3">.314</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.4">.231</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.5.3.5.1">.239</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.6">.335</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.1.6.4.1">Goal</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.6.4.2.1">.291</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.4.3">.313</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.6.4.4.1">.249</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.4.5">.229</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.6.4.6.1">.375</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results of Decision Transformer on topic recommendation task, using previous 20 turn-pairs as input. Best results per data subset are in bold.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:248.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(122.7pt,-70.4pt) scale(2.30308259353093,2.30308259353093) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="S4.T2.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4" id="S4.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.2.1">Context Lengths</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.2.1.1">Rewards</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.2">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.3">10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.4">15</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2.2.5">20</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.3.1.1">Full</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.1.2">0.346</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.1.3">0.345</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.1.4.1">0.403</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.1.5">0.361</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.4.2.1">Bond</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.2.2">0.284</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.2.3">0.343</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.2.4.1">0.359</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.2.5">0.335</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.5.3.1">Task</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.3.2">0.272</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.3.3">0.298</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.5.3.4.1">0.342</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.3.5">0.322</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.1.6.4.1">Goal</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.4.2">0.278</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.4.3">0.339</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.4.4">0.348</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.4.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.4.5.1">0.375</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Decision Transformer model performance trained on varying context lengths. Best results per reward scale are in bold.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Results</h3>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:76.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(114.4pt,-20.1pt) scale(2.11722373787472,2.11722373787472) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="S4.T3.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.2.1">DDPG</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.3.1">BCQ</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.4.1">TD3</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.1.2.1.1">Full</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.1.2">.264 (-.97)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.1.3">.170 (-1.91)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2.1.4">.286 (-.75)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Baseline RL performance on full-scale rewards on the full dataset, with a comparison to DT performance (the negative values suggest that these baselines perform worse than our proposed DT-based recommendation model, which improves upon them).</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The results of the Decision Transformer on a 95%/5% train-test split, reflecting the set-up of the original SupervisorBot paper, are provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4.T1" title="Table 1 ‣ 4 Evaluation ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>. We reproduce results for the other RL methods in the original paper for performance on the full-scale rewards; Decision Transformer outperforms these baselines as noted in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4.T3" title="Table 3 ‣ 4.1 Results ‣ 4 Evaluation ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>.
We note that Decision Transformer specifically performs best for all reward scales when trained on the full dataset; among individual diseases, the model performs best on the task, bond, and goal scales for anxiety.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We additionally evaluate whether or not the 20-timestep context is necessary for good performance from the Decision Transformer model, and these results are provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4.T2" title="Table 2 ‣ 4 Evaluation ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>. We note that 15 time-steps is optimal for a majority of the reward sclaes, suggesting that the Decision Transformer is better able to make decisions provided a briefer learning history. An advantage of utilizing a transformer-based model for this task is that we are able to investigate its internal structure to understand specifically which historical features––including which time-steps––are significant for inference.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Additionally, we note that the LLaMA-2 model trained on the gold-standard data does not necessarily outperform the Decision Transformer for all reward scales as indicated in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4.T4" title="Table 4 ‣ 4.1 Results ‣ 4 Evaluation ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, indicating that the off-the-shelf language model may not be conducive for a reinforcement learning task.
LLaMA-2 trained on the Decision Transformer output directly also does not perform particularly well; future work may include modifying the way in which the Decision Transformer synthetic labels are used by a language model. It’s possible that prompting the language model may yield better results than treating it as a sequence classifier.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:433.6pt;height:99.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(98.9pt,-22.7pt) scale(1.83931819897013,1.83931819897013) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="S4.T4.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T4.1.1.1.1.2">Full</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T4.1.1.1.1.3">Task</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T4.1.1.1.1.4">Bond</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T4.1.1.1.1.5">Goal</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.1.1.1">LLaMA-2 7B + DT</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.2.1.2">.148</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.2.1.3">.118</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.2.1.4">.158</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.2.1.5">.115</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.2.1.1">LLaMA-2 7B + Gold</span></th>
<td class="ltx_td ltx_align_left" id="S4.T4.1.1.3.2.2">.371</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.1.3.2.3">.259</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.1.3.2.4">.315</td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.1.3.2.5">.332</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results of fine-tuning LLaMA-2 7B on DT output and gold-standard labels.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Additional Analysis for Interpretability</h3>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="456" id="S4.F2.g1" src="extracted/5584765/figures/abs_attns_nopad.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Normalized attention scores associated with absolute timesteps, <span class="ltx_text ltx_font_italic" id="S4.F2.2.1">without</span> padded sequences.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="456" id="S4.F3.g1" src="extracted/5584765/figures/relative_timesteps.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Normalized attention scores associated with relative timesteps.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We extract the final layer of attention weights from the Decision Transformer models trained on the four reward scales for the three types of inputs: returns, states, and actions. We observe both the attention weights for the individual input types as well as the aggregated and averaged set of weights aross all input types. Due to the auto-regressive nature of Decision Transformer, attention weights are assigned to the <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">timesteps</span> prior to the recommendation made at a given timestep.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">We provide visual analyses of attention scores through normalized aggregate attention scores per timestep for absolute timestep values (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4.F2" title="Figure 2 ‣ 4.2 Additional Analysis for Interpretability ‣ 4 Evaluation ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>) as well as relative timestep values (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#S4.F3" title="Figure 3 ‣ 4.2 Additional Analysis for Interpretability ‣ 4 Evaluation ‣ Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>). We note that the model refines its attention to generally focus on items in earlier positions in given input sequence, both in the case for absolute and relative timesteps. These results, in tandem with the generally higher performances of the model on 15 previous timesteps rather than 20 timesteps, indicate that potentially there is a beginning index to the current context that can be key for the model’s inference ability. Future work may include adjusting the context window dynamically, both for training and inference.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Due to limitations of computational resources, experimentation with fine-tuning LLMs is restricted by model size. Future work can build on this work by applying similar experiments on increasing model sizes or non-quantized versions of models, effectively demonstrating (positively or negatively) that performance scales with model size.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Ethical Considerations</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">When implementing a topic recommendation system in counseling contexts, ethical considerations are important due to the sensitive nature of digital mental health discussions, as discussed in <cite class="ltx_cite ltx_citemacro_cite">Lin (<a class="ltx_ref" href="https://arxiv.org/html/2405.05060v1#bib.bib12" title="">2022</a>)</cite>. One of the primary concerns is the potential limitation imposed by a static set of discussion topics. While such a system can streamline the counseling process, it risks limiting the creativity and flexibility of counselors, particularly those in training, and in the long term, inhibit consideration of their own perspectives on how to continue the conversation. This could inadvertently restrict their ability to tailor sessions according to the unique needs of each patient.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">This is particularly relevant since the topics pulled are from one specific dataset that covers only four mental health conditions. The training dataset, derived from this limited number of mental health conditions, might not be representative of the broader population or other conditions. This limitation can lead to biased recommendations if not carefully managed. To mitigate this, it is essential to consider a more dynamic approach where the set of topics can evolve based on ongoing input from practicing counselors and feedback from therapy sessions. This adaptation would help in maintaining the relevance and sensitivity of the recommendations to diverse patient needs. In deployment, we can also imagine that topics are dynamically chosen, or chosen using human feedback; for example, perhaps before the system is put into use, counselors can input their own topics.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">In addition to dataset limitations, the calculation of rewards, based on the Working Alliance Inventory (WAI), while rooted in established psychological theory, may benefit from enhancements through reinforcement learning with human feedback (RLHF). Incorporating direct input from users could refine the understanding and alignment of counselor and patient goals, improving the system’s effectiveness and ethical alignment.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this study, we introduced a Decision-Transformer-based recommendation system which outperforms baseline RL-based methods in counseling topic recommendation, indicating that transformer-based methods may have better performance in general when it comes to modeling conversation direction and alignment. We additionally find that the model performs best for certain reward scales on shorter input sequences, indicating that some exploration of optimal sequence length can be an avenue for future work.
Through additional analysis of the attention scores, we additionally find that the model pays more attention to items earlier on in the input sequence.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bucker et al. (2023)</span>
<span class="ltx_bibblock">
Arthur Bucker, Luis Figueredo, Sami Haddadin, Ashish Kapoor, Shuang Ma, Sai Vemprala, and Rogerio Bonatti. 2023.

</span>
<span class="ltx_bibblock">Latte: Language trajectory transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, pages 7287–7294. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srinivas, and Igor Mordatch. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:235294299" title="">Decision transformer: Reinforcement learning via sequence modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demasi et al. (2019)</span>
<span class="ltx_bibblock">
Orianna Demasi, Marti A. Hearst, and Benjamin Recht. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:163160685" title="">Towards augmenting crisis counselor training by improving message retrieval</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dieng et al. (2020)</span>
<span class="ltx_bibblock">
Adji B Dieng, Francisco JR Ruiz, and David M Blei. 2020.

</span>
<span class="ltx_bibblock">Topic modeling in embedding spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Transactions of the Association for Computational Linguistics</em>, 8:439–453.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fujimoto et al. (2018)</span>
<span class="ltx_bibblock">
Scott Fujimoto, Herke Hoof, and David Meger. 2018.

</span>
<span class="ltx_bibblock">Addressing function approximation error in actor-critic methods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">International conference on machine learning</em>, pages 1587–1596. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fujimoto et al. (2019)</span>
<span class="ltx_bibblock">
Scott Fujimoto, David Meger, and Doina Precup. 2019.

</span>
<span class="ltx_bibblock">Off-policy deep reinforcement learning without exploration.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International conference on machine learning</em>, pages 2052–2062. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horvath and Greenberg (1989)</span>
<span class="ltx_bibblock">
Adam O Horvath and Leslie S. Greenberg. 1989.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:145223477" title="">Development and validation of the working alliance inventory.</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Journal of Counseling Psychology</em>, 36:223–233.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2106.09685</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Xin Huang, Kye Min Tan, Richeng Duan, and Bowei Zou. 2023.

</span>
<span class="ltx_bibblock">Ensemble method via ranking model for conversational modeling with subjective knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of The Eleventh Dialog System Technology Challenge</em>, pages 177–184.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023)</span>
<span class="ltx_bibblock">
Seunghyun Lee, Da Young Lee, Sujeong Im, Nan Hee Kim, and Sung-Min Park. 2023.

</span>
<span class="ltx_bibblock">Clinical decision transformer: intended treatment recommendation through goal prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2302.00612</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lillicrap et al. (2015)</span>
<span class="ltx_bibblock">
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015.

</span>
<span class="ltx_bibblock">Continuous control with deep reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:1509.02971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2022)</span>
<span class="ltx_bibblock">
Baihan Lin. 2022.

</span>
<span class="ltx_bibblock">Computational inference in cognitive science: Operational, societal and ethical considerations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2210.13526</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Baihan Lin, Djallel Bouneffouf, Yulia Landa, Rachel Jespersen, Cheryl Corcoran, and Guillermo Cecchi. 2024.

</span>
<span class="ltx_bibblock">Compass: Computational mapping of patient-therapist alliance strategies with language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2402.14701</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Baihan Lin, Guillermo Cecchi, and Djallel Bouneffouf. 2022.

</span>
<span class="ltx_bibblock">Working alliance transformer for psychotherapy dialogue classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2210.15603</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023a)</span>
<span class="ltx_bibblock">
Baihan Lin, Guillermo Cecchi, and Djallel Bouneffouf. 2023a.

</span>
<span class="ltx_bibblock">Deep annotation of therapeutic working alliance in psychotherapy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">International workshop on health intelligence</em>, pages 193–207. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023b)</span>
<span class="ltx_bibblock">
Baihan Lin, Guillermo Cecchi, and Djallel Bouneffouf. 2023b.

</span>
<span class="ltx_bibblock">Supervisorbot: Nlp-annotated real-time recommendations of psychotherapy treatment strategies with deep reinforcement learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence</em>, pages 7149–7153.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Memisevic et al. (2022)</span>
<span class="ltx_bibblock">
Roland Memisevic, Sunny Panchal, and Mingu Lee. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=N47cSU036T" title="">Decision making as language generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">NeurIPS 2022 Foundation Models for Decision Making Workshop</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:16447573" title="">Distributed representations of words and phrases and their compositionality</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. (2022)</span>
<span class="ltx_bibblock">
Do June Min, Verónica Pérez-Rosas, Kenneth Resnicow, and Rada Mihalcea. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:256461465" title="">Pair: Prompt-aware margin ranking for counselor reflection scoring in motivational interviewing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Putterman et al. (2021)</span>
<span class="ltx_bibblock">
Aaron L Putterman, Kevin Lu, Igor Mordatch, and Pieter Abbeel. 2021.

</span>
<span class="ltx_bibblock">Pretraining for language conditioned imitation with transformers.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saunders et al. (2022)</span>
<span class="ltx_bibblock">
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Ouyang Long, Jonathan Ward, and Jan Leike. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:249626555" title="">Self-critiquing models for assisting human evaluators</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ArXiv</em>, abs/2206.05802.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2022)</span>
<span class="ltx_bibblock">
Ashish Sharma, Inna Wanyin Lin, Adam S. Miner, David C. Atkins, and Tim Althoff. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:247778407" title="">Human–ai collaboration enables more empathic conversations in text-based peer-to-peer mental health support</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Nature Machine Intelligence</em>, 5:46–57.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stiennon et al. (2020)</span>
<span class="ltx_bibblock">
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020.

</span>
<span class="ltx_bibblock">Learning to summarize with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Advances in Neural Information Processing Systems</em>, 33:3008–3021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et al. (2023)</span>
<span class="ltx_bibblock">
E. Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:263620781" title="">Self-taught optimizer (stop): Recursively self-improving code generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ArXiv</em>, abs/2310.02304.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Ziqi Zhang, Yile Wang, Yue Zhang, and Donglin Wang. 2022.

</span>
<span class="ltx_bibblock">Can offline reinforcement learning help natural language understanding?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2212.03864</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May  8 13:53:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
