<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.05126] UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model</title><meta property="og:description" content="Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situ…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.05126">

<!--Generated on Wed Feb 28 02:14:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_ERROR undefined">\ul</span>

 
 
 
 
 

</p>
</div>
<h1 class="ltx_title ltx_title_document">UReader: Universal OCR-free Visually-situated Language 
<br class="ltx_break">Understanding with Multimodal Large Language Model
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiabo Ye<sup id="id4.4.id1" class="ltx_sup">1</sup>  , Anwen Hu<sup id="id5.5.id2" class="ltx_sup">2</sup><sup id="id6.6.id3" class="ltx_sup">∗</sup>, Haiyang Xu<sup id="id7.7.id4" class="ltx_sup">2</sup>  , Qinghao Ye<sup id="id8.8.id5" class="ltx_sup">2</sup>, Ming Yan<sup id="id9.9.id6" class="ltx_sup">2</sup><sup id="id10.10.id7" class="ltx_sup"><span id="id10.10.id7.1" class="ltx_text ltx_font_italic">†</span></sup>, Guohai Xu<sup id="id11.11.id8" class="ltx_sup">2</sup>, Chenliang Li<sup id="id12.12.id9" class="ltx_sup">2</sup>, 
<br class="ltx_break"><span id="id13.13.id10" class="ltx_text ltx_font_bold">Junfeng Tian<sup id="id13.13.id10.1" class="ltx_sup">2</sup>, Qi Qian<sup id="id13.13.id10.2" class="ltx_sup">2</sup>
, Ji Zhang<sup id="id13.13.id10.3" class="ltx_sup">2</sup>, Qin Jin<sup id="id13.13.id10.4" class="ltx_sup">3</sup>, Liang He<sup id="id13.13.id10.5" class="ltx_sup">1</sup>, Xin Lin<sup id="id13.13.id10.6" class="ltx_sup">1</sup>, Fei Huang<sup id="id13.13.id10.7" class="ltx_sup">2</sup></span> 
<br class="ltx_break"><sup id="id14.14.id11" class="ltx_sup">1</sup>East China Normal University <sup id="id15.15.id12" class="ltx_sup">2</sup>DAMO Academy, Alibaba Group <sup id="id16.16.id13" class="ltx_sup">3</sup>Renmin University of China 
<br class="ltx_break"><span id="id17.17.id14" class="ltx_text ltx_font_typewriter">jiabo.ye@stu.ecnu.edu.cn </span>
<span id="id18.18.id15" class="ltx_text ltx_font_typewriter">{huanwen.haw,ym119608,shuofeng.xhy}@alibaba-inc.com </span>
</span><span class="ltx_author_notes">  Equal contribution<sup id="id19.19.id1" class="ltx_sup"><span id="id19.19.id1.1" class="ltx_text ltx_font_italic">†</span></sup> Corresponding authors</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM). By leveraging the shallow text recognition ability of the MLLM, we only finetuned 1.2% parameters and the training cost is much lower than previous work following domain-specific pretraining and finetuning paradigms. Concretely, UReader is jointly finetuned on a wide range of Visually-situated Language Understanding tasks via a unified instruction format. To enhance the visual text and semantic understanding, we further apply two auxiliary tasks with the same format, namely text reading and key points generation tasks. We design a shape-adaptive cropping module before the encoder-decoder architecture of MLLM to leverage the frozen low-resolution vision encoder for processing high-resolution images. Without downstream finetuning, our single model achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated language understanding tasks, across 5 domains: documents, tables, charts, natural images, and webpage screenshots. Codes and instruction-tuning datasets are released at <a target="_blank" href="https://github.com/LukeForeverYoung/UReader" title="" class="ltx_ref ltx_href">https://github.com/LukeForeverYoung/UReader</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Leveraging strong Large Language Models as the language decoder, some recent works propose Multimodal Large Language Models (MLLMs) <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2023</a>); Liu et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>); Ye et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>); Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> and achieve promising vision-and-language understanding performance. Surprisingly, without in-domain training, these MLLMs exhibit shallow zero-shot visual text recognition ability when fed a low-resolution image with salient text information <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>); Liu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023b</a>)</cite>. However, due to the variety of image types and the wide range of image sizes, they are still far from universal visually-situated language understanding, such as extracting information from documents, reading texts from webpages, and visual question and answering on tables, as shown in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:intro_case</span>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing works for visually-situated language understanding can be categorized into two-stage <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>); Huang et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>); Yang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite> and end-to-end <cite class="ltx_cite ltx_citemacro_cite">Davis et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Kim et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>); Lee et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> methods according to whether relying on an off-the-shelf OCR model or API. These works all follow a domain-specific pretraining and finetuning paradigm, thus leading to high training costs, e.g. end-to-end model Donut <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> costs more than 192 A100 days.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Inspired by the shallow text recognition ability of existing MLLMs, in this work, we propose <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">UReader</span> for universal OCR-free visually-situated language understanding, which leverages the multimodal Large Language Model via low-cost instruction tuning <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>.
Different from previous works, we forgo pretraining tasks by leveraging the existing MLLM and directly finetune MLLM by taking full advantage of various Visually-situated Language Understanding datasets.
To make the most of the strong language understanding ability of MLLM,
we convert all tasks into the vision-language instruction tuning format. Besides, to enhance text recognition and semantic understanding ability across diverse domains, we design auxiliary text reading and key points generation tasks in the same instruction format.
To utilize the low-resolution encoder of MLLM for processing high-resolution images and avoid blurry and distortion problems due to resizing, we propose a shape-adaptive cropping module to cut a high-resolution image into multiple local images. Each image is firstly independently encoded with the frozen visual encoder and a trainable visual abstractor and then concatenated to feed into the language decoder. Moreover, we add learnable crop position encoding to help the model correlate local images and add a resized global image to alleviate salient information loss due to cropping.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our contributions in this work are four-fold:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We first propose instruction tuning with Multimodal Large Language Models for OCR-free Visually-situated Language Understanding.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We build an instruction-tuning dataset covering 5 domains of visually-situated language understanding: document, table, chart, natural image, and webpage screenshot.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We design a shape-adaptive cropping module to utilize the frozen low-resolution vision encoder for processing high-resolution images.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">UReader achieves state-of-the-art OCR-free performance in 8 out of 10 tasks, across 5 domains.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visually-situated Language Understanding</span> aims to comprehend images containing rich text information. The image types are quite diverse, covering document <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>); Stanislawek et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>); Svetlichnaya (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>); Zhang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>, table <cite class="ltx_cite ltx_citemacro_cite">Pasupat and Liang (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>); Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, chart <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>); Methani et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>); Kafle et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>); Kahou et al. (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite>, natural image <cite class="ltx_cite ltx_citemacro_cite">Singh et al. (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>); Mishra et al. (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>); Biten et al. (<a href="#bib.bib2" title="" class="ltx_ref">2019</a>); Hu et al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>, webpage screenshot <cite class="ltx_cite ltx_citemacro_cite">Tanaka et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>); Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>, etc. Tasks of Visually-situated Language Understanding range from visual question answering, image captioning, information extraction to natural language inference.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">According to whether using off-the-shelf OCR models or APIs to recognize texts from images, existing work can be divided into two-stage models <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>); Huang et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>); Tang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>); Yang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite> and end-to-end models <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>); Davis et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Lee et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>. Two-stage work always designs pretrianing tasks to learn cross-modality alignment between visual inputs and text inputs. For example, for document understanding, UDOP <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite> design a Joint Text-Layout Reconstruction task to recover masked texts and layout information given the visual inputs and retained text inputs. LayoutLMv3 <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> applies a Masked Image Modeling task to recover masked image tokens with the context of their surrounding text and image tokens. Without the help of an off-the-shelf OCR model, end-to-end models need to learn text recognition with a high-resolution image encoder during the pretraining stage. For example, Pix2Struct <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> proposes a Screenshot Parsing pretraining task, where the model needs to generate the complete HTML DOM tree with only a masked webpage screenshot as the input. Donut <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> designs a pretraining task to generate all texts in the document image. These work all follow a domain-specific pretraining and finetuning paradigm and therefore ask for high training costs, e.g. Donut is trained for more than 192 A100 days. In this work, by leveraging the shallow text recognition ability of Multimodal Large Language Models, we propose to directly perform instruction tuning across various types of images and greatly reduce the training cost for universal visually-situated Language Understanding.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Multimodal Large Language Model</span> is developed to empower the Large Language Model with multi-modality understanding ability, especially for vision information. These work <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>); Zhu et al. (<a href="#bib.bib42" title="" class="ltx_ref">2023</a>); Liu et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>); Ye et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>); Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>); Dai et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> mainly connect a pre-trained vision encoder (usually CLIP VIT-L/14 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite>) with a strong large language model, such as LLaMA <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>. These MLLMs show some emergent abilities, including shallow zero-shot text recognition ability <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023b</a>)</cite>. However, they are still far from universal visually-situated language understanding. Firstly, due to the pretraining data for the vision encoder being mostly natural images, MLLMs show barely acceptable text understanding performance on natural images but bad performance on other types, such as document <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023b</a>)</cite>. Secondly, most images for visuall-situated language understanding are high-resolution. Rescaling them to low resolution to adapt to the vision encoder can cause the texts blurry and distorted. In this work, we propose to fully leverage the shallow text recognition ability of MLLMs and perform instruction tuning to enhance its universal understanding ability across 5 domains. Besides, we design a shape-adaptive cropping module to alleviate the text blur and distortion problem.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>UReader</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2310.05126/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="460" height="450" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall architecture of UReader.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The primary goal of UReader is to efficiently utilize existing MLLMs for Visually-situated Language Understanding tasks. In this work, we utilize but are not limited to, the mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite> as our basic MLLM. <a href="#S3.F1" title="In 3 UReader ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> presents an overall architecture of UReader. The input image is firstly pre-processed by a shape-adaptive cropping module (in <a href="#S3.SS1" title="3.1 Shape-Adaptive Cropping Module ‣ 3 UReader ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>). The resulting sub-images are then simultaneously passed through the visual encoder and visual abstractor. To enable the large language model to correlate multiple cropped sub-images, we apply a crop position encoding module to introduce spatial information across sub-images. (in <a href="#S3.SS2" title="3.2 Cropped Images Modeling with LLM ‣ 3 UReader ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Shape-Adaptive Cropping Module</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.10" class="ltx_p">Images with texts have various aspect ratios and a great range of resolutions. Simply resizing the image to <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="H_{v},W_{v}" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.2.2" xref="S3.SS1.p1.1.m1.2.2.3.cmml"><msub id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.2.cmml">H</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.SS1.p1.1.m1.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.1.m1.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.cmml">W</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.3.cmml">v</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><list id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2"><apply id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.2">𝐻</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3">𝑣</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2">𝑊</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.3">𝑣</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">H_{v},W_{v}</annotation></semantics></math> (raw resolution of the MLLM) can result in text being blurred, distorted, and unrecognizable. Thus we propose a shape-adaptive cropping module. Specifically, as shown in <a href="#S3.F2" title="In 3.1 Shape-Adaptive Cropping Module ‣ 3 UReader ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we pre-define grids <math id="S3.SS1.p1.2.m2.2" class="ltx_Math" alttext="\{g=(n_{h}\times n_{w})|n_{h}\cdot n_{w}\leq N_{c},n_{h}\in\mathbb{N},n_{w}\in\mathbb{N}\}" display="inline"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.3.1.cmml">{</mo><mrow id="S3.SS1.p1.2.m2.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.3.cmml">g</mi><mo id="S3.SS1.p1.2.m2.1.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.1.1.2.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.2.cmml">n</mi><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.3.cmml">h</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.1.cmml">×</mo><msub id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.2.cmml">n</mi><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.3.cmml">w</mi></msub></mrow><mo stretchy="false" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.2.2.2.4" xref="S3.SS1.p1.2.m2.2.2.3.1.cmml">|</mo><mrow id="S3.SS1.p1.2.m2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.3.cmml"><mrow id="S3.SS1.p1.2.m2.2.2.2.2.1.1" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.cmml"><msub id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.2.cmml">n</mi><mi id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.3.cmml">h</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.1" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.1.cmml">⋅</mo><msub id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.2" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.2.cmml">n</mi><mi id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.3" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.3.cmml">w</mi></msub></mrow><mo id="S3.SS1.p1.2.m2.2.2.2.2.1.1.1" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.1.cmml">≤</mo><msub id="S3.SS1.p1.2.m2.2.2.2.2.1.1.3" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.2" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.2.cmml">N</mi><mi id="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.3" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.3.cmml">c</mi></msub></mrow><mo id="S3.SS1.p1.2.m2.2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.3.cmml"><mrow id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.cmml"><msub id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.2.cmml">n</mi><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.3.cmml">h</mi></msub><mo id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.1" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.1.cmml">∈</mo><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.3.cmml">ℕ</mi></mrow><mo id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.3a.cmml">,</mo><mrow id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.cmml"><msub id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.2" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.2.cmml">n</mi><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.3.cmml">w</mi></msub><mo id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.1" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.1.cmml">∈</mo><mi id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.3" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.3.cmml">ℕ</mi></mrow></mrow></mrow><mo stretchy="false" id="S3.SS1.p1.2.m2.2.2.2.5" xref="S3.SS1.p1.2.m2.2.2.3.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><apply id="S3.SS1.p1.2.m2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.3">conditional-set</csymbol><apply id="S3.SS1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1"><eq id="S3.SS1.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.2"></eq><ci id="S3.SS1.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.3">𝑔</ci><apply id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.1"></times><apply id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.2">𝑛</ci><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.3">ℎ</ci></apply><apply id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.2">𝑛</ci><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.3">𝑤</ci></apply></apply></apply><apply id="S3.SS1.p1.2.m2.2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.3a.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p1.2.m2.2.2.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1"><leq id="S3.SS1.p1.2.m2.2.2.2.2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.1"></leq><apply id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2"><ci id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.1">⋅</ci><apply id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.2">𝑛</ci><ci id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.2.3">ℎ</ci></apply><apply id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.2">𝑛</ci><ci id="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.2.3.3">𝑤</ci></apply></apply><apply id="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.2">𝑁</ci><ci id="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.1.1.3.3">𝑐</ci></apply></apply><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.2.2.3a.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1"><in id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.1"></in><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.2">𝑛</ci><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.2.3">ℎ</ci></apply><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.1.1.3">ℕ</ci></apply><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2"><in id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.1"></in><apply id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.2">𝑛</ci><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.2.3">𝑤</ci></apply><ci id="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.2.2.2.2.2.2.2.2.3">ℕ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">\{g=(n_{h}\times n_{w})|n_{h}\cdot n_{w}\leq N_{c},n_{h}\in\mathbb{N},n_{w}\in\mathbb{N}\}</annotation></semantics></math> with various shapes, where <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="n_{h}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">n</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑛</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">n_{h}</annotation></semantics></math> and <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="n_{w}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">n</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝑛</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">n_{w}</annotation></semantics></math> denote the number of rows and columns of the grid <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">g</annotation></semantics></math> and <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">N</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">𝑁</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">N_{c}</annotation></semantics></math> denotes the maximum number of the cells (sub-images). To select a suitable grid for an image <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">I</annotation></semantics></math> with shape <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="H\times W" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mrow id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.8.m8.1.1.1" xref="S3.SS1.p1.8.m8.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><times id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1.1"></times><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝐻</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">H\times W</annotation></semantics></math>, two rules should be followed: (1) The grid should preserve the resolution of the image as much as possible, and (2) the grid should fit the aspect ratio of the input image. To measure the resolution coherence and shape similarity between the image and each grid, we calculate the resolution-related and resolution-agnostic insection over union <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="\mathrm{S_{rr}}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><msub id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">rr</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">S</ci><ci id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">rr</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">\mathrm{S_{rr}}</annotation></semantics></math> and <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="\mathrm{S_{ra}}" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><msub id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml">ra</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">S</ci><ci id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">ra</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">\mathrm{S_{ra}}</annotation></semantics></math> as follows:</p>
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1X.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle\mathrm{S_{rr}}(I,g)" display="inline"><semantics id="S3.E1X.2.1.1.m1.2a"><mrow id="S3.E1X.2.1.1.m1.2.3" xref="S3.E1X.2.1.1.m1.2.3.cmml"><msub id="S3.E1X.2.1.1.m1.2.3.2" xref="S3.E1X.2.1.1.m1.2.3.2.cmml"><mi mathvariant="normal" id="S3.E1X.2.1.1.m1.2.3.2.2" xref="S3.E1X.2.1.1.m1.2.3.2.2.cmml">S</mi><mi id="S3.E1X.2.1.1.m1.2.3.2.3" xref="S3.E1X.2.1.1.m1.2.3.2.3.cmml">rr</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.2.3.1" xref="S3.E1X.2.1.1.m1.2.3.1.cmml">​</mo><mrow id="S3.E1X.2.1.1.m1.2.3.3.2" xref="S3.E1X.2.1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.E1X.2.1.1.m1.2.3.3.2.1" xref="S3.E1X.2.1.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml">I</mi><mo id="S3.E1X.2.1.1.m1.2.3.3.2.2" xref="S3.E1X.2.1.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.E1X.2.1.1.m1.2.2" xref="S3.E1X.2.1.1.m1.2.2.cmml">g</mi><mo stretchy="false" id="S3.E1X.2.1.1.m1.2.3.3.2.3" xref="S3.E1X.2.1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.2b"><apply id="S3.E1X.2.1.1.m1.2.3.cmml" xref="S3.E1X.2.1.1.m1.2.3"><times id="S3.E1X.2.1.1.m1.2.3.1.cmml" xref="S3.E1X.2.1.1.m1.2.3.1"></times><apply id="S3.E1X.2.1.1.m1.2.3.2.cmml" xref="S3.E1X.2.1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.2.3.2.1.cmml" xref="S3.E1X.2.1.1.m1.2.3.2">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.2.3.2.2.cmml" xref="S3.E1X.2.1.1.m1.2.3.2.2">S</ci><ci id="S3.E1X.2.1.1.m1.2.3.2.3.cmml" xref="S3.E1X.2.1.1.m1.2.3.2.3">rr</ci></apply><interval closure="open" id="S3.E1X.2.1.1.m1.2.3.3.1.cmml" xref="S3.E1X.2.1.1.m1.2.3.3.2"><ci id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1">𝐼</ci><ci id="S3.E1X.2.1.1.m1.2.2.cmml" xref="S3.E1X.2.1.1.m1.2.2">𝑔</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.2c">\displaystyle\mathrm{S_{rr}}(I,g)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1X.3.2.2.m1.4" class="ltx_Math" alttext="\displaystyle=\mathrm{IoU}\left((H,W),(n_{h}H_{v},n_{w}W_{v})\right)" display="inline"><semantics id="S3.E1X.3.2.2.m1.4a"><mrow id="S3.E1X.3.2.2.m1.4.4" xref="S3.E1X.3.2.2.m1.4.4.cmml"><mi id="S3.E1X.3.2.2.m1.4.4.4" xref="S3.E1X.3.2.2.m1.4.4.4.cmml"></mi><mo id="S3.E1X.3.2.2.m1.4.4.3" xref="S3.E1X.3.2.2.m1.4.4.3.cmml">=</mo><mrow id="S3.E1X.3.2.2.m1.4.4.2" xref="S3.E1X.3.2.2.m1.4.4.2.cmml"><mi id="S3.E1X.3.2.2.m1.4.4.2.4" xref="S3.E1X.3.2.2.m1.4.4.2.4.cmml">IoU</mi><mo lspace="0em" rspace="0em" id="S3.E1X.3.2.2.m1.4.4.2.3" xref="S3.E1X.3.2.2.m1.4.4.2.3.cmml">​</mo><mrow id="S3.E1X.3.2.2.m1.4.4.2.2.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.3.cmml"><mo id="S3.E1X.3.2.2.m1.4.4.2.2.2.3" xref="S3.E1X.3.2.2.m1.4.4.2.2.3.cmml">(</mo><mrow id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2.1" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.cmml">(</mo><mi id="S3.E1X.3.2.2.m1.1.1" xref="S3.E1X.3.2.2.m1.1.1.cmml">H</mi><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2.2" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.cmml">,</mo><mi id="S3.E1X.3.2.2.m1.2.2" xref="S3.E1X.3.2.2.m1.2.2.cmml">W</mi><mo stretchy="false" id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2.3" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1X.3.2.2.m1.4.4.2.2.2.4" xref="S3.E1X.3.2.2.m1.4.4.2.2.3.cmml">,</mo><mrow id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.3" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.3.cmml">(</mo><mrow id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.cmml"><msub id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.cmml"><mi id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.2.cmml">n</mi><mi id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.3" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.1" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.1.cmml">​</mo><msub id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.cmml"><mi id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.2.cmml">H</mi><mi id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.3" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.3.cmml">v</mi></msub></mrow><mo id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.4" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.3.cmml">,</mo><mrow id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.cmml"><msub id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.cmml"><mi id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.2.cmml">n</mi><mi id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.3" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.1" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.1.cmml">​</mo><msub id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.cmml"><mi id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.2" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.2.cmml">W</mi><mi id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.3" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.3.cmml">v</mi></msub></mrow><mo stretchy="false" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.5" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.E1X.3.2.2.m1.4.4.2.2.2.5" xref="S3.E1X.3.2.2.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.3.2.2.m1.4b"><apply id="S3.E1X.3.2.2.m1.4.4.cmml" xref="S3.E1X.3.2.2.m1.4.4"><eq id="S3.E1X.3.2.2.m1.4.4.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.3"></eq><csymbol cd="latexml" id="S3.E1X.3.2.2.m1.4.4.4.cmml" xref="S3.E1X.3.2.2.m1.4.4.4">absent</csymbol><apply id="S3.E1X.3.2.2.m1.4.4.2.cmml" xref="S3.E1X.3.2.2.m1.4.4.2"><times id="S3.E1X.3.2.2.m1.4.4.2.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.3"></times><ci id="S3.E1X.3.2.2.m1.4.4.2.4.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.4">IoU</ci><interval closure="open" id="S3.E1X.3.2.2.m1.4.4.2.2.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2"><interval closure="open" id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2"><ci id="S3.E1X.3.2.2.m1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1">𝐻</ci><ci id="S3.E1X.3.2.2.m1.2.2.cmml" xref="S3.E1X.3.2.2.m1.2.2">𝑊</ci></interval><interval closure="open" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2"><apply id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1"><times id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.1"></times><apply id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.1.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.2.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.2">𝑛</ci><ci id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.2.3">ℎ</ci></apply><apply id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.1.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.2.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.2">𝐻</ci><ci id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.1.1.3.3">𝑣</ci></apply></apply><apply id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2"><times id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.1.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.1"></times><apply id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.1.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.2.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.2">𝑛</ci><ci id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.2.3">𝑤</ci></apply><apply id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.1.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.2.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.2">𝑊</ci><ci id="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.3.cmml" xref="S3.E1X.3.2.2.m1.4.4.2.2.2.2.2.2.3.3">𝑣</ci></apply></apply></interval></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.3.2.2.m1.4c">\displaystyle=\mathrm{IoU}\left((H,W),(n_{h}H_{v},n_{w}W_{v})\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr id="S3.E1Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1Xa.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle\mathrm{S_{ra}}(I,g)" display="inline"><semantics id="S3.E1Xa.2.1.1.m1.2a"><mrow id="S3.E1Xa.2.1.1.m1.2.3" xref="S3.E1Xa.2.1.1.m1.2.3.cmml"><msub id="S3.E1Xa.2.1.1.m1.2.3.2" xref="S3.E1Xa.2.1.1.m1.2.3.2.cmml"><mi mathvariant="normal" id="S3.E1Xa.2.1.1.m1.2.3.2.2" xref="S3.E1Xa.2.1.1.m1.2.3.2.2.cmml">S</mi><mi id="S3.E1Xa.2.1.1.m1.2.3.2.3" xref="S3.E1Xa.2.1.1.m1.2.3.2.3.cmml">ra</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.2.3.1" xref="S3.E1Xa.2.1.1.m1.2.3.1.cmml">​</mo><mrow id="S3.E1Xa.2.1.1.m1.2.3.3.2" xref="S3.E1Xa.2.1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.2.3.3.2.1" xref="S3.E1Xa.2.1.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.E1Xa.2.1.1.m1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.cmml">I</mi><mo id="S3.E1Xa.2.1.1.m1.2.3.3.2.2" xref="S3.E1Xa.2.1.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.E1Xa.2.1.1.m1.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.cmml">g</mi><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.2.3.3.2.3" xref="S3.E1Xa.2.1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xa.2.1.1.m1.2b"><apply id="S3.E1Xa.2.1.1.m1.2.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.3"><times id="S3.E1Xa.2.1.1.m1.2.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.3.1"></times><apply id="S3.E1Xa.2.1.1.m1.2.3.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.2.3.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.3.2">subscript</csymbol><ci id="S3.E1Xa.2.1.1.m1.2.3.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.3.2.2">S</ci><ci id="S3.E1Xa.2.1.1.m1.2.3.2.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.3.2.3">ra</ci></apply><interval closure="open" id="S3.E1Xa.2.1.1.m1.2.3.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.3.3.2"><ci id="S3.E1Xa.2.1.1.m1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1">𝐼</ci><ci id="S3.E1Xa.2.1.1.m1.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2">𝑔</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xa.2.1.1.m1.2c">\displaystyle\mathrm{S_{ra}}(I,g)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1Xa.3.2.2.m1.3" class="ltx_Math" alttext="\displaystyle=\mathrm{IoU}\left((\frac{n_{w}H}{W},n_{w}),(n_{h},n_{w})\right)" display="inline"><semantics id="S3.E1Xa.3.2.2.m1.3a"><mrow id="S3.E1Xa.3.2.2.m1.3.3" xref="S3.E1Xa.3.2.2.m1.3.3.cmml"><mi id="S3.E1Xa.3.2.2.m1.3.3.4" xref="S3.E1Xa.3.2.2.m1.3.3.4.cmml"></mi><mo id="S3.E1Xa.3.2.2.m1.3.3.3" xref="S3.E1Xa.3.2.2.m1.3.3.3.cmml">=</mo><mrow id="S3.E1Xa.3.2.2.m1.3.3.2" xref="S3.E1Xa.3.2.2.m1.3.3.2.cmml"><mi id="S3.E1Xa.3.2.2.m1.3.3.2.4" xref="S3.E1Xa.3.2.2.m1.3.3.2.4.cmml">IoU</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.3.2.2.m1.3.3.2.3" xref="S3.E1Xa.3.2.2.m1.3.3.2.3.cmml">​</mo><mrow id="S3.E1Xa.3.2.2.m1.3.3.2.2.2" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.3.cmml"><mo id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.3" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.3.cmml">(</mo><mrow id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.2" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.2.cmml">(</mo><mstyle displaystyle="true" id="S3.E1Xa.3.2.2.m1.1.1" xref="S3.E1Xa.3.2.2.m1.1.1.cmml"><mfrac id="S3.E1Xa.3.2.2.m1.1.1a" xref="S3.E1Xa.3.2.2.m1.1.1.cmml"><mrow id="S3.E1Xa.3.2.2.m1.1.1.2" xref="S3.E1Xa.3.2.2.m1.1.1.2.cmml"><msub id="S3.E1Xa.3.2.2.m1.1.1.2.2" xref="S3.E1Xa.3.2.2.m1.1.1.2.2.cmml"><mi id="S3.E1Xa.3.2.2.m1.1.1.2.2.2" xref="S3.E1Xa.3.2.2.m1.1.1.2.2.2.cmml">n</mi><mi id="S3.E1Xa.3.2.2.m1.1.1.2.2.3" xref="S3.E1Xa.3.2.2.m1.1.1.2.2.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1Xa.3.2.2.m1.1.1.2.1" xref="S3.E1Xa.3.2.2.m1.1.1.2.1.cmml">​</mo><mi id="S3.E1Xa.3.2.2.m1.1.1.2.3" xref="S3.E1Xa.3.2.2.m1.1.1.2.3.cmml">H</mi></mrow><mi id="S3.E1Xa.3.2.2.m1.1.1.3" xref="S3.E1Xa.3.2.2.m1.1.1.3.cmml">W</mi></mfrac></mstyle><mo id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.3" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.2.cmml">,</mo><msub id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml">n</mi><mi id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.3.cmml">w</mi></msub><mo stretchy="false" id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.4" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow><mo id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.4" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.3.cmml">,</mo><mrow id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.3" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.3.cmml">(</mo><msub id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.cmml"><mi id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.2" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.2.cmml">n</mi><mi id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.3" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.3.cmml">h</mi></msub><mo id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.4" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.3.cmml">,</mo><msub id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.cmml"><mi id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.2" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.2.cmml">n</mi><mi id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.3" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.3.cmml">w</mi></msub><mo stretchy="false" id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.5" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.3.cmml">)</mo></mrow><mo id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.5" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xa.3.2.2.m1.3b"><apply id="S3.E1Xa.3.2.2.m1.3.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3"><eq id="S3.E1Xa.3.2.2.m1.3.3.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.3"></eq><csymbol cd="latexml" id="S3.E1Xa.3.2.2.m1.3.3.4.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.4">absent</csymbol><apply id="S3.E1Xa.3.2.2.m1.3.3.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2"><times id="S3.E1Xa.3.2.2.m1.3.3.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.3"></times><ci id="S3.E1Xa.3.2.2.m1.3.3.2.4.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.4">IoU</ci><interval closure="open" id="S3.E1Xa.3.2.2.m1.3.3.2.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2"><interval closure="open" id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1"><apply id="S3.E1Xa.3.2.2.m1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.1.1"><divide id="S3.E1Xa.3.2.2.m1.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.1.1"></divide><apply id="S3.E1Xa.3.2.2.m1.1.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.1.1.2"><times id="S3.E1Xa.3.2.2.m1.1.1.2.1.cmml" xref="S3.E1Xa.3.2.2.m1.1.1.2.1"></times><apply id="S3.E1Xa.3.2.2.m1.1.1.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1Xa.3.2.2.m1.1.1.2.2.1.cmml" xref="S3.E1Xa.3.2.2.m1.1.1.2.2">subscript</csymbol><ci id="S3.E1Xa.3.2.2.m1.1.1.2.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.1.1.2.2.2">𝑛</ci><ci id="S3.E1Xa.3.2.2.m1.1.1.2.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.1.1.2.2.3">𝑤</ci></apply><ci id="S3.E1Xa.3.2.2.m1.1.1.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.1.1.2.3">𝐻</ci></apply><ci id="S3.E1Xa.3.2.2.m1.1.1.3.cmml" xref="S3.E1Xa.3.2.2.m1.1.1.3">𝑊</ci></apply><apply id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.2">𝑛</ci><ci id="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1Xa.3.2.2.m1.2.2.1.1.1.1.1.1.3">𝑤</ci></apply></interval><interval closure="open" id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2"><apply id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.2">𝑛</ci><ci id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.1.1.3">ℎ</ci></apply><apply id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.2">𝑛</ci><ci id="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.2.2.2.2.2.2.3">𝑤</ci></apply></interval></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xa.3.2.2.m1.3c">\displaystyle=\mathrm{IoU}\left((\frac{n_{w}H}{W},n_{w}),(n_{h},n_{w})\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS1.p1.11" class="ltx_p">where <math id="S3.SS1.p1.11.m1.1" class="ltx_Math" alttext="\mathrm{IoU}" display="inline"><semantics id="S3.SS1.p1.11.m1.1a"><mi id="S3.SS1.p1.11.m1.1.1" xref="S3.SS1.p1.11.m1.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m1.1b"><ci id="S3.SS1.p1.11.m1.1.1.cmml" xref="S3.SS1.p1.11.m1.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m1.1c">\mathrm{IoU}</annotation></semantics></math> denotes the insection over the union between two rectangles centered and aligned with each other.
The matched grid is selected by maximizing the matching score:
</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="g^{*}=\operatorname*{arg\,max}_{g}{\mathrm{S_{ra}}(I,g)+\mathrm{S_{rr}}(I,g)}" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.5" xref="S3.E2.m1.4.5.cmml"><msup id="S3.E2.m1.4.5.2" xref="S3.E2.m1.4.5.2.cmml"><mi id="S3.E2.m1.4.5.2.2" xref="S3.E2.m1.4.5.2.2.cmml">g</mi><mo id="S3.E2.m1.4.5.2.3" xref="S3.E2.m1.4.5.2.3.cmml">∗</mo></msup><mo id="S3.E2.m1.4.5.1" xref="S3.E2.m1.4.5.1.cmml">=</mo><mrow id="S3.E2.m1.4.5.3" xref="S3.E2.m1.4.5.3.cmml"><mrow id="S3.E2.m1.4.5.3.2" xref="S3.E2.m1.4.5.3.2.cmml"><mrow id="S3.E2.m1.4.5.3.2.2" xref="S3.E2.m1.4.5.3.2.2.cmml"><munder id="S3.E2.m1.4.5.3.2.2.1" xref="S3.E2.m1.4.5.3.2.2.1.cmml"><mrow id="S3.E2.m1.4.5.3.2.2.1.2" xref="S3.E2.m1.4.5.3.2.2.1.2.cmml"><mi id="S3.E2.m1.4.5.3.2.2.1.2.2" xref="S3.E2.m1.4.5.3.2.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E2.m1.4.5.3.2.2.1.2.1" xref="S3.E2.m1.4.5.3.2.2.1.2.1.cmml">​</mo><mi id="S3.E2.m1.4.5.3.2.2.1.2.3" xref="S3.E2.m1.4.5.3.2.2.1.2.3.cmml">max</mi></mrow><mi id="S3.E2.m1.4.5.3.2.2.1.3" xref="S3.E2.m1.4.5.3.2.2.1.3.cmml">g</mi></munder><mo id="S3.E2.m1.4.5.3.2.2a" xref="S3.E2.m1.4.5.3.2.2.cmml">⁡</mo><msub id="S3.E2.m1.4.5.3.2.2.2" xref="S3.E2.m1.4.5.3.2.2.2.cmml"><mi mathvariant="normal" id="S3.E2.m1.4.5.3.2.2.2.2" xref="S3.E2.m1.4.5.3.2.2.2.2.cmml">S</mi><mi id="S3.E2.m1.4.5.3.2.2.2.3" xref="S3.E2.m1.4.5.3.2.2.2.3.cmml">ra</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.5.3.2.1" xref="S3.E2.m1.4.5.3.2.1.cmml">​</mo><mrow id="S3.E2.m1.4.5.3.2.3.2" xref="S3.E2.m1.4.5.3.2.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.5.3.2.3.2.1" xref="S3.E2.m1.4.5.3.2.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">I</mi><mo id="S3.E2.m1.4.5.3.2.3.2.2" xref="S3.E2.m1.4.5.3.2.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">g</mi><mo stretchy="false" id="S3.E2.m1.4.5.3.2.3.2.3" xref="S3.E2.m1.4.5.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.5.3.1" xref="S3.E2.m1.4.5.3.1.cmml">+</mo><mrow id="S3.E2.m1.4.5.3.3" xref="S3.E2.m1.4.5.3.3.cmml"><msub id="S3.E2.m1.4.5.3.3.2" xref="S3.E2.m1.4.5.3.3.2.cmml"><mi mathvariant="normal" id="S3.E2.m1.4.5.3.3.2.2" xref="S3.E2.m1.4.5.3.3.2.2.cmml">S</mi><mi id="S3.E2.m1.4.5.3.3.2.3" xref="S3.E2.m1.4.5.3.3.2.3.cmml">rr</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.5.3.3.1" xref="S3.E2.m1.4.5.3.3.1.cmml">​</mo><mrow id="S3.E2.m1.4.5.3.3.3.2" xref="S3.E2.m1.4.5.3.3.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.5.3.3.3.2.1" xref="S3.E2.m1.4.5.3.3.3.1.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">I</mi><mo id="S3.E2.m1.4.5.3.3.3.2.2" xref="S3.E2.m1.4.5.3.3.3.1.cmml">,</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">g</mi><mo stretchy="false" id="S3.E2.m1.4.5.3.3.3.2.3" xref="S3.E2.m1.4.5.3.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.5.cmml" xref="S3.E2.m1.4.5"><eq id="S3.E2.m1.4.5.1.cmml" xref="S3.E2.m1.4.5.1"></eq><apply id="S3.E2.m1.4.5.2.cmml" xref="S3.E2.m1.4.5.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.5.2.1.cmml" xref="S3.E2.m1.4.5.2">superscript</csymbol><ci id="S3.E2.m1.4.5.2.2.cmml" xref="S3.E2.m1.4.5.2.2">𝑔</ci><times id="S3.E2.m1.4.5.2.3.cmml" xref="S3.E2.m1.4.5.2.3"></times></apply><apply id="S3.E2.m1.4.5.3.cmml" xref="S3.E2.m1.4.5.3"><plus id="S3.E2.m1.4.5.3.1.cmml" xref="S3.E2.m1.4.5.3.1"></plus><apply id="S3.E2.m1.4.5.3.2.cmml" xref="S3.E2.m1.4.5.3.2"><times id="S3.E2.m1.4.5.3.2.1.cmml" xref="S3.E2.m1.4.5.3.2.1"></times><apply id="S3.E2.m1.4.5.3.2.2.cmml" xref="S3.E2.m1.4.5.3.2.2"><apply id="S3.E2.m1.4.5.3.2.2.1.cmml" xref="S3.E2.m1.4.5.3.2.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.5.3.2.2.1.1.cmml" xref="S3.E2.m1.4.5.3.2.2.1">subscript</csymbol><apply id="S3.E2.m1.4.5.3.2.2.1.2.cmml" xref="S3.E2.m1.4.5.3.2.2.1.2"><times id="S3.E2.m1.4.5.3.2.2.1.2.1.cmml" xref="S3.E2.m1.4.5.3.2.2.1.2.1"></times><ci id="S3.E2.m1.4.5.3.2.2.1.2.2.cmml" xref="S3.E2.m1.4.5.3.2.2.1.2.2">arg</ci><ci id="S3.E2.m1.4.5.3.2.2.1.2.3.cmml" xref="S3.E2.m1.4.5.3.2.2.1.2.3">max</ci></apply><ci id="S3.E2.m1.4.5.3.2.2.1.3.cmml" xref="S3.E2.m1.4.5.3.2.2.1.3">𝑔</ci></apply><apply id="S3.E2.m1.4.5.3.2.2.2.cmml" xref="S3.E2.m1.4.5.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.5.3.2.2.2.1.cmml" xref="S3.E2.m1.4.5.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.5.3.2.2.2.2.cmml" xref="S3.E2.m1.4.5.3.2.2.2.2">S</ci><ci id="S3.E2.m1.4.5.3.2.2.2.3.cmml" xref="S3.E2.m1.4.5.3.2.2.2.3">ra</ci></apply></apply><interval closure="open" id="S3.E2.m1.4.5.3.2.3.1.cmml" xref="S3.E2.m1.4.5.3.2.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐼</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑔</ci></interval></apply><apply id="S3.E2.m1.4.5.3.3.cmml" xref="S3.E2.m1.4.5.3.3"><times id="S3.E2.m1.4.5.3.3.1.cmml" xref="S3.E2.m1.4.5.3.3.1"></times><apply id="S3.E2.m1.4.5.3.3.2.cmml" xref="S3.E2.m1.4.5.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.5.3.3.2.1.cmml" xref="S3.E2.m1.4.5.3.3.2">subscript</csymbol><ci id="S3.E2.m1.4.5.3.3.2.2.cmml" xref="S3.E2.m1.4.5.3.3.2.2">S</ci><ci id="S3.E2.m1.4.5.3.3.2.3.cmml" xref="S3.E2.m1.4.5.3.3.2.3">rr</ci></apply><interval closure="open" id="S3.E2.m1.4.5.3.3.3.1.cmml" xref="S3.E2.m1.4.5.3.3.3.2"><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝐼</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝑔</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">g^{*}=\operatorname*{arg\,max}_{g}{\mathrm{S_{ra}}(I,g)+\mathrm{S_{rr}}(I,g)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.15" class="ltx_p">where <math id="S3.SS1.p1.12.m1.1" class="ltx_Math" alttext="g^{*}" display="inline"><semantics id="S3.SS1.p1.12.m1.1a"><msup id="S3.SS1.p1.12.m1.1.1" xref="S3.SS1.p1.12.m1.1.1.cmml"><mi id="S3.SS1.p1.12.m1.1.1.2" xref="S3.SS1.p1.12.m1.1.1.2.cmml">g</mi><mo id="S3.SS1.p1.12.m1.1.1.3" xref="S3.SS1.p1.12.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m1.1b"><apply id="S3.SS1.p1.12.m1.1.1.cmml" xref="S3.SS1.p1.12.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m1.1.1.1.cmml" xref="S3.SS1.p1.12.m1.1.1">superscript</csymbol><ci id="S3.SS1.p1.12.m1.1.1.2.cmml" xref="S3.SS1.p1.12.m1.1.1.2">𝑔</ci><times id="S3.SS1.p1.12.m1.1.1.3.cmml" xref="S3.SS1.p1.12.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m1.1c">g^{*}</annotation></semantics></math> is the selected grid. Then, we resize the input image to <math id="S3.SS1.p1.13.m2.2" class="ltx_Math" alttext="(n_{h}H_{v},n_{w}W_{v})" display="inline"><semantics id="S3.SS1.p1.13.m2.2a"><mrow id="S3.SS1.p1.13.m2.2.2.2" xref="S3.SS1.p1.13.m2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.13.m2.2.2.2.3" xref="S3.SS1.p1.13.m2.2.2.3.cmml">(</mo><mrow id="S3.SS1.p1.13.m2.1.1.1.1" xref="S3.SS1.p1.13.m2.1.1.1.1.cmml"><msub id="S3.SS1.p1.13.m2.1.1.1.1.2" xref="S3.SS1.p1.13.m2.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.13.m2.1.1.1.1.2.2" xref="S3.SS1.p1.13.m2.1.1.1.1.2.2.cmml">n</mi><mi id="S3.SS1.p1.13.m2.1.1.1.1.2.3" xref="S3.SS1.p1.13.m2.1.1.1.1.2.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.13.m2.1.1.1.1.1" xref="S3.SS1.p1.13.m2.1.1.1.1.1.cmml">​</mo><msub id="S3.SS1.p1.13.m2.1.1.1.1.3" xref="S3.SS1.p1.13.m2.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.13.m2.1.1.1.1.3.2" xref="S3.SS1.p1.13.m2.1.1.1.1.3.2.cmml">H</mi><mi id="S3.SS1.p1.13.m2.1.1.1.1.3.3" xref="S3.SS1.p1.13.m2.1.1.1.1.3.3.cmml">v</mi></msub></mrow><mo id="S3.SS1.p1.13.m2.2.2.2.4" xref="S3.SS1.p1.13.m2.2.2.3.cmml">,</mo><mrow id="S3.SS1.p1.13.m2.2.2.2.2" xref="S3.SS1.p1.13.m2.2.2.2.2.cmml"><msub id="S3.SS1.p1.13.m2.2.2.2.2.2" xref="S3.SS1.p1.13.m2.2.2.2.2.2.cmml"><mi id="S3.SS1.p1.13.m2.2.2.2.2.2.2" xref="S3.SS1.p1.13.m2.2.2.2.2.2.2.cmml">n</mi><mi id="S3.SS1.p1.13.m2.2.2.2.2.2.3" xref="S3.SS1.p1.13.m2.2.2.2.2.2.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.13.m2.2.2.2.2.1" xref="S3.SS1.p1.13.m2.2.2.2.2.1.cmml">​</mo><msub id="S3.SS1.p1.13.m2.2.2.2.2.3" xref="S3.SS1.p1.13.m2.2.2.2.2.3.cmml"><mi id="S3.SS1.p1.13.m2.2.2.2.2.3.2" xref="S3.SS1.p1.13.m2.2.2.2.2.3.2.cmml">W</mi><mi id="S3.SS1.p1.13.m2.2.2.2.2.3.3" xref="S3.SS1.p1.13.m2.2.2.2.2.3.3.cmml">v</mi></msub></mrow><mo stretchy="false" id="S3.SS1.p1.13.m2.2.2.2.5" xref="S3.SS1.p1.13.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m2.2b"><interval closure="open" id="S3.SS1.p1.13.m2.2.2.3.cmml" xref="S3.SS1.p1.13.m2.2.2.2"><apply id="S3.SS1.p1.13.m2.1.1.1.1.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1"><times id="S3.SS1.p1.13.m2.1.1.1.1.1.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.1"></times><apply id="S3.SS1.p1.13.m2.1.1.1.1.2.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m2.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.13.m2.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.2.2">𝑛</ci><ci id="S3.SS1.p1.13.m2.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.2.3">ℎ</ci></apply><apply id="S3.SS1.p1.13.m2.1.1.1.1.3.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m2.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.13.m2.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.3.2">𝐻</ci><ci id="S3.SS1.p1.13.m2.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.13.m2.1.1.1.1.3.3">𝑣</ci></apply></apply><apply id="S3.SS1.p1.13.m2.2.2.2.2.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2"><times id="S3.SS1.p1.13.m2.2.2.2.2.1.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.1"></times><apply id="S3.SS1.p1.13.m2.2.2.2.2.2.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m2.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.13.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.2.2">𝑛</ci><ci id="S3.SS1.p1.13.m2.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.2.3">𝑤</ci></apply><apply id="S3.SS1.p1.13.m2.2.2.2.2.3.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m2.2.2.2.2.3.1.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.3">subscript</csymbol><ci id="S3.SS1.p1.13.m2.2.2.2.2.3.2.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.3.2">𝑊</ci><ci id="S3.SS1.p1.13.m2.2.2.2.2.3.3.cmml" xref="S3.SS1.p1.13.m2.2.2.2.2.3.3">𝑣</ci></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m2.2c">(n_{h}H_{v},n_{w}W_{v})</annotation></semantics></math> and crop it to <math id="S3.SS1.p1.14.m3.1" class="ltx_Math" alttext="n_{h}\cdot n_{w}" display="inline"><semantics id="S3.SS1.p1.14.m3.1a"><mrow id="S3.SS1.p1.14.m3.1.1" xref="S3.SS1.p1.14.m3.1.1.cmml"><msub id="S3.SS1.p1.14.m3.1.1.2" xref="S3.SS1.p1.14.m3.1.1.2.cmml"><mi id="S3.SS1.p1.14.m3.1.1.2.2" xref="S3.SS1.p1.14.m3.1.1.2.2.cmml">n</mi><mi id="S3.SS1.p1.14.m3.1.1.2.3" xref="S3.SS1.p1.14.m3.1.1.2.3.cmml">h</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.14.m3.1.1.1" xref="S3.SS1.p1.14.m3.1.1.1.cmml">⋅</mo><msub id="S3.SS1.p1.14.m3.1.1.3" xref="S3.SS1.p1.14.m3.1.1.3.cmml"><mi id="S3.SS1.p1.14.m3.1.1.3.2" xref="S3.SS1.p1.14.m3.1.1.3.2.cmml">n</mi><mi id="S3.SS1.p1.14.m3.1.1.3.3" xref="S3.SS1.p1.14.m3.1.1.3.3.cmml">w</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m3.1b"><apply id="S3.SS1.p1.14.m3.1.1.cmml" xref="S3.SS1.p1.14.m3.1.1"><ci id="S3.SS1.p1.14.m3.1.1.1.cmml" xref="S3.SS1.p1.14.m3.1.1.1">⋅</ci><apply id="S3.SS1.p1.14.m3.1.1.2.cmml" xref="S3.SS1.p1.14.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.14.m3.1.1.2.1.cmml" xref="S3.SS1.p1.14.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.14.m3.1.1.2.2.cmml" xref="S3.SS1.p1.14.m3.1.1.2.2">𝑛</ci><ci id="S3.SS1.p1.14.m3.1.1.2.3.cmml" xref="S3.SS1.p1.14.m3.1.1.2.3">ℎ</ci></apply><apply id="S3.SS1.p1.14.m3.1.1.3.cmml" xref="S3.SS1.p1.14.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.14.m3.1.1.3.1.cmml" xref="S3.SS1.p1.14.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.14.m3.1.1.3.2.cmml" xref="S3.SS1.p1.14.m3.1.1.3.2">𝑛</ci><ci id="S3.SS1.p1.14.m3.1.1.3.3.cmml" xref="S3.SS1.p1.14.m3.1.1.3.3">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m3.1c">n_{h}\cdot n_{w}</annotation></semantics></math> local images.
To maintain the global structure information of the image, we also resize the input image to <math id="S3.SS1.p1.15.m4.2" class="ltx_Math" alttext="(H_{v},W_{v})" display="inline"><semantics id="S3.SS1.p1.15.m4.2a"><mrow id="S3.SS1.p1.15.m4.2.2.2" xref="S3.SS1.p1.15.m4.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p1.15.m4.2.2.2.3" xref="S3.SS1.p1.15.m4.2.2.3.cmml">(</mo><msub id="S3.SS1.p1.15.m4.1.1.1.1" xref="S3.SS1.p1.15.m4.1.1.1.1.cmml"><mi id="S3.SS1.p1.15.m4.1.1.1.1.2" xref="S3.SS1.p1.15.m4.1.1.1.1.2.cmml">H</mi><mi id="S3.SS1.p1.15.m4.1.1.1.1.3" xref="S3.SS1.p1.15.m4.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.SS1.p1.15.m4.2.2.2.4" xref="S3.SS1.p1.15.m4.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.15.m4.2.2.2.2" xref="S3.SS1.p1.15.m4.2.2.2.2.cmml"><mi id="S3.SS1.p1.15.m4.2.2.2.2.2" xref="S3.SS1.p1.15.m4.2.2.2.2.2.cmml">W</mi><mi id="S3.SS1.p1.15.m4.2.2.2.2.3" xref="S3.SS1.p1.15.m4.2.2.2.2.3.cmml">v</mi></msub><mo stretchy="false" id="S3.SS1.p1.15.m4.2.2.2.5" xref="S3.SS1.p1.15.m4.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m4.2b"><interval closure="open" id="S3.SS1.p1.15.m4.2.2.3.cmml" xref="S3.SS1.p1.15.m4.2.2.2"><apply id="S3.SS1.p1.15.m4.1.1.1.1.cmml" xref="S3.SS1.p1.15.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m4.1.1.1.1.1.cmml" xref="S3.SS1.p1.15.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.15.m4.1.1.1.1.2.cmml" xref="S3.SS1.p1.15.m4.1.1.1.1.2">𝐻</ci><ci id="S3.SS1.p1.15.m4.1.1.1.1.3.cmml" xref="S3.SS1.p1.15.m4.1.1.1.1.3">𝑣</ci></apply><apply id="S3.SS1.p1.15.m4.2.2.2.2.cmml" xref="S3.SS1.p1.15.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.15.m4.2.2.2.2.1.cmml" xref="S3.SS1.p1.15.m4.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.15.m4.2.2.2.2.2.cmml" xref="S3.SS1.p1.15.m4.2.2.2.2.2">𝑊</ci><ci id="S3.SS1.p1.15.m4.2.2.2.2.3.cmml" xref="S3.SS1.p1.15.m4.2.2.2.2.3">𝑣</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m4.2c">(H_{v},W_{v})</annotation></semantics></math> as a global image. All images are then passed on to the visual encoder and visual abstractor in parallel.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.8" class="ltx_p">The visual encoder extracts visual feature <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="V\in\mathbb{R}^{N\times(H^{\prime}\cdot W^{\prime})\times d_{v}}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.2" xref="S3.SS1.p2.1.m1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.2.2" xref="S3.SS1.p2.1.m1.1.2.2.cmml">V</mi><mo id="S3.SS1.p2.1.m1.1.2.1" xref="S3.SS1.p2.1.m1.1.2.1.cmml">∈</mo><msup id="S3.SS1.p2.1.m1.1.2.3" xref="S3.SS1.p2.1.m1.1.2.3.cmml"><mi id="S3.SS1.p2.1.m1.1.2.3.2" xref="S3.SS1.p2.1.m1.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.1.m1.1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.1.2.cmml">×</mo><mrow id="S3.SS1.p2.1.m1.1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.1.m1.1.1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.cmml"><msup id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.2" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.2.cmml">H</mi><mo id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.3" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.3.cmml">′</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.cmml">⋅</mo><msup id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.2.cmml">W</mi><mo id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo rspace="0.055em" stretchy="false" id="S3.SS1.p2.1.m1.1.1.1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.SS1.p2.1.m1.1.1.1.2a" xref="S3.SS1.p2.1.m1.1.1.1.2.cmml">×</mo><msub id="S3.SS1.p2.1.m1.1.1.1.4" xref="S3.SS1.p2.1.m1.1.1.1.4.cmml"><mi id="S3.SS1.p2.1.m1.1.1.1.4.2" xref="S3.SS1.p2.1.m1.1.1.1.4.2.cmml">d</mi><mi id="S3.SS1.p2.1.m1.1.1.1.4.3" xref="S3.SS1.p2.1.m1.1.1.1.4.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.2"><in id="S3.SS1.p2.1.m1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.2.1"></in><ci id="S3.SS1.p2.1.m1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.2.2">𝑉</ci><apply id="S3.SS1.p2.1.m1.1.2.3.cmml" xref="S3.SS1.p2.1.m1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.2.3.1.cmml" xref="S3.SS1.p2.1.m1.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.2.3.2.cmml" xref="S3.SS1.p2.1.m1.1.2.3.2">ℝ</ci><apply id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.2"></times><ci id="S3.SS1.p2.1.m1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.3">𝑁</ci><apply id="S3.SS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1"><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.1">⋅</ci><apply id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.2">𝐻</ci><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.2.3">′</ci></apply><apply id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.2">𝑊</ci><ci id="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.1.1.1.3.3">′</ci></apply></apply><apply id="S3.SS1.p2.1.m1.1.1.1.4.cmml" xref="S3.SS1.p2.1.m1.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.4.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1.4">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.1.4.2.cmml" xref="S3.SS1.p2.1.m1.1.1.1.4.2">𝑑</ci><ci id="S3.SS1.p2.1.m1.1.1.1.4.3.cmml" xref="S3.SS1.p2.1.m1.1.1.1.4.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">V\in\mathbb{R}^{N\times(H^{\prime}\cdot W^{\prime})\times d_{v}}</annotation></semantics></math> from the input images <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{I}\in\mathbb{R}^{N\times H\times W\times 3}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">𝐈</mi><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.2.m2.1.1.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3.3.2" xref="S3.SS1.p2.2.m2.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.3.3.1" xref="S3.SS1.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.2.m2.1.1.3.3.3" xref="S3.SS1.p2.2.m2.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.3.3.1a" xref="S3.SS1.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.2.m2.1.1.3.3.4" xref="S3.SS1.p2.2.m2.1.1.3.3.4.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.2.m2.1.1.3.3.1b" xref="S3.SS1.p2.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS1.p2.2.m2.1.1.3.3.5" xref="S3.SS1.p2.2.m2.1.1.3.3.5.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><in id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></in><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝐈</ci><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3"><times id="S3.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.2">𝑁</ci><ci id="S3.SS1.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.3">𝐻</ci><ci id="S3.SS1.p2.2.m2.1.1.3.3.4.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.4">𝑊</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.3.5.cmml" xref="S3.SS1.p2.2.m2.1.1.3.3.5">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathbf{I}\in\mathbb{R}^{N\times H\times W\times 3}</annotation></semantics></math>, where <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="N=(n_{h}\cdot n_{w})+1" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">N</mi><mo id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml"><mrow id="S3.SS1.p2.3.m3.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.3.m3.1.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.2.cmml">n</mi><mi id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.3.cmml">h</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.1.cmml">⋅</mo><msub id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.2.cmml">n</mi><mi id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.3.cmml">w</mi></msub></mrow><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS1.p2.3.m3.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.2.cmml">+</mo><mn id="S3.SS1.p2.3.m3.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><eq id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2"></eq><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝑁</ci><apply id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"><plus id="S3.SS1.p2.3.m3.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.2"></plus><apply id="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1"><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.1">⋅</ci><apply id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.2">𝑛</ci><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.3">ℎ</ci></apply><apply id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.2">𝑛</ci><ci id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.3">𝑤</ci></apply></apply><cn type="integer" id="S3.SS1.p2.3.m3.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">N=(n_{h}\cdot n_{w})+1</annotation></semantics></math>, <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="H^{\prime}\cdot W^{\prime}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><msup id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2.2" xref="S3.SS1.p2.4.m4.1.1.2.2.cmml">H</mi><mo id="S3.SS1.p2.4.m4.1.1.2.3" xref="S3.SS1.p2.4.m4.1.1.2.3.cmml">′</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">⋅</mo><msup id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">W</mi><mo id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><ci id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1">⋅</ci><apply id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2.2">𝐻</ci><ci id="S3.SS1.p2.4.m4.1.1.2.3.cmml" xref="S3.SS1.p2.4.m4.1.1.2.3">′</ci></apply><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">𝑊</ci><ci id="S3.SS1.p2.4.m4.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">H^{\prime}\cdot W^{\prime}</annotation></semantics></math> and <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="d_{v}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">d</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝑑</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">d_{v}</annotation></semantics></math> denote the number and dimension of the extracted visual features, respectively. The visual abstractor further summarizes visual information and obtains higher semantic visual representations <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="V^{l}\in\mathbb{R}^{N\times N_{q}\times d_{l}}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><msup id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2.2" xref="S3.SS1.p2.6.m6.1.1.2.2.cmml">V</mi><mi id="S3.SS1.p2.6.m6.1.1.2.3" xref="S3.SS1.p2.6.m6.1.1.2.3.cmml">l</mi></msup><mo id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.2" xref="S3.SS1.p2.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.6.m6.1.1.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.3.2" xref="S3.SS1.p2.6.m6.1.1.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.3.3.1" xref="S3.SS1.p2.6.m6.1.1.3.3.1.cmml">×</mo><msub id="S3.SS1.p2.6.m6.1.1.3.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.3.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.3.3.2" xref="S3.SS1.p2.6.m6.1.1.3.3.3.2.cmml">N</mi><mi id="S3.SS1.p2.6.m6.1.1.3.3.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.3.3.cmml">q</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.3.3.1a" xref="S3.SS1.p2.6.m6.1.1.3.3.1.cmml">×</mo><msub id="S3.SS1.p2.6.m6.1.1.3.3.4" xref="S3.SS1.p2.6.m6.1.1.3.3.4.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.3.4.2" xref="S3.SS1.p2.6.m6.1.1.3.3.4.2.cmml">d</mi><mi id="S3.SS1.p2.6.m6.1.1.3.3.4.3" xref="S3.SS1.p2.6.m6.1.1.3.3.4.3.cmml">l</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><in id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1"></in><apply id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.2.1.cmml" xref="S3.SS1.p2.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2.2">𝑉</ci><ci id="S3.SS1.p2.6.m6.1.1.2.3.cmml" xref="S3.SS1.p2.6.m6.1.1.2.3">𝑙</ci></apply><apply id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.6.m6.1.1.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3"><times id="S3.SS1.p2.6.m6.1.1.3.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.1"></times><ci id="S3.SS1.p2.6.m6.1.1.3.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.2">𝑁</ci><apply id="S3.SS1.p2.6.m6.1.1.3.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.3.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.3.3.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.3.2">𝑁</ci><ci id="S3.SS1.p2.6.m6.1.1.3.3.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.3.3">𝑞</ci></apply><apply id="S3.SS1.p2.6.m6.1.1.3.3.4.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.3.4.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.4">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.3.3.4.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.4.2">𝑑</ci><ci id="S3.SS1.p2.6.m6.1.1.3.3.4.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3.3.4.3">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">V^{l}\in\mathbb{R}^{N\times N_{q}\times d_{l}}</annotation></semantics></math> in language feature space by several learnable queries, where <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="d_{l}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">d</mi><mi id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝑑</ci><ci id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">d_{l}</annotation></semantics></math> denotes the dimension of language feature space and <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="N_{q}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><msub id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">N</mi><mi id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝑁</ci><ci id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">N_{q}</annotation></semantics></math> denotes the number of learnable queries.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2310.05126/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="462" height="396" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The Shape-Adaptive Cropping Module.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Cropped Images Modeling with LLM</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">MLLMs are mostly trained with a single image as the input. Due to the cropping module, we need to input visual features from multiple images into the language model. The 1-dimensional position embeddings of LLM can not reflect the spatial position of each sub-image, which is critical to correlate local images.
Therefore, we incorporate a 2-dimensional crop position encoding to help the language model to understand the spatial relationship between cropped images. Specifically, we assign a location index <math id="S3.SS2.p1.1.m1.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.1" xref="S3.SS2.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">i</mi><mo id="S3.SS2.p1.1.m1.2.3.2.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.3" xref="S3.SS2.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><interval closure="open" id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑖</ci><ci id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">(i,j)</annotation></semantics></math> for each cell of the selected grid and obtain their row embedding and column embedding by two auxiliary embedding layers as follows:</p>
<table id="S3.E3" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E3X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3X.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle\mathbf{e}^{row}_{i,j}" display="inline"><semantics id="S3.E3X.2.1.1.m1.2a"><msubsup id="S3.E3X.2.1.1.m1.2.3" xref="S3.E3X.2.1.1.m1.2.3.cmml"><mi id="S3.E3X.2.1.1.m1.2.3.2.2" xref="S3.E3X.2.1.1.m1.2.3.2.2.cmml">𝐞</mi><mrow id="S3.E3X.2.1.1.m1.2.2.2.4" xref="S3.E3X.2.1.1.m1.2.2.2.3.cmml"><mi id="S3.E3X.2.1.1.m1.1.1.1.1" xref="S3.E3X.2.1.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E3X.2.1.1.m1.2.2.2.4.1" xref="S3.E3X.2.1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E3X.2.1.1.m1.2.2.2.2" xref="S3.E3X.2.1.1.m1.2.2.2.2.cmml">j</mi></mrow><mrow id="S3.E3X.2.1.1.m1.2.3.2.3" xref="S3.E3X.2.1.1.m1.2.3.2.3.cmml"><mi id="S3.E3X.2.1.1.m1.2.3.2.3.2" xref="S3.E3X.2.1.1.m1.2.3.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.2.3.2.3.1" xref="S3.E3X.2.1.1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.2.3.2.3.3" xref="S3.E3X.2.1.1.m1.2.3.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3X.2.1.1.m1.2.3.2.3.1a" xref="S3.E3X.2.1.1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.E3X.2.1.1.m1.2.3.2.3.4" xref="S3.E3X.2.1.1.m1.2.3.2.3.4.cmml">w</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.E3X.2.1.1.m1.2b"><apply id="S3.E3X.2.1.1.m1.2.3.cmml" xref="S3.E3X.2.1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.2.3.1.cmml" xref="S3.E3X.2.1.1.m1.2.3">subscript</csymbol><apply id="S3.E3X.2.1.1.m1.2.3.2.cmml" xref="S3.E3X.2.1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.2.3.2.1.cmml" xref="S3.E3X.2.1.1.m1.2.3">superscript</csymbol><ci id="S3.E3X.2.1.1.m1.2.3.2.2.cmml" xref="S3.E3X.2.1.1.m1.2.3.2.2">𝐞</ci><apply id="S3.E3X.2.1.1.m1.2.3.2.3.cmml" xref="S3.E3X.2.1.1.m1.2.3.2.3"><times id="S3.E3X.2.1.1.m1.2.3.2.3.1.cmml" xref="S3.E3X.2.1.1.m1.2.3.2.3.1"></times><ci id="S3.E3X.2.1.1.m1.2.3.2.3.2.cmml" xref="S3.E3X.2.1.1.m1.2.3.2.3.2">𝑟</ci><ci id="S3.E3X.2.1.1.m1.2.3.2.3.3.cmml" xref="S3.E3X.2.1.1.m1.2.3.2.3.3">𝑜</ci><ci id="S3.E3X.2.1.1.m1.2.3.2.3.4.cmml" xref="S3.E3X.2.1.1.m1.2.3.2.3.4">𝑤</ci></apply></apply><list id="S3.E3X.2.1.1.m1.2.2.2.3.cmml" xref="S3.E3X.2.1.1.m1.2.2.2.4"><ci id="S3.E3X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1">𝑖</ci><ci id="S3.E3X.2.1.1.m1.2.2.2.2.cmml" xref="S3.E3X.2.1.1.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.2.1.1.m1.2c">\displaystyle\mathbf{e}^{row}_{i,j}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3X.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=\mathrm{Embedding_{row}}(i)" display="inline"><semantics id="S3.E3X.3.2.2.m1.1a"><mrow id="S3.E3X.3.2.2.m1.1.2" xref="S3.E3X.3.2.2.m1.1.2.cmml"><mi id="S3.E3X.3.2.2.m1.1.2.2" xref="S3.E3X.3.2.2.m1.1.2.2.cmml"></mi><mo id="S3.E3X.3.2.2.m1.1.2.1" xref="S3.E3X.3.2.2.m1.1.2.1.cmml">=</mo><mrow id="S3.E3X.3.2.2.m1.1.2.3" xref="S3.E3X.3.2.2.m1.1.2.3.cmml"><msub id="S3.E3X.3.2.2.m1.1.2.3.2" xref="S3.E3X.3.2.2.m1.1.2.3.2.cmml"><mi id="S3.E3X.3.2.2.m1.1.2.3.2.2" xref="S3.E3X.3.2.2.m1.1.2.3.2.2.cmml">Embedding</mi><mi id="S3.E3X.3.2.2.m1.1.2.3.2.3" xref="S3.E3X.3.2.2.m1.1.2.3.2.3.cmml">row</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.2.3.1" xref="S3.E3X.3.2.2.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E3X.3.2.2.m1.1.2.3.3.2" xref="S3.E3X.3.2.2.m1.1.2.3.cmml"><mo stretchy="false" id="S3.E3X.3.2.2.m1.1.2.3.3.2.1" xref="S3.E3X.3.2.2.m1.1.2.3.cmml">(</mo><mi id="S3.E3X.3.2.2.m1.1.1" xref="S3.E3X.3.2.2.m1.1.1.cmml">i</mi><mo stretchy="false" id="S3.E3X.3.2.2.m1.1.2.3.3.2.2" xref="S3.E3X.3.2.2.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3X.3.2.2.m1.1b"><apply id="S3.E3X.3.2.2.m1.1.2.cmml" xref="S3.E3X.3.2.2.m1.1.2"><eq id="S3.E3X.3.2.2.m1.1.2.1.cmml" xref="S3.E3X.3.2.2.m1.1.2.1"></eq><csymbol cd="latexml" id="S3.E3X.3.2.2.m1.1.2.2.cmml" xref="S3.E3X.3.2.2.m1.1.2.2">absent</csymbol><apply id="S3.E3X.3.2.2.m1.1.2.3.cmml" xref="S3.E3X.3.2.2.m1.1.2.3"><times id="S3.E3X.3.2.2.m1.1.2.3.1.cmml" xref="S3.E3X.3.2.2.m1.1.2.3.1"></times><apply id="S3.E3X.3.2.2.m1.1.2.3.2.cmml" xref="S3.E3X.3.2.2.m1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E3X.3.2.2.m1.1.2.3.2.1.cmml" xref="S3.E3X.3.2.2.m1.1.2.3.2">subscript</csymbol><ci id="S3.E3X.3.2.2.m1.1.2.3.2.2.cmml" xref="S3.E3X.3.2.2.m1.1.2.3.2.2">Embedding</ci><ci id="S3.E3X.3.2.2.m1.1.2.3.2.3.cmml" xref="S3.E3X.3.2.2.m1.1.2.3.2.3">row</ci></apply><ci id="S3.E3X.3.2.2.m1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.3.2.2.m1.1c">\displaystyle=\mathrm{Embedding_{row}}(i)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="3" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
<tr id="S3.E3Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3Xa.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle\mathbf{e}^{column}_{i,j}" display="inline"><semantics id="S3.E3Xa.2.1.1.m1.2a"><msubsup id="S3.E3Xa.2.1.1.m1.2.3" xref="S3.E3Xa.2.1.1.m1.2.3.cmml"><mi id="S3.E3Xa.2.1.1.m1.2.3.2.2" xref="S3.E3Xa.2.1.1.m1.2.3.2.2.cmml">𝐞</mi><mrow id="S3.E3Xa.2.1.1.m1.2.2.2.4" xref="S3.E3Xa.2.1.1.m1.2.2.2.3.cmml"><mi id="S3.E3Xa.2.1.1.m1.1.1.1.1" xref="S3.E3Xa.2.1.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E3Xa.2.1.1.m1.2.2.2.4.1" xref="S3.E3Xa.2.1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E3Xa.2.1.1.m1.2.2.2.2" xref="S3.E3Xa.2.1.1.m1.2.2.2.2.cmml">j</mi></mrow><mrow id="S3.E3Xa.2.1.1.m1.2.3.2.3" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.cmml"><mi id="S3.E3Xa.2.1.1.m1.2.3.2.3.2" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.2.1.1.m1.2.3.2.3.1" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.E3Xa.2.1.1.m1.2.3.2.3.3" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.2.1.1.m1.2.3.2.3.1a" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.E3Xa.2.1.1.m1.2.3.2.3.4" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.2.1.1.m1.2.3.2.3.1b" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.E3Xa.2.1.1.m1.2.3.2.3.5" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.2.1.1.m1.2.3.2.3.1c" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.E3Xa.2.1.1.m1.2.3.2.3.6" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.2.1.1.m1.2.3.2.3.1d" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.1.cmml">​</mo><mi id="S3.E3Xa.2.1.1.m1.2.3.2.3.7" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.7.cmml">n</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.E3Xa.2.1.1.m1.2b"><apply id="S3.E3Xa.2.1.1.m1.2.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.E3Xa.2.1.1.m1.2.3.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.3">subscript</csymbol><apply id="S3.E3Xa.2.1.1.m1.2.3.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.E3Xa.2.1.1.m1.2.3.2.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.3">superscript</csymbol><ci id="S3.E3Xa.2.1.1.m1.2.3.2.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.2">𝐞</ci><apply id="S3.E3Xa.2.1.1.m1.2.3.2.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.3"><times id="S3.E3Xa.2.1.1.m1.2.3.2.3.1.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.1"></times><ci id="S3.E3Xa.2.1.1.m1.2.3.2.3.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.2">𝑐</ci><ci id="S3.E3Xa.2.1.1.m1.2.3.2.3.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.3">𝑜</ci><ci id="S3.E3Xa.2.1.1.m1.2.3.2.3.4.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.4">𝑙</ci><ci id="S3.E3Xa.2.1.1.m1.2.3.2.3.5.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.5">𝑢</ci><ci id="S3.E3Xa.2.1.1.m1.2.3.2.3.6.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.6">𝑚</ci><ci id="S3.E3Xa.2.1.1.m1.2.3.2.3.7.cmml" xref="S3.E3Xa.2.1.1.m1.2.3.2.3.7">𝑛</ci></apply></apply><list id="S3.E3Xa.2.1.1.m1.2.2.2.3.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.2.4"><ci id="S3.E3Xa.2.1.1.m1.1.1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.1.1">𝑖</ci><ci id="S3.E3Xa.2.1.1.m1.2.2.2.2.cmml" xref="S3.E3Xa.2.1.1.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xa.2.1.1.m1.2c">\displaystyle\mathbf{e}^{column}_{i,j}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3Xa.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=\mathrm{Embedding_{column}}(j)" display="inline"><semantics id="S3.E3Xa.3.2.2.m1.1a"><mrow id="S3.E3Xa.3.2.2.m1.1.2" xref="S3.E3Xa.3.2.2.m1.1.2.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.2.2" xref="S3.E3Xa.3.2.2.m1.1.2.2.cmml"></mi><mo id="S3.E3Xa.3.2.2.m1.1.2.1" xref="S3.E3Xa.3.2.2.m1.1.2.1.cmml">=</mo><mrow id="S3.E3Xa.3.2.2.m1.1.2.3" xref="S3.E3Xa.3.2.2.m1.1.2.3.cmml"><msub id="S3.E3Xa.3.2.2.m1.1.2.3.2" xref="S3.E3Xa.3.2.2.m1.1.2.3.2.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.2.3.2.2" xref="S3.E3Xa.3.2.2.m1.1.2.3.2.2.cmml">Embedding</mi><mi id="S3.E3Xa.3.2.2.m1.1.2.3.2.3" xref="S3.E3Xa.3.2.2.m1.1.2.3.2.3.cmml">column</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.2.3.1" xref="S3.E3Xa.3.2.2.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E3Xa.3.2.2.m1.1.2.3.3.2" xref="S3.E3Xa.3.2.2.m1.1.2.3.cmml"><mo stretchy="false" id="S3.E3Xa.3.2.2.m1.1.2.3.3.2.1" xref="S3.E3Xa.3.2.2.m1.1.2.3.cmml">(</mo><mi id="S3.E3Xa.3.2.2.m1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.cmml">j</mi><mo stretchy="false" id="S3.E3Xa.3.2.2.m1.1.2.3.3.2.2" xref="S3.E3Xa.3.2.2.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3Xa.3.2.2.m1.1b"><apply id="S3.E3Xa.3.2.2.m1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.2"><eq id="S3.E3Xa.3.2.2.m1.1.2.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.2.1"></eq><csymbol cd="latexml" id="S3.E3Xa.3.2.2.m1.1.2.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.2.2">absent</csymbol><apply id="S3.E3Xa.3.2.2.m1.1.2.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.2.3"><times id="S3.E3Xa.3.2.2.m1.1.2.3.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.2.3.1"></times><apply id="S3.E3Xa.3.2.2.m1.1.2.3.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E3Xa.3.2.2.m1.1.2.3.2.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.2.3.2">subscript</csymbol><ci id="S3.E3Xa.3.2.2.m1.1.2.3.2.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.2.3.2.2">Embedding</ci><ci id="S3.E3Xa.3.2.2.m1.1.2.3.2.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.2.3.2.3">column</ci></apply><ci id="S3.E3Xa.3.2.2.m1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xa.3.2.2.m1.1c">\displaystyle=\mathrm{Embedding_{column}}(j)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S3.E3Xb" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3Xb.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle\mathbf{e}_{i,j}" display="inline"><semantics id="S3.E3Xb.2.1.1.m1.2a"><msub id="S3.E3Xb.2.1.1.m1.2.3" xref="S3.E3Xb.2.1.1.m1.2.3.cmml"><mi id="S3.E3Xb.2.1.1.m1.2.3.2" xref="S3.E3Xb.2.1.1.m1.2.3.2.cmml">𝐞</mi><mrow id="S3.E3Xb.2.1.1.m1.2.2.2.4" xref="S3.E3Xb.2.1.1.m1.2.2.2.3.cmml"><mi id="S3.E3Xb.2.1.1.m1.1.1.1.1" xref="S3.E3Xb.2.1.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E3Xb.2.1.1.m1.2.2.2.4.1" xref="S3.E3Xb.2.1.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E3Xb.2.1.1.m1.2.2.2.2" xref="S3.E3Xb.2.1.1.m1.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E3Xb.2.1.1.m1.2b"><apply id="S3.E3Xb.2.1.1.m1.2.3.cmml" xref="S3.E3Xb.2.1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.E3Xb.2.1.1.m1.2.3.1.cmml" xref="S3.E3Xb.2.1.1.m1.2.3">subscript</csymbol><ci id="S3.E3Xb.2.1.1.m1.2.3.2.cmml" xref="S3.E3Xb.2.1.1.m1.2.3.2">𝐞</ci><list id="S3.E3Xb.2.1.1.m1.2.2.2.3.cmml" xref="S3.E3Xb.2.1.1.m1.2.2.2.4"><ci id="S3.E3Xb.2.1.1.m1.1.1.1.1.cmml" xref="S3.E3Xb.2.1.1.m1.1.1.1.1">𝑖</ci><ci id="S3.E3Xb.2.1.1.m1.2.2.2.2.cmml" xref="S3.E3Xb.2.1.1.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xb.2.1.1.m1.2c">\displaystyle\mathbf{e}_{i,j}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3Xb.3.2.2.m1.4" class="ltx_Math" alttext="\displaystyle=\mathbf{e}^{row}_{i,j}+\mathbf{e}^{column}_{i,j}" display="inline"><semantics id="S3.E3Xb.3.2.2.m1.4a"><mrow id="S3.E3Xb.3.2.2.m1.4.5" xref="S3.E3Xb.3.2.2.m1.4.5.cmml"><mi id="S3.E3Xb.3.2.2.m1.4.5.2" xref="S3.E3Xb.3.2.2.m1.4.5.2.cmml"></mi><mo id="S3.E3Xb.3.2.2.m1.4.5.1" xref="S3.E3Xb.3.2.2.m1.4.5.1.cmml">=</mo><mrow id="S3.E3Xb.3.2.2.m1.4.5.3" xref="S3.E3Xb.3.2.2.m1.4.5.3.cmml"><msubsup id="S3.E3Xb.3.2.2.m1.4.5.3.2" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.cmml"><mi id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.2" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.2.cmml">𝐞</mi><mrow id="S3.E3Xb.3.2.2.m1.2.2.2.4" xref="S3.E3Xb.3.2.2.m1.2.2.2.3.cmml"><mi id="S3.E3Xb.3.2.2.m1.1.1.1.1" xref="S3.E3Xb.3.2.2.m1.1.1.1.1.cmml">i</mi><mo id="S3.E3Xb.3.2.2.m1.2.2.2.4.1" xref="S3.E3Xb.3.2.2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E3Xb.3.2.2.m1.2.2.2.2" xref="S3.E3Xb.3.2.2.m1.2.2.2.2.cmml">j</mi></mrow><mrow id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.cmml"><mi id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.2" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.1" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.1.cmml">​</mo><mi id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.3" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.1a" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.1.cmml">​</mo><mi id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.4" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.4.cmml">w</mi></mrow></msubsup><mo id="S3.E3Xb.3.2.2.m1.4.5.3.1" xref="S3.E3Xb.3.2.2.m1.4.5.3.1.cmml">+</mo><msubsup id="S3.E3Xb.3.2.2.m1.4.5.3.3" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.cmml"><mi id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.2" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.2.cmml">𝐞</mi><mrow id="S3.E3Xb.3.2.2.m1.4.4.2.4" xref="S3.E3Xb.3.2.2.m1.4.4.2.3.cmml"><mi id="S3.E3Xb.3.2.2.m1.3.3.1.1" xref="S3.E3Xb.3.2.2.m1.3.3.1.1.cmml">i</mi><mo id="S3.E3Xb.3.2.2.m1.4.4.2.4.1" xref="S3.E3Xb.3.2.2.m1.4.4.2.3.cmml">,</mo><mi id="S3.E3Xb.3.2.2.m1.4.4.2.2" xref="S3.E3Xb.3.2.2.m1.4.4.2.2.cmml">j</mi></mrow><mrow id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.cmml"><mi id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.2" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1.cmml">​</mo><mi id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.3" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1a" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1.cmml">​</mo><mi id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.4" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1b" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1.cmml">​</mo><mi id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.5" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1c" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1.cmml">​</mo><mi id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.6" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1d" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1.cmml">​</mo><mi id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.7" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.7.cmml">n</mi></mrow></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3Xb.3.2.2.m1.4b"><apply id="S3.E3Xb.3.2.2.m1.4.5.cmml" xref="S3.E3Xb.3.2.2.m1.4.5"><eq id="S3.E3Xb.3.2.2.m1.4.5.1.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.1"></eq><csymbol cd="latexml" id="S3.E3Xb.3.2.2.m1.4.5.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.2">absent</csymbol><apply id="S3.E3Xb.3.2.2.m1.4.5.3.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3"><plus id="S3.E3Xb.3.2.2.m1.4.5.3.1.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.1"></plus><apply id="S3.E3Xb.3.2.2.m1.4.5.3.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2"><csymbol cd="ambiguous" id="S3.E3Xb.3.2.2.m1.4.5.3.2.1.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2">subscript</csymbol><apply id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2"><csymbol cd="ambiguous" id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.1.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2">superscript</csymbol><ci id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.2">𝐞</ci><apply id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3"><times id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.1.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.1"></times><ci id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.2">𝑟</ci><ci id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.3.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.3">𝑜</ci><ci id="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.4.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.2.2.3.4">𝑤</ci></apply></apply><list id="S3.E3Xb.3.2.2.m1.2.2.2.3.cmml" xref="S3.E3Xb.3.2.2.m1.2.2.2.4"><ci id="S3.E3Xb.3.2.2.m1.1.1.1.1.cmml" xref="S3.E3Xb.3.2.2.m1.1.1.1.1">𝑖</ci><ci id="S3.E3Xb.3.2.2.m1.2.2.2.2.cmml" xref="S3.E3Xb.3.2.2.m1.2.2.2.2">𝑗</ci></list></apply><apply id="S3.E3Xb.3.2.2.m1.4.5.3.3.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3"><csymbol cd="ambiguous" id="S3.E3Xb.3.2.2.m1.4.5.3.3.1.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3">subscript</csymbol><apply id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3"><csymbol cd="ambiguous" id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.1.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3">superscript</csymbol><ci id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.2">𝐞</ci><apply id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3"><times id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.1"></times><ci id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.2">𝑐</ci><ci id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.3.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.3">𝑜</ci><ci id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.4.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.4">𝑙</ci><ci id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.5.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.5">𝑢</ci><ci id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.6.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.6">𝑚</ci><ci id="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.7.cmml" xref="S3.E3Xb.3.2.2.m1.4.5.3.3.2.3.7">𝑛</ci></apply></apply><list id="S3.E3Xb.3.2.2.m1.4.4.2.3.cmml" xref="S3.E3Xb.3.2.2.m1.4.4.2.4"><ci id="S3.E3Xb.3.2.2.m1.3.3.1.1.cmml" xref="S3.E3Xb.3.2.2.m1.3.3.1.1">𝑖</ci><ci id="S3.E3Xb.3.2.2.m1.4.4.2.2.cmml" xref="S3.E3Xb.3.2.2.m1.4.4.2.2">𝑗</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xb.3.2.2.m1.4c">\displaystyle=\mathbf{e}^{row}_{i,j}+\mathbf{e}^{column}_{i,j}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.SS2.p1.5" class="ltx_p">where <math id="S3.SS2.p1.2.m1.2" class="ltx_Math" alttext="\mathbf{e}_{i,j}\in\mathbb{R}^{D_{l}}" display="inline"><semantics id="S3.SS2.p1.2.m1.2a"><mrow id="S3.SS2.p1.2.m1.2.3" xref="S3.SS2.p1.2.m1.2.3.cmml"><msub id="S3.SS2.p1.2.m1.2.3.2" xref="S3.SS2.p1.2.m1.2.3.2.cmml"><mi id="S3.SS2.p1.2.m1.2.3.2.2" xref="S3.SS2.p1.2.m1.2.3.2.2.cmml">𝐞</mi><mrow id="S3.SS2.p1.2.m1.2.2.2.4" xref="S3.SS2.p1.2.m1.2.2.2.3.cmml"><mi id="S3.SS2.p1.2.m1.1.1.1.1" xref="S3.SS2.p1.2.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p1.2.m1.2.2.2.4.1" xref="S3.SS2.p1.2.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p1.2.m1.2.2.2.2" xref="S3.SS2.p1.2.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.SS2.p1.2.m1.2.3.1" xref="S3.SS2.p1.2.m1.2.3.1.cmml">∈</mo><msup id="S3.SS2.p1.2.m1.2.3.3" xref="S3.SS2.p1.2.m1.2.3.3.cmml"><mi id="S3.SS2.p1.2.m1.2.3.3.2" xref="S3.SS2.p1.2.m1.2.3.3.2.cmml">ℝ</mi><msub id="S3.SS2.p1.2.m1.2.3.3.3" xref="S3.SS2.p1.2.m1.2.3.3.3.cmml"><mi id="S3.SS2.p1.2.m1.2.3.3.3.2" xref="S3.SS2.p1.2.m1.2.3.3.3.2.cmml">D</mi><mi id="S3.SS2.p1.2.m1.2.3.3.3.3" xref="S3.SS2.p1.2.m1.2.3.3.3.3.cmml">l</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m1.2b"><apply id="S3.SS2.p1.2.m1.2.3.cmml" xref="S3.SS2.p1.2.m1.2.3"><in id="S3.SS2.p1.2.m1.2.3.1.cmml" xref="S3.SS2.p1.2.m1.2.3.1"></in><apply id="S3.SS2.p1.2.m1.2.3.2.cmml" xref="S3.SS2.p1.2.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m1.2.3.2.1.cmml" xref="S3.SS2.p1.2.m1.2.3.2">subscript</csymbol><ci id="S3.SS2.p1.2.m1.2.3.2.2.cmml" xref="S3.SS2.p1.2.m1.2.3.2.2">𝐞</ci><list id="S3.SS2.p1.2.m1.2.2.2.3.cmml" xref="S3.SS2.p1.2.m1.2.2.2.4"><ci id="S3.SS2.p1.2.m1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m1.1.1.1.1">𝑖</ci><ci id="S3.SS2.p1.2.m1.2.2.2.2.cmml" xref="S3.SS2.p1.2.m1.2.2.2.2">𝑗</ci></list></apply><apply id="S3.SS2.p1.2.m1.2.3.3.cmml" xref="S3.SS2.p1.2.m1.2.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m1.2.3.3.1.cmml" xref="S3.SS2.p1.2.m1.2.3.3">superscript</csymbol><ci id="S3.SS2.p1.2.m1.2.3.3.2.cmml" xref="S3.SS2.p1.2.m1.2.3.3.2">ℝ</ci><apply id="S3.SS2.p1.2.m1.2.3.3.3.cmml" xref="S3.SS2.p1.2.m1.2.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m1.2.3.3.3.1.cmml" xref="S3.SS2.p1.2.m1.2.3.3.3">subscript</csymbol><ci id="S3.SS2.p1.2.m1.2.3.3.3.2.cmml" xref="S3.SS2.p1.2.m1.2.3.3.3.2">𝐷</ci><ci id="S3.SS2.p1.2.m1.2.3.3.3.3.cmml" xref="S3.SS2.p1.2.m1.2.3.3.3.3">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m1.2c">\mathbf{e}_{i,j}\in\mathbb{R}^{D_{l}}</annotation></semantics></math> denotes the crop position embedding of the cell <math id="S3.SS2.p1.3.m2.2" class="ltx_Math" alttext="(c_{i},c_{j})" display="inline"><semantics id="S3.SS2.p1.3.m2.2a"><mrow id="S3.SS2.p1.3.m2.2.2.2" xref="S3.SS2.p1.3.m2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.p1.3.m2.2.2.2.3" xref="S3.SS2.p1.3.m2.2.2.3.cmml">(</mo><msub id="S3.SS2.p1.3.m2.1.1.1.1" xref="S3.SS2.p1.3.m2.1.1.1.1.cmml"><mi id="S3.SS2.p1.3.m2.1.1.1.1.2" xref="S3.SS2.p1.3.m2.1.1.1.1.2.cmml">c</mi><mi id="S3.SS2.p1.3.m2.1.1.1.1.3" xref="S3.SS2.p1.3.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p1.3.m2.2.2.2.4" xref="S3.SS2.p1.3.m2.2.2.3.cmml">,</mo><msub id="S3.SS2.p1.3.m2.2.2.2.2" xref="S3.SS2.p1.3.m2.2.2.2.2.cmml"><mi id="S3.SS2.p1.3.m2.2.2.2.2.2" xref="S3.SS2.p1.3.m2.2.2.2.2.2.cmml">c</mi><mi id="S3.SS2.p1.3.m2.2.2.2.2.3" xref="S3.SS2.p1.3.m2.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS2.p1.3.m2.2.2.2.5" xref="S3.SS2.p1.3.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m2.2b"><interval closure="open" id="S3.SS2.p1.3.m2.2.2.3.cmml" xref="S3.SS2.p1.3.m2.2.2.2"><apply id="S3.SS2.p1.3.m2.1.1.1.1.cmml" xref="S3.SS2.p1.3.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m2.1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m2.1.1.1.1.2.cmml" xref="S3.SS2.p1.3.m2.1.1.1.1.2">𝑐</ci><ci id="S3.SS2.p1.3.m2.1.1.1.1.3.cmml" xref="S3.SS2.p1.3.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.p1.3.m2.2.2.2.2.cmml" xref="S3.SS2.p1.3.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m2.2.2.2.2.1.cmml" xref="S3.SS2.p1.3.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.3.m2.2.2.2.2.2.cmml" xref="S3.SS2.p1.3.m2.2.2.2.2.2">𝑐</ci><ci id="S3.SS2.p1.3.m2.2.2.2.2.3.cmml" xref="S3.SS2.p1.3.m2.2.2.2.2.3">𝑗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m2.2c">(c_{i},c_{j})</annotation></semantics></math>. We add the embedding to the visual feature of each cell in the language space via broadcasting along the dimension of learnable queries: <math id="S3.SS2.p1.4.m3.6" class="ltx_Math" alttext="\bar{V}^{l}_{i,j}=V^{l}_{i,j}+\mathbf{e}_{i,j}" display="inline"><semantics id="S3.SS2.p1.4.m3.6a"><mrow id="S3.SS2.p1.4.m3.6.7" xref="S3.SS2.p1.4.m3.6.7.cmml"><msubsup id="S3.SS2.p1.4.m3.6.7.2" xref="S3.SS2.p1.4.m3.6.7.2.cmml"><mover accent="true" id="S3.SS2.p1.4.m3.6.7.2.2.2" xref="S3.SS2.p1.4.m3.6.7.2.2.2.cmml"><mi id="S3.SS2.p1.4.m3.6.7.2.2.2.2" xref="S3.SS2.p1.4.m3.6.7.2.2.2.2.cmml">V</mi><mo id="S3.SS2.p1.4.m3.6.7.2.2.2.1" xref="S3.SS2.p1.4.m3.6.7.2.2.2.1.cmml">¯</mo></mover><mrow id="S3.SS2.p1.4.m3.2.2.2.4" xref="S3.SS2.p1.4.m3.2.2.2.3.cmml"><mi id="S3.SS2.p1.4.m3.1.1.1.1" xref="S3.SS2.p1.4.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p1.4.m3.2.2.2.4.1" xref="S3.SS2.p1.4.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p1.4.m3.2.2.2.2" xref="S3.SS2.p1.4.m3.2.2.2.2.cmml">j</mi></mrow><mi id="S3.SS2.p1.4.m3.6.7.2.2.3" xref="S3.SS2.p1.4.m3.6.7.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS2.p1.4.m3.6.7.1" xref="S3.SS2.p1.4.m3.6.7.1.cmml">=</mo><mrow id="S3.SS2.p1.4.m3.6.7.3" xref="S3.SS2.p1.4.m3.6.7.3.cmml"><msubsup id="S3.SS2.p1.4.m3.6.7.3.2" xref="S3.SS2.p1.4.m3.6.7.3.2.cmml"><mi id="S3.SS2.p1.4.m3.6.7.3.2.2.2" xref="S3.SS2.p1.4.m3.6.7.3.2.2.2.cmml">V</mi><mrow id="S3.SS2.p1.4.m3.4.4.2.4" xref="S3.SS2.p1.4.m3.4.4.2.3.cmml"><mi id="S3.SS2.p1.4.m3.3.3.1.1" xref="S3.SS2.p1.4.m3.3.3.1.1.cmml">i</mi><mo id="S3.SS2.p1.4.m3.4.4.2.4.1" xref="S3.SS2.p1.4.m3.4.4.2.3.cmml">,</mo><mi id="S3.SS2.p1.4.m3.4.4.2.2" xref="S3.SS2.p1.4.m3.4.4.2.2.cmml">j</mi></mrow><mi id="S3.SS2.p1.4.m3.6.7.3.2.2.3" xref="S3.SS2.p1.4.m3.6.7.3.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS2.p1.4.m3.6.7.3.1" xref="S3.SS2.p1.4.m3.6.7.3.1.cmml">+</mo><msub id="S3.SS2.p1.4.m3.6.7.3.3" xref="S3.SS2.p1.4.m3.6.7.3.3.cmml"><mi id="S3.SS2.p1.4.m3.6.7.3.3.2" xref="S3.SS2.p1.4.m3.6.7.3.3.2.cmml">𝐞</mi><mrow id="S3.SS2.p1.4.m3.6.6.2.4" xref="S3.SS2.p1.4.m3.6.6.2.3.cmml"><mi id="S3.SS2.p1.4.m3.5.5.1.1" xref="S3.SS2.p1.4.m3.5.5.1.1.cmml">i</mi><mo id="S3.SS2.p1.4.m3.6.6.2.4.1" xref="S3.SS2.p1.4.m3.6.6.2.3.cmml">,</mo><mi id="S3.SS2.p1.4.m3.6.6.2.2" xref="S3.SS2.p1.4.m3.6.6.2.2.cmml">j</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m3.6b"><apply id="S3.SS2.p1.4.m3.6.7.cmml" xref="S3.SS2.p1.4.m3.6.7"><eq id="S3.SS2.p1.4.m3.6.7.1.cmml" xref="S3.SS2.p1.4.m3.6.7.1"></eq><apply id="S3.SS2.p1.4.m3.6.7.2.cmml" xref="S3.SS2.p1.4.m3.6.7.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m3.6.7.2.1.cmml" xref="S3.SS2.p1.4.m3.6.7.2">subscript</csymbol><apply id="S3.SS2.p1.4.m3.6.7.2.2.cmml" xref="S3.SS2.p1.4.m3.6.7.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m3.6.7.2.2.1.cmml" xref="S3.SS2.p1.4.m3.6.7.2">superscript</csymbol><apply id="S3.SS2.p1.4.m3.6.7.2.2.2.cmml" xref="S3.SS2.p1.4.m3.6.7.2.2.2"><ci id="S3.SS2.p1.4.m3.6.7.2.2.2.1.cmml" xref="S3.SS2.p1.4.m3.6.7.2.2.2.1">¯</ci><ci id="S3.SS2.p1.4.m3.6.7.2.2.2.2.cmml" xref="S3.SS2.p1.4.m3.6.7.2.2.2.2">𝑉</ci></apply><ci id="S3.SS2.p1.4.m3.6.7.2.2.3.cmml" xref="S3.SS2.p1.4.m3.6.7.2.2.3">𝑙</ci></apply><list id="S3.SS2.p1.4.m3.2.2.2.3.cmml" xref="S3.SS2.p1.4.m3.2.2.2.4"><ci id="S3.SS2.p1.4.m3.1.1.1.1.cmml" xref="S3.SS2.p1.4.m3.1.1.1.1">𝑖</ci><ci id="S3.SS2.p1.4.m3.2.2.2.2.cmml" xref="S3.SS2.p1.4.m3.2.2.2.2">𝑗</ci></list></apply><apply id="S3.SS2.p1.4.m3.6.7.3.cmml" xref="S3.SS2.p1.4.m3.6.7.3"><plus id="S3.SS2.p1.4.m3.6.7.3.1.cmml" xref="S3.SS2.p1.4.m3.6.7.3.1"></plus><apply id="S3.SS2.p1.4.m3.6.7.3.2.cmml" xref="S3.SS2.p1.4.m3.6.7.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m3.6.7.3.2.1.cmml" xref="S3.SS2.p1.4.m3.6.7.3.2">subscript</csymbol><apply id="S3.SS2.p1.4.m3.6.7.3.2.2.cmml" xref="S3.SS2.p1.4.m3.6.7.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m3.6.7.3.2.2.1.cmml" xref="S3.SS2.p1.4.m3.6.7.3.2">superscript</csymbol><ci id="S3.SS2.p1.4.m3.6.7.3.2.2.2.cmml" xref="S3.SS2.p1.4.m3.6.7.3.2.2.2">𝑉</ci><ci id="S3.SS2.p1.4.m3.6.7.3.2.2.3.cmml" xref="S3.SS2.p1.4.m3.6.7.3.2.2.3">𝑙</ci></apply><list id="S3.SS2.p1.4.m3.4.4.2.3.cmml" xref="S3.SS2.p1.4.m3.4.4.2.4"><ci id="S3.SS2.p1.4.m3.3.3.1.1.cmml" xref="S3.SS2.p1.4.m3.3.3.1.1">𝑖</ci><ci id="S3.SS2.p1.4.m3.4.4.2.2.cmml" xref="S3.SS2.p1.4.m3.4.4.2.2">𝑗</ci></list></apply><apply id="S3.SS2.p1.4.m3.6.7.3.3.cmml" xref="S3.SS2.p1.4.m3.6.7.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m3.6.7.3.3.1.cmml" xref="S3.SS2.p1.4.m3.6.7.3.3">subscript</csymbol><ci id="S3.SS2.p1.4.m3.6.7.3.3.2.cmml" xref="S3.SS2.p1.4.m3.6.7.3.3.2">𝐞</ci><list id="S3.SS2.p1.4.m3.6.6.2.3.cmml" xref="S3.SS2.p1.4.m3.6.6.2.4"><ci id="S3.SS2.p1.4.m3.5.5.1.1.cmml" xref="S3.SS2.p1.4.m3.5.5.1.1">𝑖</ci><ci id="S3.SS2.p1.4.m3.6.6.2.2.cmml" xref="S3.SS2.p1.4.m3.6.6.2.2">𝑗</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m3.6c">\bar{V}^{l}_{i,j}=V^{l}_{i,j}+\mathbf{e}_{i,j}</annotation></semantics></math>. We then reshape the visual features into <math id="S3.SS2.p1.5.m4.1" class="ltx_Math" alttext="\bar{\mathbf{V}}^{l}\in\mathbb{R}^{(N\cdot N_{q})\times d_{l}}" display="inline"><semantics id="S3.SS2.p1.5.m4.1a"><mrow id="S3.SS2.p1.5.m4.1.2" xref="S3.SS2.p1.5.m4.1.2.cmml"><msup id="S3.SS2.p1.5.m4.1.2.2" xref="S3.SS2.p1.5.m4.1.2.2.cmml"><mover accent="true" id="S3.SS2.p1.5.m4.1.2.2.2" xref="S3.SS2.p1.5.m4.1.2.2.2.cmml"><mi id="S3.SS2.p1.5.m4.1.2.2.2.2" xref="S3.SS2.p1.5.m4.1.2.2.2.2.cmml">𝐕</mi><mo id="S3.SS2.p1.5.m4.1.2.2.2.1" xref="S3.SS2.p1.5.m4.1.2.2.2.1.cmml">¯</mo></mover><mi id="S3.SS2.p1.5.m4.1.2.2.3" xref="S3.SS2.p1.5.m4.1.2.2.3.cmml">l</mi></msup><mo id="S3.SS2.p1.5.m4.1.2.1" xref="S3.SS2.p1.5.m4.1.2.1.cmml">∈</mo><msup id="S3.SS2.p1.5.m4.1.2.3" xref="S3.SS2.p1.5.m4.1.2.3.cmml"><mi id="S3.SS2.p1.5.m4.1.2.3.2" xref="S3.SS2.p1.5.m4.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.5.m4.1.1.1" xref="S3.SS2.p1.5.m4.1.1.1.cmml"><mrow id="S3.SS2.p1.5.m4.1.1.1.1.1" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.5.m4.1.1.1.1.1.2" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.5.m4.1.1.1.1.1.1" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.5.m4.1.1.1.1.1.1.2" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.5.m4.1.1.1.1.1.1.1" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.1.cmml">⋅</mo><msub id="S3.SS2.p1.5.m4.1.1.1.1.1.1.3" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.2" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.2.cmml">N</mi><mi id="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.3" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.3.cmml">q</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.SS2.p1.5.m4.1.1.1.1.1.3" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.SS2.p1.5.m4.1.1.1.2" xref="S3.SS2.p1.5.m4.1.1.1.2.cmml">×</mo><msub id="S3.SS2.p1.5.m4.1.1.1.3" xref="S3.SS2.p1.5.m4.1.1.1.3.cmml"><mi id="S3.SS2.p1.5.m4.1.1.1.3.2" xref="S3.SS2.p1.5.m4.1.1.1.3.2.cmml">d</mi><mi id="S3.SS2.p1.5.m4.1.1.1.3.3" xref="S3.SS2.p1.5.m4.1.1.1.3.3.cmml">l</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m4.1b"><apply id="S3.SS2.p1.5.m4.1.2.cmml" xref="S3.SS2.p1.5.m4.1.2"><in id="S3.SS2.p1.5.m4.1.2.1.cmml" xref="S3.SS2.p1.5.m4.1.2.1"></in><apply id="S3.SS2.p1.5.m4.1.2.2.cmml" xref="S3.SS2.p1.5.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m4.1.2.2.1.cmml" xref="S3.SS2.p1.5.m4.1.2.2">superscript</csymbol><apply id="S3.SS2.p1.5.m4.1.2.2.2.cmml" xref="S3.SS2.p1.5.m4.1.2.2.2"><ci id="S3.SS2.p1.5.m4.1.2.2.2.1.cmml" xref="S3.SS2.p1.5.m4.1.2.2.2.1">¯</ci><ci id="S3.SS2.p1.5.m4.1.2.2.2.2.cmml" xref="S3.SS2.p1.5.m4.1.2.2.2.2">𝐕</ci></apply><ci id="S3.SS2.p1.5.m4.1.2.2.3.cmml" xref="S3.SS2.p1.5.m4.1.2.2.3">𝑙</ci></apply><apply id="S3.SS2.p1.5.m4.1.2.3.cmml" xref="S3.SS2.p1.5.m4.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m4.1.2.3.1.cmml" xref="S3.SS2.p1.5.m4.1.2.3">superscript</csymbol><ci id="S3.SS2.p1.5.m4.1.2.3.2.cmml" xref="S3.SS2.p1.5.m4.1.2.3.2">ℝ</ci><apply id="S3.SS2.p1.5.m4.1.1.1.cmml" xref="S3.SS2.p1.5.m4.1.1.1"><times id="S3.SS2.p1.5.m4.1.1.1.2.cmml" xref="S3.SS2.p1.5.m4.1.1.1.2"></times><apply id="S3.SS2.p1.5.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.5.m4.1.1.1.1.1"><ci id="S3.SS2.p1.5.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.1">⋅</ci><ci id="S3.SS2.p1.5.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.2">𝑁</ci><apply id="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.2">𝑁</ci><ci id="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.5.m4.1.1.1.1.1.1.3.3">𝑞</ci></apply></apply><apply id="S3.SS2.p1.5.m4.1.1.1.3.cmml" xref="S3.SS2.p1.5.m4.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m4.1.1.1.3.1.cmml" xref="S3.SS2.p1.5.m4.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.5.m4.1.1.1.3.2.cmml" xref="S3.SS2.p1.5.m4.1.1.1.3.2">𝑑</ci><ci id="S3.SS2.p1.5.m4.1.1.1.3.3.cmml" xref="S3.SS2.p1.5.m4.1.1.1.3.3">𝑙</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m4.1c">\bar{\mathbf{V}}^{l}\in\mathbb{R}^{(N\cdot N_{q})\times d_{l}}</annotation></semantics></math>. The resulting spatial-aware visual features and word embeddings of the input sentences are concatenated at sequence dimension and sent to the large language model.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In order to enhance the language model’s ability to effectively model multiple images while keeping low training costs, we freeze the origin language model and adopt the low-rank adaptation approach (LoRA)  <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Instruction Tuning</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">For developing a universal visually-situated language understanding model that could process various types of images and perform different comprehension tasks, we conduct low-cost instruction tuning with a Multimodal Large Language Model. Without introducing any large-scale pretraining datasets, we directly ensemble multiple downstream datasets and perform joint training. Different downstream tasks are all reorganized to the unified instruction format <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. Besides, we design auxiliary text reading and key points generation tasks to enhance text recognition and semantic understanding ability.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Tuning Tasks</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Unified downstream task.</span> Downstream tasks of Visuall-situated Language Understanding cover Visual Question Answering, Information Extraction, Natural Language Inference, and Image Captioning. For developing a universal model, we reorganize all tasks into the instruction tuning format <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. Concretely, for the Visual Question Answering task, the question is directly used as the instruction: "Human: {question} AI: {answer}". For the Information Extraction task, each category and value pair is expressed with a prompt as "Human: What is the value for the {category}? AI: {value}". If some categories don’t exist in the image, the value is ‘None’. In the raw annotation of the Natural Language Inference task, ‘1’ means ‘Entailed’ and ‘0’ means ‘Refuted’. We reorganize the NLI task by constructing the instruction "Human: {statement}, Yes or No? AI: {answer}", where ‘Yes’ means ‘Entailed’. For the Image captioning task, we refer to 11 prompts from LLaVa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>)</cite> to instruct the model to briefly describe the image and randomly choose 1 prompt for each caption, such as "Human: Provide a brief description of the given image. AI: {caption}".</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.4" class="ltx_p"><span id="S4.SS1.p2.4.1" class="ltx_text ltx_font_bold">Text Reading task.</span> Text Recognition is a basic ability for OCR-free Visuall-situated Language Understanding. Therefore, we apply an auxiliary Text Reading task to strengthen text recognition ability across different domains. With the text and position information in the image, we organize the texts in the common reading order: from top to down, from left to right. Directly utilizing all texts as targets <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> will result in the model focusing on generating the starting texts and neglecting others to reduce the loss. Instead, we randomly choose a split position <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">p</annotation></semantics></math> from <math id="S4.SS1.p2.2.m2.5" class="ltx_Math" alttext="\{0,\frac{L}{6},\frac{2L}{6},...,\frac{5L}{6}\}" display="inline"><semantics id="S4.SS1.p2.2.m2.5a"><mrow id="S4.SS1.p2.2.m2.5.6.2" xref="S4.SS1.p2.2.m2.5.6.1.cmml"><mo stretchy="false" id="S4.SS1.p2.2.m2.5.6.2.1" xref="S4.SS1.p2.2.m2.5.6.1.cmml">{</mo><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">0</mn><mo id="S4.SS1.p2.2.m2.5.6.2.2" xref="S4.SS1.p2.2.m2.5.6.1.cmml">,</mo><mfrac id="S4.SS1.p2.2.m2.2.2" xref="S4.SS1.p2.2.m2.2.2.cmml"><mi id="S4.SS1.p2.2.m2.2.2.2" xref="S4.SS1.p2.2.m2.2.2.2.cmml">L</mi><mn id="S4.SS1.p2.2.m2.2.2.3" xref="S4.SS1.p2.2.m2.2.2.3.cmml">6</mn></mfrac><mo id="S4.SS1.p2.2.m2.5.6.2.3" xref="S4.SS1.p2.2.m2.5.6.1.cmml">,</mo><mfrac id="S4.SS1.p2.2.m2.3.3" xref="S4.SS1.p2.2.m2.3.3.cmml"><mrow id="S4.SS1.p2.2.m2.3.3.2" xref="S4.SS1.p2.2.m2.3.3.2.cmml"><mn id="S4.SS1.p2.2.m2.3.3.2.2" xref="S4.SS1.p2.2.m2.3.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.3.3.2.1" xref="S4.SS1.p2.2.m2.3.3.2.1.cmml">​</mo><mi id="S4.SS1.p2.2.m2.3.3.2.3" xref="S4.SS1.p2.2.m2.3.3.2.3.cmml">L</mi></mrow><mn id="S4.SS1.p2.2.m2.3.3.3" xref="S4.SS1.p2.2.m2.3.3.3.cmml">6</mn></mfrac><mo id="S4.SS1.p2.2.m2.5.6.2.4" xref="S4.SS1.p2.2.m2.5.6.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p2.2.m2.4.4" xref="S4.SS1.p2.2.m2.4.4.cmml">…</mi><mo id="S4.SS1.p2.2.m2.5.6.2.5" xref="S4.SS1.p2.2.m2.5.6.1.cmml">,</mo><mfrac id="S4.SS1.p2.2.m2.5.5" xref="S4.SS1.p2.2.m2.5.5.cmml"><mrow id="S4.SS1.p2.2.m2.5.5.2" xref="S4.SS1.p2.2.m2.5.5.2.cmml"><mn id="S4.SS1.p2.2.m2.5.5.2.2" xref="S4.SS1.p2.2.m2.5.5.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.5.5.2.1" xref="S4.SS1.p2.2.m2.5.5.2.1.cmml">​</mo><mi id="S4.SS1.p2.2.m2.5.5.2.3" xref="S4.SS1.p2.2.m2.5.5.2.3.cmml">L</mi></mrow><mn id="S4.SS1.p2.2.m2.5.5.3" xref="S4.SS1.p2.2.m2.5.5.3.cmml">6</mn></mfrac><mo stretchy="false" id="S4.SS1.p2.2.m2.5.6.2.6" xref="S4.SS1.p2.2.m2.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.5b"><set id="S4.SS1.p2.2.m2.5.6.1.cmml" xref="S4.SS1.p2.2.m2.5.6.2"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">0</cn><apply id="S4.SS1.p2.2.m2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2"><divide id="S4.SS1.p2.2.m2.2.2.1.cmml" xref="S4.SS1.p2.2.m2.2.2"></divide><ci id="S4.SS1.p2.2.m2.2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2">𝐿</ci><cn type="integer" id="S4.SS1.p2.2.m2.2.2.3.cmml" xref="S4.SS1.p2.2.m2.2.2.3">6</cn></apply><apply id="S4.SS1.p2.2.m2.3.3.cmml" xref="S4.SS1.p2.2.m2.3.3"><divide id="S4.SS1.p2.2.m2.3.3.1.cmml" xref="S4.SS1.p2.2.m2.3.3"></divide><apply id="S4.SS1.p2.2.m2.3.3.2.cmml" xref="S4.SS1.p2.2.m2.3.3.2"><times id="S4.SS1.p2.2.m2.3.3.2.1.cmml" xref="S4.SS1.p2.2.m2.3.3.2.1"></times><cn type="integer" id="S4.SS1.p2.2.m2.3.3.2.2.cmml" xref="S4.SS1.p2.2.m2.3.3.2.2">2</cn><ci id="S4.SS1.p2.2.m2.3.3.2.3.cmml" xref="S4.SS1.p2.2.m2.3.3.2.3">𝐿</ci></apply><cn type="integer" id="S4.SS1.p2.2.m2.3.3.3.cmml" xref="S4.SS1.p2.2.m2.3.3.3">6</cn></apply><ci id="S4.SS1.p2.2.m2.4.4.cmml" xref="S4.SS1.p2.2.m2.4.4">…</ci><apply id="S4.SS1.p2.2.m2.5.5.cmml" xref="S4.SS1.p2.2.m2.5.5"><divide id="S4.SS1.p2.2.m2.5.5.1.cmml" xref="S4.SS1.p2.2.m2.5.5"></divide><apply id="S4.SS1.p2.2.m2.5.5.2.cmml" xref="S4.SS1.p2.2.m2.5.5.2"><times id="S4.SS1.p2.2.m2.5.5.2.1.cmml" xref="S4.SS1.p2.2.m2.5.5.2.1"></times><cn type="integer" id="S4.SS1.p2.2.m2.5.5.2.2.cmml" xref="S4.SS1.p2.2.m2.5.5.2.2">5</cn><ci id="S4.SS1.p2.2.m2.5.5.2.3.cmml" xref="S4.SS1.p2.2.m2.5.5.2.3">𝐿</ci></apply><cn type="integer" id="S4.SS1.p2.2.m2.5.5.3.cmml" xref="S4.SS1.p2.2.m2.5.5.3">6</cn></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.5c">\{0,\frac{L}{6},\frac{2L}{6},...,\frac{5L}{6}\}</annotation></semantics></math>, where <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">L</annotation></semantics></math> is the text sequence length. The left part is used as the input and the right one is the target. <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="p=0" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mi id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">p</mi><mo id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><eq id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1"></eq><ci id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">𝑝</ci><cn type="integer" id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">p=0</annotation></semantics></math> means to generate all texts while other cases ask the model to continue reading following the input texts. Such a design could enforce the model to read different parts of texts with the context. Starting texts always convey key information about the image, such as the chart title. Therefore, we apply a bigger sample rate (0.5) for the ‘0’ position and 0.1 for other positions. To distinguish reading from the beginning and continuing reading, we design two groups of prompts and randomly choose 1 prompt for each sample. For example, an instruction of reading from the beginning can be "Human: Recognize text in the image. AI: {all texts}" and an instruction of continuing reading can be "Human: The words on this picture are {left texts}. Continue reading the text. AI: {right texts}".</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Key Points Generation task.</span> Large Language Models learn strong understanding ability from the tough language modeling task. Therefore, for stronger vision-and-language semantic comprehension ability, we propose to design an auxiliary Key Points Generation task, which requires the model to give some key points about the image. To support this task, we collect QA pairs of each image and convert them to declarative sentences with Vicuna <cite class="ltx_cite ltx_citemacro_cite">Vicuna (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. These declarative sentences are finally regarded as key points about the image. We also build a set of templates to instruct this task, such as "Human: Identify some key points in this picture. AI: {key points}".</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">All templates for Text Reading and Key Points Generation tasks can be found in Appendix <a href="#A4" title="Appendix D Instruction Templates ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Instruction Data Resources</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with ocr-free methods on various types of visually-situated language understanding tasks. ‘TSFT’ means task-spcific fine-tuning on the downstream dataset. ‘<span id="S4.T1.7.1" class="ltx_text ltx_framed ltx_framed_underline">underline</span>’ means achieving 80% SOTA performance.</figcaption>
<table id="S4.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.5.6.1" class="ltx_tr">
<th id="S4.T1.5.6.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" rowspan="2">
<span id="S4.T1.5.6.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.1.1.1" class="ltx_p" style="width:47.7pt;"><span id="S4.T1.5.6.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.5.6.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.2.1.1" class="ltx_p" style="width:17.3pt;"><span id="S4.T1.5.6.1.2.1.1.1" class="ltx_text ltx_font_bold">Train</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S4.T1.5.6.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.3.1.1" class="ltx_p" style="width:8.7pt;"><span id="S4.T1.5.6.1.3.1.1.1" class="ltx_text ltx_font_bold">TS</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.5.6.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.4.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.6.1.4.1.1.1" class="ltx_text ltx_font_bold">Doc</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.5.6.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.5.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.6.1.5.1.1.1" class="ltx_text ltx_font_bold">Info</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.5.6.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.6.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.6.1.6.1.1.1" class="ltx_text ltx_font_bold">Deep</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" rowspan="2">
<span id="S4.T1.5.6.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.7.1.1" class="ltx_p" style="width:17.3pt;"><span id="S4.T1.5.6.1.7.1.1.1" class="ltx_text ltx_font_bold">KLC</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" rowspan="2">
<span id="S4.T1.5.6.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.8.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.6.1.8.1.1.1" class="ltx_text ltx_font_bold">WTQ</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" rowspan="2">
<span id="S4.T1.5.6.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.9.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T1.5.6.1.9.1.1.1" class="ltx_text ltx_font_bold">TabFact</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.10" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" rowspan="2">
<span id="S4.T1.5.6.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.10.1.1" class="ltx_p" style="width:30.4pt;"><span id="S4.T1.5.6.1.10.1.1.1" class="ltx_text ltx_font_bold">ChartQA</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.11" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" rowspan="2">
<span id="S4.T1.5.6.1.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.11.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T1.5.6.1.11.1.1.1" class="ltx_text ltx_font_bold">TextVQA</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.12" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" rowspan="2">
<span id="S4.T1.5.6.1.12.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.12.1.1" class="ltx_p" style="width:30.4pt;"><span id="S4.T1.5.6.1.12.1.1.1" class="ltx_text ltx_font_bold">TextCaps</span></span>
</span>
</th>
<th id="S4.T1.5.6.1.13" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T1.5.6.1.13.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.6.1.13.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T1.5.6.1.13.1.1.1" class="ltx_text ltx_font_bold">Visual</span></span>
</span>
</th>
</tr>
<tr id="S4.T1.5.7.2" class="ltx_tr">
<th id="S4.T1.5.7.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S4.T1.5.7.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.7.2.1.1.1" class="ltx_p" style="width:17.3pt;"><span id="S4.T1.5.7.2.1.1.1.1" class="ltx_text ltx_font_bold">Param</span></span>
</span>
</th>
<th id="S4.T1.5.7.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r">
<span id="S4.T1.5.7.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.7.2.2.1.1" class="ltx_p" style="width:8.7pt;"><span id="S4.T1.5.7.2.2.1.1.1" class="ltx_text ltx_font_bold">FT</span></span>
</span>
</th>
<th id="S4.T1.5.7.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S4.T1.5.7.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.7.2.3.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.7.2.3.1.1.1" class="ltx_text ltx_font_bold">VQA</span></span>
</span>
</th>
<th id="S4.T1.5.7.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S4.T1.5.7.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.7.2.4.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.7.2.4.1.1.1" class="ltx_text ltx_font_bold">VQA</span></span>
</span>
</th>
<th id="S4.T1.5.7.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S4.T1.5.7.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.7.2.5.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.7.2.5.1.1.1" class="ltx_text ltx_font_bold">Form</span></span>
</span>
</th>
<th id="S4.T1.5.7.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S4.T1.5.7.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.7.2.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T1.5.7.2.6.1.1.1" class="ltx_text ltx_font_bold">MRC</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.5.8.1" class="ltx_tr">
<td id="S4.T1.5.8.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.5.8.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.1.1.1" class="ltx_p" style="width:47.7pt;">Dessurt</span>
</span>
</td>
<td id="S4.T1.5.8.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.5.8.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.2.1.1" class="ltx_p" style="width:17.3pt;">127M</span>
</span>
</td>
<td id="S4.T1.5.8.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.5.8.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S4.T1.5.8.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.5.8.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.4.1.1" class="ltx_p" style="width:13.0pt;">63.2</span>
</span>
</td>
<td id="S4.T1.5.8.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.5.8.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.5.1.1" class="ltx_p" style="width:13.0pt;">-</span>
</span>
</td>
<td id="S4.T1.5.8.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.5.8.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.6.1.1" class="ltx_p" style="width:13.0pt;">-</span>
</span>
</td>
<td id="S4.T1.5.8.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.5.8.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.7.1.1" class="ltx_p" style="width:17.3pt;">-</span>
</span>
</td>
<td id="S4.T1.5.8.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.5.8.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.8.1.1" class="ltx_p" style="width:13.0pt;">-</span>
</span>
</td>
<td id="S4.T1.5.8.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.5.8.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.9.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
<td id="S4.T1.5.8.1.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.5.8.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.10.1.1" class="ltx_p" style="width:30.4pt;">-</span>
</span>
</td>
<td id="S4.T1.5.8.1.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.5.8.1.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.11.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
<td id="S4.T1.5.8.1.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.5.8.1.12.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.12.1.1" class="ltx_p" style="width:30.4pt;">-</span>
</span>
</td>
<td id="S4.T1.5.8.1.13" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.5.8.1.13.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.8.1.13.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
</tr>
<tr id="S4.T1.5.9.2" class="ltx_tr">
<td id="S4.T1.5.9.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.5.9.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.1.1.1" class="ltx_p" style="width:47.7pt;">Donut</span>
</span>
</td>
<td id="S4.T1.5.9.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.5.9.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.2.1.1" class="ltx_p" style="width:17.3pt;">176M</span>
</span>
</td>
<td id="S4.T1.5.9.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.5.9.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S4.T1.5.9.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.5.9.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.4.1.1" class="ltx_p" style="width:13.0pt;">67.5</span>
</span>
</td>
<td id="S4.T1.5.9.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.5.9.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.5.1.1" class="ltx_p" style="width:13.0pt;">11.6</span>
</span>
</td>
<td id="S4.T1.5.9.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.5.9.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.6.1.1" class="ltx_p" style="width:13.0pt;">61.6</span>
</span>
</td>
<td id="S4.T1.5.9.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.5.9.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.7.1.1" class="ltx_p" style="width:17.3pt;">30.0</span>
</span>
</td>
<td id="S4.T1.5.9.2.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.5.9.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.8.1.1" class="ltx_p" style="width:13.0pt;">18.8</span>
</span>
</td>
<td id="S4.T1.5.9.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.5.9.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.9.1.1" class="ltx_p" style="width:26.0pt;">54.6</span>
</span>
</td>
<td id="S4.T1.5.9.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.5.9.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.10.1.1" class="ltx_p" style="width:30.4pt;">41.8</span>
</span>
</td>
<td id="S4.T1.5.9.2.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.5.9.2.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.11.1.1" class="ltx_p" style="width:26.0pt;">43.5</span>
</span>
</td>
<td id="S4.T1.5.9.2.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.5.9.2.12.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.12.1.1" class="ltx_p" style="width:30.4pt;">74.4</span>
</span>
</td>
<td id="S4.T1.5.9.2.13" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.5.9.2.13.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.9.2.13.1.1" class="ltx_p" style="width:26.0pt;">93.91</span>
</span>
</td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1" class="ltx_p" style="width:47.7pt;">Pix2Struct<sub id="S4.T1.1.1.1.1.1.1" class="ltx_sub"><span id="S4.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">base</span></sub></span>
</span>
</td>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.3.1.1" class="ltx_p" style="width:17.3pt;">282M</span>
</span>
</td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.2.1.1" class="ltx_p" style="width:8.7pt;"><math id="S4.T1.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T1.2.2.2.1.1.m1.1a"><mi mathvariant="normal" id="S4.T1.2.2.2.1.1.m1.1.1" xref="S4.T1.2.2.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.1.m1.1b"><ci id="S4.T1.2.2.2.1.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.1.m1.1c">\checkmark</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.4.1.1" class="ltx_p" style="width:13.0pt;">72.1</span>
</span>
</td>
<td id="S4.T1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.5.1.1" class="ltx_p" style="width:13.0pt;">38.2</span>
</span>
</td>
<td id="S4.T1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.6.1.1" class="ltx_p" style="width:13.0pt;">-</span>
</span>
</td>
<td id="S4.T1.2.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.7.1.1" class="ltx_p" style="width:17.3pt;">-</span>
</span>
</td>
<td id="S4.T1.2.2.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.8.1.1" class="ltx_p" style="width:13.0pt;">-</span>
</span>
</td>
<td id="S4.T1.2.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.2.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.9.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
<td id="S4.T1.2.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.2.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.10.1.1" class="ltx_p" style="width:30.4pt;">56.0</span>
</span>
</td>
<td id="S4.T1.2.2.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.2.2.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.11.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
<td id="S4.T1.2.2.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.2.2.12.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.12.1.1" class="ltx_p" style="width:30.4pt;">88.0</span>
</span>
</td>
<td id="S4.T1.2.2.13" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.2.2.13.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.13.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.1.1.1" class="ltx_p" style="width:47.7pt;">Pix2Struct<sub id="S4.T1.3.3.1.1.1.1" class="ltx_sub"><span id="S4.T1.3.3.1.1.1.1.1" class="ltx_text ltx_font_italic">large</span></sub></span>
</span>
</td>
<td id="S4.T1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.3.1.1" class="ltx_p" style="width:17.3pt;">1.3B</span>
</span>
</td>
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.2.1.1" class="ltx_p" style="width:8.7pt;"><math id="S4.T1.4.4.2.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S4.T1.4.4.2.1.1.m1.1a"><mi mathvariant="normal" id="S4.T1.4.4.2.1.1.m1.1.1" xref="S4.T1.4.4.2.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.2.1.1.m1.1b"><ci id="S4.T1.4.4.2.1.1.m1.1.1.cmml" xref="S4.T1.4.4.2.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.2.1.1.m1.1c">\checkmark</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.4.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.4.4.4.1.1.1" class="ltx_text ltx_font_bold">76.6</span></span>
</span>
</td>
<td id="S4.T1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.5.1.1" class="ltx_p" style="width:13.0pt;">40.0</span>
</span>
</td>
<td id="S4.T1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.6.1.1" class="ltx_p" style="width:13.0pt;">-</span>
</span>
</td>
<td id="S4.T1.4.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.7.1.1" class="ltx_p" style="width:17.3pt;">-</span>
</span>
</td>
<td id="S4.T1.4.4.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.8.1.1" class="ltx_p" style="width:13.0pt;">-</span>
</span>
</td>
<td id="S4.T1.4.4.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.4.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.9.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
<td id="S4.T1.4.4.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.4.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.10.1.1" class="ltx_p" style="width:30.4pt;">58.6</span>
</span>
</td>
<td id="S4.T1.4.4.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.4.4.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.11.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
<td id="S4.T1.4.4.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T1.4.4.12.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.12.1.1" class="ltx_p" style="width:30.4pt;">95.5</span>
</span>
</td>
<td id="S4.T1.4.4.13" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.4.4.13.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.13.1.1" class="ltx_p" style="width:26.0pt;">-</span>
</span>
</td>
</tr>
<tr id="S4.T1.5.5" class="ltx_tr">
<td id="S4.T1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.2.1.1" class="ltx_p" style="width:47.7pt;">UReader</span>
</span>
</td>
<td id="S4.T1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.3.1.1" class="ltx_p" style="width:17.3pt;">86M</span>
</span>
</td>
<td id="S4.T1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S4.T1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.1.1.1" class="ltx_p" style="width:8.7pt;"><math id="S4.T1.5.5.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T1.5.5.1.1.1.m1.1a"><mo id="S4.T1.5.5.1.1.1.m1.1.1" xref="S4.T1.5.5.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.1.1.m1.1b"><times id="S4.T1.5.5.1.1.1.m1.1.1.cmml" xref="S4.T1.5.5.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.1.1.m1.1c">\times</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.4.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.5.4.1.1.1" class="ltx_text ltx_framed ltx_framed_underline">65.4</span></span>
</span>
</td>
<td id="S4.T1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.5.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.5.5.1.1.1" class="ltx_text ltx_font_bold">42.2</span></span>
</span>
</td>
<td id="S4.T1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.6.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.5.6.1.1.1" class="ltx_text ltx_framed ltx_framed_underline">49.5</span></span>
</span>
</td>
<td id="S4.T1.5.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S4.T1.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.7.1.1" class="ltx_p" style="width:17.3pt;"><span id="S4.T1.5.5.7.1.1.1" class="ltx_text ltx_font_bold">32.8</span></span>
</span>
</td>
<td id="S4.T1.5.5.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.5.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.8.1.1" class="ltx_p" style="width:13.0pt;"><span id="S4.T1.5.5.8.1.1.1" class="ltx_text ltx_font_bold">29.4</span></span>
</span>
</td>
<td id="S4.T1.5.5.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S4.T1.5.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.9.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T1.5.5.9.1.1.1" class="ltx_text ltx_font_bold">67.6</span></span>
</span>
</td>
<td id="S4.T1.5.5.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S4.T1.5.5.10.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.10.1.1" class="ltx_p" style="width:30.4pt;"><span id="S4.T1.5.5.10.1.1.1" class="ltx_text ltx_font_bold">59.3</span></span>
</span>
</td>
<td id="S4.T1.5.5.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.5.5.11.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.11.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T1.5.5.11.1.1.1" class="ltx_text ltx_font_bold">57.6</span></span>
</span>
</td>
<td id="S4.T1.5.5.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S4.T1.5.5.12.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.12.1.1" class="ltx_p" style="width:30.4pt;"><span id="S4.T1.5.5.12.1.1.1" class="ltx_text ltx_font_bold">118.4</span></span>
</span>
</td>
<td id="S4.T1.5.5.13" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T1.5.5.13.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.5.5.13.1.1" class="ltx_p" style="width:26.0pt;"><span id="S4.T1.5.5.13.1.1.1" class="ltx_text ltx_font_bold">221.7</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.2" class="ltx_p"><span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_bold">Document.</span> DocVQA <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> comprises 50k question and answer(QA) paris on 12k document images from UCSF Industry Documents Library. InfographicsVQA (InfoVQA) <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite> collects 5k diverse infographics from the internet and annotates 30k QA pairs. DeepForm<sup id="S4.SS2.p1.2.2" class="ltx_sup">∗</sup><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Superscript <sup id="footnote1.1" class="ltx_sup">∗</sup> means the reformulated or modified version in DUE-benchmark <cite class="ltx_cite ltx_citemacro_cite">Borchmann et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Svetlichnaya (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> and Kleister Charity (KLC) <cite class="ltx_cite ltx_citemacro_cite">Stanislawek et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> are two Information Extraction datasets. DeepForm<sup id="S4.SS2.p1.2.3" class="ltx_sup">∗</sup> contains 1.1k documents related to election spending. 2.7k documents of KLC come from published reports of charity organizations.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.2" class="ltx_p"><span id="S4.SS2.p2.2.1" class="ltx_text ltx_font_bold">Table.</span> WikiTableQuestions (WTQ<sup id="S4.SS2.p2.2.2" class="ltx_sup">∗</sup>) <cite class="ltx_cite ltx_citemacro_cite">Pasupat and Liang (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> comprises 2.1k table images from Wikipedia and is annotated with 23k question and answer pairs demanding comparison and arithmetic operations. TabFact<sup id="S4.SS2.p2.2.3" class="ltx_sup">∗</sup> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> is a Natural Language Inference dataset, which contains 112k ‘entailed’ or ‘refuted’ statements about 16k Wikipedia tables.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Chart.</span> ChartQA <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite> collects various topics and types of charts from four sources: Statista (statista.com), The Pew
research (pewresearch.org), OWID (ourworldindata.org) and OECD (oecd.org). It totally contains 21k chart images and 32k QA pairs.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Natural Images.</span> TextVQA <cite class="ltx_cite ltx_citemacro_cite">Singh et al. (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite> filters 28k natural images with texts from Open Images V3 <cite class="ltx_cite ltx_citemacro_cite">Krasin et al. (<a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite> and annotates 45k QA pairs. To support image captioning with reading comprehension, TextCaps <cite class="ltx_cite ltx_citemacro_cite">Sidorov et al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite> further collects 145k captions based on TextVQA.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">WebPage Screenshot.</span> VisualMRC <cite class="ltx_cite ltx_citemacro_cite">Tanaka et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> collects 5k full screenshots of webpages from 35 websites. There are 30k annotated QA pairs where answers are expressed in fluent sentences (avg. 9.53 words) and much longer than the ones of QA datasets mentioned above.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Implementation Details</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.7" class="ltx_p">We conduct experiments on a recently proposed MLLM named mPLUG-Owl <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite> without modifying its hyperparameters. The number of learnable queries of visual abstractor is <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="65" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">65</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><cn type="integer" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">65</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">65</annotation></semantics></math>. The dimension of hidden states <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="d_{v}" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><msub id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">d</mi><mi id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝑑</ci><ci id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">d_{v}</annotation></semantics></math> and <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="d_{l}" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><msub id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">d</mi><mi id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">𝑑</ci><ci id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">d_{l}</annotation></semantics></math> are 1024. For the shape-adaptive cropping module, we set the maximum number of cells <math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><msub id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mi id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml">N</mi><mi id="S5.SS1.p1.4.m4.1.1.3" xref="S5.SS1.p1.4.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2">𝑁</ci><ci id="S5.SS1.p1.4.m4.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">N_{c}</annotation></semantics></math> to 20 by default. During instruction tuning, the maximum sequence length is limited to 2048, and <math id="S5.SS1.p1.5.m5.2" class="ltx_Math" alttext="H_{v},W_{v}" display="inline"><semantics id="S5.SS1.p1.5.m5.2a"><mrow id="S5.SS1.p1.5.m5.2.2.2" xref="S5.SS1.p1.5.m5.2.2.3.cmml"><msub id="S5.SS1.p1.5.m5.1.1.1.1" xref="S5.SS1.p1.5.m5.1.1.1.1.cmml"><mi id="S5.SS1.p1.5.m5.1.1.1.1.2" xref="S5.SS1.p1.5.m5.1.1.1.1.2.cmml">H</mi><mi id="S5.SS1.p1.5.m5.1.1.1.1.3" xref="S5.SS1.p1.5.m5.1.1.1.1.3.cmml">v</mi></msub><mo id="S5.SS1.p1.5.m5.2.2.2.3" xref="S5.SS1.p1.5.m5.2.2.3.cmml">,</mo><msub id="S5.SS1.p1.5.m5.2.2.2.2" xref="S5.SS1.p1.5.m5.2.2.2.2.cmml"><mi id="S5.SS1.p1.5.m5.2.2.2.2.2" xref="S5.SS1.p1.5.m5.2.2.2.2.2.cmml">W</mi><mi id="S5.SS1.p1.5.m5.2.2.2.2.3" xref="S5.SS1.p1.5.m5.2.2.2.2.3.cmml">v</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.2b"><list id="S5.SS1.p1.5.m5.2.2.3.cmml" xref="S5.SS1.p1.5.m5.2.2.2"><apply id="S5.SS1.p1.5.m5.1.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1.1.1">subscript</csymbol><ci id="S5.SS1.p1.5.m5.1.1.1.1.2.cmml" xref="S5.SS1.p1.5.m5.1.1.1.1.2">𝐻</ci><ci id="S5.SS1.p1.5.m5.1.1.1.1.3.cmml" xref="S5.SS1.p1.5.m5.1.1.1.1.3">𝑣</ci></apply><apply id="S5.SS1.p1.5.m5.2.2.2.2.cmml" xref="S5.SS1.p1.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.2.2.2.2.1.cmml" xref="S5.SS1.p1.5.m5.2.2.2.2">subscript</csymbol><ci id="S5.SS1.p1.5.m5.2.2.2.2.2.cmml" xref="S5.SS1.p1.5.m5.2.2.2.2.2">𝑊</ci><ci id="S5.SS1.p1.5.m5.2.2.2.2.3.cmml" xref="S5.SS1.p1.5.m5.2.2.2.2.3">𝑣</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.2c">H_{v},W_{v}</annotation></semantics></math> are set to 224 to match the pretrained resolution of the visual encoder. For LoRA, we set the rank <math id="S5.SS1.p1.6.m6.1" class="ltx_Math" alttext="r=8" display="inline"><semantics id="S5.SS1.p1.6.m6.1a"><mrow id="S5.SS1.p1.6.m6.1.1" xref="S5.SS1.p1.6.m6.1.1.cmml"><mi id="S5.SS1.p1.6.m6.1.1.2" xref="S5.SS1.p1.6.m6.1.1.2.cmml">r</mi><mo id="S5.SS1.p1.6.m6.1.1.1" xref="S5.SS1.p1.6.m6.1.1.1.cmml">=</mo><mn id="S5.SS1.p1.6.m6.1.1.3" xref="S5.SS1.p1.6.m6.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.1b"><apply id="S5.SS1.p1.6.m6.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1"><eq id="S5.SS1.p1.6.m6.1.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1.1"></eq><ci id="S5.SS1.p1.6.m6.1.1.2.cmml" xref="S5.SS1.p1.6.m6.1.1.2">𝑟</ci><cn type="integer" id="S5.SS1.p1.6.m6.1.1.3.cmml" xref="S5.SS1.p1.6.m6.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.1c">r=8</annotation></semantics></math>. The learning rate schedule uses a linear warmup of 36 steps to <math id="S5.SS1.p1.7.m7.1" class="ltx_Math" alttext="1e^{-4}" display="inline"><semantics id="S5.SS1.p1.7.m7.1a"><mrow id="S5.SS1.p1.7.m7.1.1" xref="S5.SS1.p1.7.m7.1.1.cmml"><mn id="S5.SS1.p1.7.m7.1.1.2" xref="S5.SS1.p1.7.m7.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p1.7.m7.1.1.1" xref="S5.SS1.p1.7.m7.1.1.1.cmml">​</mo><msup id="S5.SS1.p1.7.m7.1.1.3" xref="S5.SS1.p1.7.m7.1.1.3.cmml"><mi id="S5.SS1.p1.7.m7.1.1.3.2" xref="S5.SS1.p1.7.m7.1.1.3.2.cmml">e</mi><mrow id="S5.SS1.p1.7.m7.1.1.3.3" xref="S5.SS1.p1.7.m7.1.1.3.3.cmml"><mo id="S5.SS1.p1.7.m7.1.1.3.3a" xref="S5.SS1.p1.7.m7.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p1.7.m7.1.1.3.3.2" xref="S5.SS1.p1.7.m7.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.7.m7.1b"><apply id="S5.SS1.p1.7.m7.1.1.cmml" xref="S5.SS1.p1.7.m7.1.1"><times id="S5.SS1.p1.7.m7.1.1.1.cmml" xref="S5.SS1.p1.7.m7.1.1.1"></times><cn type="integer" id="S5.SS1.p1.7.m7.1.1.2.cmml" xref="S5.SS1.p1.7.m7.1.1.2">1</cn><apply id="S5.SS1.p1.7.m7.1.1.3.cmml" xref="S5.SS1.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.7.m7.1.1.3.1.cmml" xref="S5.SS1.p1.7.m7.1.1.3">superscript</csymbol><ci id="S5.SS1.p1.7.m7.1.1.3.2.cmml" xref="S5.SS1.p1.7.m7.1.1.3.2">𝑒</ci><apply id="S5.SS1.p1.7.m7.1.1.3.3.cmml" xref="S5.SS1.p1.7.m7.1.1.3.3"><minus id="S5.SS1.p1.7.m7.1.1.3.3.1.cmml" xref="S5.SS1.p1.7.m7.1.1.3.3"></minus><cn type="integer" id="S5.SS1.p1.7.m7.1.1.3.3.2.cmml" xref="S5.SS1.p1.7.m7.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.7.m7.1c">1e^{-4}</annotation></semantics></math>, followed by cosine decay to 0. The batch size is set to 256. For better convergence of each dataset, DocVQA is up-sampled 3 times, InfoVQA, WTQ, DeepForm, and KLC are up-sampled 2 times. The total number of training samples including Text Reading and Key Points Generation is 514,764.
The instruction tuning process takes 16 A100 days for 20k training steps (10 epochs).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We use official training splits as tuning data and evaluate models on test splits. Following previous works <cite class="ltx_cite ltx_citemacro_cite">Borchmann et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>); Lee et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>, DocVQA and InfoVQA are evaluated by ANLS <cite class="ltx_cite ltx_citemacro_cite">Biten et al. (<a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>, DeepForm and KLC are evaluated by F1 score. WTQ, TabFact and TextVQA are evaluated by accuracy. ChartQA is evaluated with the relaxed accuracy <cite class="ltx_cite ltx_citemacro_cite">Methani et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>. TextCaps and VisualMRC are measured by CIDEr <cite class="ltx_cite ltx_citemacro_cite">Vedantam et al. (<a href="#bib.bib36" title="" class="ltx_ref">2015</a>)</cite>. Evaluation of TextVQA and TextCaps are performed with the official challenge website.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation study about auxiliary training tasks, trainable model architectures, cross-domain joint training and shape-adaptive cropping. ‘KPG’ and ‘TR’ refer to Key Points Generation and Text Reading tasks, respectively. ‘Abs’ refers to the visual abstractor. ‘Doc Data’ means using 4 document datasets as training data or not. ‘Global’ means using a resized global image as input. ‘Crops’ refers to <math id="S5.T2.2.m1.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="S5.T2.2.m1.1b"><msub id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml"><mi id="S5.T2.2.m1.1.1.2" xref="S5.T2.2.m1.1.1.2.cmml">N</mi><mi id="S5.T2.2.m1.1.1.3" xref="S5.T2.2.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><apply id="S5.T2.2.m1.1.1.cmml" xref="S5.T2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.2.m1.1.1.1.cmml" xref="S5.T2.2.m1.1.1">subscript</csymbol><ci id="S5.T2.2.m1.1.1.2.cmml" xref="S5.T2.2.m1.1.1.2">𝑁</ci><ci id="S5.T2.2.m1.1.1.3.cmml" xref="S5.T2.2.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">N_{c}</annotation></semantics></math>, the maximum number of local images after cropping. ‘CropPos’ refers to the crop position embedding. </figcaption>
<table id="S5.T2.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.3.1.1" class="ltx_tr">
<td id="S5.T2.3.1.1.1" class="ltx_td ltx_align_top ltx_border_r ltx_border_tt"></td>
<td id="S5.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_tt" colspan="2"><span id="S5.T2.3.1.1.2.1" class="ltx_text ltx_font_bold">Tasks</span></td>
<td id="S5.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_tt" colspan="2"><span id="S5.T2.3.1.1.3.1" class="ltx_text ltx_font_bold">Trainable</span></td>
<td id="S5.T2.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S5.T2.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.1.1.4.1.1" class="ltx_p" style="width:21.7pt;"><span id="S5.T2.3.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Doc</span></span>
</span>
</td>
<td id="S5.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_tt" colspan="3"><span id="S5.T2.3.1.1.5.1" class="ltx_text ltx_font_bold">Shape-adaptive Cropping</span></td>
<td id="S5.T2.3.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="2">
<span id="S5.T2.3.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.1.1.6.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.1.1.6.1.1.1" class="ltx_text ltx_font_bold">DocVQA</span></span>
</span>
</td>
<td id="S5.T2.3.1.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="2">
<span id="S5.T2.3.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.1.1.7.1.1" class="ltx_p" style="width:17.3pt;"><span id="S5.T2.3.1.1.7.1.1.1" class="ltx_text ltx_font_bold">WTQ</span></span>
</span>
</td>
<td id="S5.T2.3.1.1.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="2">
<span id="S5.T2.3.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.1.1.8.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.1.1.8.1.1.1" class="ltx_text ltx_font_bold">ChartQA</span></span>
</span>
</td>
<td id="S5.T2.3.1.1.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="2">
<span id="S5.T2.3.1.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.1.1.9.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.1.1.9.1.1.1" class="ltx_text ltx_font_bold">TextVQA</span></span>
</span>
</td>
<td id="S5.T2.3.1.1.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S5.T2.3.1.1.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.1.1.10.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.1.1.10.1.1.1" class="ltx_text ltx_font_bold">Visual</span></span>
</span>
</td>
</tr>
<tr id="S5.T2.3.2.2" class="ltx_tr">
<td id="S5.T2.3.2.2.1" class="ltx_td ltx_align_top ltx_border_r"></td>
<td id="S5.T2.3.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.2.1.1" class="ltx_p" style="width:8.7pt;">KPG</span>
</span>
</td>
<td id="S5.T2.3.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.3.1.1" class="ltx_p" style="width:8.7pt;">TR</span>
</span>
</td>
<td id="S5.T2.3.2.2.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.4.1.1" class="ltx_p" style="width:17.3pt;">Abs</span>
</span>
</td>
<td id="S5.T2.3.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.5.1.1" class="ltx_p" style="width:17.3pt;">LoRA</span>
</span>
</td>
<td id="S5.T2.3.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.6.1.1" class="ltx_p" style="width:21.7pt;"><span id="S5.T2.3.2.2.6.1.1.1" class="ltx_text ltx_font_bold">Data</span></span>
</span>
</td>
<td id="S5.T2.3.2.2.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.7.1.1" class="ltx_p" style="width:17.3pt;">Global</span>
</span>
</td>
<td id="S5.T2.3.2.2.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.2.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.8.1.1" class="ltx_p" style="width:26.0pt;">CropPos</span>
</span>
</td>
<td id="S5.T2.3.2.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.2.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.9.1.1" class="ltx_p" style="width:30.4pt;">Crops</span>
</span>
</td>
<td id="S5.T2.3.2.2.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.2.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.2.2.10.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.2.2.10.1.1.1" class="ltx_text ltx_font_bold">MRC</span></span>
</span>
</td>
</tr>
<tr id="S5.T2.3.3.3" class="ltx_tr">
<td id="S5.T2.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.1.1.1" class="ltx_p" style="width:8.7pt;">r1</span>
</span>
</td>
<td id="S5.T2.3.3.3.2" class="ltx_td ltx_align_top ltx_border_t"></td>
<td id="S5.T2.3.3.3.3" class="ltx_td ltx_align_top ltx_border_r ltx_border_t"></td>
<td id="S5.T2.3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.3.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.7.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.3.3.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.8.1.1" class="ltx_p" style="width:26.0pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.3.3.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.3.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.9.1.1" class="ltx_p" style="width:30.4pt;">20</span>
</span>
</td>
<td id="S5.T2.3.3.3.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.10.1.1" class="ltx_p" style="width:26.0pt;">56.7</span>
</span>
</td>
<td id="S5.T2.3.3.3.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.11.1.1" class="ltx_p" style="width:17.3pt;">22.9</span>
</span>
</td>
<td id="S5.T2.3.3.3.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.12.1.1" class="ltx_p" style="width:26.0pt;">56.7</span>
</span>
</td>
<td id="S5.T2.3.3.3.13" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.13.1.1" class="ltx_p" style="width:26.0pt;">54.3</span>
</span>
</td>
<td id="S5.T2.3.3.3.14" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.14.1.1" class="ltx_p" style="width:26.0pt;">205.0</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.4.4" class="ltx_tr">
<td id="S5.T2.3.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.1.1.1" class="ltx_p" style="width:8.7pt;">r2</span>
</span>
</td>
<td id="S5.T2.3.4.4.2" class="ltx_td ltx_align_top"></td>
<td id="S5.T2.3.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.4.4.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.7.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.4.4.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.4.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.8.1.1" class="ltx_p" style="width:26.0pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.4.4.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.4.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.9.1.1" class="ltx_p" style="width:30.4pt;">20</span>
</span>
</td>
<td id="S5.T2.3.4.4.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.4.4.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.10.1.1" class="ltx_p" style="width:26.0pt;">64.3</span>
</span>
</td>
<td id="S5.T2.3.4.4.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.4.4.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.11.1.1" class="ltx_p" style="width:17.3pt;">28.1</span>
</span>
</td>
<td id="S5.T2.3.4.4.12" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.4.4.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.12.1.1" class="ltx_p" style="width:26.0pt;">58.6</span>
</span>
</td>
<td id="S5.T2.3.4.4.13" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.4.4.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.13.1.1" class="ltx_p" style="width:26.0pt;">56.0</span>
</span>
</td>
<td id="S5.T2.3.4.4.14" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.4.4.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.4.4.14.1.1" class="ltx_p" style="width:26.0pt;">213.5</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.5.5" class="ltx_tr">
<td id="S5.T2.3.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.1.1.1" class="ltx_p" style="width:8.7pt;">r3</span>
</span>
</td>
<td id="S5.T2.3.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.2.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.4.1.1" class="ltx_p" style="width:17.3pt;"></span>
</span>
</td>
<td id="S5.T2.3.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.5.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.7.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.5.5.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.8.1.1" class="ltx_p" style="width:26.0pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.5.5.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.5.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.9.1.1" class="ltx_p" style="width:30.4pt;">20</span>
</span>
</td>
<td id="S5.T2.3.5.5.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.10.1.1" class="ltx_p" style="width:26.0pt;">52.4</span>
</span>
</td>
<td id="S5.T2.3.5.5.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.11.1.1" class="ltx_p" style="width:17.3pt;">20.5</span>
</span>
</td>
<td id="S5.T2.3.5.5.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.12.1.1" class="ltx_p" style="width:26.0pt;">43.5</span>
</span>
</td>
<td id="S5.T2.3.5.5.13" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.13.1.1" class="ltx_p" style="width:26.0pt;">54.9</span>
</span>
</td>
<td id="S5.T2.3.5.5.14" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.5.5.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.5.5.14.1.1" class="ltx_p" style="width:26.0pt;">194.9</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.6.6" class="ltx_tr">
<td id="S5.T2.3.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.1.1.1" class="ltx_p" style="width:8.7pt;">r4</span>
</span>
</td>
<td id="S5.T2.3.6.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.2.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.6.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.5.1.1" class="ltx_p" style="width:17.3pt;"></span>
</span>
</td>
<td id="S5.T2.3.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.6.6.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.7.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.6.6.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.8.1.1" class="ltx_p" style="width:26.0pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.6.6.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.6.6.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.9.1.1" class="ltx_p" style="width:30.4pt;">20</span>
</span>
</td>
<td id="S5.T2.3.6.6.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.10.1.1" class="ltx_p" style="width:26.0pt;">59.5</span>
</span>
</td>
<td id="S5.T2.3.6.6.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.11.1.1" class="ltx_p" style="width:17.3pt;">23.5</span>
</span>
</td>
<td id="S5.T2.3.6.6.12" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.12.1.1" class="ltx_p" style="width:26.0pt;">58.5</span>
</span>
</td>
<td id="S5.T2.3.6.6.13" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.13.1.1" class="ltx_p" style="width:26.0pt;">53.3</span>
</span>
</td>
<td id="S5.T2.3.6.6.14" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.6.6.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.6.6.14.1.1" class="ltx_p" style="width:26.0pt;">177.0</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.7.7" class="ltx_tr">
<td id="S5.T2.3.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.1.1.1" class="ltx_p" style="width:8.7pt;">r5</span>
</span>
</td>
<td id="S5.T2.3.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.2.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.7.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.7.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.7.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.6.1.1" class="ltx_p" style="width:21.7pt;"></span>
</span>
</td>
<td id="S5.T2.3.7.7.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.7.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.7.7.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.8.1.1" class="ltx_p" style="width:26.0pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.7.7.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.7.7.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.9.1.1" class="ltx_p" style="width:30.4pt;">20</span>
</span>
</td>
<td id="S5.T2.3.7.7.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.10.1.1" class="ltx_p" style="width:26.0pt;">46.2</span>
</span>
</td>
<td id="S5.T2.3.7.7.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.11.1.1" class="ltx_p" style="width:17.3pt;">27.4</span>
</span>
</td>
<td id="S5.T2.3.7.7.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.12.1.1" class="ltx_p" style="width:26.0pt;">59.8</span>
</span>
</td>
<td id="S5.T2.3.7.7.13" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.13.1.1" class="ltx_p" style="width:26.0pt;">54.0</span>
</span>
</td>
<td id="S5.T2.3.7.7.14" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.7.7.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.7.7.14.1.1" class="ltx_p" style="width:26.0pt;">185.6</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.8.8" class="ltx_tr">
<td id="S5.T2.3.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.1.1.1" class="ltx_p" style="width:8.7pt;">r6</span>
</span>
</td>
<td id="S5.T2.3.8.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.2.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.8.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.8.8.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.7.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.8.8.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.8.1.1" class="ltx_p" style="width:26.0pt;"></span>
</span>
</td>
<td id="S5.T2.3.8.8.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.3.8.8.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.9.1.1" class="ltx_p" style="width:30.4pt;">-</span>
</span>
</td>
<td id="S5.T2.3.8.8.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.10.1.1" class="ltx_p" style="width:26.0pt;">22.0</span>
</span>
</td>
<td id="S5.T2.3.8.8.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.11.1.1" class="ltx_p" style="width:17.3pt;">13.4</span>
</span>
</td>
<td id="S5.T2.3.8.8.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.12.1.1" class="ltx_p" style="width:26.0pt;">24.2</span>
</span>
</td>
<td id="S5.T2.3.8.8.13" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.13.1.1" class="ltx_p" style="width:26.0pt;">34.4</span>
</span>
</td>
<td id="S5.T2.3.8.8.14" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.8.8.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.8.8.14.1.1" class="ltx_p" style="width:26.0pt;">157.4</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.9.9" class="ltx_tr">
<td id="S5.T2.3.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.1.1.1" class="ltx_p" style="width:8.7pt;">r7</span>
</span>
</td>
<td id="S5.T2.3.9.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.2.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.9.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.9.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.9.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.9.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.9.9.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.7.1.1" class="ltx_p" style="width:17.3pt;"></span>
</span>
</td>
<td id="S5.T2.3.9.9.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.8.1.1" class="ltx_p" style="width:26.0pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.9.9.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.9.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.9.1.1" class="ltx_p" style="width:30.4pt;">9</span>
</span>
</td>
<td id="S5.T2.3.9.9.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.10.1.1" class="ltx_p" style="width:26.0pt;">58.0</span>
</span>
</td>
<td id="S5.T2.3.9.9.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.11.1.1" class="ltx_p" style="width:17.3pt;">24.7</span>
</span>
</td>
<td id="S5.T2.3.9.9.12" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.12.1.1" class="ltx_p" style="width:26.0pt;">58.9</span>
</span>
</td>
<td id="S5.T2.3.9.9.13" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.13.1.1" class="ltx_p" style="width:26.0pt;">55.5</span>
</span>
</td>
<td id="S5.T2.3.9.9.14" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.9.9.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.9.9.14.1.1" class="ltx_p" style="width:26.0pt;">215.3</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.10.10" class="ltx_tr">
<td id="S5.T2.3.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.1.1.1" class="ltx_p" style="width:8.7pt;">r8</span>
</span>
</td>
<td id="S5.T2.3.10.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.2.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.10.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.10.10.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.10.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.10.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.10.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.10.10.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.7.1.1" class="ltx_p" style="width:17.3pt;"></span>
</span>
</td>
<td id="S5.T2.3.10.10.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.8.1.1" class="ltx_p" style="width:26.0pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.10.10.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.10.10.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.9.1.1" class="ltx_p" style="width:30.4pt;">20</span>
</span>
</td>
<td id="S5.T2.3.10.10.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.10.1.1" class="ltx_p" style="width:26.0pt;">64.1</span>
</span>
</td>
<td id="S5.T2.3.10.10.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.11.1.1" class="ltx_p" style="width:17.3pt;">27.6</span>
</span>
</td>
<td id="S5.T2.3.10.10.12" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.12.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.10.10.12.1.1.1" class="ltx_text ltx_font_bold">60.7</span></span>
</span>
</td>
<td id="S5.T2.3.10.10.13" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.13.1.1" class="ltx_p" style="width:26.0pt;">56.5</span>
</span>
</td>
<td id="S5.T2.3.10.10.14" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.10.10.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.10.10.14.1.1" class="ltx_p" style="width:26.0pt;">210.7</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.11.11" class="ltx_tr">
<td id="S5.T2.3.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.1.1.1" class="ltx_p" style="width:8.7pt;">r9</span>
</span>
</td>
<td id="S5.T2.3.11.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.2.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.11.11.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.11.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.11.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.11.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.11.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.11.11.7" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.7.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.11.11.8" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.8.1.1" class="ltx_p" style="width:26.0pt;"></span>
</span>
</td>
<td id="S5.T2.3.11.11.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T2.3.11.11.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.9.1.1" class="ltx_p" style="width:30.4pt;">20</span>
</span>
</td>
<td id="S5.T2.3.11.11.10" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.10.1.1" class="ltx_p" style="width:26.0pt;">62.8</span>
</span>
</td>
<td id="S5.T2.3.11.11.11" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.11.1.1" class="ltx_p" style="width:17.3pt;">26.7</span>
</span>
</td>
<td id="S5.T2.3.11.11.12" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.12.1.1" class="ltx_p" style="width:26.0pt;">58.7</span>
</span>
</td>
<td id="S5.T2.3.11.11.13" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.13.1.1" class="ltx_p" style="width:26.0pt;">55.4</span>
</span>
</td>
<td id="S5.T2.3.11.11.14" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T2.3.11.11.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.11.11.14.1.1" class="ltx_p" style="width:26.0pt;">181.1</span>
</span>
</td>
</tr>
<tr id="S5.T2.3.12.12" class="ltx_tr">
<td id="S5.T2.3.12.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T2.3.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.1.1.1" class="ltx_p" style="width:8.7pt;">r10</span>
</span>
</td>
<td id="S5.T2.3.12.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.2.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.12.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T2.3.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.3.1.1" class="ltx_p" style="width:8.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.12.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.4.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.12.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T2.3.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.5.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.12.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T2.3.12.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.6.1.1" class="ltx_p" style="width:21.7pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.12.12.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.7.1.1" class="ltx_p" style="width:17.3pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.12.12.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.8.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.8.1.1" class="ltx_p" style="width:26.0pt;">✓</span>
</span>
</td>
<td id="S5.T2.3.12.12.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T2.3.12.12.9.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.9.1.1" class="ltx_p" style="width:30.4pt;">20</span>
</span>
</td>
<td id="S5.T2.3.12.12.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.10.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.10.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.12.12.10.1.1.1" class="ltx_text ltx_font_bold">65.4</span></span>
</span>
</td>
<td id="S5.T2.3.12.12.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.11.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.11.1.1" class="ltx_p" style="width:17.3pt;"><span id="S5.T2.3.12.12.11.1.1.1" class="ltx_text ltx_font_bold">29.4</span></span>
</span>
</td>
<td id="S5.T2.3.12.12.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.12.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.12.1.1" class="ltx_p" style="width:26.0pt;">59.3</span>
</span>
</td>
<td id="S5.T2.3.12.12.13" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.13.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.13.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.12.12.13.1.1.1" class="ltx_text ltx_font_bold">57.6</span></span>
</span>
</td>
<td id="S5.T2.3.12.12.14" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S5.T2.3.12.12.14.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.12.12.14.1.1" class="ltx_p" style="width:26.0pt;"><span id="S5.T2.3.12.12.14.1.1.1" class="ltx_text ltx_font_bold">221.7</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Main Results</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.2" class="ltx_p">We first compare UReader with state-of-the-art ocr-free models on 10 datasets. For fair and consistent comparison across all datasets, we finetune the strong and accessible baseline Dount on unreported datasets. As shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Instruction Data Resources ‣ 4 Instruction Tuning ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, UReader achieves state-of-the-art performance in 8 tasks across 5 domains, covering Visual Question Answering, Information Extraction, Natural Language Inference and Image Captioning tasks. With much fewer trainable parameters (86M vs 1.3B) and without a specific finetuning stage, UReader outperforms the strong pretriaining model Pix2Struct<sub id="S5.SS3.p1.2.1" class="ltx_sub"><span id="S5.SS3.p1.2.1.1" class="ltx_text ltx_font_italic">large</span></sub> in InfoVQA, ChartQA, and TextCaps. Considering that Pix2Struct<sub id="S5.SS3.p1.2.2" class="ltx_sub"><span id="S5.SS3.p1.2.2.1" class="ltx_text ltx_font_italic">large</span></sub> is trained more than 170k steps with a batch size of 1024 on 128 TPUs, this validates that with the help of open-domain Multimodal Large Language Models, learning costs for universal visually-situated language understanding can be greatly reduced. More detailed analysis can be found in <a href="#A2" title="Appendix B Detailed Analysis on Performance ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Ablation Study</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">We perform comprehensive ablation experiments to validate the contribution of two auxiliary tasks, trainable architectures, cross-domain joint training and the design of shape-adaptive cropping module.</p>
</div>
<section id="S5.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Auxiliary Tasks.</h4>

<div id="S5.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS4.SSS0.Px1.p1.1" class="ltx_p">As shown in Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Evaluation ‣ 5 Experiments ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, dropping the Key Points Generation task (r10 vs r2) causes a performance decrease on all domains of datasets, demonstrating that this task helps the model better understand the vision-and-language semantic. Further removing the Text Reading task (r2 vs r1) causes more significant performance degradation, which validates the importance of enhancing text recognition ability across different domains.</p>
</div>
</section>
<section id="S5.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Trainable Architectures.</h4>

<div id="S5.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS4.SSS0.Px2.p1.1" class="ltx_p">Both the visual abstractor and LoRA in LLM are finetuned in UReader (r10). Freezing either the visual abstractor (r3) or LoRA (r4)
causes performance decrease, which demonstrates that both the vision and language parts should be finetuned for adjusting to Visually-situated Language Understanding.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2310.05126/assets/cut_map.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of the frequency of selected grid with shape-adaptive cropping module. The cell at row <math id="S5.F3.4.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.F3.4.m1.1b"><mi id="S5.F3.4.m1.1.1" xref="S5.F3.4.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.F3.4.m1.1c"><ci id="S5.F3.4.m1.1.1.cmml" xref="S5.F3.4.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.4.m1.1d">i</annotation></semantics></math> and column <math id="S5.F3.5.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S5.F3.5.m2.1b"><mi id="S5.F3.5.m2.1.1" xref="S5.F3.5.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S5.F3.5.m2.1c"><ci id="S5.F3.5.m2.1.1.cmml" xref="S5.F3.5.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.5.m2.1d">j</annotation></semantics></math> denotes the selected frequency of grid <math id="S5.F3.6.m3.1" class="ltx_Math" alttext="(n_{h}=i,n_{w}=j)" display="inline"><semantics id="S5.F3.6.m3.1b"><mrow id="S5.F3.6.m3.1.1.1"><mo stretchy="false" id="S5.F3.6.m3.1.1.1.2">(</mo><mrow id="S5.F3.6.m3.1.1.1.1.2" xref="S5.F3.6.m3.1.1.1.1.3.cmml"><mrow id="S5.F3.6.m3.1.1.1.1.1.1" xref="S5.F3.6.m3.1.1.1.1.1.1.cmml"><msub id="S5.F3.6.m3.1.1.1.1.1.1.2" xref="S5.F3.6.m3.1.1.1.1.1.1.2.cmml"><mi id="S5.F3.6.m3.1.1.1.1.1.1.2.2" xref="S5.F3.6.m3.1.1.1.1.1.1.2.2.cmml">n</mi><mi id="S5.F3.6.m3.1.1.1.1.1.1.2.3" xref="S5.F3.6.m3.1.1.1.1.1.1.2.3.cmml">h</mi></msub><mo id="S5.F3.6.m3.1.1.1.1.1.1.1" xref="S5.F3.6.m3.1.1.1.1.1.1.1.cmml">=</mo><mi id="S5.F3.6.m3.1.1.1.1.1.1.3" xref="S5.F3.6.m3.1.1.1.1.1.1.3.cmml">i</mi></mrow><mo id="S5.F3.6.m3.1.1.1.1.2.3" xref="S5.F3.6.m3.1.1.1.1.3a.cmml">,</mo><mrow id="S5.F3.6.m3.1.1.1.1.2.2" xref="S5.F3.6.m3.1.1.1.1.2.2.cmml"><msub id="S5.F3.6.m3.1.1.1.1.2.2.2" xref="S5.F3.6.m3.1.1.1.1.2.2.2.cmml"><mi id="S5.F3.6.m3.1.1.1.1.2.2.2.2" xref="S5.F3.6.m3.1.1.1.1.2.2.2.2.cmml">n</mi><mi id="S5.F3.6.m3.1.1.1.1.2.2.2.3" xref="S5.F3.6.m3.1.1.1.1.2.2.2.3.cmml">w</mi></msub><mo id="S5.F3.6.m3.1.1.1.1.2.2.1" xref="S5.F3.6.m3.1.1.1.1.2.2.1.cmml">=</mo><mi id="S5.F3.6.m3.1.1.1.1.2.2.3" xref="S5.F3.6.m3.1.1.1.1.2.2.3.cmml">j</mi></mrow></mrow><mo stretchy="false" id="S5.F3.6.m3.1.1.1.3">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.6.m3.1c"><apply id="S5.F3.6.m3.1.1.1.1.3.cmml" xref="S5.F3.6.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.F3.6.m3.1.1.1.1.3a.cmml" xref="S5.F3.6.m3.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S5.F3.6.m3.1.1.1.1.1.1.cmml" xref="S5.F3.6.m3.1.1.1.1.1.1"><eq id="S5.F3.6.m3.1.1.1.1.1.1.1.cmml" xref="S5.F3.6.m3.1.1.1.1.1.1.1"></eq><apply id="S5.F3.6.m3.1.1.1.1.1.1.2.cmml" xref="S5.F3.6.m3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.F3.6.m3.1.1.1.1.1.1.2.1.cmml" xref="S5.F3.6.m3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.F3.6.m3.1.1.1.1.1.1.2.2.cmml" xref="S5.F3.6.m3.1.1.1.1.1.1.2.2">𝑛</ci><ci id="S5.F3.6.m3.1.1.1.1.1.1.2.3.cmml" xref="S5.F3.6.m3.1.1.1.1.1.1.2.3">ℎ</ci></apply><ci id="S5.F3.6.m3.1.1.1.1.1.1.3.cmml" xref="S5.F3.6.m3.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S5.F3.6.m3.1.1.1.1.2.2.cmml" xref="S5.F3.6.m3.1.1.1.1.2.2"><eq id="S5.F3.6.m3.1.1.1.1.2.2.1.cmml" xref="S5.F3.6.m3.1.1.1.1.2.2.1"></eq><apply id="S5.F3.6.m3.1.1.1.1.2.2.2.cmml" xref="S5.F3.6.m3.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S5.F3.6.m3.1.1.1.1.2.2.2.1.cmml" xref="S5.F3.6.m3.1.1.1.1.2.2.2">subscript</csymbol><ci id="S5.F3.6.m3.1.1.1.1.2.2.2.2.cmml" xref="S5.F3.6.m3.1.1.1.1.2.2.2.2">𝑛</ci><ci id="S5.F3.6.m3.1.1.1.1.2.2.2.3.cmml" xref="S5.F3.6.m3.1.1.1.1.2.2.2.3">𝑤</ci></apply><ci id="S5.F3.6.m3.1.1.1.1.2.2.3.cmml" xref="S5.F3.6.m3.1.1.1.1.2.2.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.6.m3.1d">(n_{h}=i,n_{w}=j)</annotation></semantics></math>. Deeper colors represent higher selection frequencies.</figcaption>
</figure>
</section>
<section id="S5.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Cross-domain Joint Training.</h4>

<div id="S5.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS4.SSS0.Px3.p1.1" class="ltx_p">After removing 4 document datasets from the training data, UReader achieves worse performance (r10 vs r5) on the table, natural image, and webpage domains, validating that images of different domains share some common characteristics and cross-domain joint training improves the universal performance. Besides, although trained without document data, our model achieves a 46.2 score on the DocVQA dataset, showing the potential out-of-domain understanding ability of our training paradigm.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2310.05126/assets/x3.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative results of UReader. Crucial regions are enlarged for clearer visualization.</figcaption>
</figure>
</section>
<section id="S5.SS4.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Shape-adaptive Cropping.</h4>

<div id="S5.SS4.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS4.SSS0.Px4.p1.1" class="ltx_p">The r6 in Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Evaluation ‣ 5 Experiments ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> represents directly tuning the mPLUG-Owl without any model revisions. With the shape-adaptive cropping, UReader achieves significantly better performance (r7 vs r6), showing that our cropping module is indispensable to leverage pretrained low-resolution vision encoder for universal visually-situated language understanding. Besides, increasing the cropping numbers (r8 vs r7) improves the model’s performance. Due to the resolution of each local image being constant (224x224), more crops mean higher overall resolution and therefore achieves better performance. Furthermore, adding a resized global image bring a slight improvement in most datasets (r10 vs r8), validating that a complete image could alleviate possible information loss due to image cropping. Finally, dropping crop position encoding also hurts the model’s performance (r10 vs r9), proving the effectiveness of crop position encoding for correlating local images.</p>
</div>
<div id="S5.SS4.SSS0.Px4.p2" class="ltx_para">
<p id="S5.SS4.SSS0.Px4.p2.1" class="ltx_p">For alleviating the distortion problem due to resizing, we propose to crop images according to their raw aspect ratio.
<a href="#S5.F3" title="In Trainable Architectures. ‣ 5.4 Ablation Study ‣ 5 Experiments ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the frequency distribution of grids selected by our shape-adaptive cropping module on DocVQA, VisualMRC and WikiTableQuestions (the distribution on more datasets can be found in the Appendix <a href="#A1" title="Appendix A Grid Distribution on Downstream Datasets ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>).
For aesthetic purposes, we present the distribution with <math id="S5.SS4.SSS0.Px4.p2.1.m1.1" class="ltx_Math" alttext="N_{c}=9" display="inline"><semantics id="S5.SS4.SSS0.Px4.p2.1.m1.1a"><mrow id="S5.SS4.SSS0.Px4.p2.1.m1.1.1" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.cmml"><msub id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.cmml"><mi id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.2" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.2.cmml">N</mi><mi id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.3" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.3.cmml">c</mi></msub><mo id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.1" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.3" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS0.Px4.p2.1.m1.1b"><apply id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.cmml" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1"><eq id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.1.cmml" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.1"></eq><apply id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.cmml" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.1.cmml" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.2.cmml" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.2">𝑁</ci><ci id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.3.cmml" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.2.3">𝑐</ci></apply><cn type="integer" id="S5.SS4.SSS0.Px4.p2.1.m1.1.1.3.cmml" xref="S5.SS4.SSS0.Px4.p2.1.m1.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS0.Px4.p2.1.m1.1c">N_{c}=9</annotation></semantics></math>. Apparently, different domains of images have different shape distributions. For most document images in DocVQA, their height is greater than the width, while table images are the opposite. As webpages are scrollable, their screenshots are always in the form of a long rectangular shape. With the shape-adaptive cropping design, our model can easily adapt to various image shapes without domain-specific fine-tuning.</p>
</div>
<div id="S5.SS4.SSS0.Px4.p3" class="ltx_para">
<p id="S5.SS4.SSS0.Px4.p3.1" class="ltx_p">Text distortion may pose little influence on visual question answering because they are always about partial text information. But it is harmful for reading texts in the image because every text matters. For quantitative analysis of the influence of shape-adaptive design, we directly evaluate the performance of reading all texts. We choose the Bleu <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a href="#bib.bib26" title="" class="ltx_ref">2002</a>)</cite> as the metric because it directly measures the n-gram overlap between the ground-truth and predicted text sequence. The evaluation set is built by combining 100 randomly-selected test images from each dataset. As shown in <a href="#S5.T3" title="In Shape-adaptive Cropping. ‣ 5.4 Ablation Study ‣ 5 Experiments ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>, compared with cropping all images with a fixed grid, UReader could better recognize texts in the image due to our shape-adaptive design that alleviates the text distortion problem.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>The Text Reading performance of UReader under the condition of <math id="S5.T3.3.m1.1" class="ltx_Math" alttext="N_{c}=9" display="inline"><semantics id="S5.T3.3.m1.1b"><mrow id="S5.T3.3.m1.1.1" xref="S5.T3.3.m1.1.1.cmml"><msub id="S5.T3.3.m1.1.1.2" xref="S5.T3.3.m1.1.1.2.cmml"><mi id="S5.T3.3.m1.1.1.2.2" xref="S5.T3.3.m1.1.1.2.2.cmml">N</mi><mi id="S5.T3.3.m1.1.1.2.3" xref="S5.T3.3.m1.1.1.2.3.cmml">c</mi></msub><mo id="S5.T3.3.m1.1.1.1" xref="S5.T3.3.m1.1.1.1.cmml">=</mo><mn id="S5.T3.3.m1.1.1.3" xref="S5.T3.3.m1.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.3.m1.1c"><apply id="S5.T3.3.m1.1.1.cmml" xref="S5.T3.3.m1.1.1"><eq id="S5.T3.3.m1.1.1.1.cmml" xref="S5.T3.3.m1.1.1.1"></eq><apply id="S5.T3.3.m1.1.1.2.cmml" xref="S5.T3.3.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T3.3.m1.1.1.2.1.cmml" xref="S5.T3.3.m1.1.1.2">subscript</csymbol><ci id="S5.T3.3.m1.1.1.2.2.cmml" xref="S5.T3.3.m1.1.1.2.2">𝑁</ci><ci id="S5.T3.3.m1.1.1.2.3.cmml" xref="S5.T3.3.m1.1.1.2.3">𝑐</ci></apply><cn type="integer" id="S5.T3.3.m1.1.1.3.cmml" xref="S5.T3.3.m1.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.m1.1d">N_{c}=9</annotation></semantics></math>. ‘w/o adapt means removing the shape-adaptive design and cropping the image with a fixed grid <math id="S5.T3.4.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S5.T3.4.m2.1b"><mrow id="S5.T3.4.m2.1.1" xref="S5.T3.4.m2.1.1.cmml"><mn id="S5.T3.4.m2.1.1.2" xref="S5.T3.4.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S5.T3.4.m2.1.1.1" xref="S5.T3.4.m2.1.1.1.cmml">×</mo><mn id="S5.T3.4.m2.1.1.3" xref="S5.T3.4.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.4.m2.1c"><apply id="S5.T3.4.m2.1.1.cmml" xref="S5.T3.4.m2.1.1"><times id="S5.T3.4.m2.1.1.1.cmml" xref="S5.T3.4.m2.1.1.1"></times><cn type="integer" id="S5.T3.4.m2.1.1.2.cmml" xref="S5.T3.4.m2.1.1.2">3</cn><cn type="integer" id="S5.T3.4.m2.1.1.3.cmml" xref="S5.T3.4.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.m2.1d">3\times 3</annotation></semantics></math>.</figcaption>
<table id="S5.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.5.1.1" class="ltx_tr">
<th id="S5.T3.5.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">
<span id="S5.T3.5.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.1.1.1.1.1" class="ltx_p" style="width:173.4pt;"><span id="S5.T3.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span>
</th>
<th id="S5.T3.5.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.5.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.1.1.2.1.1" class="ltx_p" style="width:30.4pt;"><span id="S5.T3.5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Bleu1</span></span>
</span>
</th>
<th id="S5.T3.5.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.5.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.1.1.3.1.1" class="ltx_p" style="width:30.4pt;"><span id="S5.T3.5.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Bleu2</span></span>
</span>
</th>
<th id="S5.T3.5.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.5.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.1.1.4.1.1" class="ltx_p" style="width:30.4pt;"><span id="S5.T3.5.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Bleu3</span></span>
</span>
</th>
<th id="S5.T3.5.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T3.5.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.1.1.5.1.1" class="ltx_p" style="width:30.4pt;"><span id="S5.T3.5.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Bleu4</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.5.2.1" class="ltx_tr">
<th id="S5.T3.5.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S5.T3.5.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.2.1.1.1.1" class="ltx_p" style="width:173.4pt;">UReader w/o adapt</span>
</span>
</th>
<td id="S5.T3.5.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.5.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.2.1.2.1.1" class="ltx_p" style="width:30.4pt;">21.4</span>
</span>
</td>
<td id="S5.T3.5.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.5.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.2.1.3.1.1" class="ltx_p" style="width:30.4pt;">15.4</span>
</span>
</td>
<td id="S5.T3.5.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.5.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.2.1.4.1.1" class="ltx_p" style="width:30.4pt;">12.0</span>
</span>
</td>
<td id="S5.T3.5.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T3.5.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.2.1.5.1.1" class="ltx_p" style="width:30.4pt;">9.7</span>
</span>
</td>
</tr>
<tr id="S5.T3.5.3.2" class="ltx_tr">
<th id="S5.T3.5.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_bb ltx_border_r">
<span id="S5.T3.5.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.3.2.1.1.1" class="ltx_p" style="width:173.4pt;">UReader</span>
</span>
</th>
<td id="S5.T3.5.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.5.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.3.2.2.1.1" class="ltx_p" style="width:30.4pt;"><span id="S5.T3.5.3.2.2.1.1.1" class="ltx_text ltx_font_bold">24.9</span></span>
</span>
</td>
<td id="S5.T3.5.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.5.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.3.2.3.1.1" class="ltx_p" style="width:30.4pt;"><span id="S5.T3.5.3.2.3.1.1.1" class="ltx_text ltx_font_bold">18.1</span></span>
</span>
</td>
<td id="S5.T3.5.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.5.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.3.2.4.1.1" class="ltx_p" style="width:30.4pt;"><span id="S5.T3.5.3.2.4.1.1.1" class="ltx_text ltx_font_bold">14.3</span></span>
</span>
</td>
<td id="S5.T3.5.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S5.T3.5.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.5.3.2.5.1.1" class="ltx_p" style="width:30.4pt;"><span id="S5.T3.5.3.2.5.1.1.1" class="ltx_text ltx_font_bold">11.7</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Qualitative Results</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><a href="#S5.F4" title="In Cross-domain Joint Training. ‣ 5.4 Ablation Study ‣ 5 Experiments ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> show some qualitative results produced by our UReader on different types of images. UReader could not only extract information from the document (case a), but also understand different instructions and provide corresponding answers by attending to different regions (case b).
Table understanding always involves layout comprehension and statistics. As shown in case c, given a table image, UReader could well relate different columns to answer the ‘first movie’ and perform simple statistics about the ‘total number’. As for images with multiple paragraphs of text, e.g. webpage screenshot in case e, UReader could also locate the relevant paragraph, understand the texts and answer the question accurately. Case d shows the text reading performance. With the help of the Text Reading task, UReader is able to read texts from top left to bottom right. But, due to the language decoding manner, when given an image with rich texts, such as a page of a book, the model often reads the beginning texts and then continues writing without watching the image. More qualitative results can be found in <a href="#A3" title="Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>. Finally, as shown in case f, UReader is able to list some key points about the chart by combining the title and line information. Listing key points in this work is just a superficial attempt at open-ended generation, and its performance is far from promising, e.g., UReader makes a mistake about the lowest line. More effort is needed towards a comprehensive understanding of images with rich text.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We first propose to leverage existing Multimodal Large Language Models for universal ocr-free visually-situated language understanding through low-cost instruction tuning. All downstream tasks are reorganized into a unified instruction-tuning format. Besides, we design the Text Reading task and Key Points Generation task to enhance text recognition and vision-and-language semantic comprehension abilities. To utilize the pre-trained vision encoder for processing high-resolution images, we design a shape-adaptive cropping module, which cuts the image into multiple local images considering its raw aspect ratio and resolution. UReader achieve state-of-the-art ocr-free performance in 8 out of 10 datasets, ranging from documents, tables, charts, and natural images to webpage screenshots.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Our experiments validate that UReader is able to correlate local images after cropping a high-resolution image. However, UReader struggles to understand multi-page documents (e.g. books and papers) due to lacking ability to correlate different pages and the limited sequence length of the decoder. Besides, UReader feeds an equal number of features for each local image into the language decoder. But, not all local images contain rich vision or text information. In the future, we will explore a more efficient way to encode different crops. Furthermore, the open-ended generation about Visually-situated Language understanding is far from well studied. We try developing key points generation ability in this work but more difficult generation tasks are not currently considered, such as giving the chain-of-the-thought of the answer. How to simulate such abilities through instruction tuning is a topic worth studying. Finally, the Text Reading task helps the model recognize texts, but the text reading performance with the LLM as the decoder is far from satisfactory due to the hallucination problem. Instructing the LLM to read texts strictly according to images is a challenging topic.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">Our UReader relies on multi-modal large language models that are trained on large-scale image and text data from the web and therefore may be subject to issues such as toxic language and bias <cite class="ltx_cite ltx_citemacro_cite">Bender et al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>. However, our model is further fine-tuned on publicly available datasets and is used specifically in the domain of visually-situated language understanding, where these issues have minimal impact.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender et al. (2021)</span>
<span class="ltx_bibblock">
Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language models be too big?

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, pages 610–623.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biten et al. (2019)</span>
<span class="ltx_bibblock">
Ali Furkan Biten, Rubèn Tito, Andrés Mafla, Lluís Gómez i Bigorda, Marçal Rusiñol, C. V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. 2019.

</span>
<span class="ltx_bibblock">Scene text visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, pages 4290–4300. IEEE.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borchmann et al. (2021)</span>
<span class="ltx_bibblock">
Lukasz Borchmann, Michal Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Michal Turski, Karolina Szyndler, and Filip Gralinski. 2021.

</span>
<span class="ltx_bibblock">DUE: end-to-end document understanding benchmark.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">NeurIPS Datasets and Benchmarks</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020.

</span>
<span class="ltx_bibblock">Tabfact : A large-scale dataset for table-based fact verification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, Addis Ababa, Ethiopia.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. 2021.

</span>
<span class="ltx_bibblock">Websrc: A dataset for web-based structural reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 4173–4185.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.06500.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davis et al. (2022)</span>
<span class="ltx_bibblock">
Brian L. Davis, Bryan S. Morse, Brian L. Price, Chris Tensmeyer, Curtis Wigington, and Vlad I. Morariu. 2022.

</span>
<span class="ltx_bibblock">End-to-end document recognition and understanding with dessurt.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ECCV Workshops (4)</em>, volume 13804 of <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 280–296. Springer.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2023)</span>
<span class="ltx_bibblock">
Hao Feng, Zijian Wang, Ji Tang, Jinghui Lu, Wen gang Zhou, Houqiang Li, and Can Huang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:261065237" title="" class="ltx_ref ltx_href">Unidoc: A universal large multimodal model for simultaneous text detection, recognition, spotting and understanding</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2308.11592.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Anwen Hu, Shizhe Chen, and Qin Jin. 2021.

</span>
<span class="ltx_bibblock">Question-controlled text-aware image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ACM Multimedia</em>, pages 3097–3105. ACM.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">Language is not all you need: Aligning perception with language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.14045.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022)</span>
<span class="ltx_bibblock">
Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">Layoutlmv3: Pre-training for document AI with unified text and image masking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ACM Multimedia</em>, pages 4083–4091. ACM.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle et al. (2018)</span>
<span class="ltx_bibblock">
Kushal Kafle, Brian L. Price, Scott Cohen, and Christopher Kanan. 2018.

</span>
<span class="ltx_bibblock">DVQA: understanding data visualizations via question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 5648–5656. Computer Vision Foundation / IEEE Computer Society.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kahou et al. (2018)</span>
<span class="ltx_bibblock">
Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. 2018.

</span>
<span class="ltx_bibblock">Figureqa: An annotated figure dataset for visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ICLR (Workshop)</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2022)</span>
<span class="ltx_bibblock">
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2022.

</span>
<span class="ltx_bibblock">Ocr-free document understanding transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ECCV (28)</em>, volume 13688 of <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 498–517. Springer.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krasin et al. (2017)</span>
<span class="ltx_bibblock">
Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Shahab Kamali, Matteo Malloci, Jordi Pont-Tuset, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. 2017.

</span>
<span class="ltx_bibblock">Openimages: A public dataset for large-scale multi-label and multi-class image classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Dataset available from https://storage.googleapis.com/openimages/web/index.html</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. 2022.

</span>
<span class="ltx_bibblock">Pix2struct: Screenshot parsing as pretraining for visual language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2210.03347.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023.

</span>
<span class="ltx_bibblock">BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2301.12597.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.08485.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. 2023b.

</span>
<span class="ltx_bibblock">On the hidden mystery of ocr in large multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07895</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et al. (2022)</span>
<span class="ltx_bibblock">
Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. 2022.

</span>
<span class="ltx_bibblock">Chartqa: A benchmark for question answering about charts with visual and logical reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ACL (Findings)</em>, pages 2263–2279. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. (2022)</span>
<span class="ltx_bibblock">
Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. 2022.

</span>
<span class="ltx_bibblock">Infographicvqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">WACV</em>, pages 2582–2591. IEEE.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. (2021)</span>
<span class="ltx_bibblock">
Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2021.

</span>
<span class="ltx_bibblock">Docvqa: A dataset for VQA on document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">WACV</em>, pages 2199–2208. IEEE.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Methani et al. (2020)</span>
<span class="ltx_bibblock">
Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. 2020.

</span>
<span class="ltx_bibblock">Plotqa: Reasoning over scientific plots.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">WACV</em>, pages 1516–1525. IEEE.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2019)</span>
<span class="ltx_bibblock">
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019.

</span>
<span class="ltx_bibblock">OCR-VQA: visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ICDAR</em>, pages 947–952. IEEE.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat and Liang (2015)</span>
<span class="ltx_bibblock">
Panupong Pasupat and Percy Liang. 2015.

</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>, pages 1470–1480. The Association for Computer Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ICML</em>, volume 139 of <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 8748–8763. PMLR.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidorov et al. (2020)</span>
<span class="ltx_bibblock">
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. 2020.

</span>
<span class="ltx_bibblock">Textcaps: A dataset for image captioning with reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ECCV (2)</em>, volume 12347 of <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 742–758. Springer.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2019)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019.

</span>
<span class="ltx_bibblock">Towards VQA models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 8317–8326. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stanislawek et al. (2021)</span>
<span class="ltx_bibblock">
Tomasz Stanislawek, Filip Gralinski, Anna Wróblewska, Dawid Lipinski, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and Przemyslaw Biecek. 2021.

</span>
<span class="ltx_bibblock">Kleister: Key information extraction datasets involving long documents with complex layouts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ICDAR (1)</em>, volume 12821 of <em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 564–579. Springer.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Svetlichnaya (2020)</span>
<span class="ltx_bibblock">
S Svetlichnaya. 2020.

</span>
<span class="ltx_bibblock">Deepform: Understand structured documents at scale.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanaka et al. (2021)</span>
<span class="ltx_bibblock">
Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021.

</span>
<span class="ltx_bibblock">Visualmrc: Machine reading comprehension on document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, pages 13878–13888. AAAI Press.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. 2023.

</span>
<span class="ltx_bibblock">Unifying vision, text, and layout for universal document processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 19254–19264.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vedantam et al. (2015)</span>
<span class="ltx_bibblock">
Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">Cider: Consensus-based image description evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 4566–4575. IEEE Computer Society.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vicuna (2023)</span>
<span class="ltx_bibblock">
Vicuna. 2023.

</span>
<span class="ltx_bibblock">Vicuna: An open chatbot impressing gpt-4.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/lm-sys/FastChat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lm-sys/FastChat</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2021)</span>
<span class="ltx_bibblock">
Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei A. F. Florêncio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. 2021.

</span>
<span class="ltx_bibblock">Layoutlmv2: Multi-modal pre-training for visually-rich document understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ACL/IJCNLP (1)</em>, pages 2579–2591. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2021)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florêncio, Lijuan Wang, Cha Zhang, Lei Zhang, and Jiebo Luo. 2021.

</span>
<span class="ltx_bibblock">TAP: text-aware pre-training for text-vqa and text-caption.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 8751–8761. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. 2023.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.14178.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Liang Zhang, Anwen Hu, Jing Zhang, Shuo Hu, and Qin Jin. 2023.

</span>
<span class="ltx_bibblock">MPMQA: multimodal question answering on product manuals.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.09660.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced large language models.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A0.F5" class="ltx_figure"><img src="/html/2310.05126/assets/cut_map_full.png" id="A0.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Visualization of the frequency of selected grid with the shape-adaptive cropping module on 10 downstream datasets.</figcaption>
</figure>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Grid Distribution on Downstream Datasets</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We visualize the frequency distribution of grids selected by our shape-adaptive cropping module on all ten datasets in <a href="#A0.F5" title="In UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>. The wide variety of image shapes in downstream tasks highlights the crucial role of the shape-adaptive cropping module.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Detailed Analysis on Performance</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Underperforms Ocr-Free Baselines on DocVQA and DeepForm</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">It can be seen that UReaderunderperforms ocr-free baselines on DocVQA and DeepForm. There are two main factors: (1) Donut performs the pretraining on large-scale document dataset IIT-CDIP (11M document images), which is the same domain as DocVQA and DeepForm. But UReader does no have a pretraining process and is just instruction finetuned on ensembled datasets (less than 0.5M assorted images). Training with more document images brings better performance. (2) The pretraining task of Pix2struct is to predict the HTML dom tree of a masked web screenshot, which requires the model to fully understand the layout information of the image. But UReader is trained to read texts from top to down, from left to right, which requires a weaker layout understanding ability. The pretraining on layout understanding also leads to improved performance on DocVQA.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p">The conclusion can also be substantiated by the observations on the other two datasets (i.e., InfoVQA and KLC) included in the document domain as previous work <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. For the InfoVQA dataset, the image is poster style and the layout is not as important as DocVQA and DeepForm but the relationship between text and vision objects matters more, like natural image and chart image. As for the KLC dataset, ocr-free models are only fed with the first page (always the cover of a report) , where the layout is much simpler than DocVQA and DeepForm. Therefore, UReadercan outperform baselines on these two document datasets.</p>
</div>
<div id="A2.SS1.p3" class="ltx_para">
<p id="A2.SS1.p3.1" class="ltx_p">In summary, compared with ocr-free model Donut and Pix2Struct, due to the pretrianing of MLMM on open-domain datasets, UReaderis better at understanding cross-modality relationships in the image but weaker at comprehending text layout information without large-scale document pretraining and specific layout understanding tasks.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Compared with Pipeline Methods</h3>

<figure id="A2.T4" class="ltx_table">
<div id="A2.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:30.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-170.0pt,11.9pt) scale(0.560452645406728,0.560452645406728) ;">
<table id="A2.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T4.1.1.1.1" class="ltx_tr">
<td id="A2.T4.1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="A2.T4.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">DocVQA</th>
<th id="A2.T4.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">InfoVQA</th>
<th id="A2.T4.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">DeepForm</th>
<th id="A2.T4.1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">KLC</th>
<th id="A2.T4.1.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">WTQ</th>
<th id="A2.T4.1.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">TabFact</th>
<th id="A2.T4.1.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">ChartQA</th>
<th id="A2.T4.1.1.1.1.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">TextVQA</th>
<th id="A2.T4.1.1.1.1.10" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">TextCaps</th>
<th id="A2.T4.1.1.1.1.11" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">VisualMRC</th>
</tr>
<tr id="A2.T4.1.1.2.2" class="ltx_tr">
<td id="A2.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">OCR-Pipline</td>
<td id="A2.T4.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">84.7(UDOP)</td>
<td id="A2.T4.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">47.4(UDOP)</td>
<td id="A2.T4.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">85.5(UDOP)</td>
<td id="A2.T4.1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_t">82.8(UDOP)</td>
<td id="A2.T4.1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_t">47.2(UDOP)</td>
<td id="A2.T4.1.1.2.2.7" class="ltx_td ltx_align_left ltx_border_t">72.9(UDOP)</td>
<td id="A2.T4.1.1.2.2.8" class="ltx_td ltx_align_left ltx_border_t">70.5(DePlot)</td>
<td id="A2.T4.1.1.2.2.9" class="ltx_td ltx_align_left ltx_border_t">56.3(PreSTU)</td>
<td id="A2.T4.1.1.2.2.10" class="ltx_td ltx_align_left ltx_border_t">139.1 (PreSTU)</td>
<td id="A2.T4.1.1.2.2.11" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">364.2(LayoutT5)</td>
</tr>
<tr id="A2.T4.1.1.3.3" class="ltx_tr">
<td id="A2.T4.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_bb">UReader</td>
<td id="A2.T4.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_bb">65.4</td>
<td id="A2.T4.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_bb">42.2</td>
<td id="A2.T4.1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_bb">49.5</td>
<td id="A2.T4.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_bb">32.8</td>
<td id="A2.T4.1.1.3.3.6" class="ltx_td ltx_align_left ltx_border_bb">29.4</td>
<td id="A2.T4.1.1.3.3.7" class="ltx_td ltx_align_left ltx_border_bb">67.6</td>
<td id="A2.T4.1.1.3.3.8" class="ltx_td ltx_align_left ltx_border_bb">59.3</td>
<td id="A2.T4.1.1.3.3.9" class="ltx_td ltx_align_left ltx_border_bb">57.6</td>
<td id="A2.T4.1.1.3.3.10" class="ltx_td ltx_align_left ltx_border_bb">118.4</td>
<td id="A2.T4.1.1.3.3.11" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">221.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison between UReaderand state-of-the-art pipeline methods.</figcaption>
</figure>
<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">We list the performance of state-of-the-art pipeline models in <a href="#A2.T4" title="In B.2 Compared with Pipeline Methods ‣ Appendix B Detailed Analysis on Performance ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>. We can summarize from the results that there are two distinct aspects. Firstly, our model achieves comparable or slightly worse results compared to the pipeline methods on TextVQA, ChartQA, InfoVQA, TextCaps and TabFact. Secondly, there is a obvious gap between our model and pipeline methods on DocVQA, DeepForm, KLC, WTQ and VisualMRC.</p>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.1" class="ltx_p">For the first aspect, there are two reasons for the similarity performance: (1) Modeling the diverse relationship between visual objects and text presents challenges for both pipeline-based methods and OCR-free methods. TextVQA, TextCaps and InfoVQA requires the relation understanding between text and visual objects (i.e. logos, icons and common objects). ChartQA asks for trend comprehension of lines. Understanding such complex cross-modality relation is challenging for both ocr-free and pipeline methods. (2) The simplicity of task formats can reduces performance gaps. Tabfact is a simply binary classification task resulting the small performance gap.</p>
</div>
<div id="A2.SS2.p3" class="ltx_para">
<p id="A2.SS2.p3.1" class="ltx_p">For this second aspect, the main performance gap appears in three categories of datasets: document, table, and webpage screenshot. The reasons are two folds: (1) The gap in terms of text recognition and layout extraction. In document, table and website, text is the dominant information source and the layout(e.g. row and column layout in table) is relatively uniformer than the chart and natural images. Therefore, with pre-extracted texts and layout information, it is more easy to understand the image. But for OCR-Free models, such as our UReader and Donut, it’s still challenging to fully recognize all texts. (2) The gap in terms of modeling capacity on multi-page document input. for multiple-page document datasets KLC (98% &gt; 4 pages) and DeepForm (75% &gt; 1 pages), OCR-Free models only input the first page and lose much information.</p>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Zero-shot Performance</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p">We test the zero-shot performance of UReader on unseen dataset OCR-VQA. With the same evaluation metrics, UReader outperforms mPLUG-Owl (41.1 vs 28.6) and a recent work UniDoc <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> (41.1 vs 34.5) with the training of layout prediction. The results show that the zero-shot performance of our method on unseen domains is acceptable.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>More Qualitative Results</h2>

<figure id="A3.F6" class="ltx_figure"><img src="/html/2310.05126/assets/x4.png" id="A3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="261" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Text Reading, Visual Question Answering and Image Captioning performance of UReader on natural images from TextVQA. Correct and wrong answers are colored <span id="A3.F6.3.1" class="ltx_text" style="color:#00FF00;">green</span> and <span id="A3.F6.4.2" class="ltx_text" style="color:#FF0000;">red</span>, respectively.</figcaption>
</figure>
<figure id="A3.F7" class="ltx_figure"><img src="/html/2310.05126/assets/x5.png" id="A3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="272" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Text Reading and Visual Question Answering performance of UReader on charts from ChartQA. Correct and wrong answers are colored <span id="A3.F7.3.1" class="ltx_text" style="color:#00FF00;">green</span> and <span id="A3.F7.4.2" class="ltx_text" style="color:#FF0000;">red</span>, respectively.</figcaption>
</figure>
<figure id="A3.F8" class="ltx_figure"><img src="/html/2310.05126/assets/x6.png" id="A3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Key Points Generation and Visual Question Answering performance of UReader on tables from WikiTableQuestions. Correct, wrong and repetitive answers are colored <span id="A3.F8.4.1" class="ltx_text" style="color:#00FF00;">green</span>, <span id="A3.F8.5.2" class="ltx_text" style="color:#FF0000;">red</span> and <span id="A3.F8.6.3" class="ltx_text" style="color:#808080;">gray</span>, respectively.</figcaption>
</figure>
<figure id="A3.F9" class="ltx_figure"><img src="/html/2310.05126/assets/x7.png" id="A3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Text Reading, Key Points Generation and Visual Question Answering performance of UReader on documents from DocVQA and InfoVQA. Correct and wrong answers are colored <span id="A3.F9.3.1" class="ltx_text" style="color:#00FF00;">green</span> and <span id="A3.F9.4.2" class="ltx_text" style="color:#FF0000;">red</span>, respectively.</figcaption>
</figure>
<figure id="A3.F10" class="ltx_figure"><img src="/html/2310.05126/assets/x8.png" id="A3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Text Reading and Visual Question Answering performance of UReader on a webpage screenshot from VisualMRC. Correct and wrong answers are colored <span id="A3.F10.3.1" class="ltx_text" style="color:#00FF00;">green</span> and <span id="A3.F10.4.2" class="ltx_text" style="color:#FF0000;">red</span>, respectively.</figcaption>
</figure>
<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Downstream Results</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">More qualitative results on natural images, charts, tables, documents and webpage screenshots are shown in Figure <a href="#A3.F6" title="Figure 6 ‣ Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>-<a href="#A3.F10" title="Figure 10 ‣ Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div id="A3.SS1.p2" class="ltx_para">
<p id="A3.SS1.p2.1" class="ltx_p"><a href="#A3.F10" title="In Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a> show a sample of Text Reading and Visual Question Answering about a webpage screenshot from VisualMRC. As mentioned in <a href="#S5.SS5" title="5.5 Qualitative Results ‣ 5 Experiments ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.5</span></a>, when given an instruction about reading all texts in the image, UReader can read the beginning texts but sometimes is easy to continue to generate vision-irrelevant texts. With appropriate instructions, UReader could indeed recognize texts in other regions, such as ‘exercise increases cellular recycling’. Therefore, the hallucination problem during text reading is not because UReader cannot recognize texts, but the generating manner of LLM decoder. When beginning texts are read from the image, the decoder may generate the following texts according to the closer text context rather than the image.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Open-domain Results</h3>

<figure id="A3.F11" class="ltx_figure"><img src="/html/2310.05126/assets/x9.png" id="A3.F11.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="415" height="611" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Comparsion with mPLUG-Owl on open-domain Visually-situated Language Understanding. The key words in the answers and the key regions in the images are annotated with the same color. The incorrect response of UReader is colored <span id="A3.F11.2.1" class="ltx_text" style="color:#FF0000;">red</span>.</figcaption>
</figure>
<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">We present open-domain examples in <a href="#A3.F11" title="In C.2 Open-domain Results ‣ Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>. We use randomly collected images and freely ask questions to the model based on the content of these images. The original mPLUG-Owl is used for comparison.</p>
</div>
<div id="A3.SS2.p2" class="ltx_para">
<p id="A3.SS2.p2.1" class="ltx_p">In <a href="#A3.F11" title="In C.2 Open-domain Results ‣ Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a> (a), UReader is able to accurately recognize and answer questions about the small text in natural images ("Name of passenger" and "MORRIS/KARLA"). In contrast, mPLUG-Owl does not respond with the name in the first round and gives an incorrect answer even with a prompt in the second round.</p>
</div>
<div id="A3.SS2.p3" class="ltx_para">
<p id="A3.SS2.p3.1" class="ltx_p">In <a href="#A3.F11" title="In C.2 Open-domain Results ‣ Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a> (b), we raise a query consisting of two cascaded questions, which requires the model to simultaneously understand the spatial position of the non-textual objects referred to by the query and locate the corresponding fields. It can be seen that the UReader completes this task well, while mPLUG-Owl answers incorrectly in both object recognition and price extraction.</p>
</div>
<div id="A3.SS2.p4" class="ltx_para">
<p id="A3.SS2.p4.1" class="ltx_p">In <a href="#A3.F11" title="In C.2 Open-domain Results ‣ Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a> (c), we conduct multi-turn conversions with UReader on a screenshot. The questions included references to the history of the conversation. Some questions also require a certain amount of common sense. For example, the time of account creation is equivalent to the time of joining Twitter, and the inactive state of the Follow button indicates that the user has not followed Iron Musk. UReader answers these questions well. mPLUG-Owl can correctly recognize the Iron Mush, but is it prone to generating content that is unrelated to the image, leading to some erroneous statements.</p>
</div>
<div id="A3.SS2.p5" class="ltx_para">
<p id="A3.SS2.p5.1" class="ltx_p">In <a href="#A3.F11" title="In C.2 Open-domain Results ‣ Appendix C More Qualitative Results ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a> (d), we ask the UReader about the price and its components based on an image consisting of multiple forms. Although UReader wrongly includes the header in the answer and does not list the prices for each component, we notice that it proactively filters out the components with a price of $0, making the answer more consistent with the user’s intention. It indicates that UReader can find the form related to the question and comprehensively understand the meaning of each field in the form. In contrast, mPLUG-Owl generates responses that are full of illusions due to the loss of textual information in the image.</p>
</div>
<div id="A3.SS2.p6" class="ltx_para">
<p id="A3.SS2.p6.1" class="ltx_p">These results reveal that UReader maintains some interactive ability of MLMM in the open domain and shows stronger visually-situated language understanding ability.</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Instruction Templates</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">The instruction templates of the auxiliary Text Reading and Key Points Generation tasks are shown in <a href="#A4.T5" title="In Appendix D Instruction Templates ‣ UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="A4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Instructuion templates used for text reading from the beginning, continue reading and key points generation tasks. The complete instruction for continuing reading is a random combination of a prompt from part A and another one from part B.</figcaption>
<table id="A4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T5.1.1.1" class="ltx_tr">
<td id="A4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A4.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="A4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A4.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Part</span></td>
<td id="A4.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A4.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">Instruction Template</span></td>
</tr>
<tr id="A4.T5.1.2.2" class="ltx_tr">
<td id="A4.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="17"><span id="A4.T5.1.2.2.1.1" class="ltx_text">text reading from the beginning</span></td>
<td id="A4.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="17"><span id="A4.T5.1.2.2.2.1" class="ltx_text">-</span></td>
<td id="A4.T5.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">&lt;Image&gt;Human: what words are in the image? AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.3.3" class="ltx_tr">
<td id="A4.T5.1.3.3.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: what texts are in the picture? AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.4.4" class="ltx_tr">
<td id="A4.T5.1.4.4.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: what does the image read? AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.5.5" class="ltx_tr">
<td id="A4.T5.1.5.5.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: what does the picture say? AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.6.6" class="ltx_tr">
<td id="A4.T5.1.6.6.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: what is written in the image? AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.7.7" class="ltx_tr">
<td id="A4.T5.1.7.7.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: list the words in the image. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.8.8" class="ltx_tr">
<td id="A4.T5.1.8.8.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: list the texts in the picture. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.9.9" class="ltx_tr">
<td id="A4.T5.1.9.9.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Recognize text in the image. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.10.10" class="ltx_tr">
<td id="A4.T5.1.10.10.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Identify text in the picture. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.11.11" class="ltx_tr">
<td id="A4.T5.1.11.11.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Deciphering written content in the photo. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.12.12" class="ltx_tr">
<td id="A4.T5.1.12.12.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Extract words from the graphic. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.13.13" class="ltx_tr">
<td id="A4.T5.1.13.13.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Parse text from imagery. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.14.14" class="ltx_tr">
<td id="A4.T5.1.14.14.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Read written language in the visuals. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.15.15" class="ltx_tr">
<td id="A4.T5.1.15.15.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Decode text from the snapshot. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.16.16" class="ltx_tr">
<td id="A4.T5.1.16.16.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Translate text in the picture. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.17.17" class="ltx_tr">
<td id="A4.T5.1.17.17.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Retrieve written information from the image. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.18.18" class="ltx_tr">
<td id="A4.T5.1.18.18.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Detect words in the photograph. AI: {all texts}.</td>
</tr>
<tr id="A4.T5.1.19.19" class="ltx_tr">
<td id="A4.T5.1.19.19.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="15"><span id="A4.T5.1.19.19.1.1" class="ltx_text">continue reading</span></td>
<td id="A4.T5.1.19.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="11"><span id="A4.T5.1.19.19.2.1" class="ltx_text">A</span></td>
<td id="A4.T5.1.19.19.3" class="ltx_td ltx_align_left ltx_border_t">&lt;Image&gt;Human: The picture reads {left texts}.</td>
</tr>
<tr id="A4.T5.1.20.20" class="ltx_tr">
<td id="A4.T5.1.20.20.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: The image says {left texts}.</td>
</tr>
<tr id="A4.T5.1.21.21" class="ltx_tr">
<td id="A4.T5.1.21.21.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: There are words {left texts} in the image.</td>
</tr>
<tr id="A4.T5.1.22.22" class="ltx_tr">
<td id="A4.T5.1.22.22.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Words {left texts} are in the picture.</td>
</tr>
<tr id="A4.T5.1.23.23" class="ltx_tr">
<td id="A4.T5.1.23.23.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: The texts in this image read {left texts}.</td>
</tr>
<tr id="A4.T5.1.24.24" class="ltx_tr">
<td id="A4.T5.1.24.24.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: The words on this picture are {left texts}.</td>
</tr>
<tr id="A4.T5.1.25.25" class="ltx_tr">
<td id="A4.T5.1.25.25.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: The script depicted in this image reads {left texts}.</td>
</tr>
<tr id="A4.T5.1.26.26" class="ltx_tr">
<td id="A4.T5.1.26.26.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: The writing on this visual representation states {left texts}.</td>
</tr>
<tr id="A4.T5.1.27.27" class="ltx_tr">
<td id="A4.T5.1.27.27.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: The content presented in this diagram states {left texts}.</td>
</tr>
<tr id="A4.T5.1.28.28" class="ltx_tr">
<td id="A4.T5.1.28.28.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: The language used in this photograph says {left texts}.</td>
</tr>
<tr id="A4.T5.1.29.29" class="ltx_tr">
<td id="A4.T5.1.29.29.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: The inscription on this picture explain {left texts}.</td>
</tr>
<tr id="A4.T5.1.30.30" class="ltx_tr">
<td id="A4.T5.1.30.30.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="A4.T5.1.30.30.1.1" class="ltx_text">B</span></td>
<td id="A4.T5.1.30.30.2" class="ltx_td ltx_align_left ltx_border_t">Continue reading the text. AI: {right texts}.</td>
</tr>
<tr id="A4.T5.1.31.31" class="ltx_tr">
<td id="A4.T5.1.31.31.1" class="ltx_td ltx_align_left">Read the following text. AI: {right texts}.</td>
</tr>
<tr id="A4.T5.1.32.32" class="ltx_tr">
<td id="A4.T5.1.32.32.1" class="ltx_td ltx_align_left">Read the text behind. AI: {right texts}.</td>
</tr>
<tr id="A4.T5.1.33.33" class="ltx_tr">
<td id="A4.T5.1.33.33.1" class="ltx_td ltx_align_left">What is the following text? AI: {right texts}.</td>
</tr>
<tr id="A4.T5.1.34.34" class="ltx_tr">
<td id="A4.T5.1.34.34.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="10"><span id="A4.T5.1.34.34.1.1" class="ltx_text">key points generation</span></td>
<td id="A4.T5.1.34.34.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="10"><span id="A4.T5.1.34.34.2.1" class="ltx_text">-</span></td>
<td id="A4.T5.1.34.34.3" class="ltx_td ltx_align_left ltx_border_t">&lt;Image&gt;Human: Identify some key points in this picture. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.35.35" class="ltx_tr">
<td id="A4.T5.1.35.35.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Point out several critical features in this image. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.36.36" class="ltx_tr">
<td id="A4.T5.1.36.36.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Highlight a few significant elements in this photo. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.37.37" class="ltx_tr">
<td id="A4.T5.1.37.37.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Give some essential details in this illustration. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.38.38" class="ltx_tr">
<td id="A4.T5.1.38.38.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Draw attention to some important aspects in this diagram. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.39.39" class="ltx_tr">
<td id="A4.T5.1.39.39.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Mention a couple of crucial points in this snapshot. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.40.40" class="ltx_tr">
<td id="A4.T5.1.40.40.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Indicate a few pertinent items in this graphic. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.41.41" class="ltx_tr">
<td id="A4.T5.1.41.41.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Outline some significant characteristics in this image. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.42.42" class="ltx_tr">
<td id="A4.T5.1.42.42.1" class="ltx_td ltx_align_left">&lt;Image&gt;Human: Specify some key components in this picture. AI: {key points}.</td>
</tr>
<tr id="A4.T5.1.43.43" class="ltx_tr">
<td id="A4.T5.1.43.43.1" class="ltx_td ltx_align_left ltx_border_bb">&lt;Image&gt;Human: List a handful of essential elements in this visual. AI: {key points}.</td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.05125" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.05126" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.05126">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.05126" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.05127" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 02:14:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
