<!DOCTYPE html>
<html lang="en" prefix="dcterms: http://purl.org/dc/terms/">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data</title>
<!--Generated on Sun Aug 25 13:28:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.13833v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S1" title="In Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2" title="In Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS1" title="In 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS1.SSS1" title="In 2.1 Benchmarks ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Clinical Case Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS1.SSS2" title="In 2.1 Benchmarks ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>MeDiSumQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS1.SSS3" title="In 2.1 Benchmarks ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>MeDiSumCode</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS1.SSS4" title="In 2.1 Benchmarks ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>MedNLI</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS1.SSS5" title="In 2.1 Benchmarks ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.5 </span>MeQSum</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS1.SSS6" title="In 2.1 Benchmarks ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.6 </span>ProblemSummary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS1.SSS7" title="In 2.1 Benchmarks ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.7 </span>LongHealth</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS2" title="In 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Models Evaluated</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS2.SSS1" title="In 2.2 Models Evaluated ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Generalist Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS2.SSS2" title="In 2.2 Models Evaluated ‣ 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Biomedical models based on Llama</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S2.SS3" title="In 2 Methods ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Evaluation Procedure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3" title="In Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.SS1" title="In 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Clinical Case Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.SS2" title="In 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>MedNLI Task</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.SS3" title="In 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>ProblemSummary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.SS4" title="In 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>LongHealth</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.SS5" title="In 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>MeDiSumQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.SS6" title="In 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>MeDiSumQA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S4" title="In Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id4.1.id1">Felix J. Dorfner</span> <sup class="ltx_sup" id="id5.2.id2">*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Radiology, Charité - Universitätsmedizin Berlin corporate member of Freie Universität Berlin and Humboldt Universität zu Berlin, Hindenburgdamm 30, 12203 Berlin, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital and Harvard Medical School, 149 Thirteenth St, Charlestown, MA 02129, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amin Dada
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Institute for AI in Medicine (IKIM), University Hospital Essen (AöR), Essen, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felix Busch
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine and Health, Klinikum rechts der Isar, TUM University Hospital, Ismaninger Strasse 22, 81675, Munich, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianyu Han
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Diagnostic and Interventional Radiology, University Hospital Aachen, Pauwelsstr. 30, 52074 Aachen, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Truhn
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Diagnostic and Interventional Radiology, University Hospital Aachen, Pauwelsstr. 30, 52074 Aachen, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jens Kleesiek
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Institute for AI in Medicine (IKIM), University Hospital Essen (AöR), Essen, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">Cancer Research Center Cologne Essen (CCCE), West German Cancer Center Essen, University Hospital Essen (AöR), Essen, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">German Cancer Consortium (DKTK, Partner site Essen), Heidelberg, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">Department of Physics, TU Dortmund, Dortmund, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Madhumita Sushil
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Bakar Computational Health Sciences Institute, University of California, San Francisco, San Francisco
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jacqueline Lammert
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Gynecology and Center for Hereditary Breast and Ovarian Cancer, Technical University of Munich, School of Medicine and Health, Klinikum rechts der Isar, TUM University Hospital, Ismaninger Strasse 22, 81675, Munich, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marcus R. Makowski
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine and Health, Klinikum rechts der Isar, TUM University Hospital, Ismaninger Strasse 22, 81675, Munich, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id1.1.1">Lisa C. Adams<sup class="ltx_sup" id="id1.1.1.1"><math alttext="\ddagger" class="ltx_Math" display="inline" id="id1.1.1.1.m1.1"><semantics id="id1.1.1.1.m1.1a"><mo id="id1.1.1.1.m1.1.1" xref="id1.1.1.1.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.1.m1.1b"><ci id="id1.1.1.1.m1.1.1.cmml" xref="id1.1.1.1.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.1.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.1.m1.1d">‡</annotation></semantics></math></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine and Health, Klinikum rechts der Isar, TUM University Hospital, Ismaninger Strasse 22, 81675, Munich, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id3.2.2">Keno K. Bressem<sup class="ltx_sup" id="id2.1.1.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="id2.1.1.1.m1.1"><semantics id="id2.1.1.1.m1.1a"><mo id="id2.1.1.1.m1.1.1" xref="id2.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id2.1.1.1.m1.1b"><ci id="id2.1.1.1.m1.1.1.cmml" xref="id2.1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="id2.1.1.1.m1.1d">†</annotation></semantics></math></sup><sup class="ltx_sup" id="id3.2.2.2"><math alttext="\ddagger" class="ltx_Math" display="inline" id="id3.2.2.2.m1.1"><semantics id="id3.2.2.2.m1.1a"><mo id="id3.2.2.2.m1.1.1" xref="id3.2.2.2.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id3.2.2.2.m1.1b"><ci id="id3.2.2.2.m1.1.1.cmml" xref="id3.2.2.2.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.2.2.2.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="id3.2.2.2.m1.1d">‡</annotation></semantics></math></sup></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine and Health, Klinikum rechts der Isar, TUM University Hospital, Ismaninger Strasse 22, 81675, Munich, Germany
</span>
<span class="ltx_contact ltx_role_affiliation">Department of Cardiovascular Radiology and Nuclear Medicine, Technical University of Munich, School of Medicine and Health, German Heart Center, TUM University Hospital, Lazarethstr. 36, 80636, Munich, Germany
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Large language models (LLMs) have shown potential in biomedical applications, leading to efforts to fine-tune them on domain-specific data. However, the effectiveness of this approach remains unclear. This study evaluates the performance of biomedically fine-tuned LLMs against their general-purpose counterparts on a variety of clinical tasks. We evaluated their performance on clinical case challenges from the New England Journal of Medicine (NEJM) and the Journal of the American Medical Association (JAMA) and on several clinical tasks (e.g., information extraction, document summarization, and clinical coding). Using benchmarks specifically chosen to be likely outside the fine-tuning datasets of biomedical models, we found that biomedical LLMs mostly perform inferior to their general-purpose counterparts, especially on tasks not focused on medical knowledge. While larger models showed similar performance on case tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA cases), smaller biomedical models showed more pronounced underperformance (e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases). Similar trends were observed across the CLUE (Clinical Language Understanding Evaluation) benchmark tasks, with general-purpose models often performing better on text generation, question answering, and coding tasks. Our results suggest that fine-tuning LLMs to biomedical data may not provide the expected benefits and may potentially lead to reduced performance, challenging prevailing assumptions about domain-specific adaptation of LLMs and highlighting the need for more rigorous evaluation frameworks in healthcare AI. Alternative approaches, such as retrieval-augmented generation, may be more effective in enhancing the biomedical capabilities of LLMs without compromising their general knowledge.</p>
<p class="ltx_p" id="id7.id2"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="id7.id2.1">Keywords</em> Large Language Models, Radiology, Natural Language Processing</p>
</div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.2"><sup class="ltx_sup" id="p1.2.3">*</sup>First author.
<br class="ltx_break"/><sup class="ltx_sup" id="p1.1.1"><math alttext="\dagger" class="ltx_Math" display="inline" id="p1.1.1.m1.1"><semantics id="p1.1.1.m1.1a"><mo id="p1.1.1.m1.1.1" xref="p1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="p1.1.1.m1.1b"><ci id="p1.1.1.m1.1.1.cmml" xref="p1.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="p1.1.1.m1.1d">†</annotation></semantics></math></sup>Corresponding author: keno.bressem@tum.de 
<br class="ltx_break"/><sup class="ltx_sup" id="p1.2.2"><math alttext="\ddagger" class="ltx_Math" display="inline" id="p1.2.2.m1.1"><semantics id="p1.2.2.m1.1a"><mo id="p1.2.2.m1.1.1" xref="p1.2.2.m1.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="p1.2.2.m1.1b"><ci id="p1.2.2.m1.1.1.cmml" xref="p1.2.2.m1.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.2.m1.1c">\ddagger</annotation><annotation encoding="application/x-llamapun" id="p1.2.2.m1.1d">‡</annotation></semantics></math></sup>These authors contributed equally as last authors.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) have shown remarkable potential for various applications, including in the biomedical domain.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib3" title="">3</a>]</cite> These models can serve as knowledge sources, aid in information retrieval from patient notes, assist with data structuring or coding, and support patient anamnesis. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib3" title="">3</a>]</cite> To enhance LLMs’ performance on domain-specific tasks, several initiatives have focused on fine-tuning these models using biomedical data.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib9" title="">9</a>]</cite>
Recent approaches include BioMistral-7b,<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib6" title="">6</a>]</cite> based on Mistral 7b,<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib10" title="">10</a>]</cite> and OpenBioLLM, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib5" title="">5</a>]</cite> which uses the Llama3 (Large Language Model Meta AI) models as a foundation.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib11" title="">11</a>]</cite> These efforts aim to create specialized models for biomedical applications.
However, assessing the performance of fine-tuned models presents challenges. While using exam questions, such as those from the United States Medical Licensing Examination (USMLE), is common, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib12" title="">12</a>]</cite> this approach may not accurately reflect a model’s performance in real clinical practice. Additionally, the prolonged availability of these questions online risks data leakage and benchmark corruption.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib13" title="">13</a>]</cite>
The latest general-purpose LLMs, like Llama 3 and Mistral, are trained on vast amounts of web data, likely including substantial medical information. This raises questions about the added value of fine-tuning, with limited availability of domain-specific data, research groups may struggle to introduce novel information not already present in the training data of large AI companies. Consequently, biomedical LLMs fine-tuned on similar web-based data might face issues such as redundant learning or even performance degradation due to catastrophic forgetting. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib14" title="">14</a>]</cite>
To address these concerns, we conducted a comparative study of recent biomedical LLMs and their general-purpose baseline models. Our evaluation used data from recent benchmarks, specifically chosen to likely be outside the fine-tuning process of the biomedical models, ensuring a fair assessment. Given the domain-specific nature of this data, we hypothesized that the biomedical models outperform their general-purpose counterparts.
This study aims to provide insights into the effectiveness of domain-specific fine-tuning for biomedical LLMs and to contribute to the ongoing discussion about optimal strategies for adapting LLMs to specialized fields.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The benchmarks used in this study were carefully selected to represent a wide range of clinical tasks while ensuring they were likely outside the fine-tuning datasets of biomedical models. We chose recently published case vignettes and newly developed benchmarks to minimize the risk of data contamination. The biomedical and general-purpose models were selected to represent the state-of-the-art in both categories, covering different model sizes and architectures to ensure a comprehensive comparison.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Benchmarks</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Clinical Case Challenges</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">We evaluated the performance of biomedical and general-purpose LLMs using clinical challenges from two medical journals: the New England Journal of Medicine (NEJM) and the Journal of the American Medical Association (JAMA). These case vignettes represent real-world clinical scenarios and cover a wide range of medical specialties and conditions. The NEJM dataset consists of 347 questions, while the JAMA dataset contains 140 questions. These case vignettes, presented with multiple-choice answers, cover a wide range of medical knowledge and provide a comprehensive assessment of the models’ performance on unseen medical data. Models are evaluated by accuracy of correctly answered questions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>MeDiSumQA</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">This task involves answering questions based on discharge summaries from the MIMIC-IV database.16 It tests the model’s ability to extract relevant information from lengthy clinical documents and provide accurate, patient-friendly responses. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib15" title="">15</a>]</cite></p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>MeDiSumCode</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">This task evaluates the model’s capability to assign appropriate ICD-10 codes to diagnoses and procedures mentioned in discharge summaries. The task was introduced together with MeDiSumQA as part of the CLUE benchmark and requires both accurate information extraction and a deep understanding of medical coding systems. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib15" title="">15</a>]</cite></p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>MedNLI</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS4.p1">
<p class="ltx_p" id="S2.SS1.SSS4.p1.1">Based on the MIMIC-III dataset,<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib16" title="">16</a>]</cite> this natural language inference task assesses the model’s ability to determine the logical relationship between a premise (a sentence from a clinical note) and a hypothesis.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib17" title="">17</a>]</cite></p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>MeQSum</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS5.p1">
<p class="ltx_p" id="S2.SS1.SSS5.p1.1">This task involves summarizing consumer health queries, testing the model’s ability to understand lay language and reformulate it into concise, medically sound queries.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib18" title="">18</a>]</cite></p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.6 </span>ProblemSummary</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS6.p1">
<p class="ltx_p" id="S2.SS1.SSS6.p1.1">Using clinical notes organized according to the SOAP (Subjective, Objective, Assessment, Plan) principle, this task requires models to predict a patient’s current health problems based on the Subjective and Assessment sections.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib19" title="">19</a>]</cite></p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.7 </span>LongHealth</h4>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS7.p1">
<p class="ltx_p" id="S2.SS1.SSS7.p1.1">This task uses 20 fictional patient records to test the model’s ability to handle long documents, answer questions about them, and identify when information is not available in the given context.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib20" title="">20</a>]</cite> Evaluation is split into three sub-tasks: a) Answering questions about long document. b) Handling increased input length with unrelated documents c) Identifying when information is not available.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS7.p2">
<p class="ltx_p" id="S2.SS1.SSS7.p2.1">MeDiSumQA, MeDiSumCode and LongHealth require model with longer context size, limiting the number of models that can be evaluated on these benchmarks.
For text generation tasks (MeDiSumQA, MeQSum, and ProblemSummary), ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) and BERTScore are used to evaluate the quality and semantic similarity of generated outputs. The MeDiSumCode task is assessed using F1-scores for exact and approximate matches of ICD-10 codes, as well as the ratio of valid codes generated. MedNLI uses accuracy to evaluate the model’s ability to classify relationships between premises and hypotheses. For tasks involving entity recognition and medical concepts (ProblemSummary), F1-scores for UMLS entity extraction are also employed. The LongHealth task primarily uses accuracy across its subtasks to evaluate comprehension and question-answering abilities on long clinical documents. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib15" title="">15</a>]</cite></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Models Evaluated</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Generalist Models</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Llama (Large Language Models by Meta AI) is a series of foundation models developed by Meta AI. We evaluated models based on three generations of Llama. Llama 1, released in 2023, Llama 1 was trained on trillions of tokens from publicly available datasets.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib21" title="">21</a>]</cite> As the base model is no longer publicly accessible, Llama 1 was not evaluated directly. Llama 2, an improved version released later in 2023, Llama 2 features enhanced training on a larger dataset and improvements in model architecture.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib22" title="">22</a>]</cite> Llama 2 models were released as base models as well as finetuned chat models. Llama 3 Is the latest iteration in the Llama series, released in 2024.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib11" title="">11</a>]</cite> Llama 3 incorporates advanced training techniques and the largest training dataset of all Llama models. It was again released as base model and finetuned chat model.
Mistral 7B is an open-source language model developed by Mistral AI, released in 2023, it utilizes novel architectural choices such as grouped-query attention and sliding window attention, allowing for efficient processing of long sequences.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib10" title="">10</a>]</cite> For all base models, the chat/instruction tuned versions, provided by the developers were used (Llama-2-7b-chat-hf, Llama-2-70b-chat-hf, Llama-3-8B-Instruct, Llama-3-70B-Instruct and Mistral-7B-Instruct-v0.2).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Biomedical models based on Llama</h4>
<div class="ltx_para ltx_noindent" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">PMC-Llama-7B, based on Llama 1, was fine-tuned on biomedical literature from PubMed Central.7 MedAlpaca-7B, also Llama 1-based, was fine-tuned on a curated dataset of medical information, including medical wikis and medical flash cards.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib9" title="">9</a>]</cite> Meditron-7B, med42 and ClinicalCamel-70B are models based on Llama 2 with 7B parameters or 70B parameters.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib24" title="">24</a>]</cite> OpenBioLLM-70B and OpenBioLLM-8B are the most recent biomedical LLMs, based on Llama 3. They currently achieve state of the art on USMLE question answering. BioMistral-7B and JSL-MedMNX-7B-SFT are finetuned versions of Mistral-7B using biomedical data.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib6" title="">6</a>]</cite></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Evaluation Procedure</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The evaluation of the models was conducted using a combination of methods tailored to each benchmark. For the clinical case challenges from NEJM and JAMA, we employed a GPU server equipped with four NVIDIA A100 GPUs. All models were evaluated in their version that is available through the Hugging Face transformers library (version 4.40.1). For compatible models, vLLM (version 0.4.2) was used, which provides a parallelized engine around the Hugging Face models for faster inference. All models were tested with identical parameters: temperature set to 0, top p at 0.95, and both frequency and presence penalties at 0. The 70B models were run across all four GPUs using Ray (version 2.21.0), while the smaller models were executed on a single A100 GPU. The models Meditron-7b MedAlpaca 7b, and PMC-Llama-7B did not produce comprehensible outputs in the initial prompt setup. As these models were trained with a fixed system prompt, the order of the prompt parts was changed to provide the actual clinical vignette at the very end. This was deemed necessary to ensure a fair evaluation of the model capabilities. Additionally, for PMC-Llama-7B the developer recommended settings using top-k sampling with k=50 had to be used to obtain valid outputs.
All other benchmark evaluations were performed on an NVIDIA DGX node with 8 A100 GPUs. The Hugging Face Text Generation Inference Toolkit (v2.1.0) was used for model inference. Larger models were distributed across up to four GPUs, while smaller models were loaded onto a single GPU. Whenever the tokenizer provided a chat template, it was used to format the model input. For models where the template was described in the model card, the template was added manually. All models were tested with the Hugging Face default parameters (temperature 1, top p 0.95, and no penalties).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Clinical Case Challenges</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">On the JAMA case challenges, OpenBioLLM-70B achieved the highest accuracy at 66%, followed by Llama-3-70B-Instruct at 65%. JSL-MedMNX-7B-SFT achieved 54% accuracy, slightly higher than its base model Mistral-7B-Instruct-v0.2 with 52%. Notably, Llama-3-8B-Instruct (57%) outperformed its biomedical counterpart OpenBioLLM-8B (18%). Also, the Llama 1 based model MedAlpaca 7b outperformed OpenBioLLM-8B with 22% accuracy. BioMistral-7B (28%) underperformed compared to Mistral-7B-Instruct-v0.2. In contrast med42-70B and JSL-MedMNX-7B-SFT performed better than their respective baseline models.
For the NEJM cases, OpenBioLLM-70B and Llama-3-70B-Instruct both achieved 74% accuracy. Llama-3-8B-Instruct (64%) again outperformed OpenBioLLM-8B (30%). JSL-MedMNX-7B-SFT and Mistral-7B-Instruct-v0.2 performed similarly (47% and 46% respectively). BioMistral-7B (37%) again underperformed compared to its base model. PMC-Llama-7B and Meditron-7b showed poor performance on both datasets. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.T1" title="Table 1 ‣ 3.1 Clinical Case Challenges ‣ 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of model accuracy in the clinical case vignettes. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.F1" title="Figure 1 ‣ 3.6 MeDiSumQA ‣ 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_tag">1</span></a> provide an overview of the performance on all evaluated tasks.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Individual results on the clinical case challenges.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_border_t" id="S3.T1.1.1.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.1.1.2">JAMA Case Challenges</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.1.1.3">NEJM Case Challenges</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.1.1">Llama 1 Models</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.2.2.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.2.2.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.3.1">MedAlpaca 7B</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.3.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.3.2.1">22.1% (15%-29%)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.3.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.3.3.1">17.3% (13%-21%)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.4.1">PMC-Llama-7B</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.4.2">4.3% (1%-8%)</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.4.3">2.3% (1%-4%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.5.5.1.1">Llama 2 Models</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.5.5.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.5.5.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.6.6.1">Llama-2-7b-chat-hf</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.6.6.2">44.3% (36%-53%)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.6.6.3">26.8% (22%-32%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.7.1">Llama-2-70b-chat-hf</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.7.2">45.7% (38%-54%)</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.7.3">43.5% (38%-49%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.8.1">Meditron-7B</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.8.2">12.9% (7%-18%)</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.8.3">4.9% (3%-7%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.9">
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.9.1">ClinicalCamel-70b</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.9.2">49.3% (41%-57%)</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.9.3">38.9% (34%-44%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.10">
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.10.1">Med42-70b</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.10.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.10.2.1">52.9% (44%-61%)</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.10.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.10.3.1">56.5% (51%-61%)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.11.11.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.11.11.1.1">Llama 3 Models</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.11.11.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.11.11.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.12.12.1">Llama-3-8B-Instruct</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.12.12.2">57.1% (49%-65%)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.12.12.3">64.3% (59%-69%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.13">
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.13.1">Llama-3-70B-Instruct</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.13.2">65% (57%-73%)</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.13.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.13.3.1">74.6% (70%-79%)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.14">
<td class="ltx_td ltx_align_left" id="S3.T1.1.14.14.1">OpenBioLLM-8B</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.14.14.2">17.9% (12%-24%)</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.14.14.3">30% (25%-35%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15.15">
<td class="ltx_td ltx_align_left" id="S3.T1.1.15.15.1">OpenBioLLM-70B</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.15.15.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.15.15.2.1">66.4% (59%-74%)</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.15.15.3">74.1% (70%-78%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.16.16">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.16.16.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.16.16.1.1">Mistral -7B Models</span></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.16.16.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T1.1.16.16.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.17.17">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.17.17.1">Mistral-7B-Instruct-v0.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.17.17.2">52.1% (44%-60%)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.17.17.3">46.4% (41%-52%)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.18.18">
<td class="ltx_td ltx_align_left" id="S3.T1.1.18.18.1">JSL-MedMNX-7B-SFT</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.18.18.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.18.18.2.1">53.6% (45%-63%)</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.18.18.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.18.18.3.1">46.7% (41%-52%)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.19.19">
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.19.19.1">Biomistral-7B</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.19.19.2">27.9% (21%-35%)</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.19.19.3">37.2% (32%-42%)</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>MedNLI Task</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">OpenBioLLM-70B achieved the highest accuracy with 80.85%, closely followed by the generalist model Llama-3-70B-Instruct (79.37%) and JSL-MedMNX-7b-SFT (79.3%). Notably, OpenBioLLM-8B (44.93%) underperformed compared to Llama-3-8B-Instruct (74.08%). BioMistral-7B (62.75%) also showed lower accuracy than its base model Mistral-7B-Instruct-v0.2 (69.93%).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>ProblemSummary</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Llama-2-7B-chat-hf achieved the highest BERT F1 score (75.98) on MeQSum, slightly outperforming larger and more recent models. All biomedical LLMs underperformed compared to their respective generalist models across all metrics.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>LongHealth</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Models based on Llama 1 and Llama 2 did not support enough context, to be evaluated on LongHealth, MeDiSumQA and MeDiSumCode. Llama-3-70B-Instruct demonstrated superior performance across all LongHealth tasks. OpenBioLLM-70B showed lower scores than its base model, particularly in Task 3, which evaluates how prone to hallucinating non-existing information models are. JSL-MedMNX-7b-SFT underperformed compared to Mistral-7B-Instruct-v0.2, except for Task 3, where JSL-MedMNX-7b-SFT achieved a higher score than Mistral-7B-Instruct-v0.2. BioMistral-7B showed consistently worse performance compared to Mistral-7B-Instruct-v0.2.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>MeDiSumQA</h3>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Llama-3-70B-Instruct achieved the highest scores across all MeDiSumQA metrics. OpenBioLLM-70B showed comparable but slightly lower performance. BioMistral-7B and JSL-MedMNX-7b-SFT both underperformed compared to Mistral-7B-Instruct-v0.2, with notably lower ROUGE and BERT F1 scores.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>MeDiSumQA</h3>
<div class="ltx_para ltx_noindent" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Llama-3-70B-Instruct significantly outperformed all other models in MeDiSumCode, achieving the highest scores across all metrics. OpenBioLLM-70B showed lower performance than its base model, particularly in EM F1 and AP F1 scores. BioMistral-7B underperformed compared to Mistral-7B-Instruct-v0.2 (68.76), while JSL-MedMNX-7b-SFT (68.48) showed similar performance to its base model in Valid Code Accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">Tables <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.T2" title="Table 2 ‣ 3.6 MeDiSumQA ‣ 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.T3" title="Table 3 ‣ 3.6 MeDiSumQA ‣ 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_tag">3</span></a> show individual metrics of all tasks except clinical case vignettes. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#S3.F1" title="Figure 1 ‣ 3.6 MeDiSumQA ‣ 3 Results ‣ Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of all tasks and models.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results on MedNLI, ProblemSummary and MeQSum.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:411.9pt;height:243.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.6pt,58.7pt) scale(0.673986803995126,0.673986803995126) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1.1">
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.1.1.1"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.3.1">MedNLI</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S3.T2.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.4.1">ProblemSummary</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S3.T2.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.5.1">MeQSum</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2.2">
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.2">Mean score</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.3">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.4">R-L</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.5">R-1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.6">R-2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.2.7">BERT F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.8">UMLS F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.9">R-L</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.10">R-1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.11">R-2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.2.12">BERT F1</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.3.3.1.1">Llama 1 Models</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.3"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.3.4"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.3.5"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.3.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.3.3.7"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.3.8"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.3.9"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.3.10"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.3.11"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.3.3.12"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.4.4.1">MedAlpaca 7B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.2.1">27.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.3.1">22.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.4.1">14.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.5.1">17.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.6.1">5.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.4.4.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.7.1">66.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4.8"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.8.1">18.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4.9"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.9.1">28.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4.10"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.10.1">31.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4.11"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.11.1">14.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.4.4.12"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.4.12.1">69.21</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5.5">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.5.5.1">PMC-Llama-7B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.5.5.2">17.30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.5.5.3">21.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.4">8.35</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.5">10.57</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.6">3.36</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.5.5.7">54.39</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.8">14.21</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.9">6.03</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.10">6.64</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.11">2.11</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.5.5.12">35.35</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.6.6.1.1">Llama 2 Models</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.6.6.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.6.6.3"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.6.6.4"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.6.6.5"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.6.6.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.6.6.7"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.6.6.8"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.6.6.9"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.6.6.10"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.6.6.11"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.6.6.12"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.7.7.1">Llama-2-7b-chat-hf</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.7.7.2">36.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.7.7.3">41.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.7.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.4.1">17.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.7.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.5.1">22.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.7.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.6.1">6.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.7.7.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.7.1">66.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.7.8"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.8.1">21.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.7.9"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.9.1">36.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.7.10"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.10.1">39.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.7.11">18.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.7.7.12"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.7.7.12.1">75.98</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.8.8">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.8.8.1">Llama-2-70b-chat-hf</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.8.8.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.8.8.2.1">42.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.8.8.3">61.69</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.8.4">14.43</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.8.5">19.81</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.8.6">6.14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.8.8.7">65.07</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.8.8">21.37</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.8.9">34.91</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.8.10">38.81</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.8.11"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.8.8.11.1">18.48</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.8.8.12">74.37</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.9.9">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.9.9.1">Meditron-7B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.9.9.2">13.03</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.9.9.3">2.39</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.9.4">11.29</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.9.5">13.43</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.9.6">4.81</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.9.9.7">63.35</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.9.8">15.21</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.9.9">6.83</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.9.10">7.90</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.9.11">2.26</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.9.9.12">43.39</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.10.10">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.10.10.1">ClinicalCamel-70b</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.10.10.2">35.71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.10.10.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.10.10.3.1">64.65</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.10.4">8.62</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.10.5">10.83</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.10.6">3.71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.10.10.7">59.94</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.10.8">12.34</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.10.9">16.96</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.10.10">18.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.10.11">8.49</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.10.10.12">49.3</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.11.11.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.11.11.1.1">Llama 3 Models</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.11.11.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.11.11.3"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.11.11.4"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.11.11.5"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.11.11.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.11.11.7"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.11.11.8"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.11.11.9"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.11.11.10"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.11.11.11"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.11.11.12"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.12.12.1">Llama-3-8B-Instruct</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.12.12.2">48.37</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.12.12.3">74.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.12.12.4">22.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.12.12.5">28.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.12.12.6">9.87</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.12.12.7">71.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.12.12.8">25.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.12.12.9">32.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.12.12.10">36.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.12.12.11">16.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.12.12.12">72.74</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.13.13.1">Llama-3-70B-Instruct</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.13.13.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.2.1">52.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.13.13.3">79.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.13.13.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.4.1">25.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.13.13.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.5.1">33.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.13.13.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.6.1">13.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.13.13.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.7.1">73.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.13.13.8"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.8.1">29.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.13.13.9"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.9.1">36.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.13.13.10"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.10.1">40.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.13.13.11"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.11.1">19.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.13.13.12"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.13.13.12.1">75.74</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.14.14">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.14.14.1">OpenBioLLM-8B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.14.14.2">33.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.14.14.3">44.93</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.14.4">10.82</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.14.5">13.62</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.14.6">4.03</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.14.14.7">64.14</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.14.8">15.67</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.14.9">26.21</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.14.10">29.41</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.14.11">14.03</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.14.14.12">62.39</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.15.15">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.15.15.1">OpenBioLLM-70B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.15.15.2">47.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.15.15.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.15.15.3.1">80.85</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.15.4">12.10</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.15.5">16.67</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.15.6">5.58</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.15.15.7">66.51</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.15.8">17.74</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.15.9">30.72</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.15.10">34.31</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.15.11">15.55</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.15.15.12">71.99</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.16.16">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.16.16.1">Med 42</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.16.16.2">47.29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.16.16.3">75.42</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.16.4">19.73</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.16.5">25.14</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.16.6">7.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.16.16.7">67.84</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.16.8">26.36</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.16.9">29.34</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.16.10">32.07</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.16.11">15.96</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.16.16.12">70.96</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.17.17">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.17.17.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.17.17.1.1">Mistral-7B Models</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.17.17.2"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.17.17.3"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.17.17.4"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.17.17.5"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.17.17.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T2.1.1.17.17.7"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.17.17.8"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.17.17.9"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.17.17.10"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.17.17.11"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.1.17.17.12"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.18.18">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.18.18.1">Mistral-7B-Instruct-v0.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.18.18.2">46.45</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.18.18.3">69.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.18.18.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.18.18.4.1">19.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.18.18.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.18.18.5.1">25.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.18.18.6"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.18.18.6.1">8.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.18.18.7"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.18.18.7.1">69.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.18.18.8">22.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.18.18.9"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.18.18.9.1">33.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.18.18.10"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.18.18.10.1">37.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.18.18.11"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.18.18.11.1">16.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.18.18.12"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.18.18.12.1">73.47</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.19.19">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.19.19.1">JSL-MedMNX-7B-SFT</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.19.19.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.19.19.2.1">49.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.19.19.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.19.19.3.1">79.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.19.4">19.49</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.19.5">25.57</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.19.6">8.21</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.19.19.7">67.62</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.19.8"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.19.19.8.1">25.72</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.19.9">32.49</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.19.10">36.58</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.19.11">16.25</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.19.19.12">73.01</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.20.20">
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T2.1.1.20.20.1">Biomistral-7B</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T2.1.1.20.20.2">40.57</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T2.1.1.20.20.3">62.75</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.20.20.4">16.90</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.20.20.5">20.89</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.20.20.6">8.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T2.1.1.20.20.7">59.03</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.20.20.8">20.12</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.20.20.9">25.89</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.20.20.10">28.46</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.20.20.11">13.31</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.1.20.20.12">67.93</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results on MedNLI, ProblemSummary and MeQSum.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:411.9pt;height:119.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-137.9pt,39.7pt) scale(0.59903594626577,0.59903594626577) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.2"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S3.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.3.1">LongHealth</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S3.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.4.1">MeDiSumQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S3.T3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.5.1">MeDiSumCode</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.2">Mean score</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.3">Task 1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.4">Task 2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.5">Task 3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.6">R-L</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.7">R-1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.8">R-2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.9">BERT F1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.10">UMLS F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.11">EM F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.12">AP F1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2.13">Valid Code Acc</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.3.3.1.1">Llama 3 Models</span></th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.2"></th>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.3"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.5"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.6"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.7"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.8"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T3.1.1.3.3.10"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.11"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.12"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.1.1.3.3.13"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.4.4.1">Llama-3-8B-Instruct</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.2">40.61</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.3">68.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.4">66.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.5">56.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.6">22.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.7">28.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.8">9.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.9">68.62</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.4.10">22.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.11">3.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.12">17.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4.4.13">61.93</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.1.5.5.1">Llama-3-70B-Instruct</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.5.5.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.2.1">56.00</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.3.1">81.65</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.4.1">77.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.5.5.5"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.5.1">91.70</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.6"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.6.1">26.20</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.7"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.7.1">32.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.8"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.8.1">11.93</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.9"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.9.1">70.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.5.5.10"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.10.1">25.78</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.11"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.11.1">19.65</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.12"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.12.1">39.20</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.5.13"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.5.13.1">93.94</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.1.6.6.1">OpenBioLLM-8B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.6.6.2">25.44</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.3">37.55</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.4">41.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.6.6.5">1.55</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.6">22.89</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.7">27.95</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.8">10.45</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.9">68.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.6.6.10">22.15</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.11">0.84</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.12">4.84</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6.13">51.16</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.1.7.7.1">OpenBioLLM-70B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.7.7.2">45.56</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.3">80.2</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.4">75.60</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.7.7.5">62.90</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.6">21.85</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.7">27.8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.8">9.54</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.9">68.43</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.7.7.10">22.47</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.11">7.37</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.12">20.24</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.7.13">73.65</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.8.8.1.1">Mistral-7B Models</span></th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.8.8.2"></th>
<td class="ltx_td" id="S3.T3.1.1.8.8.3"></td>
<td class="ltx_td" id="S3.T3.1.1.8.8.4"></td>
<td class="ltx_td ltx_border_r" id="S3.T3.1.1.8.8.5"></td>
<td class="ltx_td" id="S3.T3.1.1.8.8.6"></td>
<td class="ltx_td" id="S3.T3.1.1.8.8.7"></td>
<td class="ltx_td" id="S3.T3.1.1.8.8.8"></td>
<td class="ltx_td" id="S3.T3.1.1.8.8.9"></td>
<td class="ltx_td ltx_border_r" id="S3.T3.1.1.8.8.10"></td>
<td class="ltx_td" id="S3.T3.1.1.8.8.11"></td>
<td class="ltx_td" id="S3.T3.1.1.8.8.12"></td>
<td class="ltx_td" id="S3.T3.1.1.8.8.13"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.9.9.1">Mistral-7B-Instruct-v0.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.2.1">38.94</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.3.1">67.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.4.1">62.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.9.9.5">42.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.6"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.6.1">21.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.7"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.7.1">27.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.8"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.8.1">9.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.9"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.9.1">68.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.9.9.10"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.10.1">20.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.11"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.11.1">3.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.12"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.12.1">18.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.9.13"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.9.13.1">68.76</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.1.1.10.10.1">JSL-MedMNX-7B-SFT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.1.1.10.10.2">34.03</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.3">53.15</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.4">40.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.10.10.5"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.10.10.5.1">52.25</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.6">15.93</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.7">20.89</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.8">6.7</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.9">65.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.10.10.10">17.33</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.11">2.82</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.12">13.3</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.10.13">68.48</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S3.T3.1.1.11.11.1">Biomistral-7B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S3.T3.1.1.11.11.2">23.83</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.3">38.05</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.4">34.25</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T3.1.1.11.11.5">7.8</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.6">14.65</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.7">17.81</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.8">5.46</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.9">59.01</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T3.1.1.11.11.10">16.88</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.11">1.67</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.12">9.92</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T3.1.1.11.11.13">54.51</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1058" id="S3.F1.g1" src="x1.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparative performance of the domain specific and general-purpose base models on the two clinical case vignette series. Bars represent the respective metric and error bars represent 95% confidence intervals obtained through bootstrapping.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our comprehensive evaluation of LLMs across clinical tasks has yielded unexpected insights regarding the performance of biomedically fine-tuned models. Contrary to our initial hypothesis, biomedical models generally underperformed compared to their general-purpose counterparts across various tasks. This suggests that fine-tuning LLMs on biomedical data may not provide the expected benefits and may even decrease rather than improve performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Several factors may contribute to this underperformance. The superior performance of general-purpose models might stem from their exposure to a more diverse range of topics and linguistic structures during pre-training. This broader knowledge base could enable more flexible reasoning and better generalization to novel tasks. Additionally, the fine-tuning process for biomedical models might inadvertently introduce biases or overly narrow the models’ focus, potentially limiting their ability to integrate broader contextual information crucial for complex clinical reasoning.
Another possibility is overfitting to specific medical datasets used during fine-tuning, leading to reduced generalization capabilities. This could be due to data leakage between training and test sets, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib25" title="">25</a>]</cite> a risk that increases with the growing size of training datasets, making it increasingly challenging for researchers to verify data integrity, especially as some models evaluated (OpenBioLLM or JSL-MedMNX-7b-SFT) do not report their training data. Overfitting can also occur indirectly through repeated evaluation on common test datasets, such as USMLE or MMLU, which may inadvertently select for models that perform well on these specific benchmarks.
A key factor to consider is the potential loss of general knowledge during the fine-tuning process, a phenomenon known as catastrophic forgetting.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib14" title="">14</a>]</cite> Our findings highlight the delicate balance required when adapting general-purpose models to specialized domains without compromising their broad capabilities. Potential evidence supporting this hypothesis comes from the observation that biomedical LLMs that only underwent supervised fine-tuning (medAlpaca-7B, ClinicalCamel-70B, OpenBioLLM-8B/70B)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib23" title="">23</a>]</cite> showed a smaller performance decrease compared to their general-purpose counterparts than models that underwent continued pretraining (BioMistral-7B, Meditron-7B). <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib23" title="">23</a>]</cite></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Interestingly, larger models exhibited smaller performance gaps between biomedical and generalist versions. Given that both were trained with the same amount of data, the overall changes in model weights of larger models might have been smaller, potentially reducing the risk of catastrophic forgetting. These observations suggest that using only fine-tuning, rather than continued pretraining, may be preferable when adapting LLMs for specific domains. However, the data presented in our analysis is insufficient for definitive conclusions, and further research is needed to investigate these phenomena thoroughly.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">The consistent strong performance of Llama-3-70B-Instruct across all benchmarks is particularly noteworthy. As the latest iteration of Llama models, Llama 3 was trained on an unprecedented 15 trillion tokens of data, a sevenfold increase compared to Llama 2 and substantially more than previously believed optimal.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib26" title="">26</a>]</cite> With this vast amount of data, it is highly likely that nearly all freely available biomedical texts on the internet are included in the training data of these general-purpose models, enabling them to inherently capture sufficient medical knowledge. Consequently, unless biomedical LLMs are fine-tuned on novel, previously unavailable data (e.g., copyrighted scientific papers, proprietary hospital data), fine-tuning on publicly accessible biomedical data may not add new knowledge. Instead, it may risk the model forgetting valuable information through continued fine-tuning.
Performance differences between biomedical and general LLMs vary depending on the task. While the performance of OpenBioLLM 70B and Llama3 70B appears to be on par for the clinical vignettes, which have a uniform multiple-choice format, OpenBioLLM performed significantly worse than Llama3 70B on the MeDiSumCode benchmark, which requires in-depth knowledge of the ICD coding system with over 70,000 individual codes. This discrepancy suggests that the benefits of biomedical fine-tuning may be task-dependent, with potentially greater advantages in highly specialized medical tasks that require deep domain knowledge.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">One critical finding of our study is the higher risk of hallucinations observed in biomedical LLMs. The results from LongHealth Task 3, which evaluates hallucination tendencies, raise important concerns about the reliability of LLMs in clinical applications. The superior performance of general-purpose models in this aspect is particularly intriguing and warrants further investigation. Recent work has highlighted the critical nature of this issue in healthcare AI, emphasizing the need for robust strategies to mitigate hallucination risks.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib28" title="">28</a>]</cite></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">These results could have significant implications for the development and application of LLMs in healthcare. They challenge the prevailing assumption that domain-specific fine-tuning is universally beneficial for specialized tasks. Instead, our findings suggest that the relationship between model performance and domain adaptation is more nuanced and complex than previously thought. This complexity may stem from the intricate interplay between a model’s general knowledge and its ability to apply that knowledge in specific contexts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">Interestingly, larger models exhibited smaller performance gaps between biomedical and generalist versions. Given that both were trained with the same amount of data, the overall changes in model weights of larger models might have been smaller, potentially reducing the risk of catastrophic forgetting. These observations suggest that using only fine-tuning, rather than continued pretraining, may be preferable when adapting LLMs for specific domains. However, further research is needed to investigate these observations more thoroughly.</p>
</div>
<section class="ltx_subsection" id="S4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Limitations </h3>
<div class="ltx_para ltx_noindent" id="S4.SSx1.p1">
<p class="ltx_p" id="S4.SSx1.p1.1">Our comparison has limitations. The two sets of case vignettes have been freely available on the web and might thus have partly been included in the training data of recent LLMs, leading to an overestimation of the LLM performance. However, since the domain-specific models are based on the general-purpose LLMs, this would not be an advantage. Rather, the inferior performance of the biomedical LLMs, could be an additional argument for the presence of catastrophic forgetting. Furthermore, while the benchmarks used cover a range of clinical tasks, they may not fully represent the complexity and diversity of real-world clinical scenarios. Specifically, they do not cover detailed medical knowledge such as nuanced diagnostic criteria, extensive patient history considerations, and comprehensive treatment recommendations.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Conclusion</h3>
<div class="ltx_para ltx_noindent" id="S4.SSx2.p1">
<p class="ltx_p" id="S4.SSx2.p1.1">In conclusion, our study challenges prevailing assumptions about the effectiveness of biomedical fine-tuning for LLMs, with potential implications for domain-specific adaptation in general. Rather than continued pre-training or fine-tuning, alternative approaches such as retrieval-augmented generation are worth exploring to enhance the biomedical capabilities of LLMs without compromising their general knowledge. Recent studies have shown promising results for these techniques.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13833v1#bib.bib30" title="">30</a>]</cite>
While biomedical LLMs perform well on widely used benchmarks such as the USMLE or MMLU, our evaluation reveals variable performance across tasks and models. This highlights the need for more rigorous, task-specific evaluation frameworks for healthcare LLMs. These evaluations should focus on clinical support tasks such as text summarization, information retrieval, and data structuring. We believe that using LLMs for these applications could provide more noticeable relief to healthcare workers than using LLMs as (potentially unreliable) knowledge bases.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eriksen et al. [2024]</span>
<span class="ltx_bibblock">
Alexander V. Eriksen, Sören Möller, and Jesper Ryg.

</span>
<span class="ltx_bibblock">Use of gpt-4 to diagnose complex clinical cases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">NEJM AI</em>, 1(1):AIp2300031, 2024.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="https://doi.org/doi:10.1056/AIp2300031" title="">doi:10.1056/AIp2300031</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.nejm.org/doi/abs/10.1056/AIp2300031" title="">https://ai.nejm.org/doi/abs/10.1056/AIp2300031</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. [2022]</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Emma Chen, Oishi Banerjee, and Eric J Topol.

</span>
<span class="ltx_bibblock">Ai in health and medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Nature medicine</em>, 28(1):31–38, 2022.

</span>
<span class="ltx_bibblock">ISSN 1078-8956.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thirunavukarasu et al. [2023]</span>
<span class="ltx_bibblock">
Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting.

</span>
<span class="ltx_bibblock">Large language models in medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Nature medicine</em>, 29(8):1930–1940, 2023.

</span>
<span class="ltx_bibblock">ISSN 1078-8956.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2020]</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.

</span>
<span class="ltx_bibblock">Biobert: a pre-trained biomedical language representation model for biomedical text mining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Bioinformatics</em>, 36(4):1234–1240, 2020.

</span>
<span class="ltx_bibblock">ISSN 1367-4803.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal and Sankarasubbu [2024]</span>
<span class="ltx_bibblock">
Ankit Pal and Malaikannan Sankarasubbu.

</span>
<span class="ltx_bibblock">Openbiollms: Advancing open-source large language models for healthcare and life sciences, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B" title="">https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Labrak et al. [2024]</span>
<span class="ltx_bibblock">
Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour.

</span>
<span class="ltx_bibblock">Biomistral: A collection of open-source pretrained large language models for medical domains.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2402.10373</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2024]</span>
<span class="ltx_bibblock">
Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang.

</span>
<span class="ltx_bibblock">Pmc-llama: toward building open-source language models for medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Journal of the American Medical Informatics Association</em>, page ocae045, 2024.

</span>
<span class="ltx_bibblock">ISSN 1067-5027.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christophe et al. [2024]</span>
<span class="ltx_bibblock">
Clément Christophe, Praveen K Kanithi, Prateek Munjal, Tathagata Raha, Nasir Hayat, Ronnie Rajan, Ahmed Al-Mahrooqi, Avani Gupta, Muhammad Umar Salman, and Gurpreet Gosal.

</span>
<span class="ltx_bibblock">Med42–evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient approaches.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2404.14779</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. [2023]</span>
<span class="ltx_bibblock">
Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno K. Bressem.

</span>
<span class="ltx_bibblock">Medalpaca – an open-source collection of medical conversational ai models and training data, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023]</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, and Lucile Saulnier.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta [2024]</span>
<span class="ltx_bibblock">
AI Meta.

</span>
<span class="ltx_bibblock">Introducing meta llama 3: The most capable openly available llm to date.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Meta AI</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kung et al. [2023]</span>
<span class="ltx_bibblock">
Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, and James Maningo.

</span>
<span class="ltx_bibblock">Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">PLoS digital health</em>, 2(2):e0000198, 2023.

</span>
<span class="ltx_bibblock">ISSN 2767-3170.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golchin and Surdeanu [2023]</span>
<span class="ltx_bibblock">
Shahriar Golchin and Mihai Surdeanu.

</span>
<span class="ltx_bibblock">Time travel in llms: Tracing data contamination in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2308.08493</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick et al. [2017]</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, and Agnieszka Grabska-Barwinska.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the national academy of sciences</em>, 114(13):3521–3526, 2017.

</span>
<span class="ltx_bibblock">ISSN 0027-8424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dada et al. [2024]</span>
<span class="ltx_bibblock">
Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, and Jens Kleesiek.

</span>
<span class="ltx_bibblock">Clue: A clinical language understanding evaluation for llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2404.04067</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. [2016]</span>
<span class="ltx_bibblock">
Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark.

</span>
<span class="ltx_bibblock">Mimic-iii, a freely accessible critical care database.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Scientific data</em>, 3(1):1–9, 2016.

</span>
<span class="ltx_bibblock">ISSN 2052-4463.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romanov and Shivade [2018]</span>
<span class="ltx_bibblock">
Alexey Romanov and Chaitanya Shivade.

</span>
<span class="ltx_bibblock">Lessons from natural language inference in the clinical domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1808.06752</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben Abacha and Demner-Fushman [2019]</span>
<span class="ltx_bibblock">
Asma Ben Abacha and Dina Demner-Fushman.

</span>
<span class="ltx_bibblock">On the summarization of consumer health questions.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 2228–2234, Florence, Italy, July 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1215" title="">10.18653/v1/P19-1215</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1215" title="">https://aclanthology.org/P19-1215</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2022]</span>
<span class="ltx_bibblock">
Yanjun Gao, Dmitriy Dligach, Timothy Miller, Dongfang Xu, Matthew M. M. Churpek, and Majid Afshar.

</span>
<span class="ltx_bibblock">Summarizing patients’ problems from hospital progress notes using pre-trained sequence-to-sequence models.

</span>
<span class="ltx_bibblock">In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 2979–2991, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.coling-1.264" title="">https://aclanthology.org/2022.coling-1.264</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adams et al. [2024]</span>
<span class="ltx_bibblock">
Lisa Adams, Felix Busch, Tianyu Han, Jean-Baptiste Excoffier, Matthieu Ortala, Alexander Löser, Hugo JWL Aerts, Jakob Nikolas Kather, Daniel Truhn, and Keno Bressem.

</span>
<span class="ltx_bibblock">Longhealth: A question answering benchmark with long clinical documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2401.14490</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023a]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, and Faisal Azhar.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023b]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, and Amirkeivan Mohtashami.

</span>
<span class="ltx_bibblock">Meditron-70b: Scaling medical pretraining for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2311.16079</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toma et al. [2023]</span>
<span class="ltx_bibblock">
Augustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G Krishnan, Barry B Rubin, and Bo Wang.

</span>
<span class="ltx_bibblock">Clinical camel: An open expert-level medical language model with dialogue-based knowledge encoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2305.12031</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balloccu et al. [2024]</span>
<span class="ltx_bibblock">
Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondřej Dušek.

</span>
<span class="ltx_bibblock">Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2402.03927</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et al. [2022]</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, and Aidan Clark.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Umapathi et al. [2023]</span>
<span class="ltx_bibblock">
Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu.

</span>
<span class="ltx_bibblock">Med-halt: Medical domain hallucination test for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2307.15343</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmad et al. [2023]</span>
<span class="ltx_bibblock">
Muhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy.

</span>
<span class="ltx_bibblock">Creating trustworthy llms: Dealing with hallucinations in healthcare ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2311.01463</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. [2020]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, and Tim Rocktäschel.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al. [2024]</span>
<span class="ltx_bibblock">
Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang.

</span>
<span class="ltx_bibblock">Benchmarking retrieval-augmented generation for medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2402.13178</em>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div about="" class="ltx_rdf" content="Felix J. Dorfner, Amin Dada, Felix Busch, Marcus R. Makowski, Tianyu Han, Daniel Truhn, Jens Kleesiek, Madhumita Sushil, Jacqueline Lammert, Lisa C. Adams, Keno K. Bressem" property="dcterms:creator"></div>
<div about="" class="ltx_rdf" content="Large Language Models, Radiology, Natural Language Processing" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="Large Language Models, Radiology, Natural Language Processing" property="dcterms:subject"></div>
<div about="" class="ltx_rdf" content="Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data" property="dcterms:title"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Aug 25 13:28:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
