<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.11471] Robust Visual Question Answering: Datasets, Methods, and Future Challenges</title><meta property="og:description" content="Visual question answering requires a system to provide an accurate natural language answer given an image and a natural language question. However, it is widely recognized that previous generic VQA methods often exhibiâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Robust Visual Question Answering: Datasets, Methods, and Future Challenges">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Robust Visual Question Answering: Datasets, Methods, and Future Challenges">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.11471">

<!--Generated on Wed Feb 28 16:52:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Visual question answering,  bias learning,  debiasing,  multi-modality learning,  vision-and-language pre-training.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Robust Visual Question Answering: Datasets, Methods, and Future Challenges</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jie Ma,Â 
Pinghui Wang, Â 
Dechen Kong<sup id="id8.1.id1" class="ltx_sup">â€ </sup>,
Zewei Wang<sup id="id9.2.id2" class="ltx_sup">â€ </sup>,
Jun Liu, Â ,
Hongbin Pei,
and
Junzhou Zhao
</span><span class="ltx_author_notes">
â€ Â denotes the authors contribute equally to the work. Jie Ma, Pinghui Wang, Hongbin Pei, and Junzhou Zhao are with the Ministry of Education of Key Laboratory for Intelligent Networks and Network Security, School of Cyber Science and Engineering, Xiâ€™an Jiaotong University, Xiâ€™an, Shaanxi 710049, China.
Dechen Kong and Zewei Wang are with the Ministry of Education of Key Laboratory for Intelligent Networks and Network Security, School of Automation Science and Engineering, Xiâ€™an Jiaotong University, Xiâ€™an, Shaanxi 710049, China.
Jun Liu is with the Shannxi Provincial Key Laboratory of Big Data Knowledge Engineering, School of Computer Science and Technology, Xiâ€™an Jiaotong University, Xiâ€™an, Shaanxi 710049, China.
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">Visual question answering requires a system to provide an accurate natural language answer given an image and a natural language question. However, it is widely recognized that previous generic VQA methods often exhibit a tendency to memorize biases present in the training data rather than learning proper behaviors, such as grounding images before predicting answers. Therefore, these methods usually achieve high in-distribution but poor out-of-distribution performance. In recent years, various datasets and debiasing methods have been proposed to evaluate and enhance the VQA robustness, respectively. This paper provides the first comprehensive survey focused on this emerging fashion. Specifically, we first provide an overview of the development process of datasets from in-distribution and out-of-distribution perspectives. Then, we examine the evaluation metrics employed by these datasets. Thirdly, we propose a typology that presents the development process, similarities and differences, robustness comparison, and technical features of existing debiasing methods. Furthermore, we analyze and discuss the robustness of representative vision-and-language pre-training models on VQA. Finally, through a thorough review of the available literature and experimental analysis, we discuss the key areas for future research from various viewpoints.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Visual question answering, bias learning, debiasing, multi-modality learning, vision-and-language pre-training.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) aims to build intelligent machines that are able to provide a natural language answer accurately given an image and a natural language question about the imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The goal of VQA, which bridges computer vision and natural language processing, is to teach machines to see and read simultaneously like humans. This task exhibits a multitude of applications, encompassing areas such as providing blind and visually-impaired individuals with information about the surrounding world, facilitating image retrieval in the absence of metadata <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, empowering intelligent virtual assistants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, enabling visual recommendation systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and contributing to autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. For instance, we can use the VQA approach to query â€œIs there a panda in the image?â€ across all candidate images to identify those that contain pandas.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In the last few years, VQA has garnered significant attention in the research community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which can be attributed to two key developments. First, a number of datasets, such as VQA v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> for fine-grained detection and recognition, FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for reasoning based on external knowledge, and GQA for compositional reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, have been built to evaluate the ability of methods from different views. Second, a variety of VQA methods have been proposed, which can be classified into three groups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>: joint embedding, attention mechanism, and external knowledge. Joint embedding-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, in particular, project image and question representations into a common space to predict answers. These methods typically learn coarse-grained multi-modal representations, which may bring noisy or irrelevant information into the prediction stage. To address this issue, attention mechanism-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> usually fuse the representations of questions and images based on the learned importance of objects and words. In the real-world scenario, VQA often requires machines to understand not only image contents but also external prior information that ranges from common sense to encyclopedic knowledge. For instance, considering the question â€œHow many fungal plants are there in the picture?â€, the machine should comprehend the word â€œfungalâ€ and know which plant belongs to this category. To this end, external knowledge-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> link the knowledge extracted from large-scale knowledge bases such as DBpedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, Freebase <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and YAGO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, into multimodality learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2307.11471/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="175" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of generic VQA methods in the in-distribution and out-of-distribution test scenarios. They predict answers by learning strong language bias, such as the connections between critical words â€œwhatâ€, and â€œsportsâ€ in questions and the most frequent answer â€œtennisâ€, rather than grounding images, which results in their high In-Distribution (ID) but poor Out-Of-Distribution (OOD) test performance. The ID situation refers to scenarios where the distribution is similar to that of the corresponding training split. The OOD scenario, on the other hand, applies to cases where the distribution differs or even opposes that of the training split.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, in parallel with the above works, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> found that the aforementioned generic methods tend to memorize statistical regularities or bias in the training data rather than ground images to predict answers. For example, in the middle bar chart of the fourth column of Fig. <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can see that â€œtennisâ€ is the most frequent answer. These methods answer the questions of the second column mainly by exploiting the connections between critical words â€œwhatâ€, and â€œsportsâ€ of the questions and â€œtennisâ€. This will cause these methods to perform well in the In-Distribution (ID) test scenario that has similar answer distributions with the training split, such as the distribution in the middle bar chart of the fourth column, but poorly in the Out-Of-Distribution (OOD) test situation that has different or even reversed answer distributions, such as the distribution in the bottom bar chart. In order to address this issue, a significant body of literature on VQA has emerged in recent years, with a particular focus on eliminating bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and evaluating robustness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The purpose of this paper is to provide a comprehensive and systematic overview of the methods, datasets, and future challenges in the area of robust VQA. To the best of our knowledge, this paper is the first survey on the topic. In Section <a href="#S2" title="2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we establish preliminary concepts for generic and robust VQA. Section <a href="#S3" title="3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> discusses the datasets from various perspectives including constructions, image sources, amounts of images and questions, and focuses. They are classified into two categories based on the ID and OOD settings. Section <a href="#S4" title="4 Evaluations â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reviews the evaluation metrics employed in the mentioned datasets including single and composite metrics. In Section <a href="#S5" title="5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we undertake a critical analysis of debiasing methods, categorizing them into four classes: ensemble learning, data augmentation, self-supervised contrastive learning, and answer re-ranking. Section <a href="#S6" title="6 Vision-and-Language Pre-training Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reviews the development of representative vision-and-language pre-training methods and divides them into four classes based on the relative computational size of text encoders, image encoders, and modality interaction modules. We also discuss the robustness of these methods on the most commonly used VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> (ID) and VQA-CP (OOD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> datasets. Furthermore, in Section <a href="#S7" title="7 Discussions and Future Directions â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we conduct in-depth discussions of future challenges from the perspective of improvements of annotation qualities, ongoing developments in dataset creation, evaluation metric advancements, method robustness, and robustness assessments, drawing on our experimental results and literature overview. Finally, we present our concluding remarks in Section <a href="#S8" title="8 Conclusion â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Preliminaries</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first describe the task formulation of VQA. Then, we briefly introduce the paradigm of current VQA methods. Finally, we define robust VQA methods from the perspective of debiasing.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.8" class="ltx_p"><span id="S2.p2.8.1" class="ltx_text ltx_font_bold">Task Formulation.</span> Given a dataset <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S2.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\mathcal{D}</annotation></semantics></math> consisting of <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">n</annotation></semantics></math> triplets <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="\{(v_{i},q_{i},a_{i})\}_{i=1}^{n}" display="inline"><semantics id="S2.p2.3.m3.1a"><msubsup id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mrow id="S2.p2.3.m3.1.1.1.1.1" xref="S2.p2.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.p2.3.m3.1.1.1.1.1.2" xref="S2.p2.3.m3.1.1.1.1.2.cmml">{</mo><mrow id="S2.p2.3.m3.1.1.1.1.1.1.3" xref="S2.p2.3.m3.1.1.1.1.1.1.4.cmml"><mo stretchy="false" id="S2.p2.3.m3.1.1.1.1.1.1.3.4" xref="S2.p2.3.m3.1.1.1.1.1.1.4.cmml">(</mo><msub id="S2.p2.3.m3.1.1.1.1.1.1.1.1" xref="S2.p2.3.m3.1.1.1.1.1.1.1.1.cmml"><mi id="S2.p2.3.m3.1.1.1.1.1.1.1.1.2" xref="S2.p2.3.m3.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S2.p2.3.m3.1.1.1.1.1.1.1.1.3" xref="S2.p2.3.m3.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.p2.3.m3.1.1.1.1.1.1.3.5" xref="S2.p2.3.m3.1.1.1.1.1.1.4.cmml">,</mo><msub id="S2.p2.3.m3.1.1.1.1.1.1.2.2" xref="S2.p2.3.m3.1.1.1.1.1.1.2.2.cmml"><mi id="S2.p2.3.m3.1.1.1.1.1.1.2.2.2" xref="S2.p2.3.m3.1.1.1.1.1.1.2.2.2.cmml">q</mi><mi id="S2.p2.3.m3.1.1.1.1.1.1.2.2.3" xref="S2.p2.3.m3.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S2.p2.3.m3.1.1.1.1.1.1.3.6" xref="S2.p2.3.m3.1.1.1.1.1.1.4.cmml">,</mo><msub id="S2.p2.3.m3.1.1.1.1.1.1.3.3" xref="S2.p2.3.m3.1.1.1.1.1.1.3.3.cmml"><mi id="S2.p2.3.m3.1.1.1.1.1.1.3.3.2" xref="S2.p2.3.m3.1.1.1.1.1.1.3.3.2.cmml">a</mi><mi id="S2.p2.3.m3.1.1.1.1.1.1.3.3.3" xref="S2.p2.3.m3.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S2.p2.3.m3.1.1.1.1.1.1.3.7" xref="S2.p2.3.m3.1.1.1.1.1.1.4.cmml">)</mo></mrow><mo stretchy="false" id="S2.p2.3.m3.1.1.1.1.1.3" xref="S2.p2.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.p2.3.m3.1.1.1.3" xref="S2.p2.3.m3.1.1.1.3.cmml"><mi id="S2.p2.3.m3.1.1.1.3.2" xref="S2.p2.3.m3.1.1.1.3.2.cmml">i</mi><mo id="S2.p2.3.m3.1.1.1.3.1" xref="S2.p2.3.m3.1.1.1.3.1.cmml">=</mo><mn id="S2.p2.3.m3.1.1.1.3.3" xref="S2.p2.3.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">n</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1">superscript</csymbol><apply id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.2.cmml" xref="S2.p2.3.m3.1.1">subscript</csymbol><set id="S2.p2.3.m3.1.1.1.1.2.cmml" xref="S2.p2.3.m3.1.1.1.1.1"><vector id="S2.p2.3.m3.1.1.1.1.1.1.4.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.3"><apply id="S2.p2.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p2.3.m3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.p2.3.m3.1.1.1.1.1.1.1.1.3.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.p2.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.1.1.1.2.2.1.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.p2.3.m3.1.1.1.1.1.1.2.2.2.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S2.p2.3.m3.1.1.1.1.1.1.2.2.3.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply><apply id="S2.p2.3.m3.1.1.1.1.1.1.3.3.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.1.1.1.3.3.1.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.p2.3.m3.1.1.1.1.1.1.3.3.2.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.3.3.2">ğ‘</ci><ci id="S2.p2.3.m3.1.1.1.1.1.1.3.3.3.cmml" xref="S2.p2.3.m3.1.1.1.1.1.1.3.3.3">ğ‘–</ci></apply></vector></set><apply id="S2.p2.3.m3.1.1.1.3.cmml" xref="S2.p2.3.m3.1.1.1.3"><eq id="S2.p2.3.m3.1.1.1.3.1.cmml" xref="S2.p2.3.m3.1.1.1.3.1"></eq><ci id="S2.p2.3.m3.1.1.1.3.2.cmml" xref="S2.p2.3.m3.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S2.p2.3.m3.1.1.1.3.3.cmml" xref="S2.p2.3.m3.1.1.1.3.3">1</cn></apply></apply><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\{(v_{i},q_{i},a_{i})\}_{i=1}^{n}</annotation></semantics></math> with an image <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="v_{i}\in\mathcal{V}" display="inline"><semantics id="S2.p2.4.m4.1a"><mrow id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><msub id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml"><mi id="S2.p2.4.m4.1.1.2.2" xref="S2.p2.4.m4.1.1.2.2.cmml">v</mi><mi id="S2.p2.4.m4.1.1.2.3" xref="S2.p2.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S2.p2.4.m4.1.1.1" xref="S2.p2.4.m4.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.p2.4.m4.1.1.3" xref="S2.p2.4.m4.1.1.3.cmml">ğ’±</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><in id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1.1"></in><apply id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.p2.4.m4.1.1.2.1.cmml" xref="S2.p2.4.m4.1.1.2">subscript</csymbol><ci id="S2.p2.4.m4.1.1.2.2.cmml" xref="S2.p2.4.m4.1.1.2.2">ğ‘£</ci><ci id="S2.p2.4.m4.1.1.2.3.cmml" xref="S2.p2.4.m4.1.1.2.3">ğ‘–</ci></apply><ci id="S2.p2.4.m4.1.1.3.cmml" xref="S2.p2.4.m4.1.1.3">ğ’±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">v_{i}\in\mathcal{V}</annotation></semantics></math>, a question <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="q_{i}\in\mathcal{Q}" display="inline"><semantics id="S2.p2.5.m5.1a"><mrow id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><msub id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml"><mi id="S2.p2.5.m5.1.1.2.2" xref="S2.p2.5.m5.1.1.2.2.cmml">q</mi><mi id="S2.p2.5.m5.1.1.2.3" xref="S2.p2.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S2.p2.5.m5.1.1.1" xref="S2.p2.5.m5.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">ğ’¬</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><in id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1.1"></in><apply id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.p2.5.m5.1.1.2.1.cmml" xref="S2.p2.5.m5.1.1.2">subscript</csymbol><ci id="S2.p2.5.m5.1.1.2.2.cmml" xref="S2.p2.5.m5.1.1.2.2">ğ‘</ci><ci id="S2.p2.5.m5.1.1.2.3.cmml" xref="S2.p2.5.m5.1.1.2.3">ğ‘–</ci></apply><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">ğ’¬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">q_{i}\in\mathcal{Q}</annotation></semantics></math>, and an answer <math id="S2.p2.6.m6.1" class="ltx_Math" alttext="a_{i}\in\mathcal{A}" display="inline"><semantics id="S2.p2.6.m6.1a"><mrow id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><msub id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml"><mi id="S2.p2.6.m6.1.1.2.2" xref="S2.p2.6.m6.1.1.2.2.cmml">a</mi><mi id="S2.p2.6.m6.1.1.2.3" xref="S2.p2.6.m6.1.1.2.3.cmml">i</mi></msub><mo id="S2.p2.6.m6.1.1.1" xref="S2.p2.6.m6.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.p2.6.m6.1.1.3" xref="S2.p2.6.m6.1.1.3.cmml">ğ’œ</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><in id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1.1"></in><apply id="S2.p2.6.m6.1.1.2.cmml" xref="S2.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S2.p2.6.m6.1.1.2.1.cmml" xref="S2.p2.6.m6.1.1.2">subscript</csymbol><ci id="S2.p2.6.m6.1.1.2.2.cmml" xref="S2.p2.6.m6.1.1.2.2">ğ‘</ci><ci id="S2.p2.6.m6.1.1.2.3.cmml" xref="S2.p2.6.m6.1.1.2.3">ğ‘–</ci></apply><ci id="S2.p2.6.m6.1.1.3.cmml" xref="S2.p2.6.m6.1.1.3">ğ’œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">a_{i}\in\mathcal{A}</annotation></semantics></math>, VQA requires machines to optimize the parameters <math id="S2.p2.7.m7.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.p2.7.m7.1a"><mi id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><ci id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">\theta</annotation></semantics></math> and predict answers <math id="S2.p2.8.m8.1" class="ltx_Math" alttext="\hat{a}_{i}" display="inline"><semantics id="S2.p2.8.m8.1a"><msub id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml"><mover accent="true" id="S2.p2.8.m8.1.1.2" xref="S2.p2.8.m8.1.1.2.cmml"><mi id="S2.p2.8.m8.1.1.2.2" xref="S2.p2.8.m8.1.1.2.2.cmml">a</mi><mo id="S2.p2.8.m8.1.1.2.1" xref="S2.p2.8.m8.1.1.2.1.cmml">^</mo></mover><mi id="S2.p2.8.m8.1.1.3" xref="S2.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><apply id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S2.p2.8.m8.1.1.1.cmml" xref="S2.p2.8.m8.1.1">subscript</csymbol><apply id="S2.p2.8.m8.1.1.2.cmml" xref="S2.p2.8.m8.1.1.2"><ci id="S2.p2.8.m8.1.1.2.1.cmml" xref="S2.p2.8.m8.1.1.2.1">^</ci><ci id="S2.p2.8.m8.1.1.2.2.cmml" xref="S2.p2.8.m8.1.1.2.2">ğ‘</ci></apply><ci id="S2.p2.8.m8.1.1.3.cmml" xref="S2.p2.8.m8.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">\hat{a}_{i}</annotation></semantics></math> accurately:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\hat{a}_{i}=\mathop{\arg\max}_{a_{i}\in\mathcal{A}}p(a_{i}|v_{i},q_{i};\theta)." display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.1.cmml"><msub id="S2.E1.m1.2.2.1.1.3" xref="S2.E1.m1.2.2.1.1.3.cmml"><mover accent="true" id="S2.E1.m1.2.2.1.1.3.2" xref="S2.E1.m1.2.2.1.1.3.2.cmml"><mi id="S2.E1.m1.2.2.1.1.3.2.2" xref="S2.E1.m1.2.2.1.1.3.2.2.cmml">a</mi><mo id="S2.E1.m1.2.2.1.1.3.2.1" xref="S2.E1.m1.2.2.1.1.3.2.1.cmml">^</mo></mover><mi id="S2.E1.m1.2.2.1.1.3.3" xref="S2.E1.m1.2.2.1.1.3.3.cmml">i</mi></msub><mo id="S2.E1.m1.2.2.1.1.2" xref="S2.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.1.cmml"><munder id="S2.E1.m1.2.2.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.2.cmml"><mrow id="S2.E1.m1.2.2.1.1.1.2.2" xref="S2.E1.m1.2.2.1.1.1.2.2.cmml"><mi id="S2.E1.m1.2.2.1.1.1.2.2.1" xref="S2.E1.m1.2.2.1.1.1.2.2.1.cmml">arg</mi><mo lspace="0.167em" id="S2.E1.m1.2.2.1.1.1.2.2a" xref="S2.E1.m1.2.2.1.1.1.2.2.cmml">â¡</mo><mi id="S2.E1.m1.2.2.1.1.1.2.2.2" xref="S2.E1.m1.2.2.1.1.1.2.2.2.cmml">max</mi></mrow><mrow id="S2.E1.m1.2.2.1.1.1.2.3" xref="S2.E1.m1.2.2.1.1.1.2.3.cmml"><msub id="S2.E1.m1.2.2.1.1.1.2.3.2" xref="S2.E1.m1.2.2.1.1.1.2.3.2.cmml"><mi id="S2.E1.m1.2.2.1.1.1.2.3.2.2" xref="S2.E1.m1.2.2.1.1.1.2.3.2.2.cmml">a</mi><mi id="S2.E1.m1.2.2.1.1.1.2.3.2.3" xref="S2.E1.m1.2.2.1.1.1.2.3.2.3.cmml">i</mi></msub><mo id="S2.E1.m1.2.2.1.1.1.2.3.1" xref="S2.E1.m1.2.2.1.1.1.2.3.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2.1.1.1.2.3.3" xref="S2.E1.m1.2.2.1.1.1.2.3.3.cmml">ğ’œ</mi></mrow></munder><mrow id="S2.E1.m1.2.2.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.2.cmml">a</mi><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml"><msub id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.cmml">q</mi><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.4" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">;</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">Î¸</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1"><eq id="S2.E1.m1.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.2"></eq><apply id="S2.E1.m1.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.3.1.cmml" xref="S2.E1.m1.2.2.1.1.3">subscript</csymbol><apply id="S2.E1.m1.2.2.1.1.3.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2"><ci id="S2.E1.m1.2.2.1.1.3.2.1.cmml" xref="S2.E1.m1.2.2.1.1.3.2.1">^</ci><ci id="S2.E1.m1.2.2.1.1.3.2.2.cmml" xref="S2.E1.m1.2.2.1.1.3.2.2">ğ‘</ci></apply><ci id="S2.E1.m1.2.2.1.1.3.3.cmml" xref="S2.E1.m1.2.2.1.1.3.3">ğ‘–</ci></apply><apply id="S2.E1.m1.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1"><apply id="S2.E1.m1.2.2.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.2">subscript</csymbol><apply id="S2.E1.m1.2.2.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.2.2"><arg id="S2.E1.m1.2.2.1.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.2.2.1"></arg><max id="S2.E1.m1.2.2.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.2.2.2"></max></apply><apply id="S2.E1.m1.2.2.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.2.3"><in id="S2.E1.m1.2.2.1.1.1.2.3.1.cmml" xref="S2.E1.m1.2.2.1.1.1.2.3.1"></in><apply id="S2.E1.m1.2.2.1.1.1.2.3.2.cmml" xref="S2.E1.m1.2.2.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.2.3.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.2.3.2">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.2.3.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.2.3.2.2">ğ‘</ci><ci id="S2.E1.m1.2.2.1.1.1.2.3.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.2.3.2.3">ğ‘–</ci></apply><ci id="S2.E1.m1.2.2.1.1.1.2.3.3.cmml" xref="S2.E1.m1.2.2.1.1.1.2.3.3">ğ’œ</ci></apply></apply><apply id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2"></times><ci id="S2.E1.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3">ğ‘</ci><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.2">ğ‘</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.3">ğ‘–</ci></apply><list id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2"><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.2">ğ‘</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.2.2.3">ğ‘–</ci></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğœƒ</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\hat{a}_{i}=\mathop{\arg\max}_{a_{i}\in\mathcal{A}}p(a_{i}|v_{i},q_{i};\theta).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.11" class="ltx_p"><span id="S2.p3.11.1" class="ltx_text ltx_font_bold">Generic Paradigm.</span> Current non-debiasing (generic) methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">f</annotation></semantics></math> typically leverage an image encoder <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="E_{\mathrm{v}}:\mathcal{V}\rightarrow\mathbb{R}^{n_{\mathrm{v}}\times d_{\mathrm{v}}}" display="inline"><semantics id="S2.p3.2.m2.1a"><mrow id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><msub id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml"><mi id="S2.p3.2.m2.1.1.2.2" xref="S2.p3.2.m2.1.1.2.2.cmml">E</mi><mi mathvariant="normal" id="S2.p3.2.m2.1.1.2.3" xref="S2.p3.2.m2.1.1.2.3.cmml">v</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S2.p3.2.m2.1.1.1" xref="S2.p3.2.m2.1.1.1.cmml">:</mo><mrow id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.2.m2.1.1.3.2" xref="S2.p3.2.m2.1.1.3.2.cmml">ğ’±</mi><mo stretchy="false" id="S2.p3.2.m2.1.1.3.1" xref="S2.p3.2.m2.1.1.3.1.cmml">â†’</mo><msup id="S2.p3.2.m2.1.1.3.3" xref="S2.p3.2.m2.1.1.3.3.cmml"><mi id="S2.p3.2.m2.1.1.3.3.2" xref="S2.p3.2.m2.1.1.3.3.2.cmml">â„</mi><mrow id="S2.p3.2.m2.1.1.3.3.3" xref="S2.p3.2.m2.1.1.3.3.3.cmml"><msub id="S2.p3.2.m2.1.1.3.3.3.2" xref="S2.p3.2.m2.1.1.3.3.3.2.cmml"><mi id="S2.p3.2.m2.1.1.3.3.3.2.2" xref="S2.p3.2.m2.1.1.3.3.3.2.2.cmml">n</mi><mi mathvariant="normal" id="S2.p3.2.m2.1.1.3.3.3.2.3" xref="S2.p3.2.m2.1.1.3.3.3.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.p3.2.m2.1.1.3.3.3.1" xref="S2.p3.2.m2.1.1.3.3.3.1.cmml">Ã—</mo><msub id="S2.p3.2.m2.1.1.3.3.3.3" xref="S2.p3.2.m2.1.1.3.3.3.3.cmml"><mi id="S2.p3.2.m2.1.1.3.3.3.3.2" xref="S2.p3.2.m2.1.1.3.3.3.3.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.2.m2.1.1.3.3.3.3.3" xref="S2.p3.2.m2.1.1.3.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><ci id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1.1">:</ci><apply id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.2.1.cmml" xref="S2.p3.2.m2.1.1.2">subscript</csymbol><ci id="S2.p3.2.m2.1.1.2.2.cmml" xref="S2.p3.2.m2.1.1.2.2">ğ¸</ci><ci id="S2.p3.2.m2.1.1.2.3.cmml" xref="S2.p3.2.m2.1.1.2.3">v</ci></apply><apply id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3"><ci id="S2.p3.2.m2.1.1.3.1.cmml" xref="S2.p3.2.m2.1.1.3.1">â†’</ci><ci id="S2.p3.2.m2.1.1.3.2.cmml" xref="S2.p3.2.m2.1.1.3.2">ğ’±</ci><apply id="S2.p3.2.m2.1.1.3.3.cmml" xref="S2.p3.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.3.3.1.cmml" xref="S2.p3.2.m2.1.1.3.3">superscript</csymbol><ci id="S2.p3.2.m2.1.1.3.3.2.cmml" xref="S2.p3.2.m2.1.1.3.3.2">â„</ci><apply id="S2.p3.2.m2.1.1.3.3.3.cmml" xref="S2.p3.2.m2.1.1.3.3.3"><times id="S2.p3.2.m2.1.1.3.3.3.1.cmml" xref="S2.p3.2.m2.1.1.3.3.3.1"></times><apply id="S2.p3.2.m2.1.1.3.3.3.2.cmml" xref="S2.p3.2.m2.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.3.3.3.2.1.cmml" xref="S2.p3.2.m2.1.1.3.3.3.2">subscript</csymbol><ci id="S2.p3.2.m2.1.1.3.3.3.2.2.cmml" xref="S2.p3.2.m2.1.1.3.3.3.2.2">ğ‘›</ci><ci id="S2.p3.2.m2.1.1.3.3.3.2.3.cmml" xref="S2.p3.2.m2.1.1.3.3.3.2.3">v</ci></apply><apply id="S2.p3.2.m2.1.1.3.3.3.3.cmml" xref="S2.p3.2.m2.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.3.3.3.3.1.cmml" xref="S2.p3.2.m2.1.1.3.3.3.3">subscript</csymbol><ci id="S2.p3.2.m2.1.1.3.3.3.3.2.cmml" xref="S2.p3.2.m2.1.1.3.3.3.3.2">ğ‘‘</ci><ci id="S2.p3.2.m2.1.1.3.3.3.3.3.cmml" xref="S2.p3.2.m2.1.1.3.3.3.3.3">v</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">E_{\mathrm{v}}:\mathcal{V}\rightarrow\mathbb{R}^{n_{\mathrm{v}}\times d_{\mathrm{v}}}</annotation></semantics></math> to learn <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="n_{\mathrm{v}}" display="inline"><semantics id="S2.p3.3.m3.1a"><msub id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml"><mi id="S2.p3.3.m3.1.1.2" xref="S2.p3.3.m3.1.1.2.cmml">n</mi><mi mathvariant="normal" id="S2.p3.3.m3.1.1.3" xref="S2.p3.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><apply id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p3.3.m3.1.1.1.cmml" xref="S2.p3.3.m3.1.1">subscript</csymbol><ci id="S2.p3.3.m3.1.1.2.cmml" xref="S2.p3.3.m3.1.1.2">ğ‘›</ci><ci id="S2.p3.3.m3.1.1.3.cmml" xref="S2.p3.3.m3.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">n_{\mathrm{v}}</annotation></semantics></math> region-level (patch-level) representations with <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="d_{\mathrm{v}}" display="inline"><semantics id="S2.p3.4.m4.1a"><msub id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml"><mi id="S2.p3.4.m4.1.1.2" xref="S2.p3.4.m4.1.1.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.4.m4.1.1.3" xref="S2.p3.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m4.1.1.1.cmml" xref="S2.p3.4.m4.1.1">subscript</csymbol><ci id="S2.p3.4.m4.1.1.2.cmml" xref="S2.p3.4.m4.1.1.2">ğ‘‘</ci><ci id="S2.p3.4.m4.1.1.3.cmml" xref="S2.p3.4.m4.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">d_{\mathrm{v}}</annotation></semantics></math> dimensions, a question encoder <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="E_{\mathrm{q}}:\mathcal{Q}\rightarrow\mathbb{R}^{n_{\mathrm{q}}\times d_{\mathrm{q}}}" display="inline"><semantics id="S2.p3.5.m5.1a"><mrow id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><msub id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml"><mi id="S2.p3.5.m5.1.1.2.2" xref="S2.p3.5.m5.1.1.2.2.cmml">E</mi><mi mathvariant="normal" id="S2.p3.5.m5.1.1.2.3" xref="S2.p3.5.m5.1.1.2.3.cmml">q</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S2.p3.5.m5.1.1.1" xref="S2.p3.5.m5.1.1.1.cmml">:</mo><mrow id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.5.m5.1.1.3.2" xref="S2.p3.5.m5.1.1.3.2.cmml">ğ’¬</mi><mo stretchy="false" id="S2.p3.5.m5.1.1.3.1" xref="S2.p3.5.m5.1.1.3.1.cmml">â†’</mo><msup id="S2.p3.5.m5.1.1.3.3" xref="S2.p3.5.m5.1.1.3.3.cmml"><mi id="S2.p3.5.m5.1.1.3.3.2" xref="S2.p3.5.m5.1.1.3.3.2.cmml">â„</mi><mrow id="S2.p3.5.m5.1.1.3.3.3" xref="S2.p3.5.m5.1.1.3.3.3.cmml"><msub id="S2.p3.5.m5.1.1.3.3.3.2" xref="S2.p3.5.m5.1.1.3.3.3.2.cmml"><mi id="S2.p3.5.m5.1.1.3.3.3.2.2" xref="S2.p3.5.m5.1.1.3.3.3.2.2.cmml">n</mi><mi mathvariant="normal" id="S2.p3.5.m5.1.1.3.3.3.2.3" xref="S2.p3.5.m5.1.1.3.3.3.2.3.cmml">q</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.p3.5.m5.1.1.3.3.3.1" xref="S2.p3.5.m5.1.1.3.3.3.1.cmml">Ã—</mo><msub id="S2.p3.5.m5.1.1.3.3.3.3" xref="S2.p3.5.m5.1.1.3.3.3.3.cmml"><mi id="S2.p3.5.m5.1.1.3.3.3.3.2" xref="S2.p3.5.m5.1.1.3.3.3.3.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.5.m5.1.1.3.3.3.3.3" xref="S2.p3.5.m5.1.1.3.3.3.3.3.cmml">q</mi></msub></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><ci id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1.1">:</ci><apply id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.2.1.cmml" xref="S2.p3.5.m5.1.1.2">subscript</csymbol><ci id="S2.p3.5.m5.1.1.2.2.cmml" xref="S2.p3.5.m5.1.1.2.2">ğ¸</ci><ci id="S2.p3.5.m5.1.1.2.3.cmml" xref="S2.p3.5.m5.1.1.2.3">q</ci></apply><apply id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3"><ci id="S2.p3.5.m5.1.1.3.1.cmml" xref="S2.p3.5.m5.1.1.3.1">â†’</ci><ci id="S2.p3.5.m5.1.1.3.2.cmml" xref="S2.p3.5.m5.1.1.3.2">ğ’¬</ci><apply id="S2.p3.5.m5.1.1.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.3.3.1.cmml" xref="S2.p3.5.m5.1.1.3.3">superscript</csymbol><ci id="S2.p3.5.m5.1.1.3.3.2.cmml" xref="S2.p3.5.m5.1.1.3.3.2">â„</ci><apply id="S2.p3.5.m5.1.1.3.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3.3"><times id="S2.p3.5.m5.1.1.3.3.3.1.cmml" xref="S2.p3.5.m5.1.1.3.3.3.1"></times><apply id="S2.p3.5.m5.1.1.3.3.3.2.cmml" xref="S2.p3.5.m5.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.3.3.3.2.1.cmml" xref="S2.p3.5.m5.1.1.3.3.3.2">subscript</csymbol><ci id="S2.p3.5.m5.1.1.3.3.3.2.2.cmml" xref="S2.p3.5.m5.1.1.3.3.3.2.2">ğ‘›</ci><ci id="S2.p3.5.m5.1.1.3.3.3.2.3.cmml" xref="S2.p3.5.m5.1.1.3.3.3.2.3">q</ci></apply><apply id="S2.p3.5.m5.1.1.3.3.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.3.3.3.3.1.cmml" xref="S2.p3.5.m5.1.1.3.3.3.3">subscript</csymbol><ci id="S2.p3.5.m5.1.1.3.3.3.3.2.cmml" xref="S2.p3.5.m5.1.1.3.3.3.3.2">ğ‘‘</ci><ci id="S2.p3.5.m5.1.1.3.3.3.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3.3.3.3">q</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">E_{\mathrm{q}}:\mathcal{Q}\rightarrow\mathbb{R}^{n_{\mathrm{q}}\times d_{\mathrm{q}}}</annotation></semantics></math> to output <math id="S2.p3.6.m6.1" class="ltx_Math" alttext="n_{\mathrm{q}}" display="inline"><semantics id="S2.p3.6.m6.1a"><msub id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mi id="S2.p3.6.m6.1.1.2" xref="S2.p3.6.m6.1.1.2.cmml">n</mi><mi mathvariant="normal" id="S2.p3.6.m6.1.1.3" xref="S2.p3.6.m6.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p3.6.m6.1.1.1.cmml" xref="S2.p3.6.m6.1.1">subscript</csymbol><ci id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2">ğ‘›</ci><ci id="S2.p3.6.m6.1.1.3.cmml" xref="S2.p3.6.m6.1.1.3">q</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">n_{\mathrm{q}}</annotation></semantics></math> word-level vectors with <math id="S2.p3.7.m7.1" class="ltx_Math" alttext="d_{\mathrm{q}}" display="inline"><semantics id="S2.p3.7.m7.1a"><msub id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml"><mi id="S2.p3.7.m7.1.1.2" xref="S2.p3.7.m7.1.1.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.7.m7.1.1.3" xref="S2.p3.7.m7.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><apply id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p3.7.m7.1.1.1.cmml" xref="S2.p3.7.m7.1.1">subscript</csymbol><ci id="S2.p3.7.m7.1.1.2.cmml" xref="S2.p3.7.m7.1.1.2">ğ‘‘</ci><ci id="S2.p3.7.m7.1.1.3.cmml" xref="S2.p3.7.m7.1.1.3">q</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">d_{\mathrm{q}}</annotation></semantics></math> dimensions, a multi-modality encoder <math id="S2.p3.8.m8.1" class="ltx_Math" alttext="E_{\mathrm{m}}:\mathbb{R}^{n_{\mathrm{q}}\times d_{\mathrm{q}}}\times\mathbb{R}^{n_{\mathrm{v}}\times d_{\mathrm{v}}}\rightarrow\mathbb{R}^{d_{\mathrm{m}}}" display="inline"><semantics id="S2.p3.8.m8.1a"><mrow id="S2.p3.8.m8.1.1" xref="S2.p3.8.m8.1.1.cmml"><msub id="S2.p3.8.m8.1.1.2" xref="S2.p3.8.m8.1.1.2.cmml"><mi id="S2.p3.8.m8.1.1.2.2" xref="S2.p3.8.m8.1.1.2.2.cmml">E</mi><mi mathvariant="normal" id="S2.p3.8.m8.1.1.2.3" xref="S2.p3.8.m8.1.1.2.3.cmml">m</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S2.p3.8.m8.1.1.1" xref="S2.p3.8.m8.1.1.1.cmml">:</mo><mrow id="S2.p3.8.m8.1.1.3" xref="S2.p3.8.m8.1.1.3.cmml"><mrow id="S2.p3.8.m8.1.1.3.2" xref="S2.p3.8.m8.1.1.3.2.cmml"><msup id="S2.p3.8.m8.1.1.3.2.2" xref="S2.p3.8.m8.1.1.3.2.2.cmml"><mi id="S2.p3.8.m8.1.1.3.2.2.2" xref="S2.p3.8.m8.1.1.3.2.2.2.cmml">â„</mi><mrow id="S2.p3.8.m8.1.1.3.2.2.3" xref="S2.p3.8.m8.1.1.3.2.2.3.cmml"><msub id="S2.p3.8.m8.1.1.3.2.2.3.2" xref="S2.p3.8.m8.1.1.3.2.2.3.2.cmml"><mi id="S2.p3.8.m8.1.1.3.2.2.3.2.2" xref="S2.p3.8.m8.1.1.3.2.2.3.2.2.cmml">n</mi><mi mathvariant="normal" id="S2.p3.8.m8.1.1.3.2.2.3.2.3" xref="S2.p3.8.m8.1.1.3.2.2.3.2.3.cmml">q</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.p3.8.m8.1.1.3.2.2.3.1" xref="S2.p3.8.m8.1.1.3.2.2.3.1.cmml">Ã—</mo><msub id="S2.p3.8.m8.1.1.3.2.2.3.3" xref="S2.p3.8.m8.1.1.3.2.2.3.3.cmml"><mi id="S2.p3.8.m8.1.1.3.2.2.3.3.2" xref="S2.p3.8.m8.1.1.3.2.2.3.3.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.8.m8.1.1.3.2.2.3.3.3" xref="S2.p3.8.m8.1.1.3.2.2.3.3.3.cmml">q</mi></msub></mrow></msup><mo lspace="0.222em" rspace="0.222em" id="S2.p3.8.m8.1.1.3.2.1" xref="S2.p3.8.m8.1.1.3.2.1.cmml">Ã—</mo><msup id="S2.p3.8.m8.1.1.3.2.3" xref="S2.p3.8.m8.1.1.3.2.3.cmml"><mi id="S2.p3.8.m8.1.1.3.2.3.2" xref="S2.p3.8.m8.1.1.3.2.3.2.cmml">â„</mi><mrow id="S2.p3.8.m8.1.1.3.2.3.3" xref="S2.p3.8.m8.1.1.3.2.3.3.cmml"><msub id="S2.p3.8.m8.1.1.3.2.3.3.2" xref="S2.p3.8.m8.1.1.3.2.3.3.2.cmml"><mi id="S2.p3.8.m8.1.1.3.2.3.3.2.2" xref="S2.p3.8.m8.1.1.3.2.3.3.2.2.cmml">n</mi><mi mathvariant="normal" id="S2.p3.8.m8.1.1.3.2.3.3.2.3" xref="S2.p3.8.m8.1.1.3.2.3.3.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.p3.8.m8.1.1.3.2.3.3.1" xref="S2.p3.8.m8.1.1.3.2.3.3.1.cmml">Ã—</mo><msub id="S2.p3.8.m8.1.1.3.2.3.3.3" xref="S2.p3.8.m8.1.1.3.2.3.3.3.cmml"><mi id="S2.p3.8.m8.1.1.3.2.3.3.3.2" xref="S2.p3.8.m8.1.1.3.2.3.3.3.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.8.m8.1.1.3.2.3.3.3.3" xref="S2.p3.8.m8.1.1.3.2.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><mo stretchy="false" id="S2.p3.8.m8.1.1.3.1" xref="S2.p3.8.m8.1.1.3.1.cmml">â†’</mo><msup id="S2.p3.8.m8.1.1.3.3" xref="S2.p3.8.m8.1.1.3.3.cmml"><mi id="S2.p3.8.m8.1.1.3.3.2" xref="S2.p3.8.m8.1.1.3.3.2.cmml">â„</mi><msub id="S2.p3.8.m8.1.1.3.3.3" xref="S2.p3.8.m8.1.1.3.3.3.cmml"><mi id="S2.p3.8.m8.1.1.3.3.3.2" xref="S2.p3.8.m8.1.1.3.3.3.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.8.m8.1.1.3.3.3.3" xref="S2.p3.8.m8.1.1.3.3.3.3.cmml">m</mi></msub></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.8.m8.1b"><apply id="S2.p3.8.m8.1.1.cmml" xref="S2.p3.8.m8.1.1"><ci id="S2.p3.8.m8.1.1.1.cmml" xref="S2.p3.8.m8.1.1.1">:</ci><apply id="S2.p3.8.m8.1.1.2.cmml" xref="S2.p3.8.m8.1.1.2"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.2.1.cmml" xref="S2.p3.8.m8.1.1.2">subscript</csymbol><ci id="S2.p3.8.m8.1.1.2.2.cmml" xref="S2.p3.8.m8.1.1.2.2">ğ¸</ci><ci id="S2.p3.8.m8.1.1.2.3.cmml" xref="S2.p3.8.m8.1.1.2.3">m</ci></apply><apply id="S2.p3.8.m8.1.1.3.cmml" xref="S2.p3.8.m8.1.1.3"><ci id="S2.p3.8.m8.1.1.3.1.cmml" xref="S2.p3.8.m8.1.1.3.1">â†’</ci><apply id="S2.p3.8.m8.1.1.3.2.cmml" xref="S2.p3.8.m8.1.1.3.2"><times id="S2.p3.8.m8.1.1.3.2.1.cmml" xref="S2.p3.8.m8.1.1.3.2.1"></times><apply id="S2.p3.8.m8.1.1.3.2.2.cmml" xref="S2.p3.8.m8.1.1.3.2.2"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.3.2.2.1.cmml" xref="S2.p3.8.m8.1.1.3.2.2">superscript</csymbol><ci id="S2.p3.8.m8.1.1.3.2.2.2.cmml" xref="S2.p3.8.m8.1.1.3.2.2.2">â„</ci><apply id="S2.p3.8.m8.1.1.3.2.2.3.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3"><times id="S2.p3.8.m8.1.1.3.2.2.3.1.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.1"></times><apply id="S2.p3.8.m8.1.1.3.2.2.3.2.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.2"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.3.2.2.3.2.1.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.2">subscript</csymbol><ci id="S2.p3.8.m8.1.1.3.2.2.3.2.2.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.2.2">ğ‘›</ci><ci id="S2.p3.8.m8.1.1.3.2.2.3.2.3.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.2.3">q</ci></apply><apply id="S2.p3.8.m8.1.1.3.2.2.3.3.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.3"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.3.2.2.3.3.1.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.3">subscript</csymbol><ci id="S2.p3.8.m8.1.1.3.2.2.3.3.2.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.3.2">ğ‘‘</ci><ci id="S2.p3.8.m8.1.1.3.2.2.3.3.3.cmml" xref="S2.p3.8.m8.1.1.3.2.2.3.3.3">q</ci></apply></apply></apply><apply id="S2.p3.8.m8.1.1.3.2.3.cmml" xref="S2.p3.8.m8.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.3.2.3.1.cmml" xref="S2.p3.8.m8.1.1.3.2.3">superscript</csymbol><ci id="S2.p3.8.m8.1.1.3.2.3.2.cmml" xref="S2.p3.8.m8.1.1.3.2.3.2">â„</ci><apply id="S2.p3.8.m8.1.1.3.2.3.3.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3"><times id="S2.p3.8.m8.1.1.3.2.3.3.1.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.1"></times><apply id="S2.p3.8.m8.1.1.3.2.3.3.2.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.2"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.3.2.3.3.2.1.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.2">subscript</csymbol><ci id="S2.p3.8.m8.1.1.3.2.3.3.2.2.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.2.2">ğ‘›</ci><ci id="S2.p3.8.m8.1.1.3.2.3.3.2.3.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.2.3">v</ci></apply><apply id="S2.p3.8.m8.1.1.3.2.3.3.3.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.3"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.3.2.3.3.3.1.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.3">subscript</csymbol><ci id="S2.p3.8.m8.1.1.3.2.3.3.3.2.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.3.2">ğ‘‘</ci><ci id="S2.p3.8.m8.1.1.3.2.3.3.3.3.cmml" xref="S2.p3.8.m8.1.1.3.2.3.3.3.3">v</ci></apply></apply></apply></apply><apply id="S2.p3.8.m8.1.1.3.3.cmml" xref="S2.p3.8.m8.1.1.3.3"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.3.3.1.cmml" xref="S2.p3.8.m8.1.1.3.3">superscript</csymbol><ci id="S2.p3.8.m8.1.1.3.3.2.cmml" xref="S2.p3.8.m8.1.1.3.3.2">â„</ci><apply id="S2.p3.8.m8.1.1.3.3.3.cmml" xref="S2.p3.8.m8.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.3.3.3.1.cmml" xref="S2.p3.8.m8.1.1.3.3.3">subscript</csymbol><ci id="S2.p3.8.m8.1.1.3.3.3.2.cmml" xref="S2.p3.8.m8.1.1.3.3.3.2">ğ‘‘</ci><ci id="S2.p3.8.m8.1.1.3.3.3.3.cmml" xref="S2.p3.8.m8.1.1.3.3.3.3">m</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.8.m8.1c">E_{\mathrm{m}}:\mathbb{R}^{n_{\mathrm{q}}\times d_{\mathrm{q}}}\times\mathbb{R}^{n_{\mathrm{v}}\times d_{\mathrm{v}}}\rightarrow\mathbb{R}^{d_{\mathrm{m}}}</annotation></semantics></math> to learn fusion representations with <math id="S2.p3.9.m9.1" class="ltx_Math" alttext="d_{\mathrm{m}}" display="inline"><semantics id="S2.p3.9.m9.1a"><msub id="S2.p3.9.m9.1.1" xref="S2.p3.9.m9.1.1.cmml"><mi id="S2.p3.9.m9.1.1.2" xref="S2.p3.9.m9.1.1.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.9.m9.1.1.3" xref="S2.p3.9.m9.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.9.m9.1b"><apply id="S2.p3.9.m9.1.1.cmml" xref="S2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p3.9.m9.1.1.1.cmml" xref="S2.p3.9.m9.1.1">subscript</csymbol><ci id="S2.p3.9.m9.1.1.2.cmml" xref="S2.p3.9.m9.1.1.2">ğ‘‘</ci><ci id="S2.p3.9.m9.1.1.3.cmml" xref="S2.p3.9.m9.1.1.3">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.9.m9.1c">d_{\mathrm{m}}</annotation></semantics></math> dimensions, and a classifier <math id="S2.p3.10.m10.1" class="ltx_Math" alttext="E_{\mathrm{c}}:\mathbb{R}^{d_{\mathrm{m}}}\rightarrow\mathbb{R}^{|\mathcal{A}|}" display="inline"><semantics id="S2.p3.10.m10.1a"><mrow id="S2.p3.10.m10.1.2" xref="S2.p3.10.m10.1.2.cmml"><msub id="S2.p3.10.m10.1.2.2" xref="S2.p3.10.m10.1.2.2.cmml"><mi id="S2.p3.10.m10.1.2.2.2" xref="S2.p3.10.m10.1.2.2.2.cmml">E</mi><mi mathvariant="normal" id="S2.p3.10.m10.1.2.2.3" xref="S2.p3.10.m10.1.2.2.3.cmml">c</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S2.p3.10.m10.1.2.1" xref="S2.p3.10.m10.1.2.1.cmml">:</mo><mrow id="S2.p3.10.m10.1.2.3" xref="S2.p3.10.m10.1.2.3.cmml"><msup id="S2.p3.10.m10.1.2.3.2" xref="S2.p3.10.m10.1.2.3.2.cmml"><mi id="S2.p3.10.m10.1.2.3.2.2" xref="S2.p3.10.m10.1.2.3.2.2.cmml">â„</mi><msub id="S2.p3.10.m10.1.2.3.2.3" xref="S2.p3.10.m10.1.2.3.2.3.cmml"><mi id="S2.p3.10.m10.1.2.3.2.3.2" xref="S2.p3.10.m10.1.2.3.2.3.2.cmml">d</mi><mi mathvariant="normal" id="S2.p3.10.m10.1.2.3.2.3.3" xref="S2.p3.10.m10.1.2.3.2.3.3.cmml">m</mi></msub></msup><mo stretchy="false" id="S2.p3.10.m10.1.2.3.1" xref="S2.p3.10.m10.1.2.3.1.cmml">â†’</mo><msup id="S2.p3.10.m10.1.2.3.3" xref="S2.p3.10.m10.1.2.3.3.cmml"><mi id="S2.p3.10.m10.1.2.3.3.2" xref="S2.p3.10.m10.1.2.3.3.2.cmml">â„</mi><mrow id="S2.p3.10.m10.1.1.1.3" xref="S2.p3.10.m10.1.1.1.2.cmml"><mo stretchy="false" id="S2.p3.10.m10.1.1.1.3.1" xref="S2.p3.10.m10.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.p3.10.m10.1.1.1.1" xref="S2.p3.10.m10.1.1.1.1.cmml">ğ’œ</mi><mo stretchy="false" id="S2.p3.10.m10.1.1.1.3.2" xref="S2.p3.10.m10.1.1.1.2.1.cmml">|</mo></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.10.m10.1b"><apply id="S2.p3.10.m10.1.2.cmml" xref="S2.p3.10.m10.1.2"><ci id="S2.p3.10.m10.1.2.1.cmml" xref="S2.p3.10.m10.1.2.1">:</ci><apply id="S2.p3.10.m10.1.2.2.cmml" xref="S2.p3.10.m10.1.2.2"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.2.2.1.cmml" xref="S2.p3.10.m10.1.2.2">subscript</csymbol><ci id="S2.p3.10.m10.1.2.2.2.cmml" xref="S2.p3.10.m10.1.2.2.2">ğ¸</ci><ci id="S2.p3.10.m10.1.2.2.3.cmml" xref="S2.p3.10.m10.1.2.2.3">c</ci></apply><apply id="S2.p3.10.m10.1.2.3.cmml" xref="S2.p3.10.m10.1.2.3"><ci id="S2.p3.10.m10.1.2.3.1.cmml" xref="S2.p3.10.m10.1.2.3.1">â†’</ci><apply id="S2.p3.10.m10.1.2.3.2.cmml" xref="S2.p3.10.m10.1.2.3.2"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.2.3.2.1.cmml" xref="S2.p3.10.m10.1.2.3.2">superscript</csymbol><ci id="S2.p3.10.m10.1.2.3.2.2.cmml" xref="S2.p3.10.m10.1.2.3.2.2">â„</ci><apply id="S2.p3.10.m10.1.2.3.2.3.cmml" xref="S2.p3.10.m10.1.2.3.2.3"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.2.3.2.3.1.cmml" xref="S2.p3.10.m10.1.2.3.2.3">subscript</csymbol><ci id="S2.p3.10.m10.1.2.3.2.3.2.cmml" xref="S2.p3.10.m10.1.2.3.2.3.2">ğ‘‘</ci><ci id="S2.p3.10.m10.1.2.3.2.3.3.cmml" xref="S2.p3.10.m10.1.2.3.2.3.3">m</ci></apply></apply><apply id="S2.p3.10.m10.1.2.3.3.cmml" xref="S2.p3.10.m10.1.2.3.3"><csymbol cd="ambiguous" id="S2.p3.10.m10.1.2.3.3.1.cmml" xref="S2.p3.10.m10.1.2.3.3">superscript</csymbol><ci id="S2.p3.10.m10.1.2.3.3.2.cmml" xref="S2.p3.10.m10.1.2.3.3.2">â„</ci><apply id="S2.p3.10.m10.1.1.1.2.cmml" xref="S2.p3.10.m10.1.1.1.3"><abs id="S2.p3.10.m10.1.1.1.2.1.cmml" xref="S2.p3.10.m10.1.1.1.3.1"></abs><ci id="S2.p3.10.m10.1.1.1.1.cmml" xref="S2.p3.10.m10.1.1.1.1">ğ’œ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.10.m10.1c">E_{\mathrm{c}}:\mathbb{R}^{d_{\mathrm{m}}}\rightarrow\mathbb{R}^{|\mathcal{A}|}</annotation></semantics></math> to obtain predictions over the answer space <math id="S2.p3.11.m11.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S2.p3.11.m11.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p3.11.m11.1.1" xref="S2.p3.11.m11.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S2.p3.11.m11.1b"><ci id="S2.p3.11.m11.1.1.cmml" xref="S2.p3.11.m11.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.11.m11.1c">\mathcal{A}</annotation></semantics></math>. The paradigm can be denoted as follows and shown in Fig. <a href="#S2.F2" title="Figure 2 â€£ 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>:</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2307.11471/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Generic paradigm for VQA. This paradigm regards the image region (patch) and the question word as different types of tokens, while the other paradigm considers them to be the same kinds of tokens. Specifically, the latter always employs Transformer to replace <math id="S2.F2.4.m1.1" class="ltx_Math" alttext="E_{\mathrm{q}}" display="inline"><semantics id="S2.F2.4.m1.1b"><msub id="S2.F2.4.m1.1.1" xref="S2.F2.4.m1.1.1.cmml"><mi id="S2.F2.4.m1.1.1.2" xref="S2.F2.4.m1.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S2.F2.4.m1.1.1.3" xref="S2.F2.4.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.4.m1.1c"><apply id="S2.F2.4.m1.1.1.cmml" xref="S2.F2.4.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.4.m1.1.1.1.cmml" xref="S2.F2.4.m1.1.1">subscript</csymbol><ci id="S2.F2.4.m1.1.1.2.cmml" xref="S2.F2.4.m1.1.1.2">ğ¸</ci><ci id="S2.F2.4.m1.1.1.3.cmml" xref="S2.F2.4.m1.1.1.3">q</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.m1.1d">E_{\mathrm{q}}</annotation></semantics></math>, <math id="S2.F2.5.m2.1" class="ltx_Math" alttext="E_{\mathrm{v}}" display="inline"><semantics id="S2.F2.5.m2.1b"><msub id="S2.F2.5.m2.1.1" xref="S2.F2.5.m2.1.1.cmml"><mi id="S2.F2.5.m2.1.1.2" xref="S2.F2.5.m2.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S2.F2.5.m2.1.1.3" xref="S2.F2.5.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.5.m2.1c"><apply id="S2.F2.5.m2.1.1.cmml" xref="S2.F2.5.m2.1.1"><csymbol cd="ambiguous" id="S2.F2.5.m2.1.1.1.cmml" xref="S2.F2.5.m2.1.1">subscript</csymbol><ci id="S2.F2.5.m2.1.1.2.cmml" xref="S2.F2.5.m2.1.1.2">ğ¸</ci><ci id="S2.F2.5.m2.1.1.3.cmml" xref="S2.F2.5.m2.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m2.1d">E_{\mathrm{v}}</annotation></semantics></math> and <math id="S2.F2.6.m3.1" class="ltx_Math" alttext="E_{\mathrm{m}}" display="inline"><semantics id="S2.F2.6.m3.1b"><msub id="S2.F2.6.m3.1.1" xref="S2.F2.6.m3.1.1.cmml"><mi id="S2.F2.6.m3.1.1.2" xref="S2.F2.6.m3.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S2.F2.6.m3.1.1.3" xref="S2.F2.6.m3.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.6.m3.1c"><apply id="S2.F2.6.m3.1.1.cmml" xref="S2.F2.6.m3.1.1"><csymbol cd="ambiguous" id="S2.F2.6.m3.1.1.1.cmml" xref="S2.F2.6.m3.1.1">subscript</csymbol><ci id="S2.F2.6.m3.1.1.2.cmml" xref="S2.F2.6.m3.1.1.2">ğ¸</ci><ci id="S2.F2.6.m3.1.1.3.cmml" xref="S2.F2.6.m3.1.1.3">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m3.1d">E_{\mathrm{m}}</annotation></semantics></math>.</figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2307.11471/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>VQA v1 example. Each question in this dataset is annotated by ten humans. Therefore, a question may exist multiple different ground-truth answers. The dataset is classified into three categories according to answer types: â€œNumberâ€, â€œYes/Noâ€, and â€œOtherâ€.</figcaption>
</figure>
<div id="S2.p4" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="\hat{a}_{i}=f(v_{i},q_{i};\theta)=E_{\mathrm{c}}(E_{\mathrm{m}}(E_{\mathrm{v}}(v_{i}),E_{\mathrm{q}}(q_{i})))." display="block"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.2.1" xref="S2.E2.m1.2.2.1.1.cmml"><mrow id="S2.E2.m1.2.2.1.1" xref="S2.E2.m1.2.2.1.1.cmml"><msub id="S2.E2.m1.2.2.1.1.5" xref="S2.E2.m1.2.2.1.1.5.cmml"><mover accent="true" id="S2.E2.m1.2.2.1.1.5.2" xref="S2.E2.m1.2.2.1.1.5.2.cmml"><mi id="S2.E2.m1.2.2.1.1.5.2.2" xref="S2.E2.m1.2.2.1.1.5.2.2.cmml">a</mi><mo id="S2.E2.m1.2.2.1.1.5.2.1" xref="S2.E2.m1.2.2.1.1.5.2.1.cmml">^</mo></mover><mi id="S2.E2.m1.2.2.1.1.5.3" xref="S2.E2.m1.2.2.1.1.5.3.cmml">i</mi></msub><mo id="S2.E2.m1.2.2.1.1.6" xref="S2.E2.m1.2.2.1.1.6.cmml">=</mo><mrow id="S2.E2.m1.2.2.1.1.2" xref="S2.E2.m1.2.2.1.1.2.cmml"><mi id="S2.E2.m1.2.2.1.1.2.4" xref="S2.E2.m1.2.2.1.1.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.2.3" xref="S2.E2.m1.2.2.1.1.2.3.cmml">â€‹</mo><mrow id="S2.E2.m1.2.2.1.1.2.2.2" xref="S2.E2.m1.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.2.2.2.3" xref="S2.E2.m1.2.2.1.1.2.2.3.cmml">(</mo><msub id="S2.E2.m1.2.2.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.cmml">v</mi><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E2.m1.2.2.1.1.2.2.2.4" xref="S2.E2.m1.2.2.1.1.2.2.3.cmml">,</mo><msub id="S2.E2.m1.2.2.1.1.2.2.2.2" xref="S2.E2.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S2.E2.m1.2.2.1.1.2.2.2.2.2" xref="S2.E2.m1.2.2.1.1.2.2.2.2.2.cmml">q</mi><mi id="S2.E2.m1.2.2.1.1.2.2.2.2.3" xref="S2.E2.m1.2.2.1.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.2.2.1.1.2.2.2.5" xref="S2.E2.m1.2.2.1.1.2.2.3.cmml">;</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">Î¸</mi><mo stretchy="false" id="S2.E2.m1.2.2.1.1.2.2.2.6" xref="S2.E2.m1.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.2.2.1.1.7" xref="S2.E2.m1.2.2.1.1.7.cmml">=</mo><mrow id="S2.E2.m1.2.2.1.1.3" xref="S2.E2.m1.2.2.1.1.3.cmml"><msub id="S2.E2.m1.2.2.1.1.3.3" xref="S2.E2.m1.2.2.1.1.3.3.cmml"><mi id="S2.E2.m1.2.2.1.1.3.3.2" xref="S2.E2.m1.2.2.1.1.3.3.2.cmml">E</mi><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.3.3.3" xref="S2.E2.m1.2.2.1.1.3.3.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.3.2" xref="S2.E2.m1.2.2.1.1.3.2.cmml">â€‹</mo><mrow id="S2.E2.m1.2.2.1.1.3.1.1" xref="S2.E2.m1.2.2.1.1.3.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.1.1.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.1.1.3.1.1.1" xref="S2.E2.m1.2.2.1.1.3.1.1.1.cmml"><msub id="S2.E2.m1.2.2.1.1.3.1.1.1.4" xref="S2.E2.m1.2.2.1.1.3.1.1.1.4.cmml"><mi id="S2.E2.m1.2.2.1.1.3.1.1.1.4.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.4.2.cmml">E</mi><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.3.1.1.1.4.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.4.3.cmml">m</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.3.1.1.1.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.3.cmml">â€‹</mo><mrow id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.3.cmml">(</mo><mrow id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.2.cmml">E</mi><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.4" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.3.cmml">,</mo><mrow id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.cmml"><msub id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.cmml"><mi id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.2.cmml">E</mi><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.2.cmml">â€‹</mo><mrow id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.cmml">(</mo><msub id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.2" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.2.cmml">q</mi><mi id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.5" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.1.1.3" xref="S2.E2.m1.2.2.1.1.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E2.m1.2.2.1.2" xref="S2.E2.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.2.1.1.cmml" xref="S2.E2.m1.2.2.1"><and id="S2.E2.m1.2.2.1.1a.cmml" xref="S2.E2.m1.2.2.1"></and><apply id="S2.E2.m1.2.2.1.1b.cmml" xref="S2.E2.m1.2.2.1"><eq id="S2.E2.m1.2.2.1.1.6.cmml" xref="S2.E2.m1.2.2.1.1.6"></eq><apply id="S2.E2.m1.2.2.1.1.5.cmml" xref="S2.E2.m1.2.2.1.1.5"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.5.1.cmml" xref="S2.E2.m1.2.2.1.1.5">subscript</csymbol><apply id="S2.E2.m1.2.2.1.1.5.2.cmml" xref="S2.E2.m1.2.2.1.1.5.2"><ci id="S2.E2.m1.2.2.1.1.5.2.1.cmml" xref="S2.E2.m1.2.2.1.1.5.2.1">^</ci><ci id="S2.E2.m1.2.2.1.1.5.2.2.cmml" xref="S2.E2.m1.2.2.1.1.5.2.2">ğ‘</ci></apply><ci id="S2.E2.m1.2.2.1.1.5.3.cmml" xref="S2.E2.m1.2.2.1.1.5.3">ğ‘–</ci></apply><apply id="S2.E2.m1.2.2.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.2"><times id="S2.E2.m1.2.2.1.1.2.3.cmml" xref="S2.E2.m1.2.2.1.1.2.3"></times><ci id="S2.E2.m1.2.2.1.1.2.4.cmml" xref="S2.E2.m1.2.2.1.1.2.4">ğ‘“</ci><vector id="S2.E2.m1.2.2.1.1.2.2.3.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2"><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.E2.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.2.2">ğ‘</ci><ci id="S2.E2.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S2.E2.m1.2.2.1.1.2.2.2.2.3">ğ‘–</ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ğœƒ</ci></vector></apply></apply><apply id="S2.E2.m1.2.2.1.1c.cmml" xref="S2.E2.m1.2.2.1"><eq id="S2.E2.m1.2.2.1.1.7.cmml" xref="S2.E2.m1.2.2.1.1.7"></eq><share href="#S2.E2.m1.2.2.1.1.2.cmml" id="S2.E2.m1.2.2.1.1d.cmml" xref="S2.E2.m1.2.2.1"></share><apply id="S2.E2.m1.2.2.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3"><times id="S2.E2.m1.2.2.1.1.3.2.cmml" xref="S2.E2.m1.2.2.1.1.3.2"></times><apply id="S2.E2.m1.2.2.1.1.3.3.cmml" xref="S2.E2.m1.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.3.3.1.cmml" xref="S2.E2.m1.2.2.1.1.3.3">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.3.3.2.cmml" xref="S2.E2.m1.2.2.1.1.3.3.2">ğ¸</ci><ci id="S2.E2.m1.2.2.1.1.3.3.3.cmml" xref="S2.E2.m1.2.2.1.1.3.3.3">c</ci></apply><apply id="S2.E2.m1.2.2.1.1.3.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1"><times id="S2.E2.m1.2.2.1.1.3.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.3"></times><apply id="S2.E2.m1.2.2.1.1.3.1.1.1.4.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.4"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.3.1.1.1.4.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.4">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.4.2.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.4.2">ğ¸</ci><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.4.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.4.3">m</ci></apply><interval closure="open" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2"><apply id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1"><times id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.2"></times><apply id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.2">ğ¸</ci><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.3.3">v</ci></apply><apply id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2"><times id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.2"></times><apply id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.2.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.2">ğ¸</ci><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.3.3">q</ci></apply><apply id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.2">ğ‘</ci><ci id="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3.1.1.1.2.2.2.1.1.1.3">ğ‘–</ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\hat{a}_{i}=f(v_{i},q_{i};\theta)=E_{\mathrm{c}}(E_{\mathrm{m}}(E_{\mathrm{v}}(v_{i}),E_{\mathrm{q}}(q_{i}))).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.4" class="ltx_p">Furthermore, the success <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> of Transformer in natural language processing has given rise to a new paradigm for VQA. Multiple studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> regard the image region (patch) and the question word as the same type of tokens and apply Transformer to learn multimodal representations. In other words, they leverage <math id="S2.p5.1.m1.1" class="ltx_Math" alttext="\mathrm{Transformer}" display="inline"><semantics id="S2.p5.1.m1.1a"><mi id="S2.p5.1.m1.1.1" xref="S2.p5.1.m1.1.1.cmml">Transformer</mi><annotation-xml encoding="MathML-Content" id="S2.p5.1.m1.1b"><ci id="S2.p5.1.m1.1.1.cmml" xref="S2.p5.1.m1.1.1">Transformer</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">\mathrm{Transformer}</annotation></semantics></math> to replace <math id="S2.p5.2.m2.1" class="ltx_Math" alttext="E_{\mathrm{q}}" display="inline"><semantics id="S2.p5.2.m2.1a"><msub id="S2.p5.2.m2.1.1" xref="S2.p5.2.m2.1.1.cmml"><mi id="S2.p5.2.m2.1.1.2" xref="S2.p5.2.m2.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S2.p5.2.m2.1.1.3" xref="S2.p5.2.m2.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p5.2.m2.1b"><apply id="S2.p5.2.m2.1.1.cmml" xref="S2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p5.2.m2.1.1.1.cmml" xref="S2.p5.2.m2.1.1">subscript</csymbol><ci id="S2.p5.2.m2.1.1.2.cmml" xref="S2.p5.2.m2.1.1.2">ğ¸</ci><ci id="S2.p5.2.m2.1.1.3.cmml" xref="S2.p5.2.m2.1.1.3">q</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.2.m2.1c">E_{\mathrm{q}}</annotation></semantics></math>, <math id="S2.p5.3.m3.1" class="ltx_Math" alttext="E_{\mathrm{v}}" display="inline"><semantics id="S2.p5.3.m3.1a"><msub id="S2.p5.3.m3.1.1" xref="S2.p5.3.m3.1.1.cmml"><mi id="S2.p5.3.m3.1.1.2" xref="S2.p5.3.m3.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S2.p5.3.m3.1.1.3" xref="S2.p5.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p5.3.m3.1b"><apply id="S2.p5.3.m3.1.1.cmml" xref="S2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p5.3.m3.1.1.1.cmml" xref="S2.p5.3.m3.1.1">subscript</csymbol><ci id="S2.p5.3.m3.1.1.2.cmml" xref="S2.p5.3.m3.1.1.2">ğ¸</ci><ci id="S2.p5.3.m3.1.1.3.cmml" xref="S2.p5.3.m3.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.3.m3.1c">E_{\mathrm{v}}</annotation></semantics></math> and <math id="S2.p5.4.m4.1" class="ltx_Math" alttext="E_{\mathrm{m}}" display="inline"><semantics id="S2.p5.4.m4.1a"><msub id="S2.p5.4.m4.1.1" xref="S2.p5.4.m4.1.1.cmml"><mi id="S2.p5.4.m4.1.1.2" xref="S2.p5.4.m4.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S2.p5.4.m4.1.1.3" xref="S2.p5.4.m4.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p5.4.m4.1b"><apply id="S2.p5.4.m4.1.1.cmml" xref="S2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p5.4.m4.1.1.1.cmml" xref="S2.p5.4.m4.1.1">subscript</csymbol><ci id="S2.p5.4.m4.1.1.2.cmml" xref="S2.p5.4.m4.1.1.2">ğ¸</ci><ci id="S2.p5.4.m4.1.1.3.cmml" xref="S2.p5.4.m4.1.1.3">m</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.4.m4.1c">E_{\mathrm{m}}</annotation></semantics></math>. This paradigm can be denoted as follows:</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_math_unparsed" alttext="\hat{a}_{i}=f(v_{i},q_{i};\theta)=E_{\mathrm{c}}(\mathrm{Transformer}(\bm{v}_{i}||\bm{q}_{i}))," display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1b"><msub id="S2.E3.m1.1.2"><mover accent="true" id="S2.E3.m1.1.2.2"><mi id="S2.E3.m1.1.2.2.2">a</mi><mo id="S2.E3.m1.1.2.2.1">^</mo></mover><mi id="S2.E3.m1.1.2.3">i</mi></msub><mo id="S2.E3.m1.1.3">=</mo><mi id="S2.E3.m1.1.4">f</mi><mrow id="S2.E3.m1.1.5"><mo stretchy="false" id="S2.E3.m1.1.5.1">(</mo><msub id="S2.E3.m1.1.5.2"><mi id="S2.E3.m1.1.5.2.2">v</mi><mi id="S2.E3.m1.1.5.2.3">i</mi></msub><mo id="S2.E3.m1.1.5.3">,</mo><msub id="S2.E3.m1.1.5.4"><mi id="S2.E3.m1.1.5.4.2">q</mi><mi id="S2.E3.m1.1.5.4.3">i</mi></msub><mo id="S2.E3.m1.1.5.5">;</mo><mi id="S2.E3.m1.1.1">Î¸</mi><mo stretchy="false" id="S2.E3.m1.1.5.6">)</mo></mrow><mo id="S2.E3.m1.1.6">=</mo><msub id="S2.E3.m1.1.7"><mi id="S2.E3.m1.1.7.2">E</mi><mi mathvariant="normal" id="S2.E3.m1.1.7.3">c</mi></msub><mrow id="S2.E3.m1.1.8"><mo stretchy="false" id="S2.E3.m1.1.8.1">(</mo><mi id="S2.E3.m1.1.8.2">Transformer</mi><mrow id="S2.E3.m1.1.8.3"><mo stretchy="false" id="S2.E3.m1.1.8.3.1">(</mo><msub id="S2.E3.m1.1.8.3.2"><mi id="S2.E3.m1.1.8.3.2.2">ğ’—</mi><mi id="S2.E3.m1.1.8.3.2.3">i</mi></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E3.m1.1.8.3.3">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.E3.m1.1.8.3.4">|</mo><msub id="S2.E3.m1.1.8.3.5"><mi id="S2.E3.m1.1.8.3.5.2">ğ’’</mi><mi id="S2.E3.m1.1.8.3.5.3">i</mi></msub><mo stretchy="false" id="S2.E3.m1.1.8.3.6">)</mo></mrow><mo stretchy="false" id="S2.E3.m1.1.8.4">)</mo></mrow><mo id="S2.E3.m1.1.9">,</mo></mrow><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\hat{a}_{i}=f(v_{i},q_{i};\theta)=E_{\mathrm{c}}(\mathrm{Transformer}(\bm{v}_{i}||\bm{q}_{i})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.p5.7" class="ltx_p">where <math id="S2.p5.5.m1.1" class="ltx_Math" alttext="\bm{v}_{i}" display="inline"><semantics id="S2.p5.5.m1.1a"><msub id="S2.p5.5.m1.1.1" xref="S2.p5.5.m1.1.1.cmml"><mi id="S2.p5.5.m1.1.1.2" xref="S2.p5.5.m1.1.1.2.cmml">ğ’—</mi><mi id="S2.p5.5.m1.1.1.3" xref="S2.p5.5.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p5.5.m1.1b"><apply id="S2.p5.5.m1.1.1.cmml" xref="S2.p5.5.m1.1.1"><csymbol cd="ambiguous" id="S2.p5.5.m1.1.1.1.cmml" xref="S2.p5.5.m1.1.1">subscript</csymbol><ci id="S2.p5.5.m1.1.1.2.cmml" xref="S2.p5.5.m1.1.1.2">ğ’—</ci><ci id="S2.p5.5.m1.1.1.3.cmml" xref="S2.p5.5.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.5.m1.1c">\bm{v}_{i}</annotation></semantics></math> is the representation of regions or patches, <math id="S2.p5.6.m2.1" class="ltx_Math" alttext="\bm{q}_{i}" display="inline"><semantics id="S2.p5.6.m2.1a"><msub id="S2.p5.6.m2.1.1" xref="S2.p5.6.m2.1.1.cmml"><mi id="S2.p5.6.m2.1.1.2" xref="S2.p5.6.m2.1.1.2.cmml">ğ’’</mi><mi id="S2.p5.6.m2.1.1.3" xref="S2.p5.6.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p5.6.m2.1b"><apply id="S2.p5.6.m2.1.1.cmml" xref="S2.p5.6.m2.1.1"><csymbol cd="ambiguous" id="S2.p5.6.m2.1.1.1.cmml" xref="S2.p5.6.m2.1.1">subscript</csymbol><ci id="S2.p5.6.m2.1.1.2.cmml" xref="S2.p5.6.m2.1.1.2">ğ’’</ci><ci id="S2.p5.6.m2.1.1.3.cmml" xref="S2.p5.6.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.6.m2.1c">\bm{q}_{i}</annotation></semantics></math> denotes the embeddings of words, and <math id="S2.p5.7.m3.1" class="ltx_math_unparsed" alttext="||" display="inline"><semantics id="S2.p5.7.m3.1a"><mrow id="S2.p5.7.m3.1b"><mo fence="false" rspace="0.167em" stretchy="false" id="S2.p5.7.m3.1.1">|</mo><mo fence="false" stretchy="false" id="S2.p5.7.m3.1.2">|</mo></mrow><annotation encoding="application/x-tex" id="S2.p5.7.m3.1c">||</annotation></semantics></math> represents the concatenation. This paradigm is always employed in large pre-trained vision-language models, which we will discuss separately in Section <a href="#S6" title="6 Vision-and-Language Pre-training Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Here, our primary focus will be on small models.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.2" class="ltx_p"><span id="S2.p6.2.3" class="ltx_text ltx_font_bold">Robust VQA.</span> Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> found that the generic methods are apt to exploit the training bias excessively rather than learn proper behaviors. For example, they may learn the spurious connections between critical words of questions and answers, such as â€œwhatâ€, â€œsportsâ€ and â€œtennisâ€. <span id="S2.p6.2.2" class="ltx_text" style="color:#000000;"> Therefore, the generic methods predict answers mainly relying on one of <math id="S2.p6.1.1.m1.1" class="ltx_Math" alttext="E_{\mathrm{v}}(v_{i})" display="inline"><semantics id="S2.p6.1.1.m1.1a"><mrow id="S2.p6.1.1.m1.1.1" xref="S2.p6.1.1.m1.1.1.cmml"><msub id="S2.p6.1.1.m1.1.1.3" xref="S2.p6.1.1.m1.1.1.3.cmml"><mi mathcolor="#000000" id="S2.p6.1.1.m1.1.1.3.2" xref="S2.p6.1.1.m1.1.1.3.2.cmml">E</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.p6.1.1.m1.1.1.3.3" xref="S2.p6.1.1.m1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S2.p6.1.1.m1.1.1.2" xref="S2.p6.1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S2.p6.1.1.m1.1.1.1.1" xref="S2.p6.1.1.m1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.p6.1.1.m1.1.1.1.1.2" xref="S2.p6.1.1.m1.1.1.1.1.1.cmml">(</mo><msub id="S2.p6.1.1.m1.1.1.1.1.1" xref="S2.p6.1.1.m1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.p6.1.1.m1.1.1.1.1.1.2" xref="S2.p6.1.1.m1.1.1.1.1.1.2.cmml">v</mi><mi mathcolor="#000000" id="S2.p6.1.1.m1.1.1.1.1.1.3" xref="S2.p6.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.p6.1.1.m1.1.1.1.1.3" xref="S2.p6.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.1.1.m1.1b"><apply id="S2.p6.1.1.m1.1.1.cmml" xref="S2.p6.1.1.m1.1.1"><times id="S2.p6.1.1.m1.1.1.2.cmml" xref="S2.p6.1.1.m1.1.1.2"></times><apply id="S2.p6.1.1.m1.1.1.3.cmml" xref="S2.p6.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.p6.1.1.m1.1.1.3.1.cmml" xref="S2.p6.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.p6.1.1.m1.1.1.3.2.cmml" xref="S2.p6.1.1.m1.1.1.3.2">ğ¸</ci><ci id="S2.p6.1.1.m1.1.1.3.3.cmml" xref="S2.p6.1.1.m1.1.1.3.3">v</ci></apply><apply id="S2.p6.1.1.m1.1.1.1.1.1.cmml" xref="S2.p6.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p6.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.p6.1.1.m1.1.1.1.1">subscript</csymbol><ci id="S2.p6.1.1.m1.1.1.1.1.1.2.cmml" xref="S2.p6.1.1.m1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.p6.1.1.m1.1.1.1.1.1.3.cmml" xref="S2.p6.1.1.m1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.1.m1.1c">E_{\mathrm{v}}(v_{i})</annotation></semantics></math> and <math id="S2.p6.2.2.m2.1" class="ltx_Math" alttext="E_{\mathrm{q}}(q_{i})" display="inline"><semantics id="S2.p6.2.2.m2.1a"><mrow id="S2.p6.2.2.m2.1.1" xref="S2.p6.2.2.m2.1.1.cmml"><msub id="S2.p6.2.2.m2.1.1.3" xref="S2.p6.2.2.m2.1.1.3.cmml"><mi mathcolor="#000000" id="S2.p6.2.2.m2.1.1.3.2" xref="S2.p6.2.2.m2.1.1.3.2.cmml">E</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.p6.2.2.m2.1.1.3.3" xref="S2.p6.2.2.m2.1.1.3.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S2.p6.2.2.m2.1.1.2" xref="S2.p6.2.2.m2.1.1.2.cmml">â€‹</mo><mrow id="S2.p6.2.2.m2.1.1.1.1" xref="S2.p6.2.2.m2.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.p6.2.2.m2.1.1.1.1.2" xref="S2.p6.2.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S2.p6.2.2.m2.1.1.1.1.1" xref="S2.p6.2.2.m2.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.p6.2.2.m2.1.1.1.1.1.2" xref="S2.p6.2.2.m2.1.1.1.1.1.2.cmml">q</mi><mi mathcolor="#000000" id="S2.p6.2.2.m2.1.1.1.1.1.3" xref="S2.p6.2.2.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.p6.2.2.m2.1.1.1.1.3" xref="S2.p6.2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.2.2.m2.1b"><apply id="S2.p6.2.2.m2.1.1.cmml" xref="S2.p6.2.2.m2.1.1"><times id="S2.p6.2.2.m2.1.1.2.cmml" xref="S2.p6.2.2.m2.1.1.2"></times><apply id="S2.p6.2.2.m2.1.1.3.cmml" xref="S2.p6.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.p6.2.2.m2.1.1.3.1.cmml" xref="S2.p6.2.2.m2.1.1.3">subscript</csymbol><ci id="S2.p6.2.2.m2.1.1.3.2.cmml" xref="S2.p6.2.2.m2.1.1.3.2">ğ¸</ci><ci id="S2.p6.2.2.m2.1.1.3.3.cmml" xref="S2.p6.2.2.m2.1.1.3.3">q</ci></apply><apply id="S2.p6.2.2.m2.1.1.1.1.1.cmml" xref="S2.p6.2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.p6.2.2.m2.1.1.1.1.1.1.cmml" xref="S2.p6.2.2.m2.1.1.1.1">subscript</csymbol><ci id="S2.p6.2.2.m2.1.1.1.1.1.2.cmml" xref="S2.p6.2.2.m2.1.1.1.1.1.2">ğ‘</ci><ci id="S2.p6.2.2.m2.1.1.1.1.1.3.cmml" xref="S2.p6.2.2.m2.1.1.1.1.1.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.2.m2.1c">E_{\mathrm{q}}(q_{i})</annotation></semantics></math>. This can be formulated as follows:</span></p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E4.m1.1" class="ltx_Math" alttext="\displaystyle\hat{a}_{i}=f_{\mathrm{l}}(v_{i},q_{i};\theta_{\mathrm{l}})=E_{\mathrm{c}}(E_{\mathrm{q}}(q_{i}))," display="inline"><semantics id="S2.E4.m1.1a"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.6" xref="S2.E4.m1.1.1.1.1.6.cmml"><mover accent="true" id="S2.E4.m1.1.1.1.1.6.2" xref="S2.E4.m1.1.1.1.1.6.2.cmml"><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.6.2.2" xref="S2.E4.m1.1.1.1.1.6.2.2.cmml">a</mi><mo mathcolor="#000000" id="S2.E4.m1.1.1.1.1.6.2.1" xref="S2.E4.m1.1.1.1.1.6.2.1.cmml">^</mo></mover><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.6.3" xref="S2.E4.m1.1.1.1.1.6.3.cmml">i</mi></msub><mo mathcolor="#000000" id="S2.E4.m1.1.1.1.1.7" xref="S2.E4.m1.1.1.1.1.7.cmml">=</mo><mrow id="S2.E4.m1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.3.cmml"><msub id="S2.E4.m1.1.1.1.1.3.5" xref="S2.E4.m1.1.1.1.1.3.5.cmml"><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.3.5.2" xref="S2.E4.m1.1.1.1.1.3.5.2.cmml">f</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.E4.m1.1.1.1.1.3.5.3" xref="S2.E4.m1.1.1.1.1.3.5.3.cmml">l</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.3.4" xref="S2.E4.m1.1.1.1.1.3.4.cmml">â€‹</mo><mrow id="S2.E4.m1.1.1.1.1.3.3.3" xref="S2.E4.m1.1.1.1.1.3.3.4.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.1.1.1.1.3.3.3.4" xref="S2.E4.m1.1.1.1.1.3.3.4.cmml">(</mo><msub id="S2.E4.m1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo mathcolor="#000000" id="S2.E4.m1.1.1.1.1.3.3.3.5" xref="S2.E4.m1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S2.E4.m1.1.1.1.1.2.2.2.2" xref="S2.E4.m1.1.1.1.1.2.2.2.2.cmml"><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.2.2.2.2.2" xref="S2.E4.m1.1.1.1.1.2.2.2.2.2.cmml">q</mi><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.2.2.2.2.3" xref="S2.E4.m1.1.1.1.1.2.2.2.2.3.cmml">i</mi></msub><mo mathcolor="#000000" id="S2.E4.m1.1.1.1.1.3.3.3.6" xref="S2.E4.m1.1.1.1.1.3.3.4.cmml">;</mo><msub id="S2.E4.m1.1.1.1.1.3.3.3.3" xref="S2.E4.m1.1.1.1.1.3.3.3.3.cmml"><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.3.3.3.3.2" xref="S2.E4.m1.1.1.1.1.3.3.3.3.2.cmml">Î¸</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.E4.m1.1.1.1.1.3.3.3.3.3" xref="S2.E4.m1.1.1.1.1.3.3.3.3.3.cmml">l</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.1.1.1.1.3.3.3.7" xref="S2.E4.m1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" id="S2.E4.m1.1.1.1.1.8" xref="S2.E4.m1.1.1.1.1.8.cmml">=</mo><mrow id="S2.E4.m1.1.1.1.1.4" xref="S2.E4.m1.1.1.1.1.4.cmml"><msub id="S2.E4.m1.1.1.1.1.4.3" xref="S2.E4.m1.1.1.1.1.4.3.cmml"><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.4.3.2" xref="S2.E4.m1.1.1.1.1.4.3.2.cmml">E</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.E4.m1.1.1.1.1.4.3.3" xref="S2.E4.m1.1.1.1.1.4.3.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.4.2" xref="S2.E4.m1.1.1.1.1.4.2.cmml">â€‹</mo><mrow id="S2.E4.m1.1.1.1.1.4.1.1" xref="S2.E4.m1.1.1.1.1.4.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.1.1.1.1.4.1.1.2" xref="S2.E4.m1.1.1.1.1.4.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.1.1.1.1.4.1.1.1" xref="S2.E4.m1.1.1.1.1.4.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.4.1.1.1.3" xref="S2.E4.m1.1.1.1.1.4.1.1.1.3.cmml"><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.4.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.4.1.1.1.3.2.cmml">E</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.E4.m1.1.1.1.1.4.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.4.1.1.1.3.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.1.4.1.1.1.2" xref="S2.E4.m1.1.1.1.1.4.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.2.cmml">q</mi><mi mathcolor="#000000" id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" stretchy="false" id="S2.E4.m1.1.1.1.1.4.1.1.3" xref="S2.E4.m1.1.1.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo mathcolor="#000000" id="S2.E4.m1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><and id="S2.E4.m1.1.1.1.1a.cmml" xref="S2.E4.m1.1.1.1"></and><apply id="S2.E4.m1.1.1.1.1b.cmml" xref="S2.E4.m1.1.1.1"><eq id="S2.E4.m1.1.1.1.1.7.cmml" xref="S2.E4.m1.1.1.1.1.7"></eq><apply id="S2.E4.m1.1.1.1.1.6.cmml" xref="S2.E4.m1.1.1.1.1.6"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.6.1.cmml" xref="S2.E4.m1.1.1.1.1.6">subscript</csymbol><apply id="S2.E4.m1.1.1.1.1.6.2.cmml" xref="S2.E4.m1.1.1.1.1.6.2"><ci id="S2.E4.m1.1.1.1.1.6.2.1.cmml" xref="S2.E4.m1.1.1.1.1.6.2.1">^</ci><ci id="S2.E4.m1.1.1.1.1.6.2.2.cmml" xref="S2.E4.m1.1.1.1.1.6.2.2">ğ‘</ci></apply><ci id="S2.E4.m1.1.1.1.1.6.3.cmml" xref="S2.E4.m1.1.1.1.1.6.3">ğ‘–</ci></apply><apply id="S2.E4.m1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.3"><times id="S2.E4.m1.1.1.1.1.3.4.cmml" xref="S2.E4.m1.1.1.1.1.3.4"></times><apply id="S2.E4.m1.1.1.1.1.3.5.cmml" xref="S2.E4.m1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.3.5.1.cmml" xref="S2.E4.m1.1.1.1.1.3.5">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.3.5.2.cmml" xref="S2.E4.m1.1.1.1.1.3.5.2">ğ‘“</ci><ci id="S2.E4.m1.1.1.1.1.3.5.3.cmml" xref="S2.E4.m1.1.1.1.1.3.5.3">l</ci></apply><vector id="S2.E4.m1.1.1.1.1.3.3.4.cmml" xref="S2.E4.m1.1.1.1.1.3.3.3"><apply id="S2.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.E4.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.E4.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E4.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E4.m1.1.1.1.1.2.2.2.2.2">ğ‘</ci><ci id="S2.E4.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.E4.m1.1.1.1.1.2.2.2.2.3">ğ‘–</ci></apply><apply id="S2.E4.m1.1.1.1.1.3.3.3.3.cmml" xref="S2.E4.m1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E4.m1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E4.m1.1.1.1.1.3.3.3.3.2">ğœƒ</ci><ci id="S2.E4.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S2.E4.m1.1.1.1.1.3.3.3.3.3">l</ci></apply></vector></apply></apply><apply id="S2.E4.m1.1.1.1.1c.cmml" xref="S2.E4.m1.1.1.1"><eq id="S2.E4.m1.1.1.1.1.8.cmml" xref="S2.E4.m1.1.1.1.1.8"></eq><share href="#S2.E4.m1.1.1.1.1.3.cmml" id="S2.E4.m1.1.1.1.1d.cmml" xref="S2.E4.m1.1.1.1"></share><apply id="S2.E4.m1.1.1.1.1.4.cmml" xref="S2.E4.m1.1.1.1.1.4"><times id="S2.E4.m1.1.1.1.1.4.2.cmml" xref="S2.E4.m1.1.1.1.1.4.2"></times><apply id="S2.E4.m1.1.1.1.1.4.3.cmml" xref="S2.E4.m1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.4.3.1.cmml" xref="S2.E4.m1.1.1.1.1.4.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.4.3.2.cmml" xref="S2.E4.m1.1.1.1.1.4.3.2">ğ¸</ci><ci id="S2.E4.m1.1.1.1.1.4.3.3.cmml" xref="S2.E4.m1.1.1.1.1.4.3.3">c</ci></apply><apply id="S2.E4.m1.1.1.1.1.4.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1"><times id="S2.E4.m1.1.1.1.1.4.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.2"></times><apply id="S2.E4.m1.1.1.1.1.4.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.4.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.4.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.3.2">ğ¸</ci><ci id="S2.E4.m1.1.1.1.1.4.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.3.3">q</ci></apply><apply id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.4.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\displaystyle\hat{a}_{i}=f_{\mathrm{l}}(v_{i},q_{i};\theta_{\mathrm{l}})=E_{\mathrm{c}}(E_{\mathrm{q}}(q_{i})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S2.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E5.m1.1" class="ltx_Math" alttext="\displaystyle\hat{a}_{i}=f_{\mathrm{v}}(v_{i},q_{i};\theta_{\mathrm{v}})=E_{\mathrm{c}}(E_{\mathrm{v}}(v_{i}))," display="inline"><semantics id="S2.E5.m1.1a"><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><mrow id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml"><msub id="S2.E5.m1.1.1.1.1.6" xref="S2.E5.m1.1.1.1.1.6.cmml"><mover accent="true" id="S2.E5.m1.1.1.1.1.6.2" xref="S2.E5.m1.1.1.1.1.6.2.cmml"><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.6.2.2" xref="S2.E5.m1.1.1.1.1.6.2.2.cmml">a</mi><mo mathcolor="#000000" id="S2.E5.m1.1.1.1.1.6.2.1" xref="S2.E5.m1.1.1.1.1.6.2.1.cmml">^</mo></mover><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.6.3" xref="S2.E5.m1.1.1.1.1.6.3.cmml">i</mi></msub><mo mathcolor="#000000" id="S2.E5.m1.1.1.1.1.7" xref="S2.E5.m1.1.1.1.1.7.cmml">=</mo><mrow id="S2.E5.m1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.3.cmml"><msub id="S2.E5.m1.1.1.1.1.3.5" xref="S2.E5.m1.1.1.1.1.3.5.cmml"><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.3.5.2" xref="S2.E5.m1.1.1.1.1.3.5.2.cmml">f</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.E5.m1.1.1.1.1.3.5.3" xref="S2.E5.m1.1.1.1.1.3.5.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.1.3.4" xref="S2.E5.m1.1.1.1.1.3.4.cmml">â€‹</mo><mrow id="S2.E5.m1.1.1.1.1.3.3.3" xref="S2.E5.m1.1.1.1.1.3.3.4.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.1.1.1.1.3.3.3.4" xref="S2.E5.m1.1.1.1.1.3.3.4.cmml">(</mo><msub id="S2.E5.m1.1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo mathcolor="#000000" id="S2.E5.m1.1.1.1.1.3.3.3.5" xref="S2.E5.m1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S2.E5.m1.1.1.1.1.2.2.2.2" xref="S2.E5.m1.1.1.1.1.2.2.2.2.cmml"><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.2.2.2.2.2" xref="S2.E5.m1.1.1.1.1.2.2.2.2.2.cmml">q</mi><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.2.2.2.2.3" xref="S2.E5.m1.1.1.1.1.2.2.2.2.3.cmml">i</mi></msub><mo mathcolor="#000000" id="S2.E5.m1.1.1.1.1.3.3.3.6" xref="S2.E5.m1.1.1.1.1.3.3.4.cmml">;</mo><msub id="S2.E5.m1.1.1.1.1.3.3.3.3" xref="S2.E5.m1.1.1.1.1.3.3.3.3.cmml"><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.3.3.3.3.2" xref="S2.E5.m1.1.1.1.1.3.3.3.3.2.cmml">Î¸</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.E5.m1.1.1.1.1.3.3.3.3.3" xref="S2.E5.m1.1.1.1.1.3.3.3.3.3.cmml">v</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.1.1.1.1.3.3.3.7" xref="S2.E5.m1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" id="S2.E5.m1.1.1.1.1.8" xref="S2.E5.m1.1.1.1.1.8.cmml">=</mo><mrow id="S2.E5.m1.1.1.1.1.4" xref="S2.E5.m1.1.1.1.1.4.cmml"><msub id="S2.E5.m1.1.1.1.1.4.3" xref="S2.E5.m1.1.1.1.1.4.3.cmml"><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.4.3.2" xref="S2.E5.m1.1.1.1.1.4.3.2.cmml">E</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.E5.m1.1.1.1.1.4.3.3" xref="S2.E5.m1.1.1.1.1.4.3.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.1.4.2" xref="S2.E5.m1.1.1.1.1.4.2.cmml">â€‹</mo><mrow id="S2.E5.m1.1.1.1.1.4.1.1" xref="S2.E5.m1.1.1.1.1.4.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.1.1.1.1.4.1.1.2" xref="S2.E5.m1.1.1.1.1.4.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.4.1.1.1" xref="S2.E5.m1.1.1.1.1.4.1.1.1.cmml"><msub id="S2.E5.m1.1.1.1.1.4.1.1.1.3" xref="S2.E5.m1.1.1.1.1.4.1.1.1.3.cmml"><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.4.1.1.1.3.2" xref="S2.E5.m1.1.1.1.1.4.1.1.1.3.2.cmml">E</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.E5.m1.1.1.1.1.4.1.1.1.3.3" xref="S2.E5.m1.1.1.1.1.4.1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.1.4.1.1.1.2" xref="S2.E5.m1.1.1.1.1.4.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.2.cmml">v</mi><mi mathcolor="#000000" id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" stretchy="false" id="S2.E5.m1.1.1.1.1.4.1.1.3" xref="S2.E5.m1.1.1.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo mathcolor="#000000" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.1b"><apply id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><and id="S2.E5.m1.1.1.1.1a.cmml" xref="S2.E5.m1.1.1.1"></and><apply id="S2.E5.m1.1.1.1.1b.cmml" xref="S2.E5.m1.1.1.1"><eq id="S2.E5.m1.1.1.1.1.7.cmml" xref="S2.E5.m1.1.1.1.1.7"></eq><apply id="S2.E5.m1.1.1.1.1.6.cmml" xref="S2.E5.m1.1.1.1.1.6"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.6.1.cmml" xref="S2.E5.m1.1.1.1.1.6">subscript</csymbol><apply id="S2.E5.m1.1.1.1.1.6.2.cmml" xref="S2.E5.m1.1.1.1.1.6.2"><ci id="S2.E5.m1.1.1.1.1.6.2.1.cmml" xref="S2.E5.m1.1.1.1.1.6.2.1">^</ci><ci id="S2.E5.m1.1.1.1.1.6.2.2.cmml" xref="S2.E5.m1.1.1.1.1.6.2.2">ğ‘</ci></apply><ci id="S2.E5.m1.1.1.1.1.6.3.cmml" xref="S2.E5.m1.1.1.1.1.6.3">ğ‘–</ci></apply><apply id="S2.E5.m1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.3"><times id="S2.E5.m1.1.1.1.1.3.4.cmml" xref="S2.E5.m1.1.1.1.1.3.4"></times><apply id="S2.E5.m1.1.1.1.1.3.5.cmml" xref="S2.E5.m1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.5.1.cmml" xref="S2.E5.m1.1.1.1.1.3.5">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.3.5.2.cmml" xref="S2.E5.m1.1.1.1.1.3.5.2">ğ‘“</ci><ci id="S2.E5.m1.1.1.1.1.3.5.3.cmml" xref="S2.E5.m1.1.1.1.1.3.5.3">v</ci></apply><vector id="S2.E5.m1.1.1.1.1.3.3.4.cmml" xref="S2.E5.m1.1.1.1.1.3.3.3"><apply id="S2.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.E5.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E5.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E5.m1.1.1.1.1.2.2.2.2.2">ğ‘</ci><ci id="S2.E5.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.E5.m1.1.1.1.1.2.2.2.2.3">ğ‘–</ci></apply><apply id="S2.E5.m1.1.1.1.1.3.3.3.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E5.m1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E5.m1.1.1.1.1.3.3.3.3.2">ğœƒ</ci><ci id="S2.E5.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S2.E5.m1.1.1.1.1.3.3.3.3.3">v</ci></apply></vector></apply></apply><apply id="S2.E5.m1.1.1.1.1c.cmml" xref="S2.E5.m1.1.1.1"><eq id="S2.E5.m1.1.1.1.1.8.cmml" xref="S2.E5.m1.1.1.1.1.8"></eq><share href="#S2.E5.m1.1.1.1.1.3.cmml" id="S2.E5.m1.1.1.1.1d.cmml" xref="S2.E5.m1.1.1.1"></share><apply id="S2.E5.m1.1.1.1.1.4.cmml" xref="S2.E5.m1.1.1.1.1.4"><times id="S2.E5.m1.1.1.1.1.4.2.cmml" xref="S2.E5.m1.1.1.1.1.4.2"></times><apply id="S2.E5.m1.1.1.1.1.4.3.cmml" xref="S2.E5.m1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.4.3.1.cmml" xref="S2.E5.m1.1.1.1.1.4.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.4.3.2.cmml" xref="S2.E5.m1.1.1.1.1.4.3.2">ğ¸</ci><ci id="S2.E5.m1.1.1.1.1.4.3.3.cmml" xref="S2.E5.m1.1.1.1.1.4.3.3">c</ci></apply><apply id="S2.E5.m1.1.1.1.1.4.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1"><times id="S2.E5.m1.1.1.1.1.4.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.2"></times><apply id="S2.E5.m1.1.1.1.1.4.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.4.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.3">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.4.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.3.2">ğ¸</ci><ci id="S2.E5.m1.1.1.1.1.4.1.1.1.3.3.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.3.3">v</ci></apply><apply id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1">subscript</csymbol><ci id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.4.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.1c">\displaystyle\hat{a}_{i}=f_{\mathrm{v}}(v_{i},q_{i};\theta_{\mathrm{v}})=E_{\mathrm{c}}(E_{\mathrm{v}}(v_{i})),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.p6.4" class="ltx_p"><span id="S2.p6.4.2" class="ltx_text" style="color:#000000;">where <math id="S2.p6.3.1.m1.1" class="ltx_Math" alttext="f_{\mathrm{l}}" display="inline"><semantics id="S2.p6.3.1.m1.1a"><msub id="S2.p6.3.1.m1.1.1" xref="S2.p6.3.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S2.p6.3.1.m1.1.1.2" xref="S2.p6.3.1.m1.1.1.2.cmml">f</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.p6.3.1.m1.1.1.3" xref="S2.p6.3.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.3.1.m1.1b"><apply id="S2.p6.3.1.m1.1.1.cmml" xref="S2.p6.3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p6.3.1.m1.1.1.1.cmml" xref="S2.p6.3.1.m1.1.1">subscript</csymbol><ci id="S2.p6.3.1.m1.1.1.2.cmml" xref="S2.p6.3.1.m1.1.1.2">ğ‘“</ci><ci id="S2.p6.3.1.m1.1.1.3.cmml" xref="S2.p6.3.1.m1.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.3.1.m1.1c">f_{\mathrm{l}}</annotation></semantics></math> refers to the language bias learning, and <math id="S2.p6.4.2.m2.1" class="ltx_Math" alttext="f_{\mathrm{v}}" display="inline"><semantics id="S2.p6.4.2.m2.1a"><msub id="S2.p6.4.2.m2.1.1" xref="S2.p6.4.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S2.p6.4.2.m2.1.1.2" xref="S2.p6.4.2.m2.1.1.2.cmml">f</mi><mi mathcolor="#000000" mathvariant="normal" id="S2.p6.4.2.m2.1.1.3" xref="S2.p6.4.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p6.4.2.m2.1b"><apply id="S2.p6.4.2.m2.1.1.cmml" xref="S2.p6.4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p6.4.2.m2.1.1.1.cmml" xref="S2.p6.4.2.m2.1.1">subscript</csymbol><ci id="S2.p6.4.2.m2.1.1.2.cmml" xref="S2.p6.4.2.m2.1.1.2">ğ‘“</ci><ci id="S2.p6.4.2.m2.1.1.3.cmml" xref="S2.p6.4.2.m2.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.4.2.m2.1c">f_{\mathrm{v}}</annotation></semantics></math> represents the vision bias learning. According to the investigation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, it has been found that language bias learning is more prevalent than vision bias learning. </span></p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.3" class="ltx_p">Obviously, the mentioned bias learning will cause the generic methods to obtain high ID but poor OOD performance. In other words, the methods are incapable of dealing with various changing real-world situations. In comparison, on the basis of generic methods, debiasing methods aim at mitigating bias learning to achieve robust VQA performance in both scenarios. Formally, the debiasing method primarily focuses on acquiring a strategy to eliminate the impact of <math id="S2.p7.1.m1.1" class="ltx_Math" alttext="f_{\mathrm{l}}" display="inline"><semantics id="S2.p7.1.m1.1a"><msub id="S2.p7.1.m1.1.1" xref="S2.p7.1.m1.1.1.cmml"><mi id="S2.p7.1.m1.1.1.2" xref="S2.p7.1.m1.1.1.2.cmml">f</mi><mi mathvariant="normal" id="S2.p7.1.m1.1.1.3" xref="S2.p7.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p7.1.m1.1b"><apply id="S2.p7.1.m1.1.1.cmml" xref="S2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p7.1.m1.1.1.1.cmml" xref="S2.p7.1.m1.1.1">subscript</csymbol><ci id="S2.p7.1.m1.1.1.2.cmml" xref="S2.p7.1.m1.1.1.2">ğ‘“</ci><ci id="S2.p7.1.m1.1.1.3.cmml" xref="S2.p7.1.m1.1.1.3">l</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p7.1.m1.1c">f_{\mathrm{l}}</annotation></semantics></math> or <math id="S2.p7.2.m2.1" class="ltx_Math" alttext="f_{\mathrm{v}}" display="inline"><semantics id="S2.p7.2.m2.1a"><msub id="S2.p7.2.m2.1.1" xref="S2.p7.2.m2.1.1.cmml"><mi id="S2.p7.2.m2.1.1.2" xref="S2.p7.2.m2.1.1.2.cmml">f</mi><mi mathvariant="normal" id="S2.p7.2.m2.1.1.3" xref="S2.p7.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p7.2.m2.1b"><apply id="S2.p7.2.m2.1.1.cmml" xref="S2.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p7.2.m2.1.1.1.cmml" xref="S2.p7.2.m2.1.1">subscript</csymbol><ci id="S2.p7.2.m2.1.1.2.cmml" xref="S2.p7.2.m2.1.1.2">ğ‘“</ci><ci id="S2.p7.2.m2.1.1.3.cmml" xref="S2.p7.2.m2.1.1.3">v</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p7.2.m2.1c">f_{\mathrm{v}}</annotation></semantics></math> on the <math id="S2.p7.3.m3.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.p7.3.m3.1a"><mi id="S2.p7.3.m3.1.1" xref="S2.p7.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.p7.3.m3.1b"><ci id="S2.p7.3.m3.1.1.cmml" xref="S2.p7.3.m3.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p7.3.m3.1c">f</annotation></semantics></math> described in Equation (<a href="#S2.E2" title="In 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and (<a href="#S2.E3" title="In 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Datasets</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">A substantial volume of VQA datasets has been constructed encompassing diverse perspectives. This section presents a comprehensive review of these datasets, categorizing them into two groups according to their ID and OOD settings.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">In-Distribution Setting</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">ID settings typically represent that the test splitâ€™s distribution is similar to that of the corresponding training split.</em> A variety of ID datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> have been developed from different angles and have made significant contributions to robust VQA research. These datasets can be particularly useful in evaluating the effectiveness of VQA methods, as they are specifically designed to assess various capabilities such as fine-grained detection and recognition, as well as common sense reasoning.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">VQA v1</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This dataset was developed by asking ten human subjects the same question about a real image. <span id="S3.SS1.p2.1.2" class="ltx_text" style="color:#000000;">As shown in the â€œotherâ€ question of Fig. <a href="#S2.F3" title="Figure 3 â€£ 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can see that ten annotators produce five gold answers, including â€œNoâ€, â€œOff dutyâ€, â€œGoingâ€, â€œComingâ€ and â€œGoing to garageâ€. Therefore, the question in this dataset may exist several ground-truth answers.</span></p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">VQA v1 is the first large-scale, open-ended, and free-form dataset to assess the ability of VQA methods such as fine-grained detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, and counting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. The dataset consists of 614.2K questions accompanied by 204.7K images, where the questions are classified into three primary categories according to answer types: â€œYes/Noâ€, â€œNumberâ€, and â€œOtherâ€. <span id="S3.SS1.p3.1.1" class="ltx_text" style="color:#000000;">Similar to the MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, VQA v1 is divided into six parts: training, validation, test-dev, test-standard, test-challenge, and test-reserve. The performance evaluation of test-dev, test-standard, and test-challenge can be obtained from the online server<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text" style="color:#000000;">1</span></span><a target="_blank" href="https://eval.ai/web/challenges/challenge-page/830/overview" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://eval.ai/web/challenges/challenge-page/830/overview</a></span></span></span> after uploading the answer predictions of VQA methods. The default test data for the VQA competition is test-standard, while test-challenge is used to determine the competition winners. Test-reserve is utilized to prevent possible overfitting, and the results of this split are not publicly available. This competition has significantly stimulated the advancement of the VQA community.</span></p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">However, Goyal <span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> found that VQA v1 contains strong language priors or bias. For example, the most common answer â€œtennisâ€ is the correct answer for 41% of the questions starting with â€œwhat sport isâ€. This may drive VQA methods to answer questions depending on the bias rather than grounding images.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">VQA v2</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. To alleviate the mentioned dilemma, this dataset was developed by collecting complementary images for almost every question in VQA v1. Specifically, there are two images that appear to be similar but have different answers to the question. As shown in Fig. <a href="#S3.F4" title="Figure 4 â€£ 3.1 In-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, VQA v2 adds a complementary image for the question â€œWhat sport is being played?â€ to mitigate the bias in VQA v1, where â€œbadmintonâ€ is the complemented answer. In this way, the volume of VQA v2 is almost twice that of VQA v1, containing 1.1M questions with 479K images.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2307.11471/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of balancing answer distributions in VQA v2. This dataset incorporates a complementary image that pertains to the same question in VQA v1 but has a different answer.</figcaption>
</figure>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text" style="color:#000000;">Despite achieving substantial success <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, VQA v2 still faces several issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. Firstly, the similarity between the distributions of training and test splits still exists, although the language prior is significantly weaker. Secondly, the simplicity of the question in the aforementioned dataset exacerbates the limitation stemming from the modelsâ€™ dependence on statistical biases. Lastly, the absence of annotations pertaining to question structures and contents poses difficulties in comprehending the factors that influence model behaviors.</span></p>
</div>
<div id="S3.SS1.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.p7.1" class="ltx_p"><span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_bold">GQA</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. To address the mentioned issues, this dataset was developed by leveraging the content provided by the scene graphs in Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, such as object information, attributes and relations, and structures from extensive linguistic grammar. <span id="S3.SS1.p7.1.2" class="ltx_text" style="color:#000000;">Therefore, the question is more complex compared with that in VQA v2. Unlike VQA v1 and v2, each question in this dataset only has one ground-truth answer. GQA contains 22M questions with 113K images, which is split into four parts: training (70%), validation (10%), test (10%), and challenge (10%). Analogous to VQA v2, the result on this challenge split is applied to determine the competition winners<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text" style="color:#000000;">2</span></span><a target="_blank" href="https://eval.ai/web/challenges/challenge-page/225/overview" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://eval.ai/web/challenges/challenge-page/225/overview</a></span></span></span>. </span></p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.1" class="ltx_p"><span id="S3.SS1.p8.1.1" class="ltx_text" style="color:#000000;">As shown in Fig. <a href="#S3.F5" title="Figure 5 â€£ 3.1 In-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, GQA provides a scene graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> for each image and a functional program for each question. The scene graph exhibits the relations between objects such as the â€œto the right ofâ€ relation between â€œmanâ€ and â€œladyâ€, while the program delineates a series of reasoning steps required to derive answers. In other words, this dataset combines detailed visual annotations with question-answer pairs accompanied by succinct explanatory statements. Based on this, VQA models can associate individual words from questions and answers with visual pointers, guiding more attention to pertinent regions within the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. Consequently, this dataset fosters the advancement of more explainable models that necessitate diverse reasoning and multi-step inference abilities, specifically by enhancing the comprehension of both visual and linguistic components in a refined manner. Nevertheless, GQA cannot evaluate the ability to query and fuse common sense due to the defect in dataset collections.
</span></p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2307.11471/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="280" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>GQA example. This dataset provides a scene graph for each image and a functional program for each question. The program enumerates a series of logical operations (reasoning steps) necessary to obtain the answer.</figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2307.11471/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>CRIC example. This dataset provides additional object-level knowledge triples compared with GQA.</figcaption>
</figure>
<div id="S3.SS1.p9" class="ltx_para ltx_noindent">
<p id="S3.SS1.p9.1" class="ltx_p"><span id="S3.SS1.p9.1.1" class="ltx_text ltx_font_bold">CRIC</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. To solve the mentioned issue, this dataset<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://cricvqa.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cricvqa.github.io/</a></span></span></span> was developed by generating questions and corresponding annotations from the scene graph of a given image and a knowledge graph respectively. This dataset, which is split into three parts: training (70%), validation (10%), and test (20%), contains 494K questions with 96K images. To collect satisfactory knowledge items and generate compositional questions, the authors used a graph-to-graph format for representing the knowledge items, which is better at aligning entities in the knowledge graph to objects in the image and depicting the commonsense relations between objects. For instance, as shown in Fig. <a href="#S3.F6" title="Figure 6 â€£ 3.1 In-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, it can be observed that this dataset not only includes identical components to GQA but also encompasses additional object-level knowledge triples such as the commonsense knowledge â€œ(subject: fork, relation: is used for, object: moving food)â€. Therefore, CRIC requires machines to ground commonsense in the visual world and perform multi-hop reasoning on images and knowledge graphs.</p>
</div>
<div id="S3.SS1.p10" class="ltx_para">
<p id="S3.SS1.p10.1" class="ltx_p">Although the development of these datasets has promoted VQA research, some studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> have revealed that existing methods tend to memorize or exploit the bias in the training data excessively rather than ground images first and then predict answers. To put it another way, the performance on the aforementioned datasets alone may not be a reliable indicator for VQA methods.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">Out-of-Distribution Setting</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To solve the mentioned issues, various OOD datasets have been developed successively, where <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">the distribution of the test split differs from or is even reversed to that of the training split</em>. These datasets aim to offer more challenging and intricate test scenarios from different perspectives, ranging from altering answer distributions to rephrasing questions. In this way, we are able to assess whether VQA methods can handle diverse real-world situations simultaneously or whether they are robust.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">VQA-CP v1 &amp; VQA-CP v2</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. These two datasets were created by reorganizing VQA v1 and v2 according to the answer distribution, resulting in them sharing the same characteristics as VQA v1 and v2, such as multiple ground-truth answers for a single question. They are the first to explore the robustness evaluation of VQA methods, which motivates subsequent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> to focus on VQA robustness. VQA-CP v1<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="http://data.lip6.fr/cadene/murel/vqacp2.tar.gz" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://data.lip6.fr/cadene/murel/vqacp2.tar.gz</a></span></span></span> contains 370K questions with 205K images, while VQA-CP v2 contains 658K questions with 219K images.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2307.11471/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Answer distributions under specific types of questions in VQA-CP v1 training and test splits.</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text" style="color:#000000;">The distributions between the training and test splits of VQA-CP exhibit significant differences, and in some cases, they are even reversed. For instance, Fig. <a href="#S3.F7" title="Figure 7 â€£ 3.2 Out-of-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that the most common answer to the question â€œwhat sportâ€ in the training split of VQA-CP v1 is â€œtennisâ€, while in the test split, â€œskiingâ€ emerges as the most prevalent answer. Therefore, these two datasets can be utilized to evaluate whether VQA methods predict answers through bias learning.</span></p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">However, these datasets are not without their limitations. Firstly, they consist of only two parts: a training set (67%) and a test set (33%), without including a validation set. This arrangement may lead to the tuning of hyperparameters on the test split. Secondly, the introduction of artificially crafted distribution shifts may not accurately reflect real-world scenarios. Lastly, models developed based on these datasets tend to be tailored toward their specific configurations due to the manual crafting of these shifts, potentially resulting in reduced generalization capabilities.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.4" class="ltx_p"><span id="S3.SS2.p5.4.5" class="ltx_text ltx_font_bold">GQA-OOD</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. To address the mentioned issues, this dataset<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/gqa-ood/GQA-OOD" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/gqa-ood/GQA-OOD</a></span></span></span> was developed by reorganizing GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> in a fine-grained manner such that distribution shifts are introduced in both the validation and test splits, and are tailored to different question groups. Specifically, GQA-OOD divides questions into two groups: â€œheadâ€ and â€œtailâ€, according to the frequency of answers, instead of modifying the global distributions of the validation and test splits. <span id="S3.SS2.p5.4.4" class="ltx_text" style="color:#000000;"> As illustrated in Fig. <a href="#S3.F8" title="Figure 8 â€£ 3.2 Out-of-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, the answers to the â€œman on thingsâ€ question group are classified into two groups based on a criterion of <math id="S3.SS2.p5.1.1.m1.2" class="ltx_Math" alttext="|a_{i}|\leq 1.2\mu(a)" display="inline"><semantics id="S3.SS2.p5.1.1.m1.2a"><mrow id="S3.SS2.p5.1.1.m1.2.2" xref="S3.SS2.p5.1.1.m1.2.2.cmml"><mrow id="S3.SS2.p5.1.1.m1.2.2.1.1" xref="S3.SS2.p5.1.1.m1.2.2.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS2.p5.1.1.m1.2.2.1.1.2" xref="S3.SS2.p5.1.1.m1.2.2.1.2.1.cmml">|</mo><msub id="S3.SS2.p5.1.1.m1.2.2.1.1.1" xref="S3.SS2.p5.1.1.m1.2.2.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS2.p5.1.1.m1.2.2.1.1.1.2" xref="S3.SS2.p5.1.1.m1.2.2.1.1.1.2.cmml">a</mi><mi mathcolor="#000000" id="S3.SS2.p5.1.1.m1.2.2.1.1.1.3" xref="S3.SS2.p5.1.1.m1.2.2.1.1.1.3.cmml">i</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.SS2.p5.1.1.m1.2.2.1.1.3" xref="S3.SS2.p5.1.1.m1.2.2.1.2.1.cmml">|</mo></mrow><mo mathcolor="#000000" id="S3.SS2.p5.1.1.m1.2.2.2" xref="S3.SS2.p5.1.1.m1.2.2.2.cmml">â‰¤</mo><mrow id="S3.SS2.p5.1.1.m1.2.2.3" xref="S3.SS2.p5.1.1.m1.2.2.3.cmml"><mn mathcolor="#000000" id="S3.SS2.p5.1.1.m1.2.2.3.2" xref="S3.SS2.p5.1.1.m1.2.2.3.2.cmml">1.2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.1.m1.2.2.3.1" xref="S3.SS2.p5.1.1.m1.2.2.3.1.cmml">â€‹</mo><mi mathcolor="#000000" id="S3.SS2.p5.1.1.m1.2.2.3.3" xref="S3.SS2.p5.1.1.m1.2.2.3.3.cmml">Î¼</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.1.m1.2.2.3.1a" xref="S3.SS2.p5.1.1.m1.2.2.3.1.cmml">â€‹</mo><mrow id="S3.SS2.p5.1.1.m1.2.2.3.4.2" xref="S3.SS2.p5.1.1.m1.2.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS2.p5.1.1.m1.2.2.3.4.2.1" xref="S3.SS2.p5.1.1.m1.2.2.3.cmml">(</mo><mi mathcolor="#000000" id="S3.SS2.p5.1.1.m1.1.1" xref="S3.SS2.p5.1.1.m1.1.1.cmml">a</mi><mo mathcolor="#000000" stretchy="false" id="S3.SS2.p5.1.1.m1.2.2.3.4.2.2" xref="S3.SS2.p5.1.1.m1.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.1.m1.2b"><apply id="S3.SS2.p5.1.1.m1.2.2.cmml" xref="S3.SS2.p5.1.1.m1.2.2"><leq id="S3.SS2.p5.1.1.m1.2.2.2.cmml" xref="S3.SS2.p5.1.1.m1.2.2.2"></leq><apply id="S3.SS2.p5.1.1.m1.2.2.1.2.cmml" xref="S3.SS2.p5.1.1.m1.2.2.1.1"><abs id="S3.SS2.p5.1.1.m1.2.2.1.2.1.cmml" xref="S3.SS2.p5.1.1.m1.2.2.1.1.2"></abs><apply id="S3.SS2.p5.1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p5.1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.p5.1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.p5.1.1.m1.2.2.1.1.1.2">ğ‘</ci><ci id="S3.SS2.p5.1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.p5.1.1.m1.2.2.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.SS2.p5.1.1.m1.2.2.3.cmml" xref="S3.SS2.p5.1.1.m1.2.2.3"><times id="S3.SS2.p5.1.1.m1.2.2.3.1.cmml" xref="S3.SS2.p5.1.1.m1.2.2.3.1"></times><cn type="float" id="S3.SS2.p5.1.1.m1.2.2.3.2.cmml" xref="S3.SS2.p5.1.1.m1.2.2.3.2">1.2</cn><ci id="S3.SS2.p5.1.1.m1.2.2.3.3.cmml" xref="S3.SS2.p5.1.1.m1.2.2.3.3">ğœ‡</ci><ci id="S3.SS2.p5.1.1.m1.1.1.cmml" xref="S3.SS2.p5.1.1.m1.1.1">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.1.m1.2c">|a_{i}|\leq 1.2\mu(a)</annotation></semantics></math>, where <math id="S3.SS2.p5.2.2.m2.1" class="ltx_Math" alttext="|a_{i}|" display="inline"><semantics id="S3.SS2.p5.2.2.m2.1a"><mrow id="S3.SS2.p5.2.2.m2.1.1.1" xref="S3.SS2.p5.2.2.m2.1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS2.p5.2.2.m2.1.1.1.2" xref="S3.SS2.p5.2.2.m2.1.1.2.1.cmml">|</mo><msub id="S3.SS2.p5.2.2.m2.1.1.1.1" xref="S3.SS2.p5.2.2.m2.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS2.p5.2.2.m2.1.1.1.1.2" xref="S3.SS2.p5.2.2.m2.1.1.1.1.2.cmml">a</mi><mi mathcolor="#000000" id="S3.SS2.p5.2.2.m2.1.1.1.1.3" xref="S3.SS2.p5.2.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.SS2.p5.2.2.m2.1.1.1.3" xref="S3.SS2.p5.2.2.m2.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.2.m2.1b"><apply id="S3.SS2.p5.2.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.2.m2.1.1.1"><abs id="S3.SS2.p5.2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p5.2.2.m2.1.1.1.2"></abs><apply id="S3.SS2.p5.2.2.m2.1.1.1.1.cmml" xref="S3.SS2.p5.2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p5.2.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p5.2.2.m2.1.1.1.1.2">ğ‘</ci><ci id="S3.SS2.p5.2.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p5.2.2.m2.1.1.1.1.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.2.m2.1c">|a_{i}|</annotation></semantics></math> denotes the number of samples within the class <math id="S3.SS2.p5.3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p5.3.3.m3.1a"><mi mathcolor="#000000" id="S3.SS2.p5.3.3.m3.1.1" xref="S3.SS2.p5.3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.3.m3.1b"><ci id="S3.SS2.p5.3.3.m3.1.1.cmml" xref="S3.SS2.p5.3.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.3.m3.1c">i</annotation></semantics></math> such as â€œskateboardâ€ and â€œsurfboardâ€, and <math id="S3.SS2.p5.4.4.m4.1" class="ltx_Math" alttext="\mu(a)" display="inline"><semantics id="S3.SS2.p5.4.4.m4.1a"><mrow id="S3.SS2.p5.4.4.m4.1.2" xref="S3.SS2.p5.4.4.m4.1.2.cmml"><mi mathcolor="#000000" id="S3.SS2.p5.4.4.m4.1.2.2" xref="S3.SS2.p5.4.4.m4.1.2.2.cmml">Î¼</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.4.4.m4.1.2.1" xref="S3.SS2.p5.4.4.m4.1.2.1.cmml">â€‹</mo><mrow id="S3.SS2.p5.4.4.m4.1.2.3.2" xref="S3.SS2.p5.4.4.m4.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS2.p5.4.4.m4.1.2.3.2.1" xref="S3.SS2.p5.4.4.m4.1.2.cmml">(</mo><mi mathcolor="#000000" id="S3.SS2.p5.4.4.m4.1.1" xref="S3.SS2.p5.4.4.m4.1.1.cmml">a</mi><mo mathcolor="#000000" stretchy="false" id="S3.SS2.p5.4.4.m4.1.2.3.2.2" xref="S3.SS2.p5.4.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.4.m4.1b"><apply id="S3.SS2.p5.4.4.m4.1.2.cmml" xref="S3.SS2.p5.4.4.m4.1.2"><times id="S3.SS2.p5.4.4.m4.1.2.1.cmml" xref="S3.SS2.p5.4.4.m4.1.2.1"></times><ci id="S3.SS2.p5.4.4.m4.1.2.2.cmml" xref="S3.SS2.p5.4.4.m4.1.2.2">ğœ‡</ci><ci id="S3.SS2.p5.4.4.m4.1.1.cmml" xref="S3.SS2.p5.4.4.m4.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.4.m4.1c">\mu(a)</annotation></semantics></math> represents the average sample count for this group. </span></p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">GQA-OOD contains 53K questions with 9.7K images, which is split into two parts: validation (95%) and testdev (5%). Each part has two subsets: â€œtailâ€ and â€œheadâ€. The â€œtailâ€ group is used to evaluate the OOD performance, while the â€œheadâ€ group is employed to assess the ID performance. <span id="S3.SS2.p6.1.1" class="ltx_text" style="color:#000000;"> This dataset requires models to be trained on the original GQA training split. In this way, it compels models to minimize bias in their test results while simultaneously exposing the models to bias captured within the training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Therefore, this dataset promotes the intrinsic development of debiasing methods rather than relying solely on the purification of training data. </span></p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2307.11471/assets/x8.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Answer distributions or frequencies within the â€œMan on thingsâ€ group. The â€œtailâ€ group is utilized to evaluate the OOD performance, while the â€œheadâ€ group is employed to assess the ID performance.</figcaption>
</figure>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p"><span id="S3.SS2.p7.1.1" class="ltx_text ltx_font_bold">VQA-Rephrasings</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. This dataset was developed by first considering the validation split of VQA v2.0 as the base set, then sampling from this set to form a subset, and finally collecting three human-provided rephrasing for each question in this subset. <span id="S3.SS2.p7.1.2" class="ltx_text" style="color:#000000;"> As illustrated in Fig. <a href="#S3.F9" title="Figure 9 â€£ 3.2 Out-of-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the right questions are obtained by changing the expression of the left original question while retaining its original meaning. In this way, VQA-Rehprasings collects 162.0K questions with 40.5K images.</span> This dataset enables the evaluation of VQA methods for robustness and consistency across alternative rephrasing of questions with the same meaning.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2307.11471/assets/x9.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>VQA-Rephrasings example. The original question is selected from the validation split of VQA v2, while the rephrased question is collected from three humans under the condition of preserving the original meaning.</figcaption>
</figure>
<div id="S3.SS2.p8" class="ltx_para ltx_noindent">
<p id="S3.SS2.p8.1" class="ltx_p"><span id="S3.SS2.p8.1.1" class="ltx_text ltx_font_bold">VQACE</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>. The previously mentioned datasets commonly overlook multimodal bias, instead prioritizing the assessment of language bias learning. <span id="S3.SS2.p8.1.2" class="ltx_text" style="color:#000000;"> Multimodal bias is such a phenomenon that the frequent co-occurrence of textual and visual elements within the training data predicts specific answers accurately. Similar to the language bias, multimodal bias often persists and transfers to the validation set, potentially impacting the generalizability of VQA models.</span> In order to address this issue, Dancette <span id="S3.SS2.p8.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> proposed a method for identifying shortcuts that predicts the correct answer based on the appearance of words in the question and visual elements in the image. Then, they built VQA counter examples where the shortcut rules result in inaccurate answers as the evaluation protocol. <span id="S3.SS2.p8.1.4" class="ltx_text" style="color:#000000;"> As depicted in Fig. <a href="#S3.F10" title="Figure 10 â€£ 3.2 Out-of-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, VQA models may learn or memorize certain words such as â€œdoingâ€ and objects like â€œmanâ€, â€œsurfboardâ€, and â€œhandâ€ to predict answers. Although this can result in accurate predictions for some examples (as seen in the left example), it can also lead to incorrect responses (as observed in the right example). There is also an easy subset in which the correct answers can be derived through at least one of the shortcuts.</span> Statistics show that 90% of the bias in this dataset<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/cdancette/detect-shortcuts" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/cdancette/detect-shortcuts</a></span></span></span> is multimodal, indicating that previous successful debiasing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> in addressing the shift in language distributions such as the shift in VQA-CP and VQA-Rephrasings but may not be as effective in reducing natural shortcuts from VQA.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.1" class="ltx_p"><span id="S3.SS2.p9.1.1" class="ltx_text ltx_font_bold">VQA-VS</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. Like VQACE, this dataset<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/PhoebusSi/VQA-VS" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PhoebusSi/VQA-VS</a></span></span></span> takes into account the natural shortcuts in VQA, such as language, vision, and multimodality bias. It goes a step further by introducing several concrete shortcuts for each bias, such as the question-type shortcut in language bias and the key-object-pair shortcut in vision bias. <span id="S3.SS2.p9.1.2" class="ltx_text" style="color:#000000;">For instance, the multimodal bias of VQACE in Fig. <a href="#S3.F10" title="Figure 10 â€£ 3.2 Out-of-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> belongs to the shortcut of â€œcomposite of keyword and key objectâ€ in this dataset.</span> These shortcuts may be used to assess OOD performance in a more refined manner. In addition, this dataset also includes a split to evaluate ID performance.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2307.11471/assets/x10.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="462" height="338" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Example of a multimodal shortcut in VQACE, with its supporting examples and counter examples. This shortcut is a combination of words â€œdoingâ€ and objects including â€œmanâ€, â€œsurfboardâ€, and â€œhandâ€. Generic VQA methods usually make accurate/wrong predictions when facing the left/right examples respectively.</figcaption>
</figure>
<div id="S3.SS2.p10" class="ltx_para">
<p id="S3.SS2.p10.1" class="ltx_p"><span id="S3.SS2.p10.1.1" class="ltx_text" style="color:#000000;">However, the datasets mentioned above have certain limitations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. Firstly, these datasets are mainly developed based on heuristic rules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>. Furthermore, synthetic images or questions are often used in these datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, instead of being generated by humans. They may not represent real-world scenarios accurately.</span></p>
</div>
<div id="S3.SS2.p11" class="ltx_para ltx_noindent">
<p id="S3.SS2.p11.1" class="ltx_p"><span id="S3.SS2.p11.1.1" class="ltx_text ltx_font_bold">AVQA</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. To address the above issues, this dataset<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://adversarialvqa.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://adversarialvqa.org/</a></span></span></span> was gathered iteratively using an adversarial â€œhuman-and-model-in-the-loopâ€ procedure. The process of data collection can be viewed as a game played in <math id="S3.SS2.p11.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p11.1.m1.1a"><mi id="S3.SS2.p11.1.m1.1.1" xref="S3.SS2.p11.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p11.1.m1.1b"><ci id="S3.SS2.p11.1.m1.1.1.cmml" xref="S3.SS2.p11.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p11.1.m1.1c">n</annotation></semantics></math> rounds between two parties: a human annotator and a well-trained model. AVQA consists of 243.0K questions accompanied by 37.9K images and is developed through three rounds. Specifically, the image-question pairs are gathered once VQA models that are trained on VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> cannot predict answers accurately. <span id="S3.SS2.p11.1.2" class="ltx_text" style="color:#000000;">To avoid over-fitting, the VQA model is randomly chosen from LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, UNITER-B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, and UNITER-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. To collect harder questions, the model is re-trained based on the data collected from previous rounds. Note that the question is written by humans rather than rule-based questions such as in GQA. Therefore, AVQA may be more challenging than previous datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. </span></p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of datasets and evaluation metrics. ID denotes in-distribution, and OOD represents out-of-distribution. A single graph denotes a scene graph of an image. Double graphs denote scene and knowledge graphs. CC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> is the dataset of conceptual captions. Fakeddit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> is a multimodal dataset for fake news detection. Natural shortcuts include language, vision, and multimodality bias.</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:123.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-195.3pt,55.5pt) scale(0.526041923314705,0.526041923314705) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_right ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Development</span></td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Image Source</span></td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Images</span></td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Questions</span></td>
<td id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Focus</span></td>
<td id="S3.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">ID</span></td>
<td id="S3.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.8.1" class="ltx_text ltx_font_bold">OOD</span></td>
<td id="S3.T1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.9.1" class="ltx_text ltx_font_bold">Metrics</span></td>
</tr>
<tr id="S3.T1.1.1.2" class="ltx_tr">
<td id="S3.T1.1.1.2.1" class="ltx_td ltx_align_right ltx_border_t">VQA v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S3.T1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">asking human questions</td>
<td id="S3.T1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">COCO</td>
<td id="S3.T1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">204.7K</td>
<td id="S3.T1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">614.2K</td>
<td id="S3.T1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">vision</td>
<td id="S3.T1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">âœ“</td>
<td id="S3.T1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t">âœ—</td>
<td id="S3.T1.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t">open-ended accuracy</td>
</tr>
<tr id="S3.T1.1.1.3" class="ltx_tr">
<td id="S3.T1.1.1.3.1" class="ltx_td ltx_align_right">VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S3.T1.1.1.3.2" class="ltx_td ltx_align_center">complement on VQA v1</td>
<td id="S3.T1.1.1.3.3" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.T1.1.1.3.4" class="ltx_td ltx_align_center">479K</td>
<td id="S3.T1.1.1.3.5" class="ltx_td ltx_align_center">1.1M</td>
<td id="S3.T1.1.1.3.6" class="ltx_td ltx_align_center">vision</td>
<td id="S3.T1.1.1.3.7" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.3.8" class="ltx_td ltx_align_center">âœ—</td>
<td id="S3.T1.1.1.3.9" class="ltx_td ltx_align_center">open-ended accuracy</td>
</tr>
<tr id="S3.T1.1.1.4" class="ltx_tr">
<td id="S3.T1.1.1.4.1" class="ltx_td ltx_align_right">GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S3.T1.1.1.4.2" class="ltx_td ltx_align_center">generate questions from a single graph</td>
<td id="S3.T1.1.1.4.3" class="ltx_td ltx_align_center">Visual Genome</td>
<td id="S3.T1.1.1.4.4" class="ltx_td ltx_align_center">113K</td>
<td id="S3.T1.1.1.4.5" class="ltx_td ltx_align_center">22M</td>
<td id="S3.T1.1.1.4.6" class="ltx_td ltx_align_center">composition</td>
<td id="S3.T1.1.1.4.7" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.4.8" class="ltx_td ltx_align_center">âœ—</td>
<td id="S3.T1.1.1.4.9" class="ltx_td ltx_align_center">composite metrics</td>
</tr>
<tr id="S3.T1.1.1.5" class="ltx_tr">
<td id="S3.T1.1.1.5.1" class="ltx_td ltx_align_right">CRIC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
<td id="S3.T1.1.1.5.2" class="ltx_td ltx_align_center">generate questions from double graphs</td>
<td id="S3.T1.1.1.5.3" class="ltx_td ltx_align_center">Visual Genome</td>
<td id="S3.T1.1.1.5.4" class="ltx_td ltx_align_center">96K</td>
<td id="S3.T1.1.1.5.5" class="ltx_td ltx_align_center">494.3K</td>
<td id="S3.T1.1.1.5.6" class="ltx_td ltx_align_center">commonsense</td>
<td id="S3.T1.1.1.5.7" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.5.8" class="ltx_td ltx_align_center">âœ—</td>
<td id="S3.T1.1.1.5.9" class="ltx_td ltx_align_center">composite metrics</td>
</tr>
<tr id="S3.T1.1.1.6" class="ltx_tr">
<td id="S3.T1.1.1.6.1" class="ltx_td ltx_align_right">VQA-CP v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T1.1.1.6.2" class="ltx_td ltx_align_center">reorganization on VQA v1</td>
<td id="S3.T1.1.1.6.3" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.T1.1.1.6.4" class="ltx_td ltx_align_center">205K</td>
<td id="S3.T1.1.1.6.5" class="ltx_td ltx_align_center">370K</td>
<td id="S3.T1.1.1.6.6" class="ltx_td ltx_align_center">language bias</td>
<td id="S3.T1.1.1.6.7" class="ltx_td ltx_align_center">âœ—</td>
<td id="S3.T1.1.1.6.8" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.6.9" class="ltx_td ltx_align_center">open-ended accuracy</td>
</tr>
<tr id="S3.T1.1.1.7" class="ltx_tr">
<td id="S3.T1.1.1.7.1" class="ltx_td ltx_align_right">VQA-CP v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T1.1.1.7.2" class="ltx_td ltx_align_center">reorganization on VQA v2</td>
<td id="S3.T1.1.1.7.3" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.T1.1.1.7.4" class="ltx_td ltx_align_center">219K</td>
<td id="S3.T1.1.1.7.5" class="ltx_td ltx_align_center">658K</td>
<td id="S3.T1.1.1.7.6" class="ltx_td ltx_align_center">language bias</td>
<td id="S3.T1.1.1.7.7" class="ltx_td ltx_align_center">âœ—</td>
<td id="S3.T1.1.1.7.8" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.7.9" class="ltx_td ltx_align_center">open-ended accuracy</td>
</tr>
<tr id="S3.T1.1.1.8" class="ltx_tr">
<td id="S3.T1.1.1.8.1" class="ltx_td ltx_align_right">GQA-OOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S3.T1.1.1.8.2" class="ltx_td ltx_align_center">reorganization on GQA</td>
<td id="S3.T1.1.1.8.3" class="ltx_td ltx_align_center">Visual Genome</td>
<td id="S3.T1.1.1.8.4" class="ltx_td ltx_align_center">82.2K</td>
<td id="S3.T1.1.1.8.5" class="ltx_td ltx_align_center">996.8K</td>
<td id="S3.T1.1.1.8.6" class="ltx_td ltx_align_center">language bias</td>
<td id="S3.T1.1.1.8.7" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.8.8" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.8.9" class="ltx_td ltx_align_center">composite metrics</td>
</tr>
<tr id="S3.T1.1.1.9" class="ltx_tr">
<td id="S3.T1.1.1.9.1" class="ltx_td ltx_align_right">VQA-Rephrasings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</td>
<td id="S3.T1.1.1.9.2" class="ltx_td ltx_align_center">rephrasing on VQA v2</td>
<td id="S3.T1.1.1.9.3" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.T1.1.1.9.4" class="ltx_td ltx_align_center">40.5K</td>
<td id="S3.T1.1.1.9.5" class="ltx_td ltx_align_center">162.0K</td>
<td id="S3.T1.1.1.9.6" class="ltx_td ltx_align_center">language bias</td>
<td id="S3.T1.1.1.9.7" class="ltx_td ltx_align_center">âœ—</td>
<td id="S3.T1.1.1.9.8" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.9.9" class="ltx_td ltx_align_center">consensus score</td>
</tr>
<tr id="S3.T1.1.1.10" class="ltx_tr">
<td id="S3.T1.1.1.10.1" class="ltx_td ltx_align_right">VQA-CE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
<td id="S3.T1.1.1.10.2" class="ltx_td ltx_align_center">reorganization on VQA v2</td>
<td id="S3.T1.1.1.10.3" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.T1.1.1.10.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.10.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.10.6" class="ltx_td ltx_align_center">natural shortcuts</td>
<td id="S3.T1.1.1.10.7" class="ltx_td ltx_align_center">âœ—</td>
<td id="S3.T1.1.1.10.8" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.10.9" class="ltx_td ltx_align_center">open-ended accuracy</td>
</tr>
<tr id="S3.T1.1.1.11" class="ltx_tr">
<td id="S3.T1.1.1.11.1" class="ltx_td ltx_align_right">VQA-VS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S3.T1.1.1.11.2" class="ltx_td ltx_align_center">reorganization on VQA v2</td>
<td id="S3.T1.1.1.11.3" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.T1.1.1.11.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.11.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.1.11.6" class="ltx_td ltx_align_center">natural shortcuts</td>
<td id="S3.T1.1.1.11.7" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.11.8" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.11.9" class="ltx_td ltx_align_center">open-ended accuracy</td>
</tr>
<tr id="S3.T1.1.1.12" class="ltx_tr">
<td id="S3.T1.1.1.12.1" class="ltx_td ltx_align_right">AVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S3.T1.1.1.12.2" class="ltx_td ltx_align_center">human-and-model-in-the-loop</td>
<td id="S3.T1.1.1.12.3" class="ltx_td ltx_align_center">CC/Fakeddit/VCR</td>
<td id="S3.T1.1.1.12.4" class="ltx_td ltx_align_center">37.9K</td>
<td id="S3.T1.1.1.12.5" class="ltx_td ltx_align_center">243.0K</td>
<td id="S3.T1.1.1.12.6" class="ltx_td ltx_align_center">adversarial robustness</td>
<td id="S3.T1.1.1.12.7" class="ltx_td ltx_align_center">âœ—</td>
<td id="S3.T1.1.1.12.8" class="ltx_td ltx_align_center">âœ“</td>
<td id="S3.T1.1.1.12.9" class="ltx_td ltx_align_center">open-ended accuracy</td>
</tr>
<tr id="S3.T1.1.1.13" class="ltx_tr">
<td id="S3.T1.1.1.13.1" class="ltx_td ltx_align_right ltx_border_bb">AdVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S3.T1.1.1.13.2" class="ltx_td ltx_align_center ltx_border_bb">human-and-model-in-the-loop</td>
<td id="S3.T1.1.1.13.3" class="ltx_td ltx_align_center ltx_border_bb">COCO</td>
<td id="S3.T1.1.1.13.4" class="ltx_td ltx_align_center ltx_border_bb">41.8K</td>
<td id="S3.T1.1.1.13.5" class="ltx_td ltx_align_center ltx_border_bb">46.8K</td>
<td id="S3.T1.1.1.13.6" class="ltx_td ltx_align_center ltx_border_bb">adversarial robustness</td>
<td id="S3.T1.1.1.13.7" class="ltx_td ltx_align_center ltx_border_bb">âœ—</td>
<td id="S3.T1.1.1.13.8" class="ltx_td ltx_align_center ltx_border_bb">âœ“</td>
<td id="S3.T1.1.1.13.9" class="ltx_td ltx_align_center ltx_border_bb">open-ended accuracy</td>
</tr>
</table>
</span></div>
</figure>
<div id="S3.SS2.p12" class="ltx_para ltx_noindent">
<p id="S3.SS2.p12.1" class="ltx_p"><span id="S3.SS2.p12.1.1" class="ltx_text ltx_font_bold">AdVQA</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. Similar to AVQA, this dataset<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="https://adversarialvqa.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://adversarialvqa.org/</a></span></span></span> was also developed by the human-and-model-in-the-loop procedure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>. In other words, both of them are developed from the
perspective of adversarial robustness. This dataset contains 46.8K questions accompanied by 41.8K images. In comparison, the developing procedure for this dataset has an additional question validation phase, which manually determines whether the image is necessary and sufficient to answer the question.</p>
</div>
<div id="S3.SS2.p13" class="ltx_para">
<p id="S3.SS2.p13.1" class="ltx_p">In a nutshell, each dataset has a distinct focus and can be used to assess the ability of methods from a particular angle. More details of datasets, such as the source of images and the focus, are shown in Table <a href="#S3.T1" title="TABLE I â€£ 3.2 Out-of-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Evaluations</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The evaluation is usually associated with the annotation of datasets, which ranges from open-ended accuracy to composite metrics. Existing debiasing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> typically employ a combination of ID and OOD datasets to assess robustness. As a result, a trade-off metric such as the harmonic mean is used to assess the performance of methods comprehensively. The evaluation metrics of the mentioned datasets in Section <a href="#S3" title="3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> are shown in Table <a href="#S3.T1" title="TABLE I â€£ 3.2 Out-of-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.2" class="ltx_p"><span id="S4.p2.2.1" class="ltx_text ltx_font_bold">Open-Ended Accuracy.</span> Given an image and its related question, humans may provide various answers in real-world scenarios. For example, â€œoff dutyâ€ and â€œcomingâ€ are provided for the question that is on the right of Fig. <a href="#S2.F3" title="Figure 3 â€£ 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> by different individuals. To comprehensively evaluate VQA methods, the questions in VQA v1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> are annotated by ten humans, resulting in the employment of open-ended accuracy. Various datasets, such as VQA-CP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and VQA-CE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, also use this metric because their development is related to the mentioned two datasets. The open-ended accuracy is computed as follows:</p>
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.4" class="ltx_Math" alttext="\text{open-ended accuracy}=\min\left\{\frac{n_{\mathrm{a}}}{3},1\right\}," display="block"><semantics id="S4.E6.m1.4a"><mrow id="S4.E6.m1.4.4.1" xref="S4.E6.m1.4.4.1.1.cmml"><mrow id="S4.E6.m1.4.4.1.1" xref="S4.E6.m1.4.4.1.1.cmml"><mtext id="S4.E6.m1.4.4.1.1.2" xref="S4.E6.m1.4.4.1.1.2a.cmml">open-ended accuracy</mtext><mo id="S4.E6.m1.4.4.1.1.1" xref="S4.E6.m1.4.4.1.1.1.cmml">=</mo><mrow id="S4.E6.m1.4.4.1.1.3.2" xref="S4.E6.m1.4.4.1.1.3.1.cmml"><mi id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml">min</mi><mo id="S4.E6.m1.4.4.1.1.3.2a" xref="S4.E6.m1.4.4.1.1.3.1.cmml">â¡</mo><mrow id="S4.E6.m1.4.4.1.1.3.2.1" xref="S4.E6.m1.4.4.1.1.3.1.cmml"><mo id="S4.E6.m1.4.4.1.1.3.2.1.1" xref="S4.E6.m1.4.4.1.1.3.1.cmml">{</mo><mfrac id="S4.E6.m1.2.2" xref="S4.E6.m1.2.2.cmml"><msub id="S4.E6.m1.2.2.2" xref="S4.E6.m1.2.2.2.cmml"><mi id="S4.E6.m1.2.2.2.2" xref="S4.E6.m1.2.2.2.2.cmml">n</mi><mi mathvariant="normal" id="S4.E6.m1.2.2.2.3" xref="S4.E6.m1.2.2.2.3.cmml">a</mi></msub><mn id="S4.E6.m1.2.2.3" xref="S4.E6.m1.2.2.3.cmml">3</mn></mfrac><mo id="S4.E6.m1.4.4.1.1.3.2.1.2" xref="S4.E6.m1.4.4.1.1.3.1.cmml">,</mo><mn id="S4.E6.m1.3.3" xref="S4.E6.m1.3.3.cmml">1</mn><mo id="S4.E6.m1.4.4.1.1.3.2.1.3" xref="S4.E6.m1.4.4.1.1.3.1.cmml">}</mo></mrow></mrow></mrow><mo id="S4.E6.m1.4.4.1.2" xref="S4.E6.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.4b"><apply id="S4.E6.m1.4.4.1.1.cmml" xref="S4.E6.m1.4.4.1"><eq id="S4.E6.m1.4.4.1.1.1.cmml" xref="S4.E6.m1.4.4.1.1.1"></eq><ci id="S4.E6.m1.4.4.1.1.2a.cmml" xref="S4.E6.m1.4.4.1.1.2"><mtext id="S4.E6.m1.4.4.1.1.2.cmml" xref="S4.E6.m1.4.4.1.1.2">open-ended accuracy</mtext></ci><apply id="S4.E6.m1.4.4.1.1.3.1.cmml" xref="S4.E6.m1.4.4.1.1.3.2"><min id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1"></min><apply id="S4.E6.m1.2.2.cmml" xref="S4.E6.m1.2.2"><divide id="S4.E6.m1.2.2.1.cmml" xref="S4.E6.m1.2.2"></divide><apply id="S4.E6.m1.2.2.2.cmml" xref="S4.E6.m1.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.1.cmml" xref="S4.E6.m1.2.2.2">subscript</csymbol><ci id="S4.E6.m1.2.2.2.2.cmml" xref="S4.E6.m1.2.2.2.2">ğ‘›</ci><ci id="S4.E6.m1.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.3">a</ci></apply><cn type="integer" id="S4.E6.m1.2.2.3.cmml" xref="S4.E6.m1.2.2.3">3</cn></apply><cn type="integer" id="S4.E6.m1.3.3.cmml" xref="S4.E6.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.4c">\text{open-ended accuracy}=\min\left\{\frac{n_{\mathrm{a}}}{3},1\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S4.p2.1" class="ltx_p">where <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="n_{\mathrm{a}}" display="inline"><semantics id="S4.p2.1.m1.1a"><msub id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">n</mi><mi mathvariant="normal" id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">ğ‘›</ci><ci id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">a</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">n_{\mathrm{a}}</annotation></semantics></math> denotes the number of predicted answers that are identical to human-provided answers for questions.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Composite Metrics.</span> Unlike the datasets mentioned above, GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> only contains an answer for each question, making it possible to use the â€œstandard accuracyâ€ as evaluation metrics. In addition, this dataset introduces five new metrics to gain a better understanding of visual reasoning techniques and highlights the functions that coherent reasoning methods should have. <span id="S4.p3.1.2" class="ltx_text" style="color:#000000;"></span></p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text" style="color:#000000;">â€œConsistencyâ€ assesses the consistency of predicted answers across different questions. For example, given a question-answer pair â€œ(Is there a banana to the right of the white cup?, Yes)â€ and an image, we can infer the answers to questions such as â€œIs there a fruit to the right of the white cup?â€ and â€œIs the white cup to the left of the banana?â€.</span></p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text" style="color:#000000;">â€œValidityâ€ and â€œPlausibilityâ€ are used to evaluate whether the predicted answer is reasonable enough. The former checks whether a given answer is within the scope of the question, such as providing colors to a color-type question. The latter goes a step further, determining whether the answer makes sense, for example, pandas do not have the ability to talk.</span></p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text" style="color:#000000;">â€œDistributionâ€ assesses the overall match between the true answer distribution and the methodâ€™s predicted distribution using Chi-Square statistics </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.I1.i3.p1.1.2.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib92" title="" class="ltx_ref">92</a><span id="S4.I1.i3.p1.1.3.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="S4.I1.i3.p1.1.4" class="ltx_text" style="color:#000000;">. This metric enables us to evaluate the ability of models to predict not only the most frequent answers but also the less common ones.</span></p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text" style="color:#000000;">â€œGroundingâ€ evaluates how well the model concentrates on regions of the image that are crucial to the question.</span></p>
</div>
</li>
</ul>
<p id="S4.p3.2" class="ltx_p">As introduced in the above subsection, GQA-OOD is developed by re-splitting the validation and test splits of GQA into <em id="S4.p3.2.1" class="ltx_emph ltx_font_italic">head</em> and <em id="S4.p3.2.2" class="ltx_emph ltx_font_italic">tail</em> groups respectively. Therefore, we can also use these five metrics to evaluate VQA methods besides the â€œheadâ€ and â€œtailâ€ accuracy.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In addition to these metrics, there also exist other composite metrics to evaluate VQA performance. Specifically, CRIC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> requires methods not only to predict answers but also to provide intermediate grounding results to mitigate the impact of commonsense prior and enables us to fairly evaluate whether methods truly understand the image and commonsense. In other words, a VQA system should provide two outputs for each question including an answer and a chosen object from the image. As a result, a question is considered to be correctly answered when the two predictions are both correct.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.2" class="ltx_p"><span id="S4.p5.2.1" class="ltx_text ltx_font_bold">Consensus Score.</span> To measure the robustness of methods across various question rephrasing, VQA-Rephrasings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> introduces a metric dubbed â€œconsensus scoreâ€ based on the premise that the answer to all rephrasing of the same question should be the same. <span id="S4.p5.2.2" class="ltx_text" style="color:#000000;">For instance, a system should provide the same answer given the four questions in Fig. <a href="#S3.F9" title="Figure 9 â€£ 3.2 Out-of-Distribution Setting â€£ 3 Datasets â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</span> This consensus score <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="cs" display="inline"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mi id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><times id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1"></times><ci id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">ğ‘</ci><ci id="S4.p5.1.m1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">cs</annotation></semantics></math> is defined as the ratio of the number of subsets where all the answers are correct and the total number of subsets with size <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p5.2.m2.1a"><mi id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><ci id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">k</annotation></semantics></math>:</p>
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.23" class="ltx_Math" alttext="\begin{split}&amp;cs(k)=\sum_{\mathcal{Q}^{{}^{\prime}}\subset\mathcal{Q},\left|\mathcal{Q}^{{}^{\prime}}\right|=k}\frac{s(\mathcal{Q}^{{}^{\prime}})}{{}^{n}C_{k}},\\
&amp;\text{with }s(\mathcal{Q}^{\prime})=\left\{\begin{array}[]{ll}1&amp;\text{if}\quad\forall q\in\mathcal{Q}^{{}^{\prime}}\quad\phi(q)&gt;0\text{,}\\
0&amp;\text{otherwise.}\end{array}\right.,\end{split}" display="block"><semantics id="S4.E7.m1.23a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S4.E7.m1.23.23.3"><mtr id="S4.E7.m1.23.23.3a"><mtd id="S4.E7.m1.23.23.3b"></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E7.m1.23.23.3c"><mrow id="S4.E7.m1.22.22.2.21.11.11.11"><mrow id="S4.E7.m1.22.22.2.21.11.11.11.1"><mrow id="S4.E7.m1.22.22.2.21.11.11.11.1.1"><mi id="S4.E7.m1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.22.22.2.21.11.11.11.1.1.1">â€‹</mo><mi id="S4.E7.m1.2.2.2.2.2.2" xref="S4.E7.m1.2.2.2.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.22.22.2.21.11.11.11.1.1.1a">â€‹</mo><mrow id="S4.E7.m1.22.22.2.21.11.11.11.1.1.2"><mo stretchy="false" id="S4.E7.m1.3.3.3.3.3.3">(</mo><mi id="S4.E7.m1.4.4.4.4.4.4" xref="S4.E7.m1.4.4.4.4.4.4.cmml">k</mi><mo stretchy="false" id="S4.E7.m1.5.5.5.5.5.5">)</mo></mrow></mrow><mo rspace="0.111em" id="S4.E7.m1.6.6.6.6.6.6" xref="S4.E7.m1.6.6.6.6.6.6.cmml">=</mo><mrow id="S4.E7.m1.22.22.2.21.11.11.11.1.2"><munder id="S4.E7.m1.22.22.2.21.11.11.11.1.2.1"><mo movablelimits="false" id="S4.E7.m1.7.7.7.7.7.7" xref="S4.E7.m1.7.7.7.7.7.7.cmml">âˆ‘</mo><mrow id="S4.E7.m1.8.8.8.8.8.8.1.2" xref="S4.E7.m1.8.8.8.8.8.8.1.3.cmml"><mrow id="S4.E7.m1.8.8.8.8.8.8.1.1.1" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.cmml"><msup id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.2" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.2.cmml">ğ’¬</mi><msup id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3.cmml"><mi id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3a" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3.cmml"></mi><mo id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3.1" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3.1.cmml">â€²</mo></msup></msup><mo id="S4.E7.m1.8.8.8.8.8.8.1.1.1.1" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.1.cmml">âŠ‚</mo><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.8.8.8.8.8.8.1.1.1.3" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.3.cmml">ğ’¬</mi></mrow><mo id="S4.E7.m1.8.8.8.8.8.8.1.2.3" xref="S4.E7.m1.8.8.8.8.8.8.1.3a.cmml">,</mo><mrow id="S4.E7.m1.8.8.8.8.8.8.1.2.2" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.cmml"><mrow id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.2.cmml"><mo id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.2" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.2.1.cmml">|</mo><msup id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.2" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.2.cmml">ğ’¬</mi><msup id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3.cmml"><mi id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3a" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3.cmml"></mi><mo id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3.1" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3.1.cmml">â€²</mo></msup></msup><mo id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.3" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.2.1.cmml">|</mo></mrow><mo id="S4.E7.m1.8.8.8.8.8.8.1.2.2.2" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.2.cmml">=</mo><mi id="S4.E7.m1.8.8.8.8.8.8.1.2.2.3" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.3.cmml">k</mi></mrow></mrow></munder><mfrac id="S4.E7.m1.9.9.9.9.9.9" xref="S4.E7.m1.9.9.9.9.9.9.cmml"><mrow id="S4.E7.m1.9.9.9.9.9.9.1" xref="S4.E7.m1.9.9.9.9.9.9.1.cmml"><mi id="S4.E7.m1.9.9.9.9.9.9.1.3" xref="S4.E7.m1.9.9.9.9.9.9.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.9.9.9.9.9.9.1.2" xref="S4.E7.m1.9.9.9.9.9.9.1.2.cmml">â€‹</mo><mrow id="S4.E7.m1.9.9.9.9.9.9.1.1.1" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.cmml"><mo stretchy="false" id="S4.E7.m1.9.9.9.9.9.9.1.1.1.2" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.cmml">(</mo><msup id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.2" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.2.cmml">ğ’¬</mi><msup id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3.cmml"><mi id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3a" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3.cmml"></mi><mo id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3.1" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3.1.cmml">â€²</mo></msup></msup><mo stretchy="false" id="S4.E7.m1.9.9.9.9.9.9.1.1.1.3" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.cmml">)</mo></mrow></mrow><mmultiscripts id="S4.E7.m1.9.9.9.9.9.9.3" xref="S4.E7.m1.9.9.9.9.9.9.3.cmml"><mi id="S4.E7.m1.9.9.9.9.9.9.3.2.2" xref="S4.E7.m1.9.9.9.9.9.9.3.2.2.cmml">C</mi><mi id="S4.E7.m1.9.9.9.9.9.9.3.2.3" xref="S4.E7.m1.9.9.9.9.9.9.3.2.3.cmml">k</mi><mrow id="S4.E7.m1.9.9.9.9.9.9.3a" xref="S4.E7.m1.9.9.9.9.9.9.3.cmml"></mrow><mprescripts id="S4.E7.m1.9.9.9.9.9.9.3b" xref="S4.E7.m1.9.9.9.9.9.9.3.cmml"></mprescripts><mrow id="S4.E7.m1.9.9.9.9.9.9.3c" xref="S4.E7.m1.9.9.9.9.9.9.3.cmml"></mrow><mi id="S4.E7.m1.9.9.9.9.9.9.3.3" xref="S4.E7.m1.9.9.9.9.9.9.3.3.cmml">n</mi></mmultiscripts></mfrac></mrow></mrow><mo id="S4.E7.m1.10.10.10.10.10.10">,</mo></mrow></mtd></mtr><mtr id="S4.E7.m1.23.23.3d"><mtd id="S4.E7.m1.23.23.3e"></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E7.m1.23.23.3f"><mrow id="S4.E7.m1.23.23.3.22.11.11.11"><mrow id="S4.E7.m1.23.23.3.22.11.11.11.1"><mrow id="S4.E7.m1.23.23.3.22.11.11.11.1.1"><mtext id="S4.E7.m1.11.11.11.1.1.1" xref="S4.E7.m1.11.11.11.1.1.1a.cmml">withÂ </mtext><mo lspace="0em" rspace="0em" id="S4.E7.m1.23.23.3.22.11.11.11.1.1.2">â€‹</mo><mi id="S4.E7.m1.12.12.12.2.2.2" xref="S4.E7.m1.12.12.12.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.23.23.3.22.11.11.11.1.1.2a">â€‹</mo><mrow id="S4.E7.m1.23.23.3.22.11.11.11.1.1.1.1"><mo stretchy="false" id="S4.E7.m1.13.13.13.3.3.3">(</mo><msup id="S4.E7.m1.23.23.3.22.11.11.11.1.1.1.1.1"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.14.14.14.4.4.4" xref="S4.E7.m1.14.14.14.4.4.4.cmml">ğ’¬</mi><mo id="S4.E7.m1.15.15.15.5.5.5.1" xref="S4.E7.m1.15.15.15.5.5.5.1.cmml">â€²</mo></msup><mo stretchy="false" id="S4.E7.m1.16.16.16.6.6.6">)</mo></mrow></mrow><mo id="S4.E7.m1.17.17.17.7.7.7" xref="S4.E7.m1.17.17.17.7.7.7.cmml">=</mo><mrow id="S4.E7.m1.23.23.3.22.11.11.11.1.2"><mo id="S4.E7.m1.18.18.18.8.8.8">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E7.m1.19.19.19.9.9.9" xref="S4.E7.m1.19.19.19.9.9.9.cmml"><mtr id="S4.E7.m1.19.19.19.9.9.9a" xref="S4.E7.m1.19.19.19.9.9.9.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E7.m1.19.19.19.9.9.9b" xref="S4.E7.m1.19.19.19.9.9.9.cmml"><mn id="S4.E7.m1.19.19.19.9.9.9.4.5.1" xref="S4.E7.m1.19.19.19.9.9.9.4.5.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E7.m1.19.19.19.9.9.9c" xref="S4.E7.m1.19.19.19.9.9.9.cmml"><mrow id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.5.cmml"><mrow id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.cmml"><mrow id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.2.cmml"><mtext id="S4.E7.m1.19.19.19.9.9.9.1.1.1.1" xref="S4.E7.m1.19.19.19.9.9.9.1.1.1.1a.cmml">if</mtext><mspace width="1.167em" id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.2" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.2.cmml"></mspace><mrow id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.cmml"><mo rspace="0.167em" id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.1" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.1.cmml">âˆ€</mo><mi id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.2" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.2.cmml">q</mi></mrow></mrow><mo id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.2" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.2.cmml">âˆˆ</mo><msup id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.2" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.2.cmml">ğ’¬</mi><msup id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3.cmml"><mi id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3a" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3.cmml"></mi><mo id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3.1" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3.1.cmml">â€²</mo></msup></msup></mrow><mspace width="1em" id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.3" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.5a.cmml"></mspace><mrow id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.cmml"><mrow id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.cmml"><mi id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.2" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.2.cmml">Ï•</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.1" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.1.cmml">â€‹</mo><mrow id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.3.2" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.cmml"><mo stretchy="false" id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.3.2.1" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.cmml">(</mo><mi id="S4.E7.m1.19.19.19.9.9.9.2.2.2.2" xref="S4.E7.m1.19.19.19.9.9.9.2.2.2.2.cmml">q</mi><mo stretchy="false" id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.3.2.2" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.cmml">)</mo></mrow></mrow><mo id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.1" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.1.cmml">&gt;</mo><mrow id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.cmml"><mn id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.2" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.2.cmml">0</mn><mo lspace="0em" rspace="0em" id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.1" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.1.cmml">â€‹</mo><mtext id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.3" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.3a.cmml">,</mtext></mrow></mrow></mrow></mtd></mtr><mtr id="S4.E7.m1.19.19.19.9.9.9d" xref="S4.E7.m1.19.19.19.9.9.9.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E7.m1.19.19.19.9.9.9e" xref="S4.E7.m1.19.19.19.9.9.9.cmml"><mn id="S4.E7.m1.19.19.19.9.9.9.5.1.1" xref="S4.E7.m1.19.19.19.9.9.9.5.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E7.m1.19.19.19.9.9.9f" xref="S4.E7.m1.19.19.19.9.9.9.cmml"><mtext id="S4.E7.m1.19.19.19.9.9.9.5.2.1" xref="S4.E7.m1.19.19.19.9.9.9.5.2.1a.cmml">otherwise.</mtext></mtd></mtr></mtable><mi id="S4.E7.m1.23.23.3.22.11.11.11.1.2.1"></mi></mrow></mrow><mo id="S4.E7.m1.20.20.20.10.10.10">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S4.E7.m1.23b"><apply id="S4.E7.m1.21.21.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S4.E7.m1.21.21.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S4.E7.m1.21.21.1.1.1.1.1.cmml"><eq id="S4.E7.m1.6.6.6.6.6.6.cmml" xref="S4.E7.m1.6.6.6.6.6.6"></eq><apply id="S4.E7.m1.21.21.1.1.1.1.1.2.cmml"><times id="S4.E7.m1.21.21.1.1.1.1.1.2.1.cmml"></times><ci id="S4.E7.m1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1">ğ‘</ci><ci id="S4.E7.m1.2.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.2.2.2.2">ğ‘ </ci><ci id="S4.E7.m1.4.4.4.4.4.4.cmml" xref="S4.E7.m1.4.4.4.4.4.4">ğ‘˜</ci></apply><apply id="S4.E7.m1.21.21.1.1.1.1.1.3.cmml"><apply id="S4.E7.m1.21.21.1.1.1.1.1.3.1.cmml"><csymbol cd="ambiguous" id="S4.E7.m1.21.21.1.1.1.1.1.3.1.1.cmml">subscript</csymbol><sum id="S4.E7.m1.7.7.7.7.7.7.cmml" xref="S4.E7.m1.7.7.7.7.7.7"></sum><apply id="S4.E7.m1.8.8.8.8.8.8.1.3.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2"><csymbol cd="ambiguous" id="S4.E7.m1.8.8.8.8.8.8.1.3a.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.3">formulae-sequence</csymbol><apply id="S4.E7.m1.8.8.8.8.8.8.1.1.1.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1"><subset id="S4.E7.m1.8.8.8.8.8.8.1.1.1.1.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.1"></subset><apply id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2"><csymbol cd="ambiguous" id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.1.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2">superscript</csymbol><ci id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.2.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.2">ğ’¬</ci><apply id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3"><ci id="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3.1.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.2.3.1">â€²</ci></apply></apply><ci id="S4.E7.m1.8.8.8.8.8.8.1.1.1.3.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.1.1.3">ğ’¬</ci></apply><apply id="S4.E7.m1.8.8.8.8.8.8.1.2.2.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2"><eq id="S4.E7.m1.8.8.8.8.8.8.1.2.2.2.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.2"></eq><apply id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.2.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1"><abs id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.2.1.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.2"></abs><apply id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.1.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1">superscript</csymbol><ci id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.2.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.2">ğ’¬</ci><apply id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3"><ci id="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3.1.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.1.1.1.3.1">â€²</ci></apply></apply></apply><ci id="S4.E7.m1.8.8.8.8.8.8.1.2.2.3.cmml" xref="S4.E7.m1.8.8.8.8.8.8.1.2.2.3">ğ‘˜</ci></apply></apply></apply><apply id="S4.E7.m1.9.9.9.9.9.9.cmml" xref="S4.E7.m1.9.9.9.9.9.9"><divide id="S4.E7.m1.9.9.9.9.9.9.2.cmml" xref="S4.E7.m1.9.9.9.9.9.9"></divide><apply id="S4.E7.m1.9.9.9.9.9.9.1.cmml" xref="S4.E7.m1.9.9.9.9.9.9.1"><times id="S4.E7.m1.9.9.9.9.9.9.1.2.cmml" xref="S4.E7.m1.9.9.9.9.9.9.1.2"></times><ci id="S4.E7.m1.9.9.9.9.9.9.1.3.cmml" xref="S4.E7.m1.9.9.9.9.9.9.1.3">ğ‘ </ci><apply id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.cmml" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.1.cmml" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1">superscript</csymbol><ci id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.2.cmml" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.2">ğ’¬</ci><apply id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3.cmml" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3"><ci id="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3.1.cmml" xref="S4.E7.m1.9.9.9.9.9.9.1.1.1.1.3.1">â€²</ci></apply></apply></apply><apply id="S4.E7.m1.9.9.9.9.9.9.3.cmml" xref="S4.E7.m1.9.9.9.9.9.9.3"><csymbol cd="ambiguous" id="S4.E7.m1.9.9.9.9.9.9.3.1.cmml" xref="S4.E7.m1.9.9.9.9.9.9.3">superscript</csymbol><apply id="S4.E7.m1.9.9.9.9.9.9.3.2.cmml" xref="S4.E7.m1.9.9.9.9.9.9.3"><csymbol cd="ambiguous" id="S4.E7.m1.9.9.9.9.9.9.3.2.1.cmml" xref="S4.E7.m1.9.9.9.9.9.9.3">subscript</csymbol><ci id="S4.E7.m1.9.9.9.9.9.9.3.2.2.cmml" xref="S4.E7.m1.9.9.9.9.9.9.3.2.2">ğ¶</ci><ci id="S4.E7.m1.9.9.9.9.9.9.3.2.3.cmml" xref="S4.E7.m1.9.9.9.9.9.9.3.2.3">ğ‘˜</ci></apply><ci id="S4.E7.m1.9.9.9.9.9.9.3.3.cmml" xref="S4.E7.m1.9.9.9.9.9.9.3.3">ğ‘›</ci></apply></apply></apply></apply><apply id="S4.E7.m1.21.21.1.1.1.2.2.cmml"><eq id="S4.E7.m1.17.17.17.7.7.7.cmml" xref="S4.E7.m1.17.17.17.7.7.7"></eq><apply id="S4.E7.m1.21.21.1.1.1.2.2.1.cmml"><times id="S4.E7.m1.21.21.1.1.1.2.2.1.2.cmml"></times><ci id="S4.E7.m1.11.11.11.1.1.1a.cmml" xref="S4.E7.m1.11.11.11.1.1.1"><mtext id="S4.E7.m1.11.11.11.1.1.1.cmml" xref="S4.E7.m1.11.11.11.1.1.1">withÂ </mtext></ci><ci id="S4.E7.m1.12.12.12.2.2.2.cmml" xref="S4.E7.m1.12.12.12.2.2.2">ğ‘ </ci><apply id="S4.E7.m1.21.21.1.1.1.2.2.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S4.E7.m1.21.21.1.1.1.2.2.1.1.1.1.1.cmml">superscript</csymbol><ci id="S4.E7.m1.14.14.14.4.4.4.cmml" xref="S4.E7.m1.14.14.14.4.4.4">ğ’¬</ci><ci id="S4.E7.m1.15.15.15.5.5.5.1.cmml" xref="S4.E7.m1.15.15.15.5.5.5.1">â€²</ci></apply></apply><apply id="S4.E7.m1.21.21.1.1.1.2.2.3.cmml"><csymbol cd="latexml" id="S4.E7.m1.21.21.1.1.1.2.2.3.1.cmml">cases</csymbol><matrix id="S4.E7.m1.19.19.19.9.9.9.cmml" xref="S4.E7.m1.19.19.19.9.9.9"><matrixrow id="S4.E7.m1.19.19.19.9.9.9a.cmml" xref="S4.E7.m1.19.19.19.9.9.9"><cn type="integer" id="S4.E7.m1.19.19.19.9.9.9.4.5.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.5.1">1</cn><apply id="S4.E7.m1.19.19.19.9.9.9.4.4.4.5.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4"><csymbol cd="ambiguous" id="S4.E7.m1.19.19.19.9.9.9.4.4.4.5a.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.3">formulae-sequence</csymbol><apply id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1"><in id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.2"></in><list id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1"><ci id="S4.E7.m1.19.19.19.9.9.9.1.1.1.1a.cmml" xref="S4.E7.m1.19.19.19.9.9.9.1.1.1.1"><mtext id="S4.E7.m1.19.19.19.9.9.9.1.1.1.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.1.1.1.1">if</mtext></ci><apply id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1"><csymbol cd="latexml" id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.1">for-all</csymbol><ci id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.1.1.1.2">ğ‘</ci></apply></list><apply id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3"><csymbol cd="ambiguous" id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3">superscript</csymbol><ci id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.2">ğ’¬</ci><apply id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3"><ci id="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.3.3.3.3.1.3.3.1">â€²</ci></apply></apply></apply><apply id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2"><gt id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.1"></gt><apply id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2"><times id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.1"></times><ci id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.2.2">italic-Ï•</ci><ci id="S4.E7.m1.19.19.19.9.9.9.2.2.2.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.2.2.2.2">ğ‘</ci></apply><apply id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3"><times id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.1"></times><cn type="integer" id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.2.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.2">0</cn><ci id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.3a.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.3"><mtext id="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.3.cmml" xref="S4.E7.m1.19.19.19.9.9.9.4.4.4.4.2.3.3">,</mtext></ci></apply></apply></apply></matrixrow><matrixrow id="S4.E7.m1.19.19.19.9.9.9b.cmml" xref="S4.E7.m1.19.19.19.9.9.9"><cn type="integer" id="S4.E7.m1.19.19.19.9.9.9.5.1.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.5.1.1">0</cn><ci id="S4.E7.m1.19.19.19.9.9.9.5.2.1a.cmml" xref="S4.E7.m1.19.19.19.9.9.9.5.2.1"><mtext id="S4.E7.m1.19.19.19.9.9.9.5.2.1.cmml" xref="S4.E7.m1.19.19.19.9.9.9.5.2.1">otherwise.</mtext></ci></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.23c">\begin{split}&amp;cs(k)=\sum_{\mathcal{Q}^{{}^{\prime}}\subset\mathcal{Q},\left|\mathcal{Q}^{{}^{\prime}}\right|=k}\frac{s(\mathcal{Q}^{{}^{\prime}})}{{}^{n}C_{k}},\\
&amp;\text{with }s(\mathcal{Q}^{\prime})=\left\{\begin{array}[]{ll}1&amp;\text{if}\quad\forall q\in\mathcal{Q}^{{}^{\prime}}\quad\phi(q)&gt;0\text{,}\\
0&amp;\text{otherwise.}\end{array}\right.,\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S4.p5.8" class="ltx_p">where <math id="S4.p5.3.m1.1" class="ltx_Math" alttext="{}^{n}C_{k}" display="inline"><semantics id="S4.p5.3.m1.1a"><mmultiscripts id="S4.p5.3.m1.1.1" xref="S4.p5.3.m1.1.1.cmml"><mi id="S4.p5.3.m1.1.1.2.2" xref="S4.p5.3.m1.1.1.2.2.cmml">C</mi><mi id="S4.p5.3.m1.1.1.2.3" xref="S4.p5.3.m1.1.1.2.3.cmml">k</mi><mrow id="S4.p5.3.m1.1.1a" xref="S4.p5.3.m1.1.1.cmml"></mrow><mprescripts id="S4.p5.3.m1.1.1b" xref="S4.p5.3.m1.1.1.cmml"></mprescripts><mrow id="S4.p5.3.m1.1.1c" xref="S4.p5.3.m1.1.1.cmml"></mrow><mi id="S4.p5.3.m1.1.1.3" xref="S4.p5.3.m1.1.1.3.cmml">n</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.p5.3.m1.1b"><apply id="S4.p5.3.m1.1.1.cmml" xref="S4.p5.3.m1.1.1"><csymbol cd="ambiguous" id="S4.p5.3.m1.1.1.1.cmml" xref="S4.p5.3.m1.1.1">superscript</csymbol><apply id="S4.p5.3.m1.1.1.2.cmml" xref="S4.p5.3.m1.1.1"><csymbol cd="ambiguous" id="S4.p5.3.m1.1.1.2.1.cmml" xref="S4.p5.3.m1.1.1">subscript</csymbol><ci id="S4.p5.3.m1.1.1.2.2.cmml" xref="S4.p5.3.m1.1.1.2.2">ğ¶</ci><ci id="S4.p5.3.m1.1.1.2.3.cmml" xref="S4.p5.3.m1.1.1.2.3">ğ‘˜</ci></apply><ci id="S4.p5.3.m1.1.1.3.cmml" xref="S4.p5.3.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m1.1c">{}^{n}C_{k}</annotation></semantics></math> denotes the number of subsets with size <math id="S4.p5.4.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p5.4.m2.1a"><mi id="S4.p5.4.m2.1.1" xref="S4.p5.4.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p5.4.m2.1b"><ci id="S4.p5.4.m2.1.1.cmml" xref="S4.p5.4.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m2.1c">k</annotation></semantics></math> sampled from a set with size <math id="S4.p5.5.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p5.5.m3.1a"><mi id="S4.p5.5.m3.1.1" xref="S4.p5.5.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p5.5.m3.1b"><ci id="S4.p5.5.m3.1.1.cmml" xref="S4.p5.5.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m3.1c">n</annotation></semantics></math>, <math id="S4.p5.6.m4.1" class="ltx_Math" alttext="\mathcal{Q}^{{}^{\prime}}" display="inline"><semantics id="S4.p5.6.m4.1a"><msup id="S4.p5.6.m4.1.1" xref="S4.p5.6.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p5.6.m4.1.1.2" xref="S4.p5.6.m4.1.1.2.cmml">ğ’¬</mi><msup id="S4.p5.6.m4.1.1.3" xref="S4.p5.6.m4.1.1.3.cmml"><mi id="S4.p5.6.m4.1.1.3a" xref="S4.p5.6.m4.1.1.3.cmml"></mi><mo id="S4.p5.6.m4.1.1.3.1" xref="S4.p5.6.m4.1.1.3.1.cmml">â€²</mo></msup></msup><annotation-xml encoding="MathML-Content" id="S4.p5.6.m4.1b"><apply id="S4.p5.6.m4.1.1.cmml" xref="S4.p5.6.m4.1.1"><csymbol cd="ambiguous" id="S4.p5.6.m4.1.1.1.cmml" xref="S4.p5.6.m4.1.1">superscript</csymbol><ci id="S4.p5.6.m4.1.1.2.cmml" xref="S4.p5.6.m4.1.1.2">ğ’¬</ci><apply id="S4.p5.6.m4.1.1.3.cmml" xref="S4.p5.6.m4.1.1.3"><ci id="S4.p5.6.m4.1.1.3.1.cmml" xref="S4.p5.6.m4.1.1.3.1">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.6.m4.1c">\mathcal{Q}^{{}^{\prime}}</annotation></semantics></math> denotes a group of questions contained in <math id="S4.p5.7.m5.1" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics id="S4.p5.7.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p5.7.m5.1.1" xref="S4.p5.7.m5.1.1.cmml">ğ’¬</mi><annotation-xml encoding="MathML-Content" id="S4.p5.7.m5.1b"><ci id="S4.p5.7.m5.1.1.cmml" xref="S4.p5.7.m5.1.1">ğ’¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.7.m5.1c">\mathcal{Q}</annotation></semantics></math>, and <math id="S4.p5.8.m6.1" class="ltx_Math" alttext="\phi(q)" display="inline"><semantics id="S4.p5.8.m6.1a"><mrow id="S4.p5.8.m6.1.2" xref="S4.p5.8.m6.1.2.cmml"><mi id="S4.p5.8.m6.1.2.2" xref="S4.p5.8.m6.1.2.2.cmml">Ï•</mi><mo lspace="0em" rspace="0em" id="S4.p5.8.m6.1.2.1" xref="S4.p5.8.m6.1.2.1.cmml">â€‹</mo><mrow id="S4.p5.8.m6.1.2.3.2" xref="S4.p5.8.m6.1.2.cmml"><mo stretchy="false" id="S4.p5.8.m6.1.2.3.2.1" xref="S4.p5.8.m6.1.2.cmml">(</mo><mi id="S4.p5.8.m6.1.1" xref="S4.p5.8.m6.1.1.cmml">q</mi><mo stretchy="false" id="S4.p5.8.m6.1.2.3.2.2" xref="S4.p5.8.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.8.m6.1b"><apply id="S4.p5.8.m6.1.2.cmml" xref="S4.p5.8.m6.1.2"><times id="S4.p5.8.m6.1.2.1.cmml" xref="S4.p5.8.m6.1.2.1"></times><ci id="S4.p5.8.m6.1.2.2.cmml" xref="S4.p5.8.m6.1.2.2">italic-Ï•</ci><ci id="S4.p5.8.m6.1.1.cmml" xref="S4.p5.8.m6.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.8.m6.1c">\phi(q)</annotation></semantics></math> is a non-zero accuracy that is defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Debiasing Methods</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In recent years, based on generic methods such as UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, SMRL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, a variety of VQA debiasing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> have been proposed to improve robustness. We divide these methods into four categories according to the debiasing technique: <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">ensemble learning, data augmentation, self-supervised contrastive learning, and answer re-ranking</em>. Table <a href="#S5.T2" title="TABLE II â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents their typology as well as their performance on both the VQA-CP v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> test split and the VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> validation split.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>The typology and performance of existing debiasing methods. HM denotes the harmonic mean of overall accuracy. â—Â represents the main category that a method belongs to, while â—‹Â  denotes the method that also uses the other debiasing technique. The result marked in bold is the best performance on the dataset. The result of the method with two references following is reported by the latter reference. LM denotes the Learned Mixin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, while LMH denotes LM with an entropy penalty.</figcaption>
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:610.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-232.5pt,327.4pt) scale(0.48249913174599,0.48249913174599) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_right ltx_border_tt"><span id="S5.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Base</span></td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Year</span></td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S5.T2.1.1.1.4.1" class="ltx_text"></span> <span id="S5.T2.1.1.1.4.2" class="ltx_text">
<span id="S5.T2.1.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.1.4.2.1.1" class="ltx_tr">
<span id="S5.T2.1.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Ensemble</span></span></span>
<span id="S5.T2.1.1.1.4.2.1.2" class="ltx_tr">
<span id="S5.T2.1.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.4.2.1.2.1.1" class="ltx_text ltx_font_bold">Learning</span></span></span>
</span></span><span id="S5.T2.1.1.1.4.3" class="ltx_text"></span></td>
<td id="S5.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S5.T2.1.1.1.5.1" class="ltx_text"></span> <span id="S5.T2.1.1.1.5.2" class="ltx_text">
<span id="S5.T2.1.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.1.5.2.1.1" class="ltx_tr">
<span id="S5.T2.1.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.5.2.1.1.1.1" class="ltx_text ltx_font_bold">Data</span></span></span>
<span id="S5.T2.1.1.1.5.2.1.2" class="ltx_tr">
<span id="S5.T2.1.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.5.2.1.2.1.1" class="ltx_text ltx_font_bold">Augmentation</span></span></span>
</span></span><span id="S5.T2.1.1.1.5.3" class="ltx_text"></span></td>
<td id="S5.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S5.T2.1.1.1.6.1" class="ltx_text"></span> <span id="S5.T2.1.1.1.6.2" class="ltx_text">
<span id="S5.T2.1.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.1.6.2.1.1" class="ltx_tr">
<span id="S5.T2.1.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.6.2.1.1.1.1" class="ltx_text ltx_font_bold">Self-Supervised</span></span></span>
<span id="S5.T2.1.1.1.6.2.1.2" class="ltx_tr">
<span id="S5.T2.1.1.1.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.6.2.1.2.1.1" class="ltx_text ltx_font_bold">Contrastive Learning</span></span></span>
</span></span><span id="S5.T2.1.1.1.6.3" class="ltx_text"></span></td>
<td id="S5.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S5.T2.1.1.1.7.1" class="ltx_text"></span> <span id="S5.T2.1.1.1.7.2" class="ltx_text">
<span id="S5.T2.1.1.1.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.1.7.2.1.1" class="ltx_tr">
<span id="S5.T2.1.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.7.2.1.1.1.1" class="ltx_text ltx_font_bold">Answer</span></span></span>
<span id="S5.T2.1.1.1.7.2.1.2" class="ltx_tr">
<span id="S5.T2.1.1.1.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.7.2.1.2.1.1" class="ltx_text ltx_font_bold">Re-Ranking</span></span></span>
</span></span><span id="S5.T2.1.1.1.7.3" class="ltx_text"></span></td>
<td id="S5.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S5.T2.1.1.1.8.1" class="ltx_text"></span> <span id="S5.T2.1.1.1.8.2" class="ltx_text">
<span id="S5.T2.1.1.1.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.1.8.2.1.1" class="ltx_tr">
<span id="S5.T2.1.1.1.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.8.2.1.1.1.1" class="ltx_text ltx_font_bold">VQA-CP v2</span></span></span>
<span id="S5.T2.1.1.1.8.2.1.2" class="ltx_tr">
<span id="S5.T2.1.1.1.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.8.2.1.2.1.1" class="ltx_text ltx_font_bold">test</span></span></span>
</span></span><span id="S5.T2.1.1.1.8.3" class="ltx_text"></span></td>
<td id="S5.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S5.T2.1.1.1.9.1" class="ltx_text"></span> <span id="S5.T2.1.1.1.9.2" class="ltx_text">
<span id="S5.T2.1.1.1.9.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.1.9.2.1.1" class="ltx_tr">
<span id="S5.T2.1.1.1.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.9.2.1.1.1.1" class="ltx_text ltx_font_bold">VQA v2</span></span></span>
<span id="S5.T2.1.1.1.9.2.1.2" class="ltx_tr">
<span id="S5.T2.1.1.1.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.1.9.2.1.2.1.1" class="ltx_text ltx_font_bold">validation</span></span></span>
</span></span><span id="S5.T2.1.1.1.9.3" class="ltx_text"></span></td>
<td id="S5.T2.1.1.1.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.1.10.1" class="ltx_text ltx_font_bold">HM</span></td>
</tr>
<tr id="S5.T2.1.1.2" class="ltx_tr">
<td id="S5.T2.1.1.2.1" class="ltx_td ltx_align_right ltx_border_t">Template-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
</td>
<td id="S5.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">UpDn</td>
<td id="S5.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">2017</td>
<td id="S5.T2.1.1.2.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">â—</td>
<td id="S5.T2.1.1.2.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.2.7" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t">39.75</td>
<td id="S5.T2.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t">63.83</td>
<td id="S5.T2.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t">48.99</td>
</tr>
<tr id="S5.T2.1.1.3" class="ltx_tr">
<td id="S5.T2.1.1.3.1" class="ltx_td ltx_align_right">GVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S5.T2.1.1.3.2" class="ltx_td ltx_align_center">SAN</td>
<td id="S5.T2.1.1.3.3" class="ltx_td ltx_align_center">2018</td>
<td id="S5.T2.1.1.3.4" class="ltx_td"></td>
<td id="S5.T2.1.1.3.5" class="ltx_td"></td>
<td id="S5.T2.1.1.3.6" class="ltx_td"></td>
<td id="S5.T2.1.1.3.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.3.8" class="ltx_td ltx_align_center">31.30</td>
<td id="S5.T2.1.1.3.9" class="ltx_td ltx_align_center">48.24</td>
<td id="S5.T2.1.1.3.10" class="ltx_td ltx_align_center">37.97</td>
</tr>
<tr id="S5.T2.1.1.4" class="ltx_tr">
<td id="S5.T2.1.1.4.1" class="ltx_td ltx_align_right">AdvReg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>
</td>
<td id="S5.T2.1.1.4.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.4.3" class="ltx_td ltx_align_center">2018</td>
<td id="S5.T2.1.1.4.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.4.5" class="ltx_td"></td>
<td id="S5.T2.1.1.4.6" class="ltx_td"></td>
<td id="S5.T2.1.1.4.7" class="ltx_td"></td>
<td id="S5.T2.1.1.4.8" class="ltx_td ltx_align_center">41.17</td>
<td id="S5.T2.1.1.4.9" class="ltx_td ltx_align_center">62.75</td>
<td id="S5.T2.1.1.4.10" class="ltx_td ltx_align_center">49.72</td>
</tr>
<tr id="S5.T2.1.1.5" class="ltx_tr">
<td id="S5.T2.1.1.5.1" class="ltx_td ltx_align_right">AttAlign <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>
</td>
<td id="S5.T2.1.1.5.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.5.3" class="ltx_td ltx_align_center">2019</td>
<td id="S5.T2.1.1.5.4" class="ltx_td"></td>
<td id="S5.T2.1.1.5.5" class="ltx_td"></td>
<td id="S5.T2.1.1.5.6" class="ltx_td"></td>
<td id="S5.T2.1.1.5.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.5.8" class="ltx_td ltx_align_center">39.37</td>
<td id="S5.T2.1.1.5.9" class="ltx_td ltx_align_center">63.24</td>
<td id="S5.T2.1.1.5.10" class="ltx_td ltx_align_center">48.53</td>
</tr>
<tr id="S5.T2.1.1.6" class="ltx_tr">
<td id="S5.T2.1.1.6.1" class="ltx_td ltx_align_right">RUBi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S5.T2.1.1.6.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.6.3" class="ltx_td ltx_align_center">2019</td>
<td id="S5.T2.1.1.6.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.6.5" class="ltx_td"></td>
<td id="S5.T2.1.1.6.6" class="ltx_td"></td>
<td id="S5.T2.1.1.6.7" class="ltx_td"></td>
<td id="S5.T2.1.1.6.8" class="ltx_td ltx_align_center">47.11</td>
<td id="S5.T2.1.1.6.9" class="ltx_td ltx_align_center">61.16</td>
<td id="S5.T2.1.1.6.10" class="ltx_td ltx_align_center">53.22</td>
</tr>
<tr id="S5.T2.1.1.7" class="ltx_tr">
<td id="S5.T2.1.1.7.1" class="ltx_td ltx_align_right">HINT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>
</td>
<td id="S5.T2.1.1.7.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.7.3" class="ltx_td ltx_align_center">2019</td>
<td id="S5.T2.1.1.7.4" class="ltx_td"></td>
<td id="S5.T2.1.1.7.5" class="ltx_td"></td>
<td id="S5.T2.1.1.7.6" class="ltx_td"></td>
<td id="S5.T2.1.1.7.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.7.8" class="ltx_td ltx_align_center">46.73</td>
<td id="S5.T2.1.1.7.9" class="ltx_td ltx_align_center">63.38</td>
<td id="S5.T2.1.1.7.10" class="ltx_td ltx_align_center">53.80</td>
</tr>
<tr id="S5.T2.1.1.8" class="ltx_tr">
<td id="S5.T2.1.1.8.1" class="ltx_td ltx_align_right">LMH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>
</td>
<td id="S5.T2.1.1.8.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.8.3" class="ltx_td ltx_align_center">2019</td>
<td id="S5.T2.1.1.8.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.8.5" class="ltx_td"></td>
<td id="S5.T2.1.1.8.6" class="ltx_td"></td>
<td id="S5.T2.1.1.8.7" class="ltx_td"></td>
<td id="S5.T2.1.1.8.8" class="ltx_td ltx_align_center">52.87</td>
<td id="S5.T2.1.1.8.9" class="ltx_td ltx_align_center">55.99</td>
<td id="S5.T2.1.1.8.10" class="ltx_td ltx_align_center">54.39</td>
</tr>
<tr id="S5.T2.1.1.9" class="ltx_tr">
<td id="S5.T2.1.1.9.1" class="ltx_td ltx_align_right">SCR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>
</td>
<td id="S5.T2.1.1.9.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.9.3" class="ltx_td ltx_align_center">2019</td>
<td id="S5.T2.1.1.9.4" class="ltx_td"></td>
<td id="S5.T2.1.1.9.5" class="ltx_td"></td>
<td id="S5.T2.1.1.9.6" class="ltx_td"></td>
<td id="S5.T2.1.1.9.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.9.8" class="ltx_td ltx_align_center">49.45</td>
<td id="S5.T2.1.1.9.9" class="ltx_td ltx_align_center">62.20</td>
<td id="S5.T2.1.1.9.10" class="ltx_td ltx_align_center">55.10</td>
</tr>
<tr id="S5.T2.1.1.10" class="ltx_tr">
<td id="S5.T2.1.1.10.1" class="ltx_td ltx_align_right">ASL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>
</td>
<td id="S5.T2.1.1.10.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.10.3" class="ltx_td ltx_align_center">2019</td>
<td id="S5.T2.1.1.10.4" class="ltx_td"></td>
<td id="S5.T2.1.1.10.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.10.6" class="ltx_td"></td>
<td id="S5.T2.1.1.10.7" class="ltx_td"></td>
<td id="S5.T2.1.1.10.8" class="ltx_td ltx_align_center">46.00</td>
<td id="S5.T2.1.1.10.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.10.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.11" class="ltx_tr">
<td id="S5.T2.1.1.11.1" class="ltx_td ltx_align_right ltx_border_t">DLR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>
</td>
<td id="S5.T2.1.1.11.2" class="ltx_td ltx_align_center ltx_border_t">SAN</td>
<td id="S5.T2.1.1.11.3" class="ltx_td ltx_align_center ltx_border_t">2020</td>
<td id="S5.T2.1.1.11.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.11.5" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.11.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.11.7" class="ltx_td ltx_align_center ltx_border_t">â—</td>
<td id="S5.T2.1.1.11.8" class="ltx_td ltx_align_center ltx_border_t">34.83</td>
<td id="S5.T2.1.1.11.9" class="ltx_td ltx_align_center ltx_border_t">49.27</td>
<td id="S5.T2.1.1.11.10" class="ltx_td ltx_align_center ltx_border_t">40.81</td>
</tr>
<tr id="S5.T2.1.1.12" class="ltx_tr">
<td id="S5.T2.1.1.12.1" class="ltx_td ltx_align_right">CSS+CL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
<td id="S5.T2.1.1.12.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.12.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.12.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.12.5" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.12.6" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.12.7" class="ltx_td"></td>
<td id="S5.T2.1.1.12.8" class="ltx_td ltx_align_center">40.49</td>
<td id="S5.T2.1.1.12.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.12.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.13" class="ltx_tr">
<td id="S5.T2.1.1.13.1" class="ltx_td ltx_align_right">CVL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>
</td>
<td id="S5.T2.1.1.13.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.13.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.13.4" class="ltx_td"></td>
<td id="S5.T2.1.1.13.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.13.6" class="ltx_td"></td>
<td id="S5.T2.1.1.13.7" class="ltx_td"></td>
<td id="S5.T2.1.1.13.8" class="ltx_td ltx_align_center">42.12</td>
<td id="S5.T2.1.1.13.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.13.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.14" class="ltx_tr">
<td id="S5.T2.1.1.14.1" class="ltx_td ltx_align_right">CVL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>
</td>
<td id="S5.T2.1.1.14.2" class="ltx_td ltx_align_center">Pythia</td>
<td id="S5.T2.1.1.14.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.14.4" class="ltx_td"></td>
<td id="S5.T2.1.1.14.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.14.6" class="ltx_td"></td>
<td id="S5.T2.1.1.14.7" class="ltx_td"></td>
<td id="S5.T2.1.1.14.8" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.14.9" class="ltx_td ltx_align_center">68.77</td>
<td id="S5.T2.1.1.14.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.15" class="ltx_tr">
<td id="S5.T2.1.1.15.1" class="ltx_td ltx_align_right">GradSup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>
</td>
<td id="S5.T2.1.1.15.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.15.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.15.4" class="ltx_td"></td>
<td id="S5.T2.1.1.15.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.15.6" class="ltx_td"></td>
<td id="S5.T2.1.1.15.7" class="ltx_td"></td>
<td id="S5.T2.1.1.15.8" class="ltx_td ltx_align_center">46.80</td>
<td id="S5.T2.1.1.15.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.15.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.16" class="ltx_tr">
<td id="S5.T2.1.1.16.1" class="ltx_td ltx_align_right">RankVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>
</td>
<td id="S5.T2.1.1.16.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.16.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.16.4" class="ltx_td"></td>
<td id="S5.T2.1.1.16.5" class="ltx_td"></td>
<td id="S5.T2.1.1.16.6" class="ltx_td"></td>
<td id="S5.T2.1.1.16.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.16.8" class="ltx_td ltx_align_center">43.05</td>
<td id="S5.T2.1.1.16.9" class="ltx_td ltx_align_center">65.42</td>
<td id="S5.T2.1.1.16.10" class="ltx_td ltx_align_center">51.93</td>
</tr>
<tr id="S5.T2.1.1.17" class="ltx_tr">
<td id="S5.T2.1.1.17.1" class="ltx_td ltx_align_right">DLR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>
</td>
<td id="S5.T2.1.1.17.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.17.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.17.4" class="ltx_td"></td>
<td id="S5.T2.1.1.17.5" class="ltx_td"></td>
<td id="S5.T2.1.1.17.6" class="ltx_td"></td>
<td id="S5.T2.1.1.17.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.17.8" class="ltx_td ltx_align_center">48.87</td>
<td id="S5.T2.1.1.17.9" class="ltx_td ltx_align_center">57.96</td>
<td id="S5.T2.1.1.17.10" class="ltx_td ltx_align_center">53.03</td>
</tr>
<tr id="S5.T2.1.1.18" class="ltx_tr">
<td id="S5.T2.1.1.18.1" class="ltx_td ltx_align_right">SimpleReg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>
</td>
<td id="S5.T2.1.1.18.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.18.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.18.4" class="ltx_td"></td>
<td id="S5.T2.1.1.18.5" class="ltx_td"></td>
<td id="S5.T2.1.1.18.6" class="ltx_td"></td>
<td id="S5.T2.1.1.18.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.18.8" class="ltx_td ltx_align_center">48.90</td>
<td id="S5.T2.1.1.18.9" class="ltx_td ltx_align_center">62.60</td>
<td id="S5.T2.1.1.18.10" class="ltx_td ltx_align_center">54.91</td>
</tr>
<tr id="S5.T2.1.1.19" class="ltx_tr">
<td id="S5.T2.1.1.19.1" class="ltx_td ltx_align_right">MFE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>
</td>
<td id="S5.T2.1.1.19.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.19.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.19.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.19.5" class="ltx_td"></td>
<td id="S5.T2.1.1.19.6" class="ltx_td"></td>
<td id="S5.T2.1.1.19.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.19.8" class="ltx_td ltx_align_center">58.21</td>
<td id="S5.T2.1.1.19.9" class="ltx_td ltx_align_center">53.15</td>
<td id="S5.T2.1.1.19.10" class="ltx_td ltx_align_center">55.57</td>
</tr>
<tr id="S5.T2.1.1.20" class="ltx_tr">
<td id="S5.T2.1.1.20.1" class="ltx_td ltx_align_right">VGQE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>
</td>
<td id="S5.T2.1.1.20.2" class="ltx_td ltx_align_center">SMRL</td>
<td id="S5.T2.1.1.20.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.20.4" class="ltx_td"></td>
<td id="S5.T2.1.1.20.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.20.6" class="ltx_td"></td>
<td id="S5.T2.1.1.20.7" class="ltx_td"></td>
<td id="S5.T2.1.1.20.8" class="ltx_td ltx_align_center">50.11</td>
<td id="S5.T2.1.1.20.9" class="ltx_td ltx_align_center">63.18</td>
<td id="S5.T2.1.1.20.10" class="ltx_td ltx_align_center">55.89</td>
</tr>
<tr id="S5.T2.1.1.21" class="ltx_tr">
<td id="S5.T2.1.1.21.1" class="ltx_td ltx_align_right">RandImg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>
</td>
<td id="S5.T2.1.1.21.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.21.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.21.4" class="ltx_td"></td>
<td id="S5.T2.1.1.21.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.21.6" class="ltx_td"></td>
<td id="S5.T2.1.1.21.7" class="ltx_td"></td>
<td id="S5.T2.1.1.21.8" class="ltx_td ltx_align_center">55.37</td>
<td id="S5.T2.1.1.21.9" class="ltx_td ltx_align_center">57.24</td>
<td id="S5.T2.1.1.21.10" class="ltx_td ltx_align_center">56.29</td>
</tr>
<tr id="S5.T2.1.1.22" class="ltx_tr">
<td id="S5.T2.1.1.22.1" class="ltx_td ltx_align_right">CSS+CL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
<td id="S5.T2.1.1.22.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.22.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.22.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.22.5" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.22.6" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.22.7" class="ltx_td"></td>
<td id="S5.T2.1.1.22.8" class="ltx_td ltx_align_center">59.18</td>
<td id="S5.T2.1.1.22.9" class="ltx_td ltx_align_center">57.29</td>
<td id="S5.T2.1.1.22.10" class="ltx_td ltx_align_center">58.22</td>
</tr>
<tr id="S5.T2.1.1.23" class="ltx_tr">
<td id="S5.T2.1.1.23.1" class="ltx_td ltx_align_right">VILLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>
</td>
<td id="S5.T2.1.1.23.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.23.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.23.4" class="ltx_td"></td>
<td id="S5.T2.1.1.23.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.23.6" class="ltx_td"></td>
<td id="S5.T2.1.1.23.7" class="ltx_td"></td>
<td id="S5.T2.1.1.23.8" class="ltx_td ltx_align_center">49.10</td>
<td id="S5.T2.1.1.23.9" class="ltx_td ltx_align_center">74.69</td>
<td id="S5.T2.1.1.23.10" class="ltx_td ltx_align_center">59.25</td>
</tr>
<tr id="S5.T2.1.1.24" class="ltx_tr">
<td id="S5.T2.1.1.24.1" class="ltx_td ltx_align_right">CSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S5.T2.1.1.24.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.24.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.24.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.24.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.24.6" class="ltx_td"></td>
<td id="S5.T2.1.1.24.7" class="ltx_td"></td>
<td id="S5.T2.1.1.24.8" class="ltx_td ltx_align_center">58.95</td>
<td id="S5.T2.1.1.24.9" class="ltx_td ltx_align_center">59.91</td>
<td id="S5.T2.1.1.24.10" class="ltx_td ltx_align_center">59.43</td>
</tr>
<tr id="S5.T2.1.1.25" class="ltx_tr">
<td id="S5.T2.1.1.25.1" class="ltx_td ltx_align_right">MANGO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>
</td>
<td id="S5.T2.1.1.25.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.25.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.25.4" class="ltx_td"></td>
<td id="S5.T2.1.1.25.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.25.6" class="ltx_td"></td>
<td id="S5.T2.1.1.25.7" class="ltx_td"></td>
<td id="S5.T2.1.1.25.8" class="ltx_td ltx_align_center">52.76</td>
<td id="S5.T2.1.1.25.9" class="ltx_td ltx_align_center">74.26</td>
<td id="S5.T2.1.1.25.10" class="ltx_td ltx_align_center">61.69</td>
</tr>
<tr id="S5.T2.1.1.26" class="ltx_tr">
<td id="S5.T2.1.1.26.1" class="ltx_td ltx_align_right">MUTANT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>
</td>
<td id="S5.T2.1.1.26.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.26.3" class="ltx_td ltx_align_center">2020</td>
<td id="S5.T2.1.1.26.4" class="ltx_td"></td>
<td id="S5.T2.1.1.26.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.26.6" class="ltx_td"></td>
<td id="S5.T2.1.1.26.7" class="ltx_td"></td>
<td id="S5.T2.1.1.26.8" class="ltx_td ltx_align_center">61.72</td>
<td id="S5.T2.1.1.26.9" class="ltx_td ltx_align_center">62.56</td>
<td id="S5.T2.1.1.26.10" class="ltx_td ltx_align_center">62.14</td>
</tr>
<tr id="S5.T2.1.1.27" class="ltx_tr">
<td id="S5.T2.1.1.27.1" class="ltx_td ltx_align_right ltx_border_t">X-GGM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
<td id="S5.T2.1.1.27.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T2.1.1.27.3" class="ltx_td ltx_align_center ltx_border_t">2021</td>
<td id="S5.T2.1.1.27.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.27.5" class="ltx_td ltx_align_center ltx_border_t">â—</td>
<td id="S5.T2.1.1.27.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.27.7" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.27.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T2.1.1.27.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T2.1.1.27.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T2.1.1.28" class="ltx_tr">
<td id="S5.T2.1.1.28.1" class="ltx_td ltx_align_right">MCE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>
</td>
<td id="S5.T2.1.1.28.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.28.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.28.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.28.5" class="ltx_td"></td>
<td id="S5.T2.1.1.28.6" class="ltx_td"></td>
<td id="S5.T2.1.1.28.7" class="ltx_td"></td>
<td id="S5.T2.1.1.28.8" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.28.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.28.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.29" class="ltx_tr">
<td id="S5.T2.1.1.29.1" class="ltx_td ltx_align_right">LPF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
<td id="S5.T2.1.1.29.2" class="ltx_td ltx_align_center">BAN</td>
<td id="S5.T2.1.1.29.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.29.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.29.5" class="ltx_td"></td>
<td id="S5.T2.1.1.29.6" class="ltx_td"></td>
<td id="S5.T2.1.1.29.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.29.8" class="ltx_td ltx_align_center">50.76</td>
<td id="S5.T2.1.1.29.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.29.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.30" class="ltx_tr">
<td id="S5.T2.1.1.30.1" class="ltx_td ltx_align_right">LPF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
<td id="S5.T2.1.1.30.2" class="ltx_td ltx_align_center">SMRL</td>
<td id="S5.T2.1.1.30.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.30.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.30.5" class="ltx_td"></td>
<td id="S5.T2.1.1.30.6" class="ltx_td"></td>
<td id="S5.T2.1.1.30.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.30.8" class="ltx_td ltx_align_center">53.38</td>
<td id="S5.T2.1.1.30.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.30.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.31" class="ltx_tr">
<td id="S5.T2.1.1.31.1" class="ltx_td ltx_align_right">LBCL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</td>
<td id="S5.T2.1.1.31.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.31.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.31.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.31.5" class="ltx_td"></td>
<td id="S5.T2.1.1.31.6" class="ltx_td"></td>
<td id="S5.T2.1.1.31.7" class="ltx_td"></td>
<td id="S5.T2.1.1.31.8" class="ltx_td ltx_align_center">60.74</td>
<td id="S5.T2.1.1.31.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.31.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.32" class="ltx_tr">
<td id="S5.T2.1.1.32.1" class="ltx_td ltx_align_right">AdVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>
</td>
<td id="S5.T2.1.1.32.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.32.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.32.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.32.5" class="ltx_td"></td>
<td id="S5.T2.1.1.32.6" class="ltx_td"></td>
<td id="S5.T2.1.1.32.7" class="ltx_td"></td>
<td id="S5.T2.1.1.32.8" class="ltx_td ltx_align_center">54.67</td>
<td id="S5.T2.1.1.32.9" class="ltx_td ltx_align_center">46.98</td>
<td id="S5.T2.1.1.32.10" class="ltx_td ltx_align_center">50.53</td>
</tr>
<tr id="S5.T2.1.1.33" class="ltx_tr">
<td id="S5.T2.1.1.33.1" class="ltx_td ltx_align_right">Unshuffling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>
</td>
<td id="S5.T2.1.1.33.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.33.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.33.4" class="ltx_td"></td>
<td id="S5.T2.1.1.33.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.33.6" class="ltx_td"></td>
<td id="S5.T2.1.1.33.7" class="ltx_td"></td>
<td id="S5.T2.1.1.33.8" class="ltx_td ltx_align_center">43.37</td>
<td id="S5.T2.1.1.33.9" class="ltx_td ltx_align_center">63.47</td>
<td id="S5.T2.1.1.33.10" class="ltx_td ltx_align_center">51.53</td>
</tr>
<tr id="S5.T2.1.1.34" class="ltx_tr">
<td id="S5.T2.1.1.34.1" class="ltx_td ltx_align_right">IntroD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S5.T2.1.1.34.2" class="ltx_td ltx_align_center">RUBi</td>
<td id="S5.T2.1.1.34.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.34.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.34.5" class="ltx_td"></td>
<td id="S5.T2.1.1.34.6" class="ltx_td"></td>
<td id="S5.T2.1.1.34.7" class="ltx_td"></td>
<td id="S5.T2.1.1.34.8" class="ltx_td ltx_align_center">48.54</td>
<td id="S5.T2.1.1.34.9" class="ltx_td ltx_align_center">61.86</td>
<td id="S5.T2.1.1.34.10" class="ltx_td ltx_align_center">54.40</td>
</tr>
<tr id="S5.T2.1.1.35" class="ltx_tr">
<td id="S5.T2.1.1.35.1" class="ltx_td ltx_align_right">IntroD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S5.T2.1.1.35.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.35.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.35.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.35.5" class="ltx_td"></td>
<td id="S5.T2.1.1.35.6" class="ltx_td"></td>
<td id="S5.T2.1.1.35.7" class="ltx_td"></td>
<td id="S5.T2.1.1.35.8" class="ltx_td ltx_align_center">51.31</td>
<td id="S5.T2.1.1.35.9" class="ltx_td ltx_align_center">62.05</td>
<td id="S5.T2.1.1.35.10" class="ltx_td ltx_align_center">56.17</td>
</tr>
<tr id="S5.T2.1.1.36" class="ltx_tr">
<td id="S5.T2.1.1.36.1" class="ltx_td ltx_align_right">LPF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
<td id="S5.T2.1.1.36.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.36.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.36.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.36.5" class="ltx_td"></td>
<td id="S5.T2.1.1.36.6" class="ltx_td"></td>
<td id="S5.T2.1.1.36.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.36.8" class="ltx_td ltx_align_center">51.57</td>
<td id="S5.T2.1.1.36.9" class="ltx_td ltx_align_center">62.63</td>
<td id="S5.T2.1.1.36.10" class="ltx_td ltx_align_center">56.56</td>
</tr>
<tr id="S5.T2.1.1.37" class="ltx_tr">
<td id="S5.T2.1.1.37.1" class="ltx_td ltx_align_right">CF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>
</td>
<td id="S5.T2.1.1.37.2" class="ltx_td ltx_align_center">SMRL</td>
<td id="S5.T2.1.1.37.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.37.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.37.5" class="ltx_td"></td>
<td id="S5.T2.1.1.37.6" class="ltx_td"></td>
<td id="S5.T2.1.1.37.7" class="ltx_td"></td>
<td id="S5.T2.1.1.37.8" class="ltx_td ltx_align_center">53.55</td>
<td id="S5.T2.1.1.37.9" class="ltx_td ltx_align_center">60.94</td>
<td id="S5.T2.1.1.37.10" class="ltx_td ltx_align_center">57.01</td>
</tr>
<tr id="S5.T2.1.1.38" class="ltx_tr">
<td id="S5.T2.1.1.38.1" class="ltx_td ltx_align_right">CIKD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>
</td>
<td id="S5.T2.1.1.38.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.38.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.38.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.38.5" class="ltx_td"></td>
<td id="S5.T2.1.1.38.6" class="ltx_td"></td>
<td id="S5.T2.1.1.38.7" class="ltx_td"></td>
<td id="S5.T2.1.1.38.8" class="ltx_td ltx_align_center">54.05</td>
<td id="S5.T2.1.1.38.9" class="ltx_td ltx_align_center">61.29</td>
<td id="S5.T2.1.1.38.10" class="ltx_td ltx_align_center">57.44</td>
</tr>
<tr id="S5.T2.1.1.39" class="ltx_tr">
<td id="S5.T2.1.1.39.1" class="ltx_td ltx_align_right">SimpleAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
</td>
<td id="S5.T2.1.1.39.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.39.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.39.4" class="ltx_td"></td>
<td id="S5.T2.1.1.39.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.39.6" class="ltx_td"></td>
<td id="S5.T2.1.1.39.7" class="ltx_td"></td>
<td id="S5.T2.1.1.39.8" class="ltx_td ltx_align_center">53.70</td>
<td id="S5.T2.1.1.39.9" class="ltx_td ltx_align_center">62.63</td>
<td id="S5.T2.1.1.39.10" class="ltx_td ltx_align_center">57.82</td>
</tr>
<tr id="S5.T2.1.1.40" class="ltx_tr">
<td id="S5.T2.1.1.40.1" class="ltx_td ltx_align_right">SimpleAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
</td>
<td id="S5.T2.1.1.40.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.40.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.40.4" class="ltx_td"></td>
<td id="S5.T2.1.1.40.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.40.6" class="ltx_td"></td>
<td id="S5.T2.1.1.40.7" class="ltx_td"></td>
<td id="S5.T2.1.1.40.8" class="ltx_td ltx_align_center">52.65</td>
<td id="S5.T2.1.1.40.9" class="ltx_td ltx_align_center">64.34</td>
<td id="S5.T2.1.1.40.10" class="ltx_td ltx_align_center">57.91</td>
</tr>
<tr id="S5.T2.1.1.41" class="ltx_tr">
<td id="S5.T2.1.1.41.1" class="ltx_td ltx_align_right">GGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>
</td>
<td id="S5.T2.1.1.41.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.41.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.41.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.41.5" class="ltx_td"></td>
<td id="S5.T2.1.1.41.6" class="ltx_td"></td>
<td id="S5.T2.1.1.41.7" class="ltx_td"></td>
<td id="S5.T2.1.1.41.8" class="ltx_td ltx_align_center">57.32</td>
<td id="S5.T2.1.1.41.9" class="ltx_td ltx_align_center">59.11</td>
<td id="S5.T2.1.1.41.10" class="ltx_td ltx_align_center">58.20</td>
</tr>
<tr id="S5.T2.1.1.42" class="ltx_tr">
<td id="S5.T2.1.1.42.1" class="ltx_td ltx_align_right">CF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>
</td>
<td id="S5.T2.1.1.42.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.42.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.42.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.42.5" class="ltx_td"></td>
<td id="S5.T2.1.1.42.6" class="ltx_td"></td>
<td id="S5.T2.1.1.42.7" class="ltx_td"></td>
<td id="S5.T2.1.1.42.8" class="ltx_td ltx_align_center">55.05</td>
<td id="S5.T2.1.1.42.9" class="ltx_td ltx_align_center">63.73</td>
<td id="S5.T2.1.1.42.10" class="ltx_td ltx_align_center">59.07</td>
</tr>
<tr id="S5.T2.1.1.43" class="ltx_tr">
<td id="S5.T2.1.1.43.1" class="ltx_td ltx_align_right">CCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>
</td>
<td id="S5.T2.1.1.43.2" class="ltx_td ltx_align_center">HINT</td>
<td id="S5.T2.1.1.43.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.43.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.43.5" class="ltx_td"></td>
<td id="S5.T2.1.1.43.6" class="ltx_td"></td>
<td id="S5.T2.1.1.43.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.43.8" class="ltx_td ltx_align_center">59.12</td>
<td id="S5.T2.1.1.43.9" class="ltx_td ltx_align_center">59.17</td>
<td id="S5.T2.1.1.43.10" class="ltx_td ltx_align_center">59.14</td>
</tr>
<tr id="S5.T2.1.1.44" class="ltx_tr">
<td id="S5.T2.1.1.44.1" class="ltx_td ltx_align_right">CCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>
</td>
<td id="S5.T2.1.1.44.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.44.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.44.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.44.5" class="ltx_td"></td>
<td id="S5.T2.1.1.44.6" class="ltx_td"></td>
<td id="S5.T2.1.1.44.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.44.8" class="ltx_td ltx_align_center">57.99</td>
<td id="S5.T2.1.1.44.9" class="ltx_td ltx_align_center">60.73</td>
<td id="S5.T2.1.1.44.10" class="ltx_td ltx_align_center">59.32</td>
</tr>
<tr id="S5.T2.1.1.45" class="ltx_tr">
<td id="S5.T2.1.1.45.1" class="ltx_td ltx_align_right">LP-Focal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>
</td>
<td id="S5.T2.1.1.45.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.45.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.45.4" class="ltx_td"></td>
<td id="S5.T2.1.1.45.5" class="ltx_td"></td>
<td id="S5.T2.1.1.45.6" class="ltx_td"></td>
<td id="S5.T2.1.1.45.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.45.8" class="ltx_td ltx_align_center">58.45</td>
<td id="S5.T2.1.1.45.9" class="ltx_td ltx_align_center">62.45</td>
<td id="S5.T2.1.1.45.10" class="ltx_td ltx_align_center">60.38</td>
</tr>
<tr id="S5.T2.1.1.46" class="ltx_tr">
<td id="S5.T2.1.1.46.1" class="ltx_td ltx_align_right">SSL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>
</td>
<td id="S5.T2.1.1.46.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.46.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.46.4" class="ltx_td"></td>
<td id="S5.T2.1.1.46.5" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.46.6" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.46.7" class="ltx_td"></td>
<td id="S5.T2.1.1.46.8" class="ltx_td ltx_align_center">57.59</td>
<td id="S5.T2.1.1.46.9" class="ltx_td ltx_align_center">63.73</td>
<td id="S5.T2.1.1.46.10" class="ltx_td ltx_align_center">60.50</td>
</tr>
<tr id="S5.T2.1.1.47" class="ltx_tr">
<td id="S5.T2.1.1.47.1" class="ltx_td ltx_align_right">IntroD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S5.T2.1.1.47.2" class="ltx_td ltx_align_center">CSS</td>
<td id="S5.T2.1.1.47.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.47.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.47.5" class="ltx_td"></td>
<td id="S5.T2.1.1.47.6" class="ltx_td"></td>
<td id="S5.T2.1.1.47.7" class="ltx_td"></td>
<td id="S5.T2.1.1.47.8" class="ltx_td ltx_align_center">60.17</td>
<td id="S5.T2.1.1.47.9" class="ltx_td ltx_align_center">62.57</td>
<td id="S5.T2.1.1.47.10" class="ltx_td ltx_align_center">61.35</td>
</tr>
<tr id="S5.T2.1.1.48" class="ltx_tr">
<td id="S5.T2.1.1.48.1" class="ltx_td ltx_align_right">D-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>
</td>
<td id="S5.T2.1.1.48.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.48.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.48.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.48.5" class="ltx_td"></td>
<td id="S5.T2.1.1.48.6" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.48.7" class="ltx_td"></td>
<td id="S5.T2.1.1.48.8" class="ltx_td ltx_align_center">61.91</td>
<td id="S5.T2.1.1.48.9" class="ltx_td ltx_align_center">64.96</td>
<td id="S5.T2.1.1.48.10" class="ltx_td ltx_align_center">63.40</td>
</tr>
<tr id="S5.T2.1.1.49" class="ltx_tr">
<td id="S5.T2.1.1.49.1" class="ltx_td ltx_align_right">SAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>
</td>
<td id="S5.T2.1.1.49.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.49.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.49.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.49.5" class="ltx_td"></td>
<td id="S5.T2.1.1.49.6" class="ltx_td"></td>
<td id="S5.T2.1.1.49.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.49.8" class="ltx_td ltx_align_center">66.73</td>
<td id="S5.T2.1.1.49.9" class="ltx_td ltx_align_center">69.22</td>
<td id="S5.T2.1.1.49.10" class="ltx_td ltx_align_center">67.95</td>
</tr>
<tr id="S5.T2.1.1.50" class="ltx_tr">
<td id="S5.T2.1.1.50.1" class="ltx_td ltx_align_right">SimpleAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
</td>
<td id="S5.T2.1.1.50.2" class="ltx_td ltx_align_center">LXMERT</td>
<td id="S5.T2.1.1.50.3" class="ltx_td ltx_align_center">2021</td>
<td id="S5.T2.1.1.50.4" class="ltx_td"></td>
<td id="S5.T2.1.1.50.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.50.6" class="ltx_td"></td>
<td id="S5.T2.1.1.50.7" class="ltx_td"></td>
<td id="S5.T2.1.1.50.8" class="ltx_td ltx_align_center">62.24</td>
<td id="S5.T2.1.1.50.9" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.50.9.1" class="ltx_text ltx_font_bold">74.98</span></td>
<td id="S5.T2.1.1.50.10" class="ltx_td ltx_align_center">68.02</td>
</tr>
<tr id="S5.T2.1.1.51" class="ltx_tr">
<td id="S5.T2.1.1.51.1" class="ltx_td ltx_align_right ltx_border_t">SwapMix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S5.T2.1.1.51.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T2.1.1.51.3" class="ltx_td ltx_align_center ltx_border_t">2022</td>
<td id="S5.T2.1.1.51.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.51.5" class="ltx_td ltx_align_center ltx_border_t">â—</td>
<td id="S5.T2.1.1.51.6" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.51.7" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.1.1.51.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T2.1.1.51.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T2.1.1.51.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T2.1.1.52" class="ltx_tr">
<td id="S5.T2.1.1.52.1" class="ltx_td ltx_align_right">Loss-Rescaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S5.T2.1.1.52.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.52.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.52.4" class="ltx_td"></td>
<td id="S5.T2.1.1.52.5" class="ltx_td"></td>
<td id="S5.T2.1.1.52.6" class="ltx_td"></td>
<td id="S5.T2.1.1.52.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.52.8" class="ltx_td ltx_align_center">47.09</td>
<td id="S5.T2.1.1.52.9" class="ltx_td ltx_align_center">55.50</td>
<td id="S5.T2.1.1.52.10" class="ltx_td ltx_align_center">50.95</td>
</tr>
<tr id="S5.T2.1.1.53" class="ltx_tr">
<td id="S5.T2.1.1.53.1" class="ltx_td ltx_align_right">Loss-Rescaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S5.T2.1.1.53.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.53.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.53.4" class="ltx_td"></td>
<td id="S5.T2.1.1.53.5" class="ltx_td"></td>
<td id="S5.T2.1.1.53.6" class="ltx_td"></td>
<td id="S5.T2.1.1.53.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.53.8" class="ltx_td ltx_align_center">53.26</td>
<td id="S5.T2.1.1.53.9" class="ltx_td ltx_align_center">56.81</td>
<td id="S5.T2.1.1.53.10" class="ltx_td ltx_align_center">54.98</td>
</tr>
<tr id="S5.T2.1.1.54" class="ltx_tr">
<td id="S5.T2.1.1.54.1" class="ltx_td ltx_align_right">Loss-Rescaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S5.T2.1.1.54.2" class="ltx_td ltx_align_center">CSS</td>
<td id="S5.T2.1.1.54.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.54.4" class="ltx_td"></td>
<td id="S5.T2.1.1.54.5" class="ltx_td"></td>
<td id="S5.T2.1.1.54.6" class="ltx_td"></td>
<td id="S5.T2.1.1.54.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.54.8" class="ltx_td ltx_align_center">50.73</td>
<td id="S5.T2.1.1.54.9" class="ltx_td ltx_align_center">61.14</td>
<td id="S5.T2.1.1.54.10" class="ltx_td ltx_align_center">55.45</td>
</tr>
<tr id="S5.T2.1.1.55" class="ltx_tr">
<td id="S5.T2.1.1.55.1" class="ltx_td ltx_align_right">Loss-Rescaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S5.T2.1.1.55.2" class="ltx_td ltx_align_center">LM</td>
<td id="S5.T2.1.1.55.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.55.4" class="ltx_td"></td>
<td id="S5.T2.1.1.55.5" class="ltx_td"></td>
<td id="S5.T2.1.1.55.6" class="ltx_td"></td>
<td id="S5.T2.1.1.55.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.55.8" class="ltx_td ltx_align_center">53.17</td>
<td id="S5.T2.1.1.55.9" class="ltx_td ltx_align_center">59.45</td>
<td id="S5.T2.1.1.55.10" class="ltx_td ltx_align_center">56.13</td>
</tr>
<tr id="S5.T2.1.1.56" class="ltx_tr">
<td id="S5.T2.1.1.56.1" class="ltx_td ltx_align_right">Loss-Rescaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S5.T2.1.1.56.2" class="ltx_td ltx_align_center">CSS+LMH</td>
<td id="S5.T2.1.1.56.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.56.4" class="ltx_td"></td>
<td id="S5.T2.1.1.56.5" class="ltx_td"></td>
<td id="S5.T2.1.1.56.6" class="ltx_td"></td>
<td id="S5.T2.1.1.56.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.56.8" class="ltx_td ltx_align_center">56.55</td>
<td id="S5.T2.1.1.56.9" class="ltx_td ltx_align_center">55.96</td>
<td id="S5.T2.1.1.56.10" class="ltx_td ltx_align_center">56.25</td>
</tr>
<tr id="S5.T2.1.1.57" class="ltx_tr">
<td id="S5.T2.1.1.57.1" class="ltx_td ltx_align_right">ECD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>
</td>
<td id="S5.T2.1.1.57.2" class="ltx_td ltx_align_center">UpDn+LMH</td>
<td id="S5.T2.1.1.57.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.57.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.57.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.57.6" class="ltx_td"></td>
<td id="S5.T2.1.1.57.7" class="ltx_td"></td>
<td id="S5.T2.1.1.57.8" class="ltx_td ltx_align_center">59.92</td>
<td id="S5.T2.1.1.57.9" class="ltx_td ltx_align_center">57.38</td>
<td id="S5.T2.1.1.57.10" class="ltx_td ltx_align_center">58.62</td>
</tr>
<tr id="S5.T2.1.1.58" class="ltx_tr">
<td id="S5.T2.1.1.58.1" class="ltx_td ltx_align_right">MMBS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite>
</td>
<td id="S5.T2.1.1.58.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.58.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.58.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.58.5" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.58.6" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.58.7" class="ltx_td"></td>
<td id="S5.T2.1.1.58.8" class="ltx_td ltx_align_center">56.44</td>
<td id="S5.T2.1.1.58.9" class="ltx_td ltx_align_center">61.87</td>
<td id="S5.T2.1.1.58.10" class="ltx_td ltx_align_center">59.03</td>
</tr>
<tr id="S5.T2.1.1.59" class="ltx_tr">
<td id="S5.T2.1.1.59.1" class="ltx_td ltx_align_right">KDDAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>
</td>
<td id="S5.T2.1.1.59.2" class="ltx_td ltx_align_center">RUBi</td>
<td id="S5.T2.1.1.59.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.59.4" class="ltx_td"></td>
<td id="S5.T2.1.1.59.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.59.6" class="ltx_td"></td>
<td id="S5.T2.1.1.59.7" class="ltx_td"></td>
<td id="S5.T2.1.1.59.8" class="ltx_td ltx_align_center">59.25</td>
<td id="S5.T2.1.1.59.9" class="ltx_td ltx_align_center">60.25</td>
<td id="S5.T2.1.1.59.10" class="ltx_td ltx_align_center">59.75</td>
</tr>
<tr id="S5.T2.1.1.60" class="ltx_tr">
<td id="S5.T2.1.1.60.1" class="ltx_td ltx_align_right">KDDAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>
</td>
<td id="S5.T2.1.1.60.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.60.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.60.4" class="ltx_td"></td>
<td id="S5.T2.1.1.60.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.60.6" class="ltx_td"></td>
<td id="S5.T2.1.1.60.7" class="ltx_td"></td>
<td id="S5.T2.1.1.60.8" class="ltx_td ltx_align_center">59.54</td>
<td id="S5.T2.1.1.60.9" class="ltx_td ltx_align_center">62.09</td>
<td id="S5.T2.1.1.60.10" class="ltx_td ltx_align_center">60.79</td>
</tr>
<tr id="S5.T2.1.1.61" class="ltx_tr">
<td id="S5.T2.1.1.61.1" class="ltx_td ltx_align_right">VQA-BC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>
</td>
<td id="S5.T2.1.1.61.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.61.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.61.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.61.5" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.61.6" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.61.7" class="ltx_td"></td>
<td id="S5.T2.1.1.61.8" class="ltx_td ltx_align_center">60.81</td>
<td id="S5.T2.1.1.61.9" class="ltx_td ltx_align_center">61.74</td>
<td id="S5.T2.1.1.61.10" class="ltx_td ltx_align_center">61.27</td>
</tr>
<tr id="S5.T2.1.1.62" class="ltx_tr">
<td id="S5.T2.1.1.62.1" class="ltx_td ltx_align_right">AttReg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>
</td>
<td id="S5.T2.1.1.62.2" class="ltx_td ltx_align_center">LMH</td>
<td id="S5.T2.1.1.62.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.62.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.62.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.62.6" class="ltx_td"></td>
<td id="S5.T2.1.1.62.7" class="ltx_td"></td>
<td id="S5.T2.1.1.62.8" class="ltx_td ltx_align_center">60.00</td>
<td id="S5.T2.1.1.62.9" class="ltx_td ltx_align_center">62.74</td>
<td id="S5.T2.1.1.62.10" class="ltx_td ltx_align_center">61.34</td>
</tr>
<tr id="S5.T2.1.1.63" class="ltx_tr">
<td id="S5.T2.1.1.63.1" class="ltx_td ltx_align_right">KDDAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>
</td>
<td id="S5.T2.1.1.63.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.63.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.63.4" class="ltx_td"></td>
<td id="S5.T2.1.1.63.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.63.6" class="ltx_td"></td>
<td id="S5.T2.1.1.63.7" class="ltx_td"></td>
<td id="S5.T2.1.1.63.8" class="ltx_td ltx_align_center">60.24</td>
<td id="S5.T2.1.1.63.9" class="ltx_td ltx_align_center">62.86</td>
<td id="S5.T2.1.1.63.10" class="ltx_td ltx_align_center">61.52</td>
</tr>
<tr id="S5.T2.1.1.64" class="ltx_tr">
<td id="S5.T2.1.1.64.1" class="ltx_td ltx_align_right">KDDAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>
</td>
<td id="S5.T2.1.1.64.2" class="ltx_td ltx_align_center">CSS</td>
<td id="S5.T2.1.1.64.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.64.4" class="ltx_td"></td>
<td id="S5.T2.1.1.64.5" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.64.6" class="ltx_td"></td>
<td id="S5.T2.1.1.64.7" class="ltx_td"></td>
<td id="S5.T2.1.1.64.8" class="ltx_td ltx_align_center">61.14</td>
<td id="S5.T2.1.1.64.9" class="ltx_td ltx_align_center">62.17</td>
<td id="S5.T2.1.1.64.10" class="ltx_td ltx_align_center">61.65</td>
</tr>
<tr id="S5.T2.1.1.65" class="ltx_tr">
<td id="S5.T2.1.1.65.1" class="ltx_td ltx_align_right">Loss-Rescaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
</td>
<td id="S5.T2.1.1.65.2" class="ltx_td ltx_align_center">LXMERT</td>
<td id="S5.T2.1.1.65.3" class="ltx_td ltx_align_center">2022</td>
<td id="S5.T2.1.1.65.4" class="ltx_td"></td>
<td id="S5.T2.1.1.65.5" class="ltx_td"></td>
<td id="S5.T2.1.1.65.6" class="ltx_td"></td>
<td id="S5.T2.1.1.65.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.65.8" class="ltx_td ltx_align_center">66.40</td>
<td id="S5.T2.1.1.65.9" class="ltx_td ltx_align_center">69.76</td>
<td id="S5.T2.1.1.65.10" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.65.10.1" class="ltx_text ltx_font_bold">68.04</span></td>
</tr>
<tr id="S5.T2.1.1.66" class="ltx_tr">
<td id="S5.T2.1.1.66.1" class="ltx_td ltx_align_right">RMLVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>
</td>
<td id="S5.T2.1.1.66.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.66.3" class="ltx_td ltx_align_center">2023</td>
<td id="S5.T2.1.1.66.4" class="ltx_td ltx_align_center">â—‹</td>
<td id="S5.T2.1.1.66.5" class="ltx_td"></td>
<td id="S5.T2.1.1.66.6" class="ltx_td"></td>
<td id="S5.T2.1.1.66.7" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.66.8" class="ltx_td ltx_align_center">60.41</td>
<td id="S5.T2.1.1.66.9" class="ltx_td ltx_align_center">59.99</td>
<td id="S5.T2.1.1.66.10" class="ltx_td ltx_align_center">60.20</td>
</tr>
<tr id="S5.T2.1.1.67" class="ltx_tr">
<td id="S5.T2.1.1.67.1" class="ltx_td ltx_align_right">GGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>
</td>
<td id="S5.T2.1.1.67.2" class="ltx_td ltx_align_center">UpDn</td>
<td id="S5.T2.1.1.67.3" class="ltx_td ltx_align_center">2023</td>
<td id="S5.T2.1.1.67.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.67.5" class="ltx_td"></td>
<td id="S5.T2.1.1.67.6" class="ltx_td"></td>
<td id="S5.T2.1.1.67.7" class="ltx_td"></td>
<td id="S5.T2.1.1.67.8" class="ltx_td ltx_align_center">59.37</td>
<td id="S5.T2.1.1.67.9" class="ltx_td ltx_align_center">62.15</td>
<td id="S5.T2.1.1.67.10" class="ltx_td ltx_align_center">60.73</td>
</tr>
<tr id="S5.T2.1.1.68" class="ltx_tr">
<td id="S5.T2.1.1.68.1" class="ltx_td ltx_align_right">GenB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite>
</td>
<td id="S5.T2.1.1.68.2" class="ltx_td ltx_align_center">LXMERT</td>
<td id="S5.T2.1.1.68.3" class="ltx_td ltx_align_center">2023</td>
<td id="S5.T2.1.1.68.4" class="ltx_td ltx_align_center">â—</td>
<td id="S5.T2.1.1.68.5" class="ltx_td"></td>
<td id="S5.T2.1.1.68.6" class="ltx_td"></td>
<td id="S5.T2.1.1.68.7" class="ltx_td"></td>
<td id="S5.T2.1.1.68.8" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.68.8.1" class="ltx_text ltx_font_bold">71.16</span></td>
<td id="S5.T2.1.1.68.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.1.68.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T2.1.1.69" class="ltx_tr">
<td id="S5.T2.1.1.69.1" class="ltx_td ltx_align_right ltx_border_bb">LSP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>
</td>
<td id="S5.T2.1.1.69.2" class="ltx_td ltx_align_center ltx_border_bb">UpDn</td>
<td id="S5.T2.1.1.69.3" class="ltx_td ltx_align_center ltx_border_bb">2023</td>
<td id="S5.T2.1.1.69.4" class="ltx_td ltx_align_center ltx_border_bb">â—‹</td>
<td id="S5.T2.1.1.69.5" class="ltx_td ltx_align_center ltx_border_bb">â—‹</td>
<td id="S5.T2.1.1.69.6" class="ltx_td ltx_align_center ltx_border_bb">â—</td>
<td id="S5.T2.1.1.69.7" class="ltx_td ltx_border_bb"></td>
<td id="S5.T2.1.1.69.8" class="ltx_td ltx_align_center ltx_border_bb">61.95</td>
<td id="S5.T2.1.1.69.9" class="ltx_td ltx_align_center ltx_border_bb">65.26</td>
<td id="S5.T2.1.1.69.10" class="ltx_td ltx_align_center ltx_border_bb">63.56</td>
</tr>
</table>
</span></div>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_text ltx_font_italic">Ensemble Learning</span>
</h3>

<figure id="S5.F11" class="ltx_figure"><img src="/html/2307.11471/assets/x11.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="437" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Illustration of ensemble learning-based debiasing methods. The bias branch takes single modalities as inputs to capture bias learning in the training stage. In the test stage, the branch is removed, and answers are provided by the vanilla VQA model. <math id="S5.F11.2.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.F11.2.m1.1b"><mo id="S5.F11.2.m1.1.1" xref="S5.F11.2.m1.1.1.cmml">âŠ•</mo><annotation-xml encoding="MathML-Content" id="S5.F11.2.m1.1c"><csymbol cd="latexml" id="S5.F11.2.m1.1.1.cmml" xref="S5.F11.2.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.F11.2.m1.1d">\oplus</annotation></semantics></math> denotes the combination operation such as a sigmoid function.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.6" class="ltx_p">Ensemble learning-based methods are the first to investigate the robustness of VQA. They typically apply the combination <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi mathvariant="normal" id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">Î¦</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">Î¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\Phi</annotation></semantics></math> of a bias branch <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="E_{\mathrm{b}}" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><msub id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">ğ¸</ci><ci id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">b</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">E_{\mathrm{b}}</annotation></semantics></math> and a vanilla VQA method <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mi id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><ci id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">f</annotation></semantics></math> defined in Equation (<a href="#S2.E2" title="In 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) to predict answers <math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="\hat{a}_{i}" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><msub id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mover accent="true" id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml"><mi id="S5.SS1.p1.4.m4.1.1.2.2" xref="S5.SS1.p1.4.m4.1.1.2.2.cmml">a</mi><mo id="S5.SS1.p1.4.m4.1.1.2.1" xref="S5.SS1.p1.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="S5.SS1.p1.4.m4.1.1.3" xref="S5.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">subscript</csymbol><apply id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2"><ci id="S5.SS1.p1.4.m4.1.1.2.1.cmml" xref="S5.SS1.p1.4.m4.1.1.2.1">^</ci><ci id="S5.SS1.p1.4.m4.1.1.2.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2.2">ğ‘</ci></apply><ci id="S5.SS1.p1.4.m4.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">\hat{a}_{i}</annotation></semantics></math> comprehensively. <math id="S5.SS1.p1.5.m5.1" class="ltx_Math" alttext="E_{\mathrm{b}}" display="inline"><semantics id="S5.SS1.p1.5.m5.1a"><msub id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml"><mi id="S5.SS1.p1.5.m5.1.1.2" xref="S5.SS1.p1.5.m5.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S5.SS1.p1.5.m5.1.1.3" xref="S5.SS1.p1.5.m5.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><apply id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S5.SS1.p1.5.m5.1.1.2.cmml" xref="S5.SS1.p1.5.m5.1.1.2">ğ¸</ci><ci id="S5.SS1.p1.5.m5.1.1.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3">b</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">E_{\mathrm{b}}</annotation></semantics></math> is used to capture the bias learning, while <math id="S5.SS1.p1.6.m6.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S5.SS1.p1.6.m6.1a"><mi id="S5.SS1.p1.6.m6.1.1" xref="S5.SS1.p1.6.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.1b"><ci id="S5.SS1.p1.6.m6.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.1c">f</annotation></semantics></math> is applied to perform normal question answering. This process can be denoted as follows:</p>
<table id="S5.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E8.m1.1" class="ltx_Math" alttext="\hat{a}_{i}=\Phi\left(E_{\mathrm{b}}\left(E_{\mathrm{v}}(v_{i}),E_{\mathrm{q}}(q_{i})\right),f(v_{i},q_{i})\right)." display="block"><semantics id="S5.E8.m1.1a"><mrow id="S5.E8.m1.1.1.1" xref="S5.E8.m1.1.1.1.1.cmml"><mrow id="S5.E8.m1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.cmml"><msub id="S5.E8.m1.1.1.1.1.4" xref="S5.E8.m1.1.1.1.1.4.cmml"><mover accent="true" id="S5.E8.m1.1.1.1.1.4.2" xref="S5.E8.m1.1.1.1.1.4.2.cmml"><mi id="S5.E8.m1.1.1.1.1.4.2.2" xref="S5.E8.m1.1.1.1.1.4.2.2.cmml">a</mi><mo id="S5.E8.m1.1.1.1.1.4.2.1" xref="S5.E8.m1.1.1.1.1.4.2.1.cmml">^</mo></mover><mi id="S5.E8.m1.1.1.1.1.4.3" xref="S5.E8.m1.1.1.1.1.4.3.cmml">i</mi></msub><mo id="S5.E8.m1.1.1.1.1.3" xref="S5.E8.m1.1.1.1.1.3.cmml">=</mo><mrow id="S5.E8.m1.1.1.1.1.2" xref="S5.E8.m1.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S5.E8.m1.1.1.1.1.2.4" xref="S5.E8.m1.1.1.1.1.2.4.cmml">Î¦</mi><mo lspace="0em" rspace="0em" id="S5.E8.m1.1.1.1.1.2.3" xref="S5.E8.m1.1.1.1.1.2.3.cmml">â€‹</mo><mrow id="S5.E8.m1.1.1.1.1.2.2.2" xref="S5.E8.m1.1.1.1.1.2.2.3.cmml"><mo id="S5.E8.m1.1.1.1.1.2.2.2.3" xref="S5.E8.m1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="S5.E8.m1.1.1.1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S5.E8.m1.1.1.1.1.1.1.1.1.4" xref="S5.E8.m1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S5.E8.m1.1.1.1.1.1.1.1.1.4.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.4.2.cmml">E</mi><mi mathvariant="normal" id="S5.E8.m1.1.1.1.1.1.1.1.1.4.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.4.3.cmml">b</mi></msub><mo lspace="0em" rspace="0em" id="S5.E8.m1.1.1.1.1.1.1.1.1.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.3.cmml">â€‹</mo><mrow id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mo id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><mrow id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">E</mi><mi mathvariant="normal" id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.4" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.cmml"><msub id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">E</mi><mi mathvariant="normal" id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">â€‹</mo><mrow id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml">(</mo><msub id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.2" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.2.cmml">q</mi><mi id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.3" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.5" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S5.E8.m1.1.1.1.1.2.2.2.4" xref="S5.E8.m1.1.1.1.1.2.2.3.cmml">,</mo><mrow id="S5.E8.m1.1.1.1.1.2.2.2.2" xref="S5.E8.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S5.E8.m1.1.1.1.1.2.2.2.2.4" xref="S5.E8.m1.1.1.1.1.2.2.2.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S5.E8.m1.1.1.1.1.2.2.2.2.3" xref="S5.E8.m1.1.1.1.1.2.2.2.2.3.cmml">â€‹</mo><mrow id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.3.cmml">(</mo><msub id="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1" xref="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.cmml"><mi id="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.2" xref="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.2.cmml">v</mi><mi id="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.3" xref="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.4" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.3.cmml">,</mo><msub id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.cmml"><mi id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.2" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.2.cmml">q</mi><mi id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.3" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.5" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S5.E8.m1.1.1.1.1.2.2.2.5" xref="S5.E8.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S5.E8.m1.1.1.1.2" xref="S5.E8.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E8.m1.1b"><apply id="S5.E8.m1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1"><eq id="S5.E8.m1.1.1.1.1.3.cmml" xref="S5.E8.m1.1.1.1.1.3"></eq><apply id="S5.E8.m1.1.1.1.1.4.cmml" xref="S5.E8.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.4.1.cmml" xref="S5.E8.m1.1.1.1.1.4">subscript</csymbol><apply id="S5.E8.m1.1.1.1.1.4.2.cmml" xref="S5.E8.m1.1.1.1.1.4.2"><ci id="S5.E8.m1.1.1.1.1.4.2.1.cmml" xref="S5.E8.m1.1.1.1.1.4.2.1">^</ci><ci id="S5.E8.m1.1.1.1.1.4.2.2.cmml" xref="S5.E8.m1.1.1.1.1.4.2.2">ğ‘</ci></apply><ci id="S5.E8.m1.1.1.1.1.4.3.cmml" xref="S5.E8.m1.1.1.1.1.4.3">ğ‘–</ci></apply><apply id="S5.E8.m1.1.1.1.1.2.cmml" xref="S5.E8.m1.1.1.1.1.2"><times id="S5.E8.m1.1.1.1.1.2.3.cmml" xref="S5.E8.m1.1.1.1.1.2.3"></times><ci id="S5.E8.m1.1.1.1.1.2.4.cmml" xref="S5.E8.m1.1.1.1.1.2.4">Î¦</ci><interval closure="open" id="S5.E8.m1.1.1.1.1.2.2.3.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2"><apply id="S5.E8.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1"><times id="S5.E8.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.3"></times><apply id="S5.E8.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.4.2">ğ¸</ci><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.4.3">b</ci></apply><interval closure="open" id="S5.E8.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2"><apply id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1"><times id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¸</ci><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">v</ci></apply><apply id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2"><times id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.2"></times><apply id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.2">ğ¸</ci><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.3.3">q</ci></apply><apply id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1">subscript</csymbol><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.2">ğ‘</ci><ci id="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.3.cmml" xref="S5.E8.m1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.3">ğ‘–</ci></apply></apply></interval></apply><apply id="S5.E8.m1.1.1.1.1.2.2.2.2.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2"><times id="S5.E8.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.3"></times><ci id="S5.E8.m1.1.1.1.1.2.2.2.2.4.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.4">ğ‘“</ci><interval closure="open" id="S5.E8.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.2"><apply id="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.2.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.2">ğ‘£</ci><ci id="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.3.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.1.1.1.3">ğ‘–</ci></apply><apply id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.1.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.2.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.2">ğ‘</ci><ci id="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml" xref="S5.E8.m1.1.1.1.1.2.2.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E8.m1.1c">\hat{a}_{i}=\Phi\left(E_{\mathrm{b}}\left(E_{\mathrm{v}}(v_{i}),E_{\mathrm{q}}(q_{i})\right),f(v_{i},q_{i})\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The illustration of the ensemble learning-based method is shown in Fig. <a href="#S5.F11" title="Figure 11 â€£ 5.1 Ensemble Learning â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. <em id="S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">In the training stage</em>, a single modality, such as questions alone, is fed into the bias branch, irrespective of images. <span id="S5.SS1.p2.1.2" class="ltx_text" style="color:#000000;">In this way, the accurate prediction of the answer can be achieved to a great extent without relying on the visual information provided by the image modality.</span> In other words, the bias or statistical correlations in the training data are captured. Then, the bias branch and the vanilla method are trained in an ensemble manner. For example, their predictions are combined by a sigmoid function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Since the bias has been captured by the branch module, the vanilla method has no incentive to learn those superficial correlations. <em id="S5.SS1.p2.1.3" class="ltx_emph ltx_font_italic">In the test stage</em>, the vanilla method is used alone to provide unbiased predictions.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text" style="color:#000000;">Specifically, RUBi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, LMH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>, and AdvReg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> leverage a question-only branch with a specific mechanism, such as a sigmoid function, a learned mixin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> strategy, and an adversarial regularization, respectively, to prevent the vanilla method from looking into only one modality. It is noteworthy that this branch is usually implemented by a Recurrent Neural Network (RNN) followed by a multi-layer perceptron. GGE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> decomposes the language bias into two categories, namely distribution bias and shortcut bias. To mitigate them, it employs a greedy gradient ensemble training strategy. The architecture of GGE is also a combination of the question-only branch and the vanilla VQA model <math id="S5.SS1.p3.1.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S5.SS1.p3.1.1.m1.1a"><mi mathcolor="#000000" id="S5.SS1.p3.1.1.m1.1.1" xref="S5.SS1.p3.1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.1.m1.1b"><ci id="S5.SS1.p3.1.1.m1.1.1.cmml" xref="S5.SS1.p3.1.1.m1.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.1.m1.1c">f</annotation></semantics></math>.</span> Inspired by the causal effect, CF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> revisits ensemble-based debiasing methods from a causal inference perspective, formulating the language bias as the direct causal effect of questions on answers, <em id="S5.SS1.p3.1.2" class="ltx_emph ltx_font_italic">i.e.</em>, the pure language effect. It mitigates the bias learning by subtracting the pure language effect from the total causal effect.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">However, the above methods still suffer from the issue of achieving high OOD performance at the expense of ID performance. To address this problem, IntroD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> introduces a training paradigm that first applies CF as a causal teacher to capture the bias in ID and OOD scenarios, then blends the inductive bias of both worlds fairly, and finally performs distillation for a robust student model. <span id="S5.SS1.p4.1.1" class="ltx_text" style="color:#000000;"> Inspired by the above works, CIKD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> jointly investigates the causal effect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite>, knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>, <a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite>, and ensemble learning to address the influence of bias. However, these methods can hardly benefit from language bias. Curriculum learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> enables models to be trained initially on the easier examples and gradually shift towards the harder ones. Motivated by this, LBCL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> leverages a visually sensitive coefficient metric to measure the difficulty of each sample and applies an easy-to-hard training strategy to solve the bias learning. D-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> aims to mitigate the impact of negative bias in language and vision modalities by considering both feature-level and example-level perspectives. From the feature-level view, it leverages a question-only and image-only branch to capture the uni-modal bias respectively. From the example-level view, it builds negative examples to assist the model training.
</span></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_italic">Data Augmentation</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.6" class="ltx_p">Data augmentation-based methods typically generate additional augmented question-answer pairs <math id="S5.SS2.p1.1.m1.3" class="ltx_Math" alttext="(v_{i}^{{}^{\prime}},q_{i}^{{}^{\prime}},a_{i}^{{}^{\prime}})" display="inline"><semantics id="S5.SS2.p1.1.m1.3a"><mrow id="S5.SS2.p1.1.m1.3.3.3" xref="S5.SS2.p1.1.m1.3.3.4.cmml"><mo stretchy="false" id="S5.SS2.p1.1.m1.3.3.3.4" xref="S5.SS2.p1.1.m1.3.3.4.cmml">(</mo><msubsup id="S5.SS2.p1.1.m1.1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.1.1.2.2" xref="S5.SS2.p1.1.m1.1.1.1.1.2.2.cmml">v</mi><mi id="S5.SS2.p1.1.m1.1.1.1.1.2.3" xref="S5.SS2.p1.1.m1.1.1.1.1.2.3.cmml">i</mi><msup id="S5.SS2.p1.1.m1.1.1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.1.1.3.cmml"><mi id="S5.SS2.p1.1.m1.1.1.1.1.3a" xref="S5.SS2.p1.1.m1.1.1.1.1.3.cmml"></mi><mo id="S5.SS2.p1.1.m1.1.1.1.1.3.1" xref="S5.SS2.p1.1.m1.1.1.1.1.3.1.cmml">â€²</mo></msup></msubsup><mo id="S5.SS2.p1.1.m1.3.3.3.5" xref="S5.SS2.p1.1.m1.3.3.4.cmml">,</mo><msubsup id="S5.SS2.p1.1.m1.2.2.2.2" xref="S5.SS2.p1.1.m1.2.2.2.2.cmml"><mi id="S5.SS2.p1.1.m1.2.2.2.2.2.2" xref="S5.SS2.p1.1.m1.2.2.2.2.2.2.cmml">q</mi><mi id="S5.SS2.p1.1.m1.2.2.2.2.2.3" xref="S5.SS2.p1.1.m1.2.2.2.2.2.3.cmml">i</mi><msup id="S5.SS2.p1.1.m1.2.2.2.2.3" xref="S5.SS2.p1.1.m1.2.2.2.2.3.cmml"><mi id="S5.SS2.p1.1.m1.2.2.2.2.3a" xref="S5.SS2.p1.1.m1.2.2.2.2.3.cmml"></mi><mo id="S5.SS2.p1.1.m1.2.2.2.2.3.1" xref="S5.SS2.p1.1.m1.2.2.2.2.3.1.cmml">â€²</mo></msup></msubsup><mo id="S5.SS2.p1.1.m1.3.3.3.6" xref="S5.SS2.p1.1.m1.3.3.4.cmml">,</mo><msubsup id="S5.SS2.p1.1.m1.3.3.3.3" xref="S5.SS2.p1.1.m1.3.3.3.3.cmml"><mi id="S5.SS2.p1.1.m1.3.3.3.3.2.2" xref="S5.SS2.p1.1.m1.3.3.3.3.2.2.cmml">a</mi><mi id="S5.SS2.p1.1.m1.3.3.3.3.2.3" xref="S5.SS2.p1.1.m1.3.3.3.3.2.3.cmml">i</mi><msup id="S5.SS2.p1.1.m1.3.3.3.3.3" xref="S5.SS2.p1.1.m1.3.3.3.3.3.cmml"><mi id="S5.SS2.p1.1.m1.3.3.3.3.3a" xref="S5.SS2.p1.1.m1.3.3.3.3.3.cmml"></mi><mo id="S5.SS2.p1.1.m1.3.3.3.3.3.1" xref="S5.SS2.p1.1.m1.3.3.3.3.3.1.cmml">â€²</mo></msup></msubsup><mo stretchy="false" id="S5.SS2.p1.1.m1.3.3.3.7" xref="S5.SS2.p1.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.3b"><vector id="S5.SS2.p1.1.m1.3.3.4.cmml" xref="S5.SS2.p1.1.m1.3.3.3"><apply id="S5.SS2.p1.1.m1.1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1">superscript</csymbol><apply id="S5.SS2.p1.1.m1.1.1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.1.2.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.1.1.2.2.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1.2.2">ğ‘£</ci><ci id="S5.SS2.p1.1.m1.1.1.1.1.2.3.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S5.SS2.p1.1.m1.1.1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1.3"><ci id="S5.SS2.p1.1.m1.1.1.1.1.3.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1.3.1">â€²</ci></apply></apply><apply id="S5.SS2.p1.1.m1.2.2.2.2.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.2.2.2.2.1.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2">superscript</csymbol><apply id="S5.SS2.p1.1.m1.2.2.2.2.2.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.2.2.2.2.2.1.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S5.SS2.p1.1.m1.2.2.2.2.2.2.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2.2.2">ğ‘</ci><ci id="S5.SS2.p1.1.m1.2.2.2.2.2.3.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2.2.3">ğ‘–</ci></apply><apply id="S5.SS2.p1.1.m1.2.2.2.2.3.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2.3"><ci id="S5.SS2.p1.1.m1.2.2.2.2.3.1.cmml" xref="S5.SS2.p1.1.m1.2.2.2.2.3.1">â€²</ci></apply></apply><apply id="S5.SS2.p1.1.m1.3.3.3.3.cmml" xref="S5.SS2.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.3.3.3.3.1.cmml" xref="S5.SS2.p1.1.m1.3.3.3.3">superscript</csymbol><apply id="S5.SS2.p1.1.m1.3.3.3.3.2.cmml" xref="S5.SS2.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.3.3.3.3.2.1.cmml" xref="S5.SS2.p1.1.m1.3.3.3.3">subscript</csymbol><ci id="S5.SS2.p1.1.m1.3.3.3.3.2.2.cmml" xref="S5.SS2.p1.1.m1.3.3.3.3.2.2">ğ‘</ci><ci id="S5.SS2.p1.1.m1.3.3.3.3.2.3.cmml" xref="S5.SS2.p1.1.m1.3.3.3.3.2.3">ğ‘–</ci></apply><apply id="S5.SS2.p1.1.m1.3.3.3.3.3.cmml" xref="S5.SS2.p1.1.m1.3.3.3.3.3"><ci id="S5.SS2.p1.1.m1.3.3.3.3.3.1.cmml" xref="S5.SS2.p1.1.m1.3.3.3.3.3.1">â€²</ci></apply></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.3c">(v_{i}^{{}^{\prime}},q_{i}^{{}^{\prime}},a_{i}^{{}^{\prime}})</annotation></semantics></math> for each sample in the original dataset <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\mathcal{D}</annotation></semantics></math> to balance the distribution of training data or mitigate the data bias. In particular, the question <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="q_{i}^{{}^{\prime}}" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><msubsup id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1.2.2" xref="S5.SS2.p1.3.m3.1.1.2.2.cmml">q</mi><mi id="S5.SS2.p1.3.m3.1.1.2.3" xref="S5.SS2.p1.3.m3.1.1.2.3.cmml">i</mi><msup id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml"><mi id="S5.SS2.p1.3.m3.1.1.3a" xref="S5.SS2.p1.3.m3.1.1.3.cmml"></mi><mo id="S5.SS2.p1.3.m3.1.1.3.1" xref="S5.SS2.p1.3.m3.1.1.3.1.cmml">â€²</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">superscript</csymbol><apply id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.3.m3.1.1.2.1.cmml" xref="S5.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS2.p1.3.m3.1.1.2.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2.2">ğ‘</ci><ci id="S5.SS2.p1.3.m3.1.1.2.3.cmml" xref="S5.SS2.p1.3.m3.1.1.2.3">ğ‘–</ci></apply><apply id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3"><ci id="S5.SS2.p1.3.m3.1.1.3.1.cmml" xref="S5.SS2.p1.3.m3.1.1.3.1">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">q_{i}^{{}^{\prime}}</annotation></semantics></math> is generated using word masking or replacement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>, the image <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="v_{i}^{{}^{\prime}}" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><msubsup id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mi id="S5.SS2.p1.4.m4.1.1.2.2" xref="S5.SS2.p1.4.m4.1.1.2.2.cmml">v</mi><mi id="S5.SS2.p1.4.m4.1.1.2.3" xref="S5.SS2.p1.4.m4.1.1.2.3.cmml">i</mi><msup id="S5.SS2.p1.4.m4.1.1.3" xref="S5.SS2.p1.4.m4.1.1.3.cmml"><mi id="S5.SS2.p1.4.m4.1.1.3a" xref="S5.SS2.p1.4.m4.1.1.3.cmml"></mi><mo id="S5.SS2.p1.4.m4.1.1.3.1" xref="S5.SS2.p1.4.m4.1.1.3.1.cmml">â€²</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">superscript</csymbol><apply id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.4.m4.1.1.2.1.cmml" xref="S5.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS2.p1.4.m4.1.1.2.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2.2">ğ‘£</ci><ci id="S5.SS2.p1.4.m4.1.1.2.3.cmml" xref="S5.SS2.p1.4.m4.1.1.2.3">ğ‘–</ci></apply><apply id="S5.SS2.p1.4.m4.1.1.3.cmml" xref="S5.SS2.p1.4.m4.1.1.3"><ci id="S5.SS2.p1.4.m4.1.1.3.1.cmml" xref="S5.SS2.p1.4.m4.1.1.3.1">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">v_{i}^{{}^{\prime}}</annotation></semantics></math> is produced by object swapping and mixing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>, color conversion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite>, and image flipping and resizing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>, and the answer <math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="a_{i}^{{}^{\prime}}" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><msubsup id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml"><mi id="S5.SS2.p1.5.m5.1.1.2.2" xref="S5.SS2.p1.5.m5.1.1.2.2.cmml">a</mi><mi id="S5.SS2.p1.5.m5.1.1.2.3" xref="S5.SS2.p1.5.m5.1.1.2.3.cmml">i</mi><msup id="S5.SS2.p1.5.m5.1.1.3" xref="S5.SS2.p1.5.m5.1.1.3.cmml"><mi id="S5.SS2.p1.5.m5.1.1.3a" xref="S5.SS2.p1.5.m5.1.1.3.cmml"></mi><mo id="S5.SS2.p1.5.m5.1.1.3.1" xref="S5.SS2.p1.5.m5.1.1.3.1.cmml">â€²</mo></msup></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><apply id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.5.m5.1.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">superscript</csymbol><apply id="S5.SS2.p1.5.m5.1.1.2.cmml" xref="S5.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.5.m5.1.1.2.1.cmml" xref="S5.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S5.SS2.p1.5.m5.1.1.2.2.cmml" xref="S5.SS2.p1.5.m5.1.1.2.2">ğ‘</ci><ci id="S5.SS2.p1.5.m5.1.1.2.3.cmml" xref="S5.SS2.p1.5.m5.1.1.2.3">ğ‘–</ci></apply><apply id="S5.SS2.p1.5.m5.1.1.3.cmml" xref="S5.SS2.p1.5.m5.1.1.3"><ci id="S5.SS2.p1.5.m5.1.1.3.1.cmml" xref="S5.SS2.p1.5.m5.1.1.3.1">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">a_{i}^{{}^{\prime}}</annotation></semantics></math> is obtained by the devised answer-assigning mechanism. The answer prediction <math id="S5.SS2.p1.6.m6.1" class="ltx_Math" alttext="\hat{a}_{i}" display="inline"><semantics id="S5.SS2.p1.6.m6.1a"><msub id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml"><mover accent="true" id="S5.SS2.p1.6.m6.1.1.2" xref="S5.SS2.p1.6.m6.1.1.2.cmml"><mi id="S5.SS2.p1.6.m6.1.1.2.2" xref="S5.SS2.p1.6.m6.1.1.2.2.cmml">a</mi><mo id="S5.SS2.p1.6.m6.1.1.2.1" xref="S5.SS2.p1.6.m6.1.1.2.1.cmml">^</mo></mover><mi id="S5.SS2.p1.6.m6.1.1.3" xref="S5.SS2.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><apply id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.6.m6.1.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1">subscript</csymbol><apply id="S5.SS2.p1.6.m6.1.1.2.cmml" xref="S5.SS2.p1.6.m6.1.1.2"><ci id="S5.SS2.p1.6.m6.1.1.2.1.cmml" xref="S5.SS2.p1.6.m6.1.1.2.1">^</ci><ci id="S5.SS2.p1.6.m6.1.1.2.2.cmml" xref="S5.SS2.p1.6.m6.1.1.2.2">ğ‘</ci></apply><ci id="S5.SS2.p1.6.m6.1.1.3.cmml" xref="S5.SS2.p1.6.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">\hat{a}_{i}</annotation></semantics></math> process is the same as the paradigm described in Equation (<a href="#S2.E2" title="In 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>):</p>
<table id="S5.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E9.m1.65" class="ltx_Math" alttext="\begin{split}&amp;\hat{a}_{i}=E_{\mathrm{c}}(E_{\mathrm{m}}(E_{\mathrm{v}}(v_{i}),E_{\mathrm{q}}(q_{i}))),\\
&amp;(v_{i},q_{i},a_{i})\in\mathcal{D}\cup\{(v_{i}^{{}^{\prime}},q_{i}^{{}^{\prime}},a_{i}^{{}^{\prime}})|i\in[1,n]\}.\end{split}" display="block"><semantics id="S5.E9.m1.65a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S5.E9.m1.65.65.3"><mtr id="S5.E9.m1.65.65.3a"><mtd id="S5.E9.m1.65.65.3b"></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.E9.m1.65.65.3c"><mrow id="S5.E9.m1.64.64.2.63.26.26.26"><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1"><msub id="S5.E9.m1.64.64.2.63.26.26.26.1.2"><mover accent="true" id="S5.E9.m1.1.1.1.1.1.1" xref="S5.E9.m1.1.1.1.1.1.1.cmml"><mi id="S5.E9.m1.1.1.1.1.1.1.2" xref="S5.E9.m1.1.1.1.1.1.1.2.cmml">a</mi><mo id="S5.E9.m1.1.1.1.1.1.1.1" xref="S5.E9.m1.1.1.1.1.1.1.1.cmml">^</mo></mover><mi id="S5.E9.m1.2.2.2.2.2.2.1" xref="S5.E9.m1.2.2.2.2.2.2.1.cmml">i</mi></msub><mo id="S5.E9.m1.3.3.3.3.3.3" xref="S5.E9.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1.1"><msub id="S5.E9.m1.64.64.2.63.26.26.26.1.1.3"><mi id="S5.E9.m1.4.4.4.4.4.4" xref="S5.E9.m1.4.4.4.4.4.4.cmml">E</mi><mi mathvariant="normal" id="S5.E9.m1.5.5.5.5.5.5.1" xref="S5.E9.m1.5.5.5.5.5.5.1.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S5.E9.m1.64.64.2.63.26.26.26.1.1.2">â€‹</mo><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1"><mo stretchy="false" id="S5.E9.m1.6.6.6.6.6.6">(</mo><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1"><msub id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.4"><mi id="S5.E9.m1.7.7.7.7.7.7" xref="S5.E9.m1.7.7.7.7.7.7.cmml">E</mi><mi mathvariant="normal" id="S5.E9.m1.8.8.8.8.8.8.1" xref="S5.E9.m1.8.8.8.8.8.8.1.cmml">m</mi></msub><mo lspace="0em" rspace="0em" id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.3">â€‹</mo><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.2.2"><mo stretchy="false" id="S5.E9.m1.9.9.9.9.9.9">(</mo><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.1.1.1"><msub id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.1.1.1.3"><mi id="S5.E9.m1.10.10.10.10.10.10" xref="S5.E9.m1.10.10.10.10.10.10.cmml">E</mi><mi mathvariant="normal" id="S5.E9.m1.11.11.11.11.11.11.1" xref="S5.E9.m1.11.11.11.11.11.11.1.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.1.1.1.2">â€‹</mo><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.1.1.1.1.1"><mo stretchy="false" id="S5.E9.m1.12.12.12.12.12.12">(</mo><msub id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.1.1.1.1.1.1"><mi id="S5.E9.m1.13.13.13.13.13.13" xref="S5.E9.m1.13.13.13.13.13.13.cmml">v</mi><mi id="S5.E9.m1.14.14.14.14.14.14.1" xref="S5.E9.m1.14.14.14.14.14.14.1.cmml">i</mi></msub><mo stretchy="false" id="S5.E9.m1.15.15.15.15.15.15">)</mo></mrow></mrow><mo id="S5.E9.m1.16.16.16.16.16.16">,</mo><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.2.2.2"><msub id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.2.2.2.3"><mi id="S5.E9.m1.17.17.17.17.17.17" xref="S5.E9.m1.17.17.17.17.17.17.cmml">E</mi><mi mathvariant="normal" id="S5.E9.m1.18.18.18.18.18.18.1" xref="S5.E9.m1.18.18.18.18.18.18.1.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.2.2.2.2">â€‹</mo><mrow id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.2.2.2.1.1"><mo stretchy="false" id="S5.E9.m1.19.19.19.19.19.19">(</mo><msub id="S5.E9.m1.64.64.2.63.26.26.26.1.1.1.1.1.2.2.2.1.1.1"><mi id="S5.E9.m1.20.20.20.20.20.20" xref="S5.E9.m1.20.20.20.20.20.20.cmml">q</mi><mi id="S5.E9.m1.21.21.21.21.21.21.1" xref="S5.E9.m1.21.21.21.21.21.21.1.cmml">i</mi></msub><mo stretchy="false" id="S5.E9.m1.22.22.22.22.22.22">)</mo></mrow></mrow><mo stretchy="false" id="S5.E9.m1.23.23.23.23.23.23">)</mo></mrow></mrow><mo stretchy="false" id="S5.E9.m1.24.24.24.24.24.24">)</mo></mrow></mrow></mrow><mo id="S5.E9.m1.25.25.25.25.25.25">,</mo></mrow></mtd></mtr><mtr id="S5.E9.m1.65.65.3d"><mtd id="S5.E9.m1.65.65.3e"></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.E9.m1.65.65.3f"><mrow id="S5.E9.m1.65.65.3.64.38.38.38"><mrow id="S5.E9.m1.65.65.3.64.38.38.38.1"><mrow id="S5.E9.m1.65.65.3.64.38.38.38.1.3.3"><mo stretchy="false" id="S5.E9.m1.26.26.26.1.1.1">(</mo><msub id="S5.E9.m1.65.65.3.64.38.38.38.1.1.1.1"><mi id="S5.E9.m1.27.27.27.2.2.2" xref="S5.E9.m1.27.27.27.2.2.2.cmml">v</mi><mi id="S5.E9.m1.28.28.28.3.3.3.1" xref="S5.E9.m1.28.28.28.3.3.3.1.cmml">i</mi></msub><mo id="S5.E9.m1.29.29.29.4.4.4">,</mo><msub id="S5.E9.m1.65.65.3.64.38.38.38.1.2.2.2"><mi id="S5.E9.m1.30.30.30.5.5.5" xref="S5.E9.m1.30.30.30.5.5.5.cmml">q</mi><mi id="S5.E9.m1.31.31.31.6.6.6.1" xref="S5.E9.m1.31.31.31.6.6.6.1.cmml">i</mi></msub><mo id="S5.E9.m1.32.32.32.7.7.7">,</mo><msub id="S5.E9.m1.65.65.3.64.38.38.38.1.3.3.3"><mi id="S5.E9.m1.33.33.33.8.8.8" xref="S5.E9.m1.33.33.33.8.8.8.cmml">a</mi><mi id="S5.E9.m1.34.34.34.9.9.9.1" xref="S5.E9.m1.34.34.34.9.9.9.1.cmml">i</mi></msub><mo stretchy="false" id="S5.E9.m1.35.35.35.10.10.10">)</mo></mrow><mo id="S5.E9.m1.36.36.36.11.11.11" xref="S5.E9.m1.36.36.36.11.11.11.cmml">âˆˆ</mo><mrow id="S5.E9.m1.65.65.3.64.38.38.38.1.5"><mi class="ltx_font_mathcaligraphic" id="S5.E9.m1.37.37.37.12.12.12" xref="S5.E9.m1.37.37.37.12.12.12.cmml">ğ’Ÿ</mi><mo id="S5.E9.m1.38.38.38.13.13.13" xref="S5.E9.m1.38.38.38.13.13.13.cmml">âˆª</mo><mrow id="S5.E9.m1.65.65.3.64.38.38.38.1.5.2.2"><mo stretchy="false" id="S5.E9.m1.39.39.39.14.14.14">{</mo><mrow id="S5.E9.m1.65.65.3.64.38.38.38.1.4.1.1.1"><mo stretchy="false" id="S5.E9.m1.40.40.40.15.15.15">(</mo><msubsup id="S5.E9.m1.65.65.3.64.38.38.38.1.4.1.1.1.1.1"><mi id="S5.E9.m1.41.41.41.16.16.16" xref="S5.E9.m1.41.41.41.16.16.16.cmml">v</mi><mi id="S5.E9.m1.42.42.42.17.17.17.1" xref="S5.E9.m1.42.42.42.17.17.17.1.cmml">i</mi><msup id="S5.E9.m1.43.43.43.18.18.18.1" xref="S5.E9.m1.43.43.43.18.18.18.1.cmml"><mi id="S5.E9.m1.43.43.43.18.18.18.1a" xref="S5.E9.m1.43.43.43.18.18.18.1.cmml"></mi><mo id="S5.E9.m1.43.43.43.18.18.18.1.1" xref="S5.E9.m1.43.43.43.18.18.18.1.1.cmml">â€²</mo></msup></msubsup><mo id="S5.E9.m1.44.44.44.19.19.19">,</mo><msubsup id="S5.E9.m1.65.65.3.64.38.38.38.1.4.1.1.1.2.2"><mi id="S5.E9.m1.45.45.45.20.20.20" xref="S5.E9.m1.45.45.45.20.20.20.cmml">q</mi><mi id="S5.E9.m1.46.46.46.21.21.21.1" xref="S5.E9.m1.46.46.46.21.21.21.1.cmml">i</mi><msup id="S5.E9.m1.47.47.47.22.22.22.1" xref="S5.E9.m1.47.47.47.22.22.22.1.cmml"><mi id="S5.E9.m1.47.47.47.22.22.22.1a" xref="S5.E9.m1.47.47.47.22.22.22.1.cmml"></mi><mo id="S5.E9.m1.47.47.47.22.22.22.1.1" xref="S5.E9.m1.47.47.47.22.22.22.1.1.cmml">â€²</mo></msup></msubsup><mo id="S5.E9.m1.48.48.48.23.23.23">,</mo><msubsup id="S5.E9.m1.65.65.3.64.38.38.38.1.4.1.1.1.3.3"><mi id="S5.E9.m1.49.49.49.24.24.24" xref="S5.E9.m1.49.49.49.24.24.24.cmml">a</mi><mi id="S5.E9.m1.50.50.50.25.25.25.1" xref="S5.E9.m1.50.50.50.25.25.25.1.cmml">i</mi><msup id="S5.E9.m1.51.51.51.26.26.26.1" xref="S5.E9.m1.51.51.51.26.26.26.1.cmml"><mi id="S5.E9.m1.51.51.51.26.26.26.1a" xref="S5.E9.m1.51.51.51.26.26.26.1.cmml"></mi><mo id="S5.E9.m1.51.51.51.26.26.26.1.1" xref="S5.E9.m1.51.51.51.26.26.26.1.1.cmml">â€²</mo></msup></msubsup><mo stretchy="false" id="S5.E9.m1.52.52.52.27.27.27">)</mo></mrow><mo lspace="0em" rspace="0em" id="S5.E9.m1.53.53.53.28.28.28">|</mo><mrow id="S5.E9.m1.65.65.3.64.38.38.38.1.5.2.2.2"><mi id="S5.E9.m1.54.54.54.29.29.29" xref="S5.E9.m1.54.54.54.29.29.29.cmml">i</mi><mo id="S5.E9.m1.55.55.55.30.30.30" xref="S5.E9.m1.55.55.55.30.30.30.cmml">âˆˆ</mo><mrow id="S5.E9.m1.65.65.3.64.38.38.38.1.5.2.2.2.1"><mo stretchy="false" id="S5.E9.m1.56.56.56.31.31.31">[</mo><mn id="S5.E9.m1.57.57.57.32.32.32" xref="S5.E9.m1.57.57.57.32.32.32.cmml">1</mn><mo id="S5.E9.m1.58.58.58.33.33.33">,</mo><mi id="S5.E9.m1.59.59.59.34.34.34" xref="S5.E9.m1.59.59.59.34.34.34.cmml">n</mi><mo stretchy="false" id="S5.E9.m1.60.60.60.35.35.35">]</mo></mrow></mrow><mo stretchy="false" id="S5.E9.m1.61.61.61.36.36.36">}</mo></mrow></mrow></mrow><mo lspace="0em" id="S5.E9.m1.62.62.62.37.37.37">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S5.E9.m1.65b"><apply id="S5.E9.m1.63.63.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S5.E9.m1.63.63.1.1.1.1.1.cmml"><eq id="S5.E9.m1.3.3.3.3.3.3.cmml" xref="S5.E9.m1.3.3.3.3.3.3"></eq><apply id="S5.E9.m1.63.63.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.1.1.3.1.cmml">subscript</csymbol><apply id="S5.E9.m1.1.1.1.1.1.1.cmml" xref="S5.E9.m1.1.1.1.1.1.1"><ci id="S5.E9.m1.1.1.1.1.1.1.1.cmml" xref="S5.E9.m1.1.1.1.1.1.1.1">^</ci><ci id="S5.E9.m1.1.1.1.1.1.1.2.cmml" xref="S5.E9.m1.1.1.1.1.1.1.2">ğ‘</ci></apply><ci id="S5.E9.m1.2.2.2.2.2.2.1.cmml" xref="S5.E9.m1.2.2.2.2.2.2.1">ğ‘–</ci></apply><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.cmml"><times id="S5.E9.m1.63.63.1.1.1.1.1.1.2.cmml"></times><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.1.1.1.3.1.cmml">subscript</csymbol><ci id="S5.E9.m1.4.4.4.4.4.4.cmml" xref="S5.E9.m1.4.4.4.4.4.4">ğ¸</ci><ci id="S5.E9.m1.5.5.5.5.5.5.1.cmml" xref="S5.E9.m1.5.5.5.5.5.5.1">c</ci></apply><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.cmml"><times id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.3.cmml"></times><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.4.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.4.1.cmml">subscript</csymbol><ci id="S5.E9.m1.7.7.7.7.7.7.cmml" xref="S5.E9.m1.7.7.7.7.7.7">ğ¸</ci><ci id="S5.E9.m1.8.8.8.8.8.8.1.cmml" xref="S5.E9.m1.8.8.8.8.8.8.1">m</ci></apply><interval closure="open" id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.2.3.cmml"><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><times id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"></times><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">subscript</csymbol><ci id="S5.E9.m1.10.10.10.10.10.10.cmml" xref="S5.E9.m1.10.10.10.10.10.10">ğ¸</ci><ci id="S5.E9.m1.11.11.11.11.11.11.1.cmml" xref="S5.E9.m1.11.11.11.11.11.11.1">v</ci></apply><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">subscript</csymbol><ci id="S5.E9.m1.13.13.13.13.13.13.cmml" xref="S5.E9.m1.13.13.13.13.13.13">ğ‘£</ci><ci id="S5.E9.m1.14.14.14.14.14.14.1.cmml" xref="S5.E9.m1.14.14.14.14.14.14.1">ğ‘–</ci></apply></apply><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><times id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml"></times><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">subscript</csymbol><ci id="S5.E9.m1.17.17.17.17.17.17.cmml" xref="S5.E9.m1.17.17.17.17.17.17">ğ¸</ci><ci id="S5.E9.m1.18.18.18.18.18.18.1.cmml" xref="S5.E9.m1.18.18.18.18.18.18.1">q</ci></apply><apply id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.1.cmml">subscript</csymbol><ci id="S5.E9.m1.20.20.20.20.20.20.cmml" xref="S5.E9.m1.20.20.20.20.20.20">ğ‘</ci><ci id="S5.E9.m1.21.21.21.21.21.21.1.cmml" xref="S5.E9.m1.21.21.21.21.21.21.1">ğ‘–</ci></apply></apply></interval></apply></apply></apply><apply id="S5.E9.m1.63.63.1.1.1.2.2.cmml"><in id="S5.E9.m1.36.36.36.11.11.11.cmml" xref="S5.E9.m1.36.36.36.11.11.11"></in><vector id="S5.E9.m1.63.63.1.1.1.2.2.3.4.cmml"><apply id="S5.E9.m1.63.63.1.1.1.2.2.1.1.1.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.1.1.1.1.cmml">subscript</csymbol><ci id="S5.E9.m1.27.27.27.2.2.2.cmml" xref="S5.E9.m1.27.27.27.2.2.2">ğ‘£</ci><ci id="S5.E9.m1.28.28.28.3.3.3.1.cmml" xref="S5.E9.m1.28.28.28.3.3.3.1">ğ‘–</ci></apply><apply id="S5.E9.m1.63.63.1.1.1.2.2.2.2.2.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.2.2.2.1.cmml">subscript</csymbol><ci id="S5.E9.m1.30.30.30.5.5.5.cmml" xref="S5.E9.m1.30.30.30.5.5.5">ğ‘</ci><ci id="S5.E9.m1.31.31.31.6.6.6.1.cmml" xref="S5.E9.m1.31.31.31.6.6.6.1">ğ‘–</ci></apply><apply id="S5.E9.m1.63.63.1.1.1.2.2.3.3.3.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.3.3.3.1.cmml">subscript</csymbol><ci id="S5.E9.m1.33.33.33.8.8.8.cmml" xref="S5.E9.m1.33.33.33.8.8.8">ğ‘</ci><ci id="S5.E9.m1.34.34.34.9.9.9.1.cmml" xref="S5.E9.m1.34.34.34.9.9.9.1">ğ‘–</ci></apply></vector><apply id="S5.E9.m1.63.63.1.1.1.2.2.5.cmml"><union id="S5.E9.m1.38.38.38.13.13.13.cmml" xref="S5.E9.m1.38.38.38.13.13.13"></union><ci id="S5.E9.m1.37.37.37.12.12.12.cmml" xref="S5.E9.m1.37.37.37.12.12.12">ğ’Ÿ</ci><apply id="S5.E9.m1.63.63.1.1.1.2.2.5.2.3.cmml"><csymbol cd="latexml" id="S5.E9.m1.63.63.1.1.1.2.2.5.2.3.1.cmml">conditional-set</csymbol><vector id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.4.cmml"><apply id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.1.1.1.cmml">superscript</csymbol><apply id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.1.1.2.1.cmml">subscript</csymbol><ci id="S5.E9.m1.41.41.41.16.16.16.cmml" xref="S5.E9.m1.41.41.41.16.16.16">ğ‘£</ci><ci id="S5.E9.m1.42.42.42.17.17.17.1.cmml" xref="S5.E9.m1.42.42.42.17.17.17.1">ğ‘–</ci></apply><apply id="S5.E9.m1.43.43.43.18.18.18.1.cmml" xref="S5.E9.m1.43.43.43.18.18.18.1"><ci id="S5.E9.m1.43.43.43.18.18.18.1.1.cmml" xref="S5.E9.m1.43.43.43.18.18.18.1.1">â€²</ci></apply></apply><apply id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.2.2.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.2.2.1.cmml">superscript</csymbol><apply id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.2.2.2.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.2.2.2.1.cmml">subscript</csymbol><ci id="S5.E9.m1.45.45.45.20.20.20.cmml" xref="S5.E9.m1.45.45.45.20.20.20">ğ‘</ci><ci id="S5.E9.m1.46.46.46.21.21.21.1.cmml" xref="S5.E9.m1.46.46.46.21.21.21.1">ğ‘–</ci></apply><apply id="S5.E9.m1.47.47.47.22.22.22.1.cmml" xref="S5.E9.m1.47.47.47.22.22.22.1"><ci id="S5.E9.m1.47.47.47.22.22.22.1.1.cmml" xref="S5.E9.m1.47.47.47.22.22.22.1.1">â€²</ci></apply></apply><apply id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.3.3.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.3.3.1.cmml">superscript</csymbol><apply id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.3.3.2.cmml"><csymbol cd="ambiguous" id="S5.E9.m1.63.63.1.1.1.2.2.4.1.1.1.3.3.2.1.cmml">subscript</csymbol><ci id="S5.E9.m1.49.49.49.24.24.24.cmml" xref="S5.E9.m1.49.49.49.24.24.24">ğ‘</ci><ci id="S5.E9.m1.50.50.50.25.25.25.1.cmml" xref="S5.E9.m1.50.50.50.25.25.25.1">ğ‘–</ci></apply><apply id="S5.E9.m1.51.51.51.26.26.26.1.cmml" xref="S5.E9.m1.51.51.51.26.26.26.1"><ci id="S5.E9.m1.51.51.51.26.26.26.1.1.cmml" xref="S5.E9.m1.51.51.51.26.26.26.1.1">â€²</ci></apply></apply></vector><apply id="S5.E9.m1.63.63.1.1.1.2.2.5.2.2.2.cmml"><in id="S5.E9.m1.55.55.55.30.30.30.cmml" xref="S5.E9.m1.55.55.55.30.30.30"></in><ci id="S5.E9.m1.54.54.54.29.29.29.cmml" xref="S5.E9.m1.54.54.54.29.29.29">ğ‘–</ci><interval closure="closed" id="S5.E9.m1.63.63.1.1.1.2.2.5.2.2.2.3.cmml"><cn type="integer" id="S5.E9.m1.57.57.57.32.32.32.cmml" xref="S5.E9.m1.57.57.57.32.32.32">1</cn><ci id="S5.E9.m1.59.59.59.34.34.34.cmml" xref="S5.E9.m1.59.59.59.34.34.34">ğ‘›</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E9.m1.65c">\begin{split}&amp;\hat{a}_{i}=E_{\mathrm{c}}(E_{\mathrm{m}}(E_{\mathrm{v}}(v_{i}),E_{\mathrm{q}}(q_{i}))),\\
&amp;(v_{i},q_{i},a_{i})\in\mathcal{D}\cup\{(v_{i}^{{}^{\prime}},q_{i}^{{}^{\prime}},a_{i}^{{}^{\prime}})|i\in[1,n]\}.\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The methods usually perform data augmentation from two perspectives to achieve robust performance: 1) <em id="S5.SS2.p2.1.1" class="ltx_emph ltx_font_italic">synthetic-based</em>: generate new training samples by modifying regions or words of the original images or questions; 2) <em id="S5.SS2.p2.1.2" class="ltx_emph ltx_font_italic">pairing-based</em>: generate new samples by re-matching relevant questions for images. The examples generated by the mentioned techniques are shown in Fig. <a href="#S5.F12" title="Figure 12 â€£ 5.2 Data Augmentation â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. The left is an original sample, while the right is the augmented sample based on the left sample. The synthetic method masks the â€œsheepâ€ region in the image, resulting in the original question having a new answer â€œ0â€. Compared with this, the pairing method does not change the original image instead of retrieving related questions to it.</p>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/2307.11471/assets/x12.png" id="S5.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Comparison between original samples and augmented samples. The synthetic-based technique generates new training samples by modifying regions or words of the original images or questions, while the pairing-based technique generates new samples by re-matching relevant questions for images.</figcaption>
</figure>
<figure id="S5.F13" class="ltx_figure"><img src="/html/2307.11471/assets/x13.png" id="S5.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="107" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Illustration of self-supervised contrastive learning-based debiasing methods. The contrastive learning loss <math id="S5.F13.2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{C}}" display="inline"><semantics id="S5.F13.2.m1.1b"><msub id="S5.F13.2.m1.1.1" xref="S5.F13.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.F13.2.m1.1.1.2" xref="S5.F13.2.m1.1.1.2.cmml">â„’</mi><mi mathvariant="normal" id="S5.F13.2.m1.1.1.3" xref="S5.F13.2.m1.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S5.F13.2.m1.1c"><apply id="S5.F13.2.m1.1.1.cmml" xref="S5.F13.2.m1.1.1"><csymbol cd="ambiguous" id="S5.F13.2.m1.1.1.1.cmml" xref="S5.F13.2.m1.1.1">subscript</csymbol><ci id="S5.F13.2.m1.1.1.2.cmml" xref="S5.F13.2.m1.1.1.2">â„’</ci><ci id="S5.F13.2.m1.1.1.3.cmml" xref="S5.F13.2.m1.1.1.3">C</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F13.2.m1.1d">\mathcal{L}_{\mathrm{C}}</annotation></semantics></math> is calculated by narrowing the distance between similar samples and enlarging the distance between dissimilar samples in the multi-modal joint embedding space. The model is trained by jointly optimizing VQA and contrastive losses.</figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Synthetic-based.</span> Kafle <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> used existing semantic annotations in the dataset and RNN-based methods to produce new questions respectively.
Agarwal <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> leveraged a GAN-based model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite> to remove objects and then mitigated the bias by adversarial training. <span id="S5.SS2.p3.1.4" class="ltx_text" style="color:#000000;">An ideal VQA model should possess two key characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>: (1) visual-explainable ability: the model should make accurate predictions by leveraging relevant visual regions. (2) question-sensitive ability: the model should be sensitive to different questions, <em id="S5.SS2.p3.1.4.1" class="ltx_emph ltx_font_italic">i.e.</em>, the model is expected to yield varying responses for different questions.</span> To address this issue, CSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> and ECD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> first synthesize counterfactual image-question pairs by masking critical objects in the original image and critical words in the original question and then assign ground-truth answers to those synthesized samples. This can drive the method to employ informative data to answer questions. In order to force the method to concentrate on the critical elements of inputs, MUTANT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite> performs mutating on the input images and questions to expose the model to perceptually similar yet semantically dissimilar samples.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Pairing-based.</span> To make the synthesized samples more natural, SimpleAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> utilizes rich semantic annotations in the training data to pair images with other relevant questions, instead of generating them from scratch and employs a series of rules to ascertain the existence of ground-truth answers. However, the reasonable answers generated by the mentioned rules may limit the generalization. To address this issue, KDDAug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> relaxes the requirements for reasonable image-question pairs and generates pseudo-answers for all composed pairs using a knowledge distillation-based answer assignment.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text" style="color:#000000;">There also exist other alternatives to achieve the purpose of data augmentation. Specifically, DLR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> decomposes the question representation into three distinct phrase representations: type, object, and concept, which are then integrated to predict answers. VGQE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> learns question representations by leveraging both visual information extracted from the image and linguistic information derived from the question prior to multimodal fusion. CVL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> uses a causal model with an additional variable to generate counterfactual samples, which compels the VQA model to utilize both input modalities instead of depending on statistical patterns that are specific to either one. The aforementioned methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> augment data based on the internal (original) data. Compared with this, we can also expand data from the external source. For instance, inspired by meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>, <a href="#bib.bib146" title="" class="ltx_ref">146</a>, <a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>, ASL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> retrieves the relevant samples with image-question pairs from an external source, which is leveraged to learn adapting parameters for the VQA baseline, such as UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, thus acquiring better generalization ability.</span></p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span id="S5.SS3.1.1" class="ltx_text ltx_font_italic">Self-Supervised Contrastive Learning</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.4" class="ltx_p">Self-supervised contrastive learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite> aims at learning an embedding space where similar sample pairs are positioned closely together while dissimilar ones are widely separated. Its use in robust VQA is still at the starting stage. The paradigm of contrastive learning-based debiasing methods is to first generate positive and negative samples that differ from the original samples using data-augmentation techniques introduced in the above subsection, then perform question answering by the vanilla VQA method <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">f</annotation></semantics></math> described in Equation (<a href="#S2" title="2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), and finally be optimized jointly by the contrastive learning loss <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{C}}" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><msub id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">â„’</mi><mi mathvariant="normal" id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">â„’</ci><ci id="S5.SS3.p1.2.m2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.1.1.3">C</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">\mathcal{L}_{\mathrm{C}}</annotation></semantics></math> of multi-modal representations and the VQA loss <math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\mathrm{V}}" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><msub id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">â„’</mi><mi mathvariant="normal" id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">â„’</ci><ci id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3">V</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">\mathcal{L}_{\mathrm{V}}</annotation></semantics></math>, as shown in Fig. <a href="#S5.F13" title="Figure 13 â€£ 5.2 Data Augmentation â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. The joint loss <math id="S5.SS3.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S5.SS3.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><ci id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">\mathcal{L}</annotation></semantics></math> can be formulated as follows:</p>
<table id="S5.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E10.m1.45" class="ltx_Math" alttext="\begin{split}&amp;\mathcal{L}=\lambda_{\mathrm{C}}\mathcal{L}_{\mathrm{C}}+\lambda_{\mathrm{V}}\mathcal{L}_{\mathrm{V}},\\
&amp;\mathcal{L}_{\mathrm{C}}=\mathop{\mathbb{E}}\limits_{o,p,n\in\mathcal{D}^{*}}\left[-\log\left(\frac{e^{s(o,p)}}{e^{s(o,p)}+e^{s(o,n)}}\right)\right],\\
&amp;\mathcal{L}_{\mathrm{V}}=-\frac{1}{|\mathcal{D}^{*}|}\sum_{i=1}^{|\mathcal{D}^{*}|}[a_{i}]\log\hat{a}_{i},\\
\end{split}" display="block"><semantics id="S5.E10.m1.45a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S5.E10.m1.45.45.4"><mtr id="S5.E10.m1.45.45.4a"><mtd id="S5.E10.m1.45.45.4b"></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.E10.m1.45.45.4c"><mrow id="S5.E10.m1.43.43.2.42.13.13.13"><mrow id="S5.E10.m1.43.43.2.42.13.13.13.1"><mi class="ltx_font_mathcaligraphic" id="S5.E10.m1.1.1.1.1.1.1" xref="S5.E10.m1.1.1.1.1.1.1.cmml">â„’</mi><mo id="S5.E10.m1.2.2.2.2.2.2" xref="S5.E10.m1.2.2.2.2.2.2.cmml">=</mo><mrow id="S5.E10.m1.43.43.2.42.13.13.13.1.1"><mrow id="S5.E10.m1.43.43.2.42.13.13.13.1.1.1"><msub id="S5.E10.m1.43.43.2.42.13.13.13.1.1.1.2"><mi id="S5.E10.m1.3.3.3.3.3.3" xref="S5.E10.m1.3.3.3.3.3.3.cmml">Î»</mi><mi mathvariant="normal" id="S5.E10.m1.4.4.4.4.4.4.1" xref="S5.E10.m1.4.4.4.4.4.4.1.cmml">C</mi></msub><mo lspace="0em" rspace="0em" id="S5.E10.m1.43.43.2.42.13.13.13.1.1.1.1">â€‹</mo><msub id="S5.E10.m1.43.43.2.42.13.13.13.1.1.1.3"><mi class="ltx_font_mathcaligraphic" id="S5.E10.m1.5.5.5.5.5.5" xref="S5.E10.m1.5.5.5.5.5.5.cmml">â„’</mi><mi mathvariant="normal" id="S5.E10.m1.6.6.6.6.6.6.1" xref="S5.E10.m1.6.6.6.6.6.6.1.cmml">C</mi></msub></mrow><mo id="S5.E10.m1.7.7.7.7.7.7" xref="S5.E10.m1.7.7.7.7.7.7.cmml">+</mo><mrow id="S5.E10.m1.43.43.2.42.13.13.13.1.1.2"><msub id="S5.E10.m1.43.43.2.42.13.13.13.1.1.2.2"><mi id="S5.E10.m1.8.8.8.8.8.8" xref="S5.E10.m1.8.8.8.8.8.8.cmml">Î»</mi><mi mathvariant="normal" id="S5.E10.m1.9.9.9.9.9.9.1" xref="S5.E10.m1.9.9.9.9.9.9.1.cmml">V</mi></msub><mo lspace="0em" rspace="0em" id="S5.E10.m1.43.43.2.42.13.13.13.1.1.2.1">â€‹</mo><msub id="S5.E10.m1.43.43.2.42.13.13.13.1.1.2.3"><mi class="ltx_font_mathcaligraphic" id="S5.E10.m1.10.10.10.10.10.10" xref="S5.E10.m1.10.10.10.10.10.10.cmml">â„’</mi><mi mathvariant="normal" id="S5.E10.m1.11.11.11.11.11.11.1" xref="S5.E10.m1.11.11.11.11.11.11.1.cmml">V</mi></msub></mrow></mrow></mrow><mo id="S5.E10.m1.12.12.12.12.12.12">,</mo></mrow></mtd></mtr><mtr id="S5.E10.m1.45.45.4d"><mtd id="S5.E10.m1.45.45.4e"></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.E10.m1.45.45.4f"><mrow id="S5.E10.m1.44.44.3.43.14.14.14"><mrow id="S5.E10.m1.44.44.3.43.14.14.14.1"><msub id="S5.E10.m1.44.44.3.43.14.14.14.1.2"><mi class="ltx_font_mathcaligraphic" id="S5.E10.m1.13.13.13.1.1.1" xref="S5.E10.m1.13.13.13.1.1.1.cmml">â„’</mi><mi mathvariant="normal" id="S5.E10.m1.14.14.14.2.2.2.1" xref="S5.E10.m1.14.14.14.2.2.2.1.cmml">C</mi></msub><mo rspace="0.1389em" id="S5.E10.m1.15.15.15.3.3.3" xref="S5.E10.m1.15.15.15.3.3.3.cmml">=</mo><mrow id="S5.E10.m1.44.44.3.43.14.14.14.1.1"><munder id="S5.E10.m1.44.44.3.43.14.14.14.1.1.2"><mo lspace="0.1389em" movablelimits="false" rspace="0em" id="S5.E10.m1.16.16.16.4.4.4" xref="S5.E10.m1.16.16.16.4.4.4.cmml">ğ”¼</mo><mrow id="S5.E10.m1.17.17.17.5.5.5.1" xref="S5.E10.m1.17.17.17.5.5.5.1.cmml"><mrow id="S5.E10.m1.17.17.17.5.5.5.1.5.2" xref="S5.E10.m1.17.17.17.5.5.5.1.5.1.cmml"><mi id="S5.E10.m1.17.17.17.5.5.5.1.1" xref="S5.E10.m1.17.17.17.5.5.5.1.1.cmml">o</mi><mo id="S5.E10.m1.17.17.17.5.5.5.1.5.2.1" xref="S5.E10.m1.17.17.17.5.5.5.1.5.1.cmml">,</mo><mi id="S5.E10.m1.17.17.17.5.5.5.1.2" xref="S5.E10.m1.17.17.17.5.5.5.1.2.cmml">p</mi><mo id="S5.E10.m1.17.17.17.5.5.5.1.5.2.2" xref="S5.E10.m1.17.17.17.5.5.5.1.5.1.cmml">,</mo><mi id="S5.E10.m1.17.17.17.5.5.5.1.3" xref="S5.E10.m1.17.17.17.5.5.5.1.3.cmml">n</mi></mrow><mo id="S5.E10.m1.17.17.17.5.5.5.1.4" xref="S5.E10.m1.17.17.17.5.5.5.1.4.cmml">âˆˆ</mo><msup id="S5.E10.m1.17.17.17.5.5.5.1.6" xref="S5.E10.m1.17.17.17.5.5.5.1.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E10.m1.17.17.17.5.5.5.1.6.2" xref="S5.E10.m1.17.17.17.5.5.5.1.6.2.cmml">ğ’Ÿ</mi><mo id="S5.E10.m1.17.17.17.5.5.5.1.6.3" xref="S5.E10.m1.17.17.17.5.5.5.1.6.3.cmml">âˆ—</mo></msup></mrow></munder><mrow id="S5.E10.m1.44.44.3.43.14.14.14.1.1.1.1"><mo id="S5.E10.m1.18.18.18.6.6.6">[</mo><mrow id="S5.E10.m1.44.44.3.43.14.14.14.1.1.1.1.1"><mo rspace="0.167em" id="S5.E10.m1.44.44.3.43.14.14.14.1.1.1.1.1a">âˆ’</mo><mrow id="S5.E10.m1.44.44.3.43.14.14.14.1.1.1.1.1.1"><mi id="S5.E10.m1.20.20.20.8.8.8" xref="S5.E10.m1.20.20.20.8.8.8.cmml">log</mi><mo id="S5.E10.m1.44.44.3.43.14.14.14.1.1.1.1.1.1a">â¡</mo><mrow id="S5.E10.m1.44.44.3.43.14.14.14.1.1.1.1.1.1.1"><mo id="S5.E10.m1.21.21.21.9.9.9">(</mo><mfrac id="S5.E10.m1.22.22.22.10.10.10" xref="S5.E10.m1.22.22.22.10.10.10.cmml"><msup id="S5.E10.m1.22.22.22.10.10.10.2" xref="S5.E10.m1.22.22.22.10.10.10.2.cmml"><mi id="S5.E10.m1.22.22.22.10.10.10.2.4" xref="S5.E10.m1.22.22.22.10.10.10.2.4.cmml">e</mi><mrow id="S5.E10.m1.22.22.22.10.10.10.2.2.2" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.cmml"><mi id="S5.E10.m1.22.22.22.10.10.10.2.2.2.4" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E10.m1.22.22.22.10.10.10.2.2.2.3" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.3.cmml">â€‹</mo><mrow id="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.2" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.1.cmml"><mo stretchy="false" id="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.2.1" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.1.cmml">(</mo><mi id="S5.E10.m1.22.22.22.10.10.10.1.1.1.1" xref="S5.E10.m1.22.22.22.10.10.10.1.1.1.1.cmml">o</mi><mo id="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.2.2" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.1.cmml">,</mo><mi id="S5.E10.m1.22.22.22.10.10.10.2.2.2.2" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.2.cmml">p</mi><mo stretchy="false" id="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.2.3" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.1.cmml">)</mo></mrow></mrow></msup><mrow id="S5.E10.m1.22.22.22.10.10.10.6" xref="S5.E10.m1.22.22.22.10.10.10.6.cmml"><msup id="S5.E10.m1.22.22.22.10.10.10.6.6" xref="S5.E10.m1.22.22.22.10.10.10.6.6.cmml"><mi id="S5.E10.m1.22.22.22.10.10.10.6.6.2" xref="S5.E10.m1.22.22.22.10.10.10.6.6.2.cmml">e</mi><mrow id="S5.E10.m1.22.22.22.10.10.10.4.2.2" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.cmml"><mi id="S5.E10.m1.22.22.22.10.10.10.4.2.2.4" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E10.m1.22.22.22.10.10.10.4.2.2.3" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.3.cmml">â€‹</mo><mrow id="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.2" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.1.cmml"><mo stretchy="false" id="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.2.1" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.1.cmml">(</mo><mi id="S5.E10.m1.22.22.22.10.10.10.3.1.1.1" xref="S5.E10.m1.22.22.22.10.10.10.3.1.1.1.cmml">o</mi><mo id="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.2.2" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.1.cmml">,</mo><mi id="S5.E10.m1.22.22.22.10.10.10.4.2.2.2" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.2.cmml">p</mi><mo stretchy="false" id="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.2.3" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.1.cmml">)</mo></mrow></mrow></msup><mo id="S5.E10.m1.22.22.22.10.10.10.6.5" xref="S5.E10.m1.22.22.22.10.10.10.6.5.cmml">+</mo><msup id="S5.E10.m1.22.22.22.10.10.10.6.7" xref="S5.E10.m1.22.22.22.10.10.10.6.7.cmml"><mi id="S5.E10.m1.22.22.22.10.10.10.6.7.2" xref="S5.E10.m1.22.22.22.10.10.10.6.7.2.cmml">e</mi><mrow id="S5.E10.m1.22.22.22.10.10.10.6.4.2" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.cmml"><mi id="S5.E10.m1.22.22.22.10.10.10.6.4.2.4" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E10.m1.22.22.22.10.10.10.6.4.2.3" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.3.cmml">â€‹</mo><mrow id="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.2" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.1.cmml"><mo stretchy="false" id="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.2.1" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.1.cmml">(</mo><mi id="S5.E10.m1.22.22.22.10.10.10.5.3.1.1" xref="S5.E10.m1.22.22.22.10.10.10.5.3.1.1.cmml">o</mi><mo id="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.2.2" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.1.cmml">,</mo><mi id="S5.E10.m1.22.22.22.10.10.10.6.4.2.2" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.2.cmml">n</mi><mo stretchy="false" id="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.2.3" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.1.cmml">)</mo></mrow></mrow></msup></mrow></mfrac><mo id="S5.E10.m1.23.23.23.11.11.11">)</mo></mrow></mrow></mrow><mo id="S5.E10.m1.24.24.24.12.12.12">]</mo></mrow></mrow></mrow><mo id="S5.E10.m1.25.25.25.13.13.13">,</mo></mrow></mtd></mtr><mtr id="S5.E10.m1.45.45.4g"><mtd id="S5.E10.m1.45.45.4h"></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.E10.m1.45.45.4i"><mrow id="S5.E10.m1.45.45.4.44.17.17.17"><mrow id="S5.E10.m1.45.45.4.44.17.17.17.1"><msub id="S5.E10.m1.45.45.4.44.17.17.17.1.2"><mi class="ltx_font_mathcaligraphic" id="S5.E10.m1.26.26.26.1.1.1" xref="S5.E10.m1.26.26.26.1.1.1.cmml">â„’</mi><mi mathvariant="normal" id="S5.E10.m1.27.27.27.2.2.2.1" xref="S5.E10.m1.27.27.27.2.2.2.1.cmml">V</mi></msub><mo id="S5.E10.m1.28.28.28.3.3.3" xref="S5.E10.m1.28.28.28.3.3.3.cmml">=</mo><mrow id="S5.E10.m1.45.45.4.44.17.17.17.1.1"><mo id="S5.E10.m1.45.45.4.44.17.17.17.1.1a">âˆ’</mo><mrow id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1"><mfrac id="S5.E10.m1.30.30.30.5.5.5" xref="S5.E10.m1.30.30.30.5.5.5.cmml"><mn id="S5.E10.m1.30.30.30.5.5.5.3" xref="S5.E10.m1.30.30.30.5.5.5.3.cmml">1</mn><mrow id="S5.E10.m1.30.30.30.5.5.5.1.1" xref="S5.E10.m1.30.30.30.5.5.5.1.2.cmml"><mo stretchy="false" id="S5.E10.m1.30.30.30.5.5.5.1.1.2" xref="S5.E10.m1.30.30.30.5.5.5.1.2.1.cmml">|</mo><msup id="S5.E10.m1.30.30.30.5.5.5.1.1.1" xref="S5.E10.m1.30.30.30.5.5.5.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E10.m1.30.30.30.5.5.5.1.1.1.2" xref="S5.E10.m1.30.30.30.5.5.5.1.1.1.2.cmml">ğ’Ÿ</mi><mo id="S5.E10.m1.30.30.30.5.5.5.1.1.1.3" xref="S5.E10.m1.30.30.30.5.5.5.1.1.1.3.cmml">âˆ—</mo></msup><mo stretchy="false" id="S5.E10.m1.30.30.30.5.5.5.1.1.3" xref="S5.E10.m1.30.30.30.5.5.5.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.2">â€‹</mo><mrow id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1"><munderover id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1.2"><mo movablelimits="false" rspace="0em" id="S5.E10.m1.31.31.31.6.6.6" xref="S5.E10.m1.31.31.31.6.6.6.cmml">âˆ‘</mo><mrow id="S5.E10.m1.32.32.32.7.7.7.1" xref="S5.E10.m1.32.32.32.7.7.7.1.cmml"><mi id="S5.E10.m1.32.32.32.7.7.7.1.2" xref="S5.E10.m1.32.32.32.7.7.7.1.2.cmml">i</mi><mo id="S5.E10.m1.32.32.32.7.7.7.1.1" xref="S5.E10.m1.32.32.32.7.7.7.1.1.cmml">=</mo><mn id="S5.E10.m1.32.32.32.7.7.7.1.3" xref="S5.E10.m1.32.32.32.7.7.7.1.3.cmml">1</mn></mrow><mrow id="S5.E10.m1.33.33.33.8.8.8.1.1" xref="S5.E10.m1.33.33.33.8.8.8.1.2.cmml"><mo stretchy="false" id="S5.E10.m1.33.33.33.8.8.8.1.1.2" xref="S5.E10.m1.33.33.33.8.8.8.1.2.1.cmml">|</mo><msup id="S5.E10.m1.33.33.33.8.8.8.1.1.1" xref="S5.E10.m1.33.33.33.8.8.8.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E10.m1.33.33.33.8.8.8.1.1.1.2" xref="S5.E10.m1.33.33.33.8.8.8.1.1.1.2.cmml">ğ’Ÿ</mi><mo id="S5.E10.m1.33.33.33.8.8.8.1.1.1.3" xref="S5.E10.m1.33.33.33.8.8.8.1.1.1.3.cmml">âˆ—</mo></msup><mo stretchy="false" id="S5.E10.m1.33.33.33.8.8.8.1.1.3" xref="S5.E10.m1.33.33.33.8.8.8.1.2.1.cmml">|</mo></mrow></munderover><mrow id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1.1"><mrow id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1.1.1.1"><mo stretchy="false" id="S5.E10.m1.34.34.34.9.9.9">[</mo><msub id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1.1.1.1.1"><mi id="S5.E10.m1.35.35.35.10.10.10" xref="S5.E10.m1.35.35.35.10.10.10.cmml">a</mi><mi id="S5.E10.m1.36.36.36.11.11.11.1" xref="S5.E10.m1.36.36.36.11.11.11.1.cmml">i</mi></msub><mo stretchy="false" id="S5.E10.m1.37.37.37.12.12.12">]</mo></mrow><mo lspace="0.167em" rspace="0em" id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1.1.2">â€‹</mo><mrow id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1.1.3"><mi id="S5.E10.m1.38.38.38.13.13.13" xref="S5.E10.m1.38.38.38.13.13.13.cmml">log</mi><mo lspace="0.167em" id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1.1.3a">â¡</mo><msub id="S5.E10.m1.45.45.4.44.17.17.17.1.1.1.1.1.3.1"><mover accent="true" id="S5.E10.m1.39.39.39.14.14.14" xref="S5.E10.m1.39.39.39.14.14.14.cmml"><mi id="S5.E10.m1.39.39.39.14.14.14.2" xref="S5.E10.m1.39.39.39.14.14.14.2.cmml">a</mi><mo id="S5.E10.m1.39.39.39.14.14.14.1" xref="S5.E10.m1.39.39.39.14.14.14.1.cmml">^</mo></mover><mi id="S5.E10.m1.40.40.40.15.15.15.1" xref="S5.E10.m1.40.40.40.15.15.15.1.cmml">i</mi></msub></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S5.E10.m1.41.41.41.16.16.16">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S5.E10.m1.45b"><apply id="S5.E10.m1.42.42.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S5.E10.m1.42.42.1.1.1.1.1.cmml"><eq id="S5.E10.m1.2.2.2.2.2.2.cmml" xref="S5.E10.m1.2.2.2.2.2.2"></eq><ci id="S5.E10.m1.1.1.1.1.1.1.cmml" xref="S5.E10.m1.1.1.1.1.1.1">â„’</ci><apply id="S5.E10.m1.42.42.1.1.1.1.1.3.cmml"><plus id="S5.E10.m1.7.7.7.7.7.7.cmml" xref="S5.E10.m1.7.7.7.7.7.7"></plus><apply id="S5.E10.m1.42.42.1.1.1.1.1.3.2.cmml"><times id="S5.E10.m1.42.42.1.1.1.1.1.3.2.1.cmml"></times><apply id="S5.E10.m1.42.42.1.1.1.1.1.3.2.2.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.1.1.3.2.2.1.cmml">subscript</csymbol><ci id="S5.E10.m1.3.3.3.3.3.3.cmml" xref="S5.E10.m1.3.3.3.3.3.3">ğœ†</ci><ci id="S5.E10.m1.4.4.4.4.4.4.1.cmml" xref="S5.E10.m1.4.4.4.4.4.4.1">C</ci></apply><apply id="S5.E10.m1.42.42.1.1.1.1.1.3.2.3.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.1.1.3.2.3.1.cmml">subscript</csymbol><ci id="S5.E10.m1.5.5.5.5.5.5.cmml" xref="S5.E10.m1.5.5.5.5.5.5">â„’</ci><ci id="S5.E10.m1.6.6.6.6.6.6.1.cmml" xref="S5.E10.m1.6.6.6.6.6.6.1">C</ci></apply></apply><apply id="S5.E10.m1.42.42.1.1.1.1.1.3.3.cmml"><times id="S5.E10.m1.42.42.1.1.1.1.1.3.3.1.cmml"></times><apply id="S5.E10.m1.42.42.1.1.1.1.1.3.3.2.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.1.1.3.3.2.1.cmml">subscript</csymbol><ci id="S5.E10.m1.8.8.8.8.8.8.cmml" xref="S5.E10.m1.8.8.8.8.8.8">ğœ†</ci><ci id="S5.E10.m1.9.9.9.9.9.9.1.cmml" xref="S5.E10.m1.9.9.9.9.9.9.1">V</ci></apply><apply id="S5.E10.m1.42.42.1.1.1.1.1.3.3.3.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.1.1.3.3.3.1.cmml">subscript</csymbol><ci id="S5.E10.m1.10.10.10.10.10.10.cmml" xref="S5.E10.m1.10.10.10.10.10.10">â„’</ci><ci id="S5.E10.m1.11.11.11.11.11.11.1.cmml" xref="S5.E10.m1.11.11.11.11.11.11.1">V</ci></apply></apply></apply></apply><apply id="S5.E10.m1.42.42.1.1.1.2.2.3.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.2.2.3a.cmml">formulae-sequence</csymbol><apply id="S5.E10.m1.42.42.1.1.1.2.2.1.1.cmml"><eq id="S5.E10.m1.15.15.15.3.3.3.cmml" xref="S5.E10.m1.15.15.15.3.3.3"></eq><apply id="S5.E10.m1.42.42.1.1.1.2.2.1.1.3.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.2.2.1.1.3.1.cmml">subscript</csymbol><ci id="S5.E10.m1.13.13.13.1.1.1.cmml" xref="S5.E10.m1.13.13.13.1.1.1">â„’</ci><ci id="S5.E10.m1.14.14.14.2.2.2.1.cmml" xref="S5.E10.m1.14.14.14.2.2.2.1">C</ci></apply><apply id="S5.E10.m1.42.42.1.1.1.2.2.1.1.1.cmml"><apply id="S5.E10.m1.42.42.1.1.1.2.2.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.2.2.1.1.1.2.1.cmml">subscript</csymbol><ci id="S5.E10.m1.16.16.16.4.4.4.cmml" xref="S5.E10.m1.16.16.16.4.4.4">ğ”¼</ci><apply id="S5.E10.m1.17.17.17.5.5.5.1.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1"><in id="S5.E10.m1.17.17.17.5.5.5.1.4.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.4"></in><list id="S5.E10.m1.17.17.17.5.5.5.1.5.1.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.5.2"><ci id="S5.E10.m1.17.17.17.5.5.5.1.1.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.1">ğ‘œ</ci><ci id="S5.E10.m1.17.17.17.5.5.5.1.2.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.2">ğ‘</ci><ci id="S5.E10.m1.17.17.17.5.5.5.1.3.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.3">ğ‘›</ci></list><apply id="S5.E10.m1.17.17.17.5.5.5.1.6.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.6"><csymbol cd="ambiguous" id="S5.E10.m1.17.17.17.5.5.5.1.6.1.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.6">superscript</csymbol><ci id="S5.E10.m1.17.17.17.5.5.5.1.6.2.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.6.2">ğ’Ÿ</ci><times id="S5.E10.m1.17.17.17.5.5.5.1.6.3.cmml" xref="S5.E10.m1.17.17.17.5.5.5.1.6.3"></times></apply></apply></apply><apply id="S5.E10.m1.42.42.1.1.1.2.2.1.1.1.1.2.cmml"><csymbol cd="latexml" id="S5.E10.m1.42.42.1.1.1.2.2.1.1.1.1.2.1.cmml">delimited-[]</csymbol><apply id="S5.E10.m1.42.42.1.1.1.2.2.1.1.1.1.1.1.cmml"><minus id="S5.E10.m1.19.19.19.7.7.7.cmml"></minus><apply id="S5.E10.m1.42.42.1.1.1.2.2.1.1.1.1.1.1.2.cmml"><log id="S5.E10.m1.20.20.20.8.8.8.cmml" xref="S5.E10.m1.20.20.20.8.8.8"></log><apply id="S5.E10.m1.22.22.22.10.10.10.cmml" xref="S5.E10.m1.22.22.22.10.10.10"><divide id="S5.E10.m1.22.22.22.10.10.10.7.cmml" xref="S5.E10.m1.22.22.22.10.10.10"></divide><apply id="S5.E10.m1.22.22.22.10.10.10.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.2"><csymbol cd="ambiguous" id="S5.E10.m1.22.22.22.10.10.10.2.3.cmml" xref="S5.E10.m1.22.22.22.10.10.10.2">superscript</csymbol><ci id="S5.E10.m1.22.22.22.10.10.10.2.4.cmml" xref="S5.E10.m1.22.22.22.10.10.10.2.4">ğ‘’</ci><apply id="S5.E10.m1.22.22.22.10.10.10.2.2.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2"><times id="S5.E10.m1.22.22.22.10.10.10.2.2.2.3.cmml" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.3"></times><ci id="S5.E10.m1.22.22.22.10.10.10.2.2.2.4.cmml" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.4">ğ‘ </ci><interval closure="open" id="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.1.cmml" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.5.2"><ci id="S5.E10.m1.22.22.22.10.10.10.1.1.1.1.cmml" xref="S5.E10.m1.22.22.22.10.10.10.1.1.1.1">ğ‘œ</ci><ci id="S5.E10.m1.22.22.22.10.10.10.2.2.2.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.2.2.2.2">ğ‘</ci></interval></apply></apply><apply id="S5.E10.m1.22.22.22.10.10.10.6.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6"><plus id="S5.E10.m1.22.22.22.10.10.10.6.5.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.5"></plus><apply id="S5.E10.m1.22.22.22.10.10.10.6.6.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.6"><csymbol cd="ambiguous" id="S5.E10.m1.22.22.22.10.10.10.6.6.1.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.6">superscript</csymbol><ci id="S5.E10.m1.22.22.22.10.10.10.6.6.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.6.2">ğ‘’</ci><apply id="S5.E10.m1.22.22.22.10.10.10.4.2.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2"><times id="S5.E10.m1.22.22.22.10.10.10.4.2.2.3.cmml" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.3"></times><ci id="S5.E10.m1.22.22.22.10.10.10.4.2.2.4.cmml" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.4">ğ‘ </ci><interval closure="open" id="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.1.cmml" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.5.2"><ci id="S5.E10.m1.22.22.22.10.10.10.3.1.1.1.cmml" xref="S5.E10.m1.22.22.22.10.10.10.3.1.1.1">ğ‘œ</ci><ci id="S5.E10.m1.22.22.22.10.10.10.4.2.2.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.4.2.2.2">ğ‘</ci></interval></apply></apply><apply id="S5.E10.m1.22.22.22.10.10.10.6.7.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.7"><csymbol cd="ambiguous" id="S5.E10.m1.22.22.22.10.10.10.6.7.1.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.7">superscript</csymbol><ci id="S5.E10.m1.22.22.22.10.10.10.6.7.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.7.2">ğ‘’</ci><apply id="S5.E10.m1.22.22.22.10.10.10.6.4.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2"><times id="S5.E10.m1.22.22.22.10.10.10.6.4.2.3.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.3"></times><ci id="S5.E10.m1.22.22.22.10.10.10.6.4.2.4.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.4">ğ‘ </ci><interval closure="open" id="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.1.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.5.2"><ci id="S5.E10.m1.22.22.22.10.10.10.5.3.1.1.cmml" xref="S5.E10.m1.22.22.22.10.10.10.5.3.1.1">ğ‘œ</ci><ci id="S5.E10.m1.22.22.22.10.10.10.6.4.2.2.cmml" xref="S5.E10.m1.22.22.22.10.10.10.6.4.2.2">ğ‘›</ci></interval></apply></apply></apply></apply></apply></apply></apply></apply></apply><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.cmml"><eq id="S5.E10.m1.28.28.28.3.3.3.cmml" xref="S5.E10.m1.28.28.28.3.3.3"></eq><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.3.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.2.2.2.2.3.1.cmml">subscript</csymbol><ci id="S5.E10.m1.26.26.26.1.1.1.cmml" xref="S5.E10.m1.26.26.26.1.1.1">â„’</ci><ci id="S5.E10.m1.27.27.27.2.2.2.1.cmml" xref="S5.E10.m1.27.27.27.2.2.2.1">V</ci></apply><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.cmml"><minus id="S5.E10.m1.29.29.29.4.4.4.cmml"></minus><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.cmml"><times id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.2.cmml"></times><apply id="S5.E10.m1.30.30.30.5.5.5.cmml" xref="S5.E10.m1.30.30.30.5.5.5"><divide id="S5.E10.m1.30.30.30.5.5.5.2.cmml" xref="S5.E10.m1.30.30.30.5.5.5"></divide><cn type="integer" id="S5.E10.m1.30.30.30.5.5.5.3.cmml" xref="S5.E10.m1.30.30.30.5.5.5.3">1</cn><apply id="S5.E10.m1.30.30.30.5.5.5.1.2.cmml" xref="S5.E10.m1.30.30.30.5.5.5.1.1"><abs id="S5.E10.m1.30.30.30.5.5.5.1.2.1.cmml" xref="S5.E10.m1.30.30.30.5.5.5.1.1.2"></abs><apply id="S5.E10.m1.30.30.30.5.5.5.1.1.1.cmml" xref="S5.E10.m1.30.30.30.5.5.5.1.1.1"><csymbol cd="ambiguous" id="S5.E10.m1.30.30.30.5.5.5.1.1.1.1.cmml" xref="S5.E10.m1.30.30.30.5.5.5.1.1.1">superscript</csymbol><ci id="S5.E10.m1.30.30.30.5.5.5.1.1.1.2.cmml" xref="S5.E10.m1.30.30.30.5.5.5.1.1.1.2">ğ’Ÿ</ci><times id="S5.E10.m1.30.30.30.5.5.5.1.1.1.3.cmml" xref="S5.E10.m1.30.30.30.5.5.5.1.1.1.3"></times></apply></apply></apply><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.cmml"><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.2.1.cmml">superscript</csymbol><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.2.2.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.2.2.1.cmml">subscript</csymbol><sum id="S5.E10.m1.31.31.31.6.6.6.cmml" xref="S5.E10.m1.31.31.31.6.6.6"></sum><apply id="S5.E10.m1.32.32.32.7.7.7.1.cmml" xref="S5.E10.m1.32.32.32.7.7.7.1"><eq id="S5.E10.m1.32.32.32.7.7.7.1.1.cmml" xref="S5.E10.m1.32.32.32.7.7.7.1.1"></eq><ci id="S5.E10.m1.32.32.32.7.7.7.1.2.cmml" xref="S5.E10.m1.32.32.32.7.7.7.1.2">ğ‘–</ci><cn type="integer" id="S5.E10.m1.32.32.32.7.7.7.1.3.cmml" xref="S5.E10.m1.32.32.32.7.7.7.1.3">1</cn></apply></apply><apply id="S5.E10.m1.33.33.33.8.8.8.1.2.cmml" xref="S5.E10.m1.33.33.33.8.8.8.1.1"><abs id="S5.E10.m1.33.33.33.8.8.8.1.2.1.cmml" xref="S5.E10.m1.33.33.33.8.8.8.1.1.2"></abs><apply id="S5.E10.m1.33.33.33.8.8.8.1.1.1.cmml" xref="S5.E10.m1.33.33.33.8.8.8.1.1.1"><csymbol cd="ambiguous" id="S5.E10.m1.33.33.33.8.8.8.1.1.1.1.cmml" xref="S5.E10.m1.33.33.33.8.8.8.1.1.1">superscript</csymbol><ci id="S5.E10.m1.33.33.33.8.8.8.1.1.1.2.cmml" xref="S5.E10.m1.33.33.33.8.8.8.1.1.1.2">ğ’Ÿ</ci><times id="S5.E10.m1.33.33.33.8.8.8.1.1.1.3.cmml" xref="S5.E10.m1.33.33.33.8.8.8.1.1.1.3"></times></apply></apply></apply><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.cmml"><times id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.2.cmml"></times><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.1.2.cmml"><csymbol cd="latexml" id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.1.2.1.cmml">delimited-[]</csymbol><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.1.1.1.1.cmml">subscript</csymbol><ci id="S5.E10.m1.35.35.35.10.10.10.cmml" xref="S5.E10.m1.35.35.35.10.10.10">ğ‘</ci><ci id="S5.E10.m1.36.36.36.11.11.11.1.cmml" xref="S5.E10.m1.36.36.36.11.11.11.1">ğ‘–</ci></apply></apply><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.3.cmml"><log id="S5.E10.m1.38.38.38.13.13.13.cmml" xref="S5.E10.m1.38.38.38.13.13.13"></log><apply id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.3.2.cmml"><csymbol cd="ambiguous" id="S5.E10.m1.42.42.1.1.1.2.2.2.2.1.1.1.1.3.2.1.cmml">subscript</csymbol><apply id="S5.E10.m1.39.39.39.14.14.14.cmml" xref="S5.E10.m1.39.39.39.14.14.14"><ci id="S5.E10.m1.39.39.39.14.14.14.1.cmml" xref="S5.E10.m1.39.39.39.14.14.14.1">^</ci><ci id="S5.E10.m1.39.39.39.14.14.14.2.cmml" xref="S5.E10.m1.39.39.39.14.14.14.2">ğ‘</ci></apply><ci id="S5.E10.m1.40.40.40.15.15.15.1.cmml" xref="S5.E10.m1.40.40.40.15.15.15.1">ğ‘–</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E10.m1.45c">\begin{split}&amp;\mathcal{L}=\lambda_{\mathrm{C}}\mathcal{L}_{\mathrm{C}}+\lambda_{\mathrm{V}}\mathcal{L}_{\mathrm{V}},\\
&amp;\mathcal{L}_{\mathrm{C}}=\mathop{\mathbb{E}}\limits_{o,p,n\in\mathcal{D}^{*}}\left[-\log\left(\frac{e^{s(o,p)}}{e^{s(o,p)}+e^{s(o,n)}}\right)\right],\\
&amp;\mathcal{L}_{\mathrm{V}}=-\frac{1}{|\mathcal{D}^{*}|}\sum_{i=1}^{|\mathcal{D}^{*}|}[a_{i}]\log\hat{a}_{i},\\
\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S5.SS3.p1.14" class="ltx_p">where <math id="S5.SS3.p1.5.m1.1" class="ltx_Math" alttext="\lambda_{\mathrm{C}}" display="inline"><semantics id="S5.SS3.p1.5.m1.1a"><msub id="S5.SS3.p1.5.m1.1.1" xref="S5.SS3.p1.5.m1.1.1.cmml"><mi id="S5.SS3.p1.5.m1.1.1.2" xref="S5.SS3.p1.5.m1.1.1.2.cmml">Î»</mi><mi mathvariant="normal" id="S5.SS3.p1.5.m1.1.1.3" xref="S5.SS3.p1.5.m1.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.5.m1.1b"><apply id="S5.SS3.p1.5.m1.1.1.cmml" xref="S5.SS3.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.5.m1.1.1.1.cmml" xref="S5.SS3.p1.5.m1.1.1">subscript</csymbol><ci id="S5.SS3.p1.5.m1.1.1.2.cmml" xref="S5.SS3.p1.5.m1.1.1.2">ğœ†</ci><ci id="S5.SS3.p1.5.m1.1.1.3.cmml" xref="S5.SS3.p1.5.m1.1.1.3">C</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m1.1c">\lambda_{\mathrm{C}}</annotation></semantics></math> and <math id="S5.SS3.p1.6.m2.1" class="ltx_Math" alttext="\lambda_{\mathrm{V}}" display="inline"><semantics id="S5.SS3.p1.6.m2.1a"><msub id="S5.SS3.p1.6.m2.1.1" xref="S5.SS3.p1.6.m2.1.1.cmml"><mi id="S5.SS3.p1.6.m2.1.1.2" xref="S5.SS3.p1.6.m2.1.1.2.cmml">Î»</mi><mi mathvariant="normal" id="S5.SS3.p1.6.m2.1.1.3" xref="S5.SS3.p1.6.m2.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.6.m2.1b"><apply id="S5.SS3.p1.6.m2.1.1.cmml" xref="S5.SS3.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.6.m2.1.1.1.cmml" xref="S5.SS3.p1.6.m2.1.1">subscript</csymbol><ci id="S5.SS3.p1.6.m2.1.1.2.cmml" xref="S5.SS3.p1.6.m2.1.1.2">ğœ†</ci><ci id="S5.SS3.p1.6.m2.1.1.3.cmml" xref="S5.SS3.p1.6.m2.1.1.3">V</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.6.m2.1c">\lambda_{\mathrm{V}}</annotation></semantics></math> are used to balance contrastive learning and VQA, <math id="S5.SS3.p1.7.m3.2" class="ltx_Math" alttext="s(o,p)" display="inline"><semantics id="S5.SS3.p1.7.m3.2a"><mrow id="S5.SS3.p1.7.m3.2.3" xref="S5.SS3.p1.7.m3.2.3.cmml"><mi id="S5.SS3.p1.7.m3.2.3.2" xref="S5.SS3.p1.7.m3.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.7.m3.2.3.1" xref="S5.SS3.p1.7.m3.2.3.1.cmml">â€‹</mo><mrow id="S5.SS3.p1.7.m3.2.3.3.2" xref="S5.SS3.p1.7.m3.2.3.3.1.cmml"><mo stretchy="false" id="S5.SS3.p1.7.m3.2.3.3.2.1" xref="S5.SS3.p1.7.m3.2.3.3.1.cmml">(</mo><mi id="S5.SS3.p1.7.m3.1.1" xref="S5.SS3.p1.7.m3.1.1.cmml">o</mi><mo id="S5.SS3.p1.7.m3.2.3.3.2.2" xref="S5.SS3.p1.7.m3.2.3.3.1.cmml">,</mo><mi id="S5.SS3.p1.7.m3.2.2" xref="S5.SS3.p1.7.m3.2.2.cmml">p</mi><mo stretchy="false" id="S5.SS3.p1.7.m3.2.3.3.2.3" xref="S5.SS3.p1.7.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.7.m3.2b"><apply id="S5.SS3.p1.7.m3.2.3.cmml" xref="S5.SS3.p1.7.m3.2.3"><times id="S5.SS3.p1.7.m3.2.3.1.cmml" xref="S5.SS3.p1.7.m3.2.3.1"></times><ci id="S5.SS3.p1.7.m3.2.3.2.cmml" xref="S5.SS3.p1.7.m3.2.3.2">ğ‘ </ci><interval closure="open" id="S5.SS3.p1.7.m3.2.3.3.1.cmml" xref="S5.SS3.p1.7.m3.2.3.3.2"><ci id="S5.SS3.p1.7.m3.1.1.cmml" xref="S5.SS3.p1.7.m3.1.1">ğ‘œ</ci><ci id="S5.SS3.p1.7.m3.2.2.cmml" xref="S5.SS3.p1.7.m3.2.2">ğ‘</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.7.m3.2c">s(o,p)</annotation></semantics></math> is the scoring function between the anchor <math id="S5.SS3.p1.8.m4.1" class="ltx_Math" alttext="o" display="inline"><semantics id="S5.SS3.p1.8.m4.1a"><mi id="S5.SS3.p1.8.m4.1.1" xref="S5.SS3.p1.8.m4.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.8.m4.1b"><ci id="S5.SS3.p1.8.m4.1.1.cmml" xref="S5.SS3.p1.8.m4.1.1">ğ‘œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.8.m4.1c">o</annotation></semantics></math> and the positive sample <math id="S5.SS3.p1.9.m5.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S5.SS3.p1.9.m5.1a"><mi id="S5.SS3.p1.9.m5.1.1" xref="S5.SS3.p1.9.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.9.m5.1b"><ci id="S5.SS3.p1.9.m5.1.1.cmml" xref="S5.SS3.p1.9.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.9.m5.1c">p</annotation></semantics></math>, <math id="S5.SS3.p1.10.m6.2" class="ltx_Math" alttext="s(o,n)" display="inline"><semantics id="S5.SS3.p1.10.m6.2a"><mrow id="S5.SS3.p1.10.m6.2.3" xref="S5.SS3.p1.10.m6.2.3.cmml"><mi id="S5.SS3.p1.10.m6.2.3.2" xref="S5.SS3.p1.10.m6.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p1.10.m6.2.3.1" xref="S5.SS3.p1.10.m6.2.3.1.cmml">â€‹</mo><mrow id="S5.SS3.p1.10.m6.2.3.3.2" xref="S5.SS3.p1.10.m6.2.3.3.1.cmml"><mo stretchy="false" id="S5.SS3.p1.10.m6.2.3.3.2.1" xref="S5.SS3.p1.10.m6.2.3.3.1.cmml">(</mo><mi id="S5.SS3.p1.10.m6.1.1" xref="S5.SS3.p1.10.m6.1.1.cmml">o</mi><mo id="S5.SS3.p1.10.m6.2.3.3.2.2" xref="S5.SS3.p1.10.m6.2.3.3.1.cmml">,</mo><mi id="S5.SS3.p1.10.m6.2.2" xref="S5.SS3.p1.10.m6.2.2.cmml">n</mi><mo stretchy="false" id="S5.SS3.p1.10.m6.2.3.3.2.3" xref="S5.SS3.p1.10.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.10.m6.2b"><apply id="S5.SS3.p1.10.m6.2.3.cmml" xref="S5.SS3.p1.10.m6.2.3"><times id="S5.SS3.p1.10.m6.2.3.1.cmml" xref="S5.SS3.p1.10.m6.2.3.1"></times><ci id="S5.SS3.p1.10.m6.2.3.2.cmml" xref="S5.SS3.p1.10.m6.2.3.2">ğ‘ </ci><interval closure="open" id="S5.SS3.p1.10.m6.2.3.3.1.cmml" xref="S5.SS3.p1.10.m6.2.3.3.2"><ci id="S5.SS3.p1.10.m6.1.1.cmml" xref="S5.SS3.p1.10.m6.1.1">ğ‘œ</ci><ci id="S5.SS3.p1.10.m6.2.2.cmml" xref="S5.SS3.p1.10.m6.2.2">ğ‘›</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.10.m6.2c">s(o,n)</annotation></semantics></math> is the scoring function between the anchor and the negative sample <math id="S5.SS3.p1.11.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS3.p1.11.m7.1a"><mi id="S5.SS3.p1.11.m7.1.1" xref="S5.SS3.p1.11.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.11.m7.1b"><ci id="S5.SS3.p1.11.m7.1.1.cmml" xref="S5.SS3.p1.11.m7.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.11.m7.1c">n</annotation></semantics></math>, <math id="S5.SS3.p1.12.m8.1" class="ltx_Math" alttext="|\mathcal{D}^{*}|" display="inline"><semantics id="S5.SS3.p1.12.m8.1a"><mrow id="S5.SS3.p1.12.m8.1.1.1" xref="S5.SS3.p1.12.m8.1.1.2.cmml"><mo stretchy="false" id="S5.SS3.p1.12.m8.1.1.1.2" xref="S5.SS3.p1.12.m8.1.1.2.1.cmml">|</mo><msup id="S5.SS3.p1.12.m8.1.1.1.1" xref="S5.SS3.p1.12.m8.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS3.p1.12.m8.1.1.1.1.2" xref="S5.SS3.p1.12.m8.1.1.1.1.2.cmml">ğ’Ÿ</mi><mo id="S5.SS3.p1.12.m8.1.1.1.1.3" xref="S5.SS3.p1.12.m8.1.1.1.1.3.cmml">âˆ—</mo></msup><mo stretchy="false" id="S5.SS3.p1.12.m8.1.1.1.3" xref="S5.SS3.p1.12.m8.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.12.m8.1b"><apply id="S5.SS3.p1.12.m8.1.1.2.cmml" xref="S5.SS3.p1.12.m8.1.1.1"><abs id="S5.SS3.p1.12.m8.1.1.2.1.cmml" xref="S5.SS3.p1.12.m8.1.1.1.2"></abs><apply id="S5.SS3.p1.12.m8.1.1.1.1.cmml" xref="S5.SS3.p1.12.m8.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.12.m8.1.1.1.1.1.cmml" xref="S5.SS3.p1.12.m8.1.1.1.1">superscript</csymbol><ci id="S5.SS3.p1.12.m8.1.1.1.1.2.cmml" xref="S5.SS3.p1.12.m8.1.1.1.1.2">ğ’Ÿ</ci><times id="S5.SS3.p1.12.m8.1.1.1.1.3.cmml" xref="S5.SS3.p1.12.m8.1.1.1.1.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.12.m8.1c">|\mathcal{D}^{*}|</annotation></semantics></math> denotes the number of samples in the augmented dataset such as the dataset described in Equation (<a href="#S5.E9" title="In 5.2 Data Augmentation â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>), and <math id="S5.SS3.p1.13.m9.1" class="ltx_Math" alttext="[a_{i}]" display="inline"><semantics id="S5.SS3.p1.13.m9.1a"><mrow id="S5.SS3.p1.13.m9.1.1.1" xref="S5.SS3.p1.13.m9.1.1.2.cmml"><mo stretchy="false" id="S5.SS3.p1.13.m9.1.1.1.2" xref="S5.SS3.p1.13.m9.1.1.2.1.cmml">[</mo><msub id="S5.SS3.p1.13.m9.1.1.1.1" xref="S5.SS3.p1.13.m9.1.1.1.1.cmml"><mi id="S5.SS3.p1.13.m9.1.1.1.1.2" xref="S5.SS3.p1.13.m9.1.1.1.1.2.cmml">a</mi><mi id="S5.SS3.p1.13.m9.1.1.1.1.3" xref="S5.SS3.p1.13.m9.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S5.SS3.p1.13.m9.1.1.1.3" xref="S5.SS3.p1.13.m9.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.13.m9.1b"><apply id="S5.SS3.p1.13.m9.1.1.2.cmml" xref="S5.SS3.p1.13.m9.1.1.1"><csymbol cd="latexml" id="S5.SS3.p1.13.m9.1.1.2.1.cmml" xref="S5.SS3.p1.13.m9.1.1.1.2">delimited-[]</csymbol><apply id="S5.SS3.p1.13.m9.1.1.1.1.cmml" xref="S5.SS3.p1.13.m9.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.13.m9.1.1.1.1.1.cmml" xref="S5.SS3.p1.13.m9.1.1.1.1">subscript</csymbol><ci id="S5.SS3.p1.13.m9.1.1.1.1.2.cmml" xref="S5.SS3.p1.13.m9.1.1.1.1.2">ğ‘</ci><ci id="S5.SS3.p1.13.m9.1.1.1.1.3.cmml" xref="S5.SS3.p1.13.m9.1.1.1.1.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.13.m9.1c">[a_{i}]</annotation></semantics></math> is the index of the answer <math id="S5.SS3.p1.14.m10.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S5.SS3.p1.14.m10.1a"><msub id="S5.SS3.p1.14.m10.1.1" xref="S5.SS3.p1.14.m10.1.1.cmml"><mi id="S5.SS3.p1.14.m10.1.1.2" xref="S5.SS3.p1.14.m10.1.1.2.cmml">a</mi><mi id="S5.SS3.p1.14.m10.1.1.3" xref="S5.SS3.p1.14.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.14.m10.1b"><apply id="S5.SS3.p1.14.m10.1.1.cmml" xref="S5.SS3.p1.14.m10.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.14.m10.1.1.1.cmml" xref="S5.SS3.p1.14.m10.1.1">subscript</csymbol><ci id="S5.SS3.p1.14.m10.1.1.2.cmml" xref="S5.SS3.p1.14.m10.1.1.2">ğ‘</ci><ci id="S5.SS3.p1.14.m10.1.1.3.cmml" xref="S5.SS3.p1.14.m10.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.14.m10.1c">a_{i}</annotation></semantics></math>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Specifically, CSS+CL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> is the first to introduce self-supervised contrastive learning for VQA counterfactual samples, where CSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> is leveraged to generate factual and counterfactual samples or positive and negative samples. Nevertheless, it has been found that CSS+CL results in a decline in ID performance while only slightly improving OOD performance. To this end, MMBS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> attributes the key point of solving language bias to the positive-sample design for excluding spurious correlations, which can boost the OOD performance significantly while retaining the ID performance. It exploits unbiased information through a positive sample construction strategy and employs an algorithm to discriminate between biased and unbiased samples so that they can be handled differently. These methods mitigate training bias from the forward chaining perspective which is similar to the paradigm described in Equation (<a href="#S2.E2" title="In 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), but they rarely explore it from the backward chaining perspective. <span id="S5.SS3.p2.1.1" class="ltx_text" style="color:#000000;">Motivated by this, Lao <span id="S5.SS3.p2.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> introduced a bidirectional chaining framework. In this framework, the forward chaining process resembles the procedure described in Equation (<a href="#S2.E2" title="In 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). On the other hand, the backward chaining process aims to generate crucial visual features by leveraging the annotated answer as a guiding mechanism. Nonetheless, the negative sample generation techniques employed by the aforementioned methods may inadvertently introduce visual shortcut bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>. To tackle this problem, LSP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite> introduces selective sampling rates and question-type-guided sampling, effectively eliminating the reliance of VQA models on visual shortcut bias. Furthermore, drawing inspiration from prompt learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>, LSP introduces a question-type-guided prompt within the language context, thereby enhancing the significance of questions within the VQA model.</span></p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span><span id="S5.SS4.1.1" class="ltx_text ltx_font_italic">Answer Re-Ranking</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.4" class="ltx_p">The answer re-ranking-based methods employ the re-ranking mechanism to re-sort the candidate answers provided by vanilla VQA baselines, which can guide the baseline to make better use of visual information. Specifically, their paradigm is to first predict answers using the vanilla method <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mi id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><ci id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">f</annotation></semantics></math> described in Equation (<a href="#S2" title="2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), and then re-rank answers leveraging the re-ranking module <math id="S5.SS4.p1.2.m2.1" class="ltx_Math" alttext="E_{\mathrm{r}}" display="inline"><semantics id="S5.SS4.p1.2.m2.1a"><msub id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mi id="S5.SS4.p1.2.m2.1.1.2" xref="S5.SS4.p1.2.m2.1.1.2.cmml">E</mi><mi mathvariant="normal" id="S5.SS4.p1.2.m2.1.1.3" xref="S5.SS4.p1.2.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.2.m2.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS4.p1.2.m2.1.1.2.cmml" xref="S5.SS4.p1.2.m2.1.1.2">ğ¸</ci><ci id="S5.SS4.p1.2.m2.1.1.3.cmml" xref="S5.SS4.p1.2.m2.1.1.3">r</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">E_{\mathrm{r}}</annotation></semantics></math>, and finally guide <math id="S5.SS4.p1.3.m3.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S5.SS4.p1.3.m3.1a"><mi id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><ci id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">f</annotation></semantics></math> to provide accurate answers <math id="S5.SS4.p1.4.m4.1" class="ltx_Math" alttext="\hat{a}_{i}" display="inline"><semantics id="S5.SS4.p1.4.m4.1a"><msub id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml"><mover accent="true" id="S5.SS4.p1.4.m4.1.1.2" xref="S5.SS4.p1.4.m4.1.1.2.cmml"><mi id="S5.SS4.p1.4.m4.1.1.2.2" xref="S5.SS4.p1.4.m4.1.1.2.2.cmml">a</mi><mo id="S5.SS4.p1.4.m4.1.1.2.1" xref="S5.SS4.p1.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="S5.SS4.p1.4.m4.1.1.3" xref="S5.SS4.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><apply id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.4.m4.1.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1">subscript</csymbol><apply id="S5.SS4.p1.4.m4.1.1.2.cmml" xref="S5.SS4.p1.4.m4.1.1.2"><ci id="S5.SS4.p1.4.m4.1.1.2.1.cmml" xref="S5.SS4.p1.4.m4.1.1.2.1">^</ci><ci id="S5.SS4.p1.4.m4.1.1.2.2.cmml" xref="S5.SS4.p1.4.m4.1.1.2.2">ğ‘</ci></apply><ci id="S5.SS4.p1.4.m4.1.1.3.cmml" xref="S5.SS4.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">\hat{a}_{i}</annotation></semantics></math> by back-propagating the gradient of re-ranking losses, as shown in Fig. <a href="#S5.F14" title="Figure 14 â€£ 5.4 Answer Re-Ranking â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. This process is formulated as follows:</p>
<table id="S5.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E11.m1.1" class="ltx_Math" alttext="\hat{a}_{i}=E_{\mathrm{r}}(E_{\mathrm{c}}(E_{\mathrm{m}}(E_{\mathrm{v}}(v_{i}),E_{\mathrm{q}}(q_{i}))))." display="block"><semantics id="S5.E11.m1.1a"><mrow id="S5.E11.m1.1.1.1" xref="S5.E11.m1.1.1.1.1.cmml"><mrow id="S5.E11.m1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.cmml"><msub id="S5.E11.m1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.3.cmml"><mover accent="true" id="S5.E11.m1.1.1.1.1.3.2" xref="S5.E11.m1.1.1.1.1.3.2.cmml"><mi id="S5.E11.m1.1.1.1.1.3.2.2" xref="S5.E11.m1.1.1.1.1.3.2.2.cmml">a</mi><mo id="S5.E11.m1.1.1.1.1.3.2.1" xref="S5.E11.m1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi id="S5.E11.m1.1.1.1.1.3.3" xref="S5.E11.m1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S5.E11.m1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.2.cmml">=</mo><mrow id="S5.E11.m1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.cmml"><msub id="S5.E11.m1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.3.cmml"><mi id="S5.E11.m1.1.1.1.1.1.3.2" xref="S5.E11.m1.1.1.1.1.1.3.2.cmml">E</mi><mi mathvariant="normal" id="S5.E11.m1.1.1.1.1.1.3.3" xref="S5.E11.m1.1.1.1.1.1.3.3.cmml">r</mi></msub><mo lspace="0em" rspace="0em" id="S5.E11.m1.1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S5.E11.m1.1.1.1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E11.m1.1.1.1.1.1.1.1.1.3.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.3.2.cmml">E</mi><mi mathvariant="normal" id="S5.E11.m1.1.1.1.1.1.1.1.1.3.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.3.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S5.E11.m1.1.1.1.1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.2.cmml">E</mi><mi mathvariant="normal" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.3.cmml">m</mi></msub><mo lspace="0em" rspace="0em" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">â€‹</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">E</mi><mi mathvariant="normal" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.4" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><msub id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">E</mi><mi mathvariant="normal" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">q</mi></msub><mo lspace="0em" rspace="0em" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">â€‹</mo><mrow id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml">(</mo><msub id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.2" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.2.cmml">q</mi><mi id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.5" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S5.E11.m1.1.1.1.1.1.1.1.3" xref="S5.E11.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S5.E11.m1.1.1.1.2" xref="S5.E11.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E11.m1.1b"><apply id="S5.E11.m1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1"><eq id="S5.E11.m1.1.1.1.1.2.cmml" xref="S5.E11.m1.1.1.1.1.2"></eq><apply id="S5.E11.m1.1.1.1.1.3.cmml" xref="S5.E11.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E11.m1.1.1.1.1.3.1.cmml" xref="S5.E11.m1.1.1.1.1.3">subscript</csymbol><apply id="S5.E11.m1.1.1.1.1.3.2.cmml" xref="S5.E11.m1.1.1.1.1.3.2"><ci id="S5.E11.m1.1.1.1.1.3.2.1.cmml" xref="S5.E11.m1.1.1.1.1.3.2.1">^</ci><ci id="S5.E11.m1.1.1.1.1.3.2.2.cmml" xref="S5.E11.m1.1.1.1.1.3.2.2">ğ‘</ci></apply><ci id="S5.E11.m1.1.1.1.1.3.3.cmml" xref="S5.E11.m1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S5.E11.m1.1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1"><times id="S5.E11.m1.1.1.1.1.1.2.cmml" xref="S5.E11.m1.1.1.1.1.1.2"></times><apply id="S5.E11.m1.1.1.1.1.1.3.cmml" xref="S5.E11.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E11.m1.1.1.1.1.1.3.1.cmml" xref="S5.E11.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E11.m1.1.1.1.1.1.3.2.cmml" xref="S5.E11.m1.1.1.1.1.1.3.2">ğ¸</ci><ci id="S5.E11.m1.1.1.1.1.1.3.3.cmml" xref="S5.E11.m1.1.1.1.1.1.3.3">r</ci></apply><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1"><times id="S5.E11.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.2"></times><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E11.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.3.2">ğ¸</ci><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.3.3">c</ci></apply><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1"><times id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.3"></times><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.2">ğ¸</ci><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.4.3">m</ci></apply><interval closure="open" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2"><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¸</ci><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">v</ci></apply><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><times id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2"></times><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2">ğ¸</ci><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3">q</ci></apply><apply id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1">subscript</csymbol><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.2">ğ‘</ci><ci id="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.3.cmml" xref="S5.E11.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.1.1.3">ğ‘–</ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E11.m1.1c">\hat{a}_{i}=E_{\mathrm{r}}(E_{\mathrm{c}}(E_{\mathrm{m}}(E_{\mathrm{v}}(v_{i}),E_{\mathrm{q}}(q_{i})))).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Specifically, <span id="S5.SS4.p2.1.1" class="ltx_text" style="color:#000000;">GVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> decouples the recognition of visual objects in an image from the identification of plausible answer space for a given question. This is accomplished through the use of a question classifier, which determines the type of questions to reduce the size of answer space, as well as an answer cluster predictor that identifies the expected types of answers, such as object names, colors, and numbers.</span> SCR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> and HINT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> employ a human attention-based penalty mechanism to guide the answer ranking. For each question-answer pair, SCR first determines the region of an image that has the greatest influence on the network prediction of the correct answer. Then, it penalizes the network for concentrating on the region once the prediction is wrong. Similarly, HINT penalizes the model if the pair-wise ranking of visual region sensitivities in relation to ground-truth answers does not match the rankings calculated from human-based attention maps. These two methods made significant progress using the human attention-based penalty mechanism, but Shrestha <span id="S5.SS4.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> demonstrated that the performance improvements seen in these methods are due to a regularization effect that prevents overfitting to linguistic priors rather than enhanced visual grounding. Inspired by this, SimpleReg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> employs a simple regularization scheme that consistently penalizes the model regardless of whether its predictions are accurate or not.</p>
</div>
<figure id="S5.F14" class="ltx_figure"><img src="/html/2307.11471/assets/x14.png" id="S5.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Overview of answer re-ranking-based debiasing methods. The re-ranking module re-ranks the answers predicted by the vanilla VQA model.</figcaption>
</figure>
<figure id="S5.F15" class="ltx_figure"><img src="/html/2307.11471/assets/x15.png" id="S5.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Classification of VLP models inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>. The height of each rectangle represents its relative computational size. IE, TE, and MI are short for image encoder, text encoder, and modality interaction, respectively. In this paper, we consider the embedding layer as the component of encoders.</figcaption>
</figure>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Different from the mentioned methods, RankVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> and SAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> explore the combination of answer re-ranking and multimodality tasks to reduce bias learning. They first select candidate answers relevant to the question or the image, and then re-rank the answers by an image caption task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite> or a visual entailment task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite>, motivated by the idea that the correct answer must be more pertinent to the context of the image than the incorrect answer. These tasks play an important role in verifying whether the image semantically entails or matches the synthetic statement of questions and candidate answers.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">The aforementioned methods mainly focus on strengthening the visual feature learning capability but ignore analyzing its inherent cause and providing an explicit interpretation. Therefore, some works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> suggested taking a look at the robust VQA problem from a class-imbalance perspective. They further demonstrated the effectiveness of the loss re-scaling strategy, which assigns various weights to each answer based on the statistics of the training data to estimate the final loss. <span id="S5.SS4.p4.1.1" class="ltx_text" style="color:#000000;">For example, LPF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> leverages a dynamic weighting scheme to each training example and adjusts the VQA loss according to the output distribution of a question-only branch. In addition, there are several works addressing bias learning from the perspective of decision margins. AdVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>, for example, employs an adapted margin loss function to distinguish between frequent and sparse answer spaces for each question type. RMLVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> employs an instance-specific adaptive margin loss function to distinguish between hard and easy examples, as well as frequent and rare ones. MFE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> employs a regularization term that relies on a functional entropy, to ensure a balanced contribution of each modality towards classification.</span></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Vision-and-Language Pre-training Methods</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The majority of approaches outlined previously are task-specific models that are trained on a restricted amount of data. However, there are other avenues worth exploring, such as task-agnostic models that are trained on vast amounts of data. Notably, in recent years, Vision-and-Language Pre-training (VLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>, <a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib165" title="" class="ltx_ref">165</a>, <a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>, <a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>, <a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite> has seen significant advancements in the realm of multimodal representation learning through the use of large-scale image-text pairs. VLP models typically adopt Image-Text Matching (ITM) and Masked Language Modeling (MLM) objectives on images and their corresponding captions during pre-training and then are fine-tuned on downstream tasks such as VQA.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">It is well-known that VLP models are usually more robust than the models trained on limited data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib172" title="" class="ltx_ref">172</a>, <a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite>. Typically, VLP models consist of a text encoder, an image encoder, and a modality interaction module. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>, we classify current VLP models into four categories based on the relative computational size of text encoders, image encoders, and modality interaction modules, as illustrated in Fig. <a href="#S5.F15" title="Figure 15 â€£ 5.4 Answer Re-Ranking â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. In this paper, we regard the embedding layer as the component of encoders. As a consensus, images usually contain more rich information than text, so intuitively, the encoder on the visual end should be larger than the encoder on the textual end. Furthermore, as a multimodal task, the interaction between modalities in VLP is also important, and the computational cost of the modality interaction module should not be sacrificed. Therefore, models of the third type empirically achieve worse performance than those of the other types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">During the initial phase of VLP, early methods such as LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, UNITER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, and OSCAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite> employed object detectors like Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite> and YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>, which were pre-trained on the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, as the visual encoder to extract region/object-level features from images. Therefore, the computational size of the image encoder exceeds that of the text encoder. Additionally, these methods utilize a deep Transformer to facilitate modality interactions, thus categorizing them into the first type, as illustrated in Fig. <a href="#S5.F15" title="Figure 15 â€£ 5.4 Answer Re-Ranking â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> (a). Although they demonstrate promising performance on downstream tasks, the use of heavy object detectors results in considerable computational expenses, and the inherently separate visual and textual embeddings pose a challenge for the modality interaction module to learn unified representations.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">With the success of Transformer from natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>, <a href="#bib.bib177" title="" class="ltx_ref">177</a>, <a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> to computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib180" title="" class="ltx_ref">180</a>, <a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>, researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>, <a href="#bib.bib183" title="" class="ltx_ref">183</a>, <a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite> paid attention to its use in VLP to address the separateness of visual and textual feature learning. For instance, inspired by the use of patches in ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite>, ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite> employs a light visual encoder where images are first split into patches and then embedded simply using a linear projection layer. These image features are then concatenated with word embeddings and fed into a deep Transformer for the modality interaction. The computational size of the text encoder is nearly equivalent to that of the image encoder, thus placing them within the second type, as illustrated in Fig. <a href="#S5.F15" title="Figure 15 â€£ 5.4 Answer Re-Ranking â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> (b). Compared to the VLP model with an object detector, ViLTâ€™s convolution-free architecture significantly reduces model size and inference time, making it tens of times faster. However, the lightweight visual encoder of ViLT leads to a performance decrease on downstream tasks requiring salient visual features.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Meanwhile, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite> has shown great potential for training visual models using natural language supervision. This is achieved through an Image-Text Contrastive (ITC) objective, which determines whether an image and its corresponding caption are paired or not. With pre-training on a massive dataset of 400 million image-text pairs and the use of prompt templates, CLIP has achieved impressive results on various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>, <a href="#bib.bib186" title="" class="ltx_ref">186</a>, <a href="#bib.bib187" title="" class="ltx_ref">187</a>, <a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite>. The modality interaction module only computes the ITC loss, making it classified as the third type, as shown in Fig. <a href="#S5.F15" title="Figure 15 â€£ 5.4 Answer Re-Ranking â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> (c). Although CLIP performs well on tasks such as image-text retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>, it demonstrates limitations in handling other important VLP tasks that involve finer-grained and more abstract concepts, such as fine-grained object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>, <a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite> and object counting in an image. This underscores the significance of the modality interaction module in achieving better performance on challenging VLP tasks.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">Inspired by CLIP, some recent VLP works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>, <a href="#bib.bib193" title="" class="ltx_ref">193</a>, <a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> explore pre-training based on contrastive learning. For instance, ALBEF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite> employs a 12-layer ViT and the first six layers of BERT as the image and text encoder, respectively. The representations learned by the mentioned encoder are aligned using the ITC loss. Following this alignment, the last six layers of BERT are employed as the multimodal encoder. Ultimately, the model is trained by jointly optimizing the ITC, ITM, and MLM losses. To learn from noisy data, ALBEF utilizes a momentum model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> to generate pseudo-targets as additional supervision during training. ALBEF is a typical first-type model, which ensures that the image encoder is larger than the text encoder while also guaranteeing the expressiveness of the modality interaction module. Moreover, the abandonment of the pre-trained object detector and adoption of ViT with the ITC loss empowers ALBEF to acquire semantic visual representations of high quality, while maintaining computational efficiency.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Results of VLP models on ID and OOD situations. The symbol PT indicates whether a method has been pre-trained on large-scale datasets. Results marked with <math id="S6.T3.4.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.4.m1.1b"><mo id="S6.T3.4.m1.1.1" xref="S6.T3.4.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.4.m1.1c"><ci id="S6.T3.4.m1.1.1.cmml" xref="S6.T3.4.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.m1.1d">\dagger</annotation></semantics></math>, <math id="S6.T3.5.m2.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="S6.T3.5.m2.1b"><mo id="S6.T3.5.m2.1.1" xref="S6.T3.5.m2.1.1.cmml">â€¡</mo><annotation-xml encoding="MathML-Content" id="S6.T3.5.m2.1c"><ci id="S6.T3.5.m2.1.1.cmml" xref="S6.T3.5.m2.1.1">â€¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.5.m2.1d">\ddagger</annotation></semantics></math>, <math id="S6.T3.6.m3.1" class="ltx_Math" alttext="\Diamond" display="inline"><semantics id="S6.T3.6.m3.1b"><mi mathvariant="normal" id="S6.T3.6.m3.1.1" xref="S6.T3.6.m3.1.1.cmml">â—‡</mi><annotation-xml encoding="MathML-Content" id="S6.T3.6.m3.1c"><ci id="S6.T3.6.m3.1.1.cmml" xref="S6.T3.6.m3.1.1">â—‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.6.m3.1d">\Diamond</annotation></semantics></math> are reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>, respectively. â€œDISCâ€ denotes that methods regard VQA as a discriminative task, while â€œGENâ€ represents that methods consider VQA as a generative task. â€œTypeâ€ denotes the VLP class shown in Fig. <a href="#S5.F15" title="Figure 15 â€£ 5.4 Answer Re-Ranking â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>.</figcaption>
<div id="S6.T3.17" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:428pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(4.4pt,-4.3pt) scale(1.02054747391721,1.02054747391721) ;">
<table id="S6.T3.17.11" class="ltx_tabular ltx_align_middle">
<tr id="S6.T3.17.11.12" class="ltx_tr">
<td id="S6.T3.17.11.12.1" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T3.17.11.12.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S6.T3.17.11.12.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T3.17.11.12.2.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="S6.T3.17.11.12.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T3.17.11.12.3.1" class="ltx_text ltx_font_bold">PT</span></td>
<td id="S6.T3.17.11.12.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T3.17.11.12.4.1" class="ltx_text"></span> <span id="S6.T3.17.11.12.4.2" class="ltx_text">
<span id="S6.T3.17.11.12.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T3.17.11.12.4.2.1.1" class="ltx_tr">
<span id="S6.T3.17.11.12.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T3.17.11.12.4.2.1.1.1.1" class="ltx_text ltx_font_bold">VQA-CP v2</span></span></span>
<span id="S6.T3.17.11.12.4.2.1.2" class="ltx_tr">
<span id="S6.T3.17.11.12.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T3.17.11.12.4.2.1.2.1.1" class="ltx_text ltx_font_bold">test</span></span></span>
</span></span><span id="S6.T3.17.11.12.4.3" class="ltx_text"></span></td>
<td id="S6.T3.17.11.12.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T3.17.11.12.5.1" class="ltx_text"></span> <span id="S6.T3.17.11.12.5.2" class="ltx_text">
<span id="S6.T3.17.11.12.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T3.17.11.12.5.2.1.1" class="ltx_tr">
<span id="S6.T3.17.11.12.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T3.17.11.12.5.2.1.1.1.1" class="ltx_text ltx_font_bold">VQA v2</span></span></span>
<span id="S6.T3.17.11.12.5.2.1.2" class="ltx_tr">
<span id="S6.T3.17.11.12.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T3.17.11.12.5.2.1.2.1.1" class="ltx_text ltx_font_bold">validation</span></span></span>
</span></span><span id="S6.T3.17.11.12.5.3" class="ltx_text"></span></td>
<td id="S6.T3.17.11.12.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T3.17.11.12.6.1" class="ltx_text ltx_font_bold">HM</span></td>
</tr>
<tr id="S6.T3.17.11.13" class="ltx_tr">
<td id="S6.T3.17.11.13.1" class="ltx_td ltx_align_right ltx_border_t">UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S6.T3.17.11.13.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T3.17.11.13.3" class="ltx_td ltx_align_center ltx_border_t">No</td>
<td id="S6.T3.17.11.13.4" class="ltx_td ltx_align_center ltx_border_t">37.81</td>
<td id="S6.T3.17.11.13.5" class="ltx_td ltx_align_center ltx_border_t">65.99</td>
<td id="S6.T3.17.11.13.6" class="ltx_td ltx_align_center ltx_border_t">48.07</td>
</tr>
<tr id="S6.T3.7.1.1" class="ltx_tr">
<td id="S6.T3.7.1.1.1" class="ltx_td ltx_align_right">VILBERT<sub id="S6.T3.7.1.1.1.2" class="ltx_sub">DISC</sub><sup id="S6.T3.7.1.1.1.1" class="ltx_sup"><math id="S6.T3.7.1.1.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.7.1.1.1.1.m1.1a"><mo id="S6.T3.7.1.1.1.1.m1.1.1" xref="S6.T3.7.1.1.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.7.1.1.1.1.m1.1b"><ci id="S6.T3.7.1.1.1.1.m1.1.1.cmml" xref="S6.T3.7.1.1.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.7.1.1.1.1.m1.1c">\dagger</annotation></semantics></math></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>
</td>
<td id="S6.T3.7.1.1.2" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.T3.7.1.1.3" class="ltx_td ltx_align_center">No</td>
<td id="S6.T3.7.1.1.4" class="ltx_td ltx_align_center">42.50</td>
<td id="S6.T3.7.1.1.5" class="ltx_td ltx_align_center">66.70</td>
<td id="S6.T3.7.1.1.6" class="ltx_td ltx_align_center">51.92</td>
</tr>
<tr id="S6.T3.8.2.2" class="ltx_tr">
<td id="S6.T3.8.2.2.1" class="ltx_td ltx_align_right">VILBERT<sub id="S6.T3.8.2.2.1.2" class="ltx_sub">DISC</sub><sup id="S6.T3.8.2.2.1.1" class="ltx_sup"><math id="S6.T3.8.2.2.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.8.2.2.1.1.m1.1a"><mo id="S6.T3.8.2.2.1.1.m1.1.1" xref="S6.T3.8.2.2.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.8.2.2.1.1.m1.1b"><ci id="S6.T3.8.2.2.1.1.m1.1.1.cmml" xref="S6.T3.8.2.2.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.8.2.2.1.1.m1.1c">\dagger</annotation></semantics></math></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite>
</td>
<td id="S6.T3.8.2.2.2" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.T3.8.2.2.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.8.2.2.4" class="ltx_td ltx_align_center">42.90</td>
<td id="S6.T3.8.2.2.5" class="ltx_td ltx_align_center">67.00</td>
<td id="S6.T3.8.2.2.6" class="ltx_td ltx_align_center">52.31</td>
</tr>
<tr id="S6.T3.9.3.3" class="ltx_tr">
<td id="S6.T3.9.3.3.1" class="ltx_td ltx_align_right ltx_border_t">ALBEF<sub id="S6.T3.9.3.3.1.2" class="ltx_sub">DISC</sub><sup id="S6.T3.9.3.3.1.1" class="ltx_sup"><math id="S6.T3.9.3.3.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.9.3.3.1.1.m1.1a"><mo id="S6.T3.9.3.3.1.1.m1.1.1" xref="S6.T3.9.3.3.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.9.3.3.1.1.m1.1b"><ci id="S6.T3.9.3.3.1.1.m1.1.1.cmml" xref="S6.T3.9.3.3.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.9.3.3.1.1.m1.1c">\dagger</annotation></semantics></math></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>
</td>
<td id="S6.T3.9.3.3.2" class="ltx_td ltx_align_center ltx_border_t">(a)</td>
<td id="S6.T3.9.3.3.3" class="ltx_td ltx_align_center ltx_border_t">No</td>
<td id="S6.T3.9.3.3.4" class="ltx_td ltx_align_center ltx_border_t">40.10</td>
<td id="S6.T3.9.3.3.5" class="ltx_td ltx_align_center ltx_border_t">64.00</td>
<td id="S6.T3.9.3.3.6" class="ltx_td ltx_align_center ltx_border_t">49.31</td>
</tr>
<tr id="S6.T3.10.4.4" class="ltx_tr">
<td id="S6.T3.10.4.4.1" class="ltx_td ltx_align_right">ALBEF<sub id="S6.T3.10.4.4.1.2" class="ltx_sub">DISC</sub><sup id="S6.T3.10.4.4.1.1" class="ltx_sup"><math id="S6.T3.10.4.4.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.10.4.4.1.1.m1.1a"><mo id="S6.T3.10.4.4.1.1.m1.1.1" xref="S6.T3.10.4.4.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.10.4.4.1.1.m1.1b"><ci id="S6.T3.10.4.4.1.1.m1.1.1.cmml" xref="S6.T3.10.4.4.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.10.4.4.1.1.m1.1c">\dagger</annotation></semantics></math></sup> (4M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>
</td>
<td id="S6.T3.10.4.4.2" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.T3.10.4.4.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.10.4.4.4" class="ltx_td ltx_align_center">44.40</td>
<td id="S6.T3.10.4.4.5" class="ltx_td ltx_align_center">70.00</td>
<td id="S6.T3.10.4.4.6" class="ltx_td ltx_align_center">54.34</td>
</tr>
<tr id="S6.T3.11.5.5" class="ltx_tr">
<td id="S6.T3.11.5.5.1" class="ltx_td ltx_align_right">ALBEF<sub id="S6.T3.11.5.5.1.2" class="ltx_sub">DISC</sub><sup id="S6.T3.11.5.5.1.1" class="ltx_sup"><math id="S6.T3.11.5.5.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.11.5.5.1.1.m1.1a"><mo id="S6.T3.11.5.5.1.1.m1.1.1" xref="S6.T3.11.5.5.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.11.5.5.1.1.m1.1b"><ci id="S6.T3.11.5.5.1.1.m1.1.1.cmml" xref="S6.T3.11.5.5.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.11.5.5.1.1.m1.1c">\dagger</annotation></semantics></math></sup> (14M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>
</td>
<td id="S6.T3.11.5.5.2" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.T3.11.5.5.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.11.5.5.4" class="ltx_td ltx_align_center">45.20</td>
<td id="S6.T3.11.5.5.5" class="ltx_td ltx_align_center">70.30</td>
<td id="S6.T3.11.5.5.6" class="ltx_td ltx_align_center">55.02</td>
</tr>
<tr id="S6.T3.12.6.6" class="ltx_tr">
<td id="S6.T3.12.6.6.1" class="ltx_td ltx_align_right ltx_border_t">ALBEF<sub id="S6.T3.12.6.6.1.2" class="ltx_sub">GEN</sub><sup id="S6.T3.12.6.6.1.1" class="ltx_sup"><math id="S6.T3.12.6.6.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.12.6.6.1.1.m1.1a"><mo id="S6.T3.12.6.6.1.1.m1.1.1" xref="S6.T3.12.6.6.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.12.6.6.1.1.m1.1b"><ci id="S6.T3.12.6.6.1.1.m1.1.1.cmml" xref="S6.T3.12.6.6.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.12.6.6.1.1.m1.1c">\dagger</annotation></semantics></math></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>
</td>
<td id="S6.T3.12.6.6.2" class="ltx_td ltx_align_center ltx_border_t">(a)</td>
<td id="S6.T3.12.6.6.3" class="ltx_td ltx_align_center ltx_border_t">No</td>
<td id="S6.T3.12.6.6.4" class="ltx_td ltx_align_center ltx_border_t">36.60</td>
<td id="S6.T3.12.6.6.5" class="ltx_td ltx_align_center ltx_border_t">61.40</td>
<td id="S6.T3.12.6.6.6" class="ltx_td ltx_align_center ltx_border_t">45.86</td>
</tr>
<tr id="S6.T3.13.7.7" class="ltx_tr">
<td id="S6.T3.13.7.7.1" class="ltx_td ltx_align_right">ALBEF<sub id="S6.T3.13.7.7.1.2" class="ltx_sub">GEN</sub><sup id="S6.T3.13.7.7.1.1" class="ltx_sup"><math id="S6.T3.13.7.7.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.13.7.7.1.1.m1.1a"><mo id="S6.T3.13.7.7.1.1.m1.1.1" xref="S6.T3.13.7.7.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.13.7.7.1.1.m1.1b"><ci id="S6.T3.13.7.7.1.1.m1.1.1.cmml" xref="S6.T3.13.7.7.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.13.7.7.1.1.m1.1c">\dagger</annotation></semantics></math></sup> (4M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>
</td>
<td id="S6.T3.13.7.7.2" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.T3.13.7.7.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.13.7.7.4" class="ltx_td ltx_align_center">49.20</td>
<td id="S6.T3.13.7.7.5" class="ltx_td ltx_align_center">71.00</td>
<td id="S6.T3.13.7.7.6" class="ltx_td ltx_align_center">58.12</td>
</tr>
<tr id="S6.T3.14.8.8" class="ltx_tr">
<td id="S6.T3.14.8.8.1" class="ltx_td ltx_align_right">ALBEF<sub id="S6.T3.14.8.8.1.2" class="ltx_sub">GEN</sub><sup id="S6.T3.14.8.8.1.1" class="ltx_sup"><math id="S6.T3.14.8.8.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S6.T3.14.8.8.1.1.m1.1a"><mo id="S6.T3.14.8.8.1.1.m1.1.1" xref="S6.T3.14.8.8.1.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S6.T3.14.8.8.1.1.m1.1b"><ci id="S6.T3.14.8.8.1.1.m1.1.1.cmml" xref="S6.T3.14.8.8.1.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.14.8.8.1.1.m1.1c">\dagger</annotation></semantics></math></sup> (14M) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>
</td>
<td id="S6.T3.14.8.8.2" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.T3.14.8.8.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.14.8.8.4" class="ltx_td ltx_align_center">49.60</td>
<td id="S6.T3.14.8.8.5" class="ltx_td ltx_align_center">72.10</td>
<td id="S6.T3.14.8.8.6" class="ltx_td ltx_align_center">58.77</td>
</tr>
<tr id="S6.T3.15.9.9" class="ltx_tr">
<td id="S6.T3.15.9.9.1" class="ltx_td ltx_align_right ltx_border_t">UNITER<sub id="S6.T3.15.9.9.1.2" class="ltx_sub">Base</sub><sup id="S6.T3.15.9.9.1.1" class="ltx_sup"><math id="S6.T3.15.9.9.1.1.m1.1" class="ltx_Math" alttext="\Diamond" display="inline"><semantics id="S6.T3.15.9.9.1.1.m1.1a"><mi mathvariant="normal" id="S6.T3.15.9.9.1.1.m1.1.1" xref="S6.T3.15.9.9.1.1.m1.1.1.cmml">â—‡</mi><annotation-xml encoding="MathML-Content" id="S6.T3.15.9.9.1.1.m1.1b"><ci id="S6.T3.15.9.9.1.1.m1.1.1.cmml" xref="S6.T3.15.9.9.1.1.m1.1.1">â—‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.15.9.9.1.1.m1.1c">\Diamond</annotation></semantics></math></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S6.T3.15.9.9.2" class="ltx_td ltx_align_center ltx_border_t">(d)</td>
<td id="S6.T3.15.9.9.3" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S6.T3.15.9.9.4" class="ltx_td ltx_align_center ltx_border_t">46.93</td>
<td id="S6.T3.15.9.9.5" class="ltx_td ltx_align_center ltx_border_t">72.70</td>
<td id="S6.T3.15.9.9.6" class="ltx_td ltx_align_center ltx_border_t">57.04</td>
</tr>
<tr id="S6.T3.16.10.10" class="ltx_tr">
<td id="S6.T3.16.10.10.1" class="ltx_td ltx_align_right">UNITER<sub id="S6.T3.16.10.10.1.2" class="ltx_sub">Large</sub><sup id="S6.T3.16.10.10.1.1" class="ltx_sup"><math id="S6.T3.16.10.10.1.1.m1.1" class="ltx_Math" alttext="\Diamond" display="inline"><semantics id="S6.T3.16.10.10.1.1.m1.1a"><mi mathvariant="normal" id="S6.T3.16.10.10.1.1.m1.1.1" xref="S6.T3.16.10.10.1.1.m1.1.1.cmml">â—‡</mi><annotation-xml encoding="MathML-Content" id="S6.T3.16.10.10.1.1.m1.1b"><ci id="S6.T3.16.10.10.1.1.m1.1.1.cmml" xref="S6.T3.16.10.10.1.1.m1.1.1">â—‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.16.10.10.1.1.m1.1c">\Diamond</annotation></semantics></math></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S6.T3.16.10.10.2" class="ltx_td ltx_align_center">(d)</td>
<td id="S6.T3.16.10.10.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.16.10.10.4" class="ltx_td ltx_align_center">50.98</td>
<td id="S6.T3.16.10.10.5" class="ltx_td ltx_align_center">73.82</td>
<td id="S6.T3.16.10.10.6" class="ltx_td ltx_align_center">60.31</td>
</tr>
<tr id="S6.T3.17.11.11" class="ltx_tr">
<td id="S6.T3.17.11.11.1" class="ltx_td ltx_align_right ltx_border_t">LXMERT<sup id="S6.T3.17.11.11.1.1" class="ltx_sup"><math id="S6.T3.17.11.11.1.1.m1.1" class="ltx_Math" alttext="\ddagger" display="inline"><semantics id="S6.T3.17.11.11.1.1.m1.1a"><mo id="S6.T3.17.11.11.1.1.m1.1.1" xref="S6.T3.17.11.11.1.1.m1.1.1.cmml">â€¡</mo><annotation-xml encoding="MathML-Content" id="S6.T3.17.11.11.1.1.m1.1b"><ci id="S6.T3.17.11.11.1.1.m1.1.1.cmml" xref="S6.T3.17.11.11.1.1.m1.1.1">â€¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.17.11.11.1.1.m1.1c">\ddagger</annotation></semantics></math></sup> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S6.T3.17.11.11.2" class="ltx_td ltx_align_center ltx_border_t">(d)</td>
<td id="S6.T3.17.11.11.3" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S6.T3.17.11.11.4" class="ltx_td ltx_align_center ltx_border_t">51.78</td>
<td id="S6.T3.17.11.11.5" class="ltx_td ltx_align_center ltx_border_t">73.06</td>
<td id="S6.T3.17.11.11.6" class="ltx_td ltx_align_center ltx_border_t">60.61</td>
</tr>
<tr id="S6.T3.17.11.14" class="ltx_tr">
<td id="S6.T3.17.11.14.1" class="ltx_td ltx_align_right">ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>
</td>
<td id="S6.T3.17.11.14.2" class="ltx_td ltx_align_center">(b)</td>
<td id="S6.T3.17.11.14.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.17.11.14.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T3.17.11.14.5" class="ltx_td ltx_align_center">71.26</td>
<td id="S6.T3.17.11.14.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T3.17.11.15" class="ltx_tr">
<td id="S6.T3.17.11.15.1" class="ltx_td ltx_align_right">OSCAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite>
</td>
<td id="S6.T3.17.11.15.2" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.T3.17.11.15.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.17.11.15.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T3.17.11.15.5" class="ltx_td ltx_align_center">73.61</td>
<td id="S6.T3.17.11.15.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T3.17.11.16" class="ltx_tr">
<td id="S6.T3.17.11.16.1" class="ltx_td ltx_align_right">CLIP-ViL<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite>
</td>
<td id="S6.T3.17.11.16.2" class="ltx_td ltx_align_center">(c)</td>
<td id="S6.T3.17.11.16.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.17.11.16.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T3.17.11.16.5" class="ltx_td ltx_align_center">76.48</td>
<td id="S6.T3.17.11.16.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T3.17.11.17" class="ltx_tr">
<td id="S6.T3.17.11.17.1" class="ltx_td ltx_align_right">BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>
</td>
<td id="S6.T3.17.11.17.2" class="ltx_td ltx_align_center">(d)</td>
<td id="S6.T3.17.11.17.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.17.11.17.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T3.17.11.17.5" class="ltx_td ltx_align_center">78.25</td>
<td id="S6.T3.17.11.17.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T3.17.11.18" class="ltx_tr">
<td id="S6.T3.17.11.18.1" class="ltx_td ltx_align_right">BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite>
</td>
<td id="S6.T3.17.11.18.2" class="ltx_td ltx_align_center">(d)</td>
<td id="S6.T3.17.11.18.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.17.11.18.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T3.17.11.18.5" class="ltx_td ltx_align_center">82.19</td>
<td id="S6.T3.17.11.18.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T3.17.11.19" class="ltx_tr">
<td id="S6.T3.17.11.19.1" class="ltx_td ltx_align_right">CoCa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>
</td>
<td id="S6.T3.17.11.19.2" class="ltx_td ltx_align_center">(d)</td>
<td id="S6.T3.17.11.19.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.17.11.19.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T3.17.11.19.5" class="ltx_td ltx_align_center">82.30</td>
<td id="S6.T3.17.11.19.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T3.17.11.20" class="ltx_tr">
<td id="S6.T3.17.11.20.1" class="ltx_td ltx_align_right">BEiT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite>
</td>
<td id="S6.T3.17.11.20.2" class="ltx_td ltx_align_center">(d)</td>
<td id="S6.T3.17.11.20.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.17.11.20.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T3.17.11.20.5" class="ltx_td ltx_align_center">84.19</td>
<td id="S6.T3.17.11.20.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T3.17.11.21" class="ltx_tr">
<td id="S6.T3.17.11.21.1" class="ltx_td ltx_align_right">PaLI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>
</td>
<td id="S6.T3.17.11.21.2" class="ltx_td ltx_align_center">(a)</td>
<td id="S6.T3.17.11.21.3" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T3.17.11.21.4" class="ltx_td ltx_align_center">-</td>
<td id="S6.T3.17.11.21.5" class="ltx_td ltx_align_center">84.30</td>
<td id="S6.T3.17.11.21.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S6.T3.17.11.22" class="ltx_tr">
<td id="S6.T3.17.11.22.1" class="ltx_td ltx_align_right ltx_border_bb">PaLI-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite>
</td>
<td id="S6.T3.17.11.22.2" class="ltx_td ltx_align_center ltx_border_bb">(a)</td>
<td id="S6.T3.17.11.22.3" class="ltx_td ltx_align_center ltx_border_bb">Yes</td>
<td id="S6.T3.17.11.22.4" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S6.T3.17.11.22.5" class="ltx_td ltx_align_center ltx_border_bb">86.00</td>
<td id="S6.T3.17.11.22.6" class="ltx_td ltx_align_center ltx_border_bb">-</td>
</tr>
</table>
</span></div>
</figure>
<div id="S6.p7" class="ltx_para">
<p id="S6.p7.1" class="ltx_p">The mentioned methods mainly employ the encoder-based architecture, while there is also another line of work adopting the encoder-decoder architecture. For instance, CoCa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite> replaces both the text and multimodal encoders with decoders and utilizes the Language Modeling (LM) loss for training. This approach improves training efficiency and achieves state-of-the-art results on a wide range of downstream tasks. BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> utilizes a multimodal mixture of encoder-decoder architecture, encompassing an image encoder, a text encoder, an image-grounded text encoder, and an image-grounded text decoder. This comprehensive framework enables the integration of vision-language understanding and generation. The method undergoes joint pre-training with ITC, ITM, and LM losses, resulting in robust zero-shot generalization capabilities. Nevertheless, the cost associated with VLP has progressively become prohibitively high as a result of training large-scale models end-to-end. To address this issue, BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite> employs a lightweight querying Transformer to bridge the vision and language modality gap in two stages: vision-language representation learning from a frozen image encoder and vision-to-language generative learning from a frozen language model. Different from the above methods, BEiT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> seamlessly handles both text and images by treating images as a foreign language, such as â€œImglishâ€. This approach eliminates any inherent modeling differences and allows for a unified pretraining task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>, <a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite> known as mask-then-predict. Treating image-text pairs as â€œparallel sentencesâ€, BEiT-3 effectively captures alignments between different modalities, obtaining transferable representations of considerable strength. All the aforementioned methods utilize high-computation-size encoders for images, text, and modality interactions, thereby classifying them within the fourth type, as depicted in Fig. <a href="#S5.F15" title="Figure 15 â€£ 5.4 Answer Re-Ranking â€£ 5 Debiasing Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> (d).</p>
</div>
<div id="S6.p8" class="ltx_para">
<p id="S6.p8.1" class="ltx_p">Table <a href="#S6.T3" title="TABLE III â€£ 6 Vision-and-Language Pre-training Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> shows the results of some VLP models on the VQA v2 validation (ID) and the VQA-CP v2 test (OOD) split. From the perspective of pre-training, it is evident that VLP yields significant improvements in both ID and OOD performance. For example, LXMERT outperforms UpDn by 13.97% and 7.07% on the mentioned split respectively. Notably, models with larger sizes demonstrate superior results in both scenarios. It can be seen that ALBEF<sub id="S6.p8.1.1" class="ltx_sub">DISC</sub> (14M) is superior to ALBEF<sub id="S6.p8.1.2" class="ltx_sub">DISC</sub> (4M) by 0.8% and 0.3% on ID and OOD situations respectively. In terms of modeling (answer obtaining), the generative model proves to be more robust compared to the discriminative model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite>. For instance, ALBEF<sub id="S6.p8.1.3" class="ltx_sub">GEN</sub> (4M) outperforms ALBEF<sub id="S6.p8.1.4" class="ltx_sub">DISC</sub> (4M) by 4.8% and 1.0% on the mentioned split respectively. Furthermore, the considerable gap between ID and OOD performance indicates ample room for further improvement in VLP models. Specifically, in the case of LXMERT, this difference is quantified as 21.28%, highlighting the considerable scope for improvement in bridging this gap.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Discussions and Future Directions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Based on the comprehensive analysis of existing datasets, evaluations, and methods outlined above, it becomes apparent that there is potential for improvement in robust VQA. Consequently, our focus will now shift toward discussing the strategies and areas where future advancements can be made.</p>
</div>
<figure id="S7.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>The accuracy (%) comparison of non-debiasing (ND) and debiasing (DE) methods on various datasets. All the results are on the test split except for the result of the VQA v2.0 testdev split.</figcaption>
<div id="S7.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:114pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-127.2pt,33.3pt) scale(0.630209598094938,0.630209598094938) ;">
<table id="S7.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T4.1.1.1" class="ltx_tr">
<td id="S7.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S7.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="S7.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S7.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Backbone</span></td>
<td id="S7.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S7.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Methods</span></td>
<td id="S7.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S7.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">VQA v2.0 test-dev</span></td>
<td id="S7.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S7.T4.1.1.1.5.1" class="ltx_text ltx_font_bold">VQA-CP v1 test</span></td>
<td id="S7.T4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S7.T4.1.1.1.6.1" class="ltx_text ltx_font_bold">VQA-CP v2 test</span></td>
<td id="S7.T4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S7.T4.1.1.1.7.1" class="ltx_text ltx_font_bold">GQA-OOD val</span></td>
</tr>
<tr id="S7.T4.1.1.2" class="ltx_tr">
<td id="S7.T4.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.1.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S7.T4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.2.1" class="ltx_text ltx_font_bold">Y/N</span></td>
<td id="S7.T4.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.3.1" class="ltx_text ltx_font_bold">Num.</span></td>
<td id="S7.T4.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.4.1" class="ltx_text ltx_font_bold">Other</span></td>
<td id="S7.T4.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.5.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S7.T4.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.6.1" class="ltx_text ltx_font_bold">Y/N</span></td>
<td id="S7.T4.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.7.1" class="ltx_text ltx_font_bold">Num.</span></td>
<td id="S7.T4.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.8.1" class="ltx_text ltx_font_bold">Other</span></td>
<td id="S7.T4.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.9.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S7.T4.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.10.1" class="ltx_text ltx_font_bold">Y/N</span></td>
<td id="S7.T4.1.1.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.11.1" class="ltx_text ltx_font_bold">Num.</span></td>
<td id="S7.T4.1.1.2.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.12.1" class="ltx_text ltx_font_bold">Other</span></td>
<td id="S7.T4.1.1.2.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.13.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S7.T4.1.1.2.14" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.14.1" class="ltx_text ltx_font_bold">Tail</span></td>
<td id="S7.T4.1.1.2.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T4.1.1.2.15.1" class="ltx_text ltx_font_bold">Head</span></td>
</tr>
<tr id="S7.T4.1.1.3" class="ltx_tr">
<td id="S7.T4.1.1.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T4.1.1.3.1.1" class="ltx_text">NDE</span></td>
<td id="S7.T4.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S7.T4.1.1.3.2.1" class="ltx_text">NA</span></td>
<td id="S7.T4.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">SMRL</td>
<td id="S7.T4.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">64.76</td>
<td id="S7.T4.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">82.20</td>
<td id="S7.T4.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">46.44</td>
<td id="S7.T4.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t">54.01</td>
<td id="S7.T4.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t">36.86</td>
<td id="S7.T4.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t">43.39</td>
<td id="S7.T4.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t">12.88</td>
<td id="S7.T4.1.1.3.11" class="ltx_td ltx_align_center ltx_border_t">40.22</td>
<td id="S7.T4.1.1.3.12" class="ltx_td ltx_align_center ltx_border_t">37.09</td>
<td id="S7.T4.1.1.3.13" class="ltx_td ltx_align_center ltx_border_t">41.85</td>
<td id="S7.T4.1.1.3.14" class="ltx_td ltx_align_center ltx_border_t">12.76</td>
<td id="S7.T4.1.1.3.15" class="ltx_td ltx_align_center ltx_border_t">41.28</td>
<td id="S7.T4.1.1.3.16" class="ltx_td ltx_align_center ltx_border_t">46.32</td>
<td id="S7.T4.1.1.3.17" class="ltx_td ltx_align_center ltx_border_t">41.67</td>
<td id="S7.T4.1.1.3.18" class="ltx_td ltx_align_center ltx_border_t">49.16</td>
</tr>
<tr id="S7.T4.1.1.4" class="ltx_tr">
<td id="S7.T4.1.1.4.1" class="ltx_td ltx_align_center">UpDn</td>
<td id="S7.T4.1.1.4.2" class="ltx_td ltx_align_center">65.78</td>
<td id="S7.T4.1.1.4.3" class="ltx_td ltx_align_center">83.07</td>
<td id="S7.T4.1.1.4.4" class="ltx_td ltx_align_center">45.88</td>
<td id="S7.T4.1.1.4.5" class="ltx_td ltx_align_center">55.54</td>
<td id="S7.T4.1.1.4.6" class="ltx_td ltx_align_center">37.40</td>
<td id="S7.T4.1.1.4.7" class="ltx_td ltx_align_center">43.27</td>
<td id="S7.T4.1.1.4.8" class="ltx_td ltx_align_center">12.89</td>
<td id="S7.T4.1.1.4.9" class="ltx_td ltx_align_center">41.57</td>
<td id="S7.T4.1.1.4.10" class="ltx_td ltx_align_center">38.04</td>
<td id="S7.T4.1.1.4.11" class="ltx_td ltx_align_center">43.41</td>
<td id="S7.T4.1.1.4.12" class="ltx_td ltx_align_center">12.92</td>
<td id="S7.T4.1.1.4.13" class="ltx_td ltx_align_center">42.26</td>
<td id="S7.T4.1.1.4.14" class="ltx_td ltx_align_center">47.75</td>
<td id="S7.T4.1.1.4.15" class="ltx_td ltx_align_center">42.62</td>
<td id="S7.T4.1.1.4.16" class="ltx_td ltx_align_center">50.89</td>
</tr>
<tr id="S7.T4.1.1.5" class="ltx_tr">
<td id="S7.T4.1.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="6"><span id="S7.T4.1.1.5.1.1" class="ltx_text">DE</span></td>
<td id="S7.T4.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S7.T4.1.1.5.2.1" class="ltx_text">SMRL</span></td>
<td id="S7.T4.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t">CF Variant</td>
<td id="S7.T4.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t">62.63</td>
<td id="S7.T4.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t">82.14</td>
<td id="S7.T4.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t">44.02</td>
<td id="S7.T4.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t">50.14</td>
<td id="S7.T4.1.1.5.8" class="ltx_td ltx_align_center ltx_border_t">43.76</td>
<td id="S7.T4.1.1.5.9" class="ltx_td ltx_align_center ltx_border_t">60.83</td>
<td id="S7.T4.1.1.5.10" class="ltx_td ltx_align_center ltx_border_t">13.92</td>
<td id="S7.T4.1.1.5.11" class="ltx_td ltx_align_center ltx_border_t">38.92</td>
<td id="S7.T4.1.1.5.12" class="ltx_td ltx_align_center ltx_border_t">54.04</td>
<td id="S7.T4.1.1.5.13" class="ltx_td ltx_align_center ltx_border_t">88.23</td>
<td id="S7.T4.1.1.5.14" class="ltx_td ltx_align_center ltx_border_t">30.86</td>
<td id="S7.T4.1.1.5.15" class="ltx_td ltx_align_center ltx_border_t">42.71</td>
<td id="S7.T4.1.1.5.16" class="ltx_td ltx_align_center ltx_border_t">39.34</td>
<td id="S7.T4.1.1.5.17" class="ltx_td ltx_align_center ltx_border_t">35.09</td>
<td id="S7.T4.1.1.5.18" class="ltx_td ltx_align_center ltx_border_t">41.95</td>
</tr>
<tr id="S7.T4.1.1.6" class="ltx_tr">
<td id="S7.T4.1.1.6.1" class="ltx_td ltx_align_center">RUBi</td>
<td id="S7.T4.1.1.6.2" class="ltx_td ltx_align_center">63.28</td>
<td id="S7.T4.1.1.6.3" class="ltx_td ltx_align_center">82.28</td>
<td id="S7.T4.1.1.6.4" class="ltx_td ltx_align_center">45.46</td>
<td id="S7.T4.1.1.6.5" class="ltx_td ltx_align_center">51.05</td>
<td id="S7.T4.1.1.6.6" class="ltx_td ltx_align_center">50.83</td>
<td id="S7.T4.1.1.6.7" class="ltx_td ltx_align_center">80.18</td>
<td id="S7.T4.1.1.6.8" class="ltx_td ltx_align_center">16.52</td>
<td id="S7.T4.1.1.6.9" class="ltx_td ltx_align_center">39.43</td>
<td id="S7.T4.1.1.6.10" class="ltx_td ltx_align_center">47.61</td>
<td id="S7.T4.1.1.6.11" class="ltx_td ltx_align_center">74.68</td>
<td id="S7.T4.1.1.6.12" class="ltx_td ltx_align_center">20.31</td>
<td id="S7.T4.1.1.6.13" class="ltx_td ltx_align_center">43.23</td>
<td id="S7.T4.1.1.6.14" class="ltx_td ltx_align_center">46.78</td>
<td id="S7.T4.1.1.6.15" class="ltx_td ltx_align_center">42.52</td>
<td id="S7.T4.1.1.6.16" class="ltx_td ltx_align_center">49.39</td>
</tr>
<tr id="S7.T4.1.1.7" class="ltx_tr">
<td id="S7.T4.1.1.7.1" class="ltx_td ltx_align_center">CF</td>
<td id="S7.T4.1.1.7.2" class="ltx_td ltx_align_center">63.01</td>
<td id="S7.T4.1.1.7.3" class="ltx_td ltx_align_center">81.96</td>
<td id="S7.T4.1.1.7.4" class="ltx_td ltx_align_center">45.48</td>
<td id="S7.T4.1.1.7.5" class="ltx_td ltx_align_center">50.98</td>
<td id="S7.T4.1.1.7.6" class="ltx_td ltx_align_center">56.88</td>
<td id="S7.T4.1.1.7.7" class="ltx_td ltx_align_center">89.75</td>
<td id="S7.T4.1.1.7.8" class="ltx_td ltx_align_center">17.56</td>
<td id="S7.T4.1.1.7.9" class="ltx_td ltx_align_center">40.21</td>
<td id="S7.T4.1.1.7.10" class="ltx_td ltx_align_center">55.42</td>
<td id="S7.T4.1.1.7.11" class="ltx_td ltx_align_center">90.56</td>
<td id="S7.T4.1.1.7.12" class="ltx_td ltx_align_center">26.61</td>
<td id="S7.T4.1.1.7.13" class="ltx_td ltx_align_center">45.65</td>
<td id="S7.T4.1.1.7.14" class="ltx_td ltx_align_center">44.28</td>
<td id="S7.T4.1.1.7.15" class="ltx_td ltx_align_center">41.20</td>
<td id="S7.T4.1.1.7.16" class="ltx_td ltx_align_center">44.28</td>
</tr>
<tr id="S7.T4.1.1.8" class="ltx_tr">
<td id="S7.T4.1.1.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S7.T4.1.1.8.1.1" class="ltx_text">UpDn</span></td>
<td id="S7.T4.1.1.8.2" class="ltx_td ltx_align_center ltx_border_t">CF Variant</td>
<td id="S7.T4.1.1.8.3" class="ltx_td ltx_align_center ltx_border_t">65.19</td>
<td id="S7.T4.1.1.8.4" class="ltx_td ltx_align_center ltx_border_t">82.98</td>
<td id="S7.T4.1.1.8.5" class="ltx_td ltx_align_center ltx_border_t">44.93</td>
<td id="S7.T4.1.1.8.6" class="ltx_td ltx_align_center ltx_border_t">54.58</td>
<td id="S7.T4.1.1.8.7" class="ltx_td ltx_align_center ltx_border_t">37.26</td>
<td id="S7.T4.1.1.8.8" class="ltx_td ltx_align_center ltx_border_t">44.99</td>
<td id="S7.T4.1.1.8.9" class="ltx_td ltx_align_center ltx_border_t">13.08</td>
<td id="S7.T4.1.1.8.10" class="ltx_td ltx_align_center ltx_border_t">41.68</td>
<td id="S7.T4.1.1.8.11" class="ltx_td ltx_align_center ltx_border_t">37.59</td>
<td id="S7.T4.1.1.8.12" class="ltx_td ltx_align_center ltx_border_t">44.04</td>
<td id="S7.T4.1.1.8.13" class="ltx_td ltx_align_center ltx_border_t">13.03</td>
<td id="S7.T4.1.1.8.14" class="ltx_td ltx_align_center ltx_border_t">41.97</td>
<td id="S7.T4.1.1.8.15" class="ltx_td ltx_align_center ltx_border_t">48.03</td>
<td id="S7.T4.1.1.8.16" class="ltx_td ltx_align_center ltx_border_t">44.21</td>
<td id="S7.T4.1.1.8.17" class="ltx_td ltx_align_center ltx_border_t">50.38</td>
</tr>
<tr id="S7.T4.1.1.9" class="ltx_tr">
<td id="S7.T4.1.1.9.1" class="ltx_td ltx_align_center">RUBi</td>
<td id="S7.T4.1.1.9.2" class="ltx_td ltx_align_center">64.94</td>
<td id="S7.T4.1.1.9.3" class="ltx_td ltx_align_center">83.22</td>
<td id="S7.T4.1.1.9.4" class="ltx_td ltx_align_center">45.51</td>
<td id="S7.T4.1.1.9.5" class="ltx_td ltx_align_center">53.71</td>
<td id="S7.T4.1.1.9.6" class="ltx_td ltx_align_center">50.45</td>
<td id="S7.T4.1.1.9.7" class="ltx_td ltx_align_center">80.25</td>
<td id="S7.T4.1.1.9.8" class="ltx_td ltx_align_center">14.76</td>
<td id="S7.T4.1.1.9.9" class="ltx_td ltx_align_center">41.01</td>
<td id="S7.T4.1.1.9.10" class="ltx_td ltx_align_center">39.57</td>
<td id="S7.T4.1.1.9.11" class="ltx_td ltx_align_center">49.74</td>
<td id="S7.T4.1.1.9.12" class="ltx_td ltx_align_center">19.17</td>
<td id="S7.T4.1.1.9.13" class="ltx_td ltx_align_center">42.38</td>
<td id="S7.T4.1.1.9.14" class="ltx_td ltx_align_center">48.03</td>
<td id="S7.T4.1.1.9.15" class="ltx_td ltx_align_center">42.24</td>
<td id="S7.T4.1.1.9.16" class="ltx_td ltx_align_center">51.59</td>
</tr>
<tr id="S7.T4.1.1.10" class="ltx_tr">
<td id="S7.T4.1.1.10.1" class="ltx_td ltx_align_center ltx_border_bb">CF</td>
<td id="S7.T4.1.1.10.2" class="ltx_td ltx_align_center ltx_border_bb">65.47</td>
<td id="S7.T4.1.1.10.3" class="ltx_td ltx_align_center ltx_border_bb">83.16</td>
<td id="S7.T4.1.1.10.4" class="ltx_td ltx_align_center ltx_border_bb">44.72</td>
<td id="S7.T4.1.1.10.5" class="ltx_td ltx_align_center ltx_border_bb">55.07</td>
<td id="S7.T4.1.1.10.6" class="ltx_td ltx_align_center ltx_border_bb">57.64</td>
<td id="S7.T4.1.1.10.7" class="ltx_td ltx_align_center ltx_border_bb">89.18</td>
<td id="S7.T4.1.1.10.8" class="ltx_td ltx_align_center ltx_border_bb">14.57</td>
<td id="S7.T4.1.1.10.9" class="ltx_td ltx_align_center ltx_border_bb">43.75</td>
<td id="S7.T4.1.1.10.10" class="ltx_td ltx_align_center ltx_border_bb">54.02</td>
<td id="S7.T4.1.1.10.11" class="ltx_td ltx_align_center ltx_border_bb">91.35</td>
<td id="S7.T4.1.1.10.12" class="ltx_td ltx_align_center ltx_border_bb">13.46</td>
<td id="S7.T4.1.1.10.13" class="ltx_td ltx_align_center ltx_border_bb">45.60</td>
<td id="S7.T4.1.1.10.14" class="ltx_td ltx_align_center ltx_border_bb">45.24</td>
<td id="S7.T4.1.1.10.15" class="ltx_td ltx_align_center ltx_border_bb">41.11</td>
<td id="S7.T4.1.1.10.16" class="ltx_td ltx_align_center ltx_border_bb">47.78</td>
</tr>
</table>
</span></div>
</figure>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Does the dataset annotation exhibit a high level of quality?</span> Most of the existing datasets were developed based on the VQA v2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. However, the answer annotations in the dataset often lack consistent agreement, which can result in inaccurate evaluation outcomes. For instance, as illustrated in Fig. <a href="#S2.F3" title="Figure 3 â€£ 2 Preliminaries â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, a system that generates a â€œYesâ€ or â€œNoâ€ answer for the â€œYes/Noâ€ question would receive an accuracy score of â€œ1.00â€ or â€œ0.67â€, respectively. Therefore, current data quality cannot support the accurate performance measurement of VQA models. In the future, we should ensure the quality of data annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite>, such as introducing a process where annotations are reviewed by experienced annotators or experts for ambiguous cases.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text ltx_font_bold">What datasets should be developed?</span> Although existing datasets, especially the OOD dataset, enable us to provide insight into the robustness of VQA methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>, it is essential to note that each dataset has its own unique limitations. Taking the most commonly used OOD dataset VQA-CP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> as an example, it has two shortcomings. First, its distribution between training and test splits is significantly different or even reversed, which may not align with the real-world scenario. Some methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> may be devised based on this prior, which may not reflect the robustness of these methods accurately. Second, VQA-CP lacks the validation split, which results in methods being tuned on the test split. Although GQA-OOD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> alleviates the above issues, its test split is too small, only containing 12,578 questions. <em id="S7.p3.1.2" class="ltx_emph ltx_font_italic">Therefore, existing datasets may not be sufficient to evaluate robustness. Furthermore, the dataset does not involve fine-grained bias evaluations such as vision shortcut measurement. To address this issue, we should develop a dataset that satisfies the following properties in the future.</em></p>
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">The dataset should be sufficiently large and complete, with adequate validation splits for fine-tuning hyper-parameters and large splits for training and testing.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">The dataset should contain ID and OOD test settings simultaneously. In this way, we can conduct a comprehensive and fair evaluation of the robustness of VQA methods.</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p">The distribution between training and test splits should be more natural, rather than artificially setting significantly different or even contradictory data distributions. The artificial distribution prior may be used to improve model performance, while the prior can not be applied to other situations, leading to poor generalization ability.</p>
</div>
</li>
<li id="S7.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S7.I1.i4.p1" class="ltx_para">
<p id="S7.I1.i4.p1.1" class="ltx_p">The OOD test setting should simultaneously include language, vision, and multimodality bias, to have a more refined assessment of robustness.</p>
</div>
</li>
<li id="S7.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S7.I1.i5.p1" class="ltx_para">
<p id="S7.I1.i5.p1.1" class="ltx_p">The question format should be various, particularly in the test split. The question in existing datasets is usually generated by the template. However, the question patterns generated by templates may be not enough, and be learned or memorized easily, resulting in inaccurate comparisons.</p>
</div>
</li>
</ul>
</div>
<div id="S7.p4" class="ltx_para ltx_noindent">
<p id="S7.p4.1" class="ltx_p"><span id="S7.p4.1.1" class="ltx_text ltx_font_bold">Are the evaluation metrics effective enough?</span> Current evaluation protocols assign equal weight to each question. However, some questions, such as the OOD question requiring multi-hop reasoning, should be treated as more important. Therefore, in the future, we should devise an evaluation protocol that can assign different weights to questions according to annotations, such as the distribution they belong to and their difficulty. Furthermore, only accuracy is typically used to evaluate the robustness of methods, which may not be sufficient. In the future, we may apply composite metrics such as â€œvalidityâ€ and â€œplausibilityâ€ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to evaluate VQA methods from various angles.</p>
</div>
<div id="S7.p5" class="ltx_para ltx_noindent">
<p id="S7.p5.1" class="ltx_p"><span id="S7.p5.1.1" class="ltx_text ltx_font_bold">Are the existing debiasing methods robust enough?</span> A variety of debiasing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> for VQA have been proposed and have achieved significant success on VQA v2 and VQA-CP which are the most commonly used datasets to evaluate robustness. This inspires us to consider whether they are robust to other datasets, such as GQA-OOD. To address our curiosity, we select several non-debiasing methods, including SMRL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and debiasing methods, including RUBi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and CF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>, and then conduct experiments on these three datasets simultaneously for a fair comparison. The results are shown in Table <a href="#S7.T4" title="TABLE IV â€£ 7 Discussions and Future Directions â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. It can be seen that existing debiasing methods are backbone-sensitive and cannot achieve ongoing success on other datasets. <span id="S7.p5.1.2" class="ltx_text" style="color:#000000;">For instance, when using the CF Variant with SMRL, an accuracy of 43.76% is achieved on the VQA-CP v1 dataset. In contrast, when employing the CF variant with UpDn, the accuracy drops to 37.26%. Additionally, CF with UpDn attains an accuracy of 45.24% on the GQA-OOD validation split, which is 2.51% lower than that of UpDn alone.</span> We can also note that current debiasing methods still achieve high OOD performance at the expense of ID performance. <span id="S7.p5.1.3" class="ltx_text" style="color:#000000;">Specifically, RUBi with SMRL obtains the improvement of 0.46% on the GQA-OOD validation split, while it drops by 0.48% on the VQA v2.0 test-dev split.</span> These show that we should develop true robust methods in the future to perform well in a variety of ID and OOD performances at the same time.</p>
</div>
<div id="S7.p6" class="ltx_para ltx_noindent">
<p id="S7.p6.1" class="ltx_p"><span id="S7.p6.1.1" class="ltx_text ltx_font_bold">Are the existing VLP methods robust enough?</span> In recent years, VLP models have gained significant attention and achieved remarkable success across various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>]</cite>, establishing themselves as a prominent research area. Table <a href="#S6.T3" title="TABLE III â€£ 6 Vision-and-Language Pre-training Methods â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> presents the performance of representative VLP methods on the VQA-CP v2 test split and the VQA v2 validation split. Notably, early VLP methods like UNITER and LXMERT exhibit a certain degree of robustness, but there remains room for improvement, especially in reducing the performance gap between ID and OOD situations. Moreover, there appears to be limited research exploring the robustness of more recent VLP models such as BEiT-3 and PaLI on VQA-CP and GQA-OOD. We think that exploring the robustness of these newer models could be an intriguing direction for future research.</p>
</div>
<div id="S7.p7" class="ltx_para ltx_noindent">
<p id="S7.p7.1" class="ltx_p"><span id="S7.p7.1.1" class="ltx_text ltx_font_bold">How should we verify the robustness of the method?</span> Existing works frequently employ VQA v2 and VQA-CP to assess both debiasing methods and VLP methods. These methods, however, do not perform well on the other OOD datasets, as shown in Table <a href="#S7.T4" title="TABLE IV â€£ 7 Discussions and Future Directions â€£ Robust Visual Question Answering: Datasets, Methods, and Future Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. Therefore, considering current dataset conditions, we should leverage multiple ID and OOD datasets to verify the robustness simultaneously, which can reflect the robustness more comprehensively and accurately.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">This paper presents a comprehensive survey that focuses on the domain of robust Visual Question Answering (VQA). We conduct a systematic review of existing datasets from ID and OOD angles, evaluation metrics from the single and composite views, and methods from the perspective of debiasing techniques. Specifically, we classify the existing debiasing methods into four categories: ensemble learning, data augmentation, self-supervised contrastive learning, and answer re-ranking. We review the robustness of vision-and-language pre-training methods on VQA, classifying them into four categories according to the relative computational size of text encoders, image encoders, and modality interaction modules. We discuss future research directions that should be prioritized for robust VQA.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by the National Key Research and Development Program of China (2021YFB1715600), the National Natural Science Foundation of China (U22B2019, 62272372, 62293553, 62250066, 621737002).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S.Â Antol, A.Â Agrawal, J.Â Lu, M.Â Mitchell, D.Â Batra, C.Â L. Zitnick, and
D.Â Parikh, â€œVqa: Visual question answering,â€ in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015, pp.
2425â€“2433.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K.Â Kafle and C.Â Kanan, â€œVisual question answering: Datasets, algorithms and
future challenges,â€ <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CVIU</em>, vol. 163, pp. 3â€“20, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
N.Â Liu, Q.Â Pu, Y.Â Shi, S.Â Zhang, and L.Â Qiu, â€œOlder adultsâ€™ interaction with
intelligent virtual assistants: the role of information modality and
feedback,â€ <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IJHCS</em>, vol.Â 39, no.Â 5, pp. 1162â€“1183, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
E.Â Zangerle and C.Â Bauer, â€œEvaluating recommender systems: Survey and
framework,â€ <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, vol.Â 55, no.Â 8, pp. 1â€“38, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
L.Â Chen, Y.Â Li, C.Â Huang, B.Â Li, Y.Â Xing, D.Â Tian, L.Â Li, Z.Â Hu, X.Â Na, Z.Â Li
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œMilestones in autonomous driving and intelligent vehicles:
Survey of surveys,â€ <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">IEEE TIV</em>, vol.Â 8, no.Â 2, pp. 1046â€“1056, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S.Â Whitehead, H.Â Wu, H.Â Ji, R.Â Feris, and K.Â Saenko, â€œSeparating skills and
concepts for novel visual question answering,â€ in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021, pp.
5632â€“5641.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.Â Liu, X.Â Zhang, Q.Â Zhang, C.Â Li, F.Â Huang, X.Â Tang, and Z.Â Li, â€œDual
self-attention with co-attention networks for visual question answering,â€
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">PR</em>, vol. 117, p. 107956, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J.Â Park, J.Â Lee, and K.Â Sohn, â€œBridge to answer: Structure-aware graph
interaction network for video question answering,â€ in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021, pp.
15â€‰526â€“15â€‰535.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T.Â Rahman, S.-H. Chou, L.Â Sigal, and G.Â Carenini, â€œAn improved attention for
visual question answering,â€ in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021, pp. 1653â€“1662.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Q.Â Cao, X.Â Liang, B.Â Li, and L.Â Lin, â€œInterpretable visual question answering
by reasoning on dependency trees,â€ <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 43, no.Â 3, pp.
887â€“901, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.Â Cao, X.Â Qin, S.Â Zhao, and J.Â Shen, â€œBilateral cross-modality graph matching
attention for feature fusion in visual question answering,â€ <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE
TNNLS</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.Â Goyal, T.Â Khot, D.Â Summers-Stay, D.Â Batra, and D.Â Parikh, â€œMaking the v in
vqa matter: Elevating the role of image understanding in visual question
answering,â€ in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017, pp. 6904â€“6913.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P.Â Wang, Q.Â Wu, C.Â Shen, A.Â Dick, and A.Â Van DenÂ Hengel, â€œFvqa: Fact-based
visual question answering,â€ <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 40, no.Â 10, pp.
2413â€“2427, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K.Â Marino, M.Â Rastegari, A.Â Farhadi, and R.Â Mottaghi, â€œOk-vqa: A visual
question answering benchmark requiring external knowledge,â€ in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">CVPR</em>,
2019, pp. 3195â€“3204.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D.Â A. Hudson and C.Â D. Manning, â€œGqa: A new dataset for real-world visual
reasoning and compositional question answering,â€ in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp.
6700â€“6709.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A.Â Fukui, D.Â H. Park, D.Â Yang, A.Â Rohrbach, T.Â Darrell, and M.Â Rohrbach,
â€œMultimodal compact bilinear pooling for visual question answering and
visual grounding,â€ in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2016, pp. 457â€“468.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
L.Â Ma, Z.Â Lu, and H.Â Li, â€œLearning to answer questions from image using
convolutional neural network,â€ in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2016, pp. 3567â€“3573.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Z.Â Yang, X.Â He, J.Â Gao, L.Â Deng, and A.Â Smola, â€œStacked attention networks for
image question answering,â€ in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016, pp. 21â€“29.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P.Â Anderson, X.Â He, C.Â Buehler, D.Â Teney, M.Â Johnson, S.Â Gould, and L.Â Zhang,
â€œBottom-up and top-down attention for image captioning and visual question
answering,â€ in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018, pp. 6077â€“6086.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R.Â Cadene, H.Â Ben-Younes, M.Â Cord, and N.Â Thome, â€œMurel: Multimodal relational
reasoning for visual question answering,â€ in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp.
1989â€“1998.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
P.Â Gao, Z.Â Jiang, H.Â You, P.Â Lu, S.Â C. Hoi, X.Â Wang, and H.Â Li, â€œDynamic
fusion with intra-and inter-modality attention flow for visual question
answering,â€ in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp. 6639â€“6648.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J.Â Wu, J.Â Lu, A.Â Sabharwal, and R.Â Mottaghi, â€œMulti-modal answer validation
for knowledge-based vqa,â€ in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2022, pp. 2712â€“2721.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y.Â Ding, J.Â Yu, B.Â Liu, Y.Â Hu, M.Â Cui, and Q.Â Wu, â€œMukea: Multimodal knowledge
extraction and accumulation for knowledge-based visual question answering,â€
in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022, pp. 5089â€“5098.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S.Â Ravi, A.Â Chinchure, L.Â Sigal, R.Â Liao, and V.Â Shwartz, â€œVlc-bert: Visual
question answering with contextualized commonsense knowledge,â€ in
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">WACV</em>, 2023, pp. 1155â€“1165.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S.Â Auer, C.Â Bizer, G.Â Kobilarov, J.Â Lehmann, R.Â Cyganiak, and Z.Â Ives,
â€œDbpedia: A nucleus for a web of open data,â€ in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ISWC-ASWC</em>, 2007, pp.
722â€“735.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K.Â Bollacker, C.Â Evans, P.Â Paritosh, T.Â Sturge, and J.Â Taylor, â€œFreebase: A
collaboratively created graph database for structuring human knowledge,â€ in
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">SIGMOD</em>, 2008, pp. 1247â€“1250.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J.Â Hoffart, F.Â M. Suchanek, K.Â Berberich, and G.Â Weikum, â€œYago2: A spatially
and temporally enhanced knowledge base from wikipedia,â€ <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Artificial
intelligence</em>, vol. 194, pp. 28â€“61, 2013.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
T.Â BaltruÅ¡aitis, C.Â Ahuja, and L.-P. Morency, â€œMultimodal machine learning: A
survey and taxonomy,â€ <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 41, no.Â 2, pp. 423â€“443, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A.Â Agrawal, D.Â Batra, D.Â Parikh, and A.Â Kembhavi, â€œDonâ€™t just assume; look and
answer: Overcoming priors for visual question answering,â€ in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">CVPR</em>,
2018, pp. 4971â€“4980.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y.Â Niu and H.Â Zhang, â€œIntrospective distillation for robust question
answering,â€ in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021, pp. 16â€‰292â€“16â€‰304.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J.Â M. Kim, A.Â S. Koepke, C.Â Schmid, and Z.Â Akata, â€œExposing and mitigating
spurious correlations for cross-modal retrieval,â€ in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">CVPR Workshop</em>,
June 2023, pp. 2584â€“2594.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
X.Â Zhang, F.Â Zhang, and C.Â Xu, â€œNext-ood: Overcoming dual multiple-choice vqa
biases,â€ <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE TPMAI</em>, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Y.Â Song, X.Â Yang, Y.Â Wang, and C.Â Xu, â€œRecovering generalization via
pre-training-like knowledge distillation for out-of-distribution visual
question answering,â€ <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE TMM</em>, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R.Â Cadene, C.Â Dancette, H.Â Ben-younes, M.Â Cord, and D.Â Parikh, â€œRubi: Reducing
unimodal biases for visual question answering,â€ in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2019, pp.
841â€“852.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
V.Â Gupta, Z.Â Li, A.Â Kortylewski, C.Â Zhang, Y.Â Li, and A.Â Yuille, â€œSwapmix:
Diagnosing and regularizing the over-reliance on visual context in visual
question answering,â€ in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022, pp. 5078â€“5088.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
C.Â Kervadec, G.Â Antipov, M.Â Baccouche, and C.Â Wolf, â€œRoses are red, violets
are blueâ€¦ but should vqa expect them to?â€ in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021, pp.
2776â€“2785.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Q.Â Si, F.Â Meng, M.Â Zheng, Z.Â Lin, Y.Â Liu, P.Â Fu, Y.Â Cao, W.Â Wang, and J.Â Zhou,
â€œLanguage prior is not the only shortcut: A benchmark for shortcut learning
in vqa,â€ in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>, 2022, pp. 3698â€“3712.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
K.Â Kafle and C.Â Kanan, â€œAn analysis of visual question answering algorithms,â€
in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2017, pp. 1965â€“1973.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Q.Â Wu, C.Â Shen, P.Â Wang, A.Â Dick, and A.Â Van DenÂ Hengel, â€œImage captioning and
visual question answering based on attributes and external knowledge,â€
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 40, no.Â 6, pp. 1367â€“1381, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
W.Â Guo, Y.Â Zhang, J.Â Yang, and X.Â Yuan, â€œRe-attention for visual question
answering,â€ <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE TIP</em>, vol.Â 30, pp. 6730â€“6743, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
T.Â Wolf, L.Â Debut, V.Â Sanh, J.Â Chaumond, C.Â Delangue, A.Â Moi, P.Â Cistac,
T.Â Rault, R.Â Louf, M.Â Funtowicz <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œTransformers:
State-of-the-art natural language processing,â€ in <em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic">EMNLP</em>, 2020, pp.
38â€“45.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
T.Â Naseem, S.Â Ravishankar, N.Â Mihindukulasooriya, I.Â Abdelaziz, Y.-S. Lee,
P.Â Kapanipathi, S.Â Roukos, A.Â Gliozzo, and A.Â Gray, â€œA semantics-aware
transformer model of relation linking for knowledge base question
answering,â€ in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ACL-IJCNN</em>, 2021, pp. 256â€“262.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
S.Â Garg, T.Â Vu, and A.Â Moschitti, â€œTanda: Transfer and adapt pre-trained
transformer models for answer sentence selection,â€ in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020, pp.
7780â€“7788.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
M.Â Glass, A.Â Gliozzo, R.Â Chakravarti, A.Â Ferritto, G.Â S. Bhargav, D.Â Garg,
A.Â Sil, and L.Â Pan, â€œSpan selection pre-training for question answering,â€
in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Z.Â Zhang, Y.Â Wu, J.Â Zhou, S.Â Duan, H.Â Zhao, and R.Â Wang, â€œSg-net: Syntax
guided transformer for language representation,â€ <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 44,
no.Â 06, pp. 3285â€“3299, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Y.Â Zhou, L.Â Liao, Y.Â Gao, R.Â Wang, and H.Â Huang, â€œTopicbert: A topic-enhanced
neural language model fine-tuned for sentiment classification,â€ <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">IEEE
TNNLS</em>, 2021.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
L.Â H. Li, M.Â Yatskar, D.Â Yin, C.-J. Hsieh, and K.-W. Chang, â€œVisualbert: A
simple and performant baseline for vision and language,â€ <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1908.03557</em>, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Y.-C. Chen, L.Â Li, L.Â Yu, A.Â ElÂ Kholy, F.Â Ahmed, Z.Â Gan, Y.Â Cheng, and J.Â Liu,
â€œUniter: Universal image-text representation learning,â€ in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">ECCV</em>,
2020, pp. 104â€“120.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
G.Â Li, N.Â Duan, Y.Â Fang, M.Â Gong, and D.Â Jiang, â€œUnicoder-vl: A universal
encoder for vision and language by cross-modal pre-training,â€ in
<em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020, pp. 11â€‰336â€“11â€‰344.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Z.Â Yang, T.Â Luo, D.Â Wang, Z.Â Hu, J.Â Gao, and L.Â Wang, â€œLearning to navigate
for fine-grained classification,â€ in <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2018, pp. 420â€“435.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
S.Â Zhao, Z.Â Wen, Q.Â Qi, K.-M. Lam, and J.Â Shen, â€œLearning fine-grained
information with capsule-wise attention for salient object detection,â€
<em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE TMM</em>, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Z.-J. Zha, D.Â Liu, H.Â Zhang, Y.Â Zhang, and F.Â Wu, â€œContext-aware visual policy
network for fine-grained image captioning,â€ <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 44,
no.Â 2, pp. 710â€“722, 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
H.Â Zhao, J.Â Jia, and V.Â Koltun, â€œExploring self-attention for image
recognition,â€ in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020, pp. 10â€‰076â€“10â€‰085.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
C.Â Xie, M.Â Tan, B.Â Gong, J.Â Wang, A.Â L. Yuille, and Q.Â V. Le, â€œAdversarial
examples improve image recognition,â€ in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020, pp. 819â€“828.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
J.Â Chen, M.Â Jiang, Q.Â Dou, and Q.Â Chen, â€œFederated domain generalization for
image recognition via cross-client style transfer,â€ in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">WACV</em>, 2023,
pp. 361â€“370.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
M.Â Acharya, K.Â Kafle, and C.Â Kanan, â€œTallyqa: Answering complex counting
questions,â€ in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2019, pp. 8076â€“8084.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
V.Â A. Sindagi and V.Â M. Patel, â€œHa-ccn: Hierarchical attention-based crowd
counting network,â€ <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE TIP</em>, vol.Â 29, pp. 323â€“335, 2019.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M.Â Maire, S.Â Belongie, J.Â Hays, P.Â Perona, D.Â Ramanan,
P.Â DollÃ¡r, and C.Â L. Zitnick, â€œMicrosoft coco: Common objects in
context,â€ in <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2014, pp. 740â€“755.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
H.Â Jiang, I.Â Misra, M.Â Rohrbach, E.Â Learned-Miller, and X.Â Chen, â€œIn defense
of grid features for visual question answering,â€ in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020, pp.
10â€‰267â€“10â€‰276.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
L.Â Li, Z.Â Gan, Y.Â Cheng, and J.Â Liu, â€œRelation-aware graph attention network
for visual question answering,â€ in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019, pp. 10â€‰313â€“10â€‰322.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
J.Â Ma, J.Â Liu, Q.Â Lin, B.Â Wu, Y.Â Wang, and Y.Â You, â€œMultitask learning for
visual question answering,â€ <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">IEEE TNNLS</em>, vol.Â 34, no.Â 3, pp.
1380â€“1394, 2023.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
B.Â X. Nguyen, T.Â Do, H.Â Tran, E.Â Tjiputra, Q.Â D. Tran, and A.Â Nguyen,
â€œCoarse-to-fine reasoning for visual question answering,â€ in <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">CVPR</em>,
2022, pp. 4558â€“4566.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
G.Â Luo, Y.Â Zhou, X.Â Sun, Y.Â Wang, L.Â Cao, Y.Â Wu, F.Â Huang, and R.Â Ji, â€œTowards
lightweight transformer via group-wise transformation for vision-and-language
tasks,â€ <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">IEEE TIP</em>, vol.Â 31, pp. 3386â€“3398, 2022.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
D.Â Rosenberg, I.Â Gat, A.Â Feder, and R.Â Reichart, â€œAre vqa systems red?
measuring robustness to augmented data with focused interventions,â€ in
<em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">ACL-IJCNLP</em>, 2021, pp. 61â€“70.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
P.Â Zhang, Y.Â Goyal, D.Â Summers-Stay, D.Â Batra, and D.Â Parikh, â€œYin and yang:
Balancing and answering binary visual questions,â€ in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016, pp.
5014â€“5022.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
A.Â Das, H.Â Agrawal, L.Â Zitnick, D.Â Parikh, and D.Â Batra, â€œHuman attention in
visual question answering: Do humans and deep networks look at the same
regions?â€ <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">CVIU</em>, vol. 163, pp. 90â€“100, 2017.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J.Â Johnson, B.Â Hariharan, L.Â Van DerÂ Maaten, L.Â Fei-Fei, C.Â LawrenceÂ Zitnick,
and R.Â Girshick, â€œClevr: A diagnostic dataset for compositional language and
elementary visual reasoning,â€ in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017, pp. 2901â€“2910.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
R.Â Krishna, Y.Â Zhu, O.Â Groth, J.Â Johnson, K.Â Hata, J.Â Kravitz, S.Â Chen,
Y.Â Kalantidis, L.-J. Li, D.Â A. Shamma <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œVisual genome:
Connecting language and vision using crowdsourced dense image annotations,â€
<em id="bib.bib68.2.2" class="ltx_emph ltx_font_italic">IJCV</em>, vol. 123, no.Â 1, pp. 32â€“73, 2017.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
B.Â Thomee, D.Â A. Shamma, G.Â Friedland, B.Â Elizalde, K.Â Ni, D.Â Poland, D.Â Borth,
and L.-J. Li, â€œYfcc100m: The new data in multimedia research,â€
<em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, vol.Â 59, no.Â 2, pp. 64â€“73, 2016.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
P.Â Zhang, X.Â Li, X.Â Hu, J.Â Yang, L.Â Zhang, L.Â Wang, Y.Â Choi, and J.Â Gao,
â€œVinvl: Revisiting visual representations in vision-language models,â€ in
<em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021, pp. 5579â€“5588.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
D.Â Gao, R.Â Wang, S.Â Shan, and X.Â Chen, â€œCric: A vqa dataset for compositional
reasoning on vision and commonsense,â€ <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, pp. 1â€“18, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
M.Â Lao, Y.Â Guo, Y.Â Liu, W.Â Chen, N.Â Pu, and M.Â S. Lew, â€œFrom superficial to
deep: Language bias driven curriculum learning for visual question
answering,â€ in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">ACM MM</em>, 2021, pp. 3370â€“3379.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Z.Â Liang, H.Â Hu, and J.Â Zhu, â€œLpf: A language-prior feedback objective
function for de-biased visual question answering,â€ in <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">SIGIR</em>, 2021,
pp. 1955â€“1959.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
J.Â W. Cho, D.-J. Kim, H.Â Ryu, and I.Â S. Kweon, â€œGenerative bias for robust
visual question answering,â€ in <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2023, pp. 11â€‰681â€“11â€‰690.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
A.Â Basu, S.Â Addepalli, and R.Â V. Babu, â€œRmlvqa: A margin loss approach for
visual question answering with language biases,â€ in <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2023, pp.
11â€‰671â€“11â€‰680.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
M.Â Shah, X.Â Chen, M.Â Rohrbach, and D.Â Parikh, â€œCycle-consistency for robust
visual question answering,â€ in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp. 6642â€“6651.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
C.Â Dancette, R.Â Cadene, D.Â Teney, and M.Â Cord, â€œBeyond question-based biases:
Assessing multimodal shortcut learning in visual question answering,â€ in
<em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021, pp. 1554â€“1563.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
L.Â Li, J.Â Lei, Z.Â Gan, and J.Â Liu, â€œAdversarial vqa: A new benchmark for
evaluating the robustness of vqa models,â€ in <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021, pp.
2022â€“2031.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
V.Â Agarwal, R.Â Shetty, and M.Â Fritz, â€œTowards causal vqa: Revealing and
reducing spurious correlations by invariant and covariant semantic editing,â€
in <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020, pp. 9690â€“9698.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
T.Â Gokhale, P.Â Banerjee, C.Â Baral, and Y.Â Yang, â€œVqa-lol: Visual question
answering under the lens of logic,â€ in <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020, pp. 379â€“396.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
H.Â Tan and M.Â Bansal, â€œLxmert: Learning cross-modality encoder representations
from transformers,â€ in <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">EMNLP-IJCNLP</em>, 2019, pp. 5100â€“5111.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
M.Â Malinowski and M.Â Fritz, â€œA multi-world approach to question answering
about real-world scenes based on uncertain input,â€ in <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2014,
pp. 1682â€“1690.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
H.Â Gao, J.Â Mao, J.Â Zhou, Z.Â Huang, L.Â Wang, and W.Â Xu, â€œAre you talking to a
machine? dataset and methods for multilingual image question answering,â€ in
<em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2015, pp. 2296â€“2304.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Y.Â Zhu, O.Â Groth, M.Â Bernstein, and L.Â Fei-Fei, â€œVisual7w: Grounded question
answering in images,â€ in <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016, pp. 4995â€“5004.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
P.Â Sharma, N.Â Ding, S.Â Goodman, and R.Â Soricut, â€œConceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning,â€
in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2018, pp. 2556â€“2565.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
N.Â Kai, L.Â Sharon, and Y.Â W. William, â€œFakeddit: A new multimodal benchmark
dataset for fine-grained fake news detection,â€ in <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">LREC</em>, 2020, pp.
6149â€“6157.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
S.Â Sheng, A.Â Singh, V.Â Goswami, J.Â Magana, T.Â Thrush, W.Â Galuba, D.Â Parikh, and
D.Â Kiela, â€œHuman-adversarial visual question answering,â€ in <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>,
2021, pp. 20â€‰346â€“20â€‰359.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
D.Â Kiela, M.Â Bartolo, Y.Â Nie, D.Â Kaushik, A.Â Geiger, Z.Â Wu, B.Â Vidgen,
G.Â Prasad, A.Â Singh, P.Â Ringshia <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œDynabench: Rethinking
benchmarking in nlp,â€ in <em id="bib.bib88.2.2" class="ltx_emph ltx_font_italic">NAACL</em>, 2021, pp. 4110â€“4124.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Z.Â Liang, W.Â Jiang, H.Â Hu, and J.Â Zhu, â€œLearning to contrast the
counterfactual samples for robust visual question answering,â€ in
<em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2020, pp. 3285â€“3292.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
L.Â Chen, X.Â Yan, J.Â Xiao, H.Â Zhang, S.Â Pu, and Y.Â Zhuang, â€œCounterfactual
samples synthesizing for robust visual question answering,â€ in <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">CVPR</em>,
2020, pp. 10â€‰797â€“10â€‰806.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
M.Â Lao, Y.Â Guo, W.Â Chen, N.Â Pu, and M.Â S. Lew, â€œVqa-bc: Robust visual question
answering via bidirectional chaining,â€ in <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>, 2022, pp.
4833â€“4837.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
H.Â Lancaster and E.Â Seneta, â€œChi-square distribution. encyclopedia of
biostatistics,â€ 2005.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
A.Â Agrawal, D.Â Batra, and D.Â Parikh, â€œAnalyzing the behavior of visual
question answering models,â€ in <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2016, pp. 1955â€“1960.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J.-H. Kim, J.Â Jun, and B.-T. Zhang, â€œBilinear attention networks,â€ in
<em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2018, pp. 1571â€“1581.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
J.Â Jiang, Z.Â Liu, Y.Â Liu, Z.Â Nan, and N.Â Zheng, â€œX-ggm: Graph generative
modeling for out-of-distribution generalization in visual question
answering,â€ in <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">ACM MM</em>, 2021, pp. 199â€“208.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
M.Â Qraitem, K.Â Saenko, and B.Â A. Plummer, â€œBias mimicking: A simple sampling
approach for bias mitigation,â€ in <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2023, pp. 20â€‰311â€“20â€‰320.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
G.Â E. Hinton, â€œTraining products of experts by minimizing contrastive
divergence,â€ <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Neural Computation</em>, vol.Â 14, no.Â 8, pp. 1771â€“1800,
2002.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
K.Â Kafle, M.Â Yousefhussien, and C.Â Kanan, â€œData augmentation for visual
question answering,â€ in <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">INLG</em>, 2017, pp. 198â€“202.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
J.Â Kil, C.Â Zhang, D.Â Xuan, and W.-L. Chao, â€œDiscovering the unknown knowns:
Turning implicit knowledge in the dataset into explicit training examples for
visual question answering,â€ in <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2021, pp. 6346â€“6361.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
S.Â Ramakrishnan, A.Â Agrawal, and S.Â Lee, â€œOvercoming language priors in visual
question answering with adversarial regularization,â€ in <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>,
2018, pp. 1548â€“1558.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
J.Â Wu and R.Â Mooney, â€œSelf-critical reasoning for robust visual question
answering,â€ <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, p. 8601â€“8611, 2019.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
R.Â R. Selvaraju, S.Â Lee, Y.Â Shen, H.Â Jin, S.Â Ghosh, L.Â Heck, D.Â Batra, and
D.Â Parikh, â€œTaking a hint: Leveraging explanations to make vision and
language models more grounded,â€ in <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019, pp. 2591â€“2600.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
C.Â Clark, M.Â Yatskar, and L.Â Zettlemoyer, â€œDonâ€™t take the easy way out:
Ensemble based methods for avoiding known dataset biases,â€ in <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>,
2019, pp. 4069â€“4082.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
D.Â Teney and A.Â v.Â d. Hengel, â€œActively seeking and learning from live data,â€
in <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp. 1940â€“1949.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
C.Â Jing, Y.Â Wu, X.Â Zhang, Y.Â Jia, and Q.Â Wu, â€œOvercoming language priors in
vqa via decomposed linguistic representations,â€ in <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020, pp.
11â€‰181â€“11â€‰188.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
E.Â Abbasnejad, D.Â Teney, A.Â Parvaneh, J.Â Shi, and A.Â v.Â d. Hengel,
â€œCounterfactual vision and language learning,â€ in <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020, pp.
10â€‰044â€“10â€‰054.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
D.Â Teney, E.Â Abbasnedjad, and A.Â vanÂ den Hengel, â€œLearning what makes a
difference from counterfactual examples and gradient supervision,â€ in
<em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020, pp. 580â€“599.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Y.Â Qiao, Z.Â Yu, and J.Â Liu, â€œRankvqa: Answer re-ranking for visual question
answering,â€ in <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">ICME</em>, 2020, pp. 1â€“6.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
R.Â Shrestha, K.Â Kafle, and C.Â Kanan, â€œA negative case analysis of visual
grounding methods for vqa,â€ in <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2020, pp. 8172â€“8181.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
I.Â Gat, I.Â Schwartz, A.Â Schwing, and T.Â Hazan, â€œRemoving bias in multi-modal
classifiers: Regularization by maximizing functional entropies,â€ in
<em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020, pp. 3197â€“3208.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
G.Â Kv and A.Â Mittal, â€œReducing language biases in visual question answering
with visually-grounded question encoder,â€ in <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020, pp. 18â€“34.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
D.Â Teney, K.Â Kafle, R.Â Shrestha, E.Â Abbasnejad, C.Â Kanan, and A.Â vanÂ den
Hengel, â€œOn the value of out-of-distribution testing: An example of
goodhartâ€™s law,â€ in <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020, pp. 407â€“417.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Z.Â Gan, Y.-C. Chen, L.Â Li, C.Â Zhu, Y.Â Cheng, and J.Â Liu, â€œLarge-scale
adversarial training for vision-and-language representation learning,â€
<em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, pp. 6616â€“6628, 2020.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
L.Â Li, Z.Â Gan, and J.Â Liu, â€œA closer look at the robustness of
vision-and-language pre-trained models,â€ <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv:2012.08673</em>, 2020.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
T.Â Gokhale, P.Â Banerjee, C.Â Baral, and Y.Â Yang, â€œMutant: A training paradigm
for out-of-distribution generalization in visual question answering,â€ in
<em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2020, pp. 878â€“892.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
C.Â Clark, M.Â Yatskar, and L.Â Zettlemoyer, â€œLearning to model and ignore
dataset bias with mixed capacity ensembles,â€ in <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>,
2020, pp. 3031â€“3045.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Y.Â Guo, L.Â Nie, Z.Â Cheng, F.Â Ji, J.Â Zhang, and A.Â D. Bimbo, â€œAdavqa:
Overcoming language priors with adapted margin cosine loss,â€ in
<em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">IJCAI</em>, 2021, pp. 708â€“714.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
A.Â Basu, S.Â Addepalli, and R.Â V. Babu, â€œRmlvqa: A margin loss approach for
visual question answering with language biases,â€ in <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2023, pp.
11â€‰671â€“11â€‰680.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
D.Â Teney, E.Â Abbasnejad, and A.Â vanÂ den Hengel, â€œUnshuffling data for improved
generalization in visual question answering,â€ in <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021, pp.
1417â€“1427.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Y.Â Niu, K.Â Tang, H.Â Zhang, Z.Â Lu, X.-S. Hua, and J.-R. Wen, â€œCounterfactual
vqa: A cause-effect look at language bias,â€ in <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021, pp.
12â€‰700â€“12â€‰710.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Y.Â Pan, Z.Â Li, L.Â Zhang, and J.Â Tang, â€œDistilling knowledge in causal
inference for unbiased visual question answering,â€ in <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">MMAsia</em>, 2021,
pp. 1â€“7.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
X.Â Han, S.Â Wang, C.Â Su, Q.Â Huang, and Q.Â Tian, â€œGreedy gradient ensemble for
robust visual question answering,â€ in <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021, pp. 1584â€“1593.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
C.Â Yang, S.Â Feng, D.Â Li, H.Â Shen, G.Â Wang, and B.Â Jiang, â€œLearning content and
context with language bias for visual question answering,â€ in <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">ICME</em>,
2021, pp. 1â€“6.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
M.Â Lao, Y.Â Guo, Y.Â Liu, and M.Â S. Lew, â€œA language prior based focal loss for
visual question answering,â€ in <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">ICME</em>, 2021, pp. 1â€“6.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
X.Â Zhu, Z.Â Mao, C.Â Liu, P.Â Zhang, B.Â Wang, and Y.Â Zhang, â€œOvercoming language
priors with self-supervised learning for visual question answering,â€ in
<em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">IJCAI</em>, 2021, pp. 1083â€“1089.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Z.Â Wen, G.Â Xu, M.Â Tan, Q.Â Wu, and Q.Â Wu, â€œDebiased visual question answering
from feature and sample perspectives,â€ in <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021, pp.
3784â€“3796.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Q.Â Si, Z.Â Lin, M.Â yuÂ Zheng, P.Â Fu, and W.Â Wang, â€œCheck it again: Progressive
visual question answering via visual entailment,â€ in <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2021, pp.
4101â€“4110.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Y.Â Guo, L.Â Nie, Z.Â Cheng, Q.Â Tian, and M.Â Zhang, â€œLoss re-scaling vqa:
Revisiting the language prior problem from a class-imbalance view,â€
<em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">IEEE TIP</em>, pp. 227â€“238, 2022.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
C.Â Kolling, M.Â More, N.Â Gavenski, E.Â Pooch, O.Â Parraga, and R.Â C. Barros,
â€œEfficient counterfactual debiasing for visual question answering,â€ in
<em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">WACV</em>, 2022, pp. 3001â€“3010.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Q.Â Si, Y.Â Liu, F.Â Meng, Z.Â Lin, P.Â Fu, Y.Â Cao, W.Â Wang, and J.Â Zhou, â€œTowards
robust visual question answering: Making the most of biased samples via
contrastive learning,â€ in <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>, 2022, pp. 6650â€“6662.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
L.Â Chen, Y.Â Zheng, and J.Â Xiao, â€œRethinking data augmentation for robust
visual question answering,â€ in <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2022, pp. 95â€“112.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Y.Â Liu, Y.Â Guo, J.Â Yin, X.Â Song, W.Â Liu, L.Â Nie, and M.Â Zhang, â€œAnswer
questions with right image regions: A visual attention regularization
approach,â€ <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">ACM TOMM</em>, pp. 1â€“18, 2022.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
X.Â Han, S.Â Wang, C.Â Su, Q.Â Huang, and Q.Â Tian, â€œGeneral greedy de-bias
learning,â€ <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, pp. 1â€“17, 2023.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
J.Â W. Cho, D.-j. Kim, H.Â Ryu, and I.Â S. Kweon, â€œGenerative bias for visual
question answering,â€ in <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2023.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
J.Â Liu, C.Â Fan, F.Â Zhou, and H.Â Xu, â€œBe flexible! learn to debias by sampling
and prompting for robust visual question answering,â€ <em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">IPM</em>, vol.Â 60, p.
103296, 2023.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
W.Â Wang, J.Â Gao, and C.Â Xu, â€œWeakly-supervised video object grounding via
causal intervention,â€ <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 45, no.Â 3, pp. 3933â€“3948,
2023.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
X.Â Wang, Y.Â Wu, A.Â Zhang, F.Â Feng, X.Â He, and T.-S. Chua, â€œReinforced causal
explainer for graph neural networks,â€ <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 45, no.Â 2, pp.
2297â€“2309, 2023.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
L.Â Wang and K.-J. Yoon, â€œKnowledge distillation and student-teacher learning
for visual intelligence: A review and new outlooks,â€ <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>,
vol.Â 44, no.Â 6, pp. 3048â€“3068, 2022.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
H.-J. Ye, S.Â Lu, and D.-C. Zhan, â€œGeneralized knowledge distillation via
relationship matching,â€ <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol.Â 45, no.Â 2, pp. 1817â€“1834,
2023.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Y.Â Bengio, J.Â Louradour, R.Â Collobert, and J.Â Weston, â€œCurriculum learning,â€
in <em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2009, pp. 41â€“48.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
G.Â Gao, H.Â Huang, C.Â Fu, Z.Â Li, and R.Â He, â€œInformation bottleneck
disentanglement for identity swapping,â€ in <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021, pp.
3404â€“3413.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
T.Â Chen, S.Â Kornblith, M.Â Norouzi, and G.Â Hinton, â€œA simple framework for
contrastive learning of visual representations,â€ in <em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020, pp.
1597â€“1607.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
J.Â Park and J.Â Johnson, â€œRgb no more: Minimally-decoded jpeg vision
transformers,â€ in <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2023, pp. 22â€‰334â€“22â€‰346.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
R.Â R. Shetty, M.Â Fritz, and B.Â Schiele, â€œAdversarial scene editing: Automatic
object removal from weak supervision,â€ in <em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2018, pp.
7717â€“7727.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
C.Â Finn, P.Â Abbeel, and S.Â Levine, â€œModel-agnostic meta-learning for fast
adaptation of deep networks,â€ in <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2017, pp. 1126â€“1135.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
P.-S. Huang, C.Â Wang, R.Â Singh, W.-t. Yih, and X.Â He, â€œNatural language to
structured query generation via meta-learning,â€ in <em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2018, pp.
732â€“738.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
D.Â Teney and A.Â vanÂ den Hengel, â€œVisual question answering as a meta-learning
task,â€ in <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2018, pp. 219â€“235.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
X.Â Wang and G.-J. Qi, â€œContrastive learning with stronger augmentations,â€
<em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">IEEE TPMAI</em>, vol.Â 45, no.Â 5, pp. 5549â€“5560, 2023.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
C.-Y. Chuang, J.Â Robinson, Y.-C. Lin, A.Â Torralba, and S.Â Jegelka, â€œDebiased
contrastive learning,â€ in <em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020, pp. 8765â€“8775.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
P.Â Khosla, P.Â Teterwak, C.Â Wang, A.Â Sarna, Y.Â Tian, P.Â Isola, A.Â Maschinot,
C.Â Liu, and D.Â Krishnan, â€œSupervised contrastive learning,â€ in
<em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol.Â 33, 2020, pp. 18â€‰661â€“18â€‰673.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
P.Â Liu, W.Â Yuan, J.Â Fu, Z.Â Jiang, H.Â Hayashi, and G.Â Neubig, â€œPre-train,
prompt, and predict: A systematic survey of prompting methods in natural
language processing,â€ <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, vol.Â 55, no.Â 9, pp.
1â€“35, 2023.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
K.Â Zhou, J.Â Yang, C.Â C. Loy, and Z.Â Liu, â€œConditional prompt learning for
vision-language models,â€ in <em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022, pp. 16â€‰816â€“16â€‰825.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
P.Â Xu, X.Â Zhu, and D.Â A. Clifton, â€œMultimodal learning with transformers: A
survey,â€ <em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, 2023.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
W.Â Kim, B.Â Son, and I.Â Kim, â€œVilt: Vision-and-language transformer without
convolution or region supervision,â€ in <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2021, pp. 5583â€“5594.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
N.Â Yu, X.Â Hu, B.Â Song, J.Â Yang, and J.Â Zhang, â€œTopic-oriented image captioning
based on order-embedding,â€ <em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">IEEE TIP</em>, vol.Â 28, no.Â 6, pp. 2743â€“2754,
2018.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
S.Â Ye, J.Â Han, and N.Â Liu, â€œAttentive linear transformation for image
captioning,â€ <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">IEEE TIP</em>, vol.Â 27, no.Â 11, pp. 5514â€“5524, 2018.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
M.Â Yang, J.Â Liu, Y.Â Shen, Z.Â Zhao, X.Â Chen, Q.Â Wu, and C.Â Li, â€œAn ensemble of
generation-and retrieval-based image captioning with dual generator
generative adversarial network,â€ <em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">IEEE TIP</em>, vol.Â 29, pp. 9627â€“9640,
2020.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
H.Â Song, L.Â Dong, W.Â Zhang, T.Â Liu, and F.Â Wei, â€œClip models are few-shot
learners: Empirical studies on vqa and visual entailment,â€ in <em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">ACL</em>,
2022, pp. 6088â€“6100.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
B.Â Cao, J.Â Cao, J.Â Gui, J.Â Shen, B.Â Liu, L.Â He, Y.Â Y. Tang, and J.Â T.-Y. Kwok,
â€œAlignve: Visual entailment recognition based on alignment relations,â€
<em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">IEEE TMM</em>, 2022.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
J.Â Lu, D.Â Batra, D.Â Parikh, and S.Â Lee, â€œVilbert: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks,â€ in
<em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2019, pp. 13â€“23.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
W.Â Su, X.Â Zhu, Y.Â Cao, B.Â Li, L.Â Lu, F.Â Wei, and J.Â Dai, â€œVl-bert:
Pre-training of generic visual-linguistic representations,â€ in <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">ICLR</em>,
2020.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
X.Â Li, X.Â Yin, C.Â Li, P.Â Zhang, X.Â Hu, L.Â Zhang, L.Â Wang, H.Â Hu, L.Â Dong,
F.Â Wei <em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œOscar: Object-semantics aligned pre-training for
vision-language tasks,â€ in <em id="bib.bib162.2.2" class="ltx_emph ltx_font_italic">ECCV</em>, 2020, pp. 121â€“137.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim, C.Â Hallacy, A.Â Ramesh, G.Â Goh, S.Â Agarwal, G.Â Sastry,
A.Â Askell, P.Â Mishkin, J.Â Clark <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œLearning transferable visual
models from natural language supervision,â€ in <em id="bib.bib163.2.2" class="ltx_emph ltx_font_italic">ICML</em>, 2021, pp.
8748â€“8763.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
J.Â Li, R.Â Selvaraju, A.Â Gotmare, S.Â Joty, C.Â Xiong, and S.Â C.Â H. Hoi, â€œAlign
before fuse: Vision and language representation learning with momentum
distillation,â€ in <em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021, pp. 9694â€“9705.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
H.Â Bao, W.Â Wang, L.Â Dong, Q.Â Liu, O.Â K. Mohammed, K.Â Aggarwal, S.Â Som, S.Â Piao,
and F.Â Wei, â€œVlmo: Unified vision-language pre-training with
mixture-of-modality-experts,â€ in <em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022, pp. 32â€‰897â€“32â€‰912.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Z.Â Wang, J.Â Yu, A.Â W. Yu, Z.Â Dai, Y.Â Tsvetkov, and Y.Â Cao, â€œSimvlm: Simple
visual language model pretraining with weak supervision,â€ in <em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">ICLR</em>,
2022.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
J.Â Yu, Z.Â Wang, V.Â Vasudevan, L.Â Yeung, M.Â Seyedhosseini, and Y.Â Wu, â€œCoca:
Contrastive captioners are image-text foundation models,â€ <em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">TMLR</em>, vol.
2022, 2022.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
J.Â Li, D.Â Li, C.Â Xiong, and S.Â Hoi, â€œBlip: Bootstrapping language-image
pre-training for unified vision-language understanding and generation,â€ in
<em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2022, pp. 12â€‰888â€“12â€‰900.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
W.Â Wang, H.Â Bao, L.Â Dong, J.Â Bjorck, Z.Â Peng, Q.Â Liu, K.Â Aggarwal, O.Â K.
Mohammed, S.Â Singhal, S.Â Som <em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œImage as a foreign language:
Beit pretraining for vision and vision-language tasks,â€ in <em id="bib.bib169.2.2" class="ltx_emph ltx_font_italic">CVPR</em>,
2023, pp. 19â€‰175â€“19â€‰186.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
X.Â Chen, X.Â Wang, S.Â Changpinyo, A.Â Piergiovanni, P.Â Padlewski, D.Â Salz,
S.Â Goodman, A.Â Grycner, B.Â Mustafa, L.Â Beyer <em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œPali: A
jointly-scaled multilingual language-image model,â€ <em id="bib.bib170.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2209.06794</em>, 2022.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
J.Â Li, D.Â Li, S.Â Savarese, and S.Â Hoi, â€œBlip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models,â€
<em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12597</em>, 2023.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
A.Â Fang, G.Â Ilharco, M.Â Wortsman, Y.Â Wan, V.Â Shankar, A.Â Dave, and L.Â Schmidt,
â€œData determines distributional robustness in contrastive language image
pre-training (clip),â€ in <em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2022, pp. 6216â€“6234.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
M.Â Zhou, L.Â Yu, A.Â Singh, M.Â Wang, Z.Â Yu, and N.Â Zhang, â€œUnsupervised
vision-and-language pre-training via retrieval-based multi-granular
alignment,â€ in <em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022, pp. 16â€‰485â€“16â€‰494.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
S.Â Ren, K.Â He, R.Â Girshick, and J.Â Sun, â€œFaster r-cnn: Towards real-time
object detection with region proposal networks,â€ in <em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2015,
pp. 91â€“99.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
J.Â Redmon, S.Â Divvala, R.Â Girshick, and A.Â Farhadi, â€œYou only look once:
Unified, real-time object detection,â€ in <em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2016, pp. 779â€“788.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
L.Â Dong, N.Â Yang, W.Â Wang, F.Â Wei, X.Â Liu, Y.Â Wang, J.Â Gao, M.Â Zhou, and H.-W.
Hon, â€œUnified language model pre-training for natural language understanding
and generation,â€ in <em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2019, pp. 13â€‰063â€“13â€‰075.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
H.Â W. Chung, T.Â Fevry, H.Â Tsai, M.Â Johnson, and S.Â Ruder, â€œRethinking
embedding coupling in pre-trained language models,â€ in <em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2020.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
B.Â Wang, X.Â Deng, and H.Â Sun, â€œIteratively prompt pre-trained language models
for chain of thought,â€ in <em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2022, pp. 2714â€“2730.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
A.Â Dosovitskiy, L.Â Beyer, A.Â Kolesnikov, D.Â Weissenborn, X.Â Zhai,
T.Â Unterthiner, M.Â Dehghani, M.Â Minderer, G.Â Heigold, S.Â Gelly <em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>,
â€œAn image is worth 16x16 words: Transformers for image recognition at
scale,â€ in <em id="bib.bib179.2.2" class="ltx_emph ltx_font_italic">ICLR</em>, 2021.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
H.Â Chen, Y.Â Wang, T.Â Guo, C.Â Xu, Y.Â Deng, Z.Â Liu, S.Â Ma, C.Â Xu, C.Â Xu, and
W.Â Gao, â€œPre-trained image processing transformer,â€ in <em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021,
pp. 12â€‰299â€“12â€‰310.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Z.Â Liu, J.Â Ning, Y.Â Cao, Y.Â Wei, Z.Â Zhang, S.Â Lin, and H.Â Hu, â€œVideo swin
transformer,â€ in <em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2022, pp. 3202â€“3211.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
J.Â Duan, L.Â Chen, S.Â Tran, J.Â Yang, Y.Â Xu, B.Â Zeng, and T.Â Chilimbi,
â€œMulti-modal alignment using representation codebook,â€ in <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">CVPR</em>,
2022, pp. 15â€‰651â€“15â€‰660.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
B.Â Mustafa, C.Â Riquelme, J.Â Puigcerver, R.Â Jenatton, and N.Â Houlsby,
â€œMultimodal contrastive learning with limoe: the language-image mixture of
experts,â€ in <em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Z.Â Wang, J.Â Yu, A.Â W. Yu, Z.Â Dai, Y.Â Tsvetkov, and Y.Â Cao, â€œSimvlm: Simple
visual language model pretraining with weak supervision,â€ in <em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">ICLR</em>,
2022.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
A.Â Li, A.Â Jabri, A.Â Joulin, and L.Â Van DerÂ Maaten, â€œLearning visual n-grams
from web data,â€ in <em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017, pp. 4183â€“4192.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
A.Â Kolesnikov, L.Â Beyer, X.Â Zhai, J.Â Puigcerver, J.Â Yung, S.Â Gelly, and
N.Â Houlsby, â€œLarge scale learning of general visual representations for
transfer,â€ <em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.11370</em>, 2019.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
X.Â Zhai, J.Â Puigcerver, A.Â Kolesnikov, P.Â Ruyssen, C.Â Riquelme, M.Â Lucic,
J.Â Djolonga, A.Â S. Pinto, M.Â Neumann, A.Â Dosovitskiy <em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œA
large-scale study of representation learning with the visual task adaptation
benchmark,â€ <em id="bib.bib187.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.04867</em>, 2019.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
T.Â Brown, B.Â Mann, N.Â Ryder, M.Â Subbiah, J.Â D. Kaplan, P.Â Dhariwal,
A.Â Neelakantan, P.Â Shyam, G.Â Sastry, A.Â Askell <em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œLanguage
models are few-shot learners,â€ in <em id="bib.bib188.2.2" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020, pp. 1877â€“1901.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
N.Â C. Mithun, R.Â Panda, E.Â E. Papalexakis, and A.Â K. Roy-Chowdhury, â€œWebly
supervised joint embedding for cross-modal image-text retrieval,â€ in
<em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">ACM MM</em>, 2018, pp. 1856â€“1864.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
X.Â He and Y.Â Peng, â€œFine-grained image classification via combining vision and
language,â€ in <em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017, pp. 5994â€“6002.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
T.Â Wang, L.Â Yuan, X.Â Zhang, and J.Â Feng, â€œDistilling object detectors with
fine-grained feature imitation,â€ in <em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2019, pp. 4933â€“4942.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
M.Â Zolfaghari, Y.Â Zhu, P.Â Gehler, and T.Â Brox, â€œCrossclr: Cross-modal
contrastive learning for multi-modal video representations,â€ in <em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">ICCV</em>,
2021, pp. 1450â€“1459.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
J.Â Wu, Y.Â Liang, H.Â Akbari, Z.Â Wang, C.Â Yu <em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œScaling multimodal
pre-training via cross-modality gradient harmonization,â€ in <em id="bib.bib193.2.2" class="ltx_emph ltx_font_italic">NeurIPS</em>,
2022, pp. 36â€‰161â€“36â€‰173.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
H.Â Akbari, L.Â Yuan, R.Â Qian, W.-H. Chuang, S.-F. Chang, Y.Â Cui, and B.Â Gong,
â€œVatt: Transformers for multimodal self-supervised learning from raw video,
audio and text,â€ in <em id="bib.bib194.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021, pp. 24â€‰206â€“24â€‰221.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
K.Â He, H.Â Fan, Y.Â Wu, S.Â Xie, and R.Â Girshick, â€œMomentum contrast for
unsupervised visual representation learning,â€ in <em id="bib.bib195.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020, pp.
9729â€“9738.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
A.Â Agrawal, I.Â KajiÄ‡, E.Â Bugliarello, E.Â Davoodi, A.Â Gergely, P.Â Blunsom,
and A.Â Nematzadeh, â€œRethinking evaluation practices in visual question
answering: A case study on out-of-distribution deneralization,â€ <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2205.12191</em>, 2022.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
S.Â Shen, L.Â H. Li, H.Â Tan, M.Â Bansal, A.Â Rohrbach, K.-W. Chang, Z.Â Yao, and
K.Â Keutzer, â€œHow much can CLIP benefit vision-and-language tasks?â€ in
<em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
X.Â Chen, J.Â Djolonga, P.Â Padlewski, B.Â Mustafa, S.Â Changpinyo, J.Â Wu, C.Â R.
Ruiz, S.Â Goodman, X.Â Wang, Y.Â Tay <em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œPaLI-X: On Scaling up a
Multilingual Vision and Language Model,â€ <em id="bib.bib198.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2305.18565</em>, 2023.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
H.Â Bao, W.Â Wang, L.Â Dong, Q.Â Liu, O.Â K. Mohammed, K.Â Aggarwal, S.Â Som, S.Â Piao,
and F.Â Wei, â€œVlmo: Unified vision-language pre-training with
mixture-of-modality-experts,â€ in <em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022, pp. 32â€‰897â€“32â€‰912.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
H.Â Bao, W.Â Wang, L.Â Dong, and F.Â Wei, â€œVl-beit: Generative vision-language
pretraining,â€ <em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.01127</em>, 2022.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
R.Â M. Monarch, <em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">Human-in-the-Loop Machine Learning: Active learning and
annotation for human-centered AI</em>.Â Â Â Simon and Schuster, 2021.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
M.Â Malinowski, M.Â Rohrbach, and M.Â Fritz, â€œAsk your neurons: A deep learning
approach to visual question answering,â€ <em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">IJCV</em>, vol. 125, pp. 110â€“135,
2017.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
A.Â Wang, A.Â Liu, R.Â Zhang, A.Â Kleiman, L.Â Kim, D.Â Zhao, I.Â Shirai,
A.Â Narayanan, and O.Â Russakovsky, â€œRevise: A tool for measuring and
mitigating bias in visual datasets,â€ <em id="bib.bib203.1.1" class="ltx_emph ltx_font_italic">IJCV</em>, vol. 130, no.Â 7, pp.
1790â€“1810, 2022.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
A.Â Mogadala, M.Â Kalimuthu, and D.Â Klakow, â€œTrends in integration of vision and
language research: A survey of tasks, datasets, and methods,â€ <em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">JAIR</em>,
vol.Â 71, pp. 1183â€“1317, 2021.

</span>
</li>
</ul>
</section>
<figure id="id1" class="ltx_float biography">
<table id="id1.1" class="ltx_tabular">
<tr id="id1.1.1" class="ltx_tr">
<td id="id1.1.1.1" class="ltx_td"><img src="/html/2307.11471/assets/jiema.jpg" id="id1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="123" alt="[Uncaptioned image]"></td>
<td id="id1.1.1.2" class="ltx_td">
<span id="id1.1.1.2.1" class="ltx_inline-block">
<span id="id1.1.1.2.1.1" class="ltx_p"><span id="id1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Jie Ma</span>  (Member, IEEE) is an Assistant Professor in the School of Cyber Science and Engineering (Faculty of Electronic and Information Engineering) at Xiâ€™an Jiaotong University, Xiâ€™an, Shaanxi 710049, P.R. China. He is also a member of the Ministry of Educationâ€™s Key Lab for Intelligent Networks and Network Security. His research interests cover natural language processing and trustworthy multimodality learning, focusing particularly on knowledge graph learning, robust visual question answering, and question dialogue. He has contributed to several top journals and conferences, including IEEE TIP, TNNLS, IJCAI, and WSDM. Furthermore, he has served as a program committee member for numerous conferences such as ICLR and AAAI, and reviewed manuscripts for multiple journals such as IEEE TIP and TNNLS. For more information, please visit <a target="_blank" href="https://dr-majie.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dr-majie.github.io/</a>.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id2" class="ltx_float biography">
<table id="id2.1" class="ltx_tabular">
<tr id="id2.1.1" class="ltx_tr">
<td id="id2.1.1.1" class="ltx_td"><img src="/html/2307.11471/assets/pinghui.png" id="id2.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="82" height="125" alt="[Uncaptioned image]"></td>
<td id="id2.1.1.2" class="ltx_td">
<span id="id2.1.1.2.1" class="ltx_inline-block">
<span id="id2.1.1.2.1.1" class="ltx_p"><span id="id2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Pinghui Wang</span> 
(Senior Member, IEEE) is currently a professor with the MOE Key Laboratory for Intelligent Networks and Network Security, Xiâ€™an Jiaotong University, Xiâ€™an, China, and also with the Shenzhen Research Institute, Xiâ€™an Jiaotong University, Shenzhen, China. His research interests include internet traffic measurement and modeling, traffic classification, abnormal detection, and online social network measurement.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id3" class="ltx_float biography">
<table id="id3.1" class="ltx_tabular">
<tr id="id3.1.1" class="ltx_tr">
<td id="id3.1.1.1" class="ltx_td"><img src="/html/2307.11471/assets/dechen.jpg" id="id3.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="94" height="125" alt="[Uncaptioned image]"></td>
<td id="id3.1.1.2" class="ltx_td">
<span id="id3.1.1.2.1" class="ltx_inline-block">
<span id="id3.1.1.2.1.1" class="ltx_p"><span id="id3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Dechen Kong</span> 
received the BE degree in Electrical Engineering and Automation from Xiâ€™an Jiaotong University, China, in 2022. He is currently working toward the ME degree in Electronic and Information Engineering in Xiâ€™an Jiaotong University. His research interests include multimodal learning, natural language processing and computer vision.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id4" class="ltx_float biography">
<table id="id4.1" class="ltx_tabular">
<tr id="id4.1.1" class="ltx_tr">
<td id="id4.1.1.1" class="ltx_td"><img src="/html/2307.11471/assets/zeweiwang.jpg" id="id4.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="91" height="125" alt="[Uncaptioned image]"></td>
<td id="id4.1.1.2" class="ltx_td">
<span id="id4.1.1.2.1" class="ltx_inline-block">
<span id="id4.1.1.2.1.1" class="ltx_p"><span id="id4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Zewei Wang</span> 
received the BE degree in automation from Xiâ€™an Jiaotong University, China, in 2022. He is currently working toward the ME degree in control science and engineering with the School of Electronic and Information Engineering, Xiâ€™an Jiaotong University. His research interests include multimodal learning, knowledge graph learning, and visual question answering.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id5" class="ltx_float biography">
<table id="id5.1" class="ltx_tabular">
<tr id="id5.1.1" class="ltx_tr">
<td id="id5.1.1.1" class="ltx_td"><img src="/html/2307.11471/assets/x16.png" id="id5.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="68" height="96" alt="[Uncaptioned image]"></td>
<td id="id5.1.1.2" class="ltx_td">
<span id="id5.1.1.2.1" class="ltx_inline-block">
<span id="id5.1.1.2.1.1" class="ltx_p"><span id="id5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Jun Liu</span> 
(Senior Member, IEEE) received the B.S. in computer science and technology in 1995 and the Ph.D. degree in systems engineering in 2004, both from Xiâ€™an Jiaotong University, China. He is currently a Professor with the Department of Computer Science, Xiâ€™an Jiaotong University. He has authored more than ninety research papers in various journals and conference proceedings. He has won the best paper awards in IEEE ISSRE 2016 and IEEE ICBK 2016. His research interests include NLP and e-learning. Dr. Liu currently serves as an associate editor of IEEE TNNLS from 2020, and has served as a guest editor for many technical journals, such as Information Fusion, IEEE SYSTEMS JOURNAL. He also acted as a conference/workshop/track chair at numerous conferences.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id6" class="ltx_float biography">
<table id="id6.1" class="ltx_tabular">
<tr id="id6.1.1" class="ltx_tr">
<td id="id6.1.1.1" class="ltx_td"><img src="/html/2307.11471/assets/honbin.jpg" id="id6.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="89" height="125" alt="[Uncaptioned image]"></td>
<td id="id6.1.1.2" class="ltx_td">
<span id="id6.1.1.2.1" class="ltx_inline-block">
<span id="id6.1.1.2.1.1" class="ltx_p"><span id="id6.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Hongbin Pei</span> 
is currently an Assistant Professor in the School of Cyber Science and Engineering at Xiâ€™an Jiaotong University, China. He received his B.S., M.S., and Ph.D. degrees from Jilin University, China, in 2012, 2015, and 2021, respectively. His research interests include Deep Learning, Graph Data Mining, and Data-Driven Complex System Modeling, with Applications to Chemistry and Public Health.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id7" class="ltx_float biography">
<table id="id7.1" class="ltx_tabular">
<tr id="id7.1.1" class="ltx_tr">
<td id="id7.1.1.1" class="ltx_td"><img src="/html/2307.11471/assets/junzhou.jpg" id="id7.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="90" height="125" alt="[Uncaptioned image]"></td>
<td id="id7.1.1.2" class="ltx_td">
<span id="id7.1.1.2.1" class="ltx_inline-block">
<span id="id7.1.1.2.1.1" class="ltx_p"><span id="id7.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Junzhou Zhao</span> 
received BS and PhD degrees in control science and engineering from Xiâ€™an Jiaotong University, in 2008 and 2015, respectively. He is currently an associate professor with the School of Cyber Science and Engineering, at Xiâ€™an Jiaotong University. His research interests include graph data mining and streaming data processing.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.11470" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.11471" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.11471">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.11471" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.11472" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 16:52:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
