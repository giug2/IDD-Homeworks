<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>\thetable Comparing database performance and tuning efficiency among various knob recommendation methods. “Default” denotes the use of the default configuration.</title>
<!--Generated on Mon Aug  5 03:18:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.02213v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.02213v1#id3" title=""><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thesubsection</span> </span>Result</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\section</span>
<p class="ltx_p" id="p1.2">Performance Comparison on different LLMs(Q4)
ChatGPT has garnered significant interest from the AI community since its release due to its exceptional ability to communicate with humans. Notably, ChatGPT [LLM_survey_Chinese 84] and GPT-4 [45] have significantly enhanced existing capabilities and are considered milestones in the field. Our work primarily revolves around these two models. However, with the continuous development of large language model (LLM) technology, a variety of LLMs have emerged in the market, such as PaLM [huangXM 6] , LLaMa [34], etc. How do these large language models perform in tuning tasks compared to ChatGPT? How should we choose the appropriate LLM for knob tuning tasks? To explore these questions, this section will compare the performance of different LLMs in knob tuning.</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\subsection</span>
<p class="ltx_p" id="p2.2">Setup</p>
</div>
<figure class="ltx_table" id="tab1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table <span class="ltx_ERROR undefined" id="tab1.5.1.1">\thetable</span>: </span>Comparing database performance and tuning efficiency among various knob recommendation methods. “Default” denotes the use of the default configuration.</figcaption>
<table class="ltx_tabular ltx_align_middle" id="tab1.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="tab1.6.1.1">
<td class="ltx_td ltx_align_center" id="tab1.6.1.1.1">
<span class="ltx_ERROR undefined" id="tab1.6.1.1.1.1">\toprule</span><span class="ltx_text" id="tab1.6.1.1.1.2" style="font-size:90%;">Type</span>
</td>
<td class="ltx_td ltx_align_center" id="tab1.6.1.1.2"><span class="ltx_text" id="tab1.6.1.1.2.1" style="font-size:90%;">Method</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.1.1.3"><span class="ltx_text" id="tab1.6.1.1.3.1" style="font-size:90%;">IR TPS</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.1.1.4"><span class="ltx_text" id="tab1.6.1.1.4.1" style="font-size:90%;">ODP</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.1.1.5"><span class="ltx_text" id="tab1.6.1.1.5.1" style="font-size:90%;">TES</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.2.2">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="tab1.6.2.2.1">
<span class="ltx_ERROR undefined" id="tab1.6.2.2.1.1">\midrule</span><span class="ltx_ERROR undefined" id="tab1.6.2.2.1.2">\multirow</span><span class="ltx_text" id="tab1.6.2.2.1.3" style="font-size:90%;">4*</span><span class="ltx_ERROR undefined" id="tab1.6.2.2.1.4">\makecell</span><span class="ltx_text" id="tab1.6.2.2.1.5" style="font-size:90%;">[c]Traditional</span>
</td>
<td class="ltx_td" id="tab1.6.2.2.2"></td>
<td class="ltx_td" id="tab1.6.2.2.3"></td>
<td class="ltx_td" id="tab1.6.2.2.4"></td>
<td class="ltx_td" id="tab1.6.2.2.5"></td>
</tr>
<tr class="ltx_tr" id="tab1.6.3.3">
<td class="ltx_td ltx_align_right" id="tab1.6.3.3.1"><span class="ltx_text" id="tab1.6.3.3.1.1" style="font-size:90%;">Method</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.3.3.2"><span class="ltx_text" id="tab1.6.3.3.2.1" style="font-size:90%;">Default</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.3.3.3"><span class="ltx_text" id="tab1.6.3.3.3.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.3.3.4"><span class="ltx_text" id="tab1.6.3.3.4.1" style="font-size:90%;">17.45</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.3.3.5"><span class="ltx_text" id="tab1.6.3.3.5.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.4.4">
<td class="ltx_td" id="tab1.6.4.4.1"></td>
<td class="ltx_td ltx_align_center" id="tab1.6.4.4.2"><span class="ltx_text" id="tab1.6.4.4.2.1" style="font-size:90%;">DDPG</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.4.4.3"><span class="ltx_text" id="tab1.6.4.4.3.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.4.4.4"><span class="ltx_text" id="tab1.6.4.4.4.1" style="font-size:90%;">120.71</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.4.4.5"><span class="ltx_text ltx_font_bold" id="tab1.6.4.4.5.1" style="font-size:90%;">99</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.5.5">
<td class="ltx_td" id="tab1.6.5.5.1"></td>
<td class="ltx_td ltx_align_center" id="tab1.6.5.5.2"><span class="ltx_text" id="tab1.6.5.5.2.1" style="font-size:90%;">SMAC</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.5.5.3"><span class="ltx_text" id="tab1.6.5.5.3.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.5.5.4"><span class="ltx_text ltx_font_bold" id="tab1.6.5.5.4.1" style="font-size:90%;">157.25</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.5.5.5"><span class="ltx_text" id="tab1.6.5.5.5.1" style="font-size:90%;">375</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.6.6">
<td class="ltx_td" id="tab1.6.6.6.1"></td>
<td class="ltx_td ltx_align_center" id="tab1.6.6.6.2"><span class="ltx_text" id="tab1.6.6.6.2.1" style="font-size:90%;">VBO</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.6.6.3"><span class="ltx_text" id="tab1.6.6.6.3.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.6.6.4"><span class="ltx_text" id="tab1.6.6.6.4.1" style="font-size:90%;">155.10</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.6.6.5"><span class="ltx_text" id="tab1.6.6.6.5.1" style="font-size:90%;">316</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.7.7">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="tab1.6.7.7.1">
<span class="ltx_ERROR undefined" id="tab1.6.7.7.1.1">\midrule</span><span class="ltx_ERROR undefined" id="tab1.6.7.7.1.2">\midrule</span><span class="ltx_ERROR undefined" id="tab1.6.7.7.1.3">\multirow</span><span class="ltx_text" id="tab1.6.7.7.1.4" style="font-size:90%;">4*</span><span class="ltx_ERROR undefined" id="tab1.6.7.7.1.5">\makecell</span><span class="ltx_text" id="tab1.6.7.7.1.6" style="font-size:90%;">[c]Closed Source</span>
</td>
<td class="ltx_td" id="tab1.6.7.7.2"></td>
<td class="ltx_td" id="tab1.6.7.7.3"></td>
<td class="ltx_td" id="tab1.6.7.7.4"></td>
<td class="ltx_td" id="tab1.6.7.7.5"></td>
</tr>
<tr class="ltx_tr" id="tab1.6.8.8">
<td class="ltx_td ltx_align_right" id="tab1.6.8.8.1"><span class="ltx_text" id="tab1.6.8.8.1.1" style="font-size:90%;">LLM</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.8.8.2"><span class="ltx_text" id="tab1.6.8.8.2.1" style="font-size:90%;">GPT-3.5</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.8.8.3"><span class="ltx_text" id="tab1.6.8.8.3.1" style="font-size:90%;">16.38</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.8.8.4"><span class="ltx_text" id="tab1.6.8.8.4.1" style="font-size:90%;">116.62</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.8.8.5"><span class="ltx_text" id="tab1.6.8.8.5.1" style="font-size:90%;">24</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.9.9">
<td class="ltx_td" id="tab1.6.9.9.1"></td>
<td class="ltx_td ltx_align_center" id="tab1.6.9.9.2"><span class="ltx_text" id="tab1.6.9.9.2.1" style="font-size:90%;">GPT-4-Turbo</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.9.9.3"><span class="ltx_text ltx_font_bold" id="tab1.6.9.9.3.1" style="font-size:90%;">145.06</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.9.9.4"><span class="ltx_text ltx_font_bold" id="tab1.6.9.9.4.1" style="font-size:90%;">155.30</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.9.9.5"><span class="ltx_text ltx_font_bold" id="tab1.6.9.9.5.1" style="font-size:90%;">9</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.10.10">
<td class="ltx_td" id="tab1.6.10.10.1"></td>
<td class="ltx_td ltx_align_center" id="tab1.6.10.10.2"><span class="ltx_text" id="tab1.6.10.10.2.1" style="font-size:90%;">GPT-4o</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.10.10.3"><span class="ltx_text" id="tab1.6.10.10.3.1" style="font-size:90%;">117.96</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.10.10.4"><span class="ltx_text" id="tab1.6.10.10.4.1" style="font-size:90%;">126.05</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.10.10.5"><span class="ltx_text" id="tab1.6.10.10.5.1" style="font-size:90%;">13</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.11.11">
<td class="ltx_td" id="tab1.6.11.11.1"></td>
<td class="ltx_td ltx_align_center" id="tab1.6.11.11.2"><span class="ltx_text" id="tab1.6.11.11.2.1" style="font-size:90%;">Claude-3-Opus</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.11.11.3"><span class="ltx_text" id="tab1.6.11.11.3.1" style="font-size:90%;">26.70</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.11.11.4"><span class="ltx_text" id="tab1.6.11.11.4.1" style="font-size:90%;">148.51</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.11.11.5"><span class="ltx_text" id="tab1.6.11.11.5.1" style="font-size:90%;">23</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.12.12">
<td class="ltx_td ltx_nopad_r ltx_align_right" id="tab1.6.12.12.1">
<span class="ltx_ERROR undefined" id="tab1.6.12.12.1.1">\midrule</span><span class="ltx_ERROR undefined" id="tab1.6.12.12.1.2">\midrule</span><span class="ltx_ERROR undefined" id="tab1.6.12.12.1.3">\multirow</span><span class="ltx_text" id="tab1.6.12.12.1.4" style="font-size:90%;">3*</span><span class="ltx_ERROR undefined" id="tab1.6.12.12.1.5">\makecell</span><span class="ltx_text" id="tab1.6.12.12.1.6" style="font-size:90%;">[c]Open Source</span>
</td>
<td class="ltx_td" id="tab1.6.12.12.2"></td>
<td class="ltx_td" id="tab1.6.12.12.3"></td>
<td class="ltx_td" id="tab1.6.12.12.4"></td>
<td class="ltx_td" id="tab1.6.12.12.5"></td>
</tr>
<tr class="ltx_tr" id="tab1.6.13.13">
<td class="ltx_td ltx_align_right" id="tab1.6.13.13.1"><span class="ltx_text" id="tab1.6.13.13.1.1" style="font-size:90%;">LLM</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.13.13.2"><span class="ltx_text" id="tab1.6.13.13.2.1" style="font-size:90%;">Llama3-8B-Instruct</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.13.13.3"><span class="ltx_text" id="tab1.6.13.13.3.1" style="font-size:90%;">20.90</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.13.13.4"><span class="ltx_text" id="tab1.6.13.13.4.1" style="font-size:90%;">125.88</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.13.13.5"><span class="ltx_text" id="tab1.6.13.13.5.1" style="font-size:90%;">25</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.14.14">
<td class="ltx_td" id="tab1.6.14.14.1"></td>
<td class="ltx_td ltx_align_center" id="tab1.6.14.14.2"><span class="ltx_text" id="tab1.6.14.14.2.1" style="font-size:90%;">LlaMa3-70B-Instruct</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.14.14.3"><span class="ltx_text" id="tab1.6.14.14.3.1" style="font-size:90%;">28.84</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.14.14.4"><span class="ltx_text" id="tab1.6.14.14.4.1" style="font-size:90%;">145.12</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.14.14.5"><span class="ltx_text ltx_font_bold" id="tab1.6.14.14.5.1" style="font-size:90%;">3</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.15.15">
<td class="ltx_td" id="tab1.6.15.15.1"></td>
<td class="ltx_td ltx_align_center" id="tab1.6.15.15.2"><span class="ltx_text" id="tab1.6.15.15.2.1" style="font-size:90%;">Qwen2-7B</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.15.15.3"><span class="ltx_text ltx_font_bold" id="tab1.6.15.15.3.1" style="font-size:90%;">143.58</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.15.15.4"><span class="ltx_text ltx_font_bold" id="tab1.6.15.15.4.1" style="font-size:90%;">154.94</span></td>
<td class="ltx_td ltx_align_center" id="tab1.6.15.15.5"><span class="ltx_text" id="tab1.6.15.15.5.1" style="font-size:90%;">14</span></td>
</tr>
<tr class="ltx_tr" id="tab1.6.16.16">
<td class="ltx_td ltx_align_center" id="tab1.6.16.16.1"><span class="ltx_ERROR undefined" id="tab1.6.16.16.1.1">\bottomrule</span></td>
<td class="ltx_td" id="tab1.6.16.16.2"></td>
<td class="ltx_td" id="tab1.6.16.16.3"></td>
<td class="ltx_td" id="tab1.6.16.16.4"></td>
<td class="ltx_td" id="tab1.6.16.16.5"></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">In this section, the LLMs we use include ChatGPT-3.5, ChatGPT-4, Kimi, LLaMA, among others. From a scale perspective, these models can be divided into two categories: those with tens of billions of parameters and those with hundreds of billions of parameters. From a usage perspective, they can be categorized as either publicly accessible or closed-source. Detailed information about each model is shown in <span class="ltx_ERROR undefined" id="p3.1.1">\autoref</span>tab:llms.</p>
</div>
<section class="ltx_subsection" id="id3">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_ERROR undefined" id="id3.4.1.1">\thesubsection</span> </span>Result</h2>
<div class="ltx_para" id="id1.p1">
<p class="ltx_p" id="id1.p1.1">The test results are shown in <span class="ltx_ERROR undefined" id="id1.p1.1.1">\autoref</span>fig:llms. It can be seen that GPT-4 achieved the best results among all methods. This is not surprisingly cause GPT-4 has a larger scale, more comprehensive corpus, stronger ability to solve complex tasks, and shows significant performance improvement in many evaluation tasks compared to other models. A recent study [LLM _survey-C 40] investigate the performance of GPT-4 by conducting qualitative tests on human generated problems
The tests in cover various difficult tasks, and the results show that GPT-4 has better performance than previous GPT models. By comparing and analyzing the performance of other LLMs, it is not difficult to find that (1) the performance of closed source LLMs far exceeds of open source LLMs, (2) the larger the number of parameters in LLM, the better their performance.</p>
</div>
<figure class="ltx_figure" id="id2.fig1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="id2.fig1.4">\includegraphics</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="id2.fig1.5">[width=0.48]figures/LLMs.png</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure <span class="ltx_ERROR undefined" id="id2.fig1.7.1.1">\thefigure</span>: </span>Architecture of Configuration Tuning System</figcaption>
</figure>
<div class="ltx_para" id="id3.p2">
<p class="ltx_p" id="id3.p2.1">The main reasons for the previous pattern include (1) corpus, (2)training and (3)fine-tuning. In terms of corpus, the quantity and quality of corpus used during pretraining have a significant impact on performance. Chinchilla [survey 33] indicates that many existing LLMs suffer from suboptimal training due to a lack of sufficient pretraining data. When pre training low-quality corpora, such as noisy, harmful, and repetitive data, it may also damage the performance of the model [survey 59, 158, 160, 163]. Compared to open source models, closed source models such as GPT often have more abundant and high-quality corpus. In terms of training, most closed source models are released by enterprises such as OpenAI, which have sufficient time, hardware and other resources to fully train the model and unleash its potential. In addition, large enterprises also have more sufficient trial and error costs and the ability to explore different model architectures. In terms of fine-tuning, high-quality human feedback is crucial for aligning LLM with human preferences and values, helping LLM to further adapt its universal ability to solve various tasks to specific goals, such as the database parameter tuning that this article focuses on. And these high-quality human feedback also require human and financial support, which is exactly the advantage of enterprises.
The latter rule is easier to understand, as researchers in the President train Langauge Model (PLM) stage have found that extending PLM (such as expanding model size or data size) usually improves the model performance of downstream tasks (i.e. following the extension rule [30]). Many studies have explored performance limits by training increasingly large PLMs, such as GPT-3 with 175 billion parameters and PaLM with 540 billion parameters.
</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug  5 03:18:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
