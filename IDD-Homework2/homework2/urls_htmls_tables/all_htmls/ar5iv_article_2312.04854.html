<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="34" id="id1.g1" src="/html/2312.04854/assets/figures/title_1.jpg" width="34"/>
  <span class="ltx_text ltx_font_italic" id="id13.id1">
   Apollo’s Oracle:
  </span>
  <br class="ltx_break"/>
  Retrieval-Augmented Reasoning in Multi-Agent Debates
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Haotian Wang
    <sup class="ltx_sup" id="id14.12.id1">
     1
    </sup>
    , Xiyuan Du
    <sup class="ltx_sup" id="id15.13.id2">
     1
    </sup>
    , Weijiang Yu
    <sup class="ltx_sup" id="id16.14.id3">
     2
    </sup>
    , Qianglong Chen
    <sup class="ltx_sup" id="id17.15.id4">
     3
    </sup>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id10.9.5">
     Kun Zhu
     <sup class="ltx_sup" id="id10.9.5.1">
      <span class="ltx_text ltx_font_medium" id="id10.9.5.1.1">
       1
      </span>
     </sup>
     , Zheng Chu
     <sup class="ltx_sup" id="id10.9.5.2">
      <span class="ltx_text ltx_font_medium" id="id10.9.5.2.1">
       1
      </span>
     </sup>
     , Lian Yan
     <sup class="ltx_sup" id="id10.9.5.3">
      <span class="ltx_text ltx_font_medium" id="id10.9.5.3.1">
       1
      </span>
     </sup>
     , Yi Guan
     <sup class="ltx_sup" id="id10.9.5.4">
      <span class="ltx_text ltx_font_medium" id="id10.9.5.4.1">
       1
      </span>
     </sup>
     <span class="ltx_note ltx_role_footnotemark" id="footnotex1">
      <sup class="ltx_note_mark">
       1
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         1
        </sup>
        <span class="ltx_note_type">
         footnotemark:
        </span>
        <span class="ltx_tag ltx_tag_note">
         <span class="ltx_text ltx_font_medium" id="footnotex1.1.1.1">
          1
         </span>
        </span>
       </span>
      </span>
     </span>
     <br class="ltx_break"/>
     <sup class="ltx_sup" id="id10.9.5.5">
      <span class="ltx_text ltx_font_medium" id="id10.9.5.5.1">
       1
      </span>
     </sup>
    </span>
    Harbin Institute of Technology
    <sup class="ltx_sup" id="id18.16.id5">
     2
    </sup>
    Sun Yat-sen University
    <sup class="ltx_sup" id="id19.17.id6">
     3
    </sup>
    Zhejiang University
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id20.18.id7">
     {wanght1998,weijiangyu8,chenqianglong.ai}@gmail.com
    </span>
    ,
    <span class="ltx_text ltx_font_typewriter" id="id21.19.id8">
     guanyi@hit.edu.cn
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id22.20.id9">
     {xydu,kzhu,zchu}@ir.hit.edu.cn
    </span>
    ,
    <span class="ltx_text ltx_font_typewriter" id="id23.21.id10">
     yanlian0216@163.com
    </span>
   </span>
   <span class="ltx_author_notes">
    Corresponding author.
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id24.id1">
   Multi-agent debate systems are designed to derive accurate and consistent conclusions through adversarial interactions among agents. However, these systems often encounter challenges due to cognitive constraints, manifesting as (1) agents’ obstinate adherence to incorrect viewpoints and (2) their propensity to abandon correct viewpoints. These issues are primarily responsible for the ineffectiveness of such debates. Addressing the challenge of cognitive constraints, we introduce a novel framework, the Multi-Agent Debate with Retrieval Augmented (MADRA). MADRA incorporates retrieval of prior knowledge into the debate process, effectively breaking cognitive constraints and enhancing the agents’ reasoning capabilities. Furthermore, we have developed a self-selection module within this framework, enabling agents to autonomously select pertinent evidence, thereby minimizing the impact of irrelevant or noisy data. We have comprehensively tested and analyzed MADRA across six diverse datasets. The experimental results demonstrate that our approach significantly enhances performance across various tasks, proving the effectiveness of our proposed method.
   <span class="ltx_note ltx_role_footnote" id="footnote1">
    <sup class="ltx_note_mark">
     1
    </sup>
    <span class="ltx_note_outer">
     <span class="ltx_note_content">
      <sup class="ltx_note_mark">
       1
      </sup>
      <span class="ltx_tag ltx_tag_note">
       1
      </span>
      Code and datasets are available at
      <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/FutureForMe/MADRA" target="_blank" title="">
       https://github.com/FutureForMe/MADRA
      </a>
      .
     </span>
    </span>
   </span>
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="176" id="S1.F1.g1" src="/html/2312.04854/assets/x1.png" width="230"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Cognitive constraints of multi-agent debate process can lead to debate failure. Introducing prior knowledge into the debate process can break cognitive constraints.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Large Language Models (LLMs) have demonstrated remarkable performance in a variety of Natural Language Processing (NLP) tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     OpenAI (
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     ); Touvron et al. (
     <a class="ltx_ref" href="#bib.bib33" title="">
      2023
     </a>
     ); Pu et al. (
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023
     </a>
     )
    </cite>
    . This success is largely attributed to their emergent capabilities
    <cite class="ltx_cite ltx_citemacro_cite">
     Wei et al. (
     <a class="ltx_ref" href="#bib.bib37" title="">
      2022a
     </a>
     )
    </cite>
    , drawing significant academic and industry attention.
However, LLMs face critical challenges, notably the issue of hallucinations
    <cite class="ltx_cite ltx_citemacro_cite">
     Huang et al. (
     <a class="ltx_ref" href="#bib.bib10" title="">
      2023
     </a>
     )
    </cite>
    , which impede their ability to deliver advanced human-centric services.
To alleviate this issue, extensive research has been conducted, such as chain-of-thought (CoT)
    <cite class="ltx_cite ltx_citemacro_cite">
     Wei et al. (
     <a class="ltx_ref" href="#bib.bib38" title="">
      2022b
     </a>
     )
    </cite>
    , self-consistency
    <cite class="ltx_cite ltx_citemacro_cite">
     Wang et al. (
     <a class="ltx_ref" href="#bib.bib35" title="">
      2022
     </a>
     )
    </cite>
    , ReAct
    <cite class="ltx_cite ltx_citemacro_cite">
     Yao et al. (
     <a class="ltx_ref" href="#bib.bib44" title="">
      2022
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    While single-agent reasoning has achieved remarkable advancements, the exploration of collective intelligence capabilities remains a frontier
    <cite class="ltx_cite ltx_citemacro_cite">
     Zhang et al. (
     <a class="ltx_ref" href="#bib.bib46" title="">
      2023b
     </a>
     )
    </cite>
    . To this end, researchers are now pioneering multi-agent systems utilizing Large Language Models (LLMs) to enhance reasoning skills
    <cite class="ltx_cite ltx_citemacro_cite">
     Li et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     ); Du et al. (
     <a class="ltx_ref" href="#bib.bib4" title="">
      2023
     </a>
     ); Qian et al. (
     <a class="ltx_ref" href="#bib.bib25" title="">
      2023
     </a>
     )
    </cite>
    . These systems are designed with agents assuming varied roles and engaging in specific interaction patterns. Through such design, agents can either collaborate to accomplish tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     Qian et al. (
     <a class="ltx_ref" href="#bib.bib25" title="">
      2023
     </a>
     ); Li et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     )
    </cite>
    or engage in constructive debates by presenting divergent viewpoints
    <cite class="ltx_cite ltx_citemacro_cite">
     Du et al. (
     <a class="ltx_ref" href="#bib.bib4" title="">
      2023
     </a>
     ); Liang et al. (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    In multi-agent collaboration, the Multi-Agent Debate (MAD) approach has garnered considerable attention for its ability to mitigate hallucinations and tackle complex tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     Xiong et al. (
     <a class="ltx_ref" href="#bib.bib41" title="">
      2023
     </a>
     ); Xu et al. (
     <a class="ltx_ref" href="#bib.bib42" title="">
      2023
     </a>
     )
    </cite>
    . Studies by
    <cite class="ltx_cite ltx_citemacro_citet">
     Du et al. (
     <a class="ltx_ref" href="#bib.bib4" title="">
      2023
     </a>
     )
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Liang et al. (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    have enhanced factual consistency and reasoning skills using MAD. Moreover,
    <cite class="ltx_cite ltx_citemacro_citet">
     Chan et al. (
     <a class="ltx_ref" href="#bib.bib2" title="">
      2023
     </a>
     )
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Tang et al. (
     <a class="ltx_ref" href="#bib.bib31" title="">
      2023
     </a>
     )
    </cite>
    have applied MAD to assess responses from generative models and conduct medical reasoning tasks, respectively. However, a significant challenge persists in MAD methods:
    <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p3.1.1">
     Cognitive Constraints
    </span>
    (Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    ). This encompasses two problematic behaviors: (1) agents not recognizing their errors, leading to persistent adherence to incorrect viewpoints, and (2) agents easily abandoning correct viewpoints, both contributing to debate failures.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    A key factor contributing to cognitive constraints is the lack of prior knowledge. To address the challenge, we introduce a novel multi-agent debate framework that firstly incorporates retrieval of prior knowledge into the debate process, thereby overcoming cognitive constraints. Additionally, we present a self-selection method enabling agents to autonomously choose relevant evidence, effectively minimizing the impact of irrelevant information. Our experiments on six datasets, including knowledge-intensive reasoning tasks, demonstrate that our approach not only surpasses all baseline methods but also significantly enhances performance. Furthermore, a thorough evaluation of each model component validates its efficacy.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    The contributions of this work are as follows:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We propose a novel multi-agent debate framework that first introduces retrieval prior knowledge to break cognitive constraints, thereby enhancing the performance of model in complex reasoning tasks.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We propose a self-selection method that empowers agents to autonomously identify and utilize helpful knowledge from an evidence pool, thus effectively filtering out noisy data.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       We conduct experiments on complex reasoning tasks and fact-verification tasks respectively, with results indicating the superior performance of our model.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Multi-Agent Collaboration
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     The outstanding performance of LLMs
     <cite class="ltx_cite ltx_citemacro_cite">
      Wei et al. (
      <a class="ltx_ref" href="#bib.bib37" title="">
       2022a
      </a>
      )
     </cite>
     has laid the groundwork for the development of autonomous agents based on these models
     <cite class="ltx_cite ltx_citemacro_cite">
      Gravitas (
      <a class="ltx_ref" href="#bib.bib7" title="">
       2023
      </a>
      )
     </cite>
     . Multi-agent collaboration, a pivotal aspect of automated agents, has garnered considerable attention. This domain is typically divided into two categories: cooperative interaction and adversarial interaction
     <cite class="ltx_cite ltx_citemacro_cite">
      Xi et al. (
      <a class="ltx_ref" href="#bib.bib40" title="">
       2023
      </a>
      )
     </cite>
     . Several multi-agent collaboration frameworks, such as AgentVerse
     <cite class="ltx_cite ltx_citemacro_cite">
      Chen et al. (
      <a class="ltx_ref" href="#bib.bib3" title="">
       2023
      </a>
      )
     </cite>
     and AutoGen
     <cite class="ltx_cite ltx_citemacro_cite">
      Wu et al. (
      <a class="ltx_ref" href="#bib.bib39" title="">
       2023
      </a>
      )
     </cite>
     , have emerged, simplifying the construction of multi-agent collaboration models.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     Cooperative interaction requires the construction of agents with different roles, each of which performs its own duties and works together to complete the final task
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib17" title="">
       2023
      </a>
      ); Zhang et al. (
      <a class="ltx_ref" href="#bib.bib45" title="">
       2023a
      </a>
      )
     </cite>
     . For example,
     <cite class="ltx_cite ltx_citemacro_citet">
      Park et al. (
      <a class="ltx_ref" href="#bib.bib23" title="">
       2023
      </a>
      )
     </cite>
     demonstrated that generative agents can produce believable individual and emergent social behaviors. Further,
     <cite class="ltx_cite ltx_citemacro_citet">
      Qian et al. (
      <a class="ltx_ref" href="#bib.bib25" title="">
       2023
      </a>
      )
     </cite>
     carefully designed agents with different roles to complete the software development.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p3">
    <p class="ltx_p" id="S2.SS1.p3.1">
     Adversarial interactive methods usually build multiple agents and complete reasoning tasks in a debate manner
     <cite class="ltx_cite ltx_citemacro_cite">
      Fu et al. (
      <a class="ltx_ref" href="#bib.bib6" title="">
       2023
      </a>
      ); Wang et al. (
      <a class="ltx_ref" href="#bib.bib36" title="">
       2023
      </a>
      ); Xiong et al. (
      <a class="ltx_ref" href="#bib.bib41" title="">
       2023
      </a>
      )
     </cite>
     .
     <cite class="ltx_cite ltx_citemacro_citet">
      Du et al. (
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     and
     <cite class="ltx_cite ltx_citemacro_citet">
      Liang et al. (
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     proposed using the form of multi-agent debate to improve the consistency and reasoning capabilities of language models.
     <cite class="ltx_cite ltx_citemacro_citet">
      Chan et al. (
      <a class="ltx_ref" href="#bib.bib2" title="">
       2023
      </a>
      )
     </cite>
     used the multi-agent debate to evaluate the reply quality of LLMs. However, existing multi-agent debate frameworks do not incorporate prior knowledge to break cognitive constraints and further improve model performance.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Retrieval Augmented Models
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     Integrating external retrieval knowledge has emerged as a crucial strategy to mitigate hallucinations in LLMs
     <cite class="ltx_cite ltx_citemacro_cite">
      Shuster et al. (
      <a class="ltx_ref" href="#bib.bib30" title="">
       2021
      </a>
      ); Izacard et al. (
      <a class="ltx_ref" href="#bib.bib11" title="">
       2022
      </a>
      ); Feng et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      )
     </cite>
     . Initially, researchers incorporated externally retrieved knowledge related to the query into prompts, boosting the model’s reasoning capabilities
     <cite class="ltx_cite ltx_citemacro_cite">
      Shi et al. (
      <a class="ltx_ref" href="#bib.bib28" title="">
       2023
      </a>
      ); Ram et al. (
      <a class="ltx_ref" href="#bib.bib26" title="">
       2023
      </a>
      )
     </cite>
     .
     <cite class="ltx_cite ltx_citemacro_citet">
      Yao et al. (
      <a class="ltx_ref" href="#bib.bib44" title="">
       2022
      </a>
      )
     </cite>
     combined reasoning and action in LLMs, guiding them to invoke external retrieval modules via a few-shot approach. To further enhance retrieval capabilities, multiple-round retrieval-enhanced reasoning methods were proposed
     <cite class="ltx_cite ltx_citemacro_cite">
      He et al. (
      <a class="ltx_ref" href="#bib.bib8" title="">
       2022
      </a>
      ); Shao et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2023
      </a>
      ); Jiang et al. (
      <a class="ltx_ref" href="#bib.bib12" title="">
       2023
      </a>
      ); Trivedi et al. (
      <a class="ltx_ref" href="#bib.bib34" title="">
       2022
      </a>
      )
     </cite>
     . Additionally, some works have utilized search engines for knowledge retrieval, aiding LLMs in complex reasoning tasks
     <cite class="ltx_cite ltx_citemacro_cite">
      Nakano et al. (
      <a class="ltx_ref" href="#bib.bib21" title="">
       2021
      </a>
      ); Liu et al. (
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
      )
     </cite>
     . To address cognitive constraints in multi-agent debates, we first integrate external retrieval knowledge into the debate process.
    </p>
   </div>
   <figure class="ltx_figure" id="S2.F2">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="245" id="S2.F2.g1" src="/html/2312.04854/assets/x2.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 2:
     </span>
     Framework of our multi-agent debate. Upon receiving the question, debaters initially present their views in the simultaneous-talk stage, followed by orderly individual debates in the next stage. Prior to each point made, they select relevant evidence from the evidence pool. A judge then assesses whether to conclude the debate after each round, and a summarizer compiles the final answers post-debate.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Method
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    In this work, we propose a multi-agent debate framework based on retrieval augmented to mitigate the challenges posed by cognitive constraints. The overall framework is illustrated in Figure
    <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ 2.2 Retrieval Augmented Models ‣ 2 Related Work ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    . The specific roles of these agents are delineated in Section
    <a class="ltx_ref" href="#S3.SS1" title="3.1 Role Definition ‣ 3 Method ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      3.1
     </span>
    </a>
    . Section
    <a class="ltx_ref" href="#S3.SS2" title="3.2 Retrieval Evidences ‣ 3 Method ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      3.2
     </span>
    </a>
    delves into the process of retrieving evidence from the pool, while Section
    <a class="ltx_ref" href="#S3.SS3" title="3.3 Debate Process ‣ 3 Method ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      3.3
     </span>
    </a>
    outlines the overall procedural flow of the debate within this framework.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Role Definition
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Our debate framework is structured around three key roles: the debater, the judge, and the summarizer. Each role has specific responsibilities, as outlined below, and see the appendix
     <a class="ltx_ref" href="#A1" title="Appendix A Role Definition Prompt ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
      <span class="ltx_text ltx_ref_tag">
       A
      </span>
     </a>
     of specific role definition prompts.
    </p>
   </div>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Debater
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">
      The debater’s role has two primary duties: Firstly, based on the question, the debater must identify and select evidence from the evidence pool that can help you answer the question. This process aims to filter out irrelevant or misleading information. Secondly, the debater needs to generate a new round of responses based on the debate history and selected evidence.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Judge
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">
      The responsibilities of the judge are relatively simple yet pivotal. The judge is tasked with determining whether the debaters have achieved a consensus on their responses. If a consensus is reached, the judge is responsible for promptly ending the debate. This action is essential to prevent the debate that extended debates could transform correct responses into incorrect ones.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Summarizer
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">
      The role of the summarizer is to summarize the answers given by the different debaters to form a final response after the debate. Note that this role demands strict impartiality, we require the summarizer not to have subjective opinions and only to summarize the viewpoints of other debaters.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Retrieval Evidences
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     To break cognitive constraints in the debate, we introduce external retrieval knowledge. Retrieval results often contain noisy evidences that is relevant to the question but not helpful in answering the question. We build an evidence pool to avoid introducing negative effects, enabling agents to select useful evidence from it. The primary sources of evidence are Wikipedia and Google
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="www.google.com" title="">
         www.google.com
        </a>
       </span>
      </span>
     </span>
     .
    </p>
   </div>
   <section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Wikipedia Retrieval
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
     <p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">
      For effective retrieval evidence from the Wikipedia corpus, we utilize the dense passage retriever (DPR)
      <cite class="ltx_cite ltx_citemacro_cite">
       Karpukhin et al. (
       <a class="ltx_ref" href="#bib.bib14" title="">
        2020
       </a>
       )
      </cite>
      . DPR employs the twin-tower mechanism for encoding questions and passages, which is both straightforward to implement and yields superior results. In the search results, we retained the top 15 most relevant passages.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Google Retrieval
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
     <p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">
      Similar to
      <cite class="ltx_cite ltx_citemacro_cite">
       Luo et al. (
       <a class="ltx_ref" href="#bib.bib20" title="">
        2023
       </a>
       )
      </cite>
      , we input queries into the Google search engine and return up to the top 10 search results. Each result contains fields such as a title, a brief description, page content, and page URL. Considering the length of web page content and potential noise, we add only brief descriptions to the evidence pool to assist agents in selecting the most appropriate evidence.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Debate Process
   </h3>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Overall Process
    </h4>
    <div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">
      As illustrated in Figure
      <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ 2.2 Retrieval Augmented Models ‣ 2 Related Work ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      , the debate process is structured into three distinct stages: simultaneous-talk, orderly-talk, and summary. During the simultaneous-talk stage, each debater independently presents their views, uninfluenced by the responses of others. In the orderly-talk stage, debaters, one after another, build upon the views previously expressed. Each debater speaks once per round, and after each round, the judge evaluates whether to conclude the debate. At the end of the debate, the summarizer will summarize the final answer based on the views of all debaters. Notably, before presenting their views, each debater selects relevant evidence from a pool created by retrieving evidence from the Wikipedia corpus and Google search engine based on the question.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Simultaneous-Talk
    </h4>
    <div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">
      During the simultaneous-talk, the primary duty of each debater is to independently formulate a response to the question. This response should be as accurate as possible and grounded in the available evidence of their own selection. The prompt for this stage is as follows:
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px2.p2">
     <svg class="ltx_picture" height="156.05" id="S3.SS3.SSS0.Px2.p2.pic1" overflow="visible" version="1.1" width="600">
      <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,156.05) matrix(1 0 0 -1 0 0)">
       <g fill="#404040" fill-opacity="1.0">
        <path d="M 0 5.32 L 0 150.73 C 0 153.67 2.38 156.05 5.32 156.05 L 594.68 156.05 C 597.62 156.05 600 153.67 600 150.73 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#F2F2F2" fill-opacity="1.0">
        <path d="M 1.38 5.32 L 1.38 127.15 L 598.62 127.15 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#666666" fill-opacity="1.0">
        <path d="M 1.38 128.53 L 1.38 150.73 C 1.38 152.9 3.15 154.67 5.32 154.67 L 594.68 154.67 C 596.85 154.67 598.62 152.9 598.62 150.73 L 598.62 128.53 Z" style="stroke:none">
        </path>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 135.45)">
        <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px2.p2.pic1.1.1.1.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px2.p2.pic1.1.1.1.1.1.1">
           Prompt 3.1: Simultaneous-Talk
          </span>
         </span>
        </foreignobject>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
        <foreignobject color="#000000" height="111.93" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.1">
           Answer the question as accurately as possible based on the information given, and put the answer in the form [answer].
Here is an example:
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.2">
           {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.2.1">
            example
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.3">
           (END OF EXAMPLE)
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.4">
           {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.4.1">
            evidences
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.5">
           Question: {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.5.1">
            question
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px2.p2.pic1.2.2.2.1.1.6">
           Answer: Let’s think step by step!
          </span>
         </span>
        </foreignobject>
       </g>
      </g>
     </svg>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Orderly-Talk
    </h4>
    <div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">
      In the stage, each debater is expected to consider the responses of the other debaters from the previous round as well as the evidence of their own choice. This process involves a discussion with the other debaters to collaboratively get a final answer. This stage is critical for integrating diverse viewpoints and evidence into a cohesive conclusion. This prompt is as follows:
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px3.p2">
     <svg class="ltx_picture" height="172.66" id="S3.SS3.SSS0.Px3.p2.pic1" overflow="visible" version="1.1" width="600">
      <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,172.66) matrix(1 0 0 -1 0 0)">
       <g fill="#404040" fill-opacity="1.0">
        <path d="M 0 5.32 L 0 167.33 C 0 170.27 2.38 172.66 5.32 172.66 L 594.68 172.66 C 597.62 172.66 600 170.27 600 167.33 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#F2F2F2" fill-opacity="1.0">
        <path d="M 1.38 5.32 L 1.38 143.75 L 598.62 143.75 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#666666" fill-opacity="1.0">
        <path d="M 1.38 145.13 L 1.38 167.33 C 1.38 169.51 3.15 171.27 5.32 171.27 L 594.68 171.27 C 596.85 171.27 598.62 169.51 598.62 167.33 L 598.62 145.13 Z" style="stroke:none">
        </path>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 152.05)">
        <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px3.p2.pic1.1.1.1.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px3.p2.pic1.1.1.1.1.1.1">
           Prompt 3.2: Orderly-Talk
          </span>
         </span>
        </foreignobject>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
        <foreignobject color="#000000" height="128.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.1">
           There are a few other agents assigned the same task, it’s your responsibility to discuss with them and think critically. You can update your answer with other agents’ answers or given evidences as advice, or you can not update your answer. Please put the answer in the form [answer].
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.2">
           {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.2.1">
            evidences
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.3">
           {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.3.1">
            answer_from_other_agents
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.4">
           {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.4.1">
            your_historical_answer
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.5">
           Question: {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.5.1">
            question
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px3.p2.pic1.2.2.2.1.1.6">
           Answer: Let’s think step by step!
          </span>
         </span>
        </foreignobject>
       </g>
      </g>
     </svg>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px4">
    <h4 class="ltx_title ltx_title_paragraph">
     Judge
    </h4>
    <div class="ltx_para" id="S3.SS3.SSS0.Px4.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px4.p1.1">
      Upon the completion of each round of discussion, the judge is tasked with evaluating the debaters’ viewpoints to decide whether to conclude the debate. This prompt is as follows:
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px4.p2">
     <svg class="ltx_picture" height="156.05" id="S3.SS3.SSS0.Px4.p2.pic1" overflow="visible" version="1.1" width="600">
      <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,156.05) matrix(1 0 0 -1 0 0)">
       <g fill="#404040" fill-opacity="1.0">
        <path d="M 0 5.32 L 0 150.73 C 0 153.67 2.38 156.05 5.32 156.05 L 594.68 156.05 C 597.62 156.05 600 153.67 600 150.73 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#F2F2F2" fill-opacity="1.0">
        <path d="M 1.38 5.32 L 1.38 127.15 L 598.62 127.15 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#666666" fill-opacity="1.0">
        <path d="M 1.38 128.53 L 1.38 150.73 C 1.38 152.9 3.15 154.67 5.32 154.67 L 594.68 154.67 C 596.85 154.67 598.62 152.9 598.62 150.73 L 598.62 128.53 Z" style="stroke:none">
        </path>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 135.45)">
        <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px4.p2.pic1.1.1.1.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px4.p2.pic1.1.1.1.1.1.1">
           Prompt 3.3: Judgement
          </span>
         </span>
        </foreignobject>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
        <foreignobject color="#000000" height="111.93" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px4.p2.pic1.2.2.2.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px4.p2.pic1.2.2.2.1.1.1">
           The answer of the agents are typically denoted with the [answer] format. Your task is to extract each agent’s answer and evaluate the consistency of their answers to the question. If all agents have provided correct and consistent answers, respond with [Yes]. If their answers are inconsistent, respond with [No]. Please ensure to encase your response - Yes or No - within square brackets.
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px4.p2.pic1.2.2.2.1.1.2">
           Question: {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px4.p2.pic1.2.2.2.1.1.2.1">
            question
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px4.p2.pic1.2.2.2.1.1.3">
           Agent Responses: {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px4.p2.pic1.2.2.2.1.1.3.1">
            all_answers_from_agents
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px4.p2.pic1.2.2.2.1.1.4">
           Answer: Let’s think step by step!
          </span>
         </span>
        </foreignobject>
       </g>
      </g>
     </svg>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px5">
    <h4 class="ltx_title ltx_title_paragraph">
     Summary
    </h4>
    <div class="ltx_para" id="S3.SS3.SSS0.Px5.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px5.p1.1">
      After the end of the debate, the responsibility of the summarizer is to summarize the final answers from all the debaters’ responses. The summarizer must carefully synthesize these varied viewpoints into a coherent and comprehensive conclusion. Prompt is as follows:
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px5.p2">
     <svg class="ltx_picture" height="155.9" id="S3.SS3.SSS0.Px5.p2.pic1" overflow="visible" version="1.1" width="600">
      <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,155.9) matrix(1 0 0 -1 0 0)">
       <g fill="#404040" fill-opacity="1.0">
        <path d="M 0 5.32 L 0 150.58 C 0 153.51 2.38 155.9 5.32 155.9 L 594.68 155.9 C 597.62 155.9 600 153.51 600 150.58 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#F2F2F2" fill-opacity="1.0">
        <path d="M 1.38 5.32 L 1.38 127.15 L 598.62 127.15 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#666666" fill-opacity="1.0">
        <path d="M 1.38 128.53 L 1.38 150.58 C 1.38 152.75 3.15 154.51 5.32 154.51 L 594.68 154.51 C 596.85 154.51 598.62 152.75 598.62 150.58 L 598.62 128.53 Z" style="stroke:none">
        </path>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 135.45)">
        <foreignobject color="#FFFFFF" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px5.p2.pic1.1.1.1.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px5.p2.pic1.1.1.1.1.1.1">
           Prompt 3.4: Summary
          </span>
         </span>
        </foreignobject>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
        <foreignobject color="#000000" height="111.93" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.1">
           Please summarize the final answer from answer of all agents. Place the final answer of question in the form of [answer].
Here is some examples:
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.2">
           {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.2.1">
            examples
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.3">
           (END OF EXAMPLE)
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.4">
           Question: {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.4.1">
            question
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.5">
           {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.5.1">
            all_answers_from_agents
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px5.p2.pic1.2.2.2.1.1.6">
           Answer: Let’s think step by step!
          </span>
         </span>
        </foreignobject>
       </g>
      </g>
     </svg>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS3.SSS0.Px6">
    <h4 class="ltx_title ltx_title_paragraph">
     Self-Selection
    </h4>
    <div class="ltx_para" id="S3.SS3.SSS0.Px6.p1">
     <p class="ltx_p" id="S3.SS3.SSS0.Px6.p1.1">
      Prior to each round of the debate, debaters are provided with a consistent evidence pool. Their responsibility is to identify and select evidence from this pool that will effectively aid their argumentation. To ensure precision and relevance in their selection, each debater is limited to choosing no more than three pieces of evidence. The prompt is as follows:
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px6.p2">
     <svg class="ltx_picture" height="140.98" id="S3.SS3.SSS0.Px6.p2.pic1" overflow="visible" version="1.1" width="600">
      <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,140.98) matrix(1 0 0 -1 0 0)">
       <g fill="#404040" fill-opacity="1.0">
        <path d="M 0 5.32 L 0 135.66 C 0 138.6 2.38 140.98 5.32 140.98 L 594.68 140.98 C 597.62 140.98 600 138.6 600 135.66 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#F2F2F2" fill-opacity="1.0">
        <path d="M 1.38 5.32 L 1.38 112.08 L 598.62 112.08 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#666666" fill-opacity="1.0">
        <path d="M 1.38 113.46 L 1.38 135.66 C 1.38 137.84 3.15 139.6 5.32 139.6 L 594.68 139.6 C 596.85 139.6 598.62 137.84 598.62 135.66 L 598.62 113.46 Z" style="stroke:none">
        </path>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 120.38)">
        <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px6.p2.pic1.1.1.1.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px6.p2.pic1.1.1.1.1.1.1">
           Prompt 3.5: Self-Selection
          </span>
         </span>
        </foreignobject>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
        <foreignobject color="#000000" height="96.86" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.SS3.SSS0.Px6.p2.pic1.2.2.2.1.1" style="width:421.6pt;">
          <span class="ltx_p" id="S3.SS3.SSS0.Px6.p2.pic1.2.2.2.1.1.1">
           Please select evidence from the evidence pool that will help you answer the question. If the evidence pool does not contain the information needed to answer the question, add [No Found] at the end of your response. If the evidence pool has evidence that can help you answer the question, please return up to 3 of the most helpful evidence. Put the number in square brackets.
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px6.p2.pic1.2.2.2.1.1.2">
           {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px6.p2.pic1.2.2.2.1.1.2.1">
            evidences
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px6.p2.pic1.2.2.2.1.1.3">
           Question: {
           <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px6.p2.pic1.2.2.2.1.1.3.1">
            question
           </span>
           }
          </span>
          <span class="ltx_p" id="S3.SS3.SSS0.Px6.p2.pic1.2.2.2.1.1.4">
           Answer: Let’s think step by step!
          </span>
         </span>
        </foreignobject>
       </g>
      </g>
     </svg>
    </div>
    <figure class="ltx_table" id="S3.T1">
     <div class="ltx_inline-block ltx_transformed_outer" id="S3.T1.6" style="width:411.9pt;height:169.1pt;vertical-align:-0.8pt;">
      <span class="ltx_transformed_inner" style="transform:translate(-36.3pt,14.9pt) scale(0.849996944365058,0.849996944365058) ;">
       <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.6.6">
        <tbody class="ltx_tbody">
         <tr class="ltx_tr" id="S3.T1.6.6.7.1">
          <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S3.T1.6.6.7.1.1">
           Model
          </th>
          <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.6.7.1.2">
           TriviaQA
          </td>
          <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.6.7.1.3">
           NQ
          </td>
          <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.6.7.1.4">
           HotpotQA
          </td>
          <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.6.7.1.5">
           2WikiMultiHopQA
          </td>
          <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.6.7.1.6">
           FEVER
          </td>
          <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.6.7.1.7">
           FEVEROUS
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.8.2" style="background-color:#EBEBEB;">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.8.2.1">
           <span class="ltx_text" id="S3.T1.6.6.8.2.1.1" style="background-color:#EBEBEB;">
            GPT 4
           </span>
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.8.2.2">
           <span class="ltx_text" id="S3.T1.6.6.8.2.2.1" style="background-color:#EBEBEB;">
            0.902
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.8.2.3">
           <span class="ltx_text" id="S3.T1.6.6.8.2.3.1" style="background-color:#EBEBEB;">
            0.692
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.8.2.4">
           <span class="ltx_text" id="S3.T1.6.6.8.2.4.1" style="background-color:#EBEBEB;">
            0.566
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.8.2.5">
           <span class="ltx_text" id="S3.T1.6.6.8.2.5.1" style="background-color:#EBEBEB;">
            0.284
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.8.2.6">
           <span class="ltx_text" id="S3.T1.6.6.8.2.6.1" style="background-color:#EBEBEB;">
            0.608
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.8.2.7">
           <span class="ltx_text" id="S3.T1.6.6.8.2.7.1" style="background-color:#EBEBEB;">
            0.354
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.9.3">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.9.3.1">
           GPT 3.5 Trubo
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.9.3.2">
           0.778
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.9.3.3">
           0.532
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.9.3.4">
           0.384
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.9.3.5">
           0.210
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.9.3.6">
           0.548
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.9.3.7">
           0.366
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.10.4">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.6.6.10.4.1">
           + Zero-Shot CoT
          </th>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.10.4.2">
           0.772
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.10.4.3">
           0.588
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.10.4.4">
           0.410
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.10.4.5">
           0.190
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.10.4.6">
           0.556
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.10.4.7">
           0.374
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.11.5">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.6.6.11.5.1">
           + Self Consistency
          </th>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.11.5.2">
           <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.6.11.5.2.1">
            0.818
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.11.5.3">
           0.622
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.11.5.4">
           0.408
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.11.5.5">
           0.206
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.11.5.6">
           0.560
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.11.5.7">
           0.384
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.12.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.6.6.12.6.1">
           + Reflxion
          </th>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.12.6.2">
           0.762
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.12.6.3">
           0.586
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.12.6.4">
           0.378
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.12.6.5">
           0.254
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.12.6.6">
           –
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.12.6.7">
           –
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.13.7">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.6.6.13.7.1">
           Multi-Agent Debate1
          </th>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.13.7.2">
           0.798
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.13.7.3">
           0.648
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.13.7.4">
           0.394
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.13.7.5">
           0.186
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.13.7.6">
           0.548
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.13.7.7">
           0.276
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.6">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.6.6.6.7">
           Multi-Agent Debate2
          </th>
          <td class="ltx_td ltx_align_center" id="S3.T1.1.1.1.1">
           0.756
           <sup class="ltx_sup" id="S3.T1.1.1.1.1.1">
            ∗
           </sup>
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2">
           0.576
           <sup class="ltx_sup" id="S3.T1.2.2.2.2.1">
            ∗
           </sup>
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.3.3.3.3">
           0.450
           <sup class="ltx_sup" id="S3.T1.3.3.3.3.1">
            ∗
           </sup>
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4">
           <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.4.4.4.4.1">
            0.334
           </span>
           <sup class="ltx_sup" id="S3.T1.4.4.4.4.2">
            ∗
           </sup>
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5">
           0.486
           <sup class="ltx_sup" id="S3.T1.5.5.5.5.1">
            ∗
           </sup>
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.6.6">
           0.248
           <sup class="ltx_sup" id="S3.T1.6.6.6.6.1">
            ∗
           </sup>
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.14.8">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.14.8.1">
           Our Method + wiki
          </th>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.14.8.2">
           0.786
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.14.8.3">
           0.636
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.14.8.4">
           <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.6.14.8.4.1">
            0.506
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.14.8.5">
           0.306
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.14.8.6">
           <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.6.14.8.6.1">
            0.650
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.14.8.7">
           0.390
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.15.9">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.6.6.15.9.1">
           Our Method + google
          </th>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.15.9.2">
           <span class="ltx_text ltx_font_bold" id="S3.T1.6.6.15.9.2.1">
            0.834
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.15.9.3">
           <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.6.15.9.3.1">
            0.656
           </span>
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.15.9.4">
           0.490
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.15.9.5">
           0.296
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.15.9.6">
           0.642
          </td>
          <td class="ltx_td ltx_align_center" id="S3.T1.6.6.15.9.7">
           <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.6.6.15.9.7.1">
            0.410
           </span>
          </td>
         </tr>
         <tr class="ltx_tr" id="S3.T1.6.6.16.10">
          <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.6.6.16.10.1">
           Our Method + all
          </th>
          <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.16.10.2">
           <span class="ltx_text ltx_font_bold" id="S3.T1.6.6.16.10.2.1">
            0.834
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.16.10.3">
           <span class="ltx_text ltx_font_bold" id="S3.T1.6.6.16.10.3.1">
            0.662
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.16.10.4">
           <span class="ltx_text ltx_font_bold" id="S3.T1.6.6.16.10.4.1">
            0.542
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.16.10.5">
           <span class="ltx_text ltx_font_bold" id="S3.T1.6.6.16.10.5.1">
            0.350
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.16.10.6">
           <span class="ltx_text ltx_font_bold" id="S3.T1.6.6.16.10.6.1">
            0.662
           </span>
          </td>
          <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.16.10.7">
           <span class="ltx_text ltx_font_bold" id="S3.T1.6.6.16.10.7.1">
            0.442
           </span>
          </td>
         </tr>
        </tbody>
       </table>
      </span>
     </div>
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_table">
       Table 1:
      </span>
      Comparative experimental results on all datasets. Our method surpasses GPT4 on certain datasets. Excluding GPT4’s results, the best results are highlighted in
      <span class="ltx_text ltx_font_bold" id="S3.T1.12.1">
       bold
      </span>
      , while the second-best are
      <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.13.2">
       underlined
      </span>
      .
      <sup class="ltx_sup" id="S3.T1.14.3">
       ∗
      </sup>
      means that the content generated by this method has no fixed format, and we directly use GPT4 to evaluate its reply.
     </figcaption>
    </figure>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiment
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Dataset
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     We evaluate the effectiveness of our method on three different types of datasets, including single-hop reasoning: TriviaQA
     <cite class="ltx_cite ltx_citemacro_cite">
      Joshi et al. (
      <a class="ltx_ref" href="#bib.bib13" title="">
       2017
      </a>
      )
     </cite>
     and NQ
     <cite class="ltx_cite ltx_citemacro_cite">
      Kwiatkowski et al. (
      <a class="ltx_ref" href="#bib.bib16" title="">
       2019
      </a>
      )
     </cite>
     , multi-hop reasoning: HotpotQA
     <cite class="ltx_cite ltx_citemacro_cite">
      Yang et al. (
      <a class="ltx_ref" href="#bib.bib43" title="">
       2018
      </a>
      )
     </cite>
     and 2WikiMultiHopQA
     <cite class="ltx_cite ltx_citemacro_cite">
      Ho et al. (
      <a class="ltx_ref" href="#bib.bib9" title="">
       2020
      </a>
      )
     </cite>
     , fact verification: FEVER
     <cite class="ltx_cite ltx_citemacro_cite">
      Thorne et al. (
      <a class="ltx_ref" href="#bib.bib32" title="">
       2018
      </a>
      )
     </cite>
     and FEVEROUS
     <cite class="ltx_cite ltx_citemacro_cite">
      Aly et al. (
      <a class="ltx_ref" href="#bib.bib1" title="">
       2021
      </a>
      )
     </cite>
     . Solving these problems requires world knowledge, which can be found in Wikipedia or the Google search engine. Details of the dataset are provided in the Appendix
     <a class="ltx_ref" href="#A2" title="Appendix B Dataset Details ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Baseline
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     In order to further compare the performance of our models, we selected baselines from two reasoning methods: single-agent reasoning and multi-agent collaboration.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">
      Standard Prompt
     </span>
     uses simple prompt and examples to complete reasoning.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">
      Zero-Shot CoT
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Kojima et al. (
      <a class="ltx_ref" href="#bib.bib15" title="">
       2022
      </a>
      )
     </cite>
     adds "let’s think step by step" and CoT examples to prompt for reasoning.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">
      Self Consistency
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Wang et al. (
      <a class="ltx_ref" href="#bib.bib35" title="">
       2022
      </a>
      )
     </cite>
     samples a diverse set of reasoning paths and then selects the most consistent answer by marginalizing out all possible reasoning paths.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p5">
    <p class="ltx_p" id="S4.SS2.p5.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">
      Reflexion
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Shinn et al. (
      <a class="ltx_ref" href="#bib.bib29" title="">
       2023
      </a>
      )
     </cite>
     uses environmental feedback to help model reflection to improve model reasoning performance.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p6">
    <p class="ltx_p" id="S4.SS2.p6.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p6.1.1">
      Multi-Agent Debate1
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Du et al. (
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     utilizes multiple agents to propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p7">
    <p class="ltx_p" id="S4.SS2.p7.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS2.p7.1.1">
      Multi-Agent Debate2
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Liang et al. (
      <a class="ltx_ref" href="#bib.bib18" title="">
       2023
      </a>
      )
     </cite>
     completes the debate in a tit-for-tat format, setting up the judge to stop and summarize answers early.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Evaluation Metric
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     The evaluation metric of existing datasets mainly uses extract match (EM)
     <cite class="ltx_cite ltx_citemacro_cite">
      Jiang et al. (
      <a class="ltx_ref" href="#bib.bib12" title="">
       2023
      </a>
      )
     </cite>
     . We believe that for discriminative answers, since the answer categories are fixed, after given examples in the prompt, LLM can output the answer in the correct format. Therefore, we use EM to evaluate FEVER and FEVEROUS datasets. For generative answers, EM is more stringent and cannot accurately judge answers better. Therefore, for TriviaQA, NQ, HotpotQA, and 2WikiMultiHopQA, we extract the answers from the response and use GPT4
     <cite class="ltx_cite ltx_citemacro_cite">
      OpenAI (
      <a class="ltx_ref" href="#bib.bib22" title="">
       2023
      </a>
      )
     </cite>
     for evaluation, which is called GPT4 Eval
     <cite class="ltx_cite ltx_citemacro_cite">
      Zheng et al. (
      <a class="ltx_ref" href="#bib.bib47" title="">
       2023
      </a>
      )
     </cite>
     . See the Appendix
     <a class="ltx_ref" href="#A3" title="Appendix C Evaluation Metric ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
      <span class="ltx_text ltx_ref_tag">
       C
      </span>
     </a>
     for details.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4
    </span>
    Implementation Details
   </h3>
   <div class="ltx_para" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     We use GPT-3.5-Turbo (2023-07-01-preview) Azure OpenAI API as the backbone to complete all experiments. In order to ensure the diversity and stability of the debate process, we set the temperature parameter to 0.5. The number of agents in the experiment is 2, and the maximum number of debate rounds is 3. Considering time and funding constraints, we randomly sampled 500 pieces of data from each dataset for experiments.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS4.p2">
    <p class="ltx_p" id="S4.SS4.p2.1">
     Evidence in the evidence pool is retrieved using Google API and Wikipedia corpus. We consider the Wikipedia dump on January 20, 2021, and use the index provided by pyserini
     <span class="ltx_note ltx_role_footnote" id="footnote3">
      <sup class="ltx_note_mark">
       3
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         3
        </sup>
        <span class="ltx_tag ltx_tag_note">
         3
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/castorini/pyserini" target="_blank" title="">
         https://github.com/castorini/pyserini
        </a>
       </span>
      </span>
     </span>
     for evidence retrieval. The retrieval number of Google API and the Wikipedia corpus are set to 5 and 10.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Experiment Results
  </h2>
  <figure class="ltx_figure" id="S5.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="125" id="S5.F3.g1" src="/html/2312.04854/assets/x3.png" width="438"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 3:
    </span>
    The effects of our debate framework(without retrieval knowledge). MAD1
    <cite class="ltx_cite ltx_citemacro_cite">
     Du et al. (
     <a class="ltx_ref" href="#bib.bib4" title="">
      2023
     </a>
     )
    </cite>
    and MAD2
    <cite class="ltx_cite ltx_citemacro_cite">
     Liang et al. (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    mean baseline multi-agent debate frameworks. SA and MA mean single-agent and multi-agent.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="S5.F4">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="182" id="S5.F4.g1" src="/html/2312.04854/assets/x4.png" width="368"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 4:
    </span>
    Model performance without retrieval knowledge and with different retrieval knowledge.
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Main Results
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     The main results of our method and baseline methods are reported in Table
     <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ Self-Selection ‣ 3.3 Debate Process ‣ 3 Method ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     . Our model consistently outperforms all baselines across various datasets, proving the effectiveness of integrating external prior knowledge in breaking cognitive constraints and enhancing reasoning capabilities.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS1.p2">
    <p class="ltx_p" id="S5.SS1.p2.1">
     Notably, significant improvements are observed in multi-hop reasoning and fact verification tasks, such as HotpotQA(+9.2%) and FEVER(+10.2%). The multi-hop reasoning task requires multiple reasonings to get the correct answer, which is more difficult. Fact-verification tasks rely more heavily on relevant external evidence. Introducing external prior knowledge can significantly reduce the difficulty of solving these two types of tasks. Remarkably, our model exceeds GPT4’s performance in the 2WikiMultiHopQA(+6.6%), FEVER(+5.4%), and FEVEROUS(+8.8%) datasets. Although the improvements in other tasks are smaller, it still shows that introducing external evidence can help agents achieve better experimental results in debates. In addition, the experimental results of GPT4 show that the performance on FEVEROUS is worse than that of other baseline methods. Analysis suggests that GPT4 prefers to answer "Not Enough Info" for questions that it cannot reason about. During the evaluation, we classified this as a reasoning failure.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="133" id="S5.F5.g1" src="/html/2312.04854/assets/x5.png" width="438"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     The ablation results of self-selection module in different retrieval knowledge.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S5.F6">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="112" id="S5.F6.g1" src="/html/2312.04854/assets/x6.png" width="447"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     Effect of maximum round. Set the agent number to 2. The histogram represents the amount of inconsistent data at the maximum round.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S5.F7">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="111" id="S5.F7.g1" src="/html/2312.04854/assets/x7.png" width="447"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 7:
     </span>
     Effect of agent number. Set the maximum round to 3. The histogram represents the amount of inconsistent data at the maximum round.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Ablation Study
   </h3>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     We conducted some ablation experiments to verify our proposed module’s effectiveness further.
    </p>
   </div>
   <section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Effect of Debate Framework
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
     <p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">
      We eliminated retrieval knowledge from our framework for a direct comparison with existing multi-agent debate frameworks
      <cite class="ltx_cite ltx_citemacro_cite">
       Du et al. (
       <a class="ltx_ref" href="#bib.bib4" title="">
        2023
       </a>
       ); Liang et al. (
       <a class="ltx_ref" href="#bib.bib18" title="">
        2023
       </a>
       )
      </cite>
      . Figure
      <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5 Experiment Results ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      (a) shows our framework outperforming baseline models on most datasets, confirming its efficacy. For instance, on the TriviaQA dataset, our method offers a 2.2% and 6.4% improvement over MAD1 and MAD2, respectively. Furthermore, Figure
      <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5 Experiment Results ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      (b) reveals that in a Retrieval-Augmented Generation (RAG) setup, our multi-agent approach significantly surpasses single-agent methods in datasets like NQ (+3.2%), HotpotQA (+4.4%), and FEVEROUS (+2.4%), further validating our framework’s effectiveness.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Importance of Retrieve Augmented
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
     <p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">
      In this part, we mainly consider two critical factors: (1) the
      <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px2.p1.1.1">
       adding external retrieval knowledge
      </span>
      , (2) the
      <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px2.p1.1.2">
       source of external retrieval knowledge
      </span>
      . The experimental results are shown in Figure
      <a class="ltx_ref" href="#S5.F4" title="Figure 4 ‣ 5 Experiment Results ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      .
     </p>
    </div>
    <div class="ltx_para" id="S5.SS2.SSS0.Px2.p2">
     <p class="ltx_p" id="S5.SS2.SSS0.Px2.p2.1">
      Regarding the first factor, incorporating external evidence markedly improves performance on most datasets, such as TriviaQA(+1.4%), HotpotQA(+7.8%), and FEVER(+5.0%). This shows that external knowledge can help multi-agents reach a consistent and correct conclusion during the debates. However, we observed a slight performance dip after integrating Wikipedia on datasets like NQ(-2.0%). The reason is that evidence from Wikipedia includes many relevant but unuseful evidence, leading to challenges in effective distinction by agents. Concerning the second factor, our experiments indicate that combining Wikipedia and Google search results can achieve superior results. For single-hop reasoning tasks, Google’s direct retrieval of relevant evidence is more beneficial, making its impact more obvious than Wikipedia, like TriviaQA(+1.4%). Conversely, Google cannot complete the reasoning for multi-step reasoning tasks (e.g., comparing heights between Yao Ming and O’Neal). However, related information about "Yao Ming" and "O’Neal" can be retrieved from Wikipedia, helping the agent obtain consistent and correct answers during the debate. Thus, Wikipedia proves more effective for multi-step reasoning datasets like HotpotQA(+4.2%).
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Importance of Self-Selection
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS0.Px3.p1">
     <p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1">
      As shown in Figure
      <a class="ltx_ref" href="#S5.F5" title="Figure 5 ‣ 5.1 Main Results ‣ 5 Experiment Results ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      , we conducted ablation research on the Self-Selection module under the settings of three retrieval methods. These results indicate an enhanced performance by incorporating the self-selection module, thereby validating its effectiveness. As depicted in Figure
      <a class="ltx_ref" href="#S5.F5" title="Figure 5 ‣ 5.1 Main Results ‣ 5 Experiment Results ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      (a), the self-selection module shows superior performance in QA datasets when utilizing evidence from Wikipedia, such as NQ(+14.2%), 2WikiMultHopQA(+10.4%). This suggests that while the retriever might bring in some relevant yet unhelpful passages from Wikipedia, the module effectively allows agents to filter out this noise. On the other hand, the Google search engine can directly retrieve answers to single-hop reasoning questions, while the retrieval results for multi-hop reasoning questions contain noisy data. Figure
      <a class="ltx_ref" href="#S5.F5" title="Figure 5 ‣ 5.1 Main Results ‣ 5 Experiment Results ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        5
       </span>
      </a>
      (b) shows that compared with the single-hop datasets like TriviaQA(+1.0%), the improvement of other datasets is more obvious, like HotpotQA(+11.8%).
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S5.SS2.SSS0.Px4">
    <h4 class="ltx_title ltx_title_paragraph">
     Effect of Debate Round
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS0.Px4.p1">
     <p class="ltx_p" id="S5.SS2.SSS0.Px4.p1.1">
      We conducted an in-depth analysis of varying the maximum number of debate rounds in three tasks, as shown in Figure
      <a class="ltx_ref" href="#S5.F7" title="Figure 7 ‣ 5.1 Main Results ‣ 5 Experiment Results ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        7
       </span>
      </a>
      . We observed that the model’s inconsistency decreases as the number of rounds increases, like TriviaQA(from 68 to 25). This trend suggests that the model is gradually reaching agreement on a broader set of data. During the debate, we employed a judge to assess whether the debaters had reached a consistency, allowing for an early conclusion when the agreement was consistent. This approach effectively mitigated the impact of prolonged debates. Consequently, the model’s performance initially gradually improved before stabilizing, indicating an optimal balance between debate length and decision-making efficacy.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S5.SS2.SSS0.Px5">
    <h4 class="ltx_title ltx_title_paragraph">
     Effect of Agent Number
    </h4>
    <div class="ltx_para" id="S5.SS2.SSS0.Px5.p1">
     <p class="ltx_p" id="S5.SS2.SSS0.Px5.p1.1">
      Our study also examines the impact of varying the number of agents, with results shown in Figure
      <a class="ltx_ref" href="#S5.F7" title="Figure 7 ‣ 5.1 Main Results ‣ 5 Experiment Results ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
       <span class="ltx_text ltx_ref_tag">
        7
       </span>
      </a>
      . When the number of agents changes from 2 to 3, the number of inconsistencies increases, and the model performance decreases.
This suggests that with three agents, each agent’s influence becomes more important, making it difficult to reach an agreement and affecting the experimental results. However, as the number of agents further increases, more and more agents will obtain correct reasoning results, which plays a positive role in the debate process. The model performance gradually increases, and the inconsistency gradually decreases. Therefore, the model performance showed a slight decrease at first and then gradually improved and stabilized. The single-hop reasoning task is relatively simple, so it does not show significant growth.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this paper, we propose a multi-agent debate framework based on retrieving augmented. This framework effectively alleviates cognitive constraints in the debate process by introducing externally retrieved prior knowledge. In addition, we build an external retrieval evidence pool to allow the agent to choose evidence that is helpful to itself, effectively avoiding the impact of noisy evidence. We conducted sufficient experiments on six datasets with different tasks to demonstrate the effectiveness of our method. In the future, we will further explore real-time knowledge retrieval and effective knowledge selection methods during debates.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Aly et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Rami Aly, Zhijiang Guo, Michael Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021.
    </span>
    <span class="ltx_bibblock">
     Feverous: Fact extraction and verification over unstructured and structured information.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      arXiv preprint arXiv:2106.05707
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     Chateval: Towards better llm-based evaluators through multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      arXiv preprint arXiv:2308.07201
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      arXiv preprint arXiv:2308.10848
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Du et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023.
    </span>
    <span class="ltx_bibblock">
     Improving factuality and reasoning in language models through multiagent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      arXiv preprint arXiv:2305.14325
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Feng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Trends in integration of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2311.05876
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Fu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023.
    </span>
    <span class="ltx_bibblock">
     Improving language model negotiation with self-play and in-context learning from ai feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:2305.10142
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gravitas (2023)
    </span>
    <span class="ltx_bibblock">
     Significant Gravitas. 2023.
    </span>
    <span class="ltx_bibblock">
     Auto-gpt: An autonomous gpt-4 experiment, 2023.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      URL https://github. com/Significant-Gravitas/Auto-GPT
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     He et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Hangfeng He, Hongming Zhang, and Dan Roth. 2022.
    </span>
    <span class="ltx_bibblock">
     Rethinking with retrieval: Faithful large language model inference.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      arXiv preprint arXiv:2301.00303
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ho et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.
    </span>
    <span class="ltx_bibblock">
     Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      Proceedings of the 28th International Conference on Computational Linguistics
     </em>
     , pages 6609–6625.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      arXiv preprint arXiv:2311.05232
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Izacard et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022.
    </span>
    <span class="ltx_bibblock">
     Few-shot learning with retrieval augmented language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      arXiv preprint arXiv:2208.03299
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.
    </span>
    <span class="ltx_bibblock">
     Active retrieval augmented generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      arXiv preprint arXiv:2305.06983
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Joshi et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017.
    </span>
    <span class="ltx_bibblock">
     Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
     </em>
     , pages 1601–1611.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Karpukhin et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.
    </span>
    <span class="ltx_bibblock">
     Dense passage retrieval for open-domain question answering.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
     </em>
     , pages 6769–6781.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kojima et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.
    </span>
    <span class="ltx_bibblock">
     Large language models are zero-shot reasoners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      Advances in neural information processing systems
     </em>
     , 35:22199–22213.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kwiatkowski et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019.
    </span>
    <span class="ltx_bibblock">
     Natural questions: a benchmark for question answering research.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      Transactions of the Association for Computational Linguistics
     </em>
     , 7:452–466.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023.
    </span>
    <span class="ltx_bibblock">
     Camel: Communicative agents for" mind" exploration of large language model society.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      Thirty-seventh Conference on Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023.
    </span>
    <span class="ltx_bibblock">
     Encouraging divergent thinking in large language models through multi-agent debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      arXiv preprint arXiv:2305.19118
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3580305.3599931" target="_blank" title="">
      Webglm: Towards an efficient web-enhanced question answering system with human preferences
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
     </em>
     , KDD ’23, page 4549–4560, New York, NY, USA. Association for Computing Machinery.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Luo et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. 2023.
    </span>
    <span class="ltx_bibblock">
     Sail: Search-augmented instruction learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      arXiv preprint arXiv:2305.15225
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakano et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021.
    </span>
    <span class="ltx_bibblock">
     Webgpt: Browser-assisted question-answering with human feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      arXiv preprint arXiv:2112.09332
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023)
    </span>
    <span class="ltx_bibblock">
     OpenAI. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.08774" target="_blank" title="">
      Gpt-4 technical report
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Park et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023.
    </span>
    <span class="ltx_bibblock">
     Generative agents: Interactive simulacra of human behavior.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2304.03442
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xiao Pu, Mingqi Gao, and Xiaojun Wan. 2023.
    </span>
    <span class="ltx_bibblock">
     Summarization is (almost) dead.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      arXiv preprint arXiv:2309.09558
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023.
    </span>
    <span class="ltx_bibblock">
     Communicative agents for software development.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      arXiv preprint arXiv:2307.07924
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ram et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2302.00083" target="_blank" title="">
      In-context retrieval-augmented language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      Transactions of the Association for Computational Linguistics
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.
    </span>
    <span class="ltx_bibblock">
     Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      arXiv preprint arXiv:2305.15294
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shi et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.
    </span>
    <span class="ltx_bibblock">
     Replug: Retrieval-augmented black-box language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:2301.12652
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023.
    </span>
    <span class="ltx_bibblock">
     Reflexion: an autonomous agent with dynamic memory and self-reflection.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      arXiv preprint arXiv:2303.11366
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shuster et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.
    </span>
    <span class="ltx_bibblock">
     Retrieval augmentation reduces hallucination in conversation.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      Findings of the Association for Computational Linguistics: EMNLP 2021
     </em>
     , pages 3784–3803.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. 2023.
    </span>
    <span class="ltx_bibblock">
     Medagents: Large language models as collaborators for zero-shot medical reasoning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      arXiv preprint arXiv:2311.10537
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Thorne et al. (2018)
    </span>
    <span class="ltx_bibblock">
     James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.
    </span>
    <span class="ltx_bibblock">
     Fever: a large-scale dataset for fact extraction and verification.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)
     </em>
     , pages 809–819.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open foundation and fine-tuned chat models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      arXiv preprint arXiv:2307.09288
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Trivedi et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.
    </span>
    <span class="ltx_bibblock">
     Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      arXiv preprint arXiv:2212.10509
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.
    </span>
    <span class="ltx_bibblock">
     Self-consistency improves chain of thought reasoning in language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      The Eleventh International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023.
    </span>
    <span class="ltx_bibblock">
     Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      arXiv preprint arXiv:2307.05300
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022a)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a.
    </span>
    <span class="ltx_bibblock">
     Emergent abilities of large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      Transactions on Machine Learning Research
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022b)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b.
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 35:24824–24837.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023.
    </span>
    <span class="ltx_bibblock">
     Autogen: Enabling next-gen llm applications via multi-agent conversation framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      arXiv preprint arXiv:2308.08155
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xi et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     The rise and potential of large language model based agents: A survey.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      arXiv preprint arXiv:2309.07864
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xiong et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023.
    </span>
    <span class="ltx_bibblock">
     Examining the inter-consistency of large language models: An in-depth analysis via debate.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      arXiv e-prints
     </em>
     , pages arXiv–2305.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Zhenran Xu, Senbao Shi, Baotian Hu, Jindi Yu, Dongfang Li, Min Zhang, and Yuxiang Wu. 2023.
    </span>
    <span class="ltx_bibblock">
     Towards reasoning in large language models via multi-agent peer review collaboration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      arXiv preprint arXiv:2311.08152
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018.
    </span>
    <span class="ltx_bibblock">
     Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 2369–2380.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2022.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      The Eleventh International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, et al. 2023a.
    </span>
    <span class="ltx_bibblock">
     Proagent: Building proactive cooperative ai with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      arXiv preprint arXiv:2308.11339
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Jintian Zhang, Xin Xu, and Shumin Deng. 2023b.
    </span>
    <span class="ltx_bibblock">
     Exploring collaboration mechanisms for llm agents: A social psychology view.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      arXiv preprint arXiv:2310.02124
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zheng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Judging llm-as-a-judge with mt-bench and chatbot arena.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      arXiv preprint arXiv:2306.05685
     </em>
     .
    </span>
   </li>
  </ul>
 </section>
 <figure class="ltx_table" id="S6.T2">
  <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T2.1" style="width:433.6pt;height:88.2pt;vertical-align:-0.7pt;">
   <span class="ltx_transformed_inner" style="transform:translate(-95.4pt,19.3pt) scale(0.694380842738587,0.694380842738587) ;">
    <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T2.1.1">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S6.T2.1.1.1.1">
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T2.1.1.1.1.1">
        Dataset
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T2.1.1.1.1.2">
        Domain
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T2.1.1.1.1.3">
        Answer Type
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T2.1.1.1.1.4">
        Samples
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S6.T2.1.1.2.1">
       <td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.2.1.1">
        TriviaQA
        <cite class="ltx_cite ltx_citemacro_cite">
         Joshi et al. (
         <a class="ltx_ref" href="#bib.bib13" title="">
          2017
         </a>
         )
        </cite>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.2.1.2">
        Single Hop Reasoning
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.2.1.3">
        Generative
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.1.1.2.1.4">
        500
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T2.1.1.3.2">
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.3.2.1">
        NQ
        <cite class="ltx_cite ltx_citemacro_cite">
         Kwiatkowski et al. (
         <a class="ltx_ref" href="#bib.bib16" title="">
          2019
         </a>
         )
        </cite>
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.3.2.2">
        Single Hop Reasoning
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.3.2.3">
        Generative
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.3.2.4">
        500
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T2.1.1.4.3">
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.4.3.1">
        HotpotQA
        <cite class="ltx_cite ltx_citemacro_cite">
         Yang et al. (
         <a class="ltx_ref" href="#bib.bib43" title="">
          2018
         </a>
         )
        </cite>
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.4.3.2">
        Multi Hop Reasoning
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.4.3.3">
        Generative
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.4.3.4">
        500
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T2.1.1.5.4">
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.5.4.1">
        2WikiMultiHopQA
        <cite class="ltx_cite ltx_citemacro_cite">
         Ho et al. (
         <a class="ltx_ref" href="#bib.bib9" title="">
          2020
         </a>
         )
        </cite>
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.5.4.2">
        Multi Hop Reasoning
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.5.4.3">
        Generative
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.5.4.4">
        500
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T2.1.1.6.5">
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.6.5.1">
        FEVER
        <cite class="ltx_cite ltx_citemacro_cite">
         Thorne et al. (
         <a class="ltx_ref" href="#bib.bib32" title="">
          2018
         </a>
         )
        </cite>
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.6.5.2">
        Fact Checking
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.6.5.3">
        Discriminant / 3 categories
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T2.1.1.6.5.4">
        500
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T2.1.1.7.6">
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.1.7.6.1">
        FEVEROUS
        <cite class="ltx_cite ltx_citemacro_cite">
         Aly et al. (
         <a class="ltx_ref" href="#bib.bib1" title="">
          2021
         </a>
         )
        </cite>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.1.7.6.2">
        Fact Checking
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.1.7.6.3">
        Discriminant / 3 categories
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T2.1.1.7.6.4">
        500
       </td>
      </tr>
     </tbody>
    </table>
   </span>
  </div>
  <figcaption class="ltx_caption ltx_centering">
   <span class="ltx_tag ltx_tag_table">
    Table 2:
   </span>
   Dataset statistics table.
  </figcaption>
 </figure>
 <figure class="ltx_table" id="S6.T3">
  <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.1" style="width:433.6pt;height:89pt;vertical-align:-0.7pt;">
   <span class="ltx_transformed_inner" style="transform:translate(-83.0pt,16.9pt) scale(0.723145318702504,0.723145318702504) ;">
    <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.1.1">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S6.T3.1.1.1.1">
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.1.1.1">
         <span class="ltx_p" id="S6.T3.1.1.1.1.1.1.1" style="width:170.7pt;">
          Question
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.2">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.1.2.1">
         <span class="ltx_p" id="S6.T3.1.1.1.1.2.1.1" style="width:113.8pt;">
          groundtruth
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.3">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.1.3.1">
         <span class="ltx_p" id="S6.T3.1.1.1.1.3.1.1" style="width:113.8pt;">
          predictive answer
         </span>
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.4">
        EM
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.5">
        GPT4 Eval
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.1.6">
        Human Eval
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S6.T3.1.1.2.1" style="background-color:#EBEBEB;">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.1.1.2.1.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.2.1.1.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T3.1.1.2.1.1.1.1" style="width:170.7pt;">
          ① Who was the next British Prime Minister after Arthur Balfour?
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.1.1.2.1.2">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.2.1.2.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T3.1.1.2.1.2.1.1" style="width:113.8pt;">
          Henry Campbell Bannerman
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T3.1.1.2.1.3">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.2.1.3.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T3.1.1.2.1.3.1.1" style="width:113.8pt;">
          Henry Campbell Bannerman
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.4">
        <span class="ltx_text" id="S6.T3.1.1.2.1.4.1" style="background-color:#EBEBEB;">
         True
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.5">
        <span class="ltx_text" id="S6.T3.1.1.2.1.5.1" style="background-color:#EBEBEB;">
         True
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.1.1.2.1.6">
        <span class="ltx_text" id="S6.T3.1.1.2.1.6.1" style="background-color:#EBEBEB;">
         True
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T3.1.1.3.2">
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.1.1.3.2.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.3.2.1.1">
         <span class="ltx_p" id="S6.T3.1.1.3.2.1.1.1" style="width:170.7pt;">
          ② Robert Kirkpatrick of California grew the world’s biggest what?
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.1.1.3.2.2">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.3.2.2.1">
         <span class="ltx_p" id="S6.T3.1.1.3.2.2.1.1" style="width:113.8pt;">
          Head of garlic
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.1.1.3.2.3">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.3.2.3.1">
         <span class="ltx_p" id="S6.T3.1.1.3.2.3.1.1" style="width:113.8pt;">
          garlic head
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.4">
        False
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.5">
        True
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T3.1.1.3.2.6">
        True
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T3.1.1.4.3" style="background-color:#EBEBEB;">
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.1.1.4.3.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.4.3.1.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T3.1.1.4.3.1.1.1" style="width:170.7pt;">
          ③ Precisely where were the Winter Olympics of 1932 and 1980 both held?
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.1.1.4.3.2">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.4.3.2.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T3.1.1.4.3.2.1.1" style="width:113.8pt;">
          Lake placid
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T3.1.1.4.3.3">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.4.3.3.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T3.1.1.4.3.3.1.1" style="width:113.8pt;">
          Lake Placid, New York, United States
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.4">
        <span class="ltx_text" id="S6.T3.1.1.4.3.4.1" style="background-color:#EBEBEB;">
         False
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.5">
        <span class="ltx_text" id="S6.T3.1.1.4.3.5.1" style="background-color:#EBEBEB;">
         True
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S6.T3.1.1.4.3.6">
        <span class="ltx_text" id="S6.T3.1.1.4.3.6.1" style="background-color:#EBEBEB;">
         True
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T3.1.1.5.4">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T3.1.1.5.4.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.5.4.1.1">
         <span class="ltx_p" id="S6.T3.1.1.5.4.1.1.1" style="width:170.7pt;">
          ④ Who was the first person after Scott to reach the South Pole overland?
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T3.1.1.5.4.2">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.5.4.2.1">
         <span class="ltx_p" id="S6.T3.1.1.5.4.2.1.1" style="width:113.8pt;">
          Sir Edmund Hillary
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S6.T3.1.1.5.4.3">
        <span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.5.4.3.1">
         <span class="ltx_p" id="S6.T3.1.1.5.4.3.1.1" style="width:113.8pt;">
          Roald Amundsen and Edmund Hillary
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.5.4.4">
        False
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.5.4.5">
        False
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.1.1.5.4.6">
        False
       </td>
      </tr>
     </tbody>
    </table>
   </span>
  </div>
  <figcaption class="ltx_caption ltx_centering">
   <span class="ltx_tag ltx_tag_table">
    Table 3:
   </span>
   Evaluation results of model responses using different evaluation metrics (EM, GPT4 Eval, and Human Eval).
  </figcaption>
 </figure>
 <div class="ltx_table ltx_transformed_outer" id="S6.T4" style="width:4608.9pt;height:710.2pt;vertical-align:-0.0pt;">
  <div class="ltx_transformed_inner" style="width:710.2pt;transform:translate(1949.35pt,1949.85pt) rotate(-90deg) ;">
   <figure>
    <table class="ltx_tabular ltx_align_middle" id="S6.T4.11">
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S6.T4.11.12.1">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" colspan="2" id="S6.T4.11.12.1.1">
        <span class="ltx_text ltx_font_bold" id="S6.T4.11.12.1.1.1">
         Question
        </span>
        : Which band, Letters to Cleo or Screaming Trees, had more members?
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.11.13.2">
       <td class="ltx_td ltx_align_justify ltx_align_top" colspan="2" id="S6.T4.11.13.2.1">
        <span class="ltx_text ltx_font_bold" id="S6.T4.11.13.2.1.1">
         Answer
        </span>
        : Letters to Cleo
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.2.2" style="background-color:#EBEBEB;">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T4.1.1.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.1.1.1.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T4.1.1.1.1.1" style="width:341.4pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.1.1.1.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.1.1.1.1.1.1.1">
            <span class="ltx_text" id="S6.T4.1.1.1.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.1.1.1.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/debater1.png" width="598"/>
            </span>
           </span>
          </span>
          … On the other hand, the evidence pool does not provide specific information about the number of members in Screaming Trees. Therefore, we cannot definitively determine if Letters to Cleo or Screaming Trees had more members. [No Found]
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T4.2.2.2">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.2.2.2.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T4.2.2.2.1.1" style="width:341.4pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.2.2.2.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.2.2.2.1.1.1.1">
            <span class="ltx_text" id="S6.T4.2.2.2.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.2.2.2.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/debater2.png" width="598"/>
            </span>
           </span>
          </span>
          … [1] "Letters to Cleo" - The evidence provides information about the band’s members, including Kay Hanley, Greg McKenna, Michael Eisenstein, …[3] "Screaming Trees" - The evidence states that Screaming Trees had four members: Mark Lanegan, Gary Lee Conner, …
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.4.4">
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T4.3.3.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.3.3.1.1">
         <span class="ltx_p" id="S6.T4.3.3.1.1.1" style="width:341.4pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.3.3.1.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.3.3.1.1.1.1.1">
            <span class="ltx_text" id="S6.T4.3.3.1.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.3.3.1.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/debater1.png" width="598"/>
            </span>
           </span>
          </span>
          First, let’s compare the number of members in each band. Letters to Cleo is a band that typically consists of four members, while Screaming Trees is a band that typically consists of five members. Based on this information, Screaming Trees had more members than Letters to Cleo. Therefore, the answer is [Screaming Trees].
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top" id="S6.T4.4.4.2">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.4.4.2.1">
         <span class="ltx_p" id="S6.T4.4.4.2.1.1" style="width:341.4pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.4.4.2.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.4.4.2.1.1.1.1">
            <span class="ltx_text" id="S6.T4.4.4.2.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.4.4.2.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/debater2.png" width="598"/>
            </span>
           </span>
          </span>
          First, let’s gather the information about the bands. Letters to Cleo is an alternative rock band from Boston, Massachusetts. The band’s members are Kay Hanley, … Based on the information provided, Letters to Cleo had more members compared to Screaming Trees. Therefore, the answer is [Letters to Cleo].
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.5.5">
       <td class="ltx_td ltx_align_justify ltx_align_top" colspan="2" id="S6.T4.5.5.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.5.5.1.1">
         <span class="ltx_p" id="S6.T4.5.5.1.1.1" style="width:682.9pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.5.5.1.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.5.5.1.1.1.1.1">
            <span class="ltx_text" id="S6.T4.5.5.1.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.5.5.1.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/judge.png" width="598"/>
            </span>
           </span>
          </span>
          [NO].
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.7.7" style="background-color:#EBEBEB;">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T4.6.6.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.6.6.1.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T4.6.6.1.1.1" style="width:341.4pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.6.6.1.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.6.6.1.1.1.1.1">
            <span class="ltx_text" id="S6.T4.6.6.1.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.6.6.1.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/debater1.png" width="598"/>
            </span>
           </span>
          </span>
          Now, let’s analyze the evidence to determine which band had more members: From Evidence [0], we can see that Letters to Cleo had six members: Kay Hanley, Greg McKenna, Michael Eisenstein, … From Evidence [8], we can see that Screaming Trees had a lineup change, with Van Conner going on hiatus and Donna Dresch filling in for him during performances…
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S6.T4.7.7.2">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.7.7.2.1" style="background-color:#EBEBEB;">
         <span class="ltx_p" id="S6.T4.7.7.2.1.1" style="width:341.4pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.7.7.2.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.7.7.2.1.1.1.1">
            <span class="ltx_text" id="S6.T4.7.7.2.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.7.7.2.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/debater2.png" width="598"/>
            </span>
           </span>
          </span>
          [1] "Letters to Cleo" - The evidence provides information about the band’s members, including Kay Hanley, Greg McKenna, Michael Eisenstein, Stacy Jones, Scott Riebling, and Tom Polce. This information can be used to determine the number of members in the band.[3] "Screaming Trees" - The evidence states that Screaming Trees had four members: Mark Lanegan, Gary Lee Conner, Van Conner, and Barrett Martin.
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.8.8">
       <td class="ltx_td ltx_align_justify ltx_align_top" colspan="2" id="S6.T4.8.8.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.8.8.1.1">
         <span class="ltx_p" id="S6.T4.8.8.1.1.1" style="width:682.9pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.8.8.1.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.8.8.1.1.1.1.1">
            <span class="ltx_text" id="S6.T4.8.8.1.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.8.8.1.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/debater1.png" width="598"/>
            </span>
           </span>
          </span>
          First, let’s gather the information about the bands. Letters to Cleo is an alternative rock band from Boston, Massachusetts. The band’s members are Kay Hanley, Greg McKenna, … Based on the information provided, Screaming Trees had four members, while Letters to Cleo had six members. Therefore, Letters to Cleo had more members than Screaming Trees. Therefore, the answer is [Letters to Cleo].
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.9.9">
       <td class="ltx_td ltx_align_justify ltx_align_top" colspan="2" id="S6.T4.9.9.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.9.9.1.1">
         <span class="ltx_p" id="S6.T4.9.9.1.1.1" style="width:682.9pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.9.9.1.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.9.9.1.1.1.1.1">
            <span class="ltx_text" id="S6.T4.9.9.1.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.9.9.1.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/debater2.png" width="598"/>
            </span>
           </span>
          </span>
          First, let’s gather the information about the bands. Letters to Cleo is an alternative rock band from Boston, Massachusetts… Based on the information provided by Agent_0, Letters to Cleo had six members, while Screaming Trees had four members. Therefore, Letters to Cleo had more members than Screaming Trees. Therefore, the answer is [Letters to Cleo].
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.10.10">
       <td class="ltx_td ltx_align_justify ltx_align_top" colspan="2" id="S6.T4.10.10.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.10.10.1.1">
         <span class="ltx_p" id="S6.T4.10.10.1.1.1" style="width:682.9pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.10.10.1.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.10.10.1.1.1.1.1">
            <span class="ltx_text" id="S6.T4.10.10.1.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.10.10.1.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/judge.png" width="598"/>
            </span>
           </span>
          </span>
          [Yes]
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S6.T4.11.11">
       <td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" colspan="2" id="S6.T4.11.11.1">
        <span class="ltx_inline-block ltx_align_top" id="S6.T4.11.11.1.1">
         <span class="ltx_p" id="S6.T4.11.11.1.1.1" style="width:682.9pt;">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S6.T4.11.11.1.1.1.1" style="width:17.3pt;">
           <span class="ltx_p" id="S6.T4.11.11.1.1.1.1.1">
            <span class="ltx_text" id="S6.T4.11.11.1.1.1.1.1.1" style="position:relative; bottom:-0.3pt;">
             <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="598" id="S6.T4.11.11.1.1.1.1.1.1.g1" src="/html/2312.04854/assets/figures/imgs/summarizer.png" width="598"/>
            </span>
           </span>
          </span>
          Based on the information provided by Agent 0 and Agent 1, it can be concluded that Letters to Cleo had more members than Screaming Trees. Letters to Cleo had six members, while Screaming Trees had four members. Therefore, the final answer is [Letters to Cleo].
         </span>
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 4:
     </span>
     Case study of debate on HotpotQA dataset. In this case, there were two rounds of debate. The gray part indicates that the debater selected helpful evidence from the evidence pool.
    </figcaption>
   </figure>
  </div>
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Role Definition Prompt
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.1">
    The definition of debater role is:
   </p>
   <blockquote class="ltx_quote" id="A1.p1.2">
    <p class="ltx_p" id="A1.p1.2.1">
     <span class="ltx_text ltx_font_italic" id="A1.p1.2.1.1">
      You are an intelligent, diplomatic, and assertive debate agent. Your task is to engage in intellectual debates with other agents, striving to reach a consensus on various topics. It’s crucial to maintain your stance when you are confident about the correctness of your opinion. However, if you identify an error in your argument, you should promptly acknowledge it and amend your stance with the correct information. Your ultimate goal is to contribute to an informed, balanced, and accurate discussion.
     </span>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para" id="A1.p2">
   <p class="ltx_p" id="A1.p2.1">
    The definition of judge role prompt is:
   </p>
   <blockquote class="ltx_quote" id="A1.p2.2">
    <p class="ltx_p" id="A1.p2.2.1">
     <span class="ltx_text ltx_font_italic" id="A1.p2.2.1.1">
      As a judge agent, your primary responsibility is to impartially evaluate the responses of other agents for consistency. Please ensure your judgments are objective, relying solely on the coherence and alignment of the provided answers.
     </span>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para" id="A1.p3">
   <p class="ltx_p" id="A1.p3.1">
    The definition of summarizer role prompt is:
   </p>
  </div>
  <div class="ltx_para" id="A1.p4">
   <blockquote class="ltx_quote" id="A1.p4.1">
    <p class="ltx_p" id="A1.p4.1.1">
     <span class="ltx_text ltx_font_italic" id="A1.p4.1.1.1">
      You are an intelligent summarizer agent, tasked with synthesizing the responses of other agents into a concise and comprehensive final answer. Your role is not to generate original responses, but to condense the information provided by other agents into a succinct summary.
     </span>
    </p>
   </blockquote>
  </div>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Dataset Details
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    For different datasets, we give a brief introduction in this section. Table
    <a class="ltx_ref" href="#S6.T2" title="Table 2 ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    shows the statistivs of datasets.
   </p>
  </div>
  <section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Single-Hop Reasoning
   </h4>
   <div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
    <ul class="ltx_itemize" id="A2.I1">
     <li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I1.i1.p1">
       <p class="ltx_p" id="A2.I1.i1.p1.1">
        TriviaQA
        <cite class="ltx_cite ltx_citemacro_cite">
         Joshi et al. (
         <a class="ltx_ref" href="#bib.bib13" title="">
          2017
         </a>
         )
        </cite>
        : A question-answering dataset based on Wikipedia and real web texts, which consists of human-verified and machine-generated QA subsets.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I1.i2.p1">
       <p class="ltx_p" id="A2.I1.i2.p1.1">
        NQ
        <cite class="ltx_cite ltx_citemacro_cite">
         Kwiatkowski et al. (
         <a class="ltx_ref" href="#bib.bib16" title="">
          2019
         </a>
         )
        </cite>
        : A real user QA dataset based on Wikipedia.
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    Multi-Hop Reasoning
   </h4>
   <div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
    <ul class="ltx_itemize" id="A2.I2">
     <li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I2.i1.p1">
       <p class="ltx_p" id="A2.I2.i1.p1.1">
        HotpotQA
        <cite class="ltx_cite ltx_citemacro_cite">
         Yang et al. (
         <a class="ltx_ref" href="#bib.bib43" title="">
          2018
         </a>
         )
        </cite>
        : A multi-hop QA dataset based on English Wikipedia. Each question requires reference to two golden paragraphs to deduce the answer.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I2.i2.p1">
       <p class="ltx_p" id="A2.I2.i2.p1.1">
        2WikiMultiHopQA
        <cite class="ltx_cite ltx_citemacro_cite">
         Ho et al. (
         <a class="ltx_ref" href="#bib.bib9" title="">
          2020
         </a>
         )
        </cite>
        : Similar to HotpotQA, a QA dataset that requires multi-step reasoning.
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
  <section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
   <h4 class="ltx_title ltx_title_paragraph">
    Fact Checking
   </h4>
   <div class="ltx_para" id="A2.SS0.SSS0.Px3.p1">
    <ul class="ltx_itemize" id="A2.I3">
     <li class="ltx_item" id="A2.I3.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I3.i1.p1">
       <p class="ltx_p" id="A2.I3.i1.p1.1">
        FEVER
        <cite class="ltx_cite ltx_citemacro_cite">
         Thorne et al. (
         <a class="ltx_ref" href="#bib.bib32" title="">
          2018
         </a>
         )
        </cite>
        : A publicly available dataset for fact extraction and verification against textual source. These claims were extracted from Wikipedia by human annotators.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A2.I3.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A2.I3.i2.p1">
       <p class="ltx_p" id="A2.I3.i2.p1.1">
        FEVEROUS
        <cite class="ltx_cite ltx_citemacro_cite">
         Aly et al. (
         <a class="ltx_ref" href="#bib.bib1" title="">
          2021
         </a>
         )
        </cite>
        : Similar to FEVER, it further extends the content of the dataset.
       </p>
      </div>
     </li>
    </ul>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Evaluation Metric
  </h2>
  <div class="ltx_para" id="A3.p1">
   <p class="ltx_p" id="A3.p1.1">
    In the context of large models, the evaluation of generative tasks is indeed difficult. Some existing works use EM evaluation metric for QA tasks. We believe that this metric is relatively strict, and the evaluation is not accurate enough. As shown in the table
    <a class="ltx_ref" href="#S6.T3" title="Table 3 ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="A3.p2">
   <p class="ltx_p" id="A3.p2.1">
    <cite class="ltx_cite ltx_citemacro_citet">
     Zheng et al. (
     <a class="ltx_ref" href="#bib.bib47" title="">
      2023
     </a>
     )
    </cite>
    illustrate the advantages and disadvantages of using GPT4 for evaluation. For example, when evaluating two answers, there will be sensitivity to location, and there may be some bias. But, in the generative reasoning datasets of this work, the answer to each question is relatively short, and for the correctness of the evaluation, we provide reference answers during the evaluation process. The role of GPT4 Eval is defined as:
   </p>
   <blockquote class="ltx_quote" id="A3.p2.2">
    <p class="ltx_p" id="A3.p2.2.1">
     <span class="ltx_text ltx_font_italic" id="A3.p2.2.1.1">
      As a judge agent, your primary responsibility is to impartially evaluate the evaluation answer for correct. Please ensure your judgments are objective, relying solely on the coherence and alignment of the provided answers.
     </span>
    </p>
   </blockquote>
  </div>
  <div class="ltx_para" id="A3.p3">
   <p class="ltx_p" id="A3.p3.1">
    The full prompt for the GPT4 evaluation answer is shown in Prompt
    <a class="ltx_ref" href="#A3" title="Appendix C Evaluation Metric ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      C
     </span>
    </a>
    .
   </p>
  </div>
  <figure class="ltx_figure" id="A3.1">
   <svg class="ltx_picture ltx_centering" height="438.33" id="A3.1.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,438.33) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 433 C 0 435.94 2.38 438.33 5.32 438.33 L 594.68 438.33 C 597.62 438.33 600 435.94 600 433 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 409.42 L 598.62 409.42 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#666666" fill-opacity="1.0">
      <path d="M 1.38 410.81 L 1.38 433 C 1.38 435.18 3.15 436.94 5.32 436.94 L 594.68 436.94 C 596.85 436.94 598.62 435.18 598.62 433 L 598.62 410.81 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 417.72)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.1.pic1.1.1.1.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A3.1.pic1.1.1.1.1.1.1">
         Prompt C.1: GPT4 Eval
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
      <foreignobject color="#000000" height="394.2" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A3.1.pic1.2.2.2.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.1">
         You need to judge the correctness of the evaluation answer based on the question and the reference answer to the question. The answer of the agents are typically denoted with the [answer] format. Return [True] if the answer to be evaluated is the correct answer to the question, otherwise return [False].
Here are some examples:
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.2">
         Question: The VS-300 was a type of what?
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.3">
         Reference answers: Helicopters, Civilian helicopter, Pescara (helicopter), Cargo helicopter, Copter, Helecopter, List of deadliest helicopter crashes, Helichopper, Helocopter, Cargo Helicopter, Anatomy of a helicopter
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.4">
         Evaluation answer: helicopter
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.5">
         Answer: Let’s think step by step! The evaluation answer, "helicopter" is indeed a correct answer to the question, "The VS-300 was a type of what?" It matches with the reference answers provided, which include "Helicopters," "Civilian helicopter," "Cargo helicopter," and other variants of the word "helicopter." Therefore, the answer is [True].
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.6">
         Question: Who played a character based on Bob Fosse in a 1979 Oscar winning film?
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.7">
         Reference answers: Roy Scheider, Roy R. Scheider
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.8">
         Evaluation answer: Cliff Gorman
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.9">
         Answer: Let’s think step by step! The evaluation answer is incorrect. The character based on Bob Fosse in the 1979 Oscar-winning film "All That Jazz" was played by Roy Scheider, not Cliff Gorman. Therefore, the answer is [False].
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.10">
         (END OF EXAMPLES)
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.11">
         Question: {
         <span class="ltx_text ltx_font_bold" id="A3.1.pic1.2.2.2.1.1.11.1">
          question
         </span>
         }
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.12">
         Reference answers: {
         <span class="ltx_text ltx_font_bold" id="A3.1.pic1.2.2.2.1.1.12.1">
          reference_answers
         </span>
         }
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.13">
         Evaluation answer: {
         <span class="ltx_text ltx_font_bold" id="A3.1.pic1.2.2.2.1.1.13.1">
          evaluation_answer
         </span>
         }
        </span>
        <span class="ltx_p" id="A3.1.pic1.2.2.2.1.1.14">
         Answer: Let’s think step by step!
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </figure>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Case Study
  </h2>
  <div class="ltx_para" id="A4.p1">
   <p class="ltx_p" id="A4.p1.1">
    To analyze the performance of the agent during the debate, we selected a case in the HotpotQA data set for analysis, as shown in Table
    <a class="ltx_ref" href="#S6.T4" title="Table 4 ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    . In the first round of expressing opinions, debater 1 believed that he did not need external knowledge, so he hallucinated and generated wrong answers. In the next round of debate, by introducing external prior knowledge and debating with other debaters, debater 1 corrected his own point of view and obtained a consistent and correct answer. This proves the advantage of introducing prior knowledge, which can help the agent get out of its cognitive contraints and improve model performance.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A5">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix E
   </span>
   Prompts
  </h2>
  <div class="ltx_para" id="A5.p1">
   <p class="ltx_p" id="A5.p1.1">
    We show an example of prompts in HotpotQA dataset for different steps in the debate process in Prompt
    <a class="ltx_ref" href="#A5" title="Appendix E Prompts ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      E
     </span>
    </a>
    ,
    <a class="ltx_ref" href="#A5" title="Appendix E Prompts ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      E
     </span>
    </a>
    ,
    <a class="ltx_ref" href="#A5" title="Appendix E Prompts ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      E
     </span>
    </a>
    ,
    <a class="ltx_ref" href="#A5" title="Appendix E Prompts ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      E
     </span>
    </a>
    , and
    <a class="ltx_ref" href="#A5" title="Appendix E Prompts ‣ Apollo’s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates">
     <span class="ltx_text ltx_ref_tag">
      E
     </span>
    </a>
    .
   </p>
  </div>
  <figure class="ltx_figure" id="A5.1">
   <svg class="ltx_picture ltx_centering" height="322.09" id="A5.1.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,322.09) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 316.77 C 0 319.71 2.38 322.09 5.32 322.09 L 594.68 322.09 C 597.62 322.09 600 319.71 600 316.77 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 293.19 L 598.62 293.19 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#666666" fill-opacity="1.0">
      <path d="M 1.38 294.57 L 1.38 316.77 C 1.38 318.95 3.15 320.71 5.32 320.71 L 594.68 320.71 C 596.85 320.71 598.62 318.95 598.62 316.77 L 598.62 294.57 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 301.49)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.1.pic1.1.1.1.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.1.pic1.1.1.1.1.1.1">
         Prompt Example E.1: Self-Selection
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
      <foreignobject color="#000000" height="277.97" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.1.pic1.2.2.2.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.1">
         Please select evidence from the evidence pool that will help you answer the question. If the evidence pool does not contain the information needed to answer the question, add [No Found] at the end of your response. If the evidence pool has evidence that can help you answer the question, please return up to 3 of the most helpful evidence. Put the number in square brackets.
        </span>
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.2">
         Evidence:
        </span>
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.3">
         (0) On July 20, 1969, American astronauts Neil Armstrong (1930-2012) and Edwin B̈uzzÄldrin (1930-) became the first humans ever to land on the moon.
        </span>
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.4">
         (1) Apollo 11 (July 16–24, 1969) was the American spaceflight that first landed humans on the Moon. Commander Neil Armstrong and lunar module pilot Buzz Aldrin …
        </span>
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.5">
         (2) Neil Armstrong on the Moon. At 02:56 GMT on 21 July 1969, Armstrong became the first person to step onto the Moon. He was joined by Aldrin 19 minutes
        </span>
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.6">
         (3) It reads, "Here men from the planet Earth first set foot upon the moon. July 1969 A.D. We came in peace for all mankind." Armstrong and Aldrin
        </span>
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.7">
         (4) Apollo 11 launched from Cape Kennedy on July 16, 1969, carrying Commander Neil Armstrong, Command Module Pilot Michael Collins and Lunar
        </span>
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.8">
         Question: On what date in 1969 did Neil Armstrong first set foot on the Moon?
        </span>
        <span class="ltx_p" id="A5.1.pic1.2.2.2.1.1.9">
         Answer: Let’s think step by step!
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </figure>
  <figure class="ltx_figure" id="A5.2">
   <svg class="ltx_picture ltx_centering" height="322.09" id="A5.2.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,322.09) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 316.77 C 0 319.71 2.38 322.09 5.32 322.09 L 594.68 322.09 C 597.62 322.09 600 319.71 600 316.77 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 293.19 L 598.62 293.19 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#666666" fill-opacity="1.0">
      <path d="M 1.38 294.57 L 1.38 316.77 C 1.38 318.95 3.15 320.71 5.32 320.71 L 594.68 320.71 C 596.85 320.71 598.62 318.95 598.62 316.77 L 598.62 294.57 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 301.49)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.2.pic1.1.1.1.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.2.pic1.1.1.1.1.1.1">
         Prompt Example E.2: First Round of Debate
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
      <foreignobject color="#000000" height="277.97" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.2.pic1.2.2.2.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.1">
         Answer the question as accurately as possible based on the information given, and put the answer in the form [answer].
Here is an example:
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.2">
         Question: who was the first person killed in a car accident?
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.3">
         Answer: Let’s think step by step! This tragic event occurred on August 17, 1896, in London, England. Bridget Driscoll was struck and killed by an automobile driven by Arthur Edsall, who was driving at a speed of approximately 4 miles per hour (about 6.4 kilometers per hour) at the Crystal Palace in London. Therefore, the answer is [Bridget Driscoll].
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.4">
         (END OF EXAMPLE)
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.5">
         Evidence:
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.6">
         (0) On July 20, 1969, American astronauts Neil Armstrong (1930-2012) and Edwin B̈uzzÄldrin (1930-) became the first humans ever to land on the moon.
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.7">
         (1) Neil Armstrong on the Moon. At 02:56 GMT on 21 July 1969, Armstrong became the first person to step onto the Moon. He was joined by Aldrin 19 minutes
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.8">
         (2) Apollo 11 (July 16–24, 1969) was the American spaceflight that first landed humans on the Moon. Commander Neil Armstrong and lunar module pilot Buzz Aldri
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.9">
         Question: On what date in 1969 did Neil Armstrong first set foot on the Moon?
        </span>
        <span class="ltx_p" id="A5.2.pic1.2.2.2.1.1.10">
         Answer: Let’s think step by step!
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </figure>
  <figure class="ltx_figure" id="A5.3">
   <svg class="ltx_picture ltx_centering" height="272.28" id="A5.3.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,272.28) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 266.96 C 0 269.9 2.38 272.28 5.32 272.28 L 594.68 272.28 C 597.62 272.28 600 269.9 600 266.96 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 243.38 L 598.62 243.38 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#666666" fill-opacity="1.0">
      <path d="M 1.38 244.76 L 1.38 266.96 C 1.38 269.14 3.15 270.9 5.32 270.9 L 594.68 270.9 C 596.85 270.9 598.62 269.14 598.62 266.96 L 598.62 244.76 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 251.68)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.3.pic1.1.1.1.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.3.pic1.1.1.1.1.1.1">
         Prompt Example E.3: Second Round and Beyond of Debate Example
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
      <foreignobject color="#000000" height="228.16" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.3.pic1.2.2.2.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.3.pic1.2.2.2.1.1.1">
         There are a few other agents assigned the same task, it’s your responsibility to discuss with them and think critically. You can update your answer with other agents’ answers or given evidences as advice, or you can not update your answer. Please put the answer in the form [answer].
        </span>
        <span class="ltx_p" id="A5.3.pic1.2.2.2.1.1.2">
         Evidence:
        </span>
        <span class="ltx_p" id="A5.3.pic1.2.2.2.1.1.3">
         (0) On July 20, 1969, American astronauts Neil Armstrong (1930-2012) and Edwin "Buzz" Aldrin (1930-) became the first humans ever to land on the moon.
        </span>
        <span class="ltx_p" id="A5.3.pic1.2.2.2.1.1.4">
         Answers from other Agents:
        </span>
        <span class="ltx_p" id="A5.3.pic1.2.2.2.1.1.5">
         (Agent_1) On July 21, 1969, at 02:56 GMT, Neil Armstrong became the first person to step onto the Moon. He was joined by Buzz Aldrin 19 minutes later. Therefore, the answer is [July 21, 1969].
        </span>
        <span class="ltx_p" id="A5.3.pic1.2.2.2.1.1.6">
         Here is your historical answer: Based on the evidence provided, Neil Armstrong first set foot on the Moon on July 20, 1969. Therefore, the answer is [July 20, 1969].
        </span>
        <span class="ltx_p" id="A5.3.pic1.2.2.2.1.1.7">
         Question: On what date in 1969 did Neil Armstrong first set foot on the Moon?
        </span>
        <span class="ltx_p" id="A5.3.pic1.2.2.2.1.1.8">
         Answer: Let’s think step by step!
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </figure>
  <figure class="ltx_figure" id="A5.4">
   <svg class="ltx_picture ltx_centering" height="305.49" id="A5.4.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,305.49) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 300.17 C 0 303.11 2.38 305.49 5.32 305.49 L 594.68 305.49 C 597.62 305.49 600 303.11 600 300.17 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 276.59 L 598.62 276.59 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#666666" fill-opacity="1.0">
      <path d="M 1.38 277.97 L 1.38 300.17 C 1.38 302.34 3.15 304.11 5.32 304.11 L 594.68 304.11 C 596.85 304.11 598.62 302.34 598.62 300.17 L 598.62 277.97 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 284.89)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.4.pic1.1.1.1.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.4.pic1.1.1.1.1.1.1">
         Prompt Example E.4: Judge Example
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
      <foreignobject color="#000000" height="261.37" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.4.pic1.2.2.2.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.4.pic1.2.2.2.1.1.1">
         The answer of the agents are typically denoted with the [answer] format. Your task is to extract each agent’s answer and evaluate the consistency of their answers to the question. If all agents have provided correct and consistent answers, respond with [Yes]. If their answers are inconsistent, respond with [No]. Please ensure to encase your response - Yes or No - within square brackets.
        </span>
        <span class="ltx_p" id="A5.4.pic1.2.2.2.1.1.2">
         Question: On what date in 1969 did Neil Armstrong first set foot on the Moon?
        </span>
        <span class="ltx_p" id="A5.4.pic1.2.2.2.1.1.3">
         Agent Responses:
        </span>
        <span class="ltx_p" id="A5.4.pic1.2.2.2.1.1.4">
         (Agent_0) Based on the historical evidence provided, Neil Armstrong first set foot on the Moon on July 20, 1969. This event occurred during the Apollo 11 mission, when Armstrong and Buzz Aldrin landed the lunar module Eagle in the Sea of Tranquility. Armstrong then descended the ladder and made his famous statement. Therefore, the correct answer is [July 20, 1969].
        </span>
        <span class="ltx_p" id="A5.4.pic1.2.2.2.1.1.5">
         (Agent_1)Based on the historical evidence, Neil Armstrong first set foot on the Moon on July 20, 1969. This monumental event took place during the Apollo 11 mission. Armstrong, along with Buzz Aldrin, landed the lunar module Eagle in the Sea of Tranquility. Armstrong then descended the ladder and famously declared, "That’s one small step for man, one giant leap for mankind." Therefore, the answer is [July 20, 1969].
        </span>
        <span class="ltx_p" id="A5.4.pic1.2.2.2.1.1.6">
         Answer: Let’s think step by step!
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </figure>
  <figure class="ltx_figure" id="A5.5">
   <svg class="ltx_picture ltx_centering" height="571.16" id="A5.5.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,571.16) matrix(1 0 0 -1 0 0)">
     <g fill="#404040" fill-opacity="1.0">
      <path d="M 0 5.32 L 0 565.84 C 0 568.78 2.38 571.16 5.32 571.16 L 594.68 571.16 C 597.62 571.16 600 568.78 600 565.84 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#F2F2F2" fill-opacity="1.0">
      <path d="M 1.38 5.32 L 1.38 542.26 L 598.62 542.26 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#666666" fill-opacity="1.0">
      <path d="M 1.38 543.64 L 1.38 565.84 C 1.38 568.01 3.15 569.78 5.32 569.78 L 594.68 569.78 C 596.85 569.78 598.62 568.01 598.62 565.84 L 598.62 543.64 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 550.56)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.5.pic1.1.1.1.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.5.pic1.1.1.1.1.1.1">
         Prompt Example E.5: Summarizer Example
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)">
      <foreignobject color="#000000" height="527.04" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A5.5.pic1.2.2.2.1.1" style="width:421.6pt;">
        <span class="ltx_p" id="A5.5.pic1.2.2.2.1.1.1">
         Please summarize the final answer from answer of all agents. Place the final answer of question in the form of [answer].
Here is an example:
Question: How many husbands did the Wife of Bath have, as reported in Chaucer’s Canterbury Tales?
Agent 0: In Chaucer’s Canterbury Tales, the Wife of Bath claims to have had [five] husbands.
Agent 1: In Chaucer’s Canterbury Tales, the Wife of Bath, one of the most memorable characters, claims to have had [five] husbands.
Answer: Let’s think step by step! Based on the answers provided by Agent 0 and Agent 1, it can be concluded that in Chaucer’s Canterbury Tales, the Wife of Bath claims to have had five husbands. Therefore, the final answer is [five].
        </span>
        <span class="ltx_p" id="A5.5.pic1.2.2.2.1.1.2">
         Question: Ezzard Charles was a world champion in which sport?
Agent 0: Ezzard Charles was a world champion in the sport of boxing. He held the world heavyweight title from 1949 to 1951. Therefore, the answer is "boxing".
Agent 1: Ezzard Charles was a world champion in boxing. Therefore, the answer is "boxing".
Answer: Let’s think step by step! Based on the responses from Agent 0 and Agent 1, it is clear that Ezzard Charles was a world champion in the sport of boxing. He held the world heavyweight title from 1949 to 1951. Therefore, the final answer is [boxing].
        </span>
        <span class="ltx_p" id="A5.5.pic1.2.2.2.1.1.3">
         Question: In which city were Rotary Clubs set up in 1905?
Agent 0: The first Rotary Club was established in Chicago, Illinois, United States in 1905. Therefore, the answer is Chicago.
Agent 1: The Rotary Clubs were set up in 1905 in the city of Chicago, Illinois, United States. Therefore, the answer is City of Chicago.
Answer: Let’s think step by step! Based on the responses from both Agent 0 and Agent 1, it is clear that the Rotary Clubs were first established in the city of Chicago, Illinois, United States in the year 1905. Therefore, the final answer is [Chicago].
(END OF EXAMPLE)
        </span>
        <span class="ltx_p" id="A5.5.pic1.2.2.2.1.1.4">
         Question: On what date in 1969 did Neil Armstrong first set foot on the Moon?
        </span>
        <span class="ltx_p" id="A5.5.pic1.2.2.2.1.1.5">
         Agent Responses:
        </span>
        <span class="ltx_p" id="A5.5.pic1.2.2.2.1.1.6">
         (Agent_0) Based on the historical evidence provided, Neil Armstrong first set foot on the Moon on July 20, 1969. This event occurred during the Apollo 11 mission, when Armstrong and Buzz Aldrin landed the lunar module Eagle in the Sea of Tranquility. Armstrong then descended the ladder and made his famous statement. Therefore, the correct answer is [July 20, 1969].
        </span>
        <span class="ltx_p" id="A5.5.pic1.2.2.2.1.1.7">
         (Agent_1)Based on the historical evidence, Neil Armstrong first set foot on the Moon on July 20, 1969. This monumental event took place during the Apollo 11 mission. Armstrong, along with Buzz Aldrin, landed the lunar module Eagle in the Sea of Tranquility. Armstrong then descended the ladder and famously declared, "That’s one small step for man, one giant leap for mankind." Therefore, the answer is [July 20, 1969].
        </span>
        <span class="ltx_p" id="A5.5.pic1.2.2.2.1.1.8">
         Answer: Let’s think step by step!
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
  </figure>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
</article>
