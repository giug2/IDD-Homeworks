<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AutoPETIII: The Tracer Frontier What frontier ?</title>
<!--Generated on Thu Sep 19 12:47:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="PET Functional Imaging Segmentation Deep Learning" lang="en" name="keywords"/>
<base href="/html/2410.02807v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S1" title="In AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S2" title="In AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Competition</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S2.SS0.SSS1" title="In 2 Competition ‚Ä£ AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.0.1 </span>This year‚Äôs edition,</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S2.SS0.SSS2" title="In 2 Competition ‚Ä£ AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.0.2 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S2.SS0.SSS3" title="In 2 Competition ‚Ä£ AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.0.3 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S2.SS0.SSS4" title="In 2 Competition ‚Ä£ AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.0.4 </span>nn-Unetv2</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S3" title="In AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Our Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S3.SS0.SSS1" title="In 3 Our Method ‚Ä£ AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.0.1 </span>Windowing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S3.SS0.SSS2" title="In 3 Our Method ‚Ä£ AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.0.2 </span>Supplementary labels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S3.SS0.SSS3" title="In 3 Our Method ‚Ä£ AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.0.3 </span>Tracer Discriminator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S3.SS0.SSS4" title="In 3 Our Method ‚Ä£ AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.0.4 </span>Segmentation Networks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S4" title="In AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#S5" title="In AutoPETIII: The Tracer Frontier What frontier ?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>INSA Rouen Normandie, Univ Rouen Normandie, Universit√© Le Havre Normandie, Normandie Univ, LITIS UR 4108, F-76000 Rouen, France
<br class="ltx_break"/></span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Nuclear Medicine Department, Henri Becquerel Cancer Center, Rouen, France
<br class="ltx_break"/></span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Radiotherapy Department, Henri Becquerel Cancer Center, Rouen, France 
<br class="ltx_break"/></span></span></span><span class="ltx_note ltx_role_institutetext" id="id4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Siemens Healthineers
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id4.1"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">email: </span>zacharia.mesbah@chb-unicancer.fr</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">AutoPETIII: The Tracer Frontier 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id1.id1">What frontier ?</span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zacharia Mesbah
</span><span class="ltx_author_notes">112244</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Leo Mottay
</span><span class="ltx_author_notes">112244</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Romain Modzelewski
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pierre Decazes
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">S√©bastien Hapdey
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Su Ruan
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sebastien Thureau 
<br class="ltx_break"/>
</span><span class="ltx_author_notes">33</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">For the last three years, the AutoPET competition gathered the medical imaging community around a hot topic: lesion segmentation on Positron Emitting Tomography (PET) scans. Each year a different aspect of the problem is presented; in 2024 the multiplicity of existing and used tracers was at the core of the challenge. Specifically, this year‚Äôs edition aims to develop a fully automatic algorithm capable of performing lesion segmentation on a PET/CT scan, without knowing the tracer, which can either be a FDG or PSMA-based tracer.
In this paper we describe how we used the nnUNetv2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#bib.bib1" title="">1</a>]</cite> framework to train two sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion segmentation as well as a MIP-CNN to choose which set of models to use for segmentation.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>PET Functional Imaging Segmentation Deep Learning
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">For cancer detection and diagnosis, Positron Emission Tomography (PET) imaging is extremely valuable. It allows to explore specific functions in the body, which is especially fit for cancer detection since cancer cells are cells exhibiting abnormal behavior. PET consists in injecting a radioactive tracer in the patient‚Äôs body and observing its distribution using the gamma rays it emits.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.2">Tracers are designed to target a specific function of the body. The most common, <math alttext="{}^{18}F" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mmultiscripts id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">F</mi><mprescripts id="S1.p2.1.m1.1.1a" xref="S1.p2.1.m1.1.1.cmml"></mprescripts><mrow id="S1.p2.1.m1.1.1b" xref="S1.p2.1.m1.1.1.cmml"></mrow><mn id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">18</mn></mmultiscripts><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1">superscript</csymbol><ci id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">ùêπ</ci><cn id="S1.p2.1.m1.1.1.3.cmml" type="integer" xref="S1.p2.1.m1.1.1.3">18</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">{}^{18}F</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">start_FLOATSUPERSCRIPT 18 end_FLOATSUPERSCRIPT italic_F</annotation></semantics></math>-FluoroDesoxyGlucose (<sup class="ltx_sup" id="S1.p2.2.1"><span class="ltx_text ltx_font_italic" id="S1.p2.2.1.1">18</span></sup>FDG) is a sugar, which accumulates (i.e. uptakes) in the parts of the body that consume energy (i.e. high-function). The cancer cells, in their uncontrolled reproductive frenzy, need a lot of energy. They‚Äôre usually very visible on PET scans, which makes this imaging modality the keystone of cancer detection and cancer treatment protocols.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">It is always accompanied by a Computed Tomography (CT) scan, which serves for attenuation correction of the PET scan. However, the images from this modality also provide a different, useful information. They show the mapping of density in the body, which gives anatomical information. The combination of the anatomical and functional information is used by physicians to determine which uptakes are malignant (lesions) and which are not (physiological uptake). Some examples for non-malignant uptakes of FDG are liver, brain and kidneys. The liver and kidneys have high uptake because of their cleaning role, while the brain is always active and thus consumes sugar.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Over the last decade, another family of tracers has been used more and more. The Prostate Specific Membrane Antigen (PSMA) is a protein usually found in both healthy and cancerous prostate tissues. Due to this property, PSMA has been used in nuclear medicine to track prostate cancer in the human body. Unlike FDG, this family of tracer does not accumulate in the organs with the most activity: a healthy brain will show no activity on a PSMA PET scan. However, the waste clearing organs will still show activity as well as other organs like lacrimal glands and the gastro-intestinal tract.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">FDG and PSMA PET scans are slowly anchoring themselves in routine clinical practice, which means an increase in the work intensive task of PET analysis by physicians. The first step of this task is to detect and delineate all the malignant lesions. Indeed the number of lesions and their localization are key in establishing patient prognosis and determining further treatment.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To alleviate this task from the physicians‚Äô workload, to develop a reliable automatic PET lesions segmentation tool would be extremely helpful.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Competition</h2>
<section class="ltx_subsubsection" id="S2.SS0.SSS1">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.1 </span>This year‚Äôs edition,</h3>
<div class="ltx_para" id="S2.SS0.SSS1.p1">
<p class="ltx_p" id="S2.SS0.SSS1.p1.1">AutoPET III challenge aims, just as its predecessors did, to build an algorithm capable of delineating cancerous lesions in PET/XCT scans. However this year‚Äôs novelty is the appearance of not only FDG but also PSMA PET scans. The catch is that no prior knowledge is given (at test time) about the tracer.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS1.p2">
<p class="ltx_p" id="S2.SS0.SSS1.p2.1">To achieve this, participants are given 1014 FDG PET/CT scans and 597 PSMA PET/CT scans, gathered from two German medical centers:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">University Hospital of T√ºbingen (FDG data)</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">University Hospital of the LMU in Munich (PSMA data)</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS0.SSS1.p3">
<p class="ltx_p" id="S2.SS0.SSS1.p3.1">For each of these studies, two modalities are available, in NifTi format:</p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1">The Computed Tomography, resampled towards the PET spacing</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1">The PET, for which voxel values have been converted to Standardized Uptake Value.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS0.SSS1.p4">
<p class="ltx_p" id="S2.SS0.SSS1.p4.1">Studies selected were from patients with lung cancer, lymphoma, melanoma or healthy patients for FDG and prostate cancer patients for PSMA.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS2">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.2 </span>Dataset</h3>
<div class="ltx_para" id="S2.SS0.SSS2.p1">
<p class="ltx_p" id="S2.SS0.SSS2.p1.1">The dataset is split in three parts:</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS2.p2">
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1">The training set, which can be used by all contestants to build and train their algorithm.
It corresponds to the 1611 studies presented above.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1">The preliminary test set, containing 5 scans. Considering the statistical non-significance of this set, it should be used to make sure no catastrophic failure happens in the algorithm execution.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1">The final testing set, with 200 scans. This is the scoring set which will be used for the competition ranking.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1" style="padding:2.5pt 12.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2" style="padding:2.5pt 12.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Training set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3" style="padding:2.5pt 12.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">Valid</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.4" style="padding:2.5pt 12.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1" style="padding:2.5pt 12.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.1.1">No. of Scans</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.2.1.2" style="padding:2.5pt 12.0pt;">1611</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.2.1.3" style="padding:2.5pt 12.0pt;">5</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.2.1.4" style="padding:2.5pt 12.0pt;">200</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dataset Distribution: train / test split</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS3">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.3 </span>Evaluation</h3>
<div class="ltx_para" id="S2.SS0.SSS3.p1">
<p class="ltx_p" id="S2.SS0.SSS3.p1.1">Evaluation on the final test set is performed using a mixed model. The metrics included are:</p>
<ul class="ltx_itemize" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1">Dice Similarity Coefficient (DSC), a geometric similarity metric</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.1">False Positive Volume: The number of pixels in isolated, connected components of the prediction which are false positives</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S2.I4.i3.p1">
<p class="ltx_p" id="S2.I4.i3.p1.1">False Negative Volume: The number of pixels in isolated, connected components of the ground truth which are false negatives</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSS4">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.4 </span>nn-Unetv2</h3>
<div class="ltx_para" id="S2.SS0.SSS4.p1">
<p class="ltx_p" id="S2.SS0.SSS4.p1.1">nnUNetv2 is a deep neural network training framework, allowing anyone to train out-of-the-box segmentation networks, in two and three dimensions. It determines automatically the model architecture parameters (depth, width) and the training hyper-parameters using heuristics.
nnUNetv2 is displaying results at the state-of-the-art to this day as described in a recent paper<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS4.p2">
<p class="ltx_p" id="S2.SS0.SSS4.p2.1">It is considered an essential baseline for any research conducted on semantic segmentation, in the medical imaging domain mainly. It is also used in most winning submissions to MICCAI and other challenges based around segmentation.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Our Method</h2>
<section class="ltx_subsubsection" id="S3.SS0.SSS1">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.0.1 </span>Windowing</h3>
<div class="ltx_para" id="S3.SS0.SSS1.p1">
<p class="ltx_p" id="S3.SS0.SSS1.p1.1">We used a CT and PET windowing method. Instead of using a 2 channels input to our network, we use 4 channels. The first two channels remain the CT and PET volumes, but the third and fourth channels are clipped versions of the CT and PET scans. Precisely:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The PET scan is clipped between 0 and 20 SUV</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">The CT scan is clipped between -300 and 400 HU</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS0.SSS1.p2">
<p class="ltx_p" id="S3.SS0.SSS1.p2.1">This windowing‚Äôs goal is to deal with two problems caused by input data normalization:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">By normalizing between min and max, the values are squashed, which means that slow variations in the image are represented with very similar values which can in turn make differentiation harder for the network. We normalize over a smaller range of values to limit this effect</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">For each patient the maximum value can be different (especially in PETs). This lowers the meaningfulness of the normalized voxel values inputted in the network, as it is also dependant on the volume‚Äôs maximum value. Windowing also reduces this effect by making sure we are in a specific range of values</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS0.SSS2">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.0.2 </span>Supplementary labels</h3>
<div class="ltx_para" id="S3.SS0.SSS2.p1">
<p class="ltx_p" id="S3.SS0.SSS2.p1.1">We re-used a method from last year‚Äôs AutoPET II competition. The winning team had drastically reduced the amount of false positives by adding other anatomical contours to the segmentation. We defined a list of organs that were interesting for PET segmentation:</p>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1">the brain, heart and aorta : high FDG uptake</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1">liver, kidneys, urinary bladder: waste clearing activity</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1">spleen: sometimes very active</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I3.i4.p1">
<p class="ltx_p" id="S3.I3.i4.p1.1">digestive system, prostate: high PSMA uptake</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I3.i5.p1">
<p class="ltx_p" id="S3.I3.i5.p1.1">skeleton,</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S3.I3.i6.p1">
<p class="ltx_p" id="S3.I3.i6.p1.1">lungs, pancreas: added localization for the remaining organs</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="466" id="S3.F1.g1" src="x1.png" width="220"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="465" id="S3.F1.g2" src="x2.png" width="157"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="465" id="S3.F1.g3" src="x3.png" width="207"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="467" id="S3.F1.g4" src="x4.png" width="181"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of added labels for a FDG patient</figcaption>
</figure>
<div class="ltx_para" id="S3.SS0.SSS2.p2">
<p class="ltx_p" id="S3.SS0.SSS2.p2.1">We used TotalSegmentator<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02807v1#bib.bib3" title="">3</a>]</cite> to perform segmentation for all of these organs using the CT scans. We had to group some organs together to form broader classes (bones, digestive system, lungs etc‚Ä¶) in order to reduce the computational overhead.
On top of these organs we added the lesion back with maximum priority.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS2.p3">
<p class="ltx_p" id="S3.SS0.SSS2.p3.1">This method‚Äôs goal is to help the network figuring out malignant uptake from benign uptake.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="S3.SS0.SSS3">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.0.3 </span>Tracer Discriminator</h3>
<div class="ltx_para" id="S3.SS0.SSS3.p1">
<p class="ltx_p" id="S3.SS0.SSS3.p1.1">Our approach consists in using tracer-specific neural networks for tumor delineation. However, since the injected radio-tracer is not disclosed alongside volumes, a tracer detection strategy is required. To address this, we use a neural network for tracer discrimination. Its architecture consists of six consecutive 2D convolutions layers, followed by five fully connected layers, all with ReLU activation and a final sigmoid activation.
Initially, the volumes are resampled (3x3x3 spacing) and their maximum intensity projection (MIP) is computed in the coronal plane.
Next, a zero-padded 224x224 window is extracted around the center of the MIP and fed to our neural network.
During training, the model‚Äôs outputs (predicted tracer types) are compared against ground truth using Binary Cross-Entropy (BCE) as the loss function.
Optimization is performed using AdamW, with an initial learning rate of <math alttext="1e^{-4}" class="ltx_Math" display="inline" id="S3.SS0.SSS3.p1.1.m1.1"><semantics id="S3.SS0.SSS3.p1.1.m1.1a"><mrow id="S3.SS0.SSS3.p1.1.m1.1.1" xref="S3.SS0.SSS3.p1.1.m1.1.1.cmml"><mn id="S3.SS0.SSS3.p1.1.m1.1.1.2" xref="S3.SS0.SSS3.p1.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS0.SSS3.p1.1.m1.1.1.1" xref="S3.SS0.SSS3.p1.1.m1.1.1.1.cmml">‚Å¢</mo><msup id="S3.SS0.SSS3.p1.1.m1.1.1.3" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS3.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.2.cmml">e</mi><mrow id="S3.SS0.SSS3.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.3.cmml"><mo id="S3.SS0.SSS3.p1.1.m1.1.1.3.3a" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S3.SS0.SSS3.p1.1.m1.1.1.3.3.2" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS3.p1.1.m1.1b"><apply id="S3.SS0.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1"><times id="S3.SS0.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1.1"></times><cn id="S3.SS0.SSS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS0.SSS3.p1.1.m1.1.1.2">1</cn><apply id="S3.SS0.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.2">ùëí</ci><apply id="S3.SS0.SSS3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.3"><minus id="S3.SS0.SSS3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.3"></minus><cn id="S3.SS0.SSS3.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS0.SSS3.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS3.p1.1.m1.1c">1e^{-4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS3.p1.1.m1.1d">1 italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, over 100 epochs with early stopping to reduce over-fitting.
Inference times on the entire dataset, tested on an Intel i7-11850H CPU, averages to 2.18 seconds per patient and 5-folds cross-validation yielded 99.64% accuracy.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsubsection" id="S3.SS0.SSS4">
<h3 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.0.4 </span>Segmentation Networks</h3>
<div class="ltx_para" id="S3.SS0.SSS4.p1">
<p class="ltx_p" id="S3.SS0.SSS4.p1.1">As described earlier, we use two tracer-specific segmentation networks. Both are trained using the nnUNet framework, with mostly unchanged parameters/hyper-parameters.
The only change occurred in the setting for the FDG model. Indeed, nnUNet is using the median spacing as a target spacing for all training data. However, the median (3mmx2mmx2mm) spacing for FDG data implied too much inference time as this was limited to 5 minutes per patient. We chose to replace it by an isotropic larger spacing (3.3mmx3.3mmx3.3mm), which allowed us to run a 6 folds ensembling with Test-Time Augmentation (TTA) for most patients <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We still set a threshold on the number of voxels where we remove some TTA to stay in the time limit.</span></span></span>.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS4.p2">
<p class="ltx_p" id="S3.SS0.SSS4.p2.1">We then trained the regular 5 folds ensemble with no post-processing for both tracers.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1" style="padding:2.5pt 12.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.2" style="padding:2.5pt 12.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Dice Score(%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3" style="padding:2.5pt 12.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">FNV</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.4" style="padding:2.5pt 12.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">FPV</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1" style="padding:2.5pt 12.0pt;">nnUNet baseline (1 model)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.2" style="padding:2.5pt 12.0pt;">61.41</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.3" style="padding:2.5pt 12.0pt;">25.61</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.4" style="padding:2.5pt 12.0pt;">18.71</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.3.2.1" style="padding:2.5pt 12.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.1.1">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.3.2.2" style="padding:2.5pt 12.0pt;">74.91</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.3.2.3" style="padding:2.5pt 12.0pt;">40.72</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.3.2.4" style="padding:2.5pt 12.0pt;">0.760</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of our model on the preliminary test set</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our model obtained results that situated us middle of the pack on the preliminary test set, which does give us an idea of what our final result will be.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The key takeaway from our participation in this challenge is that a dataset of such dimensions makes the training of highly reliable PET lesions segmentation deep learning based tools accessible to researchers in cancer research centers.
We will conduct future work, as well as review the others participants‚Äô methods to attempt developing a robust PET segmentation model.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Such a model would alleviate the burden of PET segmentation off the shoulders of physicians, in turn freeing their time to conduct more impactful work.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Fabian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul¬†F. Jaeger, Simon Kohl, Jakob Wasserthal, Gregor Koehler, Tobias Norajitra, Sebastian Wirkert, and Klaus¬†H. Maier-Hein.

</span>
<span class="ltx_bibblock">nnu-net: Self-adapting framework for u-net-based medical image segmentation, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Fabian Isensee, Tassilo Wald, Constantin Ulrich, Michaal Baumgartner, Saikat Roy, Klaus Maier-Hein, and Paul¬†F. Jaeger.

</span>
<span class="ltx_bibblock">nnu-net revisited: A call for rigorous validation in 3d medical image segmentation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jakob Wasserthal, Hanns-Christian Breit, Manfred¬†T. Meyer, Maurice Pradella, Daniel Hinck, Alexander¬†W. Sauter, Tobias Heye, Daniel¬†T. Boll, Joshy Cyriac, Shan Yang, Michael Bach, and Martin Segeroth.

</span>
<span class="ltx_bibblock">Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Radiology: Artificial Intelligence</span>, 5(5), September 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 12:47:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
