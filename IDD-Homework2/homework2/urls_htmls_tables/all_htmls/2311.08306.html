<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>On-the-Fly Fusion of Large Language Models and Machine Translation</title>
<!--Generated on Wed May 15 17:49:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2311.08306v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S1" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S2" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S2.SS0.SSS0.Px1" title="In 2 Method ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title">Standard Decoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S2.SS0.SSS0.Px2" title="In 2 Method ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title">Proposed Ensemble</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S3" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S4" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S5" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S5.SS1" title="In 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>MT Model Ensembling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S5.SS2" title="In 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Mixing Ratio Interpretation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S5.SS3" title="In 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Domain Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S5.SS4" title="In 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Document Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S5.SS5" title="In 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Unprompted LM Ensembling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S6" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S6.SS0.SSS0.Px1" title="In 6 Related work ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title">LLMs for MT:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S6.SS0.SSS0.Px2" title="In 6 Related work ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title">Ensembling:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S7" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#S8" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#A1" title="In On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#A1.SS1" title="In Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#A1.SS2" title="In Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Monolingual Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#A1.SS3" title="In Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#A1.SS4" title="In Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span><math alttext="\lambda" class="ltx_Math" display="inline"><semantics><mi>Œª</mi><annotation-xml encoding="MathML-Content"><ci>ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex">\lambda</annotation><annotation encoding="application/x-llamapun">italic_Œª</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#A1.SS5" title="In Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>COMET-22 CTXPro</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">On-the-Fly Fusion of Large Language Models and Machine Translation </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hieu Hoang ‚ÄÉHuda Khayrallah ‚ÄÉMarcin Junczys-Dowmunt 
<br class="ltx_break"/> Microsoft, 1 Microsoft Way, Redmond, WA 98052, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{hihoan,hkhayrallah,marcinjd}@microsoft.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">We propose on-the-fly ensembling of a neural machine translation (NMT) model with a large language model (LLM), prompted on the same task and input.
Through experiments on 4 language directions
with varying data amounts, we find that a slightly weaker-at-translation LLM can improve translations of a NMT model, and such an ensemble can produce better translations than ensembling two stronger NMT models.
We demonstrate that our ensemble method can be combined with various techniques from LLM prompting, such as in context learning and translation context.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">On-the-Fly Fusion of Large Language Models and Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1" style="padding-bottom:5.69054pt;"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Hieu Hoang ‚ÄÉHuda Khayrallah ‚ÄÉMarcin Junczys-Dowmunt</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">Microsoft, 1 Microsoft Way, Redmond, WA 98052, USA</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.1">{hihoan,hkhayrallah,marcinjd}@microsoft.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">For many English NLP tasks, LLMs <cite class="ltx_cite ltx_citemacro_cite">Brown et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib7" title="">2020</a>); Smith et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib48" title="">2022</a>); Chowdhery et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib10" title="">2022</a>); Touvron et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib54" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib55" title="">b</a>)</cite> are the clear state-of-the-art‚Äîe.g. sentiment analysis <cite class="ltx_cite ltx_citemacro_cite">Zhang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib69" title="">2023c</a>)</cite>, summarization <cite class="ltx_cite ltx_citemacro_cite">Zhang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib68" title="">2023b</a>)</cite>.
However, dedicated NMT outperforms all but the largest closed source LLMs <cite class="ltx_cite ltx_citemacro_cite">Jiao et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib24" title="">2023</a>)</cite> and dedicated MT is stronger in low resource settings <cite class="ltx_cite ltx_citemacro_cite">Hendy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib21" title="">2023</a>); Robinson et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib44" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">We propose a novel integration of a LLM and dedicated NMT model via token-level fusion. This ensembling combines strengths of each model, which emerge from their differences.
LLMs are trained on more data than NMT models, and have more parameters. While LLMs are exposed to some parallel data <cite class="ltx_cite ltx_citemacro_cite">Briakou et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib6" title="">2023</a>)</cite>, they are trained on vastly more monolingual data, which likely gives them different domain coverage and more fluency than dedicated models. NMT models are trained on the translation task. For example, <cite class="ltx_cite ltx_citemacro_citet">Jiao et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib24" title="">2023</a>)</cite> found ChatGPT is more likely to hallucinate but is stronger at translating the spoken domain, while dedicated models are stronger for medical domains and social-media-style noisy text. LLMs can easily be prompted with auxiliary information‚Äî such as domain and document context‚Äîwhile that is more complicated for NMT.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work we:</p>
<ul class="ltx_itemize" id="S1.p3.2">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">propose on-the-fly ensembling of an MT model with a prompted-for-translation LLM,</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">combine it with domain and context prompting,</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">demonstrate that a weaker-at-translation LLM can improve translations of a MT model,</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">and demonstrate our method is better than MT ensembles and ensembles with non-prompted LLMs.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We review standard inference of encoder-decoder NMT models and decoder only LLMs and then introduce
our proposed ensemble of the two.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Standard Decoding</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.2">In encoder-decoder NMT,
the probability of token <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.1.m1.1d">italic_t</annotation></semantics></math> at the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px1.p1.2.m2.1a"><msup id="S2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">i</mi><mrow id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">t</mi><mo id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2">ùëñ</ci><apply id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3"><times id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1"></times><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.2">ùë°</ci><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.2.m2.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.2.m2.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> time step is:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\textsc{MT}}(t_{i})=p_{\textsc{MT}}(t_{i}|t_{j&lt;i},S)" class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.cmml"><msub id="S2.E1.m1.2.2.1.3" xref="S2.E1.m1.2.2.1.3.cmml"><mi id="S2.E1.m1.2.2.1.3.2" xref="S2.E1.m1.2.2.1.3.2.cmml">p</mi><mtext class="ltx_font_smallcaps" id="S2.E1.m1.2.2.1.3.3" xref="S2.E1.m1.2.2.1.3.3a.cmml">MT</mtext></msub><mo id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.2.cmml">‚Å¢</mo><mrow id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><mo id="S2.E1.m1.2.2.1.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.2.2.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml">t</mi><mi id="S2.E1.m1.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.2.2.1.1.1.3" stretchy="false" xref="S2.E1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml">=</mo><mrow id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml"><msub id="S2.E1.m1.3.3.2.3" xref="S2.E1.m1.3.3.2.3.cmml"><mi id="S2.E1.m1.3.3.2.3.2" xref="S2.E1.m1.3.3.2.3.2.cmml">p</mi><mtext class="ltx_font_smallcaps" id="S2.E1.m1.3.3.2.3.3" xref="S2.E1.m1.3.3.2.3.3a.cmml">MT</mtext></msub><mo id="S2.E1.m1.3.3.2.2" xref="S2.E1.m1.3.3.2.2.cmml">‚Å¢</mo><mrow id="S2.E1.m1.3.3.2.1.1" xref="S2.E1.m1.3.3.2.1.1.1.cmml"><mo id="S2.E1.m1.3.3.2.1.1.2" stretchy="false" xref="S2.E1.m1.3.3.2.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.2.1.1.1" xref="S2.E1.m1.3.3.2.1.1.1.cmml"><msub id="S2.E1.m1.3.3.2.1.1.1.3" xref="S2.E1.m1.3.3.2.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.2.1.1.1.3.2" xref="S2.E1.m1.3.3.2.1.1.1.3.2.cmml">t</mi><mi id="S2.E1.m1.3.3.2.1.1.1.3.3" xref="S2.E1.m1.3.3.2.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.3.3.2.1.1.1.2" xref="S2.E1.m1.3.3.2.1.1.1.2.cmml">|</mo><mrow id="S2.E1.m1.3.3.2.1.1.1.1.1" xref="S2.E1.m1.3.3.2.1.1.1.1.2.cmml"><msub id="S2.E1.m1.3.3.2.1.1.1.1.1.1" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.2.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.2.cmml">t</mi><mrow id="S2.E1.m1.3.3.2.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.2" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.2.cmml">j</mi><mo id="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.1" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E1.m1.3.3.2.1.1.1.1.1.2" xref="S2.E1.m1.3.3.2.1.1.1.1.2.cmml">,</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">S</mi></mrow></mrow><mo id="S2.E1.m1.3.3.2.1.1.3" stretchy="false" xref="S2.E1.m1.3.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><eq id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"></eq><apply id="S2.E1.m1.2.2.1.cmml" xref="S2.E1.m1.2.2.1"><times id="S2.E1.m1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.1.2"></times><apply id="S2.E1.m1.2.2.1.3.cmml" xref="S2.E1.m1.2.2.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.1.3">subscript</csymbol><ci id="S2.E1.m1.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.1.3.2">ùëù</ci><ci id="S2.E1.m1.2.2.1.3.3a.cmml" xref="S2.E1.m1.2.2.1.3.3"><mtext class="ltx_font_smallcaps" id="S2.E1.m1.2.2.1.3.3.cmml" mathsize="70%" xref="S2.E1.m1.2.2.1.3.3">MT</mtext></ci></apply><apply id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2">ùë°</ci><ci id="S2.E1.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3">ùëñ</ci></apply></apply><apply id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2"><times id="S2.E1.m1.3.3.2.2.cmml" xref="S2.E1.m1.3.3.2.2"></times><apply id="S2.E1.m1.3.3.2.3.cmml" xref="S2.E1.m1.3.3.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.2.3.1.cmml" xref="S2.E1.m1.3.3.2.3">subscript</csymbol><ci id="S2.E1.m1.3.3.2.3.2.cmml" xref="S2.E1.m1.3.3.2.3.2">ùëù</ci><ci id="S2.E1.m1.3.3.2.3.3a.cmml" xref="S2.E1.m1.3.3.2.3.3"><mtext class="ltx_font_smallcaps" id="S2.E1.m1.3.3.2.3.3.cmml" mathsize="70%" xref="S2.E1.m1.3.3.2.3.3">MT</mtext></ci></apply><apply id="S2.E1.m1.3.3.2.1.1.1.cmml" xref="S2.E1.m1.3.3.2.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.2.1.1.1.2.cmml" xref="S2.E1.m1.3.3.2.1.1.1.2">conditional</csymbol><apply id="S2.E1.m1.3.3.2.1.1.1.3.cmml" xref="S2.E1.m1.3.3.2.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.2.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.2.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.3.3.2.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.2.1.1.1.3.2">ùë°</ci><ci id="S2.E1.m1.3.3.2.1.1.1.3.3.cmml" xref="S2.E1.m1.3.3.2.1.1.1.3.3">ùëñ</ci></apply><list id="S2.E1.m1.3.3.2.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.2.1.1.1.1.1"><apply id="S2.E1.m1.3.3.2.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.2">ùë°</ci><apply id="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.3"><lt id="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.1"></lt><ci id="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.2">ùëó</ci><ci id="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.3.3.2.1.1.1.1.1.1.3.3">ùëñ</ci></apply></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ùëÜ</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">p_{\textsc{MT}}(t_{i})=p_{\textsc{MT}}(t_{i}|t_{j&lt;i},S)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">italic_p start_POSTSUBSCRIPT MT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT MT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_t start_POSTSUBSCRIPT italic_j &lt; italic_i end_POSTSUBSCRIPT , italic_S )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.4">This conditions on source sentence <math alttext="S" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.3.m1.1"><semantics id="S2.SS0.SSS0.Px1.p1.3.m1.1a"><mi id="S2.SS0.SSS0.Px1.p1.3.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.3.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.3.m1.1b"><ci id="S2.SS0.SSS0.Px1.p1.3.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m1.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.3.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.3.m1.1d">italic_S</annotation></semantics></math> as the input to the encoder and <math alttext="t_{j&lt;i}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.4.m2.1"><semantics id="S2.SS0.SSS0.Px1.p1.4.m2.1a"><msub id="S2.SS0.SSS0.Px1.p1.4.m2.1.1" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.2" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.2.cmml">t</mi><mrow id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.2" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.2.cmml">j</mi><mo id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.1" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.1.cmml">&lt;</mo><mi id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.3" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.4.m2.1b"><apply id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.2">ùë°</ci><apply id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3"><lt id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.1"></lt><ci id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.2">ùëó</ci><ci id="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m2.1.1.3.3">ùëñ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.4.m2.1c">t_{j&lt;i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.4.m2.1d">italic_t start_POSTSUBSCRIPT italic_j &lt; italic_i end_POSTSUBSCRIPT</annotation></semantics></math> as the previously generated target tokens in the MT model decoder.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.2">When using a decoder only LLM for translation,
the probability of token <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.1.m1.1"><semantics id="S2.SS0.SSS0.Px1.p2.1.m1.1a"><mi id="S2.SS0.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p2.1.m1.1b"><ci id="S2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p2.1.m1.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p2.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p2.1.m1.1d">italic_t</annotation></semantics></math> at the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.2.m2.1"><semantics id="S2.SS0.SSS0.Px1.p2.2.m2.1a"><msup id="S2.SS0.SSS0.Px1.p2.2.m2.1.1" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.2" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.2.cmml">i</mi><mrow id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.2" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.2.cmml">t</mi><mo id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.1" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.3" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p2.2.m2.1b"><apply id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.2">ùëñ</ci><apply id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3"><times id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.1"></times><ci id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.2">ùë°</ci><ci id="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px1.p2.2.m2.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p2.2.m2.1c">i^{th}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p2.2.m2.1d">italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> time step is:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\textsc{LLM}}(t_{i})=p_{\textsc{LLM}}(t_{i}|M,S,t_{j&lt;i})" class="ltx_Math" display="block" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.cmml"><msub id="S2.E2.m1.3.3.1.3" xref="S2.E2.m1.3.3.1.3.cmml"><mi id="S2.E2.m1.3.3.1.3.2" xref="S2.E2.m1.3.3.1.3.2.cmml">p</mi><mtext class="ltx_font_smallcaps" id="S2.E2.m1.3.3.1.3.3" xref="S2.E2.m1.3.3.1.3.3a.cmml">LLM</mtext></msub><mo id="S2.E2.m1.3.3.1.2" xref="S2.E2.m1.3.3.1.2.cmml">‚Å¢</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mo id="S2.E2.m1.3.3.1.1.1.2" stretchy="false" xref="S2.E2.m1.3.3.1.1.1.1.cmml">(</mo><msub id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">t</mi><mi id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E2.m1.3.3.1.1.1.3" stretchy="false" xref="S2.E2.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml">=</mo><mrow id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.2.cmml"><msub id="S2.E2.m1.4.4.2.3" xref="S2.E2.m1.4.4.2.3.cmml"><mi id="S2.E2.m1.4.4.2.3.2" xref="S2.E2.m1.4.4.2.3.2.cmml">p</mi><mtext class="ltx_font_smallcaps" id="S2.E2.m1.4.4.2.3.3" xref="S2.E2.m1.4.4.2.3.3a.cmml">LLM</mtext></msub><mo id="S2.E2.m1.4.4.2.2" xref="S2.E2.m1.4.4.2.2.cmml">‚Å¢</mo><mrow id="S2.E2.m1.4.4.2.1.1" xref="S2.E2.m1.4.4.2.1.1.1.cmml"><mo id="S2.E2.m1.4.4.2.1.1.2" stretchy="false" xref="S2.E2.m1.4.4.2.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.4.4.2.1.1.1" xref="S2.E2.m1.4.4.2.1.1.1.cmml"><msub id="S2.E2.m1.4.4.2.1.1.1.3" xref="S2.E2.m1.4.4.2.1.1.1.3.cmml"><mi id="S2.E2.m1.4.4.2.1.1.1.3.2" xref="S2.E2.m1.4.4.2.1.1.1.3.2.cmml">t</mi><mi id="S2.E2.m1.4.4.2.1.1.1.3.3" xref="S2.E2.m1.4.4.2.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E2.m1.4.4.2.1.1.1.2" xref="S2.E2.m1.4.4.2.1.1.1.2.cmml">|</mo><mrow id="S2.E2.m1.4.4.2.1.1.1.1.1" xref="S2.E2.m1.4.4.2.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">M</mi><mo id="S2.E2.m1.4.4.2.1.1.1.1.1.2" xref="S2.E2.m1.4.4.2.1.1.1.1.2.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">S</mi><mo id="S2.E2.m1.4.4.2.1.1.1.1.1.3" xref="S2.E2.m1.4.4.2.1.1.1.1.2.cmml">,</mo><msub id="S2.E2.m1.4.4.2.1.1.1.1.1.1" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.4.4.2.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.2.cmml">t</mi><mrow id="S2.E2.m1.4.4.2.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.2.cmml">j</mi><mo id="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.1" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.3" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub></mrow></mrow><mo id="S2.E2.m1.4.4.2.1.1.3" stretchy="false" xref="S2.E2.m1.4.4.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"></eq><apply id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.3.1"><times id="S2.E2.m1.3.3.1.2.cmml" xref="S2.E2.m1.3.3.1.2"></times><apply id="S2.E2.m1.3.3.1.3.cmml" xref="S2.E2.m1.3.3.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.3.1.cmml" xref="S2.E2.m1.3.3.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.3.2.cmml" xref="S2.E2.m1.3.3.1.3.2">ùëù</ci><ci id="S2.E2.m1.3.3.1.3.3a.cmml" xref="S2.E2.m1.3.3.1.3.3"><mtext class="ltx_font_smallcaps" id="S2.E2.m1.3.3.1.3.3.cmml" mathsize="70%" xref="S2.E2.m1.3.3.1.3.3">LLM</mtext></ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2">ùë°</ci><ci id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3">ùëñ</ci></apply></apply><apply id="S2.E2.m1.4.4.2.cmml" xref="S2.E2.m1.4.4.2"><times id="S2.E2.m1.4.4.2.2.cmml" xref="S2.E2.m1.4.4.2.2"></times><apply id="S2.E2.m1.4.4.2.3.cmml" xref="S2.E2.m1.4.4.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.2.3.1.cmml" xref="S2.E2.m1.4.4.2.3">subscript</csymbol><ci id="S2.E2.m1.4.4.2.3.2.cmml" xref="S2.E2.m1.4.4.2.3.2">ùëù</ci><ci id="S2.E2.m1.4.4.2.3.3a.cmml" xref="S2.E2.m1.4.4.2.3.3"><mtext class="ltx_font_smallcaps" id="S2.E2.m1.4.4.2.3.3.cmml" mathsize="70%" xref="S2.E2.m1.4.4.2.3.3">LLM</mtext></ci></apply><apply id="S2.E2.m1.4.4.2.1.1.1.cmml" xref="S2.E2.m1.4.4.2.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.2.1.1.1.2.cmml" xref="S2.E2.m1.4.4.2.1.1.1.2">conditional</csymbol><apply id="S2.E2.m1.4.4.2.1.1.1.3.cmml" xref="S2.E2.m1.4.4.2.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.2.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.2.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.4.4.2.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.2.1.1.1.3.2">ùë°</ci><ci id="S2.E2.m1.4.4.2.1.1.1.3.3.cmml" xref="S2.E2.m1.4.4.2.1.1.1.3.3">ùëñ</ci></apply><list id="S2.E2.m1.4.4.2.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.2.1.1.1.1.1"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ùëÄ</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ùëÜ</ci><apply id="S2.E2.m1.4.4.2.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.2.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.4.4.2.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.2">ùë°</ci><apply id="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.3"><lt id="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.1"></lt><ci id="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.2">ùëó</ci><ci id="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.4.4.2.1.1.1.1.1.1.3.3">ùëñ</ci></apply></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">p_{\textsc{LLM}}(t_{i})=p_{\textsc{LLM}}(t_{i}|M,S,t_{j&lt;i})</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">italic_p start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_M , italic_S , italic_t start_POSTSUBSCRIPT italic_j &lt; italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.5">The concatenation of the prompt <math alttext="M" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.3.m1.1"><semantics id="S2.SS0.SSS0.Px1.p2.3.m1.1a"><mi id="S2.SS0.SSS0.Px1.p2.3.m1.1.1" xref="S2.SS0.SSS0.Px1.p2.3.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p2.3.m1.1b"><ci id="S2.SS0.SSS0.Px1.p2.3.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p2.3.m1.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p2.3.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p2.3.m1.1d">italic_M</annotation></semantics></math>, source sentence <math alttext="S" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.4.m2.1"><semantics id="S2.SS0.SSS0.Px1.p2.4.m2.1a"><mi id="S2.SS0.SSS0.Px1.p2.4.m2.1.1" xref="S2.SS0.SSS0.Px1.p2.4.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p2.4.m2.1b"><ci id="S2.SS0.SSS0.Px1.p2.4.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p2.4.m2.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p2.4.m2.1c">S</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p2.4.m2.1d">italic_S</annotation></semantics></math> and the previous generated targets are all decoder outputs. The LLM model is prefix-decoded through the prompt and source, and then allowed to produce the target tokens. The LLM prompt <math alttext="M" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p2.5.m3.1"><semantics id="S2.SS0.SSS0.Px1.p2.5.m3.1a"><mi id="S2.SS0.SSS0.Px1.p2.5.m3.1.1" xref="S2.SS0.SSS0.Px1.p2.5.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p2.5.m3.1b"><ci id="S2.SS0.SSS0.Px1.p2.5.m3.1.1.cmml" xref="S2.SS0.SSS0.Px1.p2.5.m3.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p2.5.m3.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p2.5.m3.1d">italic_M</annotation></semantics></math> can also include additional content.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Proposed Ensemble</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.7">When combining the two for our ensemble, we have:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p_{\text{ensemble}}(t_{i})=\lambda p_{\text{MT}}(t_{i})+(1-\lambda)p_{\text{%
LLM}}(t_{i})" class="ltx_Math" display="block" id="S2.E3.m1.4"><semantics id="S2.E3.m1.4a"><mrow id="S2.E3.m1.4.4" xref="S2.E3.m1.4.4.cmml"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.3.2.cmml">p</mi><mtext id="S2.E3.m1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.3.3a.cmml">ensemble</mtext></msub><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml">t</mi><mi id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.4.4.5" xref="S2.E3.m1.4.4.5.cmml">=</mo><mrow id="S2.E3.m1.4.4.4" xref="S2.E3.m1.4.4.4.cmml"><mrow id="S2.E3.m1.2.2.2.1" xref="S2.E3.m1.2.2.2.1.cmml"><mi id="S2.E3.m1.2.2.2.1.3" xref="S2.E3.m1.2.2.2.1.3.cmml">Œª</mi><mo id="S2.E3.m1.2.2.2.1.2" xref="S2.E3.m1.2.2.2.1.2.cmml">‚Å¢</mo><msub id="S2.E3.m1.2.2.2.1.4" xref="S2.E3.m1.2.2.2.1.4.cmml"><mi id="S2.E3.m1.2.2.2.1.4.2" xref="S2.E3.m1.2.2.2.1.4.2.cmml">p</mi><mtext id="S2.E3.m1.2.2.2.1.4.3" xref="S2.E3.m1.2.2.2.1.4.3a.cmml">MT</mtext></msub><mo id="S2.E3.m1.2.2.2.1.2a" xref="S2.E3.m1.2.2.2.1.2.cmml">‚Å¢</mo><mrow id="S2.E3.m1.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mo id="S2.E3.m1.2.2.2.1.1.1.2" stretchy="false" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.2.2.2.1.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.2.1.1.1.1.2.cmml">t</mi><mi id="S2.E3.m1.2.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.2.2.2.1.1.1.3" stretchy="false" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.4.4.4.4" xref="S2.E3.m1.4.4.4.4.cmml">+</mo><mrow id="S2.E3.m1.4.4.4.3" xref="S2.E3.m1.4.4.4.3.cmml"><mrow id="S2.E3.m1.3.3.3.2.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mo id="S2.E3.m1.3.3.3.2.1.1.2" stretchy="false" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.3.3.3.2.1.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mn id="S2.E3.m1.3.3.3.2.1.1.1.2" xref="S2.E3.m1.3.3.3.2.1.1.1.2.cmml">1</mn><mo id="S2.E3.m1.3.3.3.2.1.1.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.1.cmml">‚àí</mo><mi id="S2.E3.m1.3.3.3.2.1.1.1.3" xref="S2.E3.m1.3.3.3.2.1.1.1.3.cmml">Œª</mi></mrow><mo id="S2.E3.m1.3.3.3.2.1.1.3" stretchy="false" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">)</mo></mrow><mo id="S2.E3.m1.4.4.4.3.3" xref="S2.E3.m1.4.4.4.3.3.cmml">‚Å¢</mo><msub id="S2.E3.m1.4.4.4.3.4" xref="S2.E3.m1.4.4.4.3.4.cmml"><mi id="S2.E3.m1.4.4.4.3.4.2" xref="S2.E3.m1.4.4.4.3.4.2.cmml">p</mi><mtext id="S2.E3.m1.4.4.4.3.4.3" xref="S2.E3.m1.4.4.4.3.4.3a.cmml">LLM</mtext></msub><mo id="S2.E3.m1.4.4.4.3.3a" xref="S2.E3.m1.4.4.4.3.3.cmml">‚Å¢</mo><mrow id="S2.E3.m1.4.4.4.3.2.1" xref="S2.E3.m1.4.4.4.3.2.1.1.cmml"><mo id="S2.E3.m1.4.4.4.3.2.1.2" stretchy="false" xref="S2.E3.m1.4.4.4.3.2.1.1.cmml">(</mo><msub id="S2.E3.m1.4.4.4.3.2.1.1" xref="S2.E3.m1.4.4.4.3.2.1.1.cmml"><mi id="S2.E3.m1.4.4.4.3.2.1.1.2" xref="S2.E3.m1.4.4.4.3.2.1.1.2.cmml">t</mi><mi id="S2.E3.m1.4.4.4.3.2.1.1.3" xref="S2.E3.m1.4.4.4.3.2.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.4.4.4.3.2.1.3" stretchy="false" xref="S2.E3.m1.4.4.4.3.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.4b"><apply id="S2.E3.m1.4.4.cmml" xref="S2.E3.m1.4.4"><eq id="S2.E3.m1.4.4.5.cmml" xref="S2.E3.m1.4.4.5"></eq><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><times id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.3.2">ùëù</ci><ci id="S2.E3.m1.1.1.1.3.3a.cmml" xref="S2.E3.m1.1.1.1.3.3"><mtext id="S2.E3.m1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.E3.m1.1.1.1.3.3">ensemble</mtext></ci></apply><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">ùë°</ci><ci id="S2.E3.m1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3">ùëñ</ci></apply></apply><apply id="S2.E3.m1.4.4.4.cmml" xref="S2.E3.m1.4.4.4"><plus id="S2.E3.m1.4.4.4.4.cmml" xref="S2.E3.m1.4.4.4.4"></plus><apply id="S2.E3.m1.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.1"><times id="S2.E3.m1.2.2.2.1.2.cmml" xref="S2.E3.m1.2.2.2.1.2"></times><ci id="S2.E3.m1.2.2.2.1.3.cmml" xref="S2.E3.m1.2.2.2.1.3">ùúÜ</ci><apply id="S2.E3.m1.2.2.2.1.4.cmml" xref="S2.E3.m1.2.2.2.1.4"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.4.1.cmml" xref="S2.E3.m1.2.2.2.1.4">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.4.2.cmml" xref="S2.E3.m1.2.2.2.1.4.2">ùëù</ci><ci id="S2.E3.m1.2.2.2.1.4.3a.cmml" xref="S2.E3.m1.2.2.2.1.4.3"><mtext id="S2.E3.m1.2.2.2.1.4.3.cmml" mathsize="70%" xref="S2.E3.m1.2.2.2.1.4.3">MT</mtext></ci></apply><apply id="S2.E3.m1.2.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.2">ùë°</ci><ci id="S2.E3.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.3">ùëñ</ci></apply></apply><apply id="S2.E3.m1.4.4.4.3.cmml" xref="S2.E3.m1.4.4.4.3"><times id="S2.E3.m1.4.4.4.3.3.cmml" xref="S2.E3.m1.4.4.4.3.3"></times><apply id="S2.E3.m1.3.3.3.2.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1"><minus id="S2.E3.m1.3.3.3.2.1.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.1"></minus><cn id="S2.E3.m1.3.3.3.2.1.1.1.2.cmml" type="integer" xref="S2.E3.m1.3.3.3.2.1.1.1.2">1</cn><ci id="S2.E3.m1.3.3.3.2.1.1.1.3.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.3">ùúÜ</ci></apply><apply id="S2.E3.m1.4.4.4.3.4.cmml" xref="S2.E3.m1.4.4.4.3.4"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.4.3.4.1.cmml" xref="S2.E3.m1.4.4.4.3.4">subscript</csymbol><ci id="S2.E3.m1.4.4.4.3.4.2.cmml" xref="S2.E3.m1.4.4.4.3.4.2">ùëù</ci><ci id="S2.E3.m1.4.4.4.3.4.3a.cmml" xref="S2.E3.m1.4.4.4.3.4.3"><mtext id="S2.E3.m1.4.4.4.3.4.3.cmml" mathsize="70%" xref="S2.E3.m1.4.4.4.3.4.3">LLM</mtext></ci></apply><apply id="S2.E3.m1.4.4.4.3.2.1.1.cmml" xref="S2.E3.m1.4.4.4.3.2.1"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.4.3.2.1.1.1.cmml" xref="S2.E3.m1.4.4.4.3.2.1">subscript</csymbol><ci id="S2.E3.m1.4.4.4.3.2.1.1.2.cmml" xref="S2.E3.m1.4.4.4.3.2.1.1.2">ùë°</ci><ci id="S2.E3.m1.4.4.4.3.2.1.1.3.cmml" xref="S2.E3.m1.4.4.4.3.2.1.1.3">ùëñ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.4c">p_{\text{ensemble}}(t_{i})=\lambda p_{\text{MT}}(t_{i})+(1-\lambda)p_{\text{%
LLM}}(t_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.4d">italic_p start_POSTSUBSCRIPT ensemble end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_Œª italic_p start_POSTSUBSCRIPT MT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + ( 1 - italic_Œª ) italic_p start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.6">In the ensemble, <math alttext="p_{\text{MT}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px2.p1.1.m1.1a"><msub id="S2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">p</mi><mtext id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3a.cmml">MT</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.2">ùëù</ci><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.3">MT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.1.m1.1c">p_{\text{MT}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT MT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="p_{\text{LLM}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px2.p1.2.m2.1a"><msub id="S2.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">p</mi><mtext id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.2">ùëù</ci><ci id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.2.m2.1c">p_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.2.m2.1d">italic_p start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math> condition on the tokens previously generated by the ensemble.
<math alttext="p_{\text{LLM}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px2.p1.3.m3.1a"><msub id="S2.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml">p</mi><mtext id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3a.cmml">LLM</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.2">ùëù</ci><ci id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.3">LLM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.3.m3.1c">p_{\text{LLM}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.3.m3.1d">italic_p start_POSTSUBSCRIPT LLM end_POSTSUBSCRIPT</annotation></semantics></math> still conditions on the prompt, which can be used to infuse the model with auxiliary information (e.g. domain or context).
<math alttext="p_{\text{ensemble}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S2.SS0.SSS0.Px2.p1.4.m4.1a"><msub id="S2.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">p</mi><mtext id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3a.cmml">ensemble</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.2">ùëù</ci><ci id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" mathsize="70%" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.3">ensemble</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.4.m4.1c">p_{\text{ensemble}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.4.m4.1d">italic_p start_POSTSUBSCRIPT ensemble end_POSTSUBSCRIPT</annotation></semantics></math> reduces to the LLM when <math alttext="\lambda=0" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S2.SS0.SSS0.Px2.p1.5.m5.1a"><mrow id="S2.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">Œª</mi><mo id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.1" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml">=</mo><mn id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1"><eq id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.1"></eq><ci id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.2">ùúÜ</ci><cn id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.5.m5.1c">\lambda=0</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.5.m5.1d">italic_Œª = 0</annotation></semantics></math> and to the MT model when <math alttext="\lambda=1" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.6.m6.1"><semantics id="S2.SS0.SSS0.Px2.p1.6.m6.1a"><mrow id="S2.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.2" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml">Œª</mi><mo id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.1" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.1.cmml">=</mo><mn id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.3" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.6.m6.1b"><apply id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1"><eq id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.1"></eq><ci id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.2">ùúÜ</ci><cn id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.6.m6.1c">\lambda=1</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.6.m6.1d">italic_Œª = 1</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_logical-block ltx_pruned_first" id="S2.T1.2">
<div class="ltx_para ltx_noindent ltx_align_center" id="S2.T1.2.p2">
<p class="ltx_p" id="S2.T1.2.p2.1"><span class="ltx_text" id="S2.T1.2.p2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S2.T1.2.p2.1.1.1" style="width:281.3pt;height:162pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S2.T1.2.p2.1.1.1.1"><span class="ltx_text" id="S2.T1.2.p2.1.1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.2.p2.1.1.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">German</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">Russian</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">Turkish</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.1.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">Hausa</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">Train</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">290.4m</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">38m</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">49.5m</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">600k</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.3.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">Valid</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">1000/1002</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">1000/1002</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">3007</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">2000</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.4.4">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.p2.1.1.1.1.1.1.4.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"></span>
<span class="ltx_td ltx_align_right" id="S2.T1.2.p2.1.1.1.1.1.1.4.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">WMT21</span>
<span class="ltx_td ltx_align_right" id="S2.T1.2.p2.1.1.1.1.1.1.4.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">WMT21</span>
<span class="ltx_td ltx_align_right" id="S2.T1.2.p2.1.1.1.1.1.1.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">newstest2017</span>
<span class="ltx_td ltx_align_right" id="S2.T1.2.p2.1.1.1.1.1.1.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">newsdev2021</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">Test</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">1984/2037</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">2016/2037</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">3000/3602</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">4456/4459</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.6.6">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S2.T1.2.p2.1.1.1.1.1.1.6.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"></span>
<span class="ltx_td ltx_align_right" id="S2.T1.2.p2.1.1.1.1.1.1.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">WMT22</span>
<span class="ltx_td ltx_align_right" id="S2.T1.2.p2.1.1.1.1.1.1.6.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">WMT22</span>
<span class="ltx_td ltx_align_right" id="S2.T1.2.p2.1.1.1.1.1.1.6.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">newstest2018</span>
<span class="ltx_td ltx_align_right" id="S2.T1.2.p2.1.1.1.1.1.1.6.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">newstest2021</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.7.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.7.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">TED-100</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.7.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">-</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.7.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">1132</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.7.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">-</span>
<span class="ltx_td ltx_align_right ltx_border_tt" id="S2.T1.2.p2.1.1.1.1.1.1.7.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.8.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.8.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">ParaPat</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.8.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">2000</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.8.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">2000</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.8.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">-</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.8.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</span></span>
<span class="ltx_tr" id="S2.T1.2.p2.1.1.1.1.1.1.9.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.9.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">CTXPro</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.9.9.2" style="padding-left:3.0pt;padding-right:3.0pt;">2000</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.9.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">2000</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.9.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">-</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S2.T1.2.p2.1.1.1.1.1.1.9.9.5" style="padding-left:3.0pt;padding-right:3.0pt;">-</span></span>
</span>
</span></span></span>
</span></span></span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Size of datasets used in this work. All numbers are in sentences, except for CTXPro, which is reported in paragraphs. For the validation and testsets that are different in each translation direction, numbers listed are for <math alttext="*" class="ltx_Math" display="inline" id="S2.T1.7.m1.1"><semantics id="S2.T1.7.m1.1b"><mo id="S2.T1.7.m1.1.1" xref="S2.T1.7.m1.1.1.cmml">‚àó</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.m1.1c"><times id="S2.T1.7.m1.1.1.cmml" xref="S2.T1.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.m1.1e">‚àó</annotation></semantics></math><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.8.m2.1"><semantics id="S2.T1.8.m2.1b"><mo id="S2.T1.8.m2.1.1" stretchy="false" xref="S2.T1.8.m2.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.m2.1c"><ci id="S2.T1.8.m2.1.1.cmml" xref="S2.T1.8.m2.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.8.m2.1e">‚Üí</annotation></semantics></math>en/en<math alttext="\leftarrow" class="ltx_Math" display="inline" id="S2.T1.9.m3.1"><semantics id="S2.T1.9.m3.1b"><mo id="S2.T1.9.m3.1.1" stretchy="false" xref="S2.T1.9.m3.1.1.cmml">‚Üê</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.m3.1c"><ci id="S2.T1.9.m3.1.1.cmml" xref="S2.T1.9.m3.1.1">‚Üê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.m3.1d">\leftarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.9.m3.1e">‚Üê</annotation></semantics></math><math alttext="*" class="ltx_Math" display="inline" id="S2.T1.10.m4.1"><semantics id="S2.T1.10.m4.1b"><mo id="S2.T1.10.m4.1.1" xref="S2.T1.10.m4.1.1.cmml">‚àó</mo><annotation-xml encoding="MathML-Content" id="S2.T1.10.m4.1c"><times id="S2.T1.10.m4.1.1.cmml" xref="S2.T1.10.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.m4.1d">*</annotation><annotation encoding="application/x-llamapun" id="S2.T1.10.m4.1e">‚àó</annotation></semantics></math>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We aim to understand how our proposed method performs in high and low resource settings with strong models, and design our experimental setup accordingly.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The parallel and monolingual training data for German and Russian is from the WMT22 <cite class="ltx_cite ltx_citemacro_cite">Kocmi et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib31" title="">2022</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.statmt.org/wmt22/" title="">https://www.statmt.org/wmt22/</a></span></span></span> shared task. The Hausa data is from WMT21 <cite class="ltx_cite ltx_citemacro_cite">Akhbardeh et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib2" title="">2021</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.statmt.org/wmt21/" title="">https://www.statmt.org/wmt21/</a></span></span></span> The Turkish evaluation data was based on WMT18 <cite class="ltx_cite ltx_citemacro_cite">Bojar et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib5" title="">2018</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.statmt.org/wmt18/" title="">https://www.statmt.org/wmt18/</a></span></span></span> and training data also includes additional data from OPUS <cite class="ltx_cite ltx_citemacro_cite">Tiedemann (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib52" title="">2012</a>)</cite>, excluding Paracrawl <cite class="ltx_cite ltx_citemacro_cite">Ba√±√≥n et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib4" title="">2020</a>)</cite>, since such noisy data <cite class="ltx_cite ltx_citemacro_cite">Khayrallah and Koehn (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib27" title="">2018</a>)</cite> would require filtering <cite class="ltx_cite ltx_citemacro_cite">Koehn et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib34" title="">2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib33" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib32" title="">2020</a>); Sloto et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib47" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">As domain-specific test sets we use TED-100 <cite class="ltx_cite ltx_citemacro_cite">Salesky et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib45" title="">2021</a>)</cite> and ParaPat <cite class="ltx_cite ltx_citemacro_cite">Soares et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib49" title="">2020</a>)</cite>. We also use TED-100 and CTXPro <cite class="ltx_cite ltx_citemacro_cite">Wicks and Post (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib60" title="">2023</a>)</cite> for document-level experiments.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For CTXPro, we select a random sample of 2000 paragraphs for our experiments to reduce compute usage.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S2.T1" title="Table 1 ‚Ä£ Proposed Ensemble ‚Ä£ 2 Method ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†1</span></a> summarizes the parallel training, evalution and test data and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.T6" title="Table 6 ‚Ä£ A.2 Monolingual Data ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†6</span></a> in the Appendix summarizes the monolingual data.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">We use back translation <cite class="ltx_cite ltx_citemacro_cite">Sennrich et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib46" title="">2016</a>)</cite> (with a 1:1 ratio of parallel to synthetic data) for all language pairs.
We train Transformer ‚Äòbig‚Äô models for German, Russian and Turkish, and ‚Äòbase‚Äô for Hausa <cite class="ltx_cite ltx_citemacro_cite">Vaswani et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib56" title="">2017</a>)</cite> in Marian NMT <cite class="ltx_cite ltx_citemacro_cite">Junczys-Dowmunt et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib26" title="">2018</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We convert models from Marian to Hugging Face format.</span></span></span>
We use Llama2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib55" title="">2023b</a>)</cite> with 7 and 13 billion parameters as LLMs.
The LLama2 32k token SentencePiece model <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib35" title="">2018</a>)</cite> is used for source and target MT tokenization.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>The target side vocabs must match between the LLM and MT model to be able to ensemble; the source could potentially be different. Preliminary experiments, however, found it better to use the same vocab and be able to tie the embeddings.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.2">The optimal mixing ratio is learnt using grid search <math alttext="\lambda\in\{0,0.1,...1\}" class="ltx_Math" display="inline" id="S3.p6.1.m1.3"><semantics id="S3.p6.1.m1.3a"><mrow id="S3.p6.1.m1.3.3" xref="S3.p6.1.m1.3.3.cmml"><mi id="S3.p6.1.m1.3.3.3" xref="S3.p6.1.m1.3.3.3.cmml">Œª</mi><mo id="S3.p6.1.m1.3.3.2" xref="S3.p6.1.m1.3.3.2.cmml">‚àà</mo><mrow id="S3.p6.1.m1.3.3.1.1" xref="S3.p6.1.m1.3.3.1.2.cmml"><mo id="S3.p6.1.m1.3.3.1.1.2" stretchy="false" xref="S3.p6.1.m1.3.3.1.2.cmml">{</mo><mn id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">0</mn><mo id="S3.p6.1.m1.3.3.1.1.3" xref="S3.p6.1.m1.3.3.1.2.cmml">,</mo><mn id="S3.p6.1.m1.2.2" xref="S3.p6.1.m1.2.2.cmml">0.1</mn><mo id="S3.p6.1.m1.3.3.1.1.4" xref="S3.p6.1.m1.3.3.1.2.cmml">,</mo><mrow id="S3.p6.1.m1.3.3.1.1.1" xref="S3.p6.1.m1.3.3.1.1.1.cmml"><mi id="S3.p6.1.m1.3.3.1.1.1.2" mathvariant="normal" xref="S3.p6.1.m1.3.3.1.1.1.2.cmml">‚Ä¶</mi><mo id="S3.p6.1.m1.3.3.1.1.1.1" xref="S3.p6.1.m1.3.3.1.1.1.1.cmml">‚Å¢</mo><mn id="S3.p6.1.m1.3.3.1.1.1.3" xref="S3.p6.1.m1.3.3.1.1.1.3.cmml">1</mn></mrow><mo id="S3.p6.1.m1.3.3.1.1.5" stretchy="false" xref="S3.p6.1.m1.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.3b"><apply id="S3.p6.1.m1.3.3.cmml" xref="S3.p6.1.m1.3.3"><in id="S3.p6.1.m1.3.3.2.cmml" xref="S3.p6.1.m1.3.3.2"></in><ci id="S3.p6.1.m1.3.3.3.cmml" xref="S3.p6.1.m1.3.3.3">ùúÜ</ci><set id="S3.p6.1.m1.3.3.1.2.cmml" xref="S3.p6.1.m1.3.3.1.1"><cn id="S3.p6.1.m1.1.1.cmml" type="integer" xref="S3.p6.1.m1.1.1">0</cn><cn id="S3.p6.1.m1.2.2.cmml" type="float" xref="S3.p6.1.m1.2.2">0.1</cn><apply id="S3.p6.1.m1.3.3.1.1.1.cmml" xref="S3.p6.1.m1.3.3.1.1.1"><times id="S3.p6.1.m1.3.3.1.1.1.1.cmml" xref="S3.p6.1.m1.3.3.1.1.1.1"></times><ci id="S3.p6.1.m1.3.3.1.1.1.2.cmml" xref="S3.p6.1.m1.3.3.1.1.1.2">‚Ä¶</ci><cn id="S3.p6.1.m1.3.3.1.1.1.3.cmml" type="integer" xref="S3.p6.1.m1.3.3.1.1.1.3">1</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.3c">\lambda\in\{0,0.1,...1\}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.3d">italic_Œª ‚àà { 0 , 0.1 , ‚Ä¶ 1 }</annotation></semantics></math> on the validation set. We use this same value of <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.p6.2.m2.1"><semantics id="S3.p6.2.m2.1a"><mi id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><ci id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.p6.2.m2.1d">italic_Œª</annotation></semantics></math> in domain specific experiments in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S5" title="5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">¬ß¬†5</span></a>; we do not re-sweep for each domain. Final results are reported on the test sets, translation quality is measured using <span class="ltx_text ltx_font_smallcaps" id="S3.p6.2.1">Comet-22</span> <cite class="ltx_cite ltx_citemacro_cite">Rei et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib43" title="">2022</a>)</cite>.
We use greedy search for decoding.
See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1" title="Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">¬ß¬†A</span></a> for additional experimental details, including prompts.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T2.2.3.1.1" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.3.1.2" style="padding-left:5.5pt;padding-right:5.5pt;">MT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.2.3.1.3" style="padding-left:5.5pt;padding-right:5.5pt;">LLM 7B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T2.2.3.1.4" style="padding-left:5.5pt;padding-right:5.5pt;">Ensemble w/ LLM 7B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S4.T2.2.3.1.5" style="padding-left:5.5pt;padding-right:5.5pt;">LLM 13B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T2.2.3.1.6" style="padding-left:5.5pt;padding-right:5.5pt;">Ensemble w/ LLM 13B</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<td class="ltx_td ltx_border_r" id="S4.T2.2.2.3" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.2.2.4" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5" style="padding-left:5.5pt;padding-right:5.5pt;">0-shot</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.6" style="padding-left:5.5pt;padding-right:5.5pt;">5-shot</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.1" style="padding-left:5.5pt;padding-right:5.5pt;"><math alttext="\lambda" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">italic_Œª</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7" style="padding-left:5.5pt;padding-right:5.5pt;">0-shot</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.8" style="padding-left:5.5pt;padding-right:5.5pt;">5-shot</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9" style="padding-left:5.5pt;padding-right:5.5pt;">0-shot</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.10" style="padding-left:5.5pt;padding-right:5.5pt;">5-shot</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2" style="padding-left:5.5pt;padding-right:5.5pt;"><math alttext="\lambda" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mi id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">italic_Œª</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11" style="padding-left:5.5pt;padding-right:5.5pt;">0-shot</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12" style="padding-left:5.5pt;padding-right:5.5pt;">5-shot</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.2">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.2.4.2.1" style="padding-left:5.5pt;padding-right:5.5pt;">column:</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.4.2.2" style="padding-left:5.5pt;padding-right:5.5pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.2.3" style="padding-left:5.5pt;padding-right:5.5pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.4.2.4" style="padding-left:5.5pt;padding-right:5.5pt;">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.2.5" style="padding-left:5.5pt;padding-right:5.5pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.2.6" style="padding-left:5.5pt;padding-right:5.5pt;">5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.4.2.7" style="padding-left:5.5pt;padding-right:5.5pt;">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.2.8" style="padding-left:5.5pt;padding-right:5.5pt;">7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.4.2.9" style="padding-left:5.5pt;padding-right:5.5pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.2.10" style="padding-left:5.5pt;padding-right:5.5pt;">9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.2.11" style="padding-left:5.5pt;padding-right:5.5pt;">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.4.2.12" style="padding-left:5.5pt;padding-right:5.5pt;">11</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.2.5.3.1" style="padding-left:5.5pt;padding-right:5.5pt;">de-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.5.3.2" style="padding-left:5.5pt;padding-right:5.5pt;">83.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.3.3" style="padding-left:5.5pt;padding-right:5.5pt;">82.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.5.3.4" style="padding-left:5.5pt;padding-right:5.5pt;">82.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.3.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.3.6" style="padding-left:5.5pt;padding-right:5.5pt;">83.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.5.3.7" style="padding-left:5.5pt;padding-right:5.5pt;">83.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.3.8" style="padding-left:5.5pt;padding-right:5.5pt;">82.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.5.3.9" style="padding-left:5.5pt;padding-right:5.5pt;">83.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.3.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.3.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.5.3.11.1">84.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.5.3.12" style="padding-left:5.5pt;padding-right:5.5pt;">84.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.2.6.4.1" style="padding-left:5.5pt;padding-right:5.5pt;">en-de</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.6.4.2" style="padding-left:5.5pt;padding-right:5.5pt;">85.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.3" style="padding-left:5.5pt;padding-right:5.5pt;">79.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.6.4.4" style="padding-left:5.5pt;padding-right:5.5pt;">79.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.6" style="padding-left:5.5pt;padding-right:5.5pt;">85.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.6.4.7" style="padding-left:5.5pt;padding-right:5.5pt;">85.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.8" style="padding-left:5.5pt;padding-right:5.5pt;">63.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.6.4.9" style="padding-left:5.5pt;padding-right:5.5pt;">82.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.6.4.11.1">85.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.6.4.12.1">85.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.2.7.5.1" style="padding-left:5.5pt;padding-right:5.5pt;">ru-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.7.5.2" style="padding-left:5.5pt;padding-right:5.5pt;">82.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.3" style="padding-left:5.5pt;padding-right:5.5pt;">82.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.7.5.4" style="padding-left:5.5pt;padding-right:5.5pt;">82.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.6" style="padding-left:5.5pt;padding-right:5.5pt;">84.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.7.5.7" style="padding-left:5.5pt;padding-right:5.5pt;">84.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.8" style="padding-left:5.5pt;padding-right:5.5pt;">81.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.7.5.9" style="padding-left:5.5pt;padding-right:5.5pt;">83.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.11" style="padding-left:5.5pt;padding-right:5.5pt;">84.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.7.5.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.7.5.12.1">84.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.8.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.2.8.6.1" style="padding-left:5.5pt;padding-right:5.5pt;">en-ru</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.8.6.2" style="padding-left:5.5pt;padding-right:5.5pt;">83.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.6.3" style="padding-left:5.5pt;padding-right:5.5pt;">80.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.8.6.4" style="padding-left:5.5pt;padding-right:5.5pt;">81.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.6.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.6.6" style="padding-left:5.5pt;padding-right:5.5pt;">83.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.8.6.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.8.6.7.1">84.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.6.8" style="padding-left:5.5pt;padding-right:5.5pt;">36.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.8.6.9" style="padding-left:5.5pt;padding-right:5.5pt;">81.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.6.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.6.11" style="padding-left:5.5pt;padding-right:5.5pt;">83.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.6.12" style="padding-left:5.5pt;padding-right:5.5pt;">83.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.9.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.2.9.7.1" style="padding-left:5.5pt;padding-right:5.5pt;">tr-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.9.7.2" style="padding-left:5.5pt;padding-right:5.5pt;">87.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.7.3" style="padding-left:5.5pt;padding-right:5.5pt;">75.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.9.7.4" style="padding-left:5.5pt;padding-right:5.5pt;">75.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.7.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.7.6" style="padding-left:5.5pt;padding-right:5.5pt;">87.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.9.7.7" style="padding-left:5.5pt;padding-right:5.5pt;">87.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.7.8" style="padding-left:5.5pt;padding-right:5.5pt;">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.9.7.9" style="padding-left:5.5pt;padding-right:5.5pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.7.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.7.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.9.7.11.1">87.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.9.7.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.9.7.12.1">87.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.10.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.2.10.8.1" style="padding-left:5.5pt;padding-right:5.5pt;">en-tr</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.10.8.2" style="padding-left:5.5pt;padding-right:5.5pt;">89.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.8.3" style="padding-left:5.5pt;padding-right:5.5pt;">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.10.8.4" style="padding-left:5.5pt;padding-right:5.5pt;">58.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.8.5" style="padding-left:5.5pt;padding-right:5.5pt;">1.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.8.6" style="padding-left:5.5pt;padding-right:5.5pt;">89.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.10.8.7" style="padding-left:5.5pt;padding-right:5.5pt;">89.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.8.8" style="padding-left:5.5pt;padding-right:5.5pt;">40.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.10.8.9" style="padding-left:5.5pt;padding-right:5.5pt;">69.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.8.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.8.11" style="padding-left:5.5pt;padding-right:5.5pt;">89.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.8.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.10.8.12.1">89.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.11.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.2.11.9.1" style="padding-left:5.5pt;padding-right:5.5pt;">ha-en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.11.9.2" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.11.9.2.1">60.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.11.9.3" style="padding-left:5.5pt;padding-right:5.5pt;">47.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.11.9.4" style="padding-left:5.5pt;padding-right:5.5pt;">49.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.11.9.5" style="padding-left:5.5pt;padding-right:5.5pt;">0.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.11.9.6" style="padding-left:5.5pt;padding-right:5.5pt;">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.11.9.7" style="padding-left:5.5pt;padding-right:5.5pt;">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.11.9.8" style="padding-left:5.5pt;padding-right:5.5pt;">46.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.11.9.9" style="padding-left:5.5pt;padding-right:5.5pt;">49.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.11.9.10" style="padding-left:5.5pt;padding-right:5.5pt;">0.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.11.9.11" style="padding-left:5.5pt;padding-right:5.5pt;">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.11.9.12" style="padding-left:5.5pt;padding-right:5.5pt;">54.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.12.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T2.2.12.10.1" style="padding-left:5.5pt;padding-right:5.5pt;">en-ha</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.12.10.2" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.12.10.2.1">63.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.12.10.3" style="padding-left:5.5pt;padding-right:5.5pt;">33.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.12.10.4" style="padding-left:5.5pt;padding-right:5.5pt;">37.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.12.10.5" style="padding-left:5.5pt;padding-right:5.5pt;">1.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.12.10.6" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.12.10.6.1">63.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.12.10.7" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.12.10.7.1">63.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.12.10.8" style="padding-left:5.5pt;padding-right:5.5pt;">38.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.12.10.9" style="padding-left:5.5pt;padding-right:5.5pt;">35.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.12.10.10" style="padding-left:5.5pt;padding-right:5.5pt;">1.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.12.10.11" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.12.10.11.1">63.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.12.10.12" style="padding-left:5.5pt;padding-right:5.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.12.10.12.1">63.1</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_smallcaps" id="S4.T2.9.1">Comet-22</span> on WMT test sets. Ensembling MT &amp; LLM can improve scores in high resource settings where the LLM‚Äôs <span class="ltx_text ltx_font_smallcaps" id="S4.T2.10.2">Comet</span> is somewhat worse than the MT. <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.T2.5.m1.1"><semantics id="S4.T2.5.m1.1b"><mi id="S4.T2.5.m1.1.1" xref="S4.T2.5.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.T2.5.m1.1c"><ci id="S4.T2.5.m1.1.1.cmml" xref="S4.T2.5.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.m1.1e">italic_Œª</annotation></semantics></math> is the mixing rate; higher <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.T2.6.m2.1"><semantics id="S4.T2.6.m2.1b"><mi id="S4.T2.6.m2.1.1" xref="S4.T2.6.m2.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.T2.6.m2.1c"><ci id="S4.T2.6.m2.1.1.cmml" xref="S4.T2.6.m2.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m2.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.m2.1e">italic_Œª</annotation></semantics></math> puts more emphasis on MT.</figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S4.T2" title="Table 2 ‚Ä£ 4 Results ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†2</span></a> shows the translation quality of the ensemble using the 7 billion parameters LLM (col.¬†1-6). When both models are of reasonable quality (de-en, ru-en, en-ru), ensembling (col.¬†5) results in better quality than either alone (col.¬†1 &amp; 2).</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In all cases, the LLM quality is worse than the MT model but ensembling with it improves most language directions. For de-en, the MT model is 0.9 <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.1">Comet</span> stronger than the LLM. The ensemble still improves over the MT model by 0.6 <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.2">Comet</span>.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The improvement is minor for en-de, where the LLM was 21.9 points worse than MT. The LLM translation quality for Turkish in both direction is poor while the MT is good so the ensembles are essentially reduced to the MT model. Both models are bad for Hausa and the ensembles are unusable.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.SS4" title="A.4 ùúÜ ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">¬ß¬†A.4</span></a> shows the effect of <math alttext="\lambda" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">italic_Œª</annotation></semantics></math> on translation quality.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">In-context learning:</span>
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S4.T2" title="Table 2 ‚Ä£ 4 Results ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†2</span></a> (col.¬†3) shows 5-shot learning tends to improve LLM quality but has little affect on the ensemble (col.¬†6).</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">Larger LLM:</span> <cite class="ltx_cite ltx_citemacro_citet">Xu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib62" title="">2023</a>)</cite> found that Llama-13B suffers from off-target issues, degrading translation out-of-English compared to the 7B model. We confirm their results‚Äî<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S4.T2" title="Table 2 ‚Ä£ 4 Results ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†2</span></a> (col.¬†2 vs 7)‚Äîand also reproduce their solution of using 5-shot learning, which can recover and sometime improve LLM quality (col.¬†8). However, ensembling with the MT model does not require the use of in context learning (col.¬†10 vs 11). In general, the larger language model is better for the ensemble as de-en, en-de and ru-en all improve. It should also be noted that the
MT model adds, at most, 3% to the number of parameters of the 7B LLM allowing the ensemble to outperform the nearly 2x bigger 13B LLM.</p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">Ensembles for Turkish and Hausa are still not worthwhile due to the poor LLM quality in these lower resource settings.
We use the 7B model in all analysis for the remainder of this work.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>MT Model Ensembling</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Given the compute resource required to use LLMs (not to mention train them), we compare the results of the MT + LLM ensemble to ensembling two MT models.
We create ensembles for German and Russian language pairs consisting of two MT models.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>The models differ only in the random seed.</span></span></span>
As¬†<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S5.T3" title="Table 3 ‚Ä£ 5.2 Mixing Ratio Interpretation ‚Ä£ 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†3</span></a> shows, using the LLM gives stronger translation quality in all cases except en-de, which is where the LLM underperforms the MT model by 6 <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.1.2">Comet</span> points. In all the other situations, <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">it is better to ensemble the MT model with an LLM, even though the <math alttext="2^{nd}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.1.m1.1"><semantics id="S5.SS1.p1.1.1.m1.1a"><msup id="S5.SS1.p1.1.1.m1.1.1" xref="S5.SS1.p1.1.1.m1.1.1.cmml"><mn id="S5.SS1.p1.1.1.m1.1.1.2" xref="S5.SS1.p1.1.1.m1.1.1.2.cmml">2</mn><mrow id="S5.SS1.p1.1.1.m1.1.1.3" xref="S5.SS1.p1.1.1.m1.1.1.3.cmml"><mi id="S5.SS1.p1.1.1.m1.1.1.3.2" xref="S5.SS1.p1.1.1.m1.1.1.3.2.cmml">n</mi><mo id="S5.SS1.p1.1.1.m1.1.1.3.1" xref="S5.SS1.p1.1.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S5.SS1.p1.1.1.m1.1.1.3.3" xref="S5.SS1.p1.1.1.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.1.m1.1b"><apply id="S5.SS1.p1.1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.1.m1.1.1">superscript</csymbol><cn id="S5.SS1.p1.1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS1.p1.1.1.m1.1.1.2">2</cn><apply id="S5.SS1.p1.1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.1.m1.1.1.3"><times id="S5.SS1.p1.1.1.m1.1.1.3.1.cmml" xref="S5.SS1.p1.1.1.m1.1.1.3.1"></times><ci id="S5.SS1.p1.1.1.m1.1.1.3.2.cmml" xref="S5.SS1.p1.1.1.m1.1.1.3.2">ùëõ</ci><ci id="S5.SS1.p1.1.1.m1.1.1.3.3.cmml" xref="S5.SS1.p1.1.1.m1.1.1.3.3">ùëë</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.1.m1.1c">2^{nd}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.1.m1.1d">2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> MT model has higher translation quality than the LLM by 0.5 to 2.8 <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.1.1.1">Comet</span>.</span>
This suggests that when selecting models for an ensemble, simply choosing the two highest quality models is insufficient. Instead, ensembling takes advantage of the training diversity in the models to improve quality.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Mixing Ratio Interpretation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.2">The learnt mixing ratio, <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">italic_Œª</annotation></semantics></math>, can be loosely interpreted as a relative utility of the underlying models. For ensembles with German and Russian, <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">italic_Œª</annotation></semantics></math> of 0.7 and 0.5 for the 7B LLM ensemble reflect the nearly equal contribution of both models. Due to off-target issue described above, the 13B LLM are poor at translating into German and Russian so its contribution to the ensemble is reduced.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">For Turkish and Hausa, the LLM offer negligible benefit so most weight is given to the MT model. The mixing ratio space for Hausa-English is flat (see ¬†<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.F7" title="Figure 7 ‚Ä£ A.4 ùúÜ ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†7</span></a>(g)) as both underlying models are equality poor so no interpretation should be attached to the results.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<p class="ltx_p ltx_align_center" id="S5.T3.1"><span class="ltx_text" id="S5.T3.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.1.1.1" style="width:224.9pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T3.1.1.1.1"><span class="ltx_text" id="S5.T3.1.1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S5.T3.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.1.1.1.1.1.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T3.1.1.1.1.1.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">MT</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.1.1.1.1.1.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">LLM</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.1.1.1.1.1.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">MT+LLM</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.1.1.1.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">MT+MT</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T3.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1.1.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">de-en</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.1.1.1.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">83.5</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1.1.1.2.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">83.7</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1.1.1.2.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">82.6</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1.1.1.2.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1.2.1.5.1">83.9</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.1.1.1.1.1.2.1.6" style="padding-left:4.0pt;padding-right:4.0pt;">83.8</span></span>
<span class="ltx_tr" id="S5.T3.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.1.1.1.1.1.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">en-de</span>
<span class="ltx_td ltx_align_center" id="S5.T3.1.1.1.1.1.1.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">85.4</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.1.1.1.3.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">85.4</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.1.1.1.3.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">79.4</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.1.1.1.3.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">85.5</span>
<span class="ltx_td ltx_align_center" id="S5.T3.1.1.1.1.1.1.3.2.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1.3.2.6.1">85.7</span></span></span>
<span class="ltx_tr" id="S5.T3.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.1.1.1.1.1.1.4.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">ru-en</span>
<span class="ltx_td ltx_align_center" id="S5.T3.1.1.1.1.1.1.4.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">82.8</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.1.1.1.4.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">83.0</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.1.1.1.4.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">82.5</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.1.1.1.1.1.1.4.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1.4.3.5.1">84.0</span></span>
<span class="ltx_td ltx_align_center" id="S5.T3.1.1.1.1.1.1.4.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">83.1</span></span>
<span class="ltx_tr" id="S5.T3.1.1.1.1.1.1.5.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.1.1.1.1.1.1.5.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">en-ru</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.1.1.1.1.5.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">83.1</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.1.1.1.1.1.5.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">83.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.1.1.1.1.1.5.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">80.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.1.1.1.1.1.5.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1.5.4.5.1">83.9</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.1.1.1.1.5.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">83.4</span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.1">Comet-22</span> score for two MT replicas, the LLM, the MT &amp; LLM ensemble, and the ensemble of the two MT models. The ensembling of the LLM with the MT model has the highest <span class="ltx_text ltx_font_smallcaps" id="S5.T3.5.2">Comet</span> score in all but one language pair, even though both the MT models have higher translation quality than the LLM.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Domain Prompting</h3>
<figure class="ltx_table" id="S5.T4">
<p class="ltx_p ltx_align_center" id="S5.T4.1"><span class="ltx_text" id="S5.T4.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.1.1.1" style="width:280.7pt;height:129.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T4.1.1.1.1"><span class="ltx_text" id="S5.T4.1.1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S5.T4.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.1.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T4.1.1.1.1.1.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T4.1.1.1.1.1.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">MT</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T4.1.1.1.1.1.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">LLM</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T4.1.1.1.1.1.1.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">Ensemble</span></span>
<span class="ltx_tr" id="S5.T4.1.1.1.1.1.1.2.2">
<span class="ltx_td ltx_th ltx_th_column" id="S5.T4.1.1.1.1.1.1.2.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="S5.T4.1.1.1.1.1.1.2.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">prompt:</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T4.1.1.1.1.1.1.2.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">none</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.1.1.1.1.1.1.2.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">general</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T4.1.1.1.1.1.1.2.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">+domain</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.1.1.1.1.1.1.2.2.6" style="padding-left:4.0pt;padding-right:4.0pt;">general</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.1.1.1.1.1.1.2.2.7" style="padding-left:4.0pt;padding-right:4.0pt;">+domain</span></span>
<span class="ltx_tr" id="S5.T4.1.1.1.1.1.1.3.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.1.1.1.3.3.1" style="padding-bottom:2.15277pt;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_rule" style="width:0.0pt;height:4.3pt;background:black;display:inline-block;"></span>
<span class="ltx_text" id="S5.T4.1.1.1.1.1.1.3.3.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.1.1.1.1.1.1.3.3.1.1.1" style="width:6.8pt;height:21.7pt;vertical-align:-7.4pt;"><span class="ltx_transformed_inner" style="width:21.7pt;transform:translate(-7.42pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.1.1.1.1.1.1.3.3.1.1.1.1">TED</span>
</span></span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1.1.1.3.3.2" style="padding-bottom:2.15277pt;padding-left:4.0pt;padding-right:4.0pt;">ru-en</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1.1.1.3.3.3" style="padding-bottom:2.15277pt;padding-left:4.0pt;padding-right:4.0pt;">77.3</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.1.1.1.3.3.4" style="padding-bottom:2.15277pt;padding-left:4.0pt;padding-right:4.0pt;">78.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1.1.1.3.3.5" style="padding-bottom:2.15277pt;padding-left:4.0pt;padding-right:4.0pt;">78.5</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.1.1.1.3.3.6" style="padding-bottom:2.15277pt;padding-left:4.0pt;padding-right:4.0pt;">78.7</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1.1.1.1.3.3.7" style="padding-bottom:2.15277pt;padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.3.3.7.1">78.9</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T4.1.1.1.1.1.1.4.1">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_4" id="S5.T4.1.1.1.1.1.1.4.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" id="S5.T4.1.1.1.1.1.1.4.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.1.1.1.1.1.1.4.1.1.1.1" style="width:6.8pt;height:35.8pt;vertical-align:-14.5pt;"><span class="ltx_transformed_inner" style="width:35.9pt;transform:translate(-14.51pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T4.1.1.1.1.1.1.4.1.1.1.1.1">ParaPat</span>
</span></span></span></span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1.1.1.4.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">de-en</span>
<span class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1.1.1.4.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">79.7</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T4.1.1.1.1.1.1.4.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">77.1</span>
<span class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1.1.1.4.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">78.0</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T4.1.1.1.1.1.1.4.1.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.4.1.6.1">80.0</span></span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T4.1.1.1.1.1.1.4.1.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.4.1.7.1">80.0</span></span></span>
<span class="ltx_tr" id="S5.T4.1.1.1.1.1.1.5.2">
<span class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.1.1.1.1.1.1.5.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">en-de</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T4.1.1.1.1.1.1.5.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">79.1</span>
<span class="ltx_td ltx_align_right" id="S5.T4.1.1.1.1.1.1.5.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">73.8</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T4.1.1.1.1.1.1.5.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">73.8</span>
<span class="ltx_td ltx_align_right" id="S5.T4.1.1.1.1.1.1.5.2.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.5.2.5.1">79.2</span></span>
<span class="ltx_td ltx_align_right" id="S5.T4.1.1.1.1.1.1.5.2.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.5.2.6.1">79.2</span></span></span>
<span class="ltx_tr" id="S5.T4.1.1.1.1.1.1.6.3">
<span class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.1.1.1.1.1.1.6.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">ru-en</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T4.1.1.1.1.1.1.6.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">72.2</span>
<span class="ltx_td ltx_align_right" id="S5.T4.1.1.1.1.1.1.6.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">74.5</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T4.1.1.1.1.1.1.6.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">73.9</span>
<span class="ltx_td ltx_align_right" id="S5.T4.1.1.1.1.1.1.6.3.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.6.3.5.1">75.1</span></span>
<span class="ltx_td ltx_align_right" id="S5.T4.1.1.1.1.1.1.6.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">75.0</span></span>
<span class="ltx_tr" id="S5.T4.1.1.1.1.1.1.7.4">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T4.1.1.1.1.1.1.7.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">en-ru</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T4.1.1.1.1.1.1.7.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">78.5</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.1.1.1.1.1.7.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">73.7</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T4.1.1.1.1.1.1.7.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">73.4</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.1.1.1.1.1.7.4.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.7.4.5.1">79.0</span></span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.1.1.1.1.1.7.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">78.7</span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Prompting with domain can improve <span class="ltx_text ltx_font_smallcaps" id="S5.T4.3.1">Comet-22</span> for the LLM, but is less effective for the ensemble.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The flexibility of LLM prompting can be used to add more descriptive task-specific instructions to improve quality <cite class="ltx_cite ltx_citemacro_cite">Zhang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib66" title="">2023a</a>)</cite>. Here, we prompt for domain (TED talks and patents).</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S5.T4" title="Table 4 ‚Ä£ 5.3 Domain Prompting ‚Ä£ 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†4</span></a> shows that additional domain information does not guarantee better LLM quality. For the TED-100 test set, ensembling has a 0.2 <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p2.1.1">Comet</span> improvement from an 0.5 LLM increase. Ensembling with or without the domain information in the prompt outperforms either the MT and LLM models alone. For TED, the LLM is stronger than the dedicated MT models, in contrast to our main results.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>In this work, we used the <math alttext="\lambda" class="ltx_Math" display="inline" id="footnote8.m1.1"><semantics id="footnote8.m1.1b"><mi id="footnote8.m1.1.1" xref="footnote8.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="footnote8.m1.1c"><ci id="footnote8.m1.1.1.cmml" xref="footnote8.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote8.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="footnote8.m1.1e">italic_Œª</annotation></semantics></math> set on the general validation set. Re-sweeping for each specific domain could lead to improved performance.</span></span></span> While our dedicated MT models were not trained translation for this specific domain, the LLM likely exposed to monolingual data in this domain. This highlights the complementary strengths of each paradigm‚Äîthe ensemble leverages both.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Document Context</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">For document or discourse input‚Äîsuch as TED talks‚Äîwhere the previous translated sentences are often relevant to the sentence to be translated, it may be better to provide the previous sentences and their translation. This contrasts with few-shot prompting where sentences pairs are high quality translations written by humans but are drawn from the validation so may not be relevant to the sentence at hand. Using sentence pairs from the same document should allow the LLM to enforce consistency across sentences and allow it to better translate phenomena that requires document-level context such as pronoun disambiguation.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S5.F1" title="Figure 1 ‚Ä£ 5.4 Document Context ‚Ä£ 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a> shows the <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p2.1.1">Comet-22</span> score against the number of sentence pairs in the prompt on the TED-100 test set. Prompting the LLM with document context outperforms few-shot prompting and the ensemble with context (solid orange line) to outperform all variants of ensembling and LLMs with context or few-shots, as well as the MT model. <span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.2">Conditioning on the model‚Äôs own previous outputs from the same document context outperforms few-shot prompting with the human references of less related sentences.</span></p>
</div>
<figure class="ltx_figure" id="S5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="506" id="S5.F1.g1" src="extracted/2311.08306v2/figs/context/ted100.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>TED-100 translation quality for various number of prompt examples (for few short learning or past context). Prompting with context outperforms few shot prompting, and it performs best when ensembled. </figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">Prior work found that document level-specific evaluation is required to evaluate document level phenomena <cite class="ltx_cite ltx_citemacro_cite">L√§ubli et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib36" title="">2018</a>); Toral et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib53" title="">2018</a>); Vernikos et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib57" title="">2022</a>)</cite>.
To this end, we use CTXPro <cite class="ltx_cite ltx_citemacro_cite">Wicks and Post (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib60" title="">2023</a>)</cite>, a specialized test suite which evaluates the translation accuracy of targeted words, given the document context.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<p class="ltx_p ltx_align_center" id="S5.T5.1"><span class="ltx_text" id="S5.T5.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.1.1.1" style="width:274.0pt;height:162pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T5.1.1.1.1"><span class="ltx_text" id="S5.T5.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T5.1.1.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_border_tt" id="S5.T5.1.1.1.1.1.1.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"></span>
<span class="ltx_td ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.1.1.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.1.1.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">MT</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T5.1.1.1.1.1.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;">LLM</span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T5.1.1.1.1.1.1.1.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">Ensemble</span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.2.2">
<span class="ltx_td" id="S5.T5.1.1.1.1.1.1.2.2.1" style="padding-left:5.0pt;padding-right:5.0pt;"></span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">context:</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.1.1.1.1.1.2.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">none</span>
<span class="ltx_td ltx_align_center" id="S5.T5.1.1.1.1.1.1.2.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">none</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.1.1.1.1.1.2.2.5" style="padding-left:5.0pt;padding-right:5.0pt;">10 sent</span>
<span class="ltx_td ltx_align_center" id="S5.T5.1.1.1.1.1.1.2.2.6" style="padding-left:5.0pt;padding-right:5.0pt;">none</span>
<span class="ltx_td ltx_align_center" id="S5.T5.1.1.1.1.1.1.2.2.7" style="padding-left:5.0pt;padding-right:5.0pt;">10 sent</span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.3.3">
<span class="ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_3" id="S5.T5.1.1.1.1.1.1.3.3.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S5.T5.1.1.1.1.1.1.3.3.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.1.1.1.1.1.1.3.3.1.1.1" style="width:6.9pt;height:23.3pt;vertical-align:-8.2pt;"><span class="ltx_transformed_inner" style="width:23.3pt;transform:translate(-8.19pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T5.1.1.1.1.1.1.3.3.1.1.1.1">en-de</span>
</span></span></span></span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1.1.1.3.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">auxiliary</span>
<span class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1.1.1.3.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">4.5%</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.1.1.1.3.3.4" style="padding-left:5.0pt;padding-right:5.0pt;">7.2%</span>
<span class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1.1.1.3.3.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1.3.3.5.1">28.0%</span></span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.1.1.1.3.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">6.2%</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.1.1.1.3.3.7" style="padding-left:5.0pt;padding-right:5.0pt;">13.7%</span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.4.4">
<span class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.1.1.1.1.1.1.4.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">formality</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.4.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">41.9%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.4.4.3" style="padding-left:5.0pt;padding-right:5.0pt;">38.2%</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.4.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">37.6%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.4.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">42.7%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.4.4.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1.4.4.6.1">43.8%</span></span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.5.5">
<span class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.1.1.1.1.1.1.5.5.1" style="padding-left:5.0pt;padding-right:5.0pt;">gender</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.5.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">44.6%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.5.5.3" style="padding-left:5.0pt;padding-right:5.0pt;">38.5%</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.5.5.4" style="padding-left:5.0pt;padding-right:5.0pt;">39.0%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.5.5.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1.5.5.5.1">45.8%</span></span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.5.5.6" style="padding-left:5.0pt;padding-right:5.0pt;">45.5%</span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.6.6">
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_t ltx_rowspan ltx_rowspan_4" id="S5.T5.1.1.1.1.1.1.6.6.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S5.T5.1.1.1.1.1.1.6.6.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.1.1.1.1.1.1.6.6.1.1.1" style="width:4.3pt;height:22.9pt;vertical-align:-9.3pt;"><span class="ltx_transformed_inner" style="width:22.8pt;transform:translate(-9.25pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S5.T5.1.1.1.1.1.1.6.6.1.1.1.1">en-ru</span>
</span></span></span></span>
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1.1.1.6.6.2" style="padding-left:5.0pt;padding-right:5.0pt;">auxiliary</span>
<span class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1.1.1.6.6.3" style="padding-left:5.0pt;padding-right:5.0pt;">2.6%</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.1.1.1.6.6.4" style="padding-left:5.0pt;padding-right:5.0pt;">2.3%</span>
<span class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1.1.1.6.6.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1.6.6.5.1">24.6%</span></span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.1.1.1.6.6.6" style="padding-left:5.0pt;padding-right:5.0pt;">2.6%</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S5.T5.1.1.1.1.1.1.6.6.7" style="padding-left:5.0pt;padding-right:5.0pt;">20.9%</span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.7.7">
<span class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.1.1.1.1.1.1.7.7.1" style="padding-left:5.0pt;padding-right:5.0pt;">formality</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.7.7.2" style="padding-left:5.0pt;padding-right:5.0pt;">42.5%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.7.7.3" style="padding-left:5.0pt;padding-right:5.0pt;">42.6%</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.7.7.4" style="padding-left:5.0pt;padding-right:5.0pt;">46.4%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.7.7.5" style="padding-left:5.0pt;padding-right:5.0pt;">46.4%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.7.7.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1.7.7.6.1">50.0%</span></span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.8.8">
<span class="ltx_td ltx_align_left ltx_border_r" id="S5.T5.1.1.1.1.1.1.8.8.1" style="padding-left:5.0pt;padding-right:5.0pt;">gender</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.8.8.2" style="padding-left:5.0pt;padding-right:5.0pt;">27.4%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.8.8.3" style="padding-left:5.0pt;padding-right:5.0pt;">31.9%</span>
<span class="ltx_td ltx_align_right ltx_border_r" id="S5.T5.1.1.1.1.1.1.8.8.4" style="padding-left:5.0pt;padding-right:5.0pt;">36.4%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.8.8.5" style="padding-left:5.0pt;padding-right:5.0pt;">31.6%</span>
<span class="ltx_td ltx_align_right" id="S5.T5.1.1.1.1.1.1.8.8.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1.8.8.6.1">37.6%</span></span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.9.9">
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T5.1.1.1.1.1.1.9.9.1" style="padding-left:5.0pt;padding-right:5.0pt;">inflection</span>
<span class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="S5.T5.1.1.1.1.1.1.9.9.2" style="padding-left:5.0pt;padding-right:5.0pt;">28.9%</span>
<span class="ltx_td ltx_align_right ltx_border_b" id="S5.T5.1.1.1.1.1.1.9.9.3" style="padding-left:5.0pt;padding-right:5.0pt;">22.6%</span>
<span class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="S5.T5.1.1.1.1.1.1.9.9.4" style="padding-left:5.0pt;padding-right:5.0pt;">25.6%</span>
<span class="ltx_td ltx_align_right ltx_border_b" id="S5.T5.1.1.1.1.1.1.9.9.5" style="padding-left:5.0pt;padding-right:5.0pt;">29.2%</span>
<span class="ltx_td ltx_align_right ltx_border_b" id="S5.T5.1.1.1.1.1.1.9.9.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1.9.9.6.1">31.4%</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>CTXPro accuracy. The ensembled models with context perform particularly well in to Russian. </figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S5.T5" title="Table 5 ‚Ä£ 5.4 Document Context ‚Ä£ 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†5</span></a> shows accuracy for various phenomena on en-ru and en-de. Adding context improves accuracy in all-but-one test set. Ensembling with context has the highest accuracy in 4 of 7 models. See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.SS5" title="A.5 COMET-22 CTXPro ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">¬ß¬†A.5</span></a> for <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p4.1.1">Comet</span> on this data; the ensemble is always best. So, when balancing <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p4.1.2">Comet</span> and CTXPro accuracy, the ensemble is best.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Unprompted LM Ensembling </h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Yee et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib64" title="">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Petrick et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib41" title="">2023</a>)</cite> improve translation
by ensembling with a smaller-scale language model <em class="ltx_emph ltx_font_italic" id="S5.SS5.p1.1.1">without</em> a task-specific prompt.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">We test this by ensembling the MT model with an unprompted LLM. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S5.F2" title="Figure 2 ‚Ä£ 5.5 Unprompted LM Ensembling ‚Ä£ 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†2</span></a> shows that this causes quality to drop precipitously.
The divergence from prior work may be due to differences in the base models; for example, <cite class="ltx_cite ltx_citemacro_citet">Petrick et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib41" title="">2023</a>)</cite> used an MT model trained on small amount of data, and <cite class="ltx_cite ltx_citemacro_citet">Yee et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib64" title="">2019</a>)</cite> trained their own LM. In our scenario with a strong MT model and a general purpose LLM, we do not see any benefit from using the LLM purely as a language model.</p>
</div>
<figure class="ltx_figure" id="S5.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S5.F2.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="47" id="S5.F2.1.g1" src="extracted/2311.08306v2/figs/no-prompt/legend.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="309" id="S5.F2.2.g1" src="extracted/2311.08306v2/figs/no-prompt/ru-en.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Using an LLM with a translation prompt and without any prompting (ru-en). Unprompted the ensemble is strictly worse than the MT baseline (mixing ratio <math alttext="\lambda=1" class="ltx_Math" display="inline" id="S5.F2.4.m1.1"><semantics id="S5.F2.4.m1.1b"><mrow id="S5.F2.4.m1.1.1" xref="S5.F2.4.m1.1.1.cmml"><mi id="S5.F2.4.m1.1.1.2" xref="S5.F2.4.m1.1.1.2.cmml">Œª</mi><mo id="S5.F2.4.m1.1.1.1" xref="S5.F2.4.m1.1.1.1.cmml">=</mo><mn id="S5.F2.4.m1.1.1.3" xref="S5.F2.4.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F2.4.m1.1c"><apply id="S5.F2.4.m1.1.1.cmml" xref="S5.F2.4.m1.1.1"><eq id="S5.F2.4.m1.1.1.1.cmml" xref="S5.F2.4.m1.1.1.1"></eq><ci id="S5.F2.4.m1.1.1.2.cmml" xref="S5.F2.4.m1.1.1.2">ùúÜ</ci><cn id="S5.F2.4.m1.1.1.3.cmml" type="integer" xref="S5.F2.4.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.4.m1.1d">\lambda=1</annotation><annotation encoding="application/x-llamapun" id="S5.F2.4.m1.1e">italic_Œª = 1</annotation></semantics></math>).</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related work</h2>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">LLMs for MT:</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">Pretrained LLMs can be prompted directly for translation <cite class="ltx_cite ltx_citemacro_cite">Brown et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib7" title="">2020</a>); Vilar et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib58" title="">2023</a>); Hendy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib21" title="">2023</a>); Robinson et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib44" title="">2023</a>); Zhang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib66" title="">2023a</a>); Agrawal et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib1" title="">2023</a>)</cite>, or fine-tuned for MT <cite class="ltx_cite ltx_citemacro_cite">Li et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib37" title="">2023</a>); Chen et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib9" title="">2023</a>); Moslem et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib39" title="">2023</a>); Zeng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib65" title="">2023</a>); Xu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib62" title="">2023</a>); Yang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib63" title="">2023</a>)</cite>. Our approach is complimentary‚Äîwe leverage prompting and in-context learning. We could also ensemble with a fine-tuned model. Since we perform inference-time combination of the LLM, we do not have the same training-compute burden as fine-tuning.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p2.1">Much work has explored integrating language models and NMT in various ways <cite class="ltx_cite ltx_citemacro_cite">Gulcehre et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib17" title="">2015</a>, <a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib18" title="">2017</a>); Stahlberg et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib50" title="">2018</a>); Yee et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib64" title="">2019</a>); Petrick et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib41" title="">2023</a>)</cite>, mostly by purely conditioning a language model on the target tokens; in contrast we focus on pretrained LLMs and <span class="ltx_text ltx_font_italic" id="S6.SS0.SSS0.Px1.p2.1.1">prompt</span> the LLM to produce translations.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Ensembling:</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">Diverse inputs can be combined to create stronger ensembles <cite class="ltx_cite ltx_citemacro_cite">Hansen and Salamon (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib19" title="">1990</a>); Dietterich (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib14" title="">2000</a>)</cite>.
Various model-combination methods have been used in MT.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p2.1">System combination of outputs was used for statistical machine translation (SMT)
<cite class="ltx_cite ltx_citemacro_cite">Bangalore et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib3" title="">2001</a>); Heafield and Lavie (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib20" title="">2010</a>); Freitag et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib16" title="">2014</a>)</cite>,
and averaging model weights <cite class="ltx_cite ltx_citemacro_cite">Junczys-Dowmunt et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib25" title="">2016</a>)</cite> or
ensembling <cite class="ltx_cite ltx_citemacro_cite">Chung et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib11" title="">2016</a>)</cite> are used for NMT. We build upon the latter.
<cite class="ltx_cite ltx_citemacro_citet">Jiang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib23" title="">2023</a>)</cite> propose a separate model to combine outputs from LLMs. We ensemble on-the-fly. <cite class="ltx_cite ltx_citemacro_citet">Ormazabal et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib40" title="">2023</a>)</cite> ensemble two LLM from the same family where the smaller LM was finetuned for MT. We create a hybrid ensemble of two distinct architectures and training regimes.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p3.1">Knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">Bucilu«é et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib8" title="">2006</a>); Hinton et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib22" title="">2015</a>)</cite> inspired methods can be a way to incorporate diverse models during training <cite class="ltx_cite ltx_citemacro_cite">Dakwale and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib12" title="">2017</a>); Khayrallah et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib29" title="">2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib30" title="">2020</a>)</cite>, as opposed to during inference.
<cite class="ltx_cite ltx_citemacro_citet">Jiang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib23" title="">2023</a>)</cite> introduce a separate model that combines outputs from LLMs. We ensemble on-the-fly.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p4">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p4.1">There are various methods proposed for improving translation quality by combining the adequacy and fluency advantages of SMT and NMT <cite class="ltx_cite ltx_citemacro_cite">Devlin et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib13" title="">2014</a>); Mi et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib38" title="">2016</a>); Junczys-Dowmunt et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib25" title="">2016</a>); Stahlberg et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib51" title="">2017</a>); Wang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib59" title="">2017</a>); Khayrallah et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib28" title="">2017</a>); Ding et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib15" title="">2017</a>); Zhang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib67" title="">2021</a>)</cite>. We combine the strengths of NMT and LLMs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We propose an on-the-fly ensembling of a dedicated MT model with an LLM, conditioned on the source and prompted for translation.
We demonstrate that an LLM can improve translation quality of a NMT model even if the LLM is weaker at translation, provided the LLM is good enough.
We prompt the LLM to imbue the sentence-based MT model with document-level ability, improving on sentence-level and context-focused metrics.
We find that ensembling with an LLM performs better than ensembling two MT models, even if each MT model is stronger than the LLM.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">While this work focuses on MT, the same techniques can be explored for other tasks, and may be especially useful for situations where the LLM and task-specific model have different properties and strengths.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">While we covered four languages to and from English, this is nowhere near enough to be a representative sample of languages and translation directions that would be of interest to others. We used Llama2; there are closed-access models that may be stronger at translation (e.g. GPT-4) but API access is insufficient for this method. As open-source new models are released, this method can be applied to them as well.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.2">We used a single value of <math alttext="\lambda" class="ltx_Math" display="inline" id="S8.p2.1.m1.1"><semantics id="S8.p2.1.m1.1a"><mi id="S8.p2.1.m1.1.1" xref="S8.p2.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S8.p2.1.m1.1b"><ci id="S8.p2.1.m1.1.1.cmml" xref="S8.p2.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S8.p2.1.m1.1d">italic_Œª</annotation></semantics></math>‚Äîwhich was set on the general domain validation set‚Äîfor all experiments. We did not re-sweep for each domain. While this is a more general scenario that applies when test-time domain is unknown, results might be improved for focused domains by tuning <math alttext="\lambda" class="ltx_Math" display="inline" id="S8.p2.2.m2.1"><semantics id="S8.p2.2.m2.1a"><mi id="S8.p2.2.m2.1.1" xref="S8.p2.2.m2.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S8.p2.2.m2.1b"><ci id="S8.p2.2.m2.1.1.cmml" xref="S8.p2.2.m2.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p2.2.m2.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S8.p2.2.m2.1d">italic_Œª</annotation></semantics></math> on domain-specific validation sets.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S5" title="5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">¬ß¬†5</span></a>, we explore different domains (TED talks, subtitles, and patents), and use <span class="ltx_text ltx_font_smallcaps" id="S8.p3.1.1">Comet-22</span> as a metric. <cite class="ltx_cite ltx_citemacro_citet">Zouhar et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib70" title="">2024</a>)</cite> recently demonstrated that neural fine-tuned metrics, such as <span class="ltx_text ltx_font_smallcaps" id="S8.p3.1.2">Comet</span> are not robust to domain shift, but noted that <span class="ltx_text ltx_font_smallcaps" id="S8.p3.1.3">Comet</span> still had the highest overall correlation with human judgements in their domain of study.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et¬†al. (2023)</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.564" title="">In-context examples selection for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 8857‚Äì8873, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akhbardeh et¬†al. (2021)</span>
<span class="ltx_bibblock">
Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ond≈ôej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta¬†R. Costa-jussa, Cristina Espa√±a-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera¬†Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.1" title="">Findings of the 2021 conference on machine translation (WMT21)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 1‚Äì88, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bangalore et¬†al. (2001)</span>
<span class="ltx_bibblock">
B.¬†Bangalore, G.¬†Bordel, and G.¬†Riccardi. 2001.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ASRU.2001.1034659" title="">Computing consensus translation from multiple machine translation systems</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU ‚Äô01.</em>, pages 351‚Äì354.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba√±√≥n et¬†al. (2020)</span>
<span class="ltx_bibblock">
Marta Ba√±√≥n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espl√†-Gomis, Mikel¬†L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz¬†Rojas, Leopoldo Pla¬†Sempere, Gema Ram√≠rez-S√°nchez, Elsa Sarr√≠as, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.417" title="">ParaCrawl: Web-scale acquisition of parallel corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4555‚Äì4567, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar et¬†al. (2018)</span>
<span class="ltx_bibblock">
Ond≈ôej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6401" title="">Findings of the 2018 conference on machine translation (WMT18)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>, pages 272‚Äì303, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Briakou et¬†al. (2023)</span>
<span class="ltx_bibblock">
Eleftheria Briakou, Colin Cherry, and George Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.524" title="">Searching for needles in a haystack: On the role of incidental bilingualism in PaLM‚Äôs translation capability</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 9432‚Äì9452, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et¬†al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared¬†D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, volume¬†33, pages 1877‚Äì1901. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bucilu«é et¬†al. (2006)</span>
<span class="ltx_bibblock">
Cristian Bucilu«é, Rich Caruana, and Alexandru Niculescu-Mizil. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/1150402.1150464" title="">Model compression</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, KDD ‚Äô06, page 535‚Äì541, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. (2023)</span>
<span class="ltx_bibblock">
Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2308.12674" title="">Improving translation faithfulness of large language models via augmenting instructions</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et¬†al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung¬†Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi¬†Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew¬†M. Dai, Thanumalayan¬†Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2204.02311" title="">Palm: Scaling language modeling with pathways</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et¬†al. (2016)</span>
<span class="ltx_bibblock">
Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1160" title="">A character-level decoder without explicit segmentation for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1693‚Äì1703, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dakwale and Monz (2017)</span>
<span class="ltx_bibblock">
Praveen Dakwale and Christof Monz. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2017.mtsummit-papers.13" title="">Fine-tuning for neural machine translation with limited degradation across in- and out-of-domain data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of Machine Translation Summit XVI: Research Track</em>, pages 156‚Äì169, Nagoya Japan.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et¬†al. (2014)</span>
<span class="ltx_bibblock">
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P14-1129" title="">Fast and robust neural network joint models for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1370‚Äì1380, Baltimore, Maryland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dietterich (2000)</span>
<span class="ltx_bibblock">
Thomas¬†G. Dietterich. 2000.

</span>
<span class="ltx_bibblock">Ensemble methods in machine learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Multiple Classifier Systems</em>, pages 1‚Äì15, Berlin, Heidelberg. Springer Berlin Heidelberg.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et¬†al. (2017)</span>
<span class="ltx_bibblock">
Shuoyang Ding, Huda Khayrallah, Philipp Koehn, Matt Post, Gaurav Kumar, and Kevin Duh. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-4724" title="">The JHU machine translation systems for WMT 2017</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the Second Conference on Machine Translation</em>, pages 276‚Äì282, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et¬†al. (2014)</span>
<span class="ltx_bibblock">
Markus Freitag, Matthias Huck, and Hermann Ney. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/E14-2008" title="">Jane: Open source machine translation system combination</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 29‚Äì32, Gothenburg, Sweden. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulcehre et¬†al. (2015)</span>
<span class="ltx_bibblock">
Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1503.03535" title="">On using monolingual corpora in neural machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulcehre et¬†al. (2017)</span>
<span class="ltx_bibblock">
Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, and Yoshua Bengio. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/https://doi.org/10.1016/j.csl.2017.01.014" title="">On integrating a language model into neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Computer Speech &amp; Language</em>, 45:137‚Äì148.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hansen and Salamon (1990)</span>
<span class="ltx_bibblock">
L.K. Hansen and P.¬†Salamon. 1990.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/34.58871" title="">Neural network ensembles</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 12(10):993‚Äì1001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heafield and Lavie (2010)</span>
<span class="ltx_bibblock">
Kenneth Heafield and Alon Lavie. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://kheafield.com/papers/avenue/marathon2010.pdf" title="">Combining machine translation output with open source: The Carnegie Mellon multi-engine machine translation scheme</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">The Prague Bulletin of Mathematical Linguistics</em>, 93:27‚Äì36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy et¬†al. (2023)</span>
<span class="ltx_bibblock">
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young¬†Jin Kim, Mohamed Afify, and Hany¬†Hassan Awadalla. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.09210" title="">How good are gpt models at machine translation? a comprehensive evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et¬†al. (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1503.02531" title="">Distilling the knowledge in a neural network</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et¬†al. (2023)</span>
<span class="ltx_bibblock">
Dongfu Jiang, Xiang Ren, and Bill¬†Yuchen Lin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.792" title="">LLM-blender: Ensembling large language models with pairwise ranking and generative fusion</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14165‚Äì14178, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et¬†al. (2023)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2301.08745" title="">Is chatgpt a good translator? yes with gpt-4 as the engine</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Junczys-Dowmunt et¬†al. (2016)</span>
<span class="ltx_bibblock">
Marcin Junczys-Dowmunt, Tomasz Dwojak, and Rico Sennrich. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W16-2316" title="">The AMU-UEDIN submission to the WMT16 news translation task: Attention-based NMT models as feature functions in phrase-based SMT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers</em>, pages 319‚Äì325, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Junczys-Dowmunt et¬†al. (2018)</span>
<span class="ltx_bibblock">
Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham¬†Fikri Aji, Nikolay Bogoychev, Andr√© F.¬†T. Martins, and Alexandra Birch. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-4020" title="">Marian: Fast neural machine translation in C++</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of ACL 2018, System Demonstrations</em>, pages 116‚Äì121, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khayrallah and Koehn (2018)</span>
<span class="ltx_bibblock">
Huda Khayrallah and Philipp Koehn. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-2709" title="">On the impact of various types of noise on neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</em>, pages 74‚Äì83, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khayrallah et¬†al. (2017)</span>
<span class="ltx_bibblock">
Huda Khayrallah, Gaurav Kumar, Kevin Duh, Matt Post, and Philipp Koehn. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/I17-2004" title="">Neural lattice search for domain adaptation in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, pages 20‚Äì25, Taipei, Taiwan. Asian Federation of Natural Language Processing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khayrallah et¬†al. (2018)</span>
<span class="ltx_bibblock">
Huda Khayrallah, Brian Thompson, Kevin Duh, and Philipp Koehn. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-2705" title="">Regularized training objective for continued training for domain adaptation in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</em>, pages 36‚Äì44, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khayrallah et¬†al. (2020)</span>
<span class="ltx_bibblock">
Huda Khayrallah, Brian Thompson, Matt Post, and Philipp Koehn. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.7" title="">Simulated multiple reference training improves low-resource machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 82‚Äì89, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et¬†al. (2022)</span>
<span class="ltx_bibblock">
Tom Kocmi, Rachel Bawden, Ond≈ôej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Nov√°k, Martin Popel, and Maja Popoviƒá. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.1" title="">Findings of the 2022 conference on machine translation (WMT22)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 1‚Äì45, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et¬†al. (2020)</span>
<span class="ltx_bibblock">
Philipp Koehn, Vishrav Chaudhary, Ahmed El-Kishky, Naman Goyal, Peng-Jen Chen, and Francisco Guzm√°n. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.78" title="">Findings of the WMT 2020 shared task on parallel corpus filtering and alignment</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 726‚Äì742, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et¬†al. (2019)</span>
<span class="ltx_bibblock">
Philipp Koehn, Francisco Guzm√°n, Vishrav Chaudhary, and Juan Pino. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5404" title="">Findings of the WMT 2019 shared task on parallel corpus filtering for low-resource conditions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)</em>, pages 54‚Äì72, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et¬†al. (2018)</span>
<span class="ltx_bibblock">
Philipp Koehn, Huda Khayrallah, Kenneth Heafield, and Mikel¬†L. Forcada. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6453" title="">Findings of the WMT 2018 shared task on parallel corpus filtering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>, pages 726‚Äì739, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/d18-2012" title="">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018</em>, pages 66‚Äì71. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">L√§ubli et¬†al. (2018)</span>
<span class="ltx_bibblock">
Samuel L√§ubli, Rico Sennrich, and Martin Volk. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1512" title="">Has machine translation achieved human parity? a case for document-level evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 4791‚Äì4796, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. (2023)</span>
<span class="ltx_bibblock">
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.15083" title="">Eliciting the translation ability of large language models via multilingual finetuning with translation instructions</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mi et¬†al. (2016)</span>
<span class="ltx_bibblock">
Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-2021" title="">Vocabulary manipulation for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 124‚Äì129, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moslem et¬†al. (2023)</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, John¬†D. Kelleher, and Andy Way. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eamt-1.22" title="">Adaptive machine translation with large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 227‚Äì237, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ormazabal et¬†al. (2023)</span>
<span class="ltx_bibblock">
Aitor Ormazabal, Mikel Artetxe, and Eneko Agirre. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.16876" title="">Comblm: Adapting black-box language models through small fine-tuned models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrick et¬†al. (2023)</span>
<span class="ltx_bibblock">
Frithjof Petrick, Christian Herold, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.12303" title="">Document-level language models for machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press and Wolf (2017)</span>
<span class="ltx_bibblock">
Ofir Press and Lior Wolf. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/E17-2025" title="">Using the output embedding to improve language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</em>, pages 157‚Äì163, Valencia, Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et¬†al. (2022)</span>
<span class="ltx_bibblock">
Ricardo Rei, Jos√©¬†G. C.¬†de Souza, Duarte Alves, Chrysoula Zerva, Ana¬†C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr√© F.¬†T. Martins. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.52" title="">COMET-22: Unbabel-IST 2022 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 578‚Äì585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson et¬†al. (2023)</span>
<span class="ltx_bibblock">
Nathaniel¬†R. Robinson, Perez Ogayo, David¬†R. Mortensen, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.07423" title="">Chatgpt mt: Competitive for high- (but not low-) resource languages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salesky et¬†al. (2021)</span>
<span class="ltx_bibblock">
Elizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni, Matteo Negri, Marco Turchi, Douglas¬†W. Oard, and Matt Post. 2021.

</span>
<span class="ltx_bibblock">Multilingual tedx corpus for speech recognition and translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of Interspeech</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et¬†al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1009" title="">Improving neural machine translation models with monolingual data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86‚Äì96, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sloto et¬†al. (2023)</span>
<span class="ltx_bibblock">
Steve Sloto, Brian Thompson, Huda Khayrallah, Tobias Domhan, Thamme Gowda, and Philipp Koehn. 2023.

</span>
<span class="ltx_bibblock">Findings of the WMT 2023 shared task on parallel data curation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the Eighth Conference on Machine Translation (WMT)</em>, Singapore, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et¬†al. (2022)</span>
<span class="ltx_bibblock">
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza¬†Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2201.11990" title="">Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soares et¬†al. (2020)</span>
<span class="ltx_bibblock">
Felipe Soares, Mark Stevenson, Diego Bartolome, and Anna Zaretskaya. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.lrec-1.465" title="">ParaPat: The multi-million sentences parallel corpus of patents abstracts</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, pages 3769‚Äì3774, Marseille, France. European Language Resources Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stahlberg et¬†al. (2018)</span>
<span class="ltx_bibblock">
Felix Stahlberg, James Cross, and Veselin Stoyanov. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6321" title="">Simple fusion: Return of the language model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 204‚Äì211, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stahlberg et¬†al. (2017)</span>
<span class="ltx_bibblock">
Felix Stahlberg, Adri√† de¬†Gispert, Eva Hasler, and Bill Byrne. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/E17-2058" title="">Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</em>, pages 362‚Äì368, Valencia, Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
J√∂rg Tiedemann. 2012.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in opus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC‚Äô12)</em>, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toral et¬†al. (2018)</span>
<span class="ltx_bibblock">
Antonio Toral, Sheila Castilho, Ke¬†Hu, and Andy Way. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6312" title="">Attaining the unattainable? reassessing claims of human parity in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 113‚Äì123, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et¬†al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et¬†al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian¬†Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit¬†Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric¬†Michael Smith, Ranjan Subramanian, Xiaoqing¬†Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian¬†Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et¬†al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan¬†N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Advances in Neural Information Processing Systems</em>, volume¬†30. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vernikos et¬†al. (2022)</span>
<span class="ltx_bibblock">
Giorgos Vernikos, Brian Thompson, Prashant Mathur, and Marcello Federico. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.6" title="">Embarrassingly easy document-level MT metrics: How to convert any pretrained metric into a document-level metric</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 118‚Äì128, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar et¬†al. (2023)</span>
<span class="ltx_bibblock">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.859" title="">Prompting PaLM for translation: Assessing strategies and performance</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 15406‚Äì15427, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2017)</span>
<span class="ltx_bibblock">
Xing Wang, Zhengdong Lu, Zhaopeng Tu, Hang Li, Deyi Xiong, and Min Zhang. 2017.

</span>
<span class="ltx_bibblock">Neural machine translation advised by statistical machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</em>, AAAI‚Äô17, page 3330‚Äì3336. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wicks and Post (2023)</span>
<span class="ltx_bibblock">
Rachel Wicks and Matt Post. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.42" title="">Identifying context-dependent translations for evaluation set production</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 452‚Äì467, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et¬†al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le¬†Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="">Transformers: State-of-the-art natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 38‚Äì45, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et¬†al. (2023)</span>
<span class="ltx_bibblock">
Haoran Xu, Young¬†Jin Kim, Amr Sharaf, and Hany¬†Hassan Awadalla. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.11674" title="">A paradigm shift in machine translation: Boosting translation performance of large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et¬†al. (2023)</span>
<span class="ltx_bibblock">
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.18098" title="">Bigtranslate: Augmenting large language models with multilingual translation capability over 100 languages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yee et¬†al. (2019)</span>
<span class="ltx_bibblock">
Kyra Yee, Yann Dauphin, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1571" title="">Simple and effective noisy channel modeling for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 5696‚Äì5701, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et¬†al. (2023)</span>
<span class="ltx_bibblock">
Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.04408" title="">Tim: Teaching large language models to translate with comparison</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2023a)</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Haddow, and Alexandra Birch. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/zhang23m.html" title="">Prompting large language model for machine translation: A case study</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib66.2.2">Proceedings of Machine Learning Research</em>, pages 41092‚Äì41110. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2021)</span>
<span class="ltx_bibblock">
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, and Yang Liu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TASLP.2021.3057831" title="">Neural machine translation with explicit phrase alignment</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29:1001‚Äì1010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2023b)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori¬†B. Hashimoto. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2301.13848" title="">Benchmarking large language models for news summarization</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2023c)</span>
<span class="ltx_bibblock">
Wenxuan Zhang, Yue Deng, Bing Liu, Sinno¬†Jialin Pan, and Lidong Bing. 2023c.

</span>
<span class="ltx_bibblock">Sentiment analysis in the era of large language models: A reality check.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2305.15005</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zouhar et¬†al. (2024)</span>
<span class="ltx_bibblock">
Vil√©m Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang, and Brian Thompson. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.18747" title="">Fine-tuned machine translation metrics struggle in unseen domains</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2306.07899</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experimental Details</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Hyperparameters</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">For German, Russian and Turkish, Transformer ‚Äòbig‚Äô models were trained (6 layer encoder-decoder, 1024 embedding dimensions, 4096 feed-forward dimensions, 16 heads) <cite class="ltx_cite ltx_citemacro_cite">Vaswani et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib56" title="">2017</a>)</cite>. The base Transformer architecture was used for Hausa (6 layer encoder-decoder, 512 embedding dimensions, 2048 feed-forward dimensions, 8 heads). We use weight tying <cite class="ltx_cite ltx_citemacro_cite">Press and Wolf (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib42" title="">2017</a>)</cite>. We train models using Marian NMT <cite class="ltx_cite ltx_citemacro_cite">Junczys-Dowmunt et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib26" title="">2018</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://marian-nmt.github.io/" title="">https://marian-nmt.github.io/</a></span></span></span> We convert MT models from Marian to Hugging Face format, to allow for inference with Llama2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib55" title="">2023b</a>)</cite> in the Hugging Face library <cite class="ltx_cite ltx_citemacro_cite">Wolf et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.08306v2#bib.bib61" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Monolingual Data </h3>
<figure class="ltx_table" id="A1.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T6.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T6.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="A1.T6.1.1.1.2">German</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="A1.T6.1.1.1.3">Russian</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="A1.T6.1.1.1.4">Turkish</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T6.1.1.1.5">Hausa</th>
</tr>
<tr class="ltx_tr" id="A1.T6.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T6.1.2.2.1"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T6.1.2.2.2">en</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T6.1.2.2.3">de</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T6.1.2.2.4">en</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T6.1.2.2.5">ru</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T6.1.2.2.6">en</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T6.1.2.2.7">tr</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T6.1.2.2.8">en</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A1.T6.1.2.2.9">ha</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T6.1.3.1.1">news-commentary-v18</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A1.T6.1.3.1.2">0.9m</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A1.T6.1.3.1.3">0.5m</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A1.T6.1.3.1.4">0.9m</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A1.T6.1.3.1.5">0.5m</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="A1.T6.1.3.1.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="A1.T6.1.3.1.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="A1.T6.1.3.1.8"></td>
<td class="ltx_td ltx_border_t" id="A1.T6.1.3.1.9"></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.4.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T6.1.4.2.1">europarl-v10</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.4.2.2">2.3m</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.4.2.3">2.1m</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.4.2.4"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.4.2.5"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.4.2.6"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.4.2.7"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.4.2.8"></td>
<td class="ltx_td" id="A1.T6.1.4.2.9"></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.5.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T6.1.5.3.1">news (all)</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.5.3.2">257.2m</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.5.3.3">468.9m</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.5.3.4">257.2m</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.5.3.5">142.7m</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.5.3.6"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.5.3.7"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.5.3.8"></td>
<td class="ltx_td" id="A1.T6.1.5.3.9"></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.6.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T6.1.6.4.1">news.2016</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.6.4.2"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.6.4.3"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.6.4.4"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.6.4.5"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.6.4.6">18.2m</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.6.4.7">1.7m</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.6.4.8"></td>
<td class="ltx_td" id="A1.T6.1.6.4.9"></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.7.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T6.1.7.5.1">news.2017</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.7.5.2"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.7.5.3"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.7.5.4"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.7.5.5"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.7.5.6">26.8m</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.7.5.7">3.0m</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.7.5.8"></td>
<td class="ltx_td" id="A1.T6.1.7.5.9"></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T6.1.8.6.1">news.2018</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.8.6.2"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.8.6.3"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.8.6.4"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.8.6.5"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.8.6.6"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.8.6.7"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.8.6.8">18.1m</td>
<td class="ltx_td" id="A1.T6.1.8.6.9"></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.9.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T6.1.9.7.1">news.2019</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.9.7.2"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.9.7.3"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.9.7.4"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.9.7.5"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.9.7.6"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.9.7.7"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.9.7.8">33.6m</td>
<td class="ltx_td" id="A1.T6.1.9.7.9"></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.10.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T6.1.10.8.1">news.2020</td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.10.8.2"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.10.8.3"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.10.8.4"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.10.8.5"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.10.8.6"></td>
<td class="ltx_td ltx_border_r" id="A1.T6.1.10.8.7"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A1.T6.1.10.8.8">41.4m</td>
<td class="ltx_td" id="A1.T6.1.10.8.9"></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.11.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A1.T6.1.11.9.1">CommonCrawl</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="A1.T6.1.11.9.2"></td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="A1.T6.1.11.9.3"></td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="A1.T6.1.11.9.4"></td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="A1.T6.1.11.9.5"></td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="A1.T6.1.11.9.6"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="A1.T6.1.11.9.7">511.2m</td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="A1.T6.1.11.9.8"></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T6.1.11.9.9">8.5m</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Monolingual Datasets.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Prompting</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.F3" title="Figure 3 ‚Ä£ A.3 Prompting ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†3</span></a>, <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.F4" title="Figure 4 ‚Ä£ A.3 Prompting ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†4</span></a>, <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.F5" title="Figure 5 ‚Ä£ A.3 Prompting ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†5</span></a>, and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.F6" title="Figure 6 ‚Ä£ A.3 Prompting ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†6</span></a> describe the various prompts we use.</p>
</div>
<figure class="ltx_figure" id="A1.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F3.1">Translate the following sentence from {src-language} to {tgt-language}:</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F3.2">{src-language}: {src}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F3.3">{tgt-language}:</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Baseline translation prompt.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F4.1">Translate the following sentence from {src-language} to {tgt-language} in a {style} style:</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F4.2">{src-language}: {src}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F4.3">{tgt-language}:</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Translation prompt with domain.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F5.1">Translate the following sentence from {src-language} to {tgt-language}:</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F5.2">{src-language}: {src-1}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F5.3">{tgt-language}: {tgt-1}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F5.4">‚Ä¶</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F5.5">{src-language}: {src-n}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F5.6">{tgt-language}: {tgt-n}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F5.7">{src-language}: {src}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F5.8">{tgt-language}:</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>n-shot translation prompt.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F6.1">Translate the following sentence from {src-language} to {tgt-language}:</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F6.2">{src-language}: {previous-src-n}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F6.3">{tgt-language}: {previous-translation-n}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F6.4">‚Ä¶</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F6.5">{src-language}: {previous-src}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F6.6">{tgt-language}: {previous-translation}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F6.7">{src-language}: {src}</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F6.8">{tgt-language}:</p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Context-aware translation prompt.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span><math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS4.1.m1.1"><semantics id="A1.SS4.1.m1.1b"><mi id="A1.SS4.1.m1.1.1" xref="A1.SS4.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.1.m1.1c"><ci id="A1.SS4.1.m1.1.1.cmml" xref="A1.SS4.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.1.m1.1d">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.1.m1.1e">italic_Œª</annotation></semantics></math>
</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.4"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.F7" title="Figure 7 ‚Ä£ A.4 ùúÜ ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†7</span></a> shows translation quality as we vary the mixing ratio, <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS4.p1.1.m1.1"><semantics id="A1.SS4.p1.1.m1.1a"><mi id="A1.SS4.p1.1.m1.1.1" xref="A1.SS4.p1.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.1.m1.1b"><ci id="A1.SS4.p1.1.m1.1.1.cmml" xref="A1.SS4.p1.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p1.1.m1.1d">italic_Œª</annotation></semantics></math>.
Note that <math alttext="p_{\text{ensemble}}" class="ltx_Math" display="inline" id="A1.SS4.p1.2.m2.1"><semantics id="A1.SS4.p1.2.m2.1a"><msub id="A1.SS4.p1.2.m2.1.1" xref="A1.SS4.p1.2.m2.1.1.cmml"><mi id="A1.SS4.p1.2.m2.1.1.2" xref="A1.SS4.p1.2.m2.1.1.2.cmml">p</mi><mtext id="A1.SS4.p1.2.m2.1.1.3" xref="A1.SS4.p1.2.m2.1.1.3a.cmml">ensemble</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.2.m2.1b"><apply id="A1.SS4.p1.2.m2.1.1.cmml" xref="A1.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS4.p1.2.m2.1.1.1.cmml" xref="A1.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="A1.SS4.p1.2.m2.1.1.2.cmml" xref="A1.SS4.p1.2.m2.1.1.2">ùëù</ci><ci id="A1.SS4.p1.2.m2.1.1.3a.cmml" xref="A1.SS4.p1.2.m2.1.1.3"><mtext id="A1.SS4.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="A1.SS4.p1.2.m2.1.1.3">ensemble</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.2.m2.1c">p_{\text{ensemble}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p1.2.m2.1d">italic_p start_POSTSUBSCRIPT ensemble end_POSTSUBSCRIPT</annotation></semantics></math> reduces to the LLM when <math alttext="\lambda=0" class="ltx_Math" display="inline" id="A1.SS4.p1.3.m3.1"><semantics id="A1.SS4.p1.3.m3.1a"><mrow id="A1.SS4.p1.3.m3.1.1" xref="A1.SS4.p1.3.m3.1.1.cmml"><mi id="A1.SS4.p1.3.m3.1.1.2" xref="A1.SS4.p1.3.m3.1.1.2.cmml">Œª</mi><mo id="A1.SS4.p1.3.m3.1.1.1" xref="A1.SS4.p1.3.m3.1.1.1.cmml">=</mo><mn id="A1.SS4.p1.3.m3.1.1.3" xref="A1.SS4.p1.3.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.3.m3.1b"><apply id="A1.SS4.p1.3.m3.1.1.cmml" xref="A1.SS4.p1.3.m3.1.1"><eq id="A1.SS4.p1.3.m3.1.1.1.cmml" xref="A1.SS4.p1.3.m3.1.1.1"></eq><ci id="A1.SS4.p1.3.m3.1.1.2.cmml" xref="A1.SS4.p1.3.m3.1.1.2">ùúÜ</ci><cn id="A1.SS4.p1.3.m3.1.1.3.cmml" type="integer" xref="A1.SS4.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.3.m3.1c">\lambda=0</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p1.3.m3.1d">italic_Œª = 0</annotation></semantics></math> and to the MT model when <math alttext="\lambda=1" class="ltx_Math" display="inline" id="A1.SS4.p1.4.m4.1"><semantics id="A1.SS4.p1.4.m4.1a"><mrow id="A1.SS4.p1.4.m4.1.1" xref="A1.SS4.p1.4.m4.1.1.cmml"><mi id="A1.SS4.p1.4.m4.1.1.2" xref="A1.SS4.p1.4.m4.1.1.2.cmml">Œª</mi><mo id="A1.SS4.p1.4.m4.1.1.1" xref="A1.SS4.p1.4.m4.1.1.1.cmml">=</mo><mn id="A1.SS4.p1.4.m4.1.1.3" xref="A1.SS4.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.4.m4.1b"><apply id="A1.SS4.p1.4.m4.1.1.cmml" xref="A1.SS4.p1.4.m4.1.1"><eq id="A1.SS4.p1.4.m4.1.1.1.cmml" xref="A1.SS4.p1.4.m4.1.1.1"></eq><ci id="A1.SS4.p1.4.m4.1.1.2.cmml" xref="A1.SS4.p1.4.m4.1.1.2">ùúÜ</ci><cn id="A1.SS4.p1.4.m4.1.1.3.cmml" type="integer" xref="A1.SS4.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.4.m4.1c">\lambda=1</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p1.4.m4.1d">italic_Œª = 1</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.SS4.p2">
<p class="ltx_p" id="A1.SS4.p2.1">For our results in the main section, we selected <math alttext="\lambda" class="ltx_Math" display="inline" id="A1.SS4.p2.1.m1.1"><semantics id="A1.SS4.p2.1.m1.1a"><mi id="A1.SS4.p2.1.m1.1.1" xref="A1.SS4.p2.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p2.1.m1.1b"><ci id="A1.SS4.p2.1.m1.1.1.cmml" xref="A1.SS4.p2.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p2.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p2.1.m1.1d">italic_Œª</annotation></semantics></math> on validation set translation quality.
Here we see that in cases where both models are reasonably strong (de-en, ru-en, and en-ru) the ensembling provides a quality boost.</p>
</div>
<figure class="ltx_figure" id="A1.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="17" id="A1.F7.1.g1" src="extracted/2311.08306v2/figs/7b/legend.png" width="120"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F7.sf1.g1" src="extracted/2311.08306v2/figs/7b/de-en.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>de-en</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F7.sf2.g1" src="extracted/2311.08306v2/figs/7b/en-de.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>en-de</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F7.sf3.g1" src="extracted/2311.08306v2/figs/7b/ru-en.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>ru-en</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F7.sf4.g1" src="extracted/2311.08306v2/figs/7b/en-ru.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>en-ru</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F7.sf5.g1" src="extracted/2311.08306v2/figs/7b/tr-en.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>tr-en</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F7.sf6.g1" src="extracted/2311.08306v2/figs/7b/en-tr.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>en-tr</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F7.sf7.g1" src="extracted/2311.08306v2/figs/7b/ha-en.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(g) </span>ha-en</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F7.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F7.sf8.g1" src="extracted/2311.08306v2/figs/7b/en-ha.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(h) </span>en-ha</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Ensembling MT model with 7B parameter LLM. Graphs shows <span class="ltx_text ltx_font_smallcaps" id="A1.F7.3.1">Comet-22</span> vs mixing ratio.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F8.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="21" id="A1.F8.1.g1" src="extracted/2311.08306v2/figs/multipro/legend.png" width="180"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F8.sf1.g1" src="extracted/2311.08306v2/figs/multipro/comet/en-de.gender.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>en-de gender</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F8.sf2.g1" src="extracted/2311.08306v2/figs/multipro/comet/en-de.auxiliary.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>en-de auxiliary</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F8.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F8.sf3.g1" src="extracted/2311.08306v2/figs/multipro/comet/en-de.formality.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>en-de formality</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F8.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F8.sf4.g1" src="extracted/2311.08306v2/figs/multipro/comet/en-ru.gender.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>en-ru gender</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F8.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F8.sf5.g1" src="extracted/2311.08306v2/figs/multipro/comet/en-ru.inflection.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>en-ru inflection</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F8.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F8.sf6.g1" src="extracted/2311.08306v2/figs/multipro/comet/en-ru.auxiliary.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>en-ru auxiliary</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F8.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="A1.F8.sf7.g1" src="extracted/2311.08306v2/figs/multipro/comet/en-ru.formality.png" width="157"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(g) </span>en-ru formality</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_smallcaps" id="A1.F8.3.1">Comet-22</span> on the data in CTXPro.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>COMET-22 CTXPro</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#A1.F8" title="Figure 8 ‚Ä£ A.4 ùúÜ ‚Ä£ Appendix A Experimental Details ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Figure¬†8</span></a> shows the <span class="ltx_text ltx_font_smallcaps" id="A1.SS5.p1.1.1">Comet-22</span> scores corresponding to the document translation accuracy show in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2311.08306v2#S5.T5" title="Table 5 ‚Ä£ 5.4 Document Context ‚Ä£ 5 Analysis ‚Ä£ On-the-Fly Fusion of Large Language Models and Machine Translation"><span class="ltx_text ltx_ref_tag">Table¬†5</span></a>. The ensemble is
always best on this data, then the MT, and then the LLM.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May 15 17:49:23 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
