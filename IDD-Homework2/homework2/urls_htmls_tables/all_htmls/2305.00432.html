<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.00432] Synthetic Data-based Detection of Zebras in Drone Imagery</title><meta property="og:description" content="Nowadays, there is a wide availability of datasets that enable the training of common object detectors or human detectors. These come in the form of labelled real-world images and require either a significant amount of…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data-based Detection of Zebras in Drone Imagery">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data-based Detection of Zebras in Drone Imagery">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.00432">

<!--Generated on Thu Feb 29 10:38:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Synthetic Data-based Detection of Zebras in Drone Imagery</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elia Bonetto<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">∗,†</span></sup>   and Aamir Ahmad<sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">†,∗</span></sup>
</span><span class="ltx_author_notes"><sup id="id7.7.id1" class="ltx_sup">∗</sup>Max Planck Institute for Intelligent Systems, Tübingen, Germany. <span id="id8.8.id2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">firstname.lastname@tuebingen.mpg.de</span><sup id="id9.9.id1" class="ltx_sup">†</sup>Institute of Flight Mechanics and <span id="id10.10.id2" class="ltx_text">Controls</span>, University of Stuttgart, Germany. <span id="id11.11.id3" class="ltx_text ltx_font_typewriter" style="font-size:80%;">firstname.lastname@ifr.uni-stuttgart.de</span>The authors thank Eric Price, Nitin Saini, Egor Iuganov, Chenghao Xu, and Benedikt Schwämmle for their help in collecting and processing the data.We would like to extend our deepest gratitude to the Wilhelma Zoo in Stuttgart, and in particular to Ms. Ulrike Rademacher, for permitting us to record videos of Grévy’s zebras on the premises of the Wilhelma Zoo.The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Elia Bonetto.979-8-3503-0704-7/23/$31.00 ©2023 IEEE</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">Nowadays, there is a wide availability of datasets that enable the training of common object detectors or human detectors. These come in the form of labelled real-world images and require either a significant amount of human effort, with a high probability of errors such as missing labels, or very constrained scenarios, e.g. VICON systems. On the other hand, uncommon scenarios, like aerial views, animals, like wild zebras, or difficult-to-obtain information, such as human shapes, are hardly available. To overcome this, synthetic data generation with realistic rendering technologies has recently gained traction and advanced research areas such as target tracking and human pose estimation. However, subjects such as wild animals are still usually not well represented in such datasets. In this work, we first show that a pre-trained YOLO detector can not identify zebras in real images recorded from aerial viewpoints. To solve this, we present an approach for training an animal detector using only synthetic data. We start by generating a novel synthetic zebra dataset using GRADE, a state-of-the-art framework for data generation. The dataset includes RGB, depth, skeletal joint locations, pose, shape and instance segmentations for each subject. We use this to train a YOLO detector from scratch. Through extensive evaluations of our model with real-world data from i) limited datasets available on the internet and ii) a new one collected and manually labelled by us, we show that we can detect zebras by using only synthetic data during training. The code, results, trained models, and both the generated and training data are provided as open-source at <a target="_blank" href="https://eliabntt.github.io/grade-rr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://eliabntt.github.io/grade-rr</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A large dataset that includes realism and diversity in features is a fundamental building block for obtaining any working and reliable deep-learning model. This is especially true when dealing with visual tasks such as detection, semantic segmentation and shape estimation. For these, variability in both visual appearances and environmental conditions as well as a high number of instances are required. A variety of datasets have been introduced during the last decades to address various image-based tasks like MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and PASCAL-VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. These have historically been based on real-world data, be this either images or videos, manually labelled by humans. Apart from being time-consuming and costly, this introduces errors such as missing and wrong labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Examples of these can be visualized in Fig. <a href="#S2.F2" title="Figure 2 ‣ II Related Work ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Fig. <a href="#S2.F3" title="Figure 3 ‣ II Related Work ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Furthermore, ground truth for specific tasks may not be available either because it is hard to obtain, e.g., shape or skeletal information, or because it requires costly manual labelling procedures. These limitations impede the usage of these datasets in problems such as aerial human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, animal pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, or aerial wild-animal detection. For these reasons, methodologies to generate synthetic data became more ubiquitous since the advent of rendering engines such as Unity, Blender, Unreal Engine and IsaacSim. These are advantageous in multiple aspects since they allow generation and automatic labelling of ground truth data through full customization possibilities and with minimum human effort <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Indeed, synthetic data has been used in a variety of tasks such as human detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, pose and shape estimation of humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. However, they usually lack the visual realism necessary to generalize well to real-world data if used alone. Thus, a combination of real and synthetic data is often utilized <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.00432/assets/fig/PromImg.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">An example image of our synthetically generated zebras in a Savanna environment.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Moreover, these datasets are usually application-specific and hardly generalize to different scenarios, tasks, or data. For example, wild animals are widely under-represented in datasets such as COCO or PASCAL-VOC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Indeed, apart from a limited number of labelled images and videos of uncommon animal species such as zebras, hippopotami, and giraffes, there is also a general lack of variety of scenarios in which those are recorded. Taking zebras as an example, there are only 1916 training and 85 validation images containing at least one instance of them in the COCO dataset. To solve this problem, we generate a new synthetic dataset using our GRADE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> framework, a publicly available animated zebra model and environments from the Unreal Engine marketplace. An example of the generated data can be seen in Fig. <a href="#S1.F1" title="Figure 1 ‣ I INTRODUCTION ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Fig. <a href="#S3.F4" title="Figure 4 ‣ III-A2 Dynamic assets ‣ III-A Synthetic data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We use this data to train a YOLO-based detector and perform evaluations with an extensive dataset of zebras captured by drones consisting of 104K images and the APT-36K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> dataset. With this, we show that training with our synthetic data outperforms the baseline models trained on real-world datasets. In this paper, we take zebras as an example as it is an endangered species, which is greatly under-represented in currently available datasets. However, the proposed method can be generalized to other animals as well.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The rest of the paper is structured as follows. In Sec. <a href="#S2" title="II Related Work ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> we review the current state-of-the-art in simulated worlds and animal datasets. Our method is described thoroughly in Sec. <a href="#S3" title="III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. In there, we give a general overview of the system and explain how we generated our data. We then present the results of our experiments in Sec. <a href="#S4" title="IV EXPERIMENT AND EVALUATIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and conclude the work with Sec. <a href="#S5" title="V CONCLUSIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> with comments on known limitations and possible future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we focus on two areas: animal-based datasets, and simulation engines.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Animals.</span> There are not many animals-based datasets available in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, especially considering full 3D-vertices information and precise segmentation. This is clearly related to the difficulties of collecting and labelling ground truth data in outdoor scenarios. Various approaches have been applied to overcome this problem, ranging from using toy models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, merging different datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, or using synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. However, all of them fall short in some aspects like lack of animal species variability, size, pose and shape information, skeletal joints location, or limited capturing settings. For example, <span id="S2.p2.1.2" class="ltx_text">Horse-10</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> has only horses moving left-to-right. The authors of the SMAL model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> do not release the generated data. The Grévy’s zebra dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> consists only of 900 low-resolution images that do not contain either correct bounding boxes or labels for all animals. An example of that is provided in Fig. <a href="#S2.F2" title="Figure 2 ‣ II Related Work ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. AnimalPose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> focuses on a limited set of animals, in which zebras are not included. The 4DComplete dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, although it contains various animal animations, it fails on releasing textures or textured FBX files making it impossible to customize. They do provide rendered RGB+D images and scene flow but, still, the renderings are provided without any background information. Other synthetic datasets, such as the one from Mu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, contain data which cannot be used to train a successful detector since they are generated with unrealistic backgrounds and textures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. These also suffer from the low viewpoint variability and diversity of scenarios, such as the data from COCO. We must also note that, as shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ II Related Work ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, COCO is not exempt from wrong or imprecise labelled data.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.00432/assets/fig/figure_1.jpg" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="287" height="287" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.00432/assets/fig/figure_1-3.jpg" id="S2.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="287" height="287" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Examples of missing bounding boxes and keypoints from the Grévy’s zebra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> dataset. Image ids 869 (left) and 882 (right). </span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure">
<div id="S2.F3.2" class="ltx_block">
<figure id="S2.F3.sf1" class="ltx_figure ltx_align_center"><img src="/html/2305.00432/assets/fig/20164.jpg" id="S2.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="296" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">ID: 20164, missing bounding boxes</span></figcaption>
</figure>
<figure id="S2.F3.sf2" class="ltx_figure ltx_align_center"><img src="/html/2305.00432/assets/fig/22149.jpg" id="S2.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="296" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">ID: 22149, toy labeled</span></figcaption>
</figure>
<figure id="S2.F3.sf3" class="ltx_figure ltx_align_center"><img src="/html/2305.00432/assets/fig/32206.jpg" id="S2.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="296" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">ID: 32206, wrong bounding box</span></figcaption>
</figure>
<figure id="S2.F3.sf4" class="ltx_figure ltx_align_center"><img src="/html/2305.00432/assets/fig/533961.jpg" id="S2.F3.sf4.g1" class="ltx_graphics ltx_img_landscape" width="296" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S2.F3.sf4.3.2" class="ltx_text" style="font-size:90%;">ID: 533961, imprecise bounding box</span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.4.2" class="ltx_text" style="font-size:90%;">Four examples of wrongly labelled zebras from the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> dataset. </span></figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Simulation engines.</span> Gazebo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is currently the standard for robotic simulation. High reliable physics and tight integration with ROS are its key advantages. However, the lack of visual realism and customization possibility, makes it unusable for generating visual data to be used in learning tasks. Indeed, alternatives emerged in the last years, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, along with several datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> that use Unreal Engine, Unity, and Blender for rendering. The combination of AirSim and Unreal Engine has been widely explored to generate multiple datasets focused on specific tasks such as human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and visual odometry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Simulators focused on robotics are usually limited by the type of the environment, e.g. indoor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, or the task, e.g. self-driving car <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Clearly, generalizing them to outdoor scenarios with animated animals is not trivial. GRADE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is a recently introduced method to generate synthetic data built directly upon Isaac Sim. It is a framework that includes both data generation and general robotic testing capabilities thanks to its integration with ROS and the use of ray- and path-tracing. In this work, we leverage the flexibility of GRADE to generate new synthetic data of outdoor scenarios with randomly placed zebras.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Approach</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Using the system introduced in our previous work GRADE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we generate an outdoor-environment dataset focused on zebras. GRADE is our synthetic data generation framework based on Isaac Sim. Thanks to the flexibility of GRADE, this approach will be easily applicable also to other animal species or setups. The details about the GRADE framework and the simulation management are thoroughly described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. We proceed here highlighting any major difference with respect to the already introduced system.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Synthetic data</span>
</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Environments</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">We selected nine commercial and one freely available environments from the Unreal Engine marketplace. We used the Unreal Engine Omniverse connector<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://bit.ly/3X82sph" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3X82sph</a></span></span></span> to convert them to the USD file format. We list the environments with the corresponding shortened URL in Tab. <a href="#S3.T1" title="TABLE I ‣ III-A1 Environments ‣ III-A Synthetic data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. For each environment, we used directly the available demos and pre-built scenarios. Then, we proceeded to remove the original sky sphere and fix textures when necessary. The connector indeed does not yet support full export of the terrains from Unreal Engine, resulting in a lower level of detail, e.g. missing 3D grass, some textures, and level of details. We replaced the textures with some taken from IsaacSim itself, resembling the <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">color</span> of the grass.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:368.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(100.2pt,-85.1pt) scale(1.85925306464426,1.85925306464426) ;">
<table id="S3.T1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.1.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r">Environment Name</th>
<th id="S3.T1.2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">URL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.1.2.1" class="ltx_tr">
<th id="S3.T1.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Bliss</th>
<td id="S3.T1.2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t"><a target="_blank" href="https://bit.ly/3HD3zYP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3HD3zYP</a></td>
</tr>
<tr id="S3.T1.2.1.3.2" class="ltx_tr">
<th id="S3.T1.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Forest</th>
<td id="S3.T1.2.1.3.2.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3mYQv8Z" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3mYQv8Z</a></td>
</tr>
<tr id="S3.T1.2.1.4.3" class="ltx_tr">
<th id="S3.T1.2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Grasslands</th>
<td id="S3.T1.2.1.4.3.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3HD3zYP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3HD3zYP</a></td>
</tr>
<tr id="S3.T1.2.1.5.4" class="ltx_tr">
<th id="S3.T1.2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Iceland</th>
<td id="S3.T1.2.1.5.4.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3Ax8zKi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3Ax8zKi</a></td>
</tr>
<tr id="S3.T1.2.1.6.5" class="ltx_tr">
<th id="S3.T1.2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">L_Terrain</th>
<td id="S3.T1.2.1.6.5.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3V6H7MU" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3V6H7MU</a></td>
</tr>
<tr id="S3.T1.2.1.7.6" class="ltx_tr">
<th id="S3.T1.2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Meadow</th>
<td id="S3.T1.2.1.7.6.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3Hgxk1n" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3Hgxk1n</a></td>
</tr>
<tr id="S3.T1.2.1.8.7" class="ltx_tr">
<th id="S3.T1.2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Moorlands</th>
<td id="S3.T1.2.1.8.7.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3oHT1ku" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3oHT1ku</a></td>
</tr>
<tr id="S3.T1.2.1.9.8" class="ltx_tr">
<th id="S3.T1.2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Rural Australia (Free)</th>
<td id="S3.T1.2.1.9.8.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3i5j6Hi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3i5j6Hi</a></td>
</tr>
<tr id="S3.T1.2.1.10.9" class="ltx_tr">
<th id="S3.T1.2.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Windmills</th>
<td id="S3.T1.2.1.10.9.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3AvVTDK" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3AvVTDK</a></td>
</tr>
<tr id="S3.T1.2.1.11.10" class="ltx_tr">
<th id="S3.T1.2.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Woodland</th>
<td id="S3.T1.2.1.11.10.2" class="ltx_td ltx_align_left"><a target="_blank" href="https://bit.ly/3mYQv8Z" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://bit.ly/3mYQv8Z</a></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.4.2" class="ltx_text" style="font-size:90%;">Names and shortened URLs of the used environments.</span></figcaption>
</figure>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Dynamic assets</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">We use a freely available zebra model from SketchFab <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. This model consists of 34 different in-place animation sequences, i.e. without root translation or rotation movements, for a total of 888 animation frames. We converted each animation sequence to the USD format using Blender and its Omniverse connector. Then, we post-processed the sequence to obtain per-frame vertices position and skeletal information. This allows us, for every generated frame, to have corresponding ground truth information about these two characteristics. The vertices are used to compute oriented bounding boxes that are then employed for the placement procedure.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.00432/assets/fig/rgb_conv.jpg" id="S3.F4.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="147" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.00432/assets/fig/depth_conv.jpg" id="S3.F4.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="147" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.00432/assets/fig/bbox2d_conv.jpg" id="S3.F4.g3" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="147" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.00432/assets/fig/instance_conv.jpg" id="S3.F4.g4" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="147" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.00432/assets/fig/bbox3d_conv.jpg" id="S3.F4.g5" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="147" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.00432/assets/fig/vertices_black.jpg" id="S3.F4.g6" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="147" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.00432/assets/fig/rgb_v1.jpg" id="S3.F4.g7" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="147" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.00432/assets/fig/rgb_v2.jpg" id="S3.F4.g8" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="147" height="99" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">An example of the generated data following the pipeline described in Sec. <a href="#S3" title="III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. On the first row, from the left, we can see the rendered RGB image, depth data, 2D bounding boxes, and semantic instances. On the second row, from the left, we can see 3D-oriented bounding boxes, the vertices of each mesh drawed over a black background, and the second and third views of the same scene as taken from the other drones. Best viewed in color.</span></figcaption>
</figure>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.5.1.1" class="ltx_text">III-A</span>3 </span>Placement of zebras</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Zebra placement is based on an ad-hoc procedure that is repeated every time a frame is generated. For every environment we select a specific mesh as ‘terrain’, which represents the area in which we will then place the zebras. The placement consists then of four main steps: i) selecting a random rectangular area of the terrain, ii) randomly selecting a set of zebras, iii) for each zebra select frame of its animation sequence, a scaling factor and a global orientation of the zebra, iv) place the zebra in the rectangular area considering the bounding-box occupancy. The sides of the rectangle are randomly selected to be between 40 and 120 meters, while the scaling factor ranges between 40% and 100% and allows us to obtain a higher degree of variability. The placement is an iterative procedure that considers one zebra model after the other. Any model that cannot be placed following a detected collision is removed from the simulation. The final results depend mostly on the resolution of the terrain mesh for both collisions between meshes and contact of the zebras with the ground. In general, we noted that collisions are rare and that contacts with the ground are good. Note that, as opposed to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we do not consider the full animation sequence since we lack any root translation information.</p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS4.5.1.1" class="ltx_text">III-A</span>4 </span>Data collection methodologies</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.6" class="ltx_p">Contrary to what is done for indoor environments in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, here we focus on image generation rather than video sequences. We also perform a series of randomization for each captured frame, i.e. the i) time of day, ii) number of zebras in the environment, iii) their scaling factor, iv) their specific animation frame, and the v) placement of three cameras that will record the scene. Specifically, given any environment, we set up three aerial cameras and randomly pre-load 250 zebras at the beginning of each experiment. We then uniformly select the number of zebras that will be placed in the next frame. This number is set to be between 2 and 250. Note that this is <span id="S3.SS1.SSS4.p1.6.1" class="ltx_text ltx_font_italic">not</span> the number that will appear in each frame, nor is the final number of zebras that are actually placed. As explained above, the placement strategy may remove some of the zebras and the camera may not observe all the zebras given a point of view. Once the placement happened, we randomize the location of the cameras and the time of days three times. The time of day will be 90% of the time between 5 am and 8 pm, which results in good lighting conditions given our current settings, and 10% of the time in the remaining hours, resulting in dusk-to-night light settings. This further randomizes the appearances of both the generated frames and the shadows. Cameras are placed using the average location of the zebras as a pivot point. For the placement of the cameras, we distinguish between two slightly different image-generation procedures: one more general and one more focused on capturing zebras from a nearer viewpoint. We first describe the former and then identify the minor modifications that we applied to the latter. From the pivot point, we randomize the distance in the x-y plane and the height of the camera. The height is set to be between 5 and 20 meters more than the average of the zebras, while the x and y are set to be between -/+ 100 meters. Once the position is fixed, we can compute and randomize roll, pitch, and yaw. Roll is set to be within <math id="S3.SS1.SSS4.p1.1.m1.2" class="ltx_Math" alttext="[-10,10]" display="inline"><semantics id="S3.SS1.SSS4.p1.1.m1.2a"><mrow id="S3.SS1.SSS4.p1.1.m1.2.2.1" xref="S3.SS1.SSS4.p1.1.m1.2.2.2.cmml"><mo stretchy="false" id="S3.SS1.SSS4.p1.1.m1.2.2.1.2" xref="S3.SS1.SSS4.p1.1.m1.2.2.2.cmml">[</mo><mrow id="S3.SS1.SSS4.p1.1.m1.2.2.1.1" xref="S3.SS1.SSS4.p1.1.m1.2.2.1.1.cmml"><mo id="S3.SS1.SSS4.p1.1.m1.2.2.1.1a" xref="S3.SS1.SSS4.p1.1.m1.2.2.1.1.cmml">−</mo><mn id="S3.SS1.SSS4.p1.1.m1.2.2.1.1.2" xref="S3.SS1.SSS4.p1.1.m1.2.2.1.1.2.cmml">10</mn></mrow><mo id="S3.SS1.SSS4.p1.1.m1.2.2.1.3" xref="S3.SS1.SSS4.p1.1.m1.2.2.2.cmml">,</mo><mn id="S3.SS1.SSS4.p1.1.m1.1.1" xref="S3.SS1.SSS4.p1.1.m1.1.1.cmml">10</mn><mo stretchy="false" id="S3.SS1.SSS4.p1.1.m1.2.2.1.4" xref="S3.SS1.SSS4.p1.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.1.m1.2b"><interval closure="closed" id="S3.SS1.SSS4.p1.1.m1.2.2.2.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.2.1"><apply id="S3.SS1.SSS4.p1.1.m1.2.2.1.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.2.1.1"><minus id="S3.SS1.SSS4.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.2.1.1"></minus><cn type="integer" id="S3.SS1.SSS4.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.SSS4.p1.1.m1.2.2.1.1.2">10</cn></apply><cn type="integer" id="S3.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS4.p1.1.m1.1.1">10</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.1.m1.2c">[-10,10]</annotation></semantics></math> degrees, yaw is set to be the ray that connects the camera and the pivot point with an additional random <math id="S3.SS1.SSS4.p1.2.m2.2" class="ltx_Math" alttext="[-30,30]" display="inline"><semantics id="S3.SS1.SSS4.p1.2.m2.2a"><mrow id="S3.SS1.SSS4.p1.2.m2.2.2.1" xref="S3.SS1.SSS4.p1.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.SS1.SSS4.p1.2.m2.2.2.1.2" xref="S3.SS1.SSS4.p1.2.m2.2.2.2.cmml">[</mo><mrow id="S3.SS1.SSS4.p1.2.m2.2.2.1.1" xref="S3.SS1.SSS4.p1.2.m2.2.2.1.1.cmml"><mo id="S3.SS1.SSS4.p1.2.m2.2.2.1.1a" xref="S3.SS1.SSS4.p1.2.m2.2.2.1.1.cmml">−</mo><mn id="S3.SS1.SSS4.p1.2.m2.2.2.1.1.2" xref="S3.SS1.SSS4.p1.2.m2.2.2.1.1.2.cmml">30</mn></mrow><mo id="S3.SS1.SSS4.p1.2.m2.2.2.1.3" xref="S3.SS1.SSS4.p1.2.m2.2.2.2.cmml">,</mo><mn id="S3.SS1.SSS4.p1.2.m2.1.1" xref="S3.SS1.SSS4.p1.2.m2.1.1.cmml">30</mn><mo stretchy="false" id="S3.SS1.SSS4.p1.2.m2.2.2.1.4" xref="S3.SS1.SSS4.p1.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.2.m2.2b"><interval closure="closed" id="S3.SS1.SSS4.p1.2.m2.2.2.2.cmml" xref="S3.SS1.SSS4.p1.2.m2.2.2.1"><apply id="S3.SS1.SSS4.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.SSS4.p1.2.m2.2.2.1.1"><minus id="S3.SS1.SSS4.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.SSS4.p1.2.m2.2.2.1.1"></minus><cn type="integer" id="S3.SS1.SSS4.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.SSS4.p1.2.m2.2.2.1.1.2">30</cn></apply><cn type="integer" id="S3.SS1.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS4.p1.2.m2.1.1">30</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.2.m2.2c">[-30,30]</annotation></semantics></math> degrees. Pitch is computed as <math id="S3.SS1.SSS4.p1.3.m3.2" class="ltx_Math" alttext="\theta=atan2(pivot_{z}-cam_{z},d(pivot,camera))+15" display="inline"><semantics id="S3.SS1.SSS4.p1.3.m3.2a"><mrow id="S3.SS1.SSS4.p1.3.m3.2.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.2.2.4" xref="S3.SS1.SSS4.p1.3.m3.2.2.4.cmml">θ</mi><mo id="S3.SS1.SSS4.p1.3.m3.2.2.3" xref="S3.SS1.SSS4.p1.3.m3.2.2.3.cmml">=</mo><mrow id="S3.SS1.SSS4.p1.3.m3.2.2.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.cmml"><mrow id="S3.SS1.SSS4.p1.3.m3.2.2.2.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.4" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.5" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3a" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.6" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3b" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.7" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3c" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3.cmml">​</mo><mn id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.8" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.8.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3d" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3.cmml">​</mo><mrow id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.3" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.3.cmml">(</mo><mrow id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.cmml"><mrow id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.2" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.3" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1a" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.4" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1b" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.5" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1c" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.2" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.2.cmml">t</mi><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.3" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.3.cmml">z</mi></msub></mrow><mo id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.1" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.2" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.1" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.3" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.1a" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.1.cmml">​</mo><msub id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.2" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.2.cmml">m</mi><mi id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.3" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.3.cmml">z</mi></msub></mrow></mrow><mo id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.4" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.3.cmml">,</mo><mrow id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.4" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.3" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.3.cmml">​</mo><mrow id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.3" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.3.cmml">(</mo><mrow id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.3" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1a" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.4" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1b" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.5" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1c" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.6" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.6.cmml">t</mi></mrow><mo id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.4" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.3.cmml">,</mo><mrow id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.2" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.3" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1a" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.4" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1b" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.5" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1c" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.6" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1d" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.7" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.7.cmml">a</mi></mrow><mo stretchy="false" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.5" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.5" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS1.SSS4.p1.3.m3.2.2.2.3" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.3.cmml">+</mo><mn id="S3.SS1.SSS4.p1.3.m3.2.2.2.4" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.4.cmml">15</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.3.m3.2b"><apply id="S3.SS1.SSS4.p1.3.m3.2.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2"><eq id="S3.SS1.SSS4.p1.3.m3.2.2.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.3"></eq><ci id="S3.SS1.SSS4.p1.3.m3.2.2.4.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.4">𝜃</ci><apply id="S3.SS1.SSS4.p1.3.m3.2.2.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2"><plus id="S3.SS1.SSS4.p1.3.m3.2.2.2.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.3"></plus><apply id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2"><times id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.3"></times><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.4.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.4">𝑎</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.5.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.5">𝑡</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.6.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.6">𝑎</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.7.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.7">𝑛</ci><cn type="integer" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.8.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.8">2</cn><interval closure="open" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2"><apply id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1"><minus id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.1"></minus><apply id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2"><times id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.1"></times><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.2">𝑝</ci><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.3">𝑖</ci><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.4.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.4">𝑣</ci><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.5.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.5">𝑜</ci><apply id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6"><csymbol cd="ambiguous" id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6">subscript</csymbol><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.2">𝑡</ci><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.2.6.3">𝑧</ci></apply></apply><apply id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3"><times id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.1"></times><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.2">𝑐</ci><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.3">𝑎</ci><apply id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4">subscript</csymbol><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.2">𝑚</ci><ci id="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.1.1.1.1.1.1.1.3.4.3">𝑧</ci></apply></apply></apply><apply id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2"><times id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.3"></times><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.4.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.4">𝑑</ci><interval closure="open" id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2"><apply id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1"><times id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.1"></times><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.2">𝑝</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.3">𝑖</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.4.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.4">𝑣</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.5.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.5">𝑜</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.6.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.1.1.1.6">𝑡</ci></apply><apply id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2"><times id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.1"></times><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.2">𝑐</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.3">𝑎</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.4.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.4">𝑚</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.5.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.5">𝑒</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.6.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.6">𝑟</ci><ci id="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.7.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.2.2.2.2.2.2.2.7">𝑎</ci></apply></interval></apply></interval></apply><cn type="integer" id="S3.SS1.SSS4.p1.3.m3.2.2.2.4.cmml" xref="S3.SS1.SSS4.p1.3.m3.2.2.2.4">15</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.3.m3.2c">\theta=atan2(pivot_{z}-cam_{z},d(pivot,camera))+15</annotation></semantics></math> degrees, where <math id="S3.SS1.SSS4.p1.4.m4.2" class="ltx_Math" alttext="d(pivot,camera)" display="inline"><semantics id="S3.SS1.SSS4.p1.4.m4.2a"><mrow id="S3.SS1.SSS4.p1.4.m4.2.2" xref="S3.SS1.SSS4.p1.4.m4.2.2.cmml"><mi id="S3.SS1.SSS4.p1.4.m4.2.2.4" xref="S3.SS1.SSS4.p1.4.m4.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.2.2.3" xref="S3.SS1.SSS4.p1.4.m4.2.2.3.cmml">​</mo><mrow id="S3.SS1.SSS4.p1.4.m4.2.2.2.2" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.3" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.3.cmml">(</mo><mrow id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.2" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.3" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1a" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.4" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.4.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1b" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.5" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1c" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.6" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.6.cmml">t</mi></mrow><mo id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.4" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.3.cmml">,</mo><mrow id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.cmml"><mi id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.2" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.3" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1a" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.4" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1b" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.5" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1c" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.6" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1d" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1.cmml">​</mo><mi id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.7" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.7.cmml">a</mi></mrow><mo stretchy="false" id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.5" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.4.m4.2b"><apply id="S3.SS1.SSS4.p1.4.m4.2.2.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2"><times id="S3.SS1.SSS4.p1.4.m4.2.2.3.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.3"></times><ci id="S3.SS1.SSS4.p1.4.m4.2.2.4.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.4">𝑑</ci><interval closure="open" id="S3.SS1.SSS4.p1.4.m4.2.2.2.3.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2"><apply id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1"><times id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.1"></times><ci id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.2">𝑝</ci><ci id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.3">𝑖</ci><ci id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.4.cmml" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.4">𝑣</ci><ci id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.5.cmml" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.5">𝑜</ci><ci id="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.6.cmml" xref="S3.SS1.SSS4.p1.4.m4.1.1.1.1.1.6">𝑡</ci></apply><apply id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2"><times id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.1"></times><ci id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.2">𝑐</ci><ci id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.3">𝑎</ci><ci id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.4.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.4">𝑚</ci><ci id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.5.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.5">𝑒</ci><ci id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.6.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.6">𝑟</ci><ci id="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.7.cmml" xref="S3.SS1.SSS4.p1.4.m4.2.2.2.2.2.7">𝑎</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.4.m4.2c">d(pivot,camera)</annotation></semantics></math> is the distance in the x, y plane. The modifications applied to this methodology during the second image-generation procedure are as follows: the x and y positions are set to be within 5 meters of the virtual bounding box containing all zebras, the yaw has an additional <math id="S3.SS1.SSS4.p1.5.m5.2" class="ltx_Math" alttext="[-15,15]" display="inline"><semantics id="S3.SS1.SSS4.p1.5.m5.2a"><mrow id="S3.SS1.SSS4.p1.5.m5.2.2.1" xref="S3.SS1.SSS4.p1.5.m5.2.2.2.cmml"><mo stretchy="false" id="S3.SS1.SSS4.p1.5.m5.2.2.1.2" xref="S3.SS1.SSS4.p1.5.m5.2.2.2.cmml">[</mo><mrow id="S3.SS1.SSS4.p1.5.m5.2.2.1.1" xref="S3.SS1.SSS4.p1.5.m5.2.2.1.1.cmml"><mo id="S3.SS1.SSS4.p1.5.m5.2.2.1.1a" xref="S3.SS1.SSS4.p1.5.m5.2.2.1.1.cmml">−</mo><mn id="S3.SS1.SSS4.p1.5.m5.2.2.1.1.2" xref="S3.SS1.SSS4.p1.5.m5.2.2.1.1.2.cmml">15</mn></mrow><mo id="S3.SS1.SSS4.p1.5.m5.2.2.1.3" xref="S3.SS1.SSS4.p1.5.m5.2.2.2.cmml">,</mo><mn id="S3.SS1.SSS4.p1.5.m5.1.1" xref="S3.SS1.SSS4.p1.5.m5.1.1.cmml">15</mn><mo stretchy="false" id="S3.SS1.SSS4.p1.5.m5.2.2.1.4" xref="S3.SS1.SSS4.p1.5.m5.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.5.m5.2b"><interval closure="closed" id="S3.SS1.SSS4.p1.5.m5.2.2.2.cmml" xref="S3.SS1.SSS4.p1.5.m5.2.2.1"><apply id="S3.SS1.SSS4.p1.5.m5.2.2.1.1.cmml" xref="S3.SS1.SSS4.p1.5.m5.2.2.1.1"><minus id="S3.SS1.SSS4.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.SSS4.p1.5.m5.2.2.1.1"></minus><cn type="integer" id="S3.SS1.SSS4.p1.5.m5.2.2.1.1.2.cmml" xref="S3.SS1.SSS4.p1.5.m5.2.2.1.1.2">15</cn></apply><cn type="integer" id="S3.SS1.SSS4.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS4.p1.5.m5.1.1">15</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.5.m5.2c">[-15,15]</annotation></semantics></math> degrees component instead of the <math id="S3.SS1.SSS4.p1.6.m6.2" class="ltx_Math" alttext="[-30,30]" display="inline"><semantics id="S3.SS1.SSS4.p1.6.m6.2a"><mrow id="S3.SS1.SSS4.p1.6.m6.2.2.1" xref="S3.SS1.SSS4.p1.6.m6.2.2.2.cmml"><mo stretchy="false" id="S3.SS1.SSS4.p1.6.m6.2.2.1.2" xref="S3.SS1.SSS4.p1.6.m6.2.2.2.cmml">[</mo><mrow id="S3.SS1.SSS4.p1.6.m6.2.2.1.1" xref="S3.SS1.SSS4.p1.6.m6.2.2.1.1.cmml"><mo id="S3.SS1.SSS4.p1.6.m6.2.2.1.1a" xref="S3.SS1.SSS4.p1.6.m6.2.2.1.1.cmml">−</mo><mn id="S3.SS1.SSS4.p1.6.m6.2.2.1.1.2" xref="S3.SS1.SSS4.p1.6.m6.2.2.1.1.2.cmml">30</mn></mrow><mo id="S3.SS1.SSS4.p1.6.m6.2.2.1.3" xref="S3.SS1.SSS4.p1.6.m6.2.2.2.cmml">,</mo><mn id="S3.SS1.SSS4.p1.6.m6.1.1" xref="S3.SS1.SSS4.p1.6.m6.1.1.cmml">30</mn><mo stretchy="false" id="S3.SS1.SSS4.p1.6.m6.2.2.1.4" xref="S3.SS1.SSS4.p1.6.m6.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p1.6.m6.2b"><interval closure="closed" id="S3.SS1.SSS4.p1.6.m6.2.2.2.cmml" xref="S3.SS1.SSS4.p1.6.m6.2.2.1"><apply id="S3.SS1.SSS4.p1.6.m6.2.2.1.1.cmml" xref="S3.SS1.SSS4.p1.6.m6.2.2.1.1"><minus id="S3.SS1.SSS4.p1.6.m6.2.2.1.1.1.cmml" xref="S3.SS1.SSS4.p1.6.m6.2.2.1.1"></minus><cn type="integer" id="S3.SS1.SSS4.p1.6.m6.2.2.1.1.2.cmml" xref="S3.SS1.SSS4.p1.6.m6.2.2.1.1.2">30</cn></apply><cn type="integer" id="S3.SS1.SSS4.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS4.p1.6.m6.1.1">30</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p1.6.m6.2c">[-30,30]</annotation></semantics></math>. This results in images that are closer to the zebras than the ones obtained from the former camera placement strategy.</p>
</div>
<div id="S3.SS1.SSS4.p2" class="ltx_para">
<p id="S3.SS1.SSS4.p2.1" class="ltx_p">For each environment, we randomly place the zebras 200 times, resulting in 600 frames per experiment per camera, i.e. 1.8K frames in total. After the generation process is complete for all ten environments, this totals to 18K frames captured with the first camera placement strategy, and 18K with the camera set to be more nearby, totalling 36K frames. For each frame, we save the pose of the cameras, zebra skeletal pose and meshes vertices, ground truth depth and instance segmentation.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Real-world data</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">We performed several data collection experiments in a controlled scenario within the Wilhlema Zoo in Stuttgart (DE). An example of the collected data can be found in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-B Real-world data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We used two manually flown DJI Mavic 2 Pro drones, recording images at 29.97 fps at a resolution of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="3840\times 2160" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">3840</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">2160</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">3840</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">2160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">3840\times 2160</annotation></semantics></math>, and three GoPro Hero8, also with a resolution of <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="3840\times 2160" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">3840</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">2160</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">3840</cn><cn type="integer" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">2160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">3840\times 2160</annotation></semantics></math> at 59.94 fps. None of the GoPros had fixed locations between experiments. The data has been manually synchronized by using a recorded light signal visible by all cameras at the same time. We then extracted one frame every five seconds from all the videos. Out of these, 905 images were randomly selected and annotated manually and precisely. These annotations were then used to train an SSD multibox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> detector, which, with <span id="S3.SS2.p1.2.1" class="ltx_text ltx_font_italic">Smarter-Labelme</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, allowed us to obtain bounding boxes on our video sequences with ease. Out of all the data available, we finally selected three collections during which the zebras were visible by both drones. The boxes on those sequences were then manually refined in a final step. This procedure thus resulted in 905 precisely annotated images, and 104K frames annotated with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Within this work, we release the data used during our training experiments, i.e. the 905 precisely annotated images and 200 automatically labelled ones from one of the experiments (see Sec. <a href="#S4" title="IV EXPERIMENT AND EVALUATIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> for details).</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.F5.2" class="ltx_block ltx_figure_panel">
<figure id="S3.F5.sf1" class="ltx_figure"><img src="/html/2305.00432/assets/fig/figure_1407.jpg" id="S3.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="296" height="197" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">R1</span></figcaption>
</figure>
<figure id="S3.F5.sf2" class="ltx_figure"><img src="/html/2305.00432/assets/fig/figure_2107.jpg" id="S3.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="296" height="197" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">R2</span></figcaption>
</figure>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.F5.3" class="ltx_block ltx_figure_panel">
<figure id="S3.F5.sf3" class="ltx_figure"><img src="/html/2305.00432/assets/fig/figure_1606.jpg" id="S3.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="296" height="197" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">R3</span></figcaption>
</figure>
<figure id="S3.F5.sf4" class="ltx_figure"><img src="/html/2305.00432/assets/fig/figure_prec_ground.jpg" id="S3.F5.sf4.g1" class="ltx_graphics ltx_img_landscape" width="296" height="197" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F5.sf4.3.2" class="ltx_text" style="font-size:90%;">RP</span></figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.5.2" class="ltx_text" style="font-size:90%;">An example of our real-world collected images used for testing. Three aerial and one ‘ground-level’ views. Best viewed in color.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">EXPERIMENT AND EVALUATIONS</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Here we seek to demonstrate that the synthetic data generated by our method can be used effectively for a vision-based task which is highly related to image features and context, such as the detection of zebras in outdoor wild environments from an aerial point of view. Our hypothesis is that, by training a model using only synthetic data acquired in a realistic simulation environment, we can achieve detection performance on real images comparable to a model trained on a manually and very-precisely labelled set of real images. Our goal is to prove that synthetically generated data <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">alone</span> can be used to train a network capable of detecting zebras with high accuracy in real-world images. To that end, we decided to perform various tests on YOLOv5s. We train the networks from scratch and with mixed datasets to test the performance and provide a complete overview. All the training runs are made from scratch with the default hyperparameters and for the standard 300 epochs. We do not introduce any additional data augmentation technique different from the one applied by default by the YOLOv5 code. This consists of some randomization in the scale, horizontal flip, translation and HSV colour space factors. We do not modify these values to have a fairer comparison across the models that would not require parameter grid searches or other steps when compared to the baseline pre-trained model. We save the best model, as evaluated on the specific validation set, and compare it over multiple datasets. We evaluate the performance with the COCO standard metric (mAP@[.5, .95], AP in this work) and the PASCAL VOC’s metric (mAP@.5, AP50 in this work). We also report the average and weighted average of these two metrics. We weigh based on the cardinality of each one of the evaluated datasets. With these comparisons, we demonstrate that, with our synthetic data, we can successfully capture real-world features. This, while also obtaining trained models which show, in general, improved performance when compared to the pre-trained ones.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Synthetic Full dataset (SF) is the dataset containing all the 36K synthetically generated images. These are then randomly shuffled and split into 80/20 train/validation sets. Synthetic Closeby (SC) is the synthetic data generated only by the second strategy, as described in Sec. <a href="#S3.SS1.SSS4" title="III-A4 Data collection methodologies ‣ III-A Synthetic data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>4</span></a>, i.e. 18K images for which the camera is within 5 meters of the bounding box containing all the zebras. This data is also divided randomly with an 80/20 ratio. With COCO we refer to the images of the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> which contains zebras, i.e. 1916 training and 85 validation examples. Due to the small size of the validation set of the COCO dataset, we do not perform any training on this data alone. With APT-36K we indicate the set of images from the APT-36K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> dataset which contains zebras, i.e. 1.2K samples. We then have, R1, R2, and R3 which are three sets of real-world data which is not precisely labelled, as described in Sec. <a href="#S3.SS2" title="III-B Real-world data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>. To distinguish between which drone captured the given sequence, we use the suffixes <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">_D1</span> and <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">_D2</span>. R1 consists of 19.7K images, R2 of 23.4K, and R3 of 8.8K, for each drone. Finally, we use RP to indicate the set of the 905 real-world images precisely labelled by us, sampled from representative images from the previous Rx datasets and additional images captured with the GoPros as described in Sec. <a href="#S3.SS2" title="III-B Real-world data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>. Of them, 720 are randomly used in training and 185 for validation. An example of the bounding boxes of our real-world data is provided in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-B Real-world data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We also provide two zoomed-in examples of imprecise labels in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV EXPERIMENT AND EVALUATIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Note that also other datasets, e.g. COCO (see Fig. <a href="#S2.F3" title="Figure 3 ‣ II Related Work ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), present such approximations.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:254.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(12.6pt,-7.4pt) scale(1.06196089809339,1.06196089809339) ;">
<table id="S4.T2.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.7.7.8.1" class="ltx_tr">
<th id="S4.T2.7.7.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Dataset</th>
<td id="S4.T2.7.7.8.1.2" class="ltx_td ltx_align_center ltx_border_r">Description</td>
<td id="S4.T2.7.7.8.1.3" class="ltx_td ltx_align_center ltx_border_r">Train imgs.</td>
<td id="S4.T2.7.7.8.1.4" class="ltx_td ltx_align_center">Val imgs.</td>
</tr>
<tr id="S4.T2.7.7.9.2" class="ltx_tr">
<th id="S4.T2.7.7.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SF</th>
<td id="S4.T2.7.7.9.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Our full synthetic data</td>
<td id="S4.T2.7.7.9.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29K</td>
<td id="S4.T2.7.7.9.2.4" class="ltx_td ltx_align_center ltx_border_t">7K</td>
</tr>
<tr id="S4.T2.7.7.10.3" class="ltx_tr">
<th id="S4.T2.7.7.10.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SC</th>
<td id="S4.T2.7.7.10.3.2" class="ltx_td ltx_align_center ltx_border_r">
<table id="S4.T2.7.7.10.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.7.7.10.3.2.1.1" class="ltx_tr">
<td id="S4.T2.7.7.10.3.2.1.1.1" class="ltx_td ltx_align_center">A subset of our synthetic data</td>
</tr>
<tr id="S4.T2.7.7.10.3.2.1.2" class="ltx_tr">
<td id="S4.T2.7.7.10.3.2.1.2.1" class="ltx_td ltx_align_center">focused on camera poses closer to the zebras</td>
</tr>
</table>
</td>
<td id="S4.T2.7.7.10.3.3" class="ltx_td ltx_align_center ltx_border_r">14.5K</td>
<td id="S4.T2.7.7.10.3.4" class="ltx_td ltx_align_center">3.5K</td>
</tr>
<tr id="S4.T2.7.7.11.4" class="ltx_tr">
<th id="S4.T2.7.7.11.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R1</th>
<td id="S4.T2.7.7.11.4.2" class="ltx_td ltx_align_center ltx_border_r">Aerial capture experiment 1</td>
<td id="S4.T2.7.7.11.4.3" class="ltx_td ltx_align_center ltx_border_r">—</td>
<td id="S4.T2.7.7.11.4.4" class="ltx_td ltx_align_center">19.7K</td>
</tr>
<tr id="S4.T2.7.7.12.5" class="ltx_tr">
<th id="S4.T2.7.7.12.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R2</th>
<td id="S4.T2.7.7.12.5.2" class="ltx_td ltx_align_center ltx_border_r">Aerial capture experiment 2</td>
<td id="S4.T2.7.7.12.5.3" class="ltx_td ltx_align_center ltx_border_r">—</td>
<td id="S4.T2.7.7.12.5.4" class="ltx_td ltx_align_center">23.4K</td>
</tr>
<tr id="S4.T2.7.7.13.6" class="ltx_tr">
<th id="S4.T2.7.7.13.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R3</th>
<td id="S4.T2.7.7.13.6.2" class="ltx_td ltx_align_center ltx_border_r">Aerial capture experiment 3</td>
<td id="S4.T2.7.7.13.6.3" class="ltx_td ltx_align_center ltx_border_r">—</td>
<td id="S4.T2.7.7.13.6.4" class="ltx_td ltx_align_center">8.8K</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">R3<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{100}}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><msub id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1a" xref="S4.T2.1.1.1.1.m1.1.1.cmml"></mi><mtext id="S4.T2.1.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.1a.cmml">100</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><ci id="S4.T2.1.1.1.1.m1.1.1.1a.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1">100</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">{}_{\text{100}}</annotation></semantics></math>
</th>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">100<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><times id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\times</annotation></semantics></math>2 images randomly chosen from R3</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">100</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center">100</td>
</tr>
<tr id="S4.T2.7.7.14.7" class="ltx_tr">
<th id="S4.T2.7.7.14.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RP</th>
<td id="S4.T2.7.7.14.7.2" class="ltx_td ltx_align_center ltx_border_r">Preciselly labelled images from R1/2/3 and GoPros</td>
<td id="S4.T2.7.7.14.7.3" class="ltx_td ltx_align_center ltx_border_r">720</td>
<td id="S4.T2.7.7.14.7.4" class="ltx_td ltx_align_center">185</td>
</tr>
<tr id="S4.T2.7.7.15.8" class="ltx_tr">
<th id="S4.T2.7.7.15.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">COCO</th>
<td id="S4.T2.7.7.15.8.2" class="ltx_td ltx_align_center ltx_border_r">COCO-zebras</td>
<td id="S4.T2.7.7.15.8.3" class="ltx_td ltx_align_center ltx_border_r">1916</td>
<td id="S4.T2.7.7.15.8.4" class="ltx_td ltx_align_center">85</td>
</tr>
<tr id="S4.T2.7.7.16.9" class="ltx_tr">
<th id="S4.T2.7.7.16.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Rx_D1,2</th>
<td id="S4.T2.7.7.16.9.2" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Either 1st or 2nd drone capturing during experiment Rx</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<math id="S4.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><ci id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\ast</annotation></semantics></math>-1920</th>
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_center" colspan="3">Same dataset but using 1920 image size during training</td>
</tr>
<tr id="S4.T2.7.7.7" class="ltx_tr">
<th id="S4.T2.5.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<math id="S4.T2.4.4.4.1.m1.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="S4.T2.4.4.4.1.m1.1a"><mo id="S4.T2.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><ci id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">\ast</annotation></semantics></math> + <math id="S4.T2.5.5.5.2.m2.1" class="ltx_Math" alttext="\diamond" display="inline"><semantics id="S4.T2.5.5.5.2.m2.1a"><mo id="S4.T2.5.5.5.2.m2.1.1" xref="S4.T2.5.5.5.2.m2.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.2.m2.1b"><ci id="S4.T2.5.5.5.2.m2.1.1.cmml" xref="S4.T2.5.5.5.2.m2.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.2.m2.1c">\diamond</annotation></semantics></math>
</th>
<td id="S4.T2.7.7.7.4" class="ltx_td ltx_align_center" colspan="3">Trained by merging <math id="S4.T2.6.6.6.3.m1.1" class="ltx_Math" alttext="\ast" display="inline"><semantics id="S4.T2.6.6.6.3.m1.1a"><mo id="S4.T2.6.6.6.3.m1.1.1" xref="S4.T2.6.6.6.3.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.3.m1.1b"><ci id="S4.T2.6.6.6.3.m1.1.1.cmml" xref="S4.T2.6.6.6.3.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.3.m1.1c">\ast</annotation></semantics></math> and <math id="S4.T2.7.7.7.4.m2.1" class="ltx_Math" alttext="\diamond" display="inline"><semantics id="S4.T2.7.7.7.4.m2.1a"><mo id="S4.T2.7.7.7.4.m2.1.1" xref="S4.T2.7.7.7.4.m2.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.4.m2.1b"><ci id="S4.T2.7.7.7.4.m2.1.1.cmml" xref="S4.T2.7.7.7.4.m2.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.4.m2.1c">\diamond</annotation></semantics></math> corresponding train and validation sets</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.9.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.10.2" class="ltx_text" style="font-size:90%;">Zebras datasets legend and training/validation sizes</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.00432/assets/fig/uprecise1.jpg" id="S4.F6.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="144" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.00432/assets/fig/uprecise2.jpg" id="S4.F6.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="144" height="110" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Two zoomed-in examples of imprecisely labelled data. The bounding boxes can either be slightly too loose or too tight on the zebras.</span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:112.2pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-237.5pt,61.2pt) scale(0.477241688501694,0.477241688501694) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r" colspan="2">APT-36K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r" colspan="2">R1_D1</td>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r" colspan="2">R1_D2</td>
<td id="S4.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r" colspan="2">R2_D1</td>
<td id="S4.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r" colspan="2">R2_D2</td>
<td id="S4.T3.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r" colspan="2">R3_D1</td>
<td id="S4.T3.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r" colspan="2">R3_D2</td>
<td id="S4.T3.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_r" colspan="2">RP (validation)</td>
<td id="S4.T3.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_r" colspan="2">Weigthed avg.</td>
<td id="S4.T3.1.1.2.1.11" class="ltx_td ltx_align_center" colspan="2">Avg.</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<th id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Training Dataset</th>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.12" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.14" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.16" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.18" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.19" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">mAP</td>
<td id="S4.T3.1.1.3.2.20" class="ltx_td ltx_align_center ltx_border_t">mAP50</td>
<td id="S4.T3.1.1.3.2.21" class="ltx_td ltx_align_center ltx_border_t">mAP</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<th id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SF</th>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">0.072</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.029</td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">0.770</td>
<td id="S4.T3.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.488</td>
<td id="S4.T3.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t">0.756</td>
<td id="S4.T3.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.490</td>
<td id="S4.T3.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">0.224</td>
<td id="S4.T3.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.130</td>
<td id="S4.T3.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t">0.597</td>
<td id="S4.T3.1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.393</td>
<td id="S4.T3.1.1.4.3.12" class="ltx_td ltx_align_center ltx_border_t">0.142</td>
<td id="S4.T3.1.1.4.3.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.092</td>
<td id="S4.T3.1.1.4.3.14" class="ltx_td ltx_align_center ltx_border_t">0.203</td>
<td id="S4.T3.1.1.4.3.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.127</td>
<td id="S4.T3.1.1.4.3.16" class="ltx_td ltx_align_center ltx_border_t">0.104</td>
<td id="S4.T3.1.1.4.3.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.074</td>
<td id="S4.T3.1.1.4.3.18" class="ltx_td ltx_align_center ltx_border_t">0.498</td>
<td id="S4.T3.1.1.4.3.19" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.318</td>
<td id="S4.T3.1.1.4.3.20" class="ltx_td ltx_align_center ltx_border_t">0.359</td>
<td id="S4.T3.1.1.4.3.21" class="ltx_td ltx_align_center ltx_border_t">0.228</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<th id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SF-1920</th>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center">0.103</td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.055</td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center">0.853</td>
<td id="S4.T3.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">0.568</td>
<td id="S4.T3.1.1.5.4.6" class="ltx_td ltx_align_center">0.958</td>
<td id="S4.T3.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r">0.646</td>
<td id="S4.T3.1.1.5.4.8" class="ltx_td ltx_align_center">0.873</td>
<td id="S4.T3.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_r">0.540</td>
<td id="S4.T3.1.1.5.4.10" class="ltx_td ltx_align_center">0.957</td>
<td id="S4.T3.1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_r">0.616</td>
<td id="S4.T3.1.1.5.4.12" class="ltx_td ltx_align_center">0.608</td>
<td id="S4.T3.1.1.5.4.13" class="ltx_td ltx_align_center ltx_border_r">0.443</td>
<td id="S4.T3.1.1.5.4.14" class="ltx_td ltx_align_center">0.651</td>
<td id="S4.T3.1.1.5.4.15" class="ltx_td ltx_align_center ltx_border_r">0.484</td>
<td id="S4.T3.1.1.5.4.16" class="ltx_td ltx_align_center">0.287</td>
<td id="S4.T3.1.1.5.4.17" class="ltx_td ltx_align_center ltx_border_r">0.191</td>
<td id="S4.T3.1.1.5.4.18" class="ltx_td ltx_align_center">0.853</td>
<td id="S4.T3.1.1.5.4.19" class="ltx_td ltx_align_center ltx_border_r">0.563</td>
<td id="S4.T3.1.1.5.4.20" class="ltx_td ltx_align_center">0.661</td>
<td id="S4.T3.1.1.5.4.21" class="ltx_td ltx_align_center">0.443</td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<th id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SC</th>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center">0.121</td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">0.046</td>
<td id="S4.T3.1.1.6.5.4" class="ltx_td ltx_align_center">0.714</td>
<td id="S4.T3.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">0.455</td>
<td id="S4.T3.1.1.6.5.6" class="ltx_td ltx_align_center">0.830</td>
<td id="S4.T3.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r">0.529</td>
<td id="S4.T3.1.1.6.5.8" class="ltx_td ltx_align_center">0.147</td>
<td id="S4.T3.1.1.6.5.9" class="ltx_td ltx_align_center ltx_border_r">0.084</td>
<td id="S4.T3.1.1.6.5.10" class="ltx_td ltx_align_center">0.513</td>
<td id="S4.T3.1.1.6.5.11" class="ltx_td ltx_align_center ltx_border_r">0.316</td>
<td id="S4.T3.1.1.6.5.12" class="ltx_td ltx_align_center">0.156</td>
<td id="S4.T3.1.1.6.5.13" class="ltx_td ltx_align_center ltx_border_r">0.097</td>
<td id="S4.T3.1.1.6.5.14" class="ltx_td ltx_align_center">0.375</td>
<td id="S4.T3.1.1.6.5.15" class="ltx_td ltx_align_center ltx_border_r">0.202</td>
<td id="S4.T3.1.1.6.5.16" class="ltx_td ltx_align_center">0.092</td>
<td id="S4.T3.1.1.6.5.17" class="ltx_td ltx_align_center ltx_border_r">0.061</td>
<td id="S4.T3.1.1.6.5.18" class="ltx_td ltx_align_center">0.482</td>
<td id="S4.T3.1.1.6.5.19" class="ltx_td ltx_align_center ltx_border_r">0.299</td>
<td id="S4.T3.1.1.6.5.20" class="ltx_td ltx_align_center">0.369</td>
<td id="S4.T3.1.1.6.5.21" class="ltx_td ltx_align_center">0.224</td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<th id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SC-1920</th>
<td id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_center">0.150</td>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">0.053</td>
<td id="S4.T3.1.1.7.6.4" class="ltx_td ltx_align_center">0.907</td>
<td id="S4.T3.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r">0.605</td>
<td id="S4.T3.1.1.7.6.6" class="ltx_td ltx_align_center">0.971</td>
<td id="S4.T3.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r">0.664</td>
<td id="S4.T3.1.1.7.6.8" class="ltx_td ltx_align_center">0.939</td>
<td id="S4.T3.1.1.7.6.9" class="ltx_td ltx_align_center ltx_border_r">0.593</td>
<td id="S4.T3.1.1.7.6.10" class="ltx_td ltx_align_center">0.968</td>
<td id="S4.T3.1.1.7.6.11" class="ltx_td ltx_align_center ltx_border_r">0.652</td>
<td id="S4.T3.1.1.7.6.12" class="ltx_td ltx_align_center">0.649</td>
<td id="S4.T3.1.1.7.6.13" class="ltx_td ltx_align_center ltx_border_r">0.476</td>
<td id="S4.T3.1.1.7.6.14" class="ltx_td ltx_align_center">0.819</td>
<td id="S4.T3.1.1.7.6.15" class="ltx_td ltx_align_center ltx_border_r">0.580</td>
<td id="S4.T3.1.1.7.6.16" class="ltx_td ltx_align_center">0.331</td>
<td id="S4.T3.1.1.7.6.17" class="ltx_td ltx_align_center ltx_border_r">0.228</td>
<td id="S4.T3.1.1.7.6.18" class="ltx_td ltx_align_center">0.901</td>
<td id="S4.T3.1.1.7.6.19" class="ltx_td ltx_align_center ltx_border_r">0.604</td>
<td id="S4.T3.1.1.7.6.20" class="ltx_td ltx_align_center">0.717</td>
<td id="S4.T3.1.1.7.6.21" class="ltx_td ltx_align_center">0.481</td>
</tr>
<tr id="S4.T3.1.1.8.7" class="ltx_tr">
<th id="S4.T3.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RP</th>
<td id="S4.T3.1.1.8.7.2" class="ltx_td ltx_align_center">0.260</td>
<td id="S4.T3.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">0.092</td>
<td id="S4.T3.1.1.8.7.4" class="ltx_td ltx_align_center">0.865</td>
<td id="S4.T3.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r">0.487</td>
<td id="S4.T3.1.1.8.7.6" class="ltx_td ltx_align_center">0.935</td>
<td id="S4.T3.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r">0.550</td>
<td id="S4.T3.1.1.8.7.8" class="ltx_td ltx_align_center">0.808</td>
<td id="S4.T3.1.1.8.7.9" class="ltx_td ltx_align_center ltx_border_r">0.479</td>
<td id="S4.T3.1.1.8.7.10" class="ltx_td ltx_align_center">0.946</td>
<td id="S4.T3.1.1.8.7.11" class="ltx_td ltx_align_center ltx_border_r">0.593</td>
<td id="S4.T3.1.1.8.7.12" class="ltx_td ltx_align_center">0.772</td>
<td id="S4.T3.1.1.8.7.13" class="ltx_td ltx_align_center ltx_border_r">0.380</td>
<td id="S4.T3.1.1.8.7.14" class="ltx_td ltx_align_center">0.922</td>
<td id="S4.T3.1.1.8.7.15" class="ltx_td ltx_align_center ltx_border_r">0.548</td>
<td id="S4.T3.1.1.8.7.16" class="ltx_td ltx_align_center">0.805</td>
<td id="S4.T3.1.1.8.7.17" class="ltx_td ltx_align_center ltx_border_r">0.453</td>
<td id="S4.T3.1.1.8.7.18" class="ltx_td ltx_align_center">0.873</td>
<td id="S4.T3.1.1.8.7.19" class="ltx_td ltx_align_center ltx_border_r">0.512</td>
<td id="S4.T3.1.1.8.7.20" class="ltx_td ltx_align_center">0.789</td>
<td id="S4.T3.1.1.8.7.21" class="ltx_td ltx_align_center">0.448</td>
</tr>
<tr id="S4.T3.1.1.9.8" class="ltx_tr">
<th id="S4.T3.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RP-1920</th>
<td id="S4.T3.1.1.9.8.2" class="ltx_td ltx_align_center">0.161</td>
<td id="S4.T3.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">0.066</td>
<td id="S4.T3.1.1.9.8.4" class="ltx_td ltx_align_center">0.937</td>
<td id="S4.T3.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r">0.615</td>
<td id="S4.T3.1.1.9.8.6" class="ltx_td ltx_align_center">0.980</td>
<td id="S4.T3.1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r">0.663</td>
<td id="S4.T3.1.1.9.8.8" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.9.8.8.1" class="ltx_text ltx_font_bold">0.989</span></td>
<td id="S4.T3.1.1.9.8.9" class="ltx_td ltx_align_center ltx_border_r">0.653</td>
<td id="S4.T3.1.1.9.8.10" class="ltx_td ltx_align_center">0.982</td>
<td id="S4.T3.1.1.9.8.11" class="ltx_td ltx_align_center ltx_border_r">0.666</td>
<td id="S4.T3.1.1.9.8.12" class="ltx_td ltx_align_center">0.801</td>
<td id="S4.T3.1.1.9.8.13" class="ltx_td ltx_align_center ltx_border_r">0.532</td>
<td id="S4.T3.1.1.9.8.14" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.9.8.14.1" class="ltx_text ltx_font_bold">0.986</span></td>
<td id="S4.T3.1.1.9.8.15" class="ltx_td ltx_align_center ltx_border_r">0.680</td>
<td id="S4.T3.1.1.9.8.16" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.9.8.16.1" class="ltx_text ltx_font_bold">0.914</span></td>
<td id="S4.T3.1.1.9.8.17" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.9.8.17.1" class="ltx_text ltx_font_bold">0.636</span></td>
<td id="S4.T3.1.1.9.8.18" class="ltx_td ltx_align_center">0.950</td>
<td id="S4.T3.1.1.9.8.19" class="ltx_td ltx_align_center ltx_border_r">0.636</td>
<td id="S4.T3.1.1.9.8.20" class="ltx_td ltx_align_center">0.844</td>
<td id="S4.T3.1.1.9.8.21" class="ltx_td ltx_align_center">0.564</td>
</tr>
<tr id="S4.T3.1.1.10.9" class="ltx_tr">
<th id="S4.T3.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SC+COCO-1920</th>
<td id="S4.T3.1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_t">0.709</td>
<td id="S4.T3.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.386</td>
<td id="S4.T3.1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_t">0.943</td>
<td id="S4.T3.1.1.10.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.624</td>
<td id="S4.T3.1.1.10.9.6" class="ltx_td ltx_align_center ltx_border_t">0.976</td>
<td id="S4.T3.1.1.10.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.676</td>
<td id="S4.T3.1.1.10.9.8" class="ltx_td ltx_align_center ltx_border_t">0.932</td>
<td id="S4.T3.1.1.10.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.599</td>
<td id="S4.T3.1.1.10.9.10" class="ltx_td ltx_align_center ltx_border_t">0.977</td>
<td id="S4.T3.1.1.10.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.659</td>
<td id="S4.T3.1.1.10.9.12" class="ltx_td ltx_align_center ltx_border_t">0.637</td>
<td id="S4.T3.1.1.10.9.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.481</td>
<td id="S4.T3.1.1.10.9.14" class="ltx_td ltx_align_center ltx_border_t">0.867</td>
<td id="S4.T3.1.1.10.9.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.635</td>
<td id="S4.T3.1.1.10.9.16" class="ltx_td ltx_align_center ltx_border_t">0.350</td>
<td id="S4.T3.1.1.10.9.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.253</td>
<td id="S4.T3.1.1.10.9.18" class="ltx_td ltx_align_center ltx_border_t">0.918</td>
<td id="S4.T3.1.1.10.9.19" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.621</td>
<td id="S4.T3.1.1.10.9.20" class="ltx_td ltx_align_center ltx_border_t">0.799</td>
<td id="S4.T3.1.1.10.9.21" class="ltx_td ltx_align_center ltx_border_t">0.539</td>
</tr>
<tr id="S4.T3.1.1.11.10" class="ltx_tr">
<th id="S4.T3.1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RP+COCO-1920</th>
<td id="S4.T3.1.1.11.10.2" class="ltx_td ltx_align_center">0.837</td>
<td id="S4.T3.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r">0.526</td>
<td id="S4.T3.1.1.11.10.4" class="ltx_td ltx_align_center">0.967</td>
<td id="S4.T3.1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_r">0.639</td>
<td id="S4.T3.1.1.11.10.6" class="ltx_td ltx_align_center">0.984</td>
<td id="S4.T3.1.1.11.10.7" class="ltx_td ltx_align_center ltx_border_r">0.681</td>
<td id="S4.T3.1.1.11.10.8" class="ltx_td ltx_align_center">0.980</td>
<td id="S4.T3.1.1.11.10.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.11.10.9.1" class="ltx_text ltx_font_bold">0.656</span></td>
<td id="S4.T3.1.1.11.10.10" class="ltx_td ltx_align_center">0.968</td>
<td id="S4.T3.1.1.11.10.11" class="ltx_td ltx_align_center ltx_border_r">0.684</td>
<td id="S4.T3.1.1.11.10.12" class="ltx_td ltx_align_center">0.768</td>
<td id="S4.T3.1.1.11.10.13" class="ltx_td ltx_align_center ltx_border_r">0.493</td>
<td id="S4.T3.1.1.11.10.14" class="ltx_td ltx_align_center">0.981</td>
<td id="S4.T3.1.1.11.10.15" class="ltx_td ltx_align_center ltx_border_r">0.674</td>
<td id="S4.T3.1.1.11.10.16" class="ltx_td ltx_align_center">0.911</td>
<td id="S4.T3.1.1.11.10.17" class="ltx_td ltx_align_center ltx_border_r">0.626</td>
<td id="S4.T3.1.1.11.10.18" class="ltx_td ltx_align_center">0.956</td>
<td id="S4.T3.1.1.11.10.19" class="ltx_td ltx_align_center ltx_border_r">0.650</td>
<td id="S4.T3.1.1.11.10.20" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.11.10.20.1" class="ltx_text ltx_font_bold">0.925</span></td>
<td id="S4.T3.1.1.11.10.21" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.11.10.21.1" class="ltx_text ltx_font_bold">0.622</span></td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SC+COCO+<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\text{R3}_{100}" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><mtext id="S4.T3.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.m1.1.1.2a.cmml">R3</mtext><mn id="S4.T3.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.m1.1.1.3.cmml">100</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.1.1.1.1.m1.1.1.2a.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2"><mtext id="S4.T3.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2">R3</mtext></ci><cn type="integer" id="S4.T3.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\text{R3}_{100}</annotation></semantics></math>-1920</th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center">0.704</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">0.378</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">0.975</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">0.655</span></td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">0.994</span></td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.1.7.1" class="ltx_text ltx_font_bold">0.707</span></td>
<td id="S4.T3.1.1.1.8" class="ltx_td ltx_align_center">0.963</td>
<td id="S4.T3.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r">0.636</td>
<td id="S4.T3.1.1.1.10" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.10.1" class="ltx_text ltx_font_bold">0.991</span></td>
<td id="S4.T3.1.1.1.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.1.11.1" class="ltx_text ltx_font_bold">0.691</span></td>
<td id="S4.T3.1.1.1.12" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.12.1" class="ltx_text ltx_font_bold">0.986</span></td>
<td id="S4.T3.1.1.1.13" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.1.13.1" class="ltx_text ltx_font_bold">0.733</span></td>
<td id="S4.T3.1.1.1.14" class="ltx_td ltx_align_center">0.961</td>
<td id="S4.T3.1.1.1.15" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.1.15.1" class="ltx_text ltx_font_bold">0.708</span></td>
<td id="S4.T3.1.1.1.16" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.16.1" class="ltx_text ltx_framed ltx_framed_underline">0.432</span></td>
<td id="S4.T3.1.1.1.17" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.1.17.1" class="ltx_text ltx_framed ltx_framed_underline">0.308</span></td>
<td id="S4.T3.1.1.1.18" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.18.1" class="ltx_text ltx_font_bold">0.975</span></td>
<td id="S4.T3.1.1.1.19" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.1.19.1" class="ltx_text ltx_font_bold">0.676</span></td>
<td id="S4.T3.1.1.1.20" class="ltx_td ltx_align_center">0.878</td>
<td id="S4.T3.1.1.1.21" class="ltx_td ltx_align_center">0.602</td>
</tr>
<tr id="S4.T3.1.1.12.11" class="ltx_tr">
<th id="S4.T3.1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SC+COCO+RP-1920</th>
<td id="S4.T3.1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_t">0.705</td>
<td id="S4.T3.1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.383</td>
<td id="S4.T3.1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_t">0.988</td>
<td id="S4.T3.1.1.12.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.688</td>
<td id="S4.T3.1.1.12.11.6" class="ltx_td ltx_align_center ltx_border_t">0.994</td>
<td id="S4.T3.1.1.12.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.714</td>
<td id="S4.T3.1.1.12.11.8" class="ltx_td ltx_align_center ltx_border_t">0.988</td>
<td id="S4.T3.1.1.12.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.652</td>
<td id="S4.T3.1.1.12.11.10" class="ltx_td ltx_align_center ltx_border_t">0.990</td>
<td id="S4.T3.1.1.12.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.709</td>
<td id="S4.T3.1.1.12.11.12" class="ltx_td ltx_align_center ltx_border_t">0.869</td>
<td id="S4.T3.1.1.12.11.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.614</td>
<td id="S4.T3.1.1.12.11.14" class="ltx_td ltx_align_center ltx_border_t">0.988</td>
<td id="S4.T3.1.1.12.11.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.756</td>
<td id="S4.T3.1.1.12.11.16" class="ltx_td ltx_align_center ltx_border_t">0.921</td>
<td id="S4.T3.1.1.12.11.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.639</td>
<td id="S4.T3.1.1.12.11.18" class="ltx_td ltx_align_center ltx_border_t">0.976</td>
<td id="S4.T3.1.1.12.11.19" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.685</td>
<td id="S4.T3.1.1.12.11.20" class="ltx_td ltx_align_center ltx_border_t">0.930</td>
<td id="S4.T3.1.1.12.11.21" class="ltx_td ltx_align_center ltx_border_t">0.644</td>
</tr>
<tr id="S4.T3.1.1.13.12" class="ltx_tr">
<th id="S4.T3.1.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Pretrained-COCO</th>
<td id="S4.T3.1.1.13.12.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.13.12.2.1" class="ltx_text ltx_font_bold">0.879</span></td>
<td id="S4.T3.1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.13.12.3.1" class="ltx_text ltx_font_bold">0.566</span></td>
<td id="S4.T3.1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_t">0.576</td>
<td id="S4.T3.1.1.13.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.376</td>
<td id="S4.T3.1.1.13.12.6" class="ltx_td ltx_align_center ltx_border_t">0.529</td>
<td id="S4.T3.1.1.13.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.354</td>
<td id="S4.T3.1.1.13.12.8" class="ltx_td ltx_align_center ltx_border_t">0.421</td>
<td id="S4.T3.1.1.13.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.274</td>
<td id="S4.T3.1.1.13.12.10" class="ltx_td ltx_align_center ltx_border_t">0.379</td>
<td id="S4.T3.1.1.13.12.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.258</td>
<td id="S4.T3.1.1.13.12.12" class="ltx_td ltx_align_center ltx_border_t">0.331</td>
<td id="S4.T3.1.1.13.12.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.215</td>
<td id="S4.T3.1.1.13.12.14" class="ltx_td ltx_align_center ltx_border_t">0.551</td>
<td id="S4.T3.1.1.13.12.15" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.390</td>
<td id="S4.T3.1.1.13.12.16" class="ltx_td ltx_align_center ltx_border_t">0.173</td>
<td id="S4.T3.1.1.13.12.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.123</td>
<td id="S4.T3.1.1.13.12.18" class="ltx_td ltx_align_center ltx_border_t">0.469</td>
<td id="S4.T3.1.1.13.12.19" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.312</td>
<td id="S4.T3.1.1.13.12.20" class="ltx_td ltx_align_center ltx_border_t">0.480</td>
<td id="S4.T3.1.1.13.12.21" class="ltx_td ltx_align_center ltx_border_t">0.320</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Results of the evaluations of the trained models. We report mAP50 and mAP for each dataset as well as both the average and weighted average of these metrics. We divide between models trained on mixed datasets, vanilla ones, and the model pre-trained with COCO. In bold the best results. We underline the best model not using the RP dataset during training in the corresponding validation column.</span></figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.3" class="ltx_p">Our baseline for comparison consists of the network pre-trained on the full COCO dataset. We perform some training tests on both the default <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="640\times 640" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mn id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">×</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">640</cn><cn type="integer" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">640\times 640</annotation></semantics></math> image size and the increased <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="1920\times 1920" display="inline"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mn id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml">1920</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><times id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></times><cn type="integer" id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">1920</cn><cn type="integer" id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3">1920</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">1920\times 1920</annotation></semantics></math>, identified in our table with the ‘-1920’ suffix. Once established that the bigger image size yields better results, we trained the network with mixed datasets. These are i) SC+COCO-1920, which combines SC training and validation sets with images from COCO’s corresponding splits, ii) SC+COCO+<math id="S4.p3.3.m3.1" class="ltx_Math" alttext="\text{R3}_{100}" display="inline"><semantics id="S4.p3.3.m3.1a"><msub id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mtext id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2a.cmml">R3</mtext><mn id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml">100</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1">subscript</csymbol><ci id="S4.p3.3.m3.1.1.2a.cmml" xref="S4.p3.3.m3.1.1.2"><mtext id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2">R3</mtext></ci><cn type="integer" id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">\text{R3}_{100}</annotation></semantics></math>-1920, which adds 100 train and 100 validation images randomly sampled from R3, and iii) RP+COCO-1920, which merges RP and COCO sets. Note that all models trained with RP have been exposed to representing data coming from Rx_Dx, giving them an advantage in these evaluations. The full description of the datasets, including the training a validation set sizes, is reported in <a href="#S4.T2" title="TABLE II ‣ IV EXPERIMENT AND EVALUATIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.8" class="ltx_p">All our results are reported in Tab. <a href="#S4.T3" title="TABLE III ‣ IV EXPERIMENT AND EVALUATIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Additionally, we present randomly sampled images from the COCO, <span id="S4.p4.8.1" class="ltx_text">APT-36K</span>, R2, and RP datasets for the main models in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV EXPERIMENT AND EVALUATIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Now, we proceed to analyse the results that we report in the table. First, we can notice that the models trained on the bigger image size show higher performances across all datasets and metrics. This is true both for synthetic, i.e. SF and SF-1920, SC and SC-1920, and the real data, i.e. RP and RP-1920. The only exception is the model trained with RP which in the APT-36K performs <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\sim 10\%" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml"></mi><mo id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">∼</mo><mrow id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml"><mn id="S4.p4.1.m1.1.1.3.2" xref="S4.p4.1.m1.1.1.3.2.cmml">10</mn><mo id="S4.p4.1.m1.1.1.3.1" xref="S4.p4.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">absent</csymbol><apply id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3"><csymbol cd="latexml" id="S4.p4.1.m1.1.1.3.1.cmml" xref="S4.p4.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p4.1.m1.1.1.3.2.cmml" xref="S4.p4.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\sim 10\%</annotation></semantics></math> better than RP-1920.
Overall, the model pre-trained on the COCO dataset works well only on the APT-36K dataset with a mAP50 of <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="\sim 88\%" display="inline"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mi id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml"></mi><mo id="S4.p4.2.m2.1.1.1" xref="S4.p4.2.m2.1.1.1.cmml">∼</mo><mrow id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml"><mn id="S4.p4.2.m2.1.1.3.2" xref="S4.p4.2.m2.1.1.3.2.cmml">88</mn><mo id="S4.p4.2.m2.1.1.3.1" xref="S4.p4.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><csymbol cd="latexml" id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2">absent</csymbol><apply id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3"><csymbol cd="latexml" id="S4.p4.2.m2.1.1.3.1.cmml" xref="S4.p4.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p4.2.m2.1.1.3.2.cmml" xref="S4.p4.2.m2.1.1.3.2">88</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">\sim 88\%</annotation></semantics></math>, further showing the low variability of these datasets and the incapability to generalize to both different points of view or scenarios. Indeed, the YOLO model pre-trained on COCO achieves at most <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="\sim 58\%" display="inline"><semantics id="S4.p4.3.m3.1a"><mrow id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><mi id="S4.p4.3.m3.1.1.2" xref="S4.p4.3.m3.1.1.2.cmml"></mi><mo id="S4.p4.3.m3.1.1.1" xref="S4.p4.3.m3.1.1.1.cmml">∼</mo><mrow id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3.cmml"><mn id="S4.p4.3.m3.1.1.3.2" xref="S4.p4.3.m3.1.1.3.2.cmml">58</mn><mo id="S4.p4.3.m3.1.1.3.1" xref="S4.p4.3.m3.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><csymbol cd="latexml" id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1.2">absent</csymbol><apply id="S4.p4.3.m3.1.1.3.cmml" xref="S4.p4.3.m3.1.1.3"><csymbol cd="latexml" id="S4.p4.3.m3.1.1.3.1.cmml" xref="S4.p4.3.m3.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p4.3.m3.1.1.3.2.cmml" xref="S4.p4.3.m3.1.1.3.2">58</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">\sim 58\%</annotation></semantics></math> accuracy on our data, with an overall weighted average of <math id="S4.p4.4.m4.1" class="ltx_Math" alttext="\sim 47\%" display="inline"><semantics id="S4.p4.4.m4.1a"><mrow id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml"><mi id="S4.p4.4.m4.1.1.2" xref="S4.p4.4.m4.1.1.2.cmml"></mi><mo id="S4.p4.4.m4.1.1.1" xref="S4.p4.4.m4.1.1.1.cmml">∼</mo><mrow id="S4.p4.4.m4.1.1.3" xref="S4.p4.4.m4.1.1.3.cmml"><mn id="S4.p4.4.m4.1.1.3.2" xref="S4.p4.4.m4.1.1.3.2.cmml">47</mn><mo id="S4.p4.4.m4.1.1.3.1" xref="S4.p4.4.m4.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><apply id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1"><csymbol cd="latexml" id="S4.p4.4.m4.1.1.1.cmml" xref="S4.p4.4.m4.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.4.m4.1.1.2.cmml" xref="S4.p4.4.m4.1.1.2">absent</csymbol><apply id="S4.p4.4.m4.1.1.3.cmml" xref="S4.p4.4.m4.1.1.3"><csymbol cd="latexml" id="S4.p4.4.m4.1.1.3.1.cmml" xref="S4.p4.4.m4.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p4.4.m4.1.1.3.2.cmml" xref="S4.p4.4.m4.1.1.3.2">47</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">\sim 47\%</annotation></semantics></math>. The fact that the COCO data is representative of the APT-36K dataset can be evinced also by the performance obtained by the model trained with RP+COCO-1920 dataset.
Considering now the synthetic data, i.e. SF-1920 and SC-1920, we can see that the best model overall is SC-1920 which achieves <math id="S4.p4.5.m5.1" class="ltx_Math" alttext="\sim 5\%" display="inline"><semantics id="S4.p4.5.m5.1a"><mrow id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml"><mi id="S4.p4.5.m5.1.1.2" xref="S4.p4.5.m5.1.1.2.cmml"></mi><mo id="S4.p4.5.m5.1.1.1" xref="S4.p4.5.m5.1.1.1.cmml">∼</mo><mrow id="S4.p4.5.m5.1.1.3" xref="S4.p4.5.m5.1.1.3.cmml"><mn id="S4.p4.5.m5.1.1.3.2" xref="S4.p4.5.m5.1.1.3.2.cmml">5</mn><mo id="S4.p4.5.m5.1.1.3.1" xref="S4.p4.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><apply id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1"><csymbol cd="latexml" id="S4.p4.5.m5.1.1.1.cmml" xref="S4.p4.5.m5.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.5.m5.1.1.2.cmml" xref="S4.p4.5.m5.1.1.2">absent</csymbol><apply id="S4.p4.5.m5.1.1.3.cmml" xref="S4.p4.5.m5.1.1.3"><csymbol cd="latexml" id="S4.p4.5.m5.1.1.3.1.cmml" xref="S4.p4.5.m5.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p4.5.m5.1.1.3.2.cmml" xref="S4.p4.5.m5.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">\sim 5\%</annotation></semantics></math> higher mAP and mAP50 across all tests, with a peak of <math id="S4.p4.6.m6.1" class="ltx_Math" alttext="\sim 15\%" display="inline"><semantics id="S4.p4.6.m6.1a"><mrow id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml"><mi id="S4.p4.6.m6.1.1.2" xref="S4.p4.6.m6.1.1.2.cmml"></mi><mo id="S4.p4.6.m6.1.1.1" xref="S4.p4.6.m6.1.1.1.cmml">∼</mo><mrow id="S4.p4.6.m6.1.1.3" xref="S4.p4.6.m6.1.1.3.cmml"><mn id="S4.p4.6.m6.1.1.3.2" xref="S4.p4.6.m6.1.1.3.2.cmml">15</mn><mo id="S4.p4.6.m6.1.1.3.1" xref="S4.p4.6.m6.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><apply id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1"><csymbol cd="latexml" id="S4.p4.6.m6.1.1.1.cmml" xref="S4.p4.6.m6.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.6.m6.1.1.2.cmml" xref="S4.p4.6.m6.1.1.2">absent</csymbol><apply id="S4.p4.6.m6.1.1.3.cmml" xref="S4.p4.6.m6.1.1.3"><csymbol cd="latexml" id="S4.p4.6.m6.1.1.3.1.cmml" xref="S4.p4.6.m6.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p4.6.m6.1.1.3.2.cmml" xref="S4.p4.6.m6.1.1.3.2">15</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">\sim 15\%</annotation></semantics></math> on the R3 dataset. This is probably related to the first of the two generation procedures, which resulted in long distances between the zebras and the cameras (see Sec. <a href="#S3.SS1.SSS1" title="III-A1 Environments ‣ III-A Synthetic data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>1</span></a>). Our real data instead comprises mostly zebras that are reasonably nearby the drone as seen from the pictures in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-B Real-world data ‣ III Approach ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Fig. <a href="#S4.F7" title="Figure 7 ‣ IV EXPERIMENT AND EVALUATIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> in the third and fourth rows. The synthetic models may perform poorly on RP validation set and APT-36K due to the generation process. These sets have diverse images, including zebras near the camera in a side view or hidden behind bushes and trees, e.g. second and fourth row in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV EXPERIMENT AND EVALUATIONS ‣ Synthetic Data-based Detection of Zebras in Drone Imagery" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Moreover, by comparing SC-1920 with the model pre-trained on COCO, we can see how, across all data excluding APT-36K, we obtain higher performances on both metrics of considerable amounts, ranging between <math id="S4.p4.7.m7.1" class="ltx_Math" alttext="\sim 20\%" display="inline"><semantics id="S4.p4.7.m7.1a"><mrow id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml"><mi id="S4.p4.7.m7.1.1.2" xref="S4.p4.7.m7.1.1.2.cmml"></mi><mo id="S4.p4.7.m7.1.1.1" xref="S4.p4.7.m7.1.1.1.cmml">∼</mo><mrow id="S4.p4.7.m7.1.1.3" xref="S4.p4.7.m7.1.1.3.cmml"><mn id="S4.p4.7.m7.1.1.3.2" xref="S4.p4.7.m7.1.1.3.2.cmml">20</mn><mo id="S4.p4.7.m7.1.1.3.1" xref="S4.p4.7.m7.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><apply id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1"><csymbol cd="latexml" id="S4.p4.7.m7.1.1.1.cmml" xref="S4.p4.7.m7.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.7.m7.1.1.2.cmml" xref="S4.p4.7.m7.1.1.2">absent</csymbol><apply id="S4.p4.7.m7.1.1.3.cmml" xref="S4.p4.7.m7.1.1.3"><csymbol cd="latexml" id="S4.p4.7.m7.1.1.3.1.cmml" xref="S4.p4.7.m7.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p4.7.m7.1.1.3.2.cmml" xref="S4.p4.7.m7.1.1.3.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">\sim 20\%</annotation></semantics></math> and <math id="S4.p4.8.m8.1" class="ltx_Math" alttext="\sim 45\%" display="inline"><semantics id="S4.p4.8.m8.1a"><mrow id="S4.p4.8.m8.1.1" xref="S4.p4.8.m8.1.1.cmml"><mi id="S4.p4.8.m8.1.1.2" xref="S4.p4.8.m8.1.1.2.cmml"></mi><mo id="S4.p4.8.m8.1.1.1" xref="S4.p4.8.m8.1.1.1.cmml">∼</mo><mrow id="S4.p4.8.m8.1.1.3" xref="S4.p4.8.m8.1.1.3.cmml"><mn id="S4.p4.8.m8.1.1.3.2" xref="S4.p4.8.m8.1.1.3.2.cmml">45</mn><mo id="S4.p4.8.m8.1.1.3.1" xref="S4.p4.8.m8.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.8.m8.1b"><apply id="S4.p4.8.m8.1.1.cmml" xref="S4.p4.8.m8.1.1"><csymbol cd="latexml" id="S4.p4.8.m8.1.1.1.cmml" xref="S4.p4.8.m8.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p4.8.m8.1.1.2.cmml" xref="S4.p4.8.m8.1.1.2">absent</csymbol><apply id="S4.p4.8.m8.1.1.3.cmml" xref="S4.p4.8.m8.1.1.3"><csymbol cd="latexml" id="S4.p4.8.m8.1.1.3.1.cmml" xref="S4.p4.8.m8.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p4.8.m8.1.1.3.2.cmml" xref="S4.p4.8.m8.1.1.3.2">45</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.8.m8.1c">\sim 45\%</annotation></semantics></math>.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.7" class="ltx_p">We can now compare the differences between the models trained on synthetic data and real data. For this, we will focus on comparing SC-1920 and RP-1920. The weighted average gap is only <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="4.9\%" display="inline"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mn id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">4.9</mn><mo id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><csymbol cd="latexml" id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">4.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">4.9\%</annotation></semantics></math> in the mAP50 and <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="3.2\%" display="inline"><semantics id="S4.p5.2.m2.1a"><mrow id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml"><mn id="S4.p5.2.m2.1.1.2" xref="S4.p5.2.m2.1.1.2.cmml">3.2</mn><mo id="S4.p5.2.m2.1.1.1" xref="S4.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1"><csymbol cd="latexml" id="S4.p5.2.m2.1.1.1.cmml" xref="S4.p5.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.p5.2.m2.1.1.2.cmml" xref="S4.p5.2.m2.1.1.2">3.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">3.2\%</annotation></semantics></math> on the mAP. The big difference in the simple average is mostly linked to the results obtained in the validation set of RP, which was to be expected. Indeed, we can notice how the model trained on synthetic data performs considerably worse in the RP dataset, with a <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="\sim 58\%" display="inline"><semantics id="S4.p5.3.m3.1a"><mrow id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml"><mi id="S4.p5.3.m3.1.1.2" xref="S4.p5.3.m3.1.1.2.cmml"></mi><mo id="S4.p5.3.m3.1.1.1" xref="S4.p5.3.m3.1.1.1.cmml">∼</mo><mrow id="S4.p5.3.m3.1.1.3" xref="S4.p5.3.m3.1.1.3.cmml"><mn id="S4.p5.3.m3.1.1.3.2" xref="S4.p5.3.m3.1.1.3.2.cmml">58</mn><mo id="S4.p5.3.m3.1.1.3.1" xref="S4.p5.3.m3.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><apply id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1"><csymbol cd="latexml" id="S4.p5.3.m3.1.1.1.cmml" xref="S4.p5.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p5.3.m3.1.1.2.cmml" xref="S4.p5.3.m3.1.1.2">absent</csymbol><apply id="S4.p5.3.m3.1.1.3.cmml" xref="S4.p5.3.m3.1.1.3"><csymbol cd="latexml" id="S4.p5.3.m3.1.1.3.1.cmml" xref="S4.p5.3.m3.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p5.3.m3.1.1.3.2.cmml" xref="S4.p5.3.m3.1.1.3.2">58</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">\sim 58\%</annotation></semantics></math> reduction in mAP50 and <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="\sim 41\%" display="inline"><semantics id="S4.p5.4.m4.1a"><mrow id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml"><mi id="S4.p5.4.m4.1.1.2" xref="S4.p5.4.m4.1.1.2.cmml"></mi><mo id="S4.p5.4.m4.1.1.1" xref="S4.p5.4.m4.1.1.1.cmml">∼</mo><mrow id="S4.p5.4.m4.1.1.3" xref="S4.p5.4.m4.1.1.3.cmml"><mn id="S4.p5.4.m4.1.1.3.2" xref="S4.p5.4.m4.1.1.3.2.cmml">41</mn><mo id="S4.p5.4.m4.1.1.3.1" xref="S4.p5.4.m4.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><apply id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1"><csymbol cd="latexml" id="S4.p5.4.m4.1.1.1.cmml" xref="S4.p5.4.m4.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p5.4.m4.1.1.2.cmml" xref="S4.p5.4.m4.1.1.2">absent</csymbol><apply id="S4.p5.4.m4.1.1.3.cmml" xref="S4.p5.4.m4.1.1.3"><csymbol cd="latexml" id="S4.p5.4.m4.1.1.3.1.cmml" xref="S4.p5.4.m4.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p5.4.m4.1.1.3.2.cmml" xref="S4.p5.4.m4.1.1.3.2">41</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">\sim 41\%</annotation></semantics></math> on mAP. A similar result is depicted when we consider tests on the R3_Dx data, with reductions of <math id="S4.p5.5.m5.1" class="ltx_Math" alttext="\sim 16\%" display="inline"><semantics id="S4.p5.5.m5.1a"><mrow id="S4.p5.5.m5.1.1" xref="S4.p5.5.m5.1.1.cmml"><mi id="S4.p5.5.m5.1.1.2" xref="S4.p5.5.m5.1.1.2.cmml"></mi><mo id="S4.p5.5.m5.1.1.1" xref="S4.p5.5.m5.1.1.1.cmml">∼</mo><mrow id="S4.p5.5.m5.1.1.3" xref="S4.p5.5.m5.1.1.3.cmml"><mn id="S4.p5.5.m5.1.1.3.2" xref="S4.p5.5.m5.1.1.3.2.cmml">16</mn><mo id="S4.p5.5.m5.1.1.3.1" xref="S4.p5.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.5.m5.1b"><apply id="S4.p5.5.m5.1.1.cmml" xref="S4.p5.5.m5.1.1"><csymbol cd="latexml" id="S4.p5.5.m5.1.1.1.cmml" xref="S4.p5.5.m5.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p5.5.m5.1.1.2.cmml" xref="S4.p5.5.m5.1.1.2">absent</csymbol><apply id="S4.p5.5.m5.1.1.3.cmml" xref="S4.p5.5.m5.1.1.3"><csymbol cd="latexml" id="S4.p5.5.m5.1.1.3.1.cmml" xref="S4.p5.5.m5.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p5.5.m5.1.1.3.2.cmml" xref="S4.p5.5.m5.1.1.3.2">16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m5.1c">\sim 16\%</annotation></semantics></math> and <math id="S4.p5.6.m6.1" class="ltx_Math" alttext="\sim 6-10\%" display="inline"><semantics id="S4.p5.6.m6.1a"><mrow id="S4.p5.6.m6.1.1" xref="S4.p5.6.m6.1.1.cmml"><mi id="S4.p5.6.m6.1.1.2" xref="S4.p5.6.m6.1.1.2.cmml"></mi><mo id="S4.p5.6.m6.1.1.1" xref="S4.p5.6.m6.1.1.1.cmml">∼</mo><mrow id="S4.p5.6.m6.1.1.3" xref="S4.p5.6.m6.1.1.3.cmml"><mn id="S4.p5.6.m6.1.1.3.2" xref="S4.p5.6.m6.1.1.3.2.cmml">6</mn><mo id="S4.p5.6.m6.1.1.3.1" xref="S4.p5.6.m6.1.1.3.1.cmml">−</mo><mrow id="S4.p5.6.m6.1.1.3.3" xref="S4.p5.6.m6.1.1.3.3.cmml"><mn id="S4.p5.6.m6.1.1.3.3.2" xref="S4.p5.6.m6.1.1.3.3.2.cmml">10</mn><mo id="S4.p5.6.m6.1.1.3.3.1" xref="S4.p5.6.m6.1.1.3.3.1.cmml">%</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.6.m6.1b"><apply id="S4.p5.6.m6.1.1.cmml" xref="S4.p5.6.m6.1.1"><csymbol cd="latexml" id="S4.p5.6.m6.1.1.1.cmml" xref="S4.p5.6.m6.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p5.6.m6.1.1.2.cmml" xref="S4.p5.6.m6.1.1.2">absent</csymbol><apply id="S4.p5.6.m6.1.1.3.cmml" xref="S4.p5.6.m6.1.1.3"><minus id="S4.p5.6.m6.1.1.3.1.cmml" xref="S4.p5.6.m6.1.1.3.1"></minus><cn type="integer" id="S4.p5.6.m6.1.1.3.2.cmml" xref="S4.p5.6.m6.1.1.3.2">6</cn><apply id="S4.p5.6.m6.1.1.3.3.cmml" xref="S4.p5.6.m6.1.1.3.3"><csymbol cd="latexml" id="S4.p5.6.m6.1.1.3.3.1.cmml" xref="S4.p5.6.m6.1.1.3.3.1">percent</csymbol><cn type="integer" id="S4.p5.6.m6.1.1.3.3.2.cmml" xref="S4.p5.6.m6.1.1.3.3.2">10</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.6.m6.1c">\sim 6-10\%</annotation></semantics></math> for the two considered metrics. Nonetheless, with all other datasets, the model trained on synthetic data is comparable to the one trained on real-world captured images of just <math id="S4.p5.7.m7.1" class="ltx_Math" alttext="1-5\%" display="inline"><semantics id="S4.p5.7.m7.1a"><mrow id="S4.p5.7.m7.1.1" xref="S4.p5.7.m7.1.1.cmml"><mn id="S4.p5.7.m7.1.1.2" xref="S4.p5.7.m7.1.1.2.cmml">1</mn><mo id="S4.p5.7.m7.1.1.1" xref="S4.p5.7.m7.1.1.1.cmml">−</mo><mrow id="S4.p5.7.m7.1.1.3" xref="S4.p5.7.m7.1.1.3.cmml"><mn id="S4.p5.7.m7.1.1.3.2" xref="S4.p5.7.m7.1.1.3.2.cmml">5</mn><mo id="S4.p5.7.m7.1.1.3.1" xref="S4.p5.7.m7.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.7.m7.1b"><apply id="S4.p5.7.m7.1.1.cmml" xref="S4.p5.7.m7.1.1"><minus id="S4.p5.7.m7.1.1.1.cmml" xref="S4.p5.7.m7.1.1.1"></minus><cn type="integer" id="S4.p5.7.m7.1.1.2.cmml" xref="S4.p5.7.m7.1.1.2">1</cn><apply id="S4.p5.7.m7.1.1.3.cmml" xref="S4.p5.7.m7.1.1.3"><csymbol cd="latexml" id="S4.p5.7.m7.1.1.3.1.cmml" xref="S4.p5.7.m7.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p5.7.m7.1.1.3.2.cmml" xref="S4.p5.7.m7.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.7.m7.1c">1-5\%</annotation></semantics></math>. Recall that the RP model was trained on the RP dataset itself, composed of images from the Rx experiments and additional images from point-of-views not generated by our procedure. This clearly demonstrates that, on the considered datasets, the model trained solely with the synthetic data generated using the pipeline described above is perfectly capable of detecting zebras by achieving similar performance on all but two datasets when compared with RP, and significantly overcoming the model pre-trained on COCO in all but APT-36K dataset.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Finally, we consider the mixed models. Unsurprisingly, the one based only on real data, i.e. RP+COCO-1920, performs well on all datasets. The slight reduction in performance in the R2 and R3 datasets is well compensated by the generalization in the APT-36K. This is also the model with the highest average mAP and mAP50. We believe that this is mostly linked to how the dataset was built, with RP that contains data from all Rx experiments combined with the 1916 training images of COCO. Despite that, it is interesting to notice how the models trained with a mixture of synthetic and real data are capable of generalizing across all the datasets as well. Specifically, combining SC and COCO, i.e. SC+COCO-1920, resulted practically in a significant improvement of the performance solely in the APT-36K dataset. Minor improvements are noticeable in the other datasets as well, If to this we add 100 samples from R3, i.e. SC+COCO+<math id="S4.p6.1.m1.1" class="ltx_Math" alttext="\text{R3}_{100}" display="inline"><semantics id="S4.p6.1.m1.1a"><msub id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mtext id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2a.cmml">R3</mtext><mn id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">100</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1">subscript</csymbol><ci id="S4.p6.1.m1.1.1.2a.cmml" xref="S4.p6.1.m1.1.1.2"><mtext id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2">R3</mtext></ci><cn type="integer" id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\text{R3}_{100}</annotation></semantics></math>-1920, we then achieve considerable improvements in the performance w.r.t. SC-1920 on all datasets.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/COCO/COCO.jpg" id="S4.F7.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/COCO/coco-det.jpg" id="S4.F7.g2" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/COCO/rp.jpg" id="S4.F7.g3" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/COCO/sc.jpg" id="S4.F7.g4" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/COCO/SC+R+C.jpg" id="S4.F7.g5" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/Ap36/ap36.jpg" id="S4.F7.g6" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/Ap36/coco.jpg" id="S4.F7.g7" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/Ap36/RP.jpg" id="S4.F7.g8" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/Ap36/SC.jpg" id="S4.F7.g9" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/Ap36/SC+R+C.jpg" id="S4.F7.g10" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/R2/R2.jpg" id="S4.F7.g11" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/R2/coco.jpg" id="S4.F7.g12" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/R2/rp.jpg" id="S4.F7.g13" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/R2/sc.jpg" id="S4.F7.g14" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2305.00432/assets/fig/R2/SC+R+C.jpg" id="S4.F7.g15" class="ltx_graphics ltx_figure_panel ltx_img_square" width="117" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S4.F7.4" class="ltx_block ltx_figure_panel">
<figure id="S4.F7.sf1" class="ltx_figure"><img src="/html/2305.00432/assets/fig/RP/RP.jpg" id="S4.F7.sf1.g1" class="ltx_graphics ltx_img_square" width="117" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F7.sf1.3.2" class="ltx_text" style="font-size:90%;">Ground-truth</span></figcaption>
</figure>
<figure id="S4.F7.sf2" class="ltx_figure"><img src="/html/2305.00432/assets/fig/RP/coco.jpg" id="S4.F7.sf2.g1" class="ltx_graphics ltx_img_square" width="117" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F7.sf2.3.2" class="ltx_text" style="font-size:90%;">COCO</span></figcaption>
</figure>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<div id="S4.F7.5" class="ltx_block ltx_figure_panel">
<figure id="S4.F7.sf3" class="ltx_figure"><img src="/html/2305.00432/assets/fig/RP/RP.jpg" id="S4.F7.sf3.g1" class="ltx_graphics ltx_img_square" width="117" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F7.sf3.3.2" class="ltx_text" style="font-size:90%;">RP-1920</span></figcaption>
</figure>
<figure id="S4.F7.sf4" class="ltx_figure"><img src="/html/2305.00432/assets/fig/RP/sc.jpg" id="S4.F7.sf4.g1" class="ltx_graphics ltx_img_square" width="117" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F7.sf4.3.2" class="ltx_text" style="font-size:90%;">SC-1920</span></figcaption>
</figure>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S4.F7.sf5" class="ltx_figure ltx_figure_panel"><img src="/html/2305.00432/assets/fig/RP/SC+R+C.jpg" id="S4.F7.sf5.g1" class="ltx_graphics ltx_img_square" width="117" height="110" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf5.4.2.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S4.F7.sf5.2.1" class="ltx_text" style="font-size:90%;">SC+COCO+<math id="S4.F7.sf5.2.1.m1.1" class="ltx_Math" alttext="\text{R3}_{100}" display="inline"><semantics id="S4.F7.sf5.2.1.m1.1b"><msub id="S4.F7.sf5.2.1.m1.1.1" xref="S4.F7.sf5.2.1.m1.1.1.cmml"><mtext id="S4.F7.sf5.2.1.m1.1.1.2" xref="S4.F7.sf5.2.1.m1.1.1.2a.cmml">R3</mtext><mn id="S4.F7.sf5.2.1.m1.1.1.3" xref="S4.F7.sf5.2.1.m1.1.1.3.cmml">100</mn></msub><annotation-xml encoding="MathML-Content" id="S4.F7.sf5.2.1.m1.1c"><apply id="S4.F7.sf5.2.1.m1.1.1.cmml" xref="S4.F7.sf5.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F7.sf5.2.1.m1.1.1.1.cmml" xref="S4.F7.sf5.2.1.m1.1.1">subscript</csymbol><ci id="S4.F7.sf5.2.1.m1.1.1.2a.cmml" xref="S4.F7.sf5.2.1.m1.1.1.2"><mtext id="S4.F7.sf5.2.1.m1.1.1.2.cmml" xref="S4.F7.sf5.2.1.m1.1.1.2">R3</mtext></ci><cn type="integer" id="S4.F7.sf5.2.1.m1.1.1.3.cmml" xref="S4.F7.sf5.2.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.sf5.2.1.m1.1d">\text{R3}_{100}</annotation></semantics></math>-1920</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.6.2.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.2.1" class="ltx_text" style="font-size:90%;">Sampled detections. We show in column (a) the ground truth and then the results obtained from (b) the default model (pre-trained on COCO), and the ones trained on (c) RP-1920, (d) SC-1920, and (e) SC+COCO+<math id="S4.F7.2.1.m1.1" class="ltx_Math" alttext="\text{R3}_{100}" display="inline"><semantics id="S4.F7.2.1.m1.1b"><msub id="S4.F7.2.1.m1.1.1" xref="S4.F7.2.1.m1.1.1.cmml"><mtext id="S4.F7.2.1.m1.1.1.2" xref="S4.F7.2.1.m1.1.1.2a.cmml">R3</mtext><mn id="S4.F7.2.1.m1.1.1.3" xref="S4.F7.2.1.m1.1.1.3.cmml">100</mn></msub><annotation-xml encoding="MathML-Content" id="S4.F7.2.1.m1.1c"><apply id="S4.F7.2.1.m1.1.1.cmml" xref="S4.F7.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F7.2.1.m1.1.1.1.cmml" xref="S4.F7.2.1.m1.1.1">subscript</csymbol><ci id="S4.F7.2.1.m1.1.1.2a.cmml" xref="S4.F7.2.1.m1.1.1.2"><mtext id="S4.F7.2.1.m1.1.1.2.cmml" xref="S4.F7.2.1.m1.1.1.2">R3</mtext></ci><cn type="integer" id="S4.F7.2.1.m1.1.1.3.cmml" xref="S4.F7.2.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.2.1.m1.1d">\text{R3}_{100}</annotation></semantics></math>-1920. The images are randomly taken from the COCO (first row), APT-36K, R2, and RP (last row) datasets. Best viewed in color and zoomed-in.</span></figcaption>
</figure>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.5" class="ltx_p">The most noticeable are the ones on APT-36K, of around 55%, and on the RP dataset, of around 10%. The improvement in the R3 is to be expected since we mixed 100 images from that set. Nonetheless, it is remarkable that just a small change in the data brought a <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="\sim 34\%" display="inline"><semantics id="S4.p7.1.m1.1a"><mrow id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml"></mi><mo id="S4.p7.1.m1.1.1.1" xref="S4.p7.1.m1.1.1.1.cmml">∼</mo><mrow id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml"><mn id="S4.p7.1.m1.1.1.3.2" xref="S4.p7.1.m1.1.1.3.2.cmml">34</mn><mo id="S4.p7.1.m1.1.1.3.1" xref="S4.p7.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><csymbol cd="latexml" id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">absent</csymbol><apply id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3"><csymbol cd="latexml" id="S4.p7.1.m1.1.1.3.1.cmml" xref="S4.p7.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p7.1.m1.1.1.3.2.cmml" xref="S4.p7.1.m1.1.1.3.2">34</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">\sim 34\%</annotation></semantics></math> increase in the mAP for this validation test. The SC+COCO+<math id="S4.p7.2.m2.1" class="ltx_Math" alttext="\text{R3}_{100}" display="inline"><semantics id="S4.p7.2.m2.1a"><msub id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml"><mtext id="S4.p7.2.m2.1.1.2" xref="S4.p7.2.m2.1.1.2a.cmml">R3</mtext><mn id="S4.p7.2.m2.1.1.3" xref="S4.p7.2.m2.1.1.3.cmml">100</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><apply id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.1.cmml" xref="S4.p7.2.m2.1.1">subscript</csymbol><ci id="S4.p7.2.m2.1.1.2a.cmml" xref="S4.p7.2.m2.1.1.2"><mtext id="S4.p7.2.m2.1.1.2.cmml" xref="S4.p7.2.m2.1.1.2">R3</mtext></ci><cn type="integer" id="S4.p7.2.m2.1.1.3.cmml" xref="S4.p7.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">\text{R3}_{100}</annotation></semantics></math>-1920 is the model with the highest weighted average precisions and is the second best when considering the average mAP and mAP50. We believe that this model would be further improved by having more samples from the COCO dataset in the validation set or, overall, a better-balanced set of samples. Considering that SC is made of 18K images, and both COCO and <math id="S4.p7.3.m3.1" class="ltx_Math" alttext="\text{R3}_{100}" display="inline"><semantics id="S4.p7.3.m3.1a"><msub id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml"><mtext id="S4.p7.3.m3.1.1.2" xref="S4.p7.3.m3.1.1.2a.cmml">R3</mtext><mn id="S4.p7.3.m3.1.1.3" xref="S4.p7.3.m3.1.1.3.cmml">100</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><apply id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p7.3.m3.1.1.1.cmml" xref="S4.p7.3.m3.1.1">subscript</csymbol><ci id="S4.p7.3.m3.1.1.2a.cmml" xref="S4.p7.3.m3.1.1.2"><mtext id="S4.p7.3.m3.1.1.2.cmml" xref="S4.p7.3.m3.1.1.2">R3</mtext></ci><cn type="integer" id="S4.p7.3.m3.1.1.3.cmml" xref="S4.p7.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">\text{R3}_{100}</annotation></semantics></math> make up for 2K training images and only 300 validation ones, we can expect an ‘overfit’ of the final selected model towards scenes which are strongly represented by the synthetic images. Also, in this case, the significant difference in the average mAP and mAP50 is mostly linked to the gap in the results in the RP validation set. For completeness, we also trained the SC+COCO+RP-1920 model, i.e. using the closeby synthetic data, the coco data, and the small set of real data which was precisely labelled. As expected, this is the model which performs best in the majority of the tests, excluding the APT-36K dataset, where the pretrained model performs best, and in R3_D1. However, we must note that RP contains data from all R1, R2 and R3 datasets in both the training and validation sets. Thus, the results in these case are clearly driven by this information. What is interesting to notice is that all mixed models perform similarly in the APT-36K dataset, with <math id="S4.p7.4.m4.1" class="ltx_Math" alttext="\sim 70\%" display="inline"><semantics id="S4.p7.4.m4.1a"><mrow id="S4.p7.4.m4.1.1" xref="S4.p7.4.m4.1.1.cmml"><mi id="S4.p7.4.m4.1.1.2" xref="S4.p7.4.m4.1.1.2.cmml"></mi><mo id="S4.p7.4.m4.1.1.1" xref="S4.p7.4.m4.1.1.1.cmml">∼</mo><mrow id="S4.p7.4.m4.1.1.3" xref="S4.p7.4.m4.1.1.3.cmml"><mn id="S4.p7.4.m4.1.1.3.2" xref="S4.p7.4.m4.1.1.3.2.cmml">70</mn><mo id="S4.p7.4.m4.1.1.3.1" xref="S4.p7.4.m4.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.4.m4.1b"><apply id="S4.p7.4.m4.1.1.cmml" xref="S4.p7.4.m4.1.1"><csymbol cd="latexml" id="S4.p7.4.m4.1.1.1.cmml" xref="S4.p7.4.m4.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p7.4.m4.1.1.2.cmml" xref="S4.p7.4.m4.1.1.2">absent</csymbol><apply id="S4.p7.4.m4.1.1.3.cmml" xref="S4.p7.4.m4.1.1.3"><csymbol cd="latexml" id="S4.p7.4.m4.1.1.3.1.cmml" xref="S4.p7.4.m4.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p7.4.m4.1.1.3.2.cmml" xref="S4.p7.4.m4.1.1.3.2">70</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m4.1c">\sim 70\%</annotation></semantics></math> of mAP50 and <math id="S4.p7.5.m5.1" class="ltx_Math" alttext="\sim 38\%" display="inline"><semantics id="S4.p7.5.m5.1a"><mrow id="S4.p7.5.m5.1.1" xref="S4.p7.5.m5.1.1.cmml"><mi id="S4.p7.5.m5.1.1.2" xref="S4.p7.5.m5.1.1.2.cmml"></mi><mo id="S4.p7.5.m5.1.1.1" xref="S4.p7.5.m5.1.1.1.cmml">∼</mo><mrow id="S4.p7.5.m5.1.1.3" xref="S4.p7.5.m5.1.1.3.cmml"><mn id="S4.p7.5.m5.1.1.3.2" xref="S4.p7.5.m5.1.1.3.2.cmml">38</mn><mo id="S4.p7.5.m5.1.1.3.1" xref="S4.p7.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.5.m5.1b"><apply id="S4.p7.5.m5.1.1.cmml" xref="S4.p7.5.m5.1.1"><csymbol cd="latexml" id="S4.p7.5.m5.1.1.1.cmml" xref="S4.p7.5.m5.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p7.5.m5.1.1.2.cmml" xref="S4.p7.5.m5.1.1.2">absent</csymbol><apply id="S4.p7.5.m5.1.1.3.cmml" xref="S4.p7.5.m5.1.1.3"><csymbol cd="latexml" id="S4.p7.5.m5.1.1.3.1.cmml" xref="S4.p7.5.m5.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p7.5.m5.1.1.3.2.cmml" xref="S4.p7.5.m5.1.1.3.2">38</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.5.m5.1c">\sim 38\%</annotation></semantics></math> mAP, further indicating that a better balancing in the validation set might further boost the performance of these models. Alternatively, a more representative generation strategy could be employed, by including camera locations relative to the zebras more similar to the ones that we can find in the APT-36K or in the COCO dataset. The results suggest that such an approach would be effective as well, perhaps in conjunction with a minimal amount of annotated real data. Finally, considering that zebra stripes are notoriously specific to the individual, it is interesting to notice how, despite the fact we use the same texture for all our generated zebras, we are still able to generalize to different individuals well. This suggests that the network does not focus and learn specifically the pattern it is shown, but rather the general appearance of the animal itself.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSIONS</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we first demonstrated that the currently available datasets do not generalize well to the task of detecting zebras captured from an aerial point of view. To solve this, we generated a large-scale synthetic dataset of zebras by using GRADE, a state-of-the-art framework for synthetic data generation. The dataset, which is the first of its kind both in terms of size and visual realism, has been released for the benefit of the community. By using that, we performed extensive evaluations by training and testing YOLO with a wide range of combinations of real and synthetic data. This provides strong evidence that the visual realism of the data generated is very high, because our models showed performances which are as good as the one obtained by a detector trained on real-world labelled data alone. Using synthetic information we can surpass the process of collecting and labelling data in controlled scenarios, thus avoiding the probable introduction of errors. Further testing by using combined synthetic and a small amount of real data showed that we can successfully generalize to a wide variety of scenarios. A known limitation that we need to address is the realism of the adopted environments and more precise placement strategies, which we believe could solve both the generalization problem and the usage of high-resolution images by the network. Future works include the generation of videos instead of just static images, testing with different network architectures like SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> or RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, and using the synthetic data for different tasks such as keypoints detection.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. LeCun and C. Cortes, “MNIST handwritten digit database,” 2010. [Online].
Available: <a target="_blank" href="http://yann.lecun.com/exdb/mnist/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://yann.lecun.com/exdb/mnist/</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays,
P. Perona, D. Ramanan, P. Doll’a r, and C. L. Zitnick, “Microsoft
COCO: common objects in context,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1405.0312, 2014.
[Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1405.0312" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1405.0312</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The
pascal visual object classes (voc) challenge,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Journal
of Computer Vision</em>, vol. 88, no. 2, pp. 303–338, Jun. 2010.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Rottmann and M. Reese, “Automated detection of label errors in semantic
segmentation datasets via deep learning and uncertainty quantification,” in
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision</em>, 2023, pp. 3214–3223.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
N. Saini, E. Bonetto, E. Price, A. Ahmad, and M. J. Black, “Airpose:
Multi-view fusion network for aerial 3d human pose and shape estimation,”
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 7, no. 2, pp. 4805–4812,
2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Zuffi, A. Kanazawa, D. Jacobs, and M. J. Black, “3D menagerie: Modeling
the 3D shape and pose of animals,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR)</em>, Jul. 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Zuffi, A. Kanazawa, and M. J. Black, “Lions and tigers and bears: Capturing
non-rigid, 3D, articulated shape from images,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.   IEEE Computer Society, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
E. Bonetto, C. Xu, and A. Ahmad, “GRADE: Generating realistic animated
dynamic environments for robotics research,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2303.04466</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. E. Ebadi, S. Dhakad, S. Vishwakarma, C. Wang, Y.-C. Jhang, M. Chociej,
A. Crespi, A. Thaman, and S. Ganguly, “Psp-hdri+: A synthetic dataset
generator for pre-training of human-centric computer vision models,” in
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">First Workshop on Pre-training: Perspectives, Pitfalls, and Paths
Forward at ICML 2022</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Sankaranarayanan, Y. Balaji, A. Jain, S. N. Lim, and R. Chellappa,
“Learning from synthetic data: Addressing domain shift for semantic
segmentation,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>, 2018, pp. 3752–3761.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Cao, H. Tang, H.-S. Fang, X. Shen, C. Lu, and Y.-W. Tai, “Cross-domain
adaptation for animal pose estimation,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">The IEEE International
Conference on Computer Vision (ICCV)</em>, October 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Ye, A. Mathis, and M. W. Mathis, “Panoptic animal pose estimators are
zero-shot performers,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.07436</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H. Yu, Y. Xu, J. Zhang, W. Zhao, Z. Guan, and D. Tao, “AP-10k: A benchmark
for animal pose estimation in the wild,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Thirty-fifth Conference on
Neural Information Processing Systems Datasets and Benchmarks Track (Round
2)</em>, 2021. [Online]. Available:
<a target="_blank" href="https://openreview.net/forum?id=rH8yliN6C83" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=rH8yliN6C83</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y. Yang, J. Yang, Y. Xu, J. Zhang, L. Lan, and D. Tao, “Apt-36k: A large-scale
benchmark for animal pose estimation and tracking,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>, S. Koyejo, S. Mohamed, A. Agarwal,
D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35.   Curran Associates, Inc., 2022, pp. 17 301–17 313. [Online].
Available:
<a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6e566c91d381bd7a45647d9a90838817-Paper-Datasets_and_Benchmarks.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2022/file/6e566c91d381bd7a45647d9a90838817-Paper-Datasets_and_Benchmarks.pdf</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Li, H. Takehara, T. Taketomi, B. Zheng, and M. Nießner, “4dcomplete:
Non-rigid motion estimation beyond the observable surface.” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE
International Conference on Computer Vision (ICCV)</em>, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Li and G. H. Lee, “From synthetic to real: Unsupervised domain adaptation
for animal pose estimation,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition</em>, 2021, pp. 1482–1491.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Mathis, T. Biasi, S. Schneider, M. Yuksekgonul, B. Rogers, M. Bethge, and
M. W. Mathis, “Pretraining boosts out-of-domain robustness for pose
estimation,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV)</em>, January 2021, pp. 1859–1868.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. M. Graving, D. Chae, H. Naik, L. Li, B. Koger, B. R. Costelloe, and I. D.
Couzin, “Deepposekit, a software toolkit for fast and robust animal pose
estimation using deep learning,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">eLife</em>, vol. 8, p. e47994, oct 2019.
[Online]. Available: <a target="_blank" href="https://doi.org/10.7554/eLife.47994" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.7554/eLife.47994</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Mu, W. Qiu, G. D. Hager, and A. L. Yuille, “Learning from synthetic
animals,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
N. Koenig and A. Howard, “Design and use paradigms for gazebo, an open-source
multi-robot simulator,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2004 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)</em>, vol. 3,
2004, pp. 2149–2154 vol.3.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual and
physical simulation for autonomous vehicles,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Field and Service
Robotics</em>, 2017. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/1705.05065" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1705.05065</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, M. Deitke,
K. Ehsani, D. Gordon, Y. Zhu, A. Kembhavi, A. Gupta, and A. Farhadi,
“AI2-THOR: An Interactive 3D Environment for Visual AI,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>,
vol. abs/1712.05474, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
B. Shen, F. Xia, C. Li, R. Martín-Martín, L. Fan, G. Wang,
C. Pérez-D’Arpino, S. Buch, S. Srivastava, L. P. Tchapmi, M. E. Tchapmi,
K. Vainio, J. Wong, L. Fei-Fei, and S. Savarese, “igibson 1.0: a simulation
environment for interactive tasks in large realistic scenes,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2021
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS)</em>.   IEEE, 2021, p. accepted.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Y. Zhao,
E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and
D. Batra, “Habitat: A Platform for Embodied AI Research,” in
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV)</em>, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Müller, V. Casser, J. Lahoud, N. Smith, and B. Ghanem, “Sim4cv: A
photo-realistic simulator for computer vision applications,”
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, vol. 126, no. 9, pp.
902–919, Mar. 2018. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1007/s11263-018-1073-7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s11263-018-1073-7</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA: An
open urban driving simulator,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Annual
Conference on Robot Learning</em>, ser. Proceedings of Machine Learning Research,
S. Levine, V. Vanhoucke, and K. Goldberg, Eds., vol. 78.   PMLR, 13–15 Nov 2017, pp. 1–16.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
W. Wang, Y. Hu, and S. Scherer, “Tartanvo: A generalizable learning-based
vo,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Conference on Robot Learning (CoRL)</em>, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
“Zebra motions. Free 3D model by Kapi777,”
<a target="_blank" href="https://sketchfab.com/3d-models/zebramotions-2546097d0ea94ba88452ce62c041fb87" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://sketchfab.com/3d-models/zebramotions-2546097d0ea94ba88452ce62c041fb87</a>,
[Accessed 11-Apr-2023].

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV
2016</em>, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds.   Cham: Springer International Publishing, 2016, pp. 21–37.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
E. Price and A. Ahmad, “Accelerated video annotation driven by deep detector
and tracker,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Intelligent Autonomous Systems 18</em>, 2023, to appear.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollar, and R. Girshick, “Mask r-cnn,” in
<em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision
(ICCV)</em>, Oct 2017.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Bonetto et al."></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="dataset"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Synthetic based training"></div>
<div class="ltx_rdf" about="" property="dcterms:title"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.00431" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.00432" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.00432">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.00432" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.00433" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 10:38:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
