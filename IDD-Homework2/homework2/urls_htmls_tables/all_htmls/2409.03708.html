<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RAG based Question-Answering for Contextual Response Prediction System</title>
<!--Generated on Fri Sep  6 14:16:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Retrieval Augmented Generation,  Response Prediction System,  Question Answering System,  Contact Center Agents,  Automated Hallucination Measurement,  Hallucination Reduction,  Retrieval Strategies,  Optimal Retriever Threshold,  ScaNN,  Embedding Strategies,  Contextual Relevance,  Specificity,  Completeness,  Hallucination Rate,  Missing Rate,  Human Evaluation of RAG Versus Traditional Seq-2-Seq models,  RAG Deployment." lang="en" name="keywords"/>
<base href="/html/2409.03708v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S1" title="In RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S2" title="In RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S2.SS0.SSS0.Px1" title="In 2. Related Work ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">RAG architecture:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S2.SS0.SSS0.Px2" title="In 2. Related Work ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">RAG LLM for question answering:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S3" title="In RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S3.SS1" title="In 3. Methodology ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Phase I: Data Preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S3.SS2" title="In 3. Methodology ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Phase II: RAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4" title="In RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and Findings</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS1" title="In 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Retrieval evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS1.SSS0.Px1" title="In 4.1. Retrieval evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Best setting for RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS1.SSS0.Px2" title="In 4.1. Retrieval evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Retrieval Threshold:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2" title="In 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Response Prediction System Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS1" title="In 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Automated evaluations</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS1.Px1" title="In 4.2.1. Automated evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Accuracy, Hallucination and Missing rate evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS1.Px2" title="In 4.2.1. Automated evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">AlignScore:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS1.Px3" title="In 4.2.1. Automated evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Semantic similarity:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS1.Px4" title="In 4.2.1. Automated evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Human touch:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS2" title="In 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Human evaluations</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS2.Px1" title="In 4.2.2. Human evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Human Preference Score:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS2.Px2" title="In 4.2.2. Human evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Quantitative Metrics:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS2.Px3" title="In 4.2.2. Human evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Qualitative Metrics:</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS3" title="In 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation for ReAct and prompting techniques</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS3.SSS0.Px1" title="In 4.3. Evaluation for ReAct and prompting techniques ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Experiments with ReAct</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS3.SSS0.Px2" title="In 4.3. Evaluation for ReAct and prompting techniques ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Prompting Techniques Experiments</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S5" title="In RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1" title="In RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Evaluation of RAG Approach for Open Source Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.SS1" title="In Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Dataset Statistics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.SS2" title="In Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Retrieval and Embedding Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.SS3" title="In Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Evaluation of Generated Responses</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A2" title="In RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Prompt Examples</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A2.SS0.SSS0.Px1" title="In Appendix B Prompt Examples ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Prompt for answer generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A2.SS0.SSS0.Px2" title="In Appendix B Prompt Examples ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Prompt for Hallucination Judgement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A2.SS0.SSS0.Px3" title="In Appendix B Prompt Examples ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Prompt for Generating Baseline Response and Plan Verification (Chain of Verification)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A2.SS0.SSS0.Px4" title="In Appendix B Prompt Examples ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Prompt for Executing Verification Questions and Generating Verified Response (Chain of Verification)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A2.SS0.SSS0.Px5" title="In Appendix B Prompt Examples ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_title">Prompt for generating answer from a document and question (open source datasets).</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">RAG based Question-Answering for Contextual Response Prediction System</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sriram Veturi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sriram%CB%99veturi@homedepot.com">sriram˙veturi@homedepot.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Saurabh Vaichal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:saurabh%CB%99s%CB%99vaichal@homedepot.com">saurabh˙s˙vaichal@homedepot.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">The Home Depot</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Atlanta</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Georgia</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Reshma Lal Jagadheesh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">The Home Depot</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Atlanta</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">Georgia</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nafis Irtiza Tripto
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">The Pennsylvania State University</span><span class="ltx_text ltx_affiliation_country" id="id10.2.id2">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:nit5154@psu.edu">nit5154@psu.edu</a>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:reshma%CB%99lal%CB%99jagadheesh@homedepot.com">reshma˙lal˙jagadheesh@homedepot.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nian Yan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.1.id1">The Home Depot</span><span class="ltx_text ltx_affiliation_city" id="id12.2.id2">Atlanta</span><span class="ltx_text ltx_affiliation_state" id="id13.3.id3">Georgia</span><span class="ltx_text ltx_affiliation_country" id="id14.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:nian%CB%99yan@homedepot.com">nian˙yan@homedepot.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id15.id1">Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.</p>
</div>
<div class="ltx_keywords">Retrieval Augmented Generation, Response Prediction System, Question Answering System, Contact Center Agents, Automated Hallucination Measurement, Hallucination Reduction, Retrieval Strategies, Optimal Retriever Threshold, ScaNN, Embedding Strategies, Contextual Relevance, Specificity, Completeness, Hallucination Rate, Missing Rate, Human Evaluation of RAG Versus Traditional Seq-2-Seq models, RAG Deployment.
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>CIKM 2024; October 24, 2024; Boise, Idaho, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="203" id="S1.F1.g1" src="x1.png" width="415"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Example of the Response Prediction System. <span class="ltx_text ltx_font_bold" id="S1.F1.3.1">(A)</span>: For a valid query, the system retrieves the relevant document and proposes the appropriate responses from where the agent choose. <span class="ltx_text ltx_font_bold" id="S1.F1.4.2">(B)</span>: For an out-of-domain query, it guides the user to ask a relevant question.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the advent of ChatGPT and similar tools in mainstream media, Large Language Models (LLMs) have emerged as the standard solution for addressing a wide range of language understanding tasks. However, they can generate incorrect or biased information <cite class="ltx_cite ltx_citemacro_citep">(Tonmoy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib35" title="">2024</a>)</cite>, as their responses are based on patterns learned from data that may not always contain necessary knowledge in a close domain. To address this issue, Retrieval Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib21" title="">2021</a>)</cite> is commonly used to ground LLMs in factual information. The RAG architecture processes user input by first retrieving a set of documents similar to the query, which the language model then uses to generate a final prediction. While RAG-based architectures have been successful in various open-domain question answering (Q/A) tasks <cite class="ltx_cite ltx_citemacro_citep">(Siriwardhana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib31" title="">2023</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib50" title="">2024</a>; Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib16" title="">2020</a>)</cite>, limited research has explored their scaling dynamics in real conversational scenarios. Therefore, our research is one of the pioneering efforts in exploring the feasibility of an RAG-based approach for developing a knowledge-grounded response prediction system specifically tailored for the contact center of a major retail company.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="260" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Overview of the systems: (A) Agents respond to queries by manually searching for relevant documents, (B) The existing BERT-based system, which extracts relevant Q/A pairs from the given query and provides suggested answers to the agents, (C) The proposed RAG LLM system, where the LLM retrieves relevant KB articles (if necessary) and generates answers based on the query and the retrieved articles.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">LLMs have recently been widely adopted across various industries, particularly in contact centers, to enhance chatbot development and agent-facing automation <cite class="ltx_cite ltx_citemacro_citep">(Wortmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib38" title="">2020</a>; Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib9" title="">2024</a>; Freire et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib13" title="">2024</a>)</cite>. A prime example is the Response Prediction System (RPS), an agent-assist solution that generates contextually relevant responses, enabling agents to efficiently address customer queries with a single click. This boosts productivity, improves customer experience, and streamlines communication processes. In industry settings, the focus is on generating accurate, contextually appropriate responses with minimal latency. Therefore, RAG-based responses, grounded in company policies, deliver swift and accurate resolutions to customer issues. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates a possible example of RPS in real settings, where the agent can directly utilize the generated response with a single click.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, implementing RAG for industry-specific use cases to assist human agents in generating valid responses involves several architectural decisions that can affect performance and viability. The retrieval style can be integrated into both encoder-decoder <cite class="ltx_cite ltx_citemacro_citep">(Yu, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib45" title="">2022</a>; Izacard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib17" title="">2022</a>)</cite>) and decoder-only models <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib20" title="">2020</a>; Borgeaud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib6" title="">2022</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib28" title="">2022</a>; Rubin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib27" title="">2022</a>)</cite>, with various embedding and prompting techniques influencing the final LLM output. In contact centers, where the risk of hallucinations is high and can critically impact business performance, ReAct (Reason+Act) <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib43" title="">2023</a>)</cite> prompts can help mitigate issues. Therefore, our research focuses on developing an optimal RAG based knowledge-grounded RPS for a major retail company’s contact center. To ensure response accuracy, we also conduct thorough evaluations with human evaluators and automated measures, comparing RAG-based responses to human ground truth and the existing BERT-based system (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">2</span></a> shows an overview of traditional customer care scenario with existing and proposed system).
In short, we answer the following research questions.</p>
</div>
<div class="ltx_para" id="S1.p4">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">RQ1:</span> What are the effects of different embedding techniques, retrieval strategies, and prompting methods on RAG performance?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">RQ2:</span> Do RAG-based responses provide greater assistance to human agents compared to the existing BERT-based system?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">RQ3:</span> Can the ReAct (Reason+Act) prompting improve factual accuracy and reduce hallucinations in LLM in real-time settings?</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our findings demonstrate an overall improvement over the existing system by suggesting more accurate and relevant responses, highlighting the potential of RAG LLM as an excellent choice for customer care automation.</p>
</div>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="327" id="S1.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>End to end RAG LLM framework </figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">RAG architecture:</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">RAG has emerged as a promising solution by incorporating knowledge from external databases to overcome the hallucination, outdated knowledge, transparency issues for LLMs <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib21" title="">2021</a>; Khandelwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib20" title="">2020</a>; Borgeaud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib6" title="">2022</a>; Izacard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib18" title="">2023</a>; Yasunaga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib44" title="">2023</a>)</cite>. Traditional RAG, popularized after the adoption of ChatGPT, follows a simple process of indexing, retrieval, and generation <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib15" title="">2024</a>)</cite> .
Despite advancements in Advanced and Modular RAG, Traditional RAG remains popular in the industry due to its ease of development, integration, and quicker speed to market <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib22" title="">2023</a>)</cite>. The core components of Traditional RAG include Retriever, Generator, and Augmentation Method, with research focusing on improving semantic representation <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib10" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib49" title="">2023a</a>)</cite>, query alignment <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib36" title="">2023</a>)</cite>, and integration with LLMs <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib46" title="">2023</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib29" title="">2023</a>; Izacard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib17" title="">2022</a>)</cite>, which motivate our RQ1 to find the optimum setup in this specific use case of RPS in the contact center.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">RAG LLM for question answering:</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Several open-domain question-answering (Q/A) tasks have been completed by RAG-based architectures efficiently <cite class="ltx_cite ltx_citemacro_citep">(Siriwardhana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib31" title="">2023</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib50" title="">2024</a>; Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib16" title="">2020</a>)</cite>. With the advent of LLMs in recent periods, multiple studies also focus on utilizing LLMs for customer assistance, specifically in recommendations <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib42" title="">2023</a>; Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib14" title="">2023</a>)</cite> and dialogue generation <cite class="ltx_cite ltx_citemacro_citep">(Shuster et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib30" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib48" title="">2023b</a>)</cite>. Recent work by <cite class="ltx_cite ltx_citemacro_citep">(Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib3" title="">2024</a>)</cite> proposes augmenting LLMs with user-specific context from search engine interaction histories to personalize outputs, leveraging entity-centric knowledge stores derived from users’ web activities. Similarly, <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib41" title="">2024</a>)</cite> introduces a customer service question-answering approach integrating RAG with a knowledge graph (KG) constructed from historical issue data. Therefore, our study is motivated by these prior researches to integrate RAG as a retrieval tool and utilize LLM to generate responses to answer customer queries.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To implement an end-to-end RAG framework with LLM, first, it is essential to create a comprehensive dataset comprising relevant question-answer pairs along with corresponding knowledge documents. Next, design choices for specific components of the RAG and LLM architecture must be finalized. Finally, the model should be thoroughly evaluated and refined before being deployed into production.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Phase I: Data Preparation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">An ideal golden dataset for evaluating RAG architecture (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S1.F3" title="Figure 3 ‣ 1. Introduction ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">3</span></a>) should include:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Domain-specific questions (previous queries) with their corresponding grounded responses.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Relevant knowledge base (KB) articles (company documents) containing the policies that determine answers to specific queries.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Out-of-domain questions to ensure the LLM can handle generic queries without hallucinating and can guide customers to provide relevant queries.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:216.8pt;height:100.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-65.0pt,30.0pt) scale(0.625111801042758,0.625111801042758) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1">Source</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2.1">Total #</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.3.1">length (query)</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt" id="S3.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.4.1">length (response)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.2.1.1">KB articles</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2.2">1205</th>
<td class="ltx_td ltx_align_left ltx_border_t" colspan="2" id="S3.T1.1.1.2.2.3">Avg. document length: 134.25</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.1">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.3.3.1.1">
<tr class="ltx_tr" id="S3.T1.1.1.3.3.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.1.3.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.3.3.1.1.1.1.1">In domain Q/A (generated</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.3.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.1.3.3.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.3.3.1.1.2.1.1">by LLM from KB articles)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.2">4785</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.3.3.3">10.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S3.T1.1.1.3.3.4">33.59</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.1">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.4.4.1.1">
<tr class="ltx_tr" id="S3.T1.1.1.4.4.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.1.4.4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.4.4.1.1.1.1.1">In domain Q/A (sampled</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.4.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.1.4.4.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.4.4.1.1.2.1.1">from previous history)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.2">3000</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.4.4.3">9.58</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S3.T1.1.1.4.4.4">28.81</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.1">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.5.5.1.1">
<tr class="ltx_tr" id="S3.T1.1.1.5.5.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.1.5.5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.5.5.1.1.1.1.1">Out domain Q/A (sampled</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.5.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T1.1.1.5.5.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.5.5.1.1.2.1.1">from MS-MARCO)</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.2">3660</th>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.5.5.3">5.73</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t" id="S3.T1.1.1.5.5.4">5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Total number and average length (in terms of word count) for the KB articles and various Q/A pairs for the RAG implementation</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To create a robust test set (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S3.T1" title="Table 1 ‣ 3.1. Phase I: Data Preparation ‣ 3. Methodology ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">1</span></a> for details), we utilize LLM to generate both relevant question-answer pairs from the company’s KB articles (refer to Section A in the Appendix for prompts). Additionally, we supplement these relevant pairs with samples from previous queries &amp; responses in the contact center with out-of-domain questions by sampling from open-source datasets such as MS-MARCO <cite class="ltx_cite ltx_citemacro_citep">(Bajaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib4" title="">2018</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Phase II: RAG</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The main components of the RAG architecture are the Retriever and the Generator LLM. We evaluate various strategies for each component and finalize our choices for production. Our findings are validated through experiments with several open-domain question-answer datasets, including MARCO <cite class="ltx_cite ltx_citemacro_citep">(Bajaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib4" title="">2018</a>)</cite>, SQuAD <cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib24" title="">2016</a>)</cite>, and TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib19" title="">2017</a>)</cite> (details in Appendix).
They present a comparable level of question-answering challenges where answers can be derived from the retrieved knowledge base.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Embedding Strategy:</span>
The best embedding strategy ensures high performance of the retriever and affects downstream tasks like response generation. We compare the Universal Sentence Encoder (USE) embeddings <cite class="ltx_cite ltx_citemacro_citep">(Cer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib7" title="">2018</a>)</cite>, Google’s Vertex AI embedding model text-embedding-gecko@001 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">anil-etal-2022-large</span>)</cite>, and SBERT-all-mpnet-base-v2 <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib25" title="">2019</a>)</cite> from the sentence-transformers collection.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Retrieval strategies: </span> By retrieving relevant passages from a large corpus of KB articles, the model gains crucial contextual information, enhancing response accuracy and coherence.
We specifically consider ScaNN <cite class="ltx_cite ltx_citemacro_citep">(Research, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib26" title="">2020</a>)</cite> for its efficiency in handling large-scale datasets and KNN HNSW <cite class="ltx_cite ltx_citemacro_citep">(Malkov and Yashunin, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib23" title="">2018</a>)</cite> for its efficient memory usage as retrieval strategies in our study.
Additionally, we tested different retrieval thresholds to ensure incorrect documents are not retrieved and passed to the LLM for response generation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">LLM for generation: </span>
Once the best embedding strategy, retrieval technique, and retrieval thresholds are identified, we test different prompting techniques to ensure that LLMs generate grounded factual responses. We utilize PaLM2 foundation models (text-bison, text-unicorn) <cite class="ltx_cite ltx_citemacro_citep">(Anil et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib2" title="">2023</a>)</cite> for text generation across all tasks, as they offer a clear path to production in terms of enterprise licenses and security requirements with Google’s models, compared to other available LLMs at the time of our research.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">The best model from Phase 2, incorporating the optimal embedding, retrieval, and prompting techniques, is packaged with relevant KB articles and deployed on a cloud Virtual Machine. For real-time usage, an endpoint is created that takes a customer query and the conversation context as input, generating response suggestions as output.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Results and Findings</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">First, we optimize RAG setup for our use cases, then evaluate LLM responses using automated metrics and human evaluations. Finally, we assess if prompting or ReAct strategies can improve real-world performance to an acceptable level.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Retrieval evaluation</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Best setting for RAG</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.2">We assessed retriever efficiency using the ”Recall at K” (<math alttext="R@k" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p2.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml">R</mi><mo id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.3" mathvariant="normal" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml">@</mi><mo id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.1a" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.4" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.4.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p2.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1"><times id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.1"></times><ci id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.2">𝑅</ci><ci id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.3">@</ci><ci id="S4.SS1.SSS0.Px1.p2.1.m1.1.1.4.cmml" xref="S4.SS1.SSS0.Px1.p2.1.m1.1.1.4">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p2.1.m1.1c">R@k</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p2.1.m1.1d">italic_R @ italic_k</annotation></semantics></math>) metric, where <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p2.2.m2.1"><semantics id="S4.SS1.SSS0.Px1.p2.2.m2.1a"><mi id="S4.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p2.2.m2.1b"><ci id="S4.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p2.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p2.2.m2.1d">italic_K</annotation></semantics></math> represents the top 1, 3, 5, or 10 documents retrieved, measuring how well the retriever retrieves relevant documents.
The Vertex AI - textembedding-gecko@001 (768) embedding, paired with ScaNN retrieval, yielded the best outcomes. Overall, ScaNN generally outperformed KNN HNSW in most cases due to its efficient handling of large-scale datasets and superior retrieval accuracy through quantization and re-ranking techniques <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib8" title="">2024</a>)</cite>, so we include only the ScaNN results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.T2" title="Table 2 ‣ Best setting for RAG ‣ 4.1. Retrieval evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">2</span></a>. Similarly, Vertex AI embeddings surpassed Sentence BERT and USE due to its superior ability to capture complex semantic relationships tailored for large-scale industry applications.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:216.8pt;height:64pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.5pt,4.0pt) scale(0.889211447729228,0.889211447729228) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1" style="font-size:80%;">Embedding</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.2.1" style="font-size:80%;">Embedding size</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.3.1" style="font-size:80%;">R@1</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.4.1" style="font-size:80%;">R@3</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.5.1" style="font-size:80%;">R@5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.1.1" style="font-size:80%;">USE</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.2.1.2"><span class="ltx_text" id="S4.T2.1.1.2.1.2.1" style="font-size:80%;">512</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.3"><span class="ltx_text" id="S4.T2.1.1.2.1.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.4"><span class="ltx_text" id="S4.T2.1.1.2.1.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.5"><span class="ltx_text" id="S4.T2.1.1.2.1.5.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.2.1.1" style="font-size:80%;">SBERT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.1.3.2.2"><span class="ltx_text" id="S4.T2.1.1.3.2.2.1" style="font-size:80%;">768</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.3"><span class="ltx_text" id="S4.T2.1.1.3.2.3.1" style="font-size:80%;">+15.36</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.4"><span class="ltx_text" id="S4.T2.1.1.3.2.4.1" style="font-size:80%;">+9.42</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.3.2.5"><span class="ltx_text" id="S4.T2.1.1.3.2.5.1" style="font-size:80%;">+8.22</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.1.1" style="font-size:80%;">Vertex AI</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.1.4.3.2"><span class="ltx_text" id="S4.T2.1.1.4.3.2.1" style="font-size:80%;">768</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.4.3.3"><span class="ltx_text" id="S4.T2.1.1.4.3.3.1" style="font-size:80%;">+21.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.4.3.4"><span class="ltx_text" id="S4.T2.1.1.4.3.4.1" style="font-size:80%;">+13.87</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.1.1.4.3.5"><span class="ltx_text" id="S4.T2.1.1.4.3.5.1" style="font-size:80%;">+11.85</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Performance of different embedding technique in the company data for ScaNN. Performance is shown as the (%) of improvement wrt the lowest performing embedding (USE in this scenario)</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Retrieval Threshold:</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">For out-of-domain or trivial customer queries like ”Hello” or ”Bye,” document retrieval is unnecessary, as shown by 98.59% of retrieved articles having a cosine similarity score below 0.7. In contrast, 88.96% of articles retrieved for relevant company data questions scored above 0.7 (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.F4" title="Figure 4 ‣ Retrieval Threshold: ‣ 4.1. Retrieval evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">4</span></a>). This suggests that setting the retrieval threshold at 0.7 effectively determines when retrieval is needed, thereby enhancing response generation efficiency.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="149" id="S4.F4.g1" src="extracted/5838273/internal_data_vs_OOS.png" width="236"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Cosine similarity score between query and ScaNN retrieved Document; retrieval threshold(0.7)</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Response Prediction System Evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To develop an effective Response Generation System (RPS), we conducted a comprehensive evaluation comparing RAG LLM-based responses with a current BERT-based algorithm. Using 1,000 real contact center chat transcripts (PII and PCI compliant), comprising over 5,000 messages, we analyzed customer queries, human agent responses, RAG LLM suggestions, BERT-based suggestions, and retrieved knowledge base documents to assess quality, consistency, and factuality through automated measures and human evaluations.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1" style="font-size:80%;">Metric</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1" style="font-size:80%;">Improvement</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.1.1.1" style="font-size:80%;">Accuracy</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.1.2.1.2"><span class="ltx_text" id="S4.T3.1.2.1.2.1" style="font-size:80%;">+10.15</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.1.1" style="font-size:80%;">Hallucination</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.3.2.2"><span class="ltx_text" id="S4.T3.1.3.2.2.1" style="font-size:80%;">-4.76</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T3.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.1.1" style="font-size:80%;">Missing rate</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.4.3.2"><span class="ltx_text" id="S4.T3.1.4.3.2.1" style="font-size:80%;">-5.43</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<td class="ltx_td ltx_align_left" id="S4.T3.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.4.1.1" style="font-size:80%;">AlignScore</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.5.4.2"><span class="ltx_text" id="S4.T3.1.5.4.2.1" style="font-size:80%;">+5.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<td class="ltx_td ltx_align_left" id="S4.T3.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.5.1.1" style="font-size:80%;">Semantic similarity</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.6.5.2"><span class="ltx_text" id="S4.T3.1.6.5.2.1" style="font-size:80%;">+20.01</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.6.1.1" style="font-size:80%;">AI-generated</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T3.1.7.6.2"><span class="ltx_text" id="S4.T3.1.7.6.2.1" style="font-size:80%;">-40.17</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparision between RAG based responses and existing BERT-based ones for automated evaluations. Values indicate the difference in percentage (%) as average of all samples</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Automated evaluations</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We utilize the following evaluation techniques, with Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.T3" title="Table 3 ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">3</span></a> illustrating our RAG LLM-based technique’s performance against the current BERT-based system.</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Accuracy, Hallucination and Missing rate evaluation</h5>
<div class="ltx_para" id="S4.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px1.p1.1">In a question-answer system, a response to each query can generate one of three types of responses: accurate (correctly answers the question), hallucinate (incorrect answer), or missing (no answer generated). Therefore, our approach, inspired by <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib33" title="">2023</a>)</cite> which provides 98% agreement with human judgments, utilizes an LLM-based method. We employ ChatGPT-3.5-turbo as our evaluator LLM.
We prompted the LLM with a query, generated response, and original human response, categorizing the LLM’s responses as ”correct” for factual and semantic alignment, ”incorrect” for mismatches, and ”unsure” for semantic challenges. Evaluation includes Accuracy (correct responses), Hallucination Rate (incorrect responses), and Missing Rate (unsure responses) metrics as the proportion of corresponding responses. Overall, RAG LLM improves accuracy by reducing hallucinations and missing rates compared to BERT responses.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">AlignScore:</h5>
<div class="ltx_para" id="S4.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p1.1">To ensure response alignment with KB articles, we use AlignScore <cite class="ltx_cite ltx_citemacro_citep">(Zha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib47" title="">2023</a>)</cite> to measure information consistency. Evaluating RAG LLM and BERT-based models on utterances with relevant KB article retrieval by RAG, RAG LLM shows a statistically significant 5.6% improvement via Student’s t-test. This enhancement derives from integrating retrieved documents as prompts for LLM responses, whereas BERT relies on query-answer pairs in its training dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Semantic similarity:</h5>
<div class="ltx_para" id="S4.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px3.p1.1">To ensure usability by human agents, coherence between generated and original human responses is crucial. We measure semantic similarity using LongFormer embeddings <cite class="ltx_cite ltx_citemacro_citep">(Beltagy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib5" title="">2020</a>)</cite>, calculating cosine similarity between generated and original human responses for both models. RAG LLM exhibits an average 20% higher similarity, a statistically significant improvement.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">Human touch:</h5>
<div class="ltx_para" id="S4.SS2.SSS1.Px4.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px4.p1.1">Customer service are typically preferred to be handled by humans <cite class="ltx_cite ltx_citemacro_citep">(Fernandes and Oliveira, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib12" title="">2021</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib39" title="">2022</a>)</cite>, emphasizing the importance of generating human-like responses. We use the AI text detector GPTZero <cite class="ltx_cite ltx_citemacro_citep">(Tian, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib34" title="">2023</a>)</cite>, with a 99.05% true positive rate for human responses in our dataset, to evaluate response naturalness. Assessing AI percentage (utterances identified as AI-generated), the BERT-based system, which selects responses from human-generated options, sounds more human.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Human evaluations</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Our method aims to support rather than replace humans through a human-in-the-loop approach. We thoroughly evaluate the quality of RAG LLM and BERT responses using human annotators. Each response is assessed against several criteria, and the average score is computed from all annotators’ evaluations. Evaluation metrics were grouped into three main categories:</p>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Human Preference Score:</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px1.p1.1">Following the classical approach of which version humans prefer most <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib40" title="">2023</a>; Stiennon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib32" title="">2020</a>)</cite>, we evaluated which model’s responses—”BERT” or ”RAG”—were preferred by human evaluators.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Quantitative Metrics:</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px2.p1.1">Similar to <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib33" title="">2023</a>)</cite>, we evaluated factual accuracy (based on human judgment of ’correct,’ ’incorrect,’ or ’unsure’). Accuracy, Hallucination, and Missing rates were calculated as the number of correct, incorrect, and unsure responses divided by the total number of responses evaluated, respectively.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Qualitative Metrics:</h5>
<div class="ltx_para" id="S4.SS2.SSS2.Px3.p1">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Contextual Relevance:</span> Assessed whether the predicted responses were appropriate and in line with the context of the conversation.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Completeness:</span> Checked if the predicted responses were fully-formed and could be used as complete answers by the agents in specific parts of the conversation.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Specificity:</span> Determined whether the predicted responses were tailored to the specific conversation or were too general. Human annotators scored these metrics on a scale of 0 (lowest) to 2 (highest).</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.Px3.p2">
<p class="ltx_p" id="S4.SS2.SSS2.Px3.p2.1">The results, as detailed in the Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.T4" title="Table 4 ‣ Qualitative Metrics: ‣ 4.2.2. Human evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">4</span></a>, Responses generated by the RAG model demonstrated a 45% improvement in factual accuracy and a 27% decrease in the rate of hallucinations compared to the existing model. Moreover, the human evaluators favored responses from the RAG model over the current production model 75% of the times.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.Px3.p3">
<p class="ltx_p" id="S4.SS2.SSS2.Px3.p3.1">The Response Prediction System was deployed using Flask, a standard micro web framework, and Gunicorn, chosen for its performance, flexibility, and simplicity in production system configuration. The API receives customer queries as input and provides answers as output. The API was thoroughly load tested using Locust, an open-source performance/load testing tool for HTTP and other protocols, to ensure it meets real-time latency requirements in a production setting. Finally, the API was integrated with the Agent Workspace UI to deliver predictions to Contact Center agents, assisting customers in real-time.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1" style="font-size:90%;">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1" style="font-size:90%;">Improvement</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.4"></th>
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.1.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.1.1" style="font-size:90%;">Contextual Relevance</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.2.2.2"><span class="ltx_text" id="S4.T4.1.2.2.2.1" style="font-size:90%;">+48.14</span></td>
<td class="ltx_td ltx_border_t" id="S4.T4.1.2.2.3"></td>
<td class="ltx_td ltx_border_t" id="S4.T4.1.2.2.4"></td>
<td class="ltx_td ltx_border_t" id="S4.T4.1.2.2.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.3">
<td class="ltx_td ltx_align_left" id="S4.T4.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.3.1.1" style="font-size:90%;">Specificity</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.3.3.2"><span class="ltx_text" id="S4.T4.1.3.3.2.1" style="font-size:90%;">+97.97</span></td>
<td class="ltx_td" id="S4.T4.1.3.3.3"></td>
<td class="ltx_td" id="S4.T4.1.3.3.4"></td>
<td class="ltx_td" id="S4.T4.1.3.3.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.4">
<td class="ltx_td ltx_align_left" id="S4.T4.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.4.1.1" style="font-size:90%;">Completeness</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.4.4.2"><span class="ltx_text" id="S4.T4.1.4.4.2.1" style="font-size:90%;">+70.15</span></td>
<td class="ltx_td" id="S4.T4.1.4.4.3"></td>
<td class="ltx_td" id="S4.T4.1.4.4.4"></td>
<td class="ltx_td" id="S4.T4.1.4.4.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.5">
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.5.5.1.1" style="font-size:90%;">Accuracy</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.5.2"><span class="ltx_text" id="S4.T4.1.5.5.2.1" style="font-size:90%;">+45.69</span></td>
<td class="ltx_td" id="S4.T4.1.5.5.3"></td>
<td class="ltx_td" id="S4.T4.1.5.5.4"></td>
<td class="ltx_td" id="S4.T4.1.5.5.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.6">
<td class="ltx_td ltx_align_left" id="S4.T4.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.6.1.1" style="font-size:90%;">Hallucination Rate</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.6.6.2"><span class="ltx_text" id="S4.T4.1.6.6.2.1" style="font-size:90%;">-27.49</span></td>
<td class="ltx_td" id="S4.T4.1.6.6.3"></td>
<td class="ltx_td" id="S4.T4.1.6.6.4"></td>
<td class="ltx_td" id="S4.T4.1.6.6.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7.7">
<td class="ltx_td ltx_align_left" id="S4.T4.1.7.7.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.7.7.1.1" style="font-size:90%;">Missing Rate</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.1.7.7.2"><span class="ltx_text" id="S4.T4.1.7.7.2.1" style="font-size:90%;">-70.02</span></td>
<td class="ltx_td" id="S4.T4.1.7.7.3"></td>
<td class="ltx_td" id="S4.T4.1.7.7.4"></td>
<td class="ltx_td" id="S4.T4.1.7.7.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.8.8.1.1" style="font-size:90%;">Preference</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.8.8.2"><span class="ltx_text" id="S4.T4.1.8.8.2.1" style="font-size:90%;">+200.61</span></td>
<td class="ltx_td ltx_border_bb" id="S4.T4.1.8.8.3"></td>
<td class="ltx_td ltx_border_bb" id="S4.T4.1.8.8.4"></td>
<td class="ltx_td ltx_border_bb" id="S4.T4.1.8.8.5"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>
Human evaluation comparison (% diff.) between RAG and existing BERT-based ones.
</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Evaluation for ReAct and prompting techniques</h3>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Experiments with ReAct</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">To answer our third RQ, we utilized ReAct Tools to determine when to activate the information retrieval component within the RAG framework, while maintaining the same retrieval, embeddings, and generation strategies. We evaluated two scenarios: ”RAG with ReAct” and ”RAG without ReAct,” with K = 3. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.T5" title="Table 5 ‣ Experiments with ReAct ‣ 4.3. Evaluation for ReAct and prompting techniques ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">5</span></a>. While ReAct improved the accuracy by 7% and reduced hallucination by 13.5%, it resulted in slower performance <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.T6" title="Table 6 ‣ Experiments with ReAct ‣ 4.3. Evaluation for ReAct and prompting techniques ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">6</span></a>, making it inconvenient in real-time conversation.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1" style="font-size:70%;">K</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.2.1" style="font-size:70%;">Strategy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.3.1" style="font-size:70%;">Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.4.1" style="font-size:70%;">Hallucination Rate</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.5.1" style="font-size:70%;">Missing Rate</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.2.1.1"><span class="ltx_text" id="S4.T5.1.2.1.1.1" style="font-size:70%;">1</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.2.1.2.1" style="font-size:70%;">ReAct</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.3"><span class="ltx_text" id="S4.T5.1.2.1.3.1" style="font-size:70%;">-2.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.4"><span class="ltx_text" id="S4.T5.1.2.1.4.1" style="font-size:70%;">+51.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.5"><span class="ltx_text" id="S4.T5.1.2.1.5.1" style="font-size:70%;">-34.38</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.3.2.1"><span class="ltx_text" id="S4.T5.1.3.2.1.1" style="font-size:70%;">3</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.3.2.2.1" style="font-size:70%;">ReAct</span></th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.3"><span class="ltx_text" id="S4.T5.1.3.2.3.1" style="font-size:70%;">+7.08</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.4"><span class="ltx_text" id="S4.T5.1.3.2.4.1" style="font-size:70%;">-13.48</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.5"><span class="ltx_text" id="S4.T5.1.3.2.5.1" style="font-size:70%;">-19.38</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T5.1.4.3.1"><span class="ltx_text" id="S4.T5.1.4.3.1.1" style="font-size:70%;">3</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.4.3.2.1" style="font-size:70%;">CoVe</span></th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.3"><span class="ltx_text" id="S4.T5.1.4.3.3.1" style="font-size:70%;">-43.65</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.4"><span class="ltx_text" id="S4.T5.1.4.3.4.1" style="font-size:70%;">+27.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.5"><span class="ltx_text" id="S4.T5.1.4.3.5.1" style="font-size:70%;">+11.35</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T5.1.5.4.1"><span class="ltx_text" id="S4.T5.1.5.4.1.1" style="font-size:70%;">3</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T5.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.5.4.2.1" style="font-size:70%;">CoTP</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.1.5.4.3"><span class="ltx_text" id="S4.T5.1.5.4.3.1" style="font-size:70%;">-3.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.1.5.4.4"><span class="ltx_text" id="S4.T5.1.5.4.4.1" style="font-size:70%;">+1.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T5.1.5.4.5"><span class="ltx_text" id="S4.T5.1.5.4.5.1" style="font-size:70%;">+1.98</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 5. </span>
Comparison (% diff.) of ReAct RAG (with different values of k), CoVe and CoTP performance with respect to baseline on company data.
</figcaption>
</figure>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1.1" style="font-size:70%;">RAG Strategy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.2.1" style="font-size:70%;">95th Percentile</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.3.1" style="font-size:70%;">99th Percentile</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.1"><span class="ltx_text" id="S4.T6.1.2.1.1.1" style="font-size:70%;">reAct</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.2"><span class="ltx_text" id="S4.T6.1.2.1.2.1" style="font-size:70%;">4.0942</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.3"><span class="ltx_text" id="S4.T6.1.2.1.3.1" style="font-size:70%;">6.2084</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.1.3.2.1"><span class="ltx_text" id="S4.T6.1.3.2.1.1" style="font-size:70%;">non-reAct</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.1.3.2.2"><span class="ltx_text" id="S4.T6.1.3.2.2.1" style="font-size:70%;">0.8850</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T6.1.3.2.3"><span class="ltx_text" id="S4.T6.1.3.2.3.1" style="font-size:70%;">1.1678</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 6. </span>
Latency Comparison (seconds) between ReAct RAG and non-ReAct RAG based on 10000 queries
</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Prompting Techniques Experiments</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">We evaluated Chain of Verification (CoVe) <cite class="ltx_cite ltx_citemacro_citep">(Dhuliawala et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib11" title="">2023</a>)</cite> and Chain of Thought Prompting (CoTP) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib37" title="">2023</a>)</cite> to improve factual accuracy and reduce hallucinations. However, both techniques are time-consuming, requiring multiple LLM calls per query, and did not show significant improvements for the Company data. CoVe was 43% less accurate and CoTP was 3% less accurate (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.T5" title="Table 5 ‣ Experiments with ReAct ‣ 4.3. Evaluation for ReAct and prompting techniques ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">5</span></a>). Therefore, we decided against using these prompting techniques.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we demonstrate the practical challenges of implementing a RAG-based Response Prediction System in an industry setting. We evaluated various retrieval and embedding strategies combined with different prompting techniques to identify the best combinations for different use cases. Our evaluations show that retrieving relevant knowledge base articles and generating responses from LLMs can be more contextually relevant and accurate than BERT responses, which choose from the most relevant query-answer pairs. We also highlight that ReAct and advanced prompting techniques may not be practical for industry settings due to latency issues. Overall, our approach indicates that implementing RAG-based LLM response generation for contact centers is feasible and can effectively aid humans, reducing their workload. In the future, we plan to advance our work in three directions. Firstly, we aim to evaluate other LLMs. Secondly, we will test if query rewriting and reformulation can improve retrieval performance. Lastly, we intend to explore advanced RAG approaches to integrate knowledge bases from various sources.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Despite ongoing significant research, LLMs remain unpredictable. Though this paper showcases work on grounding the generated responses, LLMs to a certain degree are still capable of generating inaccurate information based on their learnt parametric memory. This work also does not focus on other LLM issues such as context length constraints, prompt injections and quality of Knowledge base data. Addressing other open challenge such as biases along with their ethical consideration are also not considered in the scope of this paper. The paper also does not address or evaluate RAG for multilingual data sources.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The data set used for training and validation of LLMs in this paper do not have any unbalanced views or opinions of individuals that might bias the generated response. The LLMs are still capable of generating inaccurate responses based their parametric memory even when relevant contextual information might be provided. Filtering toxic responses and prompt injections are also not considered in this evaluation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury,
Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard,
Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope,
Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou,
Denny Zhou, Slav Petrov, and Yonghui Wu. 2023.

</span>
<span class="ltx_bibblock">PaLM 2 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.10403 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baek et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, and Sujay Kumar Jauhar. 2024.

</span>
<span class="ltx_bibblock">Knowledge-augmented large language models for personalized contextual query suggestion. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the ACM on Web Conference 2024</em>. 3355–3366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bajaj et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018.

</span>
<span class="ltx_bibblock">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1611.09268 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv preprint arXiv:2004.05150</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2112.04426 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cer et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018.

</span>
<span class="ltx_bibblock">Universal Sentence Encoder.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1803.11175 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wei-Cheng Chang, Jyun-Yu Jiang, Jiong Zhang, Mutasem Al-Darabsah, Choon Hui Teo, Cho-Jui Hsieh, Hsiang-Fu Yu, and SVN Vishwanathan. 2024.

</span>
<span class="ltx_bibblock">PEFA: ParamEter-Free Adapters for large-scale embedding-based retrieval models. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</em>. 77–86.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al<span class="ltx_text" id="bib.bib9.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Chatbot arena: An open platform for evaluating llms by human preference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">arXiv preprint arXiv:2403.04132</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2022.

</span>
<span class="ltx_bibblock">Promptagator: Few-shot Dense Retrieval From 8 Examples.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2209.11755 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhuliawala et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Verification Reduces Hallucination in Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.11495 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandes and Oliveira (2021)</span>
<span class="ltx_bibblock">
Teresa Fernandes and Elisabete Oliveira. 2021.

</span>
<span class="ltx_bibblock">Understanding consumers’ acceptance of automated technologies in service encounters: Drivers of digital voice assistants adoption.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Journal of Business Research</em> 122 (2021), 180–191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freire et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Samuel Kernan Freire, Chaofan Wang, and Evangelos Niforatos. 2024.

</span>
<span class="ltx_bibblock">Chatbots in knowledge-intensive contexts: Comparing intent and llm-based systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">arXiv preprint arXiv:2402.04955</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friedman et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Luke Friedman, Sameer Ahuja, David Allen, Zhenning Tan, Hakim Sidahmed, Changbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et al<span class="ltx_text" id="bib.bib14.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Leveraging large language models in conversational recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.4.1">arXiv preprint arXiv:2305.07961</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2024.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2312.10997 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2020)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2020.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2007.01282</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Atlas: Few-shot Learning with Retrieval Augmented Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2208.03299 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023.

</span>
<span class="ltx_bibblock">Atlas: Few-shot Learning with Retrieval Augmented Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Journal of Machine Learning Research</em> 24, 251 (2023), 1–43.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v24/23-0037.html" title="">http://jmlr.org/papers/v24/23-0037.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 1601–1611.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/P17-1147" title="">https://doi.org/10.18653/v1/P17-1147</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020.

</span>
<span class="ltx_bibblock">Generalization through Memorization: Nearest Neighbor Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1911.00172 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2005.11401 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.

</span>
<span class="ltx_bibblock">Lost in the Middle: How Language Models Use Long Contexts.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2307.03172 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin (2018)</span>
<span class="ltx_bibblock">
Yu. A. Malkov and D. A. Yashunin. 2018.

</span>
<span class="ltx_bibblock">Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1603.09320 [cs.DS]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ Questions for Machine Comprehension of Text. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, Jian Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Linguistics, Austin, Texas, 2383–2392.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/D16-1264" title="">https://doi.org/10.18653/v1/D16-1264</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1908.10084 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Research (2020)</span>
<span class="ltx_bibblock">
Google Research. 2020.

</span>
<span class="ltx_bibblock">ScaNN: Efficient Vector Similarity Search.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google-research/google-research/tree/master/scann" title="">https://github.com/google-research/google-research/tree/master/scann</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubin et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.

</span>
<span class="ltx_bibblock">Learning To Retrieve Prompts for In-Context Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2112.08633 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">kNN-Prompt: Nearest Neighbor Zero-Shot Inference.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2205.13792 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023.

</span>
<span class="ltx_bibblock">REPLUG: Retrieval-Augmented Black-Box Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2301.12652 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al<span class="ltx_text" id="bib.bib30.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.4.1">arXiv preprint arXiv:2208.03188</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siriwardhana et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023.

</span>
<span class="ltx_bibblock">Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Transactions of the Association for Computational Linguistics</em> 11 (2023), 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stiennon et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020.

</span>
<span class="ltx_bibblock">Learning to summarize with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Advances in Neural Information Processing Systems</em> 33 (2020), 3008–3021.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023.

</span>
<span class="ltx_bibblock">Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.10168 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian (2023)</span>
<span class="ltx_bibblock">
Edward Tian. 2023.

</span>
<span class="ltx_bibblock">GPTZero.

</span>
<span class="ltx_bibblock">Online; accessed 23-Mar-2023.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gptzero.me/" title="">https://gptzero.me/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tonmoy et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024.

</span>
<span class="ltx_bibblock">A comprehensive survey of hallucination mitigation techniques in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">arXiv preprint arXiv:2401.01313</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. 2023.

</span>
<span class="ltx_bibblock">KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.11761 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2201.11903 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wortmann et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Andreas Wortmann, Olivier Barais, Benoit Combemale, and Manuel Wimmer. 2020.

</span>
<span class="ltx_bibblock">Modeling languages in Industry 4.0: an extended systematic mapping study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Software and Systems Modeling</em> 19 (2020), 67–94.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Laurie Wu, Alei Fan, Yang Yang, and Zeya He. 2022.

</span>
<span class="ltx_bibblock">Tech-touch balance in the service encounter: The impact of supplementary human service on consumer responses.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">International Journal of Hospitality Management</em> 101 (2022), 103122.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. 2023.

</span>
<span class="ltx_bibblock">Human preference score: Better aligning text-to-image models with human preference. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 2096–2105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">arXiv preprint arXiv:2404.17723</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin Lu. 2023.

</span>
<span class="ltx_bibblock">Palr: Personalization aware llms for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">arXiv preprint arXiv:2305.07622</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023.

</span>
<span class="ltx_bibblock">ReAct: Synergizing Reasoning and Acting in Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2210.03629 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Multimodal Language Modeling.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2211.12561 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu (2022)</span>
<span class="ltx_bibblock">
Wenhao Yu. 2022.

</span>
<span class="ltx_bibblock">Retrieval-augmented Generation across Heterogeneous Knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop</em>, Daphne Ippolito, Liunian Harold Li, Maria Leonor Pacheco, Danqi Chen, and Nianwen Xue (Eds.). Association for Computational Linguistics, Hybrid: Seattle, Washington + Online, 52–58.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2022.naacl-srw.7" title="">https://doi.org/10.18653/v1/2022.naacl-srw.7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023.

</span>
<span class="ltx_bibblock">Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.17331 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.

</span>
<span class="ltx_bibblock">AlignScore: Evaluating Factual Consistency with A Unified Alignment Function. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 11328–11348.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Kai Zhang, Fubang Zhao, Yangyang Kang, and Xiaozhong Liu. 2023b.

</span>
<span class="ltx_bibblock">Memory-augmented llm personalization with short-and long-term memory coordination.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">arXiv preprint arXiv:2309.11696</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023a.

</span>
<span class="ltx_bibblock">Retrieve Anything To Augment Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.07554 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for ai-generated content: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">arXiv preprint arXiv:2402.19473</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Evaluation of RAG Approach for Open Source Datasets</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">To evaluate the effectiveness of the RAG-based approach, we also conducted a sample study using three open-domain datasets following a similar methodology.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Dataset Statistics</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We consider three popular open source question answering datasets focusing on a varitey of topics. A random subset of <span class="ltx_text ltx_font_bold" id="A1.SS1.p1.1.1">MS MARCO<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="A1.SS1.p1.1.1.1.1">(</span>Bajaj et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="A1.SS1.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib4" title="">2018</a><span class="ltx_text ltx_font_medium" id="A1.SS1.p1.1.1.3.3">)</span></cite></span>, <span class="ltx_text ltx_font_bold" id="A1.SS1.p1.1.2">SQuAD</span> <cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib24" title="">2016</a>)</cite>, <span class="ltx_text ltx_font_bold" id="A1.SS1.p1.1.3">TriviaQA</span> <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#bib.bib19" title="">2017</a>)</cite> were considered for the evaluation. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.T7" title="Table 7 ‣ A.1. Dataset Statistics ‣ Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">7</span></a> shows the brief overview of the considered datasets.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T7.1" style="width:208.1pt;height:88.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.0pt,1.3pt) scale(0.97215516670602,0.97215516670602) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T7.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T7.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1.1"><span class="ltx_text" id="A1.T7.1.1.1.1.1.1" style="font-size:50%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.1.1.1.1.2.1">
<span class="ltx_p" id="A1.T7.1.1.1.1.2.1.1"><span class="ltx_text" id="A1.T7.1.1.1.1.2.1.1.1" style="font-size:50%;">Unique</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.2.1.2"><span class="ltx_text" id="A1.T7.1.1.1.1.2.1.2.1" style="font-size:50%;">Docs</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.1.1.1.1.3.1">
<span class="ltx_p" id="A1.T7.1.1.1.1.3.1.1"><span class="ltx_text" id="A1.T7.1.1.1.1.3.1.1.1" style="font-size:50%;">Ave.</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.3.1.2"><span class="ltx_text" id="A1.T7.1.1.1.1.3.1.2.1" style="font-size:50%;">Doc.</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.3.1.3"><span class="ltx_text" id="A1.T7.1.1.1.1.3.1.3.1" style="font-size:50%;">Tokens</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.1.1.1.1.4.1">
<span class="ltx_p" id="A1.T7.1.1.1.1.4.1.1"><span class="ltx_text" id="A1.T7.1.1.1.1.4.1.1.1" style="font-size:50%;">Unique</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.4.1.2"><span class="ltx_text" id="A1.T7.1.1.1.1.4.1.2.1" style="font-size:50%;">Quest-</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.4.1.3"><span class="ltx_text" id="A1.T7.1.1.1.1.4.1.3.1" style="font-size:50%;">ions</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.1.1.1.1.5.1">
<span class="ltx_p" id="A1.T7.1.1.1.1.5.1.1"><span class="ltx_text" id="A1.T7.1.1.1.1.5.1.1.1" style="font-size:50%;">Ave.</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.5.1.2"><span class="ltx_text" id="A1.T7.1.1.1.1.5.1.2.1" style="font-size:50%;">Question</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.5.1.3"><span class="ltx_text" id="A1.T7.1.1.1.1.5.1.3.1" style="font-size:50%;">Tokens</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.1.1.1.1.6.1">
<span class="ltx_p" id="A1.T7.1.1.1.1.6.1.1"><span class="ltx_text" id="A1.T7.1.1.1.1.6.1.1.1" style="font-size:50%;">Unique</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.6.1.2"><span class="ltx_text" id="A1.T7.1.1.1.1.6.1.2.1" style="font-size:50%;">Answers</span></span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.1.1.1.1.7.1">
<span class="ltx_p" id="A1.T7.1.1.1.1.7.1.1"><span class="ltx_text" id="A1.T7.1.1.1.1.7.1.1.1" style="font-size:50%;">Ave.</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.7.1.2"><span class="ltx_text" id="A1.T7.1.1.1.1.7.1.2.1" style="font-size:50%;">Answer</span></span>
<span class="ltx_p" id="A1.T7.1.1.1.1.7.1.3"><span class="ltx_text" id="A1.T7.1.1.1.1.7.1.3.1" style="font-size:50%;">Tokens</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.2.1.1"><span class="ltx_text" id="A1.T7.1.1.2.1.1.1" style="font-size:50%;">MS-MARCO</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.2.1.2"><span class="ltx_text" id="A1.T7.1.1.2.1.2.1" style="font-size:50%;">4997</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.2.1.3"><span class="ltx_text" id="A1.T7.1.1.2.1.3.1" style="font-size:50%;">58.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.2.1.4"><span class="ltx_text" id="A1.T7.1.1.2.1.4.1" style="font-size:50%;">5000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.2.1.5"><span class="ltx_text" id="A1.T7.1.1.2.1.5.1" style="font-size:50%;">5.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.2.1.6"><span class="ltx_text" id="A1.T7.1.1.2.1.6.1" style="font-size:50%;">4999</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.2.1.7"><span class="ltx_text" id="A1.T7.1.1.2.1.7.1" style="font-size:50%;">14.37</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.3.2.1"><span class="ltx_text" id="A1.T7.1.1.3.2.1.1" style="font-size:50%;">SQUAD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.3.2.2"><span class="ltx_text" id="A1.T7.1.1.3.2.2.1" style="font-size:50%;">5000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.3.2.3"><span class="ltx_text" id="A1.T7.1.1.3.2.3.1" style="font-size:50%;">94.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.3.2.4"><span class="ltx_text" id="A1.T7.1.1.3.2.4.1" style="font-size:50%;">4994</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.3.2.5"><span class="ltx_text" id="A1.T7.1.1.3.2.5.1" style="font-size:50%;">10.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.1.3.2.6"><span class="ltx_text" id="A1.T7.1.1.3.2.6.1" style="font-size:50%;">4413</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.1.3.2.7"><span class="ltx_text" id="A1.T7.1.1.3.2.7.1" style="font-size:50%;">2.59</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T7.1.1.4.3.1"><span class="ltx_text" id="A1.T7.1.1.4.3.1.1" style="font-size:50%;">TRIVIA</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T7.1.1.4.3.2"><span class="ltx_text" id="A1.T7.1.1.4.3.2.1" style="font-size:50%;">3530</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T7.1.1.4.3.3"><span class="ltx_text" id="A1.T7.1.1.4.3.3.1" style="font-size:50%;">4321.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T7.1.1.4.3.4"><span class="ltx_text" id="A1.T7.1.1.4.3.4.1" style="font-size:50%;">4087</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T7.1.1.4.3.5"><span class="ltx_text" id="A1.T7.1.1.4.3.5.1" style="font-size:50%;">12.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T7.1.1.4.3.6"><span class="ltx_text" id="A1.T7.1.1.4.3.6.1" style="font-size:50%;">3471</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T7.1.1.4.3.7"><span class="ltx_text" id="A1.T7.1.1.4.3.7.1" style="font-size:50%;">1.67</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_table">Table 7. </span>
Open source dataset random sample statistics
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Retrieval and Embedding Performance</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We observed a specific trend in Recall values in lower k values (1, 3) versus higher k values (5, 10) for SQuAD and TRIVIA. For SQuAD, Vertex AI textembedding-gecko@001 (768) embedding with ScaNN retrieval performed the best at lower k but at higher k, SBERT-all-mpnet-base-v2 (768) with ScaNN performed better. For TRIVIA, SBERTall-mpnet-base-v2 (768) embedding with HNSW KNN retrieval performed the best at lower k but at higher k, SBERT-all-mpnet-base-v2 (768) with ScaNN performed better. For MSMARCO, Vertex AI -textembedding-gecko@001 (768) embedding with ScaNN retrieval combination was a clear winner. Refer Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.T8" title="Table 8 ‣ A.2. Retrieval and Embedding Performance ‣ Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">8</span></a> for more details.</p>
</div>
<figure class="ltx_table" id="A1.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T8.1" style="width:433.6pt;height:244.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-86.9pt,48.9pt) scale(0.713874666911309,0.713874666911309) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T8.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.2.1">Embedding Strategy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.3.1">Retrieval Strategy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.4.1">Recall @ 1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.5.1">Recall @ 3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.6.1">Recall @ 5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T8.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.7.1">Recall @ 10</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.1.2.1.1">SQUAD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.1.2.1.2">USE (512)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.1.2.1.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.1.2.1.4">0.4188</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.1.2.1.5">0.5946</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.1.2.1.6">0.6634</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.2.1.7">0.7424</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.3.2.1">SQUAD</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.3.2.2">SBERT - all-mpnet-base-v2 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.3.2.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.3.2.4">0.6708</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.3.2.5">0.8444</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.3.2.6">0.8902</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.3.2.7">0.9336</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.4.3.1">SQUAD</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.4.3.2">Vertex AI - textembedding-gecko@001 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.4.3.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.4.3.4">0.6958</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.4.3.5">0.8486</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.4.3.6">0.8804</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.4.3.7">0.911</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.5.4.1">SQUAD</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.5.4.2">USE (512)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.5.4.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.5.4.4">0.4282</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.5.4.5">0.6116</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.5.4.6">0.6834</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.5.4.7">0.7666</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.6.5.1">SQUAD</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.6.5.2">SBERT - all-mpnet-base-v2 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.6.5.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.6.5.4">0.685</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.6.5.5">0.8636</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.6.5.6">0.913</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.6.5.7">0.9584</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.7.6.1">SQUAD</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.7.6.2">Vertex AI - textembedding-gecko@001 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.7.6.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.7.6.4">0.7156</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.7.6.5">0.874</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.7.6.6">0.908</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.7.6.7">0.9414</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.8.7.1">TRIVIA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.8.7.2">USE (512)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.8.7.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.8.7.4">0.459</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.8.7.5">0.6004</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.8.7.6">0.6604</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.8.7.7">0.7333</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.9.8.1">TRIVIA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.9.8.2">SBERT - all-mpnet-base-v2 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.9.8.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.9.8.4">0.793</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.9.8.5">0.8691</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.9.8.6">0.8921</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.9.8.7">0.9171</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.10.9.1">TRIVIA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.10.9.2">Vertex AI - textembedding-gecko@001 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.10.9.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.10.9.4">0.6423</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.10.9.5">0.7487</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.10.9.6">0.782</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.10.9.7">0.8233</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.11.10.1">TRIVIA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.11.10.2">USE (512)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.11.10.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.11.10.4">0.4101</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.11.10.5">0.6039</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.11.10.6">0.6687</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.11.10.7">0.7548</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.12.11.1">TRIVIA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.12.11.2">SBERT - all-mpnet-base-v2 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.12.11.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.12.11.4">0.7086</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.12.11.5">0.8654</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.12.11.6">0.8992</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.12.11.7">0.9288</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.13.12.1">TRIVIA</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.13.12.2">Vertex AI - textembedding-gecko@001 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.13.12.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.13.12.4">0.5936</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.13.12.5">0.759</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.13.12.6">0.8038</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.13.12.7">0.8537</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.14.13.1">MS-MARCO</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.14.13.2">USE (512)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.14.13.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.14.13.4">0.5263</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.14.13.5">0.6856</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.14.13.6">0.7347</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.14.13.7">0.784</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.15.14.1">MS-MARCO</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.15.14.2">SBERT - all-mpnet-base-v2 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.15.14.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.15.14.4">0.9128</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.15.14.5">0.9798</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.15.14.6">0.9878</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.15.14.7">0.9925</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.16.15.1">MS-MARCO</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.16.15.2">Vertex AI - textembedding-gecko@001 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.16.15.3">HNSW KNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.16.15.4">0.8194</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.16.15.5">0.9241</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.16.15.6">0.9425</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.16.15.7">0.9577</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.17.16.1">MS-MARCO</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.17.16.2">USE (512)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.17.16.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.17.16.4">0.5347</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.17.16.5">0.6996</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.17.16.6">0.7518</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.17.16.7">0.8035</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.18.17.1">MS-MARCO</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.18.17.2">SBERT - all-mpnet-base-v2 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.18.17.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.18.17.4">0.9132</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.18.17.5">0.9816</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.1.18.17.6">0.9896</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.1.18.17.7">0.9944</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.19.18">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.1.19.18.1">MS-MARCO</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.1.19.18.2">Vertex AI - textembedding-gecko@001 (768)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.1.19.18.3">ScaNN</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.1.19.18.4">0.8296</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.1.19.18.5">0.9376</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.1.19.18.6">0.9581</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.1.1.19.18.7">0.9738</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8. </span>
Recall@K for retrieval and embedding strategies for different data sets.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>Evaluation of Generated Responses</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.T9" title="Table 9 ‣ A.3. Evaluation of Generated Responses ‣ Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">9</span></a> shows the accuracy, hallucination, and missing rate for the open sources datasets (through automated evaluation as described in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#S4.SS2.SSS1" title="4.2.1. Automated evaluations ‣ 4.2. Response Prediction System Evaluation ‣ 4. Results and Findings ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>. As observed in the Retrieval evaluation section, we notice a similar relationship in the accuracy and hallucination as document size increases. From Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.T7" title="Table 7 ‣ A.1. Dataset Statistics ‣ Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">7</span></a>, we observe that the token length of the TriviaQA documents is much larger than MS-MARCO and SQuAD. Similarlt, we observed lower accuracy and higher hallucination rates with TriviaQA when compared to MS-MARCO and SQuAD.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.T10" title="Table 10 ‣ A.3. Evaluation of Generated Responses ‣ Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">10</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.03708v2#A1.T11" title="Table 11 ‣ A.3. Evaluation of Generated Responses ‣ Appendix A Evaluation of RAG Approach for Open Source Datasets ‣ RAG based Question-Answering for Contextual Response Prediction System"><span class="ltx_text ltx_ref_tag">11</span></a> show performance of Chain of Thought Prompting and Chain of Verification performance on open-source datasets.
Accuracy and hallucination rate improvement vary based on the open source dataset.</p>
</div>
<figure class="ltx_table" id="A1.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T9.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A1.T9.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1" style="font-size:70%;">k</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.2.1" style="font-size:70%;">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.3.1" style="font-size:70%;">Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.4.1" style="font-size:70%;">Hallucination</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.5.1" style="font-size:70%;">Missing</span></th>
</tr>
<tr class="ltx_tr" id="A1.T9.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="A1.T9.1.2.2.1"></th>
<th class="ltx_td ltx_th ltx_th_column" id="A1.T9.1.2.2.2"></th>
<th class="ltx_td ltx_th ltx_th_column" id="A1.T9.1.2.2.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.2.2.4.1" style="font-size:70%;">rate</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T9.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A1.T9.1.2.2.5.1" style="font-size:70%;">rate</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T9.1.3.1.1"><span class="ltx_text" id="A1.T9.1.3.1.1.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.3.1.2"><span class="ltx_text" id="A1.T9.1.3.1.2.1" style="font-size:70%;">SQUAD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.3.1.3"><span class="ltx_text" id="A1.T9.1.3.1.3.1" style="font-size:70%;">84.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.3.1.4"><span class="ltx_text" id="A1.T9.1.3.1.4.1" style="font-size:70%;">8.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.3.1.5"><span class="ltx_text" id="A1.T9.1.3.1.5.1" style="font-size:70%;">6.9</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T9.1.4.2.1"><span class="ltx_text" id="A1.T9.1.4.2.1.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="A1.T9.1.4.2.2"><span class="ltx_text" id="A1.T9.1.4.2.2.1" style="font-size:70%;">TriviaQA</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.4.2.3"><span class="ltx_text" id="A1.T9.1.4.2.3.1" style="font-size:70%;">58.48</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.4.2.4"><span class="ltx_text" id="A1.T9.1.4.2.4.1" style="font-size:70%;">28.68</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.4.2.5"><span class="ltx_text" id="A1.T9.1.4.2.5.1" style="font-size:70%;">12.8</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T9.1.5.3.1"><span class="ltx_text" id="A1.T9.1.5.3.1.1" style="font-size:70%;">1</span></th>
<td class="ltx_td ltx_align_center" id="A1.T9.1.5.3.2"><span class="ltx_text" id="A1.T9.1.5.3.2.1" style="font-size:70%;">MS-MARCO</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.5.3.3"><span class="ltx_text" id="A1.T9.1.5.3.3.1" style="font-size:70%;">89.06</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.5.3.4"><span class="ltx_text" id="A1.T9.1.5.3.4.1" style="font-size:70%;">7.06</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.5.3.5"><span class="ltx_text" id="A1.T9.1.5.3.5.1" style="font-size:70%;">3.86</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T9.1.6.4.1"><span class="ltx_text" id="A1.T9.1.6.4.1.1" style="font-size:70%;">3</span></th>
<td class="ltx_td ltx_align_center" id="A1.T9.1.6.4.2"><span class="ltx_text" id="A1.T9.1.6.4.2.1" style="font-size:70%;">SQUAD</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.6.4.3"><span class="ltx_text" id="A1.T9.1.6.4.3.1" style="font-size:70%;">91.32</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.6.4.4"><span class="ltx_text" id="A1.T9.1.6.4.4.1" style="font-size:70%;">5.48</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.6.4.5"><span class="ltx_text" id="A1.T9.1.6.4.5.1" style="font-size:70%;">3.2</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A1.T9.1.7.5.1"><span class="ltx_text" id="A1.T9.1.7.5.1.1" style="font-size:70%;">3</span></th>
<td class="ltx_td ltx_align_center" id="A1.T9.1.7.5.2"><span class="ltx_text" id="A1.T9.1.7.5.2.1" style="font-size:70%;">TriviaQA</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.7.5.3"><span class="ltx_text" id="A1.T9.1.7.5.3.1" style="font-size:70%;">25.08</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.7.5.4"><span class="ltx_text" id="A1.T9.1.7.5.4.1" style="font-size:70%;">67.41</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.7.5.5"><span class="ltx_text" id="A1.T9.1.7.5.5.1" style="font-size:70%;">7.46</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="A1.T9.1.8.6.1"><span class="ltx_text" id="A1.T9.1.8.6.1.1" style="font-size:70%;">3</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.1.8.6.2"><span class="ltx_text" id="A1.T9.1.8.6.2.1" style="font-size:70%;">MS-MARCO</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.1.8.6.3"><span class="ltx_text" id="A1.T9.1.8.6.3.1" style="font-size:70%;">89.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.1.8.6.4"><span class="ltx_text" id="A1.T9.1.8.6.4.1" style="font-size:70%;">6.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.1.8.6.5"><span class="ltx_text" id="A1.T9.1.8.6.5.1" style="font-size:70%;">3.16</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 9. </span>
Generation quality metrics (%) using text-bison@001 and ScaNN
</figcaption>
</figure>
<figure class="ltx_table" id="A1.T10">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T10.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T10.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T10.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T10.1.1.1.1.1" style="font-size:50%;">dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="A1.T10.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T10.1.1.1.2.1" style="font-size:50%;">accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="A1.T10.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T10.1.1.1.3.1" style="font-size:50%;">hallucination_rate</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="A1.T10.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T10.1.1.1.4.1" style="font-size:50%;">missing_rate</span></th>
</tr>
<tr class="ltx_tr" id="A1.T10.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T10.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.T10.1.2.2.1.1" style="font-size:50%;">baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T10.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T10.1.2.2.2.1" style="font-size:50%;">CoTP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T10.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A1.T10.1.2.2.3.1" style="font-size:50%;">baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T10.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A1.T10.1.2.2.4.1" style="font-size:50%;">CoTP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T10.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A1.T10.1.2.2.5.1" style="font-size:50%;">baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T10.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A1.T10.1.2.2.6.1" style="font-size:50%;">CoTP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T10.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.1.3.1.1"><span class="ltx_text" id="A1.T10.1.3.1.1.1" style="font-size:50%;">SQUAD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.1.3.1.2"><span class="ltx_text" id="A1.T10.1.3.1.2.1" style="font-size:50%;">98.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.1.3.1.3"><span class="ltx_text" id="A1.T10.1.3.1.3.1" style="font-size:50%;">94.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.1.3.1.4"><span class="ltx_text" id="A1.T10.1.3.1.4.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.1.3.1.5"><span class="ltx_text" id="A1.T10.1.3.1.5.1" style="font-size:50%;">4.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.1.3.1.6"><span class="ltx_text" id="A1.T10.1.3.1.6.1" style="font-size:50%;">0.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.1.3.1.7"><span class="ltx_text" id="A1.T10.1.3.1.7.1" style="font-size:50%;">1.9</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.4.2">
<td class="ltx_td ltx_align_center" id="A1.T10.1.4.2.1"><span class="ltx_text" id="A1.T10.1.4.2.1.1" style="font-size:50%;">TRIVIA</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.4.2.2"><span class="ltx_text" id="A1.T10.1.4.2.2.1" style="font-size:50%;">63.95</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.4.2.3"><span class="ltx_text" id="A1.T10.1.4.2.3.1" style="font-size:50%;">86.27</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.4.2.4"><span class="ltx_text" id="A1.T10.1.4.2.4.1" style="font-size:50%;">22.85</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.4.2.5"><span class="ltx_text" id="A1.T10.1.4.2.5.1" style="font-size:50%;">6.45</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.4.2.6"><span class="ltx_text" id="A1.T10.1.4.2.6.1" style="font-size:50%;">13.18</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.4.2.7"><span class="ltx_text" id="A1.T10.1.4.2.7.1" style="font-size:50%;">6.38</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.1.5.3.1"><span class="ltx_text" id="A1.T10.1.5.3.1.1" style="font-size:50%;">MS-MARCO</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.1.5.3.2"><span class="ltx_text" id="A1.T10.1.5.3.2.1" style="font-size:50%;">92.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.1.5.3.3"><span class="ltx_text" id="A1.T10.1.5.3.3.1" style="font-size:50%;">90.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.1.5.3.4"><span class="ltx_text" id="A1.T10.1.5.3.4.1" style="font-size:50%;">4.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.1.5.3.5"><span class="ltx_text" id="A1.T10.1.5.3.5.1" style="font-size:50%;">4.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.1.5.3.6"><span class="ltx_text" id="A1.T10.1.5.3.6.1" style="font-size:50%;">3.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.1.5.3.7"><span class="ltx_text" id="A1.T10.1.5.3.7.1" style="font-size:50%;">4.74</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:50%;"><span class="ltx_tag ltx_tag_table">Table 10. </span>
Baseline vs CoTP evaluation metrics (%) without retrieval (question-doc. pair used as it is)
</figcaption>
</figure>
<figure class="ltx_table" id="A1.T11">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T11.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T11.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T11.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T11.1.1.1.1.1" style="font-size:50%;">dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="A1.T11.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T11.1.1.1.2.1" style="font-size:50%;">accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="A1.T11.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T11.1.1.1.3.1" style="font-size:50%;">hallucination_rate</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="A1.T11.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T11.1.1.1.4.1" style="font-size:50%;">missing_rate</span></th>
</tr>
<tr class="ltx_tr" id="A1.T11.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T11.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.T11.1.2.2.1.1" style="font-size:50%;">baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T11.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T11.1.2.2.2.1" style="font-size:50%;">CoVe</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T11.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A1.T11.1.2.2.3.1" style="font-size:50%;">baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T11.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A1.T11.1.2.2.4.1" style="font-size:50%;">CoVe</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T11.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A1.T11.1.2.2.5.1" style="font-size:50%;">baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T11.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A1.T11.1.2.2.6.1" style="font-size:50%;">CoVe</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T11.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.1.3.1.1"><span class="ltx_text" id="A1.T11.1.3.1.1.1" style="font-size:50%;">SQUAD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.1.3.1.2"><span class="ltx_text" id="A1.T11.1.3.1.2.1" style="font-size:50%;">98.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.1.3.1.3"><span class="ltx_text" id="A1.T11.1.3.1.3.1" style="font-size:50%;">95.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.1.3.1.4"><span class="ltx_text" id="A1.T11.1.3.1.4.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.1.3.1.5"><span class="ltx_text" id="A1.T11.1.3.1.5.1" style="font-size:50%;">3.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.1.3.1.6"><span class="ltx_text" id="A1.T11.1.3.1.6.1" style="font-size:50%;">0.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.1.3.1.7"><span class="ltx_text" id="A1.T11.1.3.1.7.1" style="font-size:50%;">0.8</span></td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.4.2">
<td class="ltx_td ltx_align_center" id="A1.T11.1.4.2.1"><span class="ltx_text" id="A1.T11.1.4.2.1.1" style="font-size:50%;">TRIVIA</span></td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.4.2.2"><span class="ltx_text" id="A1.T11.1.4.2.2.1" style="font-size:50%;">63.95</span></td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.4.2.3"><span class="ltx_text" id="A1.T11.1.4.2.3.1" style="font-size:50%;">63.76</span></td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.4.2.4"><span class="ltx_text" id="A1.T11.1.4.2.4.1" style="font-size:50%;">22.85</span></td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.4.2.5"><span class="ltx_text" id="A1.T11.1.4.2.5.1" style="font-size:50%;">23.83</span></td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.4.2.6"><span class="ltx_text" id="A1.T11.1.4.2.6.1" style="font-size:50%;">13.18</span></td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.4.2.7"><span class="ltx_text" id="A1.T11.1.4.2.7.1" style="font-size:50%;">12.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T11.1.5.3.1"><span class="ltx_text" id="A1.T11.1.5.3.1.1" style="font-size:50%;">MS-MARCO</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T11.1.5.3.2"><span class="ltx_text" id="A1.T11.1.5.3.2.1" style="font-size:50%;">92.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T11.1.5.3.3"><span class="ltx_text" id="A1.T11.1.5.3.3.1" style="font-size:50%;">92.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T11.1.5.3.4"><span class="ltx_text" id="A1.T11.1.5.3.4.1" style="font-size:50%;">4.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T11.1.5.3.5"><span class="ltx_text" id="A1.T11.1.5.3.5.1" style="font-size:50%;">5.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T11.1.5.3.6"><span class="ltx_text" id="A1.T11.1.5.3.6.1" style="font-size:50%;">3.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T11.1.5.3.7"><span class="ltx_text" id="A1.T11.1.5.3.7.1" style="font-size:50%;">1.86</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:50%;"><span class="ltx_tag ltx_tag_table">Table 11. </span>
Baseline vs CoVe evaluation metrics (%) without retrieval (question-doc. pair used as it is)
</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Prompt Examples</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We utilize LLMs for various tasks in our methodology, including question-answer pair generation from knowledge base articles, response generation, factual accuracy evaluation, and advanced CoTP &amp; CoV prompts. Therefore, we include the specific prompts used for these different tasks.</p>
</div>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Prompt for answer generation</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1">You are a reading comprehension and answer generation expert. Please answer the question from the document provided. If the document is not related to the question, simply reply: ”Sorry, I cannot answer this question”. Following are the guidelines you need to follow for generating the responses:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p2.1">1) They should always be professional, positive, friendly, and empathetic.
2) They should not contain words that have a negative connotation (Example: ”unfortunately”).
3) They should always be truthful and honest.
4) They should always be STRICTLY less than 30 words. If the generated response if greater than 30 words, rephrase and make it less than 30 words.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p3.1">document: ¡retrieved_document¿,</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p4">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p4.1">question: ¡question¿,</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p5">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p5.1">output:</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Prompt for Hallucination Judgement</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p2.1">You need to check whether the prediction of a question-answering systems to a question is correct. You should make the judgement based on a list of ground truth answers provided to you. You response should be ”correct” if the prediction is correct or ”incorrect” if the prediction is wrong. Your response should be ”unsure” where there is a valid ground truth and prediction is ”Sorry, I don’t know.” or if you are not confident if the prediction is correct.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p3.1">Below are the different cases possible:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p4">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p4.1">1) Examples where you should return ”correct”.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p5">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p5.1">Question: What is the customer registration process?</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p6">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p6.1">Ground Truth: The customer registration process is a way for customers to create an account with them. This allows them to track their purchases, receive personalized offers, and more. The process is simple and can be completed in a few minutes.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p7">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p7.1">Prediction: The customer registration process is a process that allows customers to register their information with them. This process allows customers to receive benefits such as discounts, special offers, and personalized shopping experiences.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p8">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p8.1">Correctness: correct</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p9">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p9.1">Question: What happens if my refund is pending?</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p10">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p10.1">Ground Truth: Sorry, I don’t know.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p11">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p11.1">Prediction: Sorry, I don’t know.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p12">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p12.1">Correctness: correct

<br class="ltx_break"/>2) Examples where you should return ”incorrect”.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p13">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p13.1">Question: What do I need to do to get the military discount?</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p14">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p14.1">Ground Truth: You need to have a smartphone and be registered for the discount. If you don’t have a smartphone, you can use discount code RC5. If you are in the pilot 425 stores area, you can key in your phone number.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p15">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p15.1">Prediction: The military discount is available to active duty military members, veterans, and their families. The discount is 10 percent off eligible purchases.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p16">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p16.1">Correctness: incorrect</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p17">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p17.1">Question: How do I apply for the consumer card?</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p18">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p18.1">Ground Truth: Sorry, I don’t know.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p19">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p19.1">Prediction: You can apply for the consumer card in-store, online or by mail.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p20">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p20.1">Correctness: incorrect</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p21">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p21.1">3) Examples where you should return ”unsure”.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p22">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p22.1">Question: What is the Return Policy?</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p23">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p23.1">Ground Truth: The Return Policy is available on the website. You can find it by searching for ”Return Policy” or by clicking on the link in the article.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p24">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p24.1">Prediction: Sorry, I don’t know.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p25">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p25.1">Correctness: unsure</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p26">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p26.1">Provide correctness for the below question, ground truth and prediction:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p27">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p27.1">Question: ¡question¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p28">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p28.1">Ground Truth: ¡ground truth¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p29">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p29.1">Prediction: ¡prediction¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p30">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p30.1">Correctness:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p31">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p31.1"><span class="ltx_text ltx_font_bold" id="A2.SS0.SSS0.Px2.p31.1.1">Prompt for Chain of Prompting</span>:
Prompt for quote extraction: You are a reading comprehension and quote extraction expert. Please extract, word-for-word, any quotes relevant to the question. If there are no quotes in this document that seem relevant to the provided question, please say ”I can’t find any relevant quotes”.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p32">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p32.1">For document: ¡document¿,</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p33">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p33.1">question: ¡question¿,</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p34">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p34.1">output:</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Prompt for Generating Baseline Response and Plan Verification (Chain of Verification)</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p1.1">:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p2.1">Below is a question:
¡question¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p3">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p3.1">Below is the document from which the answer should be generated:
¡document¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p4">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p4.1">You are an subject matter expert working at Contact Centers. Your expertise includes quote extraction, answer generation, and asking verification questions to improve the overall factual accuracy of the answers you provide.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p5">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p5.1">Your first goal is to extract, word-for-word, any quotes relevant to the question that could be used to answer the question. If there are no quotes in this document that seem relevant to the provided question, simply return: ”I can’t find any relevant quotes”.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p6">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p6.1">Your second goal is to use *solely* the quotes extracted from the first goal and generate a concise and accurate answer (using the below listed guideline) by rephrasing the quotes to answer the question. If the quotes could not be used to answer the question, simply return: ”Sorry, I cannot answer this question”.
1) They should always be professional, positive, friendly, and empathetic.
2) They should not contain words that have a negative connotation (Example: ”unfortunately”).
3) They should always be truthful and honest.
4) They should always be STRICTLY less than 30 words. If the generated response if greater than 30 words, rephrase and make it less than 30 words.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p7">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p7.1">Your third goal is to generate a list of potential areas that might require verification based on the content of the document to increase factual accuracy of the answer. Your response should be in the below format:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p8">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p8.1">“‘
Quotes: ¡Your Extracted Quotes¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p9">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p9.1">Answer: ¡Your Answer¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px3.p10">
<p class="ltx_p" id="A2.SS0.SSS0.Px3.p10.1">Potential Areas for Verification:
1) Your Specific point or segment from your answer.
2) Your Another point or segment from your answer.
N) Your Nth point or segment from your answer.
“‘</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Prompt for Executing Verification Questions and Generating Verified Response (Chain of Verification)</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p1.1">:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p2.1">Below is a question:
¡question¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p3">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p3.1">Below is the answer:
¡answer¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p4">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p4.1">Below is the document from which the answer was generated:
¡document¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p5">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p5.1">Based on the potential areas for verification:
¡areas of verification¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p6">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p6.1">You are an subject matter expert working at Contact Centers. Your expertise includes improvising answers to questions about the company to increase factual correctness using the factual accuracy verification questions provided to you.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p7">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p7.1">Your goal is to check each verification point against the document, provide feedback on any inconsistencies, and then generate a final verified (using the below listed guidelines), concise and accurate answer in strictly less than 30 words that addresses the factual inconsistencies.
1) They should always be professional, positive, friendly, and empathetic.
2) They should not contain words that have a negative connotation (Example: ”unfortunately”).
3) They should always be truthful and honest.
4) They should always be STRICTLY less than 30 words. If the generated response if greater than 30 words, rephrase and make it less than 30 words.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p8">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p8.1">Your response should be in the below format:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p9">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p9.1">“‘
Feedback:
1) Your Verification for point 1.
2) Your Verification for point 2.
N) Your Verification for point N.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px4.p10">
<p class="ltx_p" id="A2.SS0.SSS0.Px4.p10.1">Final Verified Response: [Your Revised Response]
“‘</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Prompt for generating answer from a document and question (open source datasets).</h5>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p1.1">You are a question answering bot.
Your job is to generate answer to the question using the provided articles.
The answers should be derived only from the articles. If the answer is not present in the articles, return the text - NOANSWERFOUND.
The answer should be less than 10 words and in a sentence format.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p2.1">Example where answer could not be found in the articles:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p3">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p3.1">Question: Which county is Smyrna city in?</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p4">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p4.1">Document: Georgia is a southeastern U.S. state whose terrain spans coastal beaches, farmland and mountains. Capital city Atlanta is home of the Georgia Aquarium and the Martin Luther King Jr. National Historic Site, dedicated to the African-American leader’s life and times.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p5">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p5.1">Return Text: NOANSWERFOUND</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p6">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p6.1">Example where answer could be found in the articles:</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p7">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p7.1">Question: Which county is Smyrna city in?</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p8">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p8.1">Document: Smyrna is a city in Cobb County, Georgia, United States. Cobb County is a county in the U.S. state of Georgia, located in the Atlanta metropolitan area in the north central portion of the state.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p9">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p9.1">Return Text: Cobb County of the state of Georgia</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p10">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p10.1">Provide answer to the below Question/Query using the below Document.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p11">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p11.1">Question: ¡question¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p12">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p12.1">Document: ¡document¿</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px5.p13">
<p class="ltx_p" id="A2.SS0.SSS0.Px5.p13.1">Return Text:</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep  6 14:16:18 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
