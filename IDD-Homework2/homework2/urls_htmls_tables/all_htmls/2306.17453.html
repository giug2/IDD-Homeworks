<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.17453] High-throughput Simulation of Federated Learning via Resource-Aware Client Placement</title><meta property="og:description" content="Federated Learning (FL) is the privacy-preserving machine learning paradigm which collaboratively trains a model across millions of devices. Simulated environments are fundamental to large-scale FL research, allowing r…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="High-throughput Simulation of Federated Learning via Resource-Aware Client Placement">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="High-throughput Simulation of Federated Learning via Resource-Aware Client Placement">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.17453">

<!--Generated on Wed Feb 28 21:11:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="federated learning,  simulation,  scalability,  deep learning,  resource management">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">High-throughput Simulation of Federated Learning
via Resource-Aware Client Placement</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lorenzo Sani
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ls985@cam.ac.uk">ls985@cam.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5621-9704" title="ORCID identifier" class="ltx_ref">0000-0001-5621-9704</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">University of Cambridge</span><span id="id4.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pedro Porto Buarque de Gusmão
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:pp524@cam.ac.uk">pp524@cam.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7072-9898" title="ORCID identifier" class="ltx_ref">0000-0002-7072-9898</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">University of Cambridge</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alex Iacob
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:aai30@cam.ac.uk">aai30@cam.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0006-8293-5628" title="ORCID identifier" class="ltx_ref">0009-0006-8293-5628</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">University of Cambridge</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wanru Zhao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wz341@cam.ac.uk">wz341@cam.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-5390-0585" title="ORCID identifier" class="ltx_ref">0000-0002-5390-0585</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">University of Cambridge</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinchi Qiu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xq227@cam.ac.uk">xq227@cam.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7654-4448" title="ORCID identifier" class="ltx_ref">0000-0002-7654-4448</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_affiliation_institution">University of Cambridge</span><span id="id12.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yan Gao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yg381@cam.ac.uk">yg381@cam.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-7922-9788" title="ORCID identifier" class="ltx_ref">0000-0001-7922-9788</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">University of Cambridge</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Javier Fernandez-Marques
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jafermarq@gmail.com">jafermarq@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-3747-6523" title="ORCID identifier" class="ltx_ref">0000-0003-3747-6523</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id15.1.id1" class="ltx_text ltx_affiliation_institution">Samsung AI</span><span id="id16.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nicholas Donald Lane
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ndl32@cam.ac.uk">ndl32@cam.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-2728-8273" title="ORCID identifier" class="ltx_ref">0000-0002-2728-8273</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id17.1.id1" class="ltx_text ltx_affiliation_institution">University of Cambridge</span><span id="id18.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span></span></span>
</div>
<div class="ltx_dates">(2023)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id19.id1" class="ltx_p">Federated Learning (FL) is the privacy-preserving machine learning paradigm which collaboratively trains a model across millions of devices. Simulated environments are fundamental to large-scale FL research, allowing researchers to quickly test new ideas to solve system and statistical heterogeneity issues.
This work proposes <em id="id19.id1.1" class="ltx_emph ltx_font_italic">Pollen</em>, a novel resource-aware system capable of speeding up FL simulations by efficiently placing clients across distributed and heterogeneous hardware.
We propose minimising server-GPU communication and using an efficient client placement policy based on the inherent trade-offs of FL client placement on heterogeneous GPUs. These trade-offs are explored experimentally.</p>
<p id="id2.2" class="ltx_p">This exploration has been conducted via relevant baselines on three popular FL tasks: image classification, speech recognition and text generation.
We compare <em id="id2.2.1" class="ltx_emph ltx_font_italic">Pollen</em> to existing ad-hoc FL frameworks, such as Flower, Flute and FedScale, and show performance gains of <math id="id1.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">50</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">50\%</annotation></semantics></math> to <math id="id2.2.m2.1" class="ltx_Math" alttext="400\%" display="inline"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mn id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">400</mn><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">400\%</annotation></semantics></math>.</p>
</div>
<div class="ltx_keywords">federated learning, simulation, scalability, deep learning, resource management
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>The 29th Annual International Conference On
Mobile Computing And Networking; Oct 02–06,
2023; Madrid, Spain</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Neural networks</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Parallel computing methodologies</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Parallel algorithms</span></span></span><span id="id11" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Massively parallel algorithms</span></span></span><span id="id12" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computer systems organization</span></span></span><span id="id13" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Simulation environments</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Machine learning (ML) is becoming increasingly viable on constrained hardware with low memory such as smartphones, wearables, and general IoT devices.
Techniques originally developed for training models on smartphones with hundreds of <span id="S1.p1.1.1" class="ltx_ERROR undefined">\unit</span><span id="S1.p1.1.2" class="ltx_ERROR undefined">\mega</span> of memory <cite class="ltx_cite ltx_citemacro_citep">(Han et al., <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite> can now run on micro-controllers with <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="256\unit{\kilo}" display="inline"><semantics id="S1.p1.1.m1.1a"><mrow id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml"><mn id="S1.p1.1.m1.1.1.2" xref="S1.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0em" rspace="0em" id="S1.p1.1.m1.1.1.1" xref="S1.p1.1.m1.1.1.1.cmml">​</mo><merror class="ltx_ERROR undefined undefined" id="S1.p1.1.m1.1.1.3" xref="S1.p1.1.m1.1.1.3b.cmml"><mtext id="S1.p1.1.m1.1.1.3a" xref="S1.p1.1.m1.1.1.3b.cmml">\unit</mtext></merror><mo lspace="0em" rspace="0em" id="S1.p1.1.m1.1.1.1a" xref="S1.p1.1.m1.1.1.1.cmml">​</mo><merror class="ltx_ERROR undefined undefined" id="S1.p1.1.m1.1.1.4" xref="S1.p1.1.m1.1.1.4b.cmml"><mtext id="S1.p1.1.m1.1.1.4a" xref="S1.p1.1.m1.1.1.4b.cmml">\kilo</mtext></merror></mrow><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><apply id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1"><times id="S1.p1.1.m1.1.1.1.cmml" xref="S1.p1.1.m1.1.1.1"></times><cn type="integer" id="S1.p1.1.m1.1.1.2.cmml" xref="S1.p1.1.m1.1.1.2">256</cn><ci id="S1.p1.1.m1.1.1.3b.cmml" xref="S1.p1.1.m1.1.1.3"><merror class="ltx_ERROR undefined undefined" id="S1.p1.1.m1.1.1.3.cmml" xref="S1.p1.1.m1.1.1.3"><mtext id="S1.p1.1.m1.1.1.3a.cmml" xref="S1.p1.1.m1.1.1.3">\unit</mtext></merror></ci><ci id="S1.p1.1.m1.1.1.4b.cmml" xref="S1.p1.1.m1.1.1.4"><merror class="ltx_ERROR undefined undefined" id="S1.p1.1.m1.1.1.4.cmml" xref="S1.p1.1.m1.1.1.4"><mtext id="S1.p1.1.m1.1.1.4a.cmml" xref="S1.p1.1.m1.1.1.4">\kilo</mtext></merror></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">256\unit{\kilo}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>.
Federated learning (FL) was introduced by <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> to allow training on thousands to millions of such edge clients in a distributed manner while avoiding the privacy and network costs of communicating their sensitive data in a centralised fashion.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While Federated learning allows edge devices to collaborate effectively, <cite class="ltx_cite ltx_citemacro_citet">Kairouz et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>, sec. 3.1)</cite> indicate that it brings unique challenges relating to clients’ different hardware and data distributions.
To effectively model and address such challenges, researchers must be able to run large-scale Federated Learning simulations efficiently on their hardware.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Unlike centralised ML, dominated by long-running execution of large jobs, simulating federated learning relies on the repeated execution of small clients corresponding to the resource-constrained devices it intends to emulate.
Since such clients cannot keep GPU utilisation high by themselves, scheduling many of them to run both on the same GPU and across GPUs is beneficial.
Crucially, clients can vary in dataset size by orders of magnitude.
Independently on the client selection procedure, existing FL frameworks <cite class="ltx_cite ltx_citemacro_citep">(Beutel et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; Dimitriadis et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>; Lai et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> treat all clients equally during simulations and can often end up with straggling GPUs.
A proper <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">placement</em> of clients on GPUs could significantly reduce training time and allow more extensive simulations.
Unfortunately, such intelligent resource-based <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">placement</em> is impossible in existing FL frameworks as they rely on a <em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">pull-based</em> system where workers sequentially sample clients from a server queue.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we propose <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">Pollen</em>, an adaptive <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">push-based</em> method for client placement in simulated FL, that is compatible with diverse client selection procedures, and show that it significantly improves FL training times.
Instead of building an entirely new framework, we chose to modify Flower <cite class="ltx_cite ltx_citemacro_citep">(Beutel et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> due to its flexibility.
We showcase the design of our system and its effectiveness and analyse the factors which decide the training time of the simulation for a given placement policy.
Our contribution is <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">threefold:</em></p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We experimentally show a proportional but nonlinear relationship between the size of a clients’ dataset and their training time across multiple workloads and GPU types. We argue that this requires intelligent client placement to optimise training time when combined with the skewness of standard FL datasets.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We build <em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Pollen</em>, a client placement system to effectively partition clients amongst potentially heterogeneous GPUs using several placement strategies. Then, we compare the effectiveness of such strategies. Our results show that for heterogeneous GPU environments, a learning-based policy outperforms others by up to <math id="S1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="81\%" display="inline"><semantics id="S1.I1.i2.p1.1.m1.1a"><mrow id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml"><mn id="S1.I1.i2.p1.1.m1.1.1.2" xref="S1.I1.i2.p1.1.m1.1.1.2.cmml">81</mn><mo id="S1.I1.i2.p1.1.m1.1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><apply id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I1.i2.p1.1.m1.1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S1.I1.i2.p1.1.m1.1.1.2.cmml" xref="S1.I1.i2.p1.1.m1.1.1.2">81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">81\%</annotation></semantics></math> as it can accurately learn to predict training time regardless of system configuration.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.6" class="ltx_p">Finally, we show that <em id="S1.I1.i3.p1.6.1" class="ltx_emph ltx_font_italic">push-based</em> client placement can significantly improve training time over previous <em id="S1.I1.i3.p1.6.2" class="ltx_emph ltx_font_italic">pull-based</em> systems.
<em id="S1.I1.i3.p1.6.3" class="ltx_emph ltx_font_italic">Pollen</em> achieves a <math id="S1.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S1.I1.i3.p1.1.m1.1a"><mrow id="S1.I1.i3.p1.1.m1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.cmml"><mn id="S1.I1.i3.p1.1.m1.1.1.2" xref="S1.I1.i3.p1.1.m1.1.1.2.cmml">50</mn><mo id="S1.I1.i3.p1.1.m1.1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.1.m1.1b"><apply id="S1.I1.i3.p1.1.m1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.I1.i3.p1.1.m1.1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S1.I1.i3.p1.1.m1.1.1.2.cmml" xref="S1.I1.i3.p1.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.1.m1.1c">50\%</annotation></semantics></math> to <math id="S1.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="378\%" display="inline"><semantics id="S1.I1.i3.p1.2.m2.1a"><mrow id="S1.I1.i3.p1.2.m2.1.1" xref="S1.I1.i3.p1.2.m2.1.1.cmml"><mn id="S1.I1.i3.p1.2.m2.1.1.2" xref="S1.I1.i3.p1.2.m2.1.1.2.cmml">378</mn><mo id="S1.I1.i3.p1.2.m2.1.1.1" xref="S1.I1.i3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.2.m2.1b"><apply id="S1.I1.i3.p1.2.m2.1.1.cmml" xref="S1.I1.i3.p1.2.m2.1.1"><csymbol cd="latexml" id="S1.I1.i3.p1.2.m2.1.1.1.cmml" xref="S1.I1.i3.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S1.I1.i3.p1.2.m2.1.1.2.cmml" xref="S1.I1.i3.p1.2.m2.1.1.2">378</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.2.m2.1c">378\%</annotation></semantics></math> reduction in training time across various datasets when training <math id="S1.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="10\,000" display="inline"><semantics id="S1.I1.i3.p1.3.m3.1a"><mn id="S1.I1.i3.p1.3.m3.1.1" xref="S1.I1.i3.p1.3.m3.1.1.cmml">10 000</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.3.m3.1b"><cn type="integer" id="S1.I1.i3.p1.3.m3.1.1.cmml" xref="S1.I1.i3.p1.3.m3.1.1">10000</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.3.m3.1c">10\,000</annotation></semantics></math> clients split evenly across <math id="S1.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S1.I1.i3.p1.4.m4.1a"><mn id="S1.I1.i3.p1.4.m4.1.1" xref="S1.I1.i3.p1.4.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.4.m4.1b"><cn type="integer" id="S1.I1.i3.p1.4.m4.1.1.cmml" xref="S1.I1.i3.p1.4.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.4.m4.1c">100</annotation></semantics></math> rounds with heterogeneous GPUs.
Moreover, for homogeneous GPUs, our method decreases communication costs enough to obtain a <math id="S1.I1.i3.p1.5.m5.1" class="ltx_Math" alttext="200\%" display="inline"><semantics id="S1.I1.i3.p1.5.m5.1a"><mrow id="S1.I1.i3.p1.5.m5.1.1" xref="S1.I1.i3.p1.5.m5.1.1.cmml"><mn id="S1.I1.i3.p1.5.m5.1.1.2" xref="S1.I1.i3.p1.5.m5.1.1.2.cmml">200</mn><mo id="S1.I1.i3.p1.5.m5.1.1.1" xref="S1.I1.i3.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.5.m5.1b"><apply id="S1.I1.i3.p1.5.m5.1.1.cmml" xref="S1.I1.i3.p1.5.m5.1.1"><csymbol cd="latexml" id="S1.I1.i3.p1.5.m5.1.1.1.cmml" xref="S1.I1.i3.p1.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S1.I1.i3.p1.5.m5.1.1.2.cmml" xref="S1.I1.i3.p1.5.m5.1.1.2">200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.5.m5.1c">200\%</annotation></semantics></math> to <math id="S1.I1.i3.p1.6.m6.1" class="ltx_Math" alttext="400\%" display="inline"><semantics id="S1.I1.i3.p1.6.m6.1a"><mrow id="S1.I1.i3.p1.6.m6.1.1" xref="S1.I1.i3.p1.6.m6.1.1.cmml"><mn id="S1.I1.i3.p1.6.m6.1.1.2" xref="S1.I1.i3.p1.6.m6.1.1.2.cmml">400</mn><mo id="S1.I1.i3.p1.6.m6.1.1.1" xref="S1.I1.i3.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.6.m6.1b"><apply id="S1.I1.i3.p1.6.m6.1.1.cmml" xref="S1.I1.i3.p1.6.m6.1.1"><csymbol cd="latexml" id="S1.I1.i3.p1.6.m6.1.1.1.cmml" xref="S1.I1.i3.p1.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S1.I1.i3.p1.6.m6.1.1.2.cmml" xref="S1.I1.i3.p1.6.m6.1.1.2">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.6.m6.1c">400\%</annotation></semantics></math> improvement over the next-fastest system.</p>
</div>
</li>
</ol>
<p id="S1.p4.2" class="ltx_p">Our method allows FL researchers to run more extensive, faster, and scalable simulations. These improvements permit rapid prototyping and development of algorithms for production settings involving millions of edge devices.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Simulating Federated Learning</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We now introduce federated learning and describe the main characteristics of the standard simulation solutions adopted by popular federated learning frameworks. We then justify our proposed system <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">Pollen</em> based on the observed limitations of previous systems.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Federated Learning is concerned with training ML models in a distributed fashion while minimising communication costs and maintaining private data on-device.
In its most popular cross-device version <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>, it takes the form of synchronised training where many clients pool their resources to train a model collaboratively.
Such devices can differ significantly both in terms of their local data and in terms of available hardware.
For example, a group of devices for human activity recognition could be composed of smartphones containing gyroscopes and accelerometers <cite class="ltx_cite ltx_citemacro_citep">(Sozinov et al., <a href="#bib.bib25" title="" class="ltx_ref">2018</a>; Tong et al., <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>, surveillance video cameras <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, and passive sensing devices using Radio-Frequency data <cite class="ltx_cite ltx_citemacro_citep">(Koupai et al., <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>.
Clients in these diverse categories would all have highly divergent data modalities, dataset sizes, computational power, network speed and training availability.
Human activity recognition data can be sensitive and often needs to stay private to the point of origin, thus requiring a federated approach.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Federated Learning algorithms, like Federated Averaging <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> for cross-device FL, maintain data privacy via a client-server training design synchronised across <em id="S2.SS1.p2.1.1" class="ltx_emph ltx_font_italic">rounds</em>.
The server controls the training and holds the federated model.
It sends the model to each client at the start of the round, where it is trained on private data using Stochastic Gradient Descent (SGD).
Then, the clients return the models to the server, aggregating them to create a new federated model for the next round.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Framework Simulation Engines</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The simulation engines of FL frameworks aim to enable experiments in a constrained environment where we may not have enough resources to virtualise all the clients in the FL setting or all the clients in the sampled cohort for each round.
We can better understand this scenario by representing each client’s training as a job to schedule.
Each of these jobs needs to allocate resources to account for a complete copy of the model to train, the dataset for the client, including the pre-processing, and the instructions for the training.
The needs of a single client are usually relatively small; as such, it is possible to fit many of them into a single GPU.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Given this setting, FL simulation engines try orchestrating the training using a server-workers paradigm.
The server orchestrates the simulated FL training by serving clients to workers and aggregates the results after every round of training.
The workers are responsible for training clients individually and sending the trained models to the server once the training is complete.
Ad-hoc FL frameworks, such as Flute <cite class="ltx_cite ltx_citemacro_citep">(Dimitriadis et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite>, FedScale <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> and Flower <cite class="ltx_cite ltx_citemacro_citep">(Beutel et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, rely on this paradigm.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">It is worth highlighting that workers in both FedScale and Flute are statically allocated to their GPU.
On the other hand, Flower’s Virtual Client Engine, developed by <cite class="ltx_cite ltx_citemacro_citet">Beutel et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> and based on Ray <cite class="ltx_cite ltx_citemacro_citep">(Moritz et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>, can dynamically move workers between GPUs during the FL training.
However, the control over this dynamic allocation is limited.
In all these frameworks, the server serves clients to workers using a pull-based queuing system involving many communication steps between the server and workers. The execution of an FL round on this system can be summarised as follows.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">At the start of a round, the server samples a subset of participating clients.
This subset defines the cohort of clients to be trained in that round.
The cohort is kept in a synchronised queue, allowing workers to read clients sequentially.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Each worker reads from the queue, extracts the first client, and starts the training.
Different workers cannot access the same element of the synchronised list.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Once a worker finishes training a client, it pings the server to notify that the client’s training is completed.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">When the server receives this message, it replies to the worker when it can receive the training results.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p">The worker finally sends the results to the server that will store them or partially aggregate them, depending on the experiment’s configuration.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">The number of workers controls the concurrency of this embarrassingly parallel simulation.
Hence, it is beneficial to increase the number of workers up until the resources of the system are saturated, or gains from concurrency cease.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Limitations of Current Systems</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The limitations of pull-based queuing systems are multifold.
Critically, the workers of a specific GPU cannot choose which client to train.
This underlying lack of control can limit the simulator’s options when attempting to train specific clients on specific GPUs.
For example, when disproportionately large clients are selected, balancing client training time across GPUs and avoiding stragglers is impossible.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Another general limitation is that the communication involved may take a significant amount of time relative to the training time of most clients.
Such communication bottlenecks are significant for settings with multiple machines (nodes) that need a network connection to communicate with each other.
In this work, <em id="S2.SS3.p2.1.1" class="ltx_emph ltx_font_italic">multinode</em> refers to hardware configurations with GPUs distributed amongst separate machines.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">We now present the specific limitations of each framework addressed in our paper.
<br class="ltx_break"></p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_bold">Flute</span>, introduced by <cite class="ltx_cite ltx_citemacro_citet">Dimitriadis et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite>, is optimised to use the <em id="S2.SS3.p4.1.2" class="ltx_emph ltx_font_italic">nccl</em> backend of PyTorch Distributed <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib17" title="" class="ltx_ref">2020a</a>)</cite> when running on GPUs and the <em id="S2.SS3.p4.1.3" class="ltx_emph ltx_font_italic">gloo</em> backend for CPU training.
It can only run a single worker per GPU and, because it cannot intermix GPU and CPU training, it requires an entire GPU to hold the parameter server that handles aggregation during FL simulation. This can be wasteful as aggregation is usually not a compute-intensive operation.
These issues cannot be easily addressed as the codebase has highly coupled components dependent on this design.
<br class="ltx_break"></p>
</div>
<div id="S2.SS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_bold">FedScale</span>, introduced by <cite class="ltx_cite ltx_citemacro_citet">Lai et al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>, depends on unreliable configurations of gRPC, which is felt across the many communication steps necessary for the pull-based design. As such, longer rounds may cause crashes as clients appear to either disconnect or time out.
Furthermore, despite being able to place multiple workers on the same GPU, later experiments show minimal benefits when increasing either the number of workers or GPUs.
Each worker is also tasked with loading the entire dataset, even if they are on the same node as other workers and can share memory.
Finally, similarly to Flute, the codebase coupling level makes refactoring difficult.
<br class="ltx_break"></p>
</div>
<div id="S2.SS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS3.p6.1" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_bold">Flower</span>, introduced by <cite class="ltx_cite ltx_citemacro_citet">Beutel et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, depends on Ray <cite class="ltx_cite ltx_citemacro_citep">(Moritz et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> as its simulation engine.
Ray may cause out of memory (OOM) in a multi-GPU configuration as workers do not fully deallocate memory from previously used GPUs. Careful configuration of parameters helps avoid OOM at the cost of slowing down the overall training.
However, unlike Flute and FedScale, the codebase is highly modular, allowing us to implement our new simulation engine on top of existing components.
<br class="ltx_break"></p>
</div>
<div id="S2.SS3.p7" class="ltx_para">
<p id="S2.SS3.p7.1" class="ltx_p">General limitations of GPU scheduling for ML tasks are also worth mentioning.
Job scheduling on a GPU cluster for traditional ML tasks is a well-studied problem <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib8" title="" class="ltx_ref">2022</a>; Xiao et al., <a href="#bib.bib29" title="" class="ltx_ref">2018</a>; Zhao et al., <a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite>.
For example, Gandiva <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite> schedules large jobs on such a cluster by profiling the rate at which they process mini-batches.
At the same time, CODA <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite> attempts to balance the CPU assignment of GPU jobs to optimise data loading.
However, both rely on the assumption that ML tasks are highly repetitive and run long enough to be effectively optimised.
As we will show in <a href="#S3" title="3. Heterogeneity and Client Placement ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>, client dataset sizes in FL are much smaller than a typical ML job and more difficult to profile.
Furthermore, client dataset sizes are highly skewed, making round durations unpredictable.
Together, these two considerations make the heuristics of existing GPU scheduling systems insufficient for FL.
This insufficiency drives us to propose an FL-specific solution rather than plugging in an existing one.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Heterogeneity and Client Placement</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We now describe the practical difficulties of deciding which GPU a client should be placed on for training. For these, we identify two causes in the form of client heterogeneity and hardware heterogeneity.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2306.17453/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="335" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Dataset size distribution over clients for OpenImage <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova et al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, Google Speech <cite class="ltx_cite ltx_citemacro_citep">(Warden, <a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite>, and Shakespeare <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al., <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>. Natural non-uniformity in data distributions will lead to different training times. A non-linear scale was chosen for the x-axis.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Client Heterogeneity</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In large-scale cross-device FL, clients collect or produce data at vastly different rates <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib18" title="" class="ltx_ref">2020b</a>)</cite>. Efficient simulations should reflect this fact and account for the different training times that simulated clients will take. While some frameworks try to circumvent this issue by fixing the number of steps each client will train for <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>, this assumes that the dataset distribution is not highly skewed and leads to two issues.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">First, it limits the contribution of clients having large datasets. Second, clients having small datasets are forced to reuse their data (increase in local epochs). Consequently, we refrain from fixing a constant number of training steps throughout our experiments and argue that this is not a reasonable assumption to be made at a framework level. <a href="#S3.F1" title="In 3. Heterogeneity and Client Placement ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows the distributions of samples for three different naturally-partitioned datasets commonly used in FL simulations. FL datasets are usually long-tailed and skewed, leading to the abovementioned issues.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We argue that the dissimilarity in dataset distributions makes it necessary to design dynamically adaptive placement strategies capable of accommodating different datasets.
Additionally, the various pre-processing pipelines such datasets use for mini-batches reinforce this requirement.
For example, image datasets may require samples to be loaded from disk and transformed, while language datasets may store features in memory.
We further argue that the internal skewness of a dataset requires intelligent placement of clients on devices even for the simplest case of <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">homogeneous</em> GPU configurations.
For example, in extreme situations, one device may train clients with orders of magnitude more batches than another.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.17453/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Training times for two GPUs running on different client dataset sizes. The GPUs are running the same clients.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Hardware Heterogeneity</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Besides client heterogeneity, it is necessary to consider the different GPUs used in FL simulations.
As previously mentioned, individual client workloads are smaller than traditional ML tasks and are highly parallelisable, allowing them to be trained across various machines with little effort. However, using heterogeneous GPUs may result in workers finishing processing their clients at very different times, leading to unnecessarily long experiments.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Roughly speaking, training a single client will depend on <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">data loading</em> and <em id="S3.SS2.p2.1.2" class="ltx_emph ltx_font_italic">actual training</em>. Data loading and pre-processing generally happen on the CPU. As the number of concurrent CPU jobs increases beyond the number of CPU cores, the variability of the data loading time grows proportionally to that of the scheduling. This can act as a bottleneck when increasing the number of workers available for processing.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The actual training of a client usually happens inside the GPU, and the time it takes is affected by the client’s local dataset size. For large clients, the training time is approximately determined by the average speed at which the GPU can process each of their batches. However, for small clients, the startup times are a more significant section of the total time the clients spend executing, which causes increased variability in total training time. Furthermore, we observe some variability even for huge clients, which should be the least affected by startup concerns.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><a href="#S3.F2" title="In 3.1. Client Heterogeneity ‣ 3. Heterogeneity and Client Placement ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows the training time distribution of two Nvidia GPUs running the same population of clients with different dataset sizes. As can be observed from the figure, the two GPUs perform very differently from one another. Therefore, allocating less work to the slower GPU is necessary to optimise training time by having them finish simultaneously.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Pollen Design</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section describes the key ideas and components used in our proposed solution. An overview of the complete system, dubbed <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">Pollen</em>, can be seen in <a href="#S4.F3" title="In 4. Pollen Design ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2306.17453/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="416" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S4.F3.4.2" class="ltx_text" style="font-size:90%;">Diagram describing
<em id="S4.F3.4.2.1" class="ltx_emph ltx_font_italic">Pollen</em>
with its relevant elements in both server and worker. The colour code has been used to distinguish between components related to clients (blue), models (green), and hardware (orange).</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Placement Strategy</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Following our investigation in <a href="#S2" title="2. Simulating Federated Learning ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we propose an alternative client placement system that addresses the issues reported in <a href="#S2.SS3" title="2.3. Limitations of Current Systems ‣ 2. Simulating Federated Learning ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.3</span></a> regarding the limitations of FL frameworks. In our approach, instead of having workers <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">requesting</em> clients from the server, the system follows a <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">placement strategy</em> to partition client workloads across workers and perform a <em id="S4.SS1.p1.1.3" class="ltx_emph ltx_font_italic">push-based</em> allocation. This method allows us to reduce the number of communication steps between the server and nodes and to assign sets of clients to appropriate GPUs.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">It is worth mentioning that our placement method acts on the underlying simulation layer of the FL framework. It is an independent procedure from <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">client selection</em> and is not affected by the sampling procedure used to generate the clients for a specific round nor any other algorithmic properties. It can be easily extended to use other sampling techniques such as FedCS <cite class="ltx_cite ltx_citemacro_citep">(Nishio and Yonetani, <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>, Power-of-Choice <cite class="ltx_cite ltx_citemacro_citep">(Cho et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> and DivFL <cite class="ltx_cite ltx_citemacro_citep">(Balakrishnan et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Resource Allocator:</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Clients having different amounts of data and being trained on heterogeneous GPUs will produce disparities in workers’ execution times. A solution to this is to be able to associate client workloads with appropriate GPUs.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">At the beginning of the FL simulation, the <em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">resource allocator</em> module receives information from all training nodes regarding their available hardware, e.g., the number of CPU cores and the number and types of GPUs. This information is used to allocate resources to workers, following user-defined constraints, such as the maximum number of workers per GPU, which can be estimated by profiling a single inference step for each GPU type contained in the node.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Partial Aggregation:</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">When using associative aggregation strategies, such as FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, the system can benefit from partially aggregating results within a worker before sending the partial result for a last aggregation on the server.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.3" class="ltx_p">In this approach, the worker keeps both a partially aggregate model <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\theta^{p}_{k}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><msubsup id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2.2" xref="S4.SS3.p2.1.m1.1.1.2.2.cmml">θ</mi><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">k</mi><mi id="S4.SS3.p2.1.m1.1.1.2.3" xref="S4.SS3.p2.1.m1.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><apply id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.2.1.cmml" xref="S4.SS3.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2.2">𝜃</ci><ci id="S4.SS3.p2.1.m1.1.1.2.3.cmml" xref="S4.SS3.p2.1.m1.1.1.2.3">𝑝</ci></apply><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\theta^{p}_{k}</annotation></semantics></math> and a total number of processed data samples <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><msub id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">N</mi><mi id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">𝑁</ci><ci id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">N_{k}</annotation></semantics></math> after having trained its <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mi id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><ci id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">k</annotation></semantics></math>-th client, as seen in <a href="#S4.E1" title="In 4.3. Partial Aggregation: ‣ 4. Pollen Design ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<table id="S9.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\displaystyle\theta^{p}_{k+1}" display="inline"><semantics id="S4.E1.m1.1a"><msubsup id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mi id="S4.E1.m1.1.1.2.2" xref="S4.E1.m1.1.1.2.2.cmml">θ</mi><mrow id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">k</mi><mo id="S4.E1.m1.1.1.3.1" xref="S4.E1.m1.1.1.3.1.cmml">+</mo><mn id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.1.1.2.3" xref="S4.E1.m1.1.1.2.3.cmml">p</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1">subscript</csymbol><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1">superscript</csymbol><ci id="S4.E1.m1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.2.2">𝜃</ci><ci id="S4.E1.m1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.2.3">𝑝</ci></apply><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><plus id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3.1"></plus><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">𝑘</ci><cn type="integer" id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\displaystyle\theta^{p}_{k+1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E1.m2.1" class="ltx_Math" alttext="\displaystyle=\frac{\theta^{p}_{k}\times N_{k}+\theta_{k+1}\times n_{k+1}}{N_{k+1}}" display="inline"><semantics id="S4.E1.m2.1a"><mrow id="S4.E1.m2.1.1" xref="S4.E1.m2.1.1.cmml"><mi id="S4.E1.m2.1.1.2" xref="S4.E1.m2.1.1.2.cmml"></mi><mo id="S4.E1.m2.1.1.1" xref="S4.E1.m2.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S4.E1.m2.1.1.3" xref="S4.E1.m2.1.1.3.cmml"><mfrac id="S4.E1.m2.1.1.3a" xref="S4.E1.m2.1.1.3.cmml"><mrow id="S4.E1.m2.1.1.3.2" xref="S4.E1.m2.1.1.3.2.cmml"><mrow id="S4.E1.m2.1.1.3.2.2" xref="S4.E1.m2.1.1.3.2.2.cmml"><msubsup id="S4.E1.m2.1.1.3.2.2.2" xref="S4.E1.m2.1.1.3.2.2.2.cmml"><mi id="S4.E1.m2.1.1.3.2.2.2.2.2" xref="S4.E1.m2.1.1.3.2.2.2.2.2.cmml">θ</mi><mi id="S4.E1.m2.1.1.3.2.2.2.3" xref="S4.E1.m2.1.1.3.2.2.2.3.cmml">k</mi><mi id="S4.E1.m2.1.1.3.2.2.2.2.3" xref="S4.E1.m2.1.1.3.2.2.2.2.3.cmml">p</mi></msubsup><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m2.1.1.3.2.2.1" xref="S4.E1.m2.1.1.3.2.2.1.cmml">×</mo><msub id="S4.E1.m2.1.1.3.2.2.3" xref="S4.E1.m2.1.1.3.2.2.3.cmml"><mi id="S4.E1.m2.1.1.3.2.2.3.2" xref="S4.E1.m2.1.1.3.2.2.3.2.cmml">N</mi><mi id="S4.E1.m2.1.1.3.2.2.3.3" xref="S4.E1.m2.1.1.3.2.2.3.3.cmml">k</mi></msub></mrow><mo id="S4.E1.m2.1.1.3.2.1" xref="S4.E1.m2.1.1.3.2.1.cmml">+</mo><mrow id="S4.E1.m2.1.1.3.2.3" xref="S4.E1.m2.1.1.3.2.3.cmml"><msub id="S4.E1.m2.1.1.3.2.3.2" xref="S4.E1.m2.1.1.3.2.3.2.cmml"><mi id="S4.E1.m2.1.1.3.2.3.2.2" xref="S4.E1.m2.1.1.3.2.3.2.2.cmml">θ</mi><mrow id="S4.E1.m2.1.1.3.2.3.2.3" xref="S4.E1.m2.1.1.3.2.3.2.3.cmml"><mi id="S4.E1.m2.1.1.3.2.3.2.3.2" xref="S4.E1.m2.1.1.3.2.3.2.3.2.cmml">k</mi><mo id="S4.E1.m2.1.1.3.2.3.2.3.1" xref="S4.E1.m2.1.1.3.2.3.2.3.1.cmml">+</mo><mn id="S4.E1.m2.1.1.3.2.3.2.3.3" xref="S4.E1.m2.1.1.3.2.3.2.3.3.cmml">1</mn></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m2.1.1.3.2.3.1" xref="S4.E1.m2.1.1.3.2.3.1.cmml">×</mo><msub id="S4.E1.m2.1.1.3.2.3.3" xref="S4.E1.m2.1.1.3.2.3.3.cmml"><mi id="S4.E1.m2.1.1.3.2.3.3.2" xref="S4.E1.m2.1.1.3.2.3.3.2.cmml">n</mi><mrow id="S4.E1.m2.1.1.3.2.3.3.3" xref="S4.E1.m2.1.1.3.2.3.3.3.cmml"><mi id="S4.E1.m2.1.1.3.2.3.3.3.2" xref="S4.E1.m2.1.1.3.2.3.3.3.2.cmml">k</mi><mo id="S4.E1.m2.1.1.3.2.3.3.3.1" xref="S4.E1.m2.1.1.3.2.3.3.3.1.cmml">+</mo><mn id="S4.E1.m2.1.1.3.2.3.3.3.3" xref="S4.E1.m2.1.1.3.2.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><msub id="S4.E1.m2.1.1.3.3" xref="S4.E1.m2.1.1.3.3.cmml"><mi id="S4.E1.m2.1.1.3.3.2" xref="S4.E1.m2.1.1.3.3.2.cmml">N</mi><mrow id="S4.E1.m2.1.1.3.3.3" xref="S4.E1.m2.1.1.3.3.3.cmml"><mi id="S4.E1.m2.1.1.3.3.3.2" xref="S4.E1.m2.1.1.3.3.3.2.cmml">k</mi><mo id="S4.E1.m2.1.1.3.3.3.1" xref="S4.E1.m2.1.1.3.3.3.1.cmml">+</mo><mn id="S4.E1.m2.1.1.3.3.3.3" xref="S4.E1.m2.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m2.1b"><apply id="S4.E1.m2.1.1.cmml" xref="S4.E1.m2.1.1"><eq id="S4.E1.m2.1.1.1.cmml" xref="S4.E1.m2.1.1.1"></eq><csymbol cd="latexml" id="S4.E1.m2.1.1.2.cmml" xref="S4.E1.m2.1.1.2">absent</csymbol><apply id="S4.E1.m2.1.1.3.cmml" xref="S4.E1.m2.1.1.3"><divide id="S4.E1.m2.1.1.3.1.cmml" xref="S4.E1.m2.1.1.3"></divide><apply id="S4.E1.m2.1.1.3.2.cmml" xref="S4.E1.m2.1.1.3.2"><plus id="S4.E1.m2.1.1.3.2.1.cmml" xref="S4.E1.m2.1.1.3.2.1"></plus><apply id="S4.E1.m2.1.1.3.2.2.cmml" xref="S4.E1.m2.1.1.3.2.2"><times id="S4.E1.m2.1.1.3.2.2.1.cmml" xref="S4.E1.m2.1.1.3.2.2.1"></times><apply id="S4.E1.m2.1.1.3.2.2.2.cmml" xref="S4.E1.m2.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m2.1.1.3.2.2.2.1.cmml" xref="S4.E1.m2.1.1.3.2.2.2">subscript</csymbol><apply id="S4.E1.m2.1.1.3.2.2.2.2.cmml" xref="S4.E1.m2.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m2.1.1.3.2.2.2.2.1.cmml" xref="S4.E1.m2.1.1.3.2.2.2">superscript</csymbol><ci id="S4.E1.m2.1.1.3.2.2.2.2.2.cmml" xref="S4.E1.m2.1.1.3.2.2.2.2.2">𝜃</ci><ci id="S4.E1.m2.1.1.3.2.2.2.2.3.cmml" xref="S4.E1.m2.1.1.3.2.2.2.2.3">𝑝</ci></apply><ci id="S4.E1.m2.1.1.3.2.2.2.3.cmml" xref="S4.E1.m2.1.1.3.2.2.2.3">𝑘</ci></apply><apply id="S4.E1.m2.1.1.3.2.2.3.cmml" xref="S4.E1.m2.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m2.1.1.3.2.2.3.1.cmml" xref="S4.E1.m2.1.1.3.2.2.3">subscript</csymbol><ci id="S4.E1.m2.1.1.3.2.2.3.2.cmml" xref="S4.E1.m2.1.1.3.2.2.3.2">𝑁</ci><ci id="S4.E1.m2.1.1.3.2.2.3.3.cmml" xref="S4.E1.m2.1.1.3.2.2.3.3">𝑘</ci></apply></apply><apply id="S4.E1.m2.1.1.3.2.3.cmml" xref="S4.E1.m2.1.1.3.2.3"><times id="S4.E1.m2.1.1.3.2.3.1.cmml" xref="S4.E1.m2.1.1.3.2.3.1"></times><apply id="S4.E1.m2.1.1.3.2.3.2.cmml" xref="S4.E1.m2.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S4.E1.m2.1.1.3.2.3.2.1.cmml" xref="S4.E1.m2.1.1.3.2.3.2">subscript</csymbol><ci id="S4.E1.m2.1.1.3.2.3.2.2.cmml" xref="S4.E1.m2.1.1.3.2.3.2.2">𝜃</ci><apply id="S4.E1.m2.1.1.3.2.3.2.3.cmml" xref="S4.E1.m2.1.1.3.2.3.2.3"><plus id="S4.E1.m2.1.1.3.2.3.2.3.1.cmml" xref="S4.E1.m2.1.1.3.2.3.2.3.1"></plus><ci id="S4.E1.m2.1.1.3.2.3.2.3.2.cmml" xref="S4.E1.m2.1.1.3.2.3.2.3.2">𝑘</ci><cn type="integer" id="S4.E1.m2.1.1.3.2.3.2.3.3.cmml" xref="S4.E1.m2.1.1.3.2.3.2.3.3">1</cn></apply></apply><apply id="S4.E1.m2.1.1.3.2.3.3.cmml" xref="S4.E1.m2.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S4.E1.m2.1.1.3.2.3.3.1.cmml" xref="S4.E1.m2.1.1.3.2.3.3">subscript</csymbol><ci id="S4.E1.m2.1.1.3.2.3.3.2.cmml" xref="S4.E1.m2.1.1.3.2.3.3.2">𝑛</ci><apply id="S4.E1.m2.1.1.3.2.3.3.3.cmml" xref="S4.E1.m2.1.1.3.2.3.3.3"><plus id="S4.E1.m2.1.1.3.2.3.3.3.1.cmml" xref="S4.E1.m2.1.1.3.2.3.3.3.1"></plus><ci id="S4.E1.m2.1.1.3.2.3.3.3.2.cmml" xref="S4.E1.m2.1.1.3.2.3.3.3.2">𝑘</ci><cn type="integer" id="S4.E1.m2.1.1.3.2.3.3.3.3.cmml" xref="S4.E1.m2.1.1.3.2.3.3.3.3">1</cn></apply></apply></apply></apply><apply id="S4.E1.m2.1.1.3.3.cmml" xref="S4.E1.m2.1.1.3.3"><csymbol cd="ambiguous" id="S4.E1.m2.1.1.3.3.1.cmml" xref="S4.E1.m2.1.1.3.3">subscript</csymbol><ci id="S4.E1.m2.1.1.3.3.2.cmml" xref="S4.E1.m2.1.1.3.3.2">𝑁</ci><apply id="S4.E1.m2.1.1.3.3.3.cmml" xref="S4.E1.m2.1.1.3.3.3"><plus id="S4.E1.m2.1.1.3.3.3.1.cmml" xref="S4.E1.m2.1.1.3.3.3.1"></plus><ci id="S4.E1.m2.1.1.3.3.3.2.cmml" xref="S4.E1.m2.1.1.3.3.3.2">𝑘</ci><cn type="integer" id="S4.E1.m2.1.1.3.3.3.3.cmml" xref="S4.E1.m2.1.1.3.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m2.1c">\displaystyle=\frac{\theta^{p}_{k}\times N_{k}+\theta_{k+1}\times n_{k+1}}{N_{k+1}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle N_{k+1}" display="inline"><semantics id="S4.E2.m1.1a"><msub id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mi id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml">N</mi><mrow id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml">k</mi><mo id="S4.E2.m1.1.1.3.1" xref="S4.E2.m1.1.1.3.1.cmml">+</mo><mn id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2">𝑁</ci><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><plus id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3.1"></plus><ci id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2">𝑘</ci><cn type="integer" id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle N_{k+1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E2.m2.1" class="ltx_Math" alttext="\displaystyle=N_{k}+n_{k+1}" display="inline"><semantics id="S4.E2.m2.1a"><mrow id="S4.E2.m2.1.1" xref="S4.E2.m2.1.1.cmml"><mi id="S4.E2.m2.1.1.2" xref="S4.E2.m2.1.1.2.cmml"></mi><mo id="S4.E2.m2.1.1.1" xref="S4.E2.m2.1.1.1.cmml">=</mo><mrow id="S4.E2.m2.1.1.3" xref="S4.E2.m2.1.1.3.cmml"><msub id="S4.E2.m2.1.1.3.2" xref="S4.E2.m2.1.1.3.2.cmml"><mi id="S4.E2.m2.1.1.3.2.2" xref="S4.E2.m2.1.1.3.2.2.cmml">N</mi><mi id="S4.E2.m2.1.1.3.2.3" xref="S4.E2.m2.1.1.3.2.3.cmml">k</mi></msub><mo id="S4.E2.m2.1.1.3.1" xref="S4.E2.m2.1.1.3.1.cmml">+</mo><msub id="S4.E2.m2.1.1.3.3" xref="S4.E2.m2.1.1.3.3.cmml"><mi id="S4.E2.m2.1.1.3.3.2" xref="S4.E2.m2.1.1.3.3.2.cmml">n</mi><mrow id="S4.E2.m2.1.1.3.3.3" xref="S4.E2.m2.1.1.3.3.3.cmml"><mi id="S4.E2.m2.1.1.3.3.3.2" xref="S4.E2.m2.1.1.3.3.3.2.cmml">k</mi><mo id="S4.E2.m2.1.1.3.3.3.1" xref="S4.E2.m2.1.1.3.3.3.1.cmml">+</mo><mn id="S4.E2.m2.1.1.3.3.3.3" xref="S4.E2.m2.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m2.1b"><apply id="S4.E2.m2.1.1.cmml" xref="S4.E2.m2.1.1"><eq id="S4.E2.m2.1.1.1.cmml" xref="S4.E2.m2.1.1.1"></eq><csymbol cd="latexml" id="S4.E2.m2.1.1.2.cmml" xref="S4.E2.m2.1.1.2">absent</csymbol><apply id="S4.E2.m2.1.1.3.cmml" xref="S4.E2.m2.1.1.3"><plus id="S4.E2.m2.1.1.3.1.cmml" xref="S4.E2.m2.1.1.3.1"></plus><apply id="S4.E2.m2.1.1.3.2.cmml" xref="S4.E2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.3.2.1.cmml" xref="S4.E2.m2.1.1.3.2">subscript</csymbol><ci id="S4.E2.m2.1.1.3.2.2.cmml" xref="S4.E2.m2.1.1.3.2.2">𝑁</ci><ci id="S4.E2.m2.1.1.3.2.3.cmml" xref="S4.E2.m2.1.1.3.2.3">𝑘</ci></apply><apply id="S4.E2.m2.1.1.3.3.cmml" xref="S4.E2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.3.3.1.cmml" xref="S4.E2.m2.1.1.3.3">subscript</csymbol><ci id="S4.E2.m2.1.1.3.3.2.cmml" xref="S4.E2.m2.1.1.3.3.2">𝑛</ci><apply id="S4.E2.m2.1.1.3.3.3.cmml" xref="S4.E2.m2.1.1.3.3.3"><plus id="S4.E2.m2.1.1.3.3.3.1.cmml" xref="S4.E2.m2.1.1.3.3.3.1"></plus><ci id="S4.E2.m2.1.1.3.3.3.2.cmml" xref="S4.E2.m2.1.1.3.3.3.2">𝑘</ci><cn type="integer" id="S4.E2.m2.1.1.3.3.3.3.cmml" xref="S4.E2.m2.1.1.3.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m2.1c">\displaystyle=N_{k}+n_{k+1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Once the <span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_italic">worker</span> has completed training its list of clients, it will send both the partially aggregated model and the total sum of the samples for the final aggregation.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2306.17453/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="316" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.10.5.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S4.F4.8.4" class="ltx_text" style="font-size:90%;">The distribution of clients’ training time for different values of concurrent workers on the same GPU. Every worker has the same load: the same list of <math id="S4.F4.5.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.F4.5.1.m1.1b"><mn id="S4.F4.5.1.m1.1.1" xref="S4.F4.5.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.F4.5.1.m1.1c"><cn type="integer" id="S4.F4.5.1.m1.1.1.cmml" xref="S4.F4.5.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.5.1.m1.1d">100</annotation></semantics></math> clients. For example, with <math id="S4.F4.6.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.F4.6.2.m2.1b"><mn id="S4.F4.6.2.m2.1.1" xref="S4.F4.6.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.F4.6.2.m2.1c"><cn type="integer" id="S4.F4.6.2.m2.1.1.cmml" xref="S4.F4.6.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.6.2.m2.1d">5</annotation></semantics></math> workers, the GPU trains <math id="S4.F4.7.3.m3.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S4.F4.7.3.m3.1b"><mn id="S4.F4.7.3.m3.1.1" xref="S4.F4.7.3.m3.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S4.F4.7.3.m3.1c"><cn type="integer" id="S4.F4.7.3.m3.1.1.cmml" xref="S4.F4.7.3.m3.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.7.3.m3.1d">500</annotation></semantics></math> clients using <math id="S4.F4.8.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.F4.8.4.m4.1b"><mn id="S4.F4.8.4.m4.1.1" xref="S4.F4.8.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.F4.8.4.m4.1c"><cn type="integer" id="S4.F4.8.4.m4.1.1.cmml" xref="S4.F4.8.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.8.4.m4.1d">5</annotation></semantics></math> concurrent processes.</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Client Allocator:</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We follow the concept from <a href="#S2" title="2. Simulating Federated Learning ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2</span></a> and define a <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">worker</span> as an entity capable of sequentially training lists of clients.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">As FL clients can only use small batch sizes and models when training on edge devices, packing many workers and oversubscribing GPUs has proven beneficial in realistic FL simulations, as seen in <a href="#S4.F4" title="In 4.3. Partial Aggregation: ‣ 4. Pollen Design ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">The <em id="S4.SS4.p3.1.1" class="ltx_emph ltx_font_italic">client allocator</em> associates clients with individual workers on specific GPUs. The server samples a list of participating clients at the beginning of a round. Then, it passes this module, which uses a pre-defined <em id="S4.SS4.p3.1.2" class="ltx_emph ltx_font_italic">client placement policy</em>, discussed in <a href="#S5" title="5. Client Placement Strategies ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5</span></a>, to determine which worker will train which client and in what order.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Client Placement Strategies</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section discusses the strategies for placing clients on GPUs that we have explored in this work.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Since our design allows the simulation to have only one communication step from the server to the workers, our exploration focused on distributing the list of <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.p2.1.m1.1a"><mi id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">N</annotation></semantics></math> randomly sampled clients across workers in one step.
For each round, all these strategies receive the list of integer clients’ IDs to be trained, and they return a list of clients’ IDs for each worker indicating which clients it should train.
Since the exploration space for determining the best distribution procedure is very broad, we rely on FL features common to most experiments.
<br class="ltx_break"></p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.3" class="ltx_p"><span id="S5.p3.3.1" class="ltx_text ltx_font_bold">Na’́ıve round-robin (RR):</span> This strategy represents the starting point of our investigation since it naively splits the list of clients in <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.p3.1.m1.1a"><mi id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">k</annotation></semantics></math> uniformly populated lists, where <math id="S5.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.p3.2.m2.1a"><mi id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><ci id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">k</annotation></semantics></math> is the number of workers.
In particular, the first sampled client will be assigned to the first worker and the second client to the second worker, etc.
If <math id="S5.p3.3.m3.1" class="ltx_Math" alttext="\frac{N}{k}" display="inline"><semantics id="S5.p3.3.m3.1a"><mfrac id="S5.p3.3.m3.1.1" xref="S5.p3.3.m3.1.1.cmml"><mi id="S5.p3.3.m3.1.1.2" xref="S5.p3.3.m3.1.1.2.cmml">N</mi><mi id="S5.p3.3.m3.1.1.3" xref="S5.p3.3.m3.1.1.3.cmml">k</mi></mfrac><annotation-xml encoding="MathML-Content" id="S5.p3.3.m3.1b"><apply id="S5.p3.3.m3.1.1.cmml" xref="S5.p3.3.m3.1.1"><divide id="S5.p3.3.m3.1.1.1.cmml" xref="S5.p3.3.m3.1.1"></divide><ci id="S5.p3.3.m3.1.1.2.cmml" xref="S5.p3.3.m3.1.1.2">𝑁</ci><ci id="S5.p3.3.m3.1.1.3.cmml" xref="S5.p3.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.3.m3.1c">\frac{N}{k}</annotation></semantics></math> is not an integer, the remainder is distributed across the first workers. 
<br class="ltx_break"></p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.2" class="ltx_p"><span id="S5.p4.2.1" class="ltx_text ltx_font_bold">Batch-sorted round-robin (SRR):</span> The first information we used in this study was the number of batches <math id="S5.p4.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.p4.1.m1.1a"><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">m</annotation></semantics></math> each client has.
As discussed above, the number of batches is a proxy for the training time in epochs-based FL training.
Before naively splitting the list as before, this strategy orders the clients by <math id="S5.p4.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.p4.2.m2.1a"><mi id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><ci id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">m</annotation></semantics></math> from top to bottom.
The intuition behind this procedure is that different workers will likely train the biggest clients.
<br class="ltx_break"></p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.3" class="ltx_p"><span id="S5.p5.3.1" class="ltx_text ltx_font_bold">Batch-Uniform distribution (BU):</span> A step forward to the previous strategy is to use the same information while changing the distribution procedure.
For homogeneous GPUs, we want the load across workers to be balanced.
To achieve this, the strategy loops over the <math id="S5.p5.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.p5.1.m1.1a"><mi id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><ci id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">N</annotation></semantics></math> clients after ordering them by <math id="S5.p5.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.p5.2.m2.1a"><mi id="S5.p5.2.m2.1.1" xref="S5.p5.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.p5.2.m2.1b"><ci id="S5.p5.2.m2.1.1.cmml" xref="S5.p5.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.2.m2.1c">m</annotation></semantics></math> from top to bottom and assigns the current client to the worker whose load is lower.
The load is estimated by summing the number of batches of all the clients assigned to the worker.
It has to be noted that the first <math id="S5.p5.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.p5.3.m3.1a"><mi id="S5.p5.3.m3.1.1" xref="S5.p5.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.p5.3.m3.1b"><ci id="S5.p5.3.m3.1.1.cmml" xref="S5.p5.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.3.m3.1c">k</annotation></semantics></math> clients of the list are assigned the same way as the previous strategy.
<br class="ltx_break"></p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.2" class="ltx_p"><span id="S5.p6.2.1" class="ltx_text ltx_font_bold">Learning-based time prediction (LB):</span> In settings with heterogeneous GPUs, the proxy for the training time given by <math id="S5.p6.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.p6.1.m1.1a"><mi id="S5.p6.1.m1.1.1" xref="S5.p6.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.p6.1.m1.1b"><ci id="S5.p6.1.m1.1.1.cmml" xref="S5.p6.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.1.m1.1c">m</annotation></semantics></math> may not be sufficient, as shown in previous sections.
Our learning-based strategy predicts the training time for each GPU.
The first FL round will use the na’́ıve round-robin strategy to collect data about client training time from all available workers.
Starting from the second round, the strategy builds one dataset for each GPU composed of tuples <math id="S5.p6.2.m2.2" class="ltx_Math" alttext="(client~{}training~{}time,~{}m)" display="inline"><semantics id="S5.p6.2.m2.2a"><mrow id="S5.p6.2.m2.2.2.1" xref="S5.p6.2.m2.2.2.2.cmml"><mo stretchy="false" id="S5.p6.2.m2.2.2.1.2" xref="S5.p6.2.m2.2.2.2.cmml">(</mo><mrow id="S5.p6.2.m2.2.2.1.1" xref="S5.p6.2.m2.2.2.1.1.cmml"><mi id="S5.p6.2.m2.2.2.1.1.2" xref="S5.p6.2.m2.2.2.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.3" xref="S5.p6.2.m2.2.2.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1a" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.4" xref="S5.p6.2.m2.2.2.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1b" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.5" xref="S5.p6.2.m2.2.2.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1c" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.6" xref="S5.p6.2.m2.2.2.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1d" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.7" xref="S5.p6.2.m2.2.2.1.1.7.cmml">t</mi><mo lspace="0.330em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1e" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.8" xref="S5.p6.2.m2.2.2.1.1.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1f" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.9" xref="S5.p6.2.m2.2.2.1.1.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1g" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.10" xref="S5.p6.2.m2.2.2.1.1.10.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1h" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.11" xref="S5.p6.2.m2.2.2.1.1.11.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1i" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.12" xref="S5.p6.2.m2.2.2.1.1.12.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1j" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.13" xref="S5.p6.2.m2.2.2.1.1.13.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1k" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.14" xref="S5.p6.2.m2.2.2.1.1.14.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1l" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.15" xref="S5.p6.2.m2.2.2.1.1.15.cmml">g</mi><mo lspace="0.330em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1m" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.16" xref="S5.p6.2.m2.2.2.1.1.16.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1n" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.17" xref="S5.p6.2.m2.2.2.1.1.17.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1o" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.18" xref="S5.p6.2.m2.2.2.1.1.18.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.p6.2.m2.2.2.1.1.1p" xref="S5.p6.2.m2.2.2.1.1.1.cmml">​</mo><mi id="S5.p6.2.m2.2.2.1.1.19" xref="S5.p6.2.m2.2.2.1.1.19.cmml">e</mi></mrow><mo rspace="0.497em" id="S5.p6.2.m2.2.2.1.3" xref="S5.p6.2.m2.2.2.2.cmml">,</mo><mi id="S5.p6.2.m2.1.1" xref="S5.p6.2.m2.1.1.cmml">m</mi><mo stretchy="false" id="S5.p6.2.m2.2.2.1.4" xref="S5.p6.2.m2.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p6.2.m2.2b"><interval closure="open" id="S5.p6.2.m2.2.2.2.cmml" xref="S5.p6.2.m2.2.2.1"><apply id="S5.p6.2.m2.2.2.1.1.cmml" xref="S5.p6.2.m2.2.2.1.1"><times id="S5.p6.2.m2.2.2.1.1.1.cmml" xref="S5.p6.2.m2.2.2.1.1.1"></times><ci id="S5.p6.2.m2.2.2.1.1.2.cmml" xref="S5.p6.2.m2.2.2.1.1.2">𝑐</ci><ci id="S5.p6.2.m2.2.2.1.1.3.cmml" xref="S5.p6.2.m2.2.2.1.1.3">𝑙</ci><ci id="S5.p6.2.m2.2.2.1.1.4.cmml" xref="S5.p6.2.m2.2.2.1.1.4">𝑖</ci><ci id="S5.p6.2.m2.2.2.1.1.5.cmml" xref="S5.p6.2.m2.2.2.1.1.5">𝑒</ci><ci id="S5.p6.2.m2.2.2.1.1.6.cmml" xref="S5.p6.2.m2.2.2.1.1.6">𝑛</ci><ci id="S5.p6.2.m2.2.2.1.1.7.cmml" xref="S5.p6.2.m2.2.2.1.1.7">𝑡</ci><ci id="S5.p6.2.m2.2.2.1.1.8.cmml" xref="S5.p6.2.m2.2.2.1.1.8">𝑡</ci><ci id="S5.p6.2.m2.2.2.1.1.9.cmml" xref="S5.p6.2.m2.2.2.1.1.9">𝑟</ci><ci id="S5.p6.2.m2.2.2.1.1.10.cmml" xref="S5.p6.2.m2.2.2.1.1.10">𝑎</ci><ci id="S5.p6.2.m2.2.2.1.1.11.cmml" xref="S5.p6.2.m2.2.2.1.1.11">𝑖</ci><ci id="S5.p6.2.m2.2.2.1.1.12.cmml" xref="S5.p6.2.m2.2.2.1.1.12">𝑛</ci><ci id="S5.p6.2.m2.2.2.1.1.13.cmml" xref="S5.p6.2.m2.2.2.1.1.13">𝑖</ci><ci id="S5.p6.2.m2.2.2.1.1.14.cmml" xref="S5.p6.2.m2.2.2.1.1.14">𝑛</ci><ci id="S5.p6.2.m2.2.2.1.1.15.cmml" xref="S5.p6.2.m2.2.2.1.1.15">𝑔</ci><ci id="S5.p6.2.m2.2.2.1.1.16.cmml" xref="S5.p6.2.m2.2.2.1.1.16">𝑡</ci><ci id="S5.p6.2.m2.2.2.1.1.17.cmml" xref="S5.p6.2.m2.2.2.1.1.17">𝑖</ci><ci id="S5.p6.2.m2.2.2.1.1.18.cmml" xref="S5.p6.2.m2.2.2.1.1.18">𝑚</ci><ci id="S5.p6.2.m2.2.2.1.1.19.cmml" xref="S5.p6.2.m2.2.2.1.1.19">𝑒</ci></apply><ci id="S5.p6.2.m2.1.1.cmml" xref="S5.p6.2.m2.1.1">𝑚</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.2.m2.2c">(client~{}training~{}time,~{}m)</annotation></semantics></math> from previous rounds.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.2" class="ltx_p">Then, for each dataset, the strategy fits the data points to the function in <a href="#S5.E3" title="In 5. Client Placement Strategies ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, where <math id="S5.p7.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S5.p7.1.m1.1a"><mi id="S5.p7.1.m1.1.1" xref="S5.p7.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S5.p7.1.m1.1b"><ci id="S5.p7.1.m1.1.1.cmml" xref="S5.p7.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p7.1.m1.1c">y</annotation></semantics></math> represents the client training time, and <math id="S5.p7.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.p7.2.m2.1a"><mi id="S5.p7.2.m2.1.1" xref="S5.p7.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.p7.2.m2.1b"><ci id="S5.p7.2.m2.1.1.cmml" xref="S5.p7.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p7.2.m2.1c">x</annotation></semantics></math> is the number of batches the client has.</p>
</div>
<div id="S5.p8" class="ltx_para">
<table id="S5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E3.m1.2" class="ltx_Math" alttext="y=ax+b\log\left(cx\right)+d" display="block"><semantics id="S5.E3.m1.2a"><mrow id="S5.E3.m1.2.2" xref="S5.E3.m1.2.2.cmml"><mi id="S5.E3.m1.2.2.3" xref="S5.E3.m1.2.2.3.cmml">y</mi><mo id="S5.E3.m1.2.2.2" xref="S5.E3.m1.2.2.2.cmml">=</mo><mrow id="S5.E3.m1.2.2.1" xref="S5.E3.m1.2.2.1.cmml"><mrow id="S5.E3.m1.2.2.1.3" xref="S5.E3.m1.2.2.1.3.cmml"><mi id="S5.E3.m1.2.2.1.3.2" xref="S5.E3.m1.2.2.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.2.2.1.3.1" xref="S5.E3.m1.2.2.1.3.1.cmml">​</mo><mi id="S5.E3.m1.2.2.1.3.3" xref="S5.E3.m1.2.2.1.3.3.cmml">x</mi></mrow><mo id="S5.E3.m1.2.2.1.2" xref="S5.E3.m1.2.2.1.2.cmml">+</mo><mrow id="S5.E3.m1.2.2.1.1" xref="S5.E3.m1.2.2.1.1.cmml"><mi id="S5.E3.m1.2.2.1.1.3" xref="S5.E3.m1.2.2.1.1.3.cmml">b</mi><mo lspace="0.167em" rspace="0em" id="S5.E3.m1.2.2.1.1.2" xref="S5.E3.m1.2.2.1.1.2.cmml">​</mo><mrow id="S5.E3.m1.2.2.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.2.cmml"><mi id="S5.E3.m1.1.1" xref="S5.E3.m1.1.1.cmml">log</mi><mo id="S5.E3.m1.2.2.1.1.1.1a" xref="S5.E3.m1.2.2.1.1.1.2.cmml">⁡</mo><mrow id="S5.E3.m1.2.2.1.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.2.cmml"><mo id="S5.E3.m1.2.2.1.1.1.1.1.2" xref="S5.E3.m1.2.2.1.1.1.2.cmml">(</mo><mrow id="S5.E3.m1.2.2.1.1.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S5.E3.m1.2.2.1.1.1.1.1.1.2" xref="S5.E3.m1.2.2.1.1.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.2.2.1.1.1.1.1.1.1" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.E3.m1.2.2.1.1.1.1.1.1.3" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S5.E3.m1.2.2.1.1.1.1.1.3" xref="S5.E3.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S5.E3.m1.2.2.1.2a" xref="S5.E3.m1.2.2.1.2.cmml">+</mo><mi id="S5.E3.m1.2.2.1.4" xref="S5.E3.m1.2.2.1.4.cmml">d</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.2b"><apply id="S5.E3.m1.2.2.cmml" xref="S5.E3.m1.2.2"><eq id="S5.E3.m1.2.2.2.cmml" xref="S5.E3.m1.2.2.2"></eq><ci id="S5.E3.m1.2.2.3.cmml" xref="S5.E3.m1.2.2.3">𝑦</ci><apply id="S5.E3.m1.2.2.1.cmml" xref="S5.E3.m1.2.2.1"><plus id="S5.E3.m1.2.2.1.2.cmml" xref="S5.E3.m1.2.2.1.2"></plus><apply id="S5.E3.m1.2.2.1.3.cmml" xref="S5.E3.m1.2.2.1.3"><times id="S5.E3.m1.2.2.1.3.1.cmml" xref="S5.E3.m1.2.2.1.3.1"></times><ci id="S5.E3.m1.2.2.1.3.2.cmml" xref="S5.E3.m1.2.2.1.3.2">𝑎</ci><ci id="S5.E3.m1.2.2.1.3.3.cmml" xref="S5.E3.m1.2.2.1.3.3">𝑥</ci></apply><apply id="S5.E3.m1.2.2.1.1.cmml" xref="S5.E3.m1.2.2.1.1"><times id="S5.E3.m1.2.2.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.2"></times><ci id="S5.E3.m1.2.2.1.1.3.cmml" xref="S5.E3.m1.2.2.1.1.3">𝑏</ci><apply id="S5.E3.m1.2.2.1.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1"><log id="S5.E3.m1.1.1.cmml" xref="S5.E3.m1.1.1"></log><apply id="S5.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1"><times id="S5.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.1"></times><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.2">𝑐</ci><ci id="S5.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S5.E3.m1.2.2.1.1.1.1.1.1.3">𝑥</ci></apply></apply></apply><ci id="S5.E3.m1.2.2.1.4.cmml" xref="S5.E3.m1.2.2.1.4">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.2c">y=ax+b\log\left(cx\right)+d</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S5.p9" class="ltx_para">
<p id="S5.p9.1" class="ltx_p">The parameters derived from the fitting are then used for predicting the training time of the clients sampled in the current round. We chose <a href="#S5.E3" title="In 5. Client Placement Strategies ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Eq.</span> <span class="ltx_text ltx_ref_tag">3</span></a> in order to match the skewed training time distribution we observed empirically in <a href="#S3.F1" title="In 3. Heterogeneity and Client Placement ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, this choice is further discussed at the end of <a href="#S5.SS1" title="5.1. Efficient Client Placement ‣ 5. Client Placement Strategies ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>.
The strategy sorts the workers by GPU type, from the fastest to the slowest, using the predicted training time of the biggest client in the current cohort.
Finally, the strategy carries on the placement of clients by balancing the load between workers, similar to the batch uniform strategy.
Clients are ordered by <math id="S5.p9.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.p9.1.m1.1a"><mi id="S5.p9.1.m1.1.1" xref="S5.p9.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.p9.1.m1.1b"><ci id="S5.p9.1.m1.1.1.cmml" xref="S5.p9.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.1.m1.1c">m</annotation></semantics></math> from top to bottom and assigned to the worker whose load is lower.
Here, the load is estimated by summing the predicted training time of all the clients assigned to the worker.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2306.17453/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;"> Clients’ training times are plotted against their number of batches. The fitting lines of the linear function and the proposed function are shown alongside The Mean Squared Errors (MSE) derived from the fitting procedure.</span></figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Efficient Client Placement</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.2" class="ltx_p">In this work, we try to answer the question of which placement strategy is better to choose in which context.
More importantly, we argue that discussing placement strategies in simulating FL is necessary.
The research on large-scale <cite class="ltx_cite ltx_citemacro_citep">(Charles et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Bonawitz et al., <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> FL often relies on simulating FL settings with large cohorts (in the range <math id="S5.SS1.p1.1.m1.2" class="ltx_Math" alttext="[10^{2},10^{4}]" display="inline"><semantics id="S5.SS1.p1.1.m1.2a"><mrow id="S5.SS1.p1.1.m1.2.2.2" xref="S5.SS1.p1.1.m1.2.2.3.cmml"><mo stretchy="false" id="S5.SS1.p1.1.m1.2.2.2.3" xref="S5.SS1.p1.1.m1.2.2.3.cmml">[</mo><msup id="S5.SS1.p1.1.m1.1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.1.cmml"><mn id="S5.SS1.p1.1.m1.1.1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.1.1.2.cmml">10</mn><mn id="S5.SS1.p1.1.m1.1.1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.1.1.3.cmml">2</mn></msup><mo id="S5.SS1.p1.1.m1.2.2.2.4" xref="S5.SS1.p1.1.m1.2.2.3.cmml">,</mo><msup id="S5.SS1.p1.1.m1.2.2.2.2" xref="S5.SS1.p1.1.m1.2.2.2.2.cmml"><mn id="S5.SS1.p1.1.m1.2.2.2.2.2" xref="S5.SS1.p1.1.m1.2.2.2.2.2.cmml">10</mn><mn id="S5.SS1.p1.1.m1.2.2.2.2.3" xref="S5.SS1.p1.1.m1.2.2.2.2.3.cmml">4</mn></msup><mo stretchy="false" id="S5.SS1.p1.1.m1.2.2.2.5" xref="S5.SS1.p1.1.m1.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.2b"><interval closure="closed" id="S5.SS1.p1.1.m1.2.2.3.cmml" xref="S5.SS1.p1.1.m1.2.2.2"><apply id="S5.SS1.p1.1.m1.1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1">superscript</csymbol><cn type="integer" id="S5.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.2">10</cn><cn type="integer" id="S5.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.3">2</cn></apply><apply id="S5.SS1.p1.1.m1.2.2.2.2.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.2.2.2.2.1.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2">superscript</csymbol><cn type="integer" id="S5.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.2">10</cn><cn type="integer" id="S5.SS1.p1.1.m1.2.2.2.2.3.cmml" xref="S5.SS1.p1.1.m1.2.2.2.2.3">4</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.2c">[10^{2},10^{4}]</annotation></semantics></math>) of clients over a relatively limited amount of hardware resources for thousands of rounds.
Reducing the latency of experiments as much as possible while exploiting the hardware resources at their best is fundamental.
Regardless of the chosen placement strategy, we expect any one-step communication method to outperform the queue design in previously mentioned frameworks.
However, we will show that an optimal placement can improve the simulation speed by up to <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="81\%" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">81</mn><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">81\%</annotation></semantics></math> compared to the slowest strategy.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Different placement strategies will result in different distributions of clients to train across workers.
However, this does not hold for the degenerate placements in which the number of clients trained per round equals the number of workers, and the workers are allocated on the same GPU and node.
The Shakespeare baseline is the only one we have explored that can fall into this degenerate case.
This is because the training procedure plans to train <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mn id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><cn type="integer" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">10</annotation></semantics></math> clients per round, which can easily be fitted into a single GPU.
Excluding the degenerate placement, the non-trivial metric that is impacted by different placement strategies is what we call “<span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">timedelta workers</span>”.
We define “<span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">timedelta workers</span>” as the time difference between the timestamp at which the fastest worker finishes their job and the timestamp at which the slowest worker finishes their job, where we intend the job to be the training of all the clients in the received list.
When using the RR strategy, we observe that the distribution across rounds of “<span id="S5.SS1.p2.1.3" class="ltx_text ltx_font_italic">timedelta workers</span>” is peaked at a time at least one order of magnitude greater than the training time of the smallest client in that round.
We could have reduced the training time by moving the smallest client from the slowest to the fastest worker.
Thus, we assume that “<span id="S5.SS1.p2.1.4" class="ltx_text ltx_font_italic">timedelta workers</span>” approximates the time wasted due to clients’ misplacement.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">The impact of the placement strategy on the training time depends on the GPUs we are dealing with.
We can distinguish between <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">homogeneous settings</span>, in which each worker executes on the same GPU type at the same speed, and <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">heterogeneous settings</span>, in which groups of workers are allocated on different GPU types having different speeds.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">In <span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_italic">homogeneous settings</span>, we expect to observe that the ratio between the number of clients per round and the number of workers drives the difference in performance.
As this ratio increases, we expect to observe similar performance across strategies.
This expectation is motivated by the fact that the noise introduced by the random sampling of clients will likely balance the load over workers naturally.
When this ratio is close to 1, only sampled cohorts of clients whose number of large-size clients is different from the number of workers can lead to different performance.
In this case, the performance gap appears because big clients cannot be evenly distributed across workers.
However, we expect the BU strategy to slightly outperform the others since it can balance the load over workers with the minimum overhead.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">The scenario in which different strategies will significantly differ in performance is the <span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_italic">heterogeneous setting</span> in which clients of the same size are not trained in the same amount of time by different workers.
For these settings, the LB strategy will outperform BU because of its ability to discriminate between workers executing on different GPU types.
The difference in performance will mostly depend on how heterogeneous the hardware settings are.
In particular, the gap between different hardware in training small clients, prevalent in FL datasets, will have the most critical impact.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">The LB placement strategy strongly relies on the performance of the fitting procedure over clients’ training time from previous rounds.
It is worth discussing the two main ingredients upon which this strategy arranges the clients’ placement: the data collected for previous rounds and the fitting function.
We chose to keep all the data from previously trained rounds.
In general, the robustness of curve fitting is proportional to the number of data points; conversely, its duration is proportional to the number of data points.
We observe that during the last round, when the data points are <math id="S5.SS1.p6.1.m1.1" class="ltx_Math" alttext="9900" display="inline"><semantics id="S5.SS1.p6.1.m1.1a"><mn id="S5.SS1.p6.1.m1.1.1" xref="S5.SS1.p6.1.m1.1.1.cmml">9900</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.1b"><cn type="integer" id="S5.SS1.p6.1.m1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1">9900</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.1c">9900</annotation></semantics></math>, the LB strategy takes an amount of time on the same order of magnitude as the RR strategy, tens of seconds.
For extended experiments where the number of data points to fit could be much more significant, it is reasonable to define a time window for deleting older data.
Second, our fitting function has been chosen for its mathematical properties.
The logarithm combined with a linear term ensures that the fitted function never predicts negative values despite the wide cloud of data points produced by small clients, as can be observed in <a href="#S5.F5" title="In 5. Client Placement Strategies ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>.
Since small clients have greater training time variance, many may take longer than their slightly bigger counterparts.
This behaviour may enforce negative slopes in the fitted curve, especially if polynomial.
The logarithmic term makes the function more robust to this situation, while the linear term ensures that the bigger clients are predicted to take longer to train. <em id="S5.SS1.p6.1.1" class="ltx_emph ltx_font_italic">As such, the chosen function allows us to avoid ever predicting a negative time for a client at the cost of overestimating the duration of small clients.</em></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Experimental Design</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This section describes a series of experiments meant to showcase our method’s superiority and validate it.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Federated Learning Tasks</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We use three representative FL tasks throughout this work to showcase our method. These are characterised by having clients with long tail distributions over the number of samples and workloads. For all tasks, we exclude clients with less than one batch of training data.
<br class="ltx_break"></p>
</div>
<div id="S6.SS1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.p2.4" class="ltx_p"><span id="S6.SS1.p2.4.1" class="ltx_text ltx_font_bold">Image Classification:</span> The goal of this task is to collaboratively train a ShuffleNetV2 <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite> network to correctly classify images amongst <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="596" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mn id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml">596</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><cn type="integer" id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">596</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">596</annotation></semantics></math> classes. For this, we use a federated version of the original OpenImage <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova et al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> dataset as implemented by FedScale. This dataset contains <math id="S6.SS1.p2.2.m2.3" class="ltx_Math" alttext="1.6\text{\times}{10}^{6}" display="inline"><semantics id="S6.SS1.p2.2.m2.3a"><mrow id="S6.SS1.p2.2.m2.3.3.3" xref="S6.SS1.p2.2.m2.3.3.3.cmml"><mn id="S6.SS1.p2.2.m2.1.1.1.1.1.1.1" xref="S6.SS1.p2.2.m2.3.3.3.cmml">1.6</mn><mtext id="S6.SS1.p2.2.m2.2.2.2.2.2.2.2" xref="S6.SS1.p2.2.m2.3.3.3.cmml">×</mtext><msup id="S6.SS1.p2.2.m2.3.3.3.3.3.3.3" xref="S6.SS1.p2.2.m2.3.3.3.cmml"><mn id="S6.SS1.p2.2.m2.3.3.3.3.3.3.3.2" xref="S6.SS1.p2.2.m2.3.3.3.cmml">10</mn><mn id="S6.SS1.p2.2.m2.3.3.3.3.3.3.3.3" xref="S6.SS1.p2.2.m2.3.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.3b"><csymbol cd="latexml" id="S6.SS1.p2.2.m2.3.3.3.cmml" xref="S6.SS1.p2.2.m2.3.3.3">1.6E6</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.3c">1.6\text{\times}{10}^{6}</annotation></semantics></math> images partitioned across <math id="S6.SS1.p2.3.m3.1" class="ltx_Math" alttext="13\,771" display="inline"><semantics id="S6.SS1.p2.3.m3.1a"><mn id="S6.SS1.p2.3.m3.1.1" xref="S6.SS1.p2.3.m3.1.1.cmml">13 771</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.1b"><cn type="integer" id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1">13771</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.1c">13\,771</annotation></semantics></math> clients. We use a batch size of <math id="S6.SS1.p2.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S6.SS1.p2.4.m4.1a"><mn id="S6.SS1.p2.4.m4.1.1" xref="S6.SS1.p2.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.4.m4.1b"><cn type="integer" id="S6.SS1.p2.4.m4.1.1.cmml" xref="S6.SS1.p2.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.4.m4.1c">20</annotation></semantics></math> samples.
<br class="ltx_break"></p>
</div>
<div id="S6.SS1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS1.p3.4" class="ltx_p"><span id="S6.SS1.p3.4.1" class="ltx_text ltx_font_bold">Speech Recognition:</span> In this task, we use the Google Speech Commands dataset <cite class="ltx_cite ltx_citemacro_citep">(Warden, <a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite> to collaboratively train a ReseNet-34 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite> to classify audio samples amongst a set of <math id="S6.SS1.p3.1.m1.1" class="ltx_Math" alttext="35" display="inline"><semantics id="S6.SS1.p3.1.m1.1a"><mn id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><cn type="integer" id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">35</annotation></semantics></math> pre-defined spoken words. The dataset contains a collection of <math id="S6.SS1.p3.2.m2.1" class="ltx_Math" alttext="157K" display="inline"><semantics id="S6.SS1.p3.2.m2.1a"><mrow id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml"><mn id="S6.SS1.p3.2.m2.1.1.2" xref="S6.SS1.p3.2.m2.1.1.2.cmml">157</mn><mo lspace="0em" rspace="0em" id="S6.SS1.p3.2.m2.1.1.1" xref="S6.SS1.p3.2.m2.1.1.1.cmml">​</mo><mi id="S6.SS1.p3.2.m2.1.1.3" xref="S6.SS1.p3.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><apply id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"><times id="S6.SS1.p3.2.m2.1.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1.1"></times><cn type="integer" id="S6.SS1.p3.2.m2.1.1.2.cmml" xref="S6.SS1.p3.2.m2.1.1.2">157</cn><ci id="S6.SS1.p3.2.m2.1.1.3.cmml" xref="S6.SS1.p3.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">157K</annotation></semantics></math> one-second-long clips naturally partitioned according to their <math id="S6.SS1.p3.3.m3.1" class="ltx_Math" alttext="2168" display="inline"><semantics id="S6.SS1.p3.3.m3.1a"><mn id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml">2168</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><cn type="integer" id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1">2168</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">2168</annotation></semantics></math> speakers. We use a batch size of <math id="S6.SS1.p3.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S6.SS1.p3.4.m4.1a"><mn id="S6.SS1.p3.4.m4.1.1" xref="S6.SS1.p3.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.4.m4.1b"><cn type="integer" id="S6.SS1.p3.4.m4.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.4.m4.1c">20</annotation></semantics></math> samples.
<br class="ltx_break"></p>
</div>
<div id="S6.SS1.p4" class="ltx_para ltx_noindent">
<p id="S6.SS1.p4.2" class="ltx_p"><span id="S6.SS1.p4.2.1" class="ltx_text ltx_font_bold">Text Generation:</span> We use the Shakespeare dataset, as implemented in TensorFlow Federated <cite class="ltx_cite ltx_citemacro_citep">(Google, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>, to train a two-cell LSTM-based language model as defined in <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al., <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> The dataset comprises sentences extracted from <span id="S6.SS1.p4.2.2" class="ltx_text ltx_font_italic">The Complete Works of William Shakespeare</span> and grouped into <math id="S6.SS1.p4.1.m1.1" class="ltx_Math" alttext="648" display="inline"><semantics id="S6.SS1.p4.1.m1.1a"><mn id="S6.SS1.p4.1.m1.1.1" xref="S6.SS1.p4.1.m1.1.1.cmml">648</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.1.m1.1b"><cn type="integer" id="S6.SS1.p4.1.m1.1.1.cmml" xref="S6.SS1.p4.1.m1.1.1">648</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.1.m1.1c">648</annotation></semantics></math> fictional characters. We follow the LEAF experimental configuration and use a batch size of <math id="S6.SS1.p4.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S6.SS1.p4.2.m2.1a"><mn id="S6.SS1.p4.2.m2.1.1" xref="S6.SS1.p4.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p4.2.m2.1b"><cn type="integer" id="S6.SS1.p4.2.m2.1.1.cmml" xref="S6.SS1.p4.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p4.2.m2.1c">4</annotation></semantics></math> samples.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Hardware Configuration</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We consider two hardware configurations that reflect common research centres, namely <em id="S6.SS2.p1.1.1" class="ltx_emph ltx_font_italic">single-node</em> and <em id="S6.SS2.p1.1.2" class="ltx_emph ltx_font_italic">multi-node</em>. We also further distinguish between simulations using <em id="S6.SS2.p1.1.3" class="ltx_emph ltx_font_italic">homogeneous</em> and <em id="S6.SS2.p1.1.4" class="ltx_emph ltx_font_italic">heterogeneous</em> GPUs. As previously indicated, all the FedScale, Google Speech, and Shakespeare experiments use a fixed batch size. This makes differences in worker VRAM attributable only to model size and input data shape.
<br class="ltx_break"></p>
</div>
<div id="S6.SS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS2.p2.6" class="ltx_p"><span id="S6.SS2.p2.6.1" class="ltx_text ltx_font_bold">Single-node:</span> Our single-node experiments are all run on <em id="S6.SS2.p2.6.2" class="ltx_emph ltx_font_italic">node 0</em> containing Nvidia A40 GPUs and an Intel (R) Xeon (R) Gold 6152 with <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="88" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mn id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">88</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><cn type="integer" id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">88</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">88</annotation></semantics></math> cores. For OpenImage, the A40s are filled with <math id="S6.SS2.p2.2.m2.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S6.SS2.p2.2.m2.1a"><mn id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><cn type="integer" id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">13</annotation></semantics></math> workers each, given the size of ShuffleNetV2 <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite> and the input size. For Google Speech, we use only <math id="S6.SS2.p2.3.m3.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S6.SS2.p2.3.m3.1a"><mn id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><cn type="integer" id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">4</annotation></semantics></math> workers per GPU due to data loading requirements, while for Shakespeare, we use <math id="S6.SS2.p2.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.SS2.p2.4.m4.1a"><mn id="S6.SS2.p2.4.m4.1.1" xref="S6.SS2.p2.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.4.m4.1b"><cn type="integer" id="S6.SS2.p2.4.m4.1.1.cmml" xref="S6.SS2.p2.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.4.m4.1c">10</annotation></semantics></math> workers total to match the number of clients. Each A40 is paired with <math id="S6.SS2.p2.5.m5.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S6.SS2.p2.5.m5.1a"><mn id="S6.SS2.p2.5.m5.1.1" xref="S6.SS2.p2.5.m5.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.5.m5.1b"><cn type="integer" id="S6.SS2.p2.5.m5.1.1.cmml" xref="S6.SS2.p2.5.m5.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.5.m5.1c">11</annotation></semantics></math> CPU cores out of the <math id="S6.SS2.p2.6.m6.1" class="ltx_Math" alttext="88" display="inline"><semantics id="S6.SS2.p2.6.m6.1a"><mn id="S6.SS2.p2.6.m6.1.1" xref="S6.SS2.p2.6.m6.1.1.cmml">88</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.6.m6.1b"><cn type="integer" id="S6.SS2.p2.6.m6.1.1.cmml" xref="S6.SS2.p2.6.m6.1.1">88</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.6.m6.1c">88</annotation></semantics></math> mentioned above. We run all our experiments with <em id="S6.SS2.p2.6.3" class="ltx_emph ltx_font_italic">Two homogeneous GPUs</em> on this node using two A40s with 22 CPU cores available.
<br class="ltx_break"></p>
</div>
<div id="S6.SS2.p3" class="ltx_para ltx_noindent">
<p id="S6.SS2.p3.5" class="ltx_p"><span id="S6.SS2.p3.5.1" class="ltx_text ltx_font_bold">Multi-node:</span> Our multi-node experiments are run on a combination of the aforementioned <em id="S6.SS2.p3.5.2" class="ltx_emph ltx_font_italic">node 0</em> containing A40s and <em id="S6.SS2.p3.5.3" class="ltx_emph ltx_font_italic">node 1</em>. <em id="S6.SS2.p3.5.4" class="ltx_emph ltx_font_italic">Node 1</em> contains Nvidia RTX 2080 Ti GPUs and an Intel(R) Xeon(R) CPU E5-2680 v4 containing <math id="S6.SS2.p3.1.m1.1" class="ltx_Math" alttext="56" display="inline"><semantics id="S6.SS2.p3.1.m1.1a"><mn id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml">56</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><cn type="integer" id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1">56</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">56</annotation></semantics></math> cores. For OpenImage, we use <math id="S6.SS2.p3.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S6.SS2.p3.2.m2.1a"><mn id="S6.SS2.p3.2.m2.1.1" xref="S6.SS2.p3.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.2.m2.1b"><cn type="integer" id="S6.SS2.p3.2.m2.1.1.cmml" xref="S6.SS2.p3.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.2.m2.1c">4</annotation></semantics></math> workers per 2080 due to VRAM constraints. The worker setup for Google Speech and Shakespeare is the same as on <em id="S6.SS2.p3.5.5" class="ltx_emph ltx_font_italic">node 0</em>. Each 2080 is paired with <math id="S6.SS2.p3.3.m3.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S6.SS2.p3.3.m3.1a"><mn id="S6.SS2.p3.3.m3.1.1" xref="S6.SS2.p3.3.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.3.m3.1b"><cn type="integer" id="S6.SS2.p3.3.m3.1.1.cmml" xref="S6.SS2.p3.3.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.3.m3.1c">8</annotation></semantics></math> CPU cores out of the <math id="S6.SS2.p3.4.m4.1" class="ltx_Math" alttext="56" display="inline"><semantics id="S6.SS2.p3.4.m4.1a"><mn id="S6.SS2.p3.4.m4.1.1" xref="S6.SS2.p3.4.m4.1.1.cmml">56</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.4.m4.1b"><cn type="integer" id="S6.SS2.p3.4.m4.1.1.cmml" xref="S6.SS2.p3.4.m4.1.1">56</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.4.m4.1c">56</annotation></semantics></math> mentioned above. Our experiments for heterogeneous hardware shall use one A40 from <em id="S6.SS2.p3.5.6" class="ltx_emph ltx_font_italic">node 0</em> paired with <math id="S6.SS2.p3.5.m5.1" class="ltx_Math" alttext="1\mbox{-}4" display="inline"><semantics id="S6.SS2.p3.5.m5.1a"><mrow id="S6.SS2.p3.5.m5.1.1" xref="S6.SS2.p3.5.m5.1.1.cmml"><mn id="S6.SS2.p3.5.m5.1.1.2" xref="S6.SS2.p3.5.m5.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S6.SS2.p3.5.m5.1.1.1" xref="S6.SS2.p3.5.m5.1.1.1.cmml">​</mo><mtext id="S6.SS2.p3.5.m5.1.1.3" xref="S6.SS2.p3.5.m5.1.1.3a.cmml">-</mtext><mo lspace="0em" rspace="0em" id="S6.SS2.p3.5.m5.1.1.1a" xref="S6.SS2.p3.5.m5.1.1.1.cmml">​</mo><mn id="S6.SS2.p3.5.m5.1.1.4" xref="S6.SS2.p3.5.m5.1.1.4.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.5.m5.1b"><apply id="S6.SS2.p3.5.m5.1.1.cmml" xref="S6.SS2.p3.5.m5.1.1"><times id="S6.SS2.p3.5.m5.1.1.1.cmml" xref="S6.SS2.p3.5.m5.1.1.1"></times><cn type="integer" id="S6.SS2.p3.5.m5.1.1.2.cmml" xref="S6.SS2.p3.5.m5.1.1.2">1</cn><ci id="S6.SS2.p3.5.m5.1.1.3a.cmml" xref="S6.SS2.p3.5.m5.1.1.3"><mtext id="S6.SS2.p3.5.m5.1.1.3.cmml" xref="S6.SS2.p3.5.m5.1.1.3">-</mtext></ci><cn type="integer" id="S6.SS2.p3.5.m5.1.1.4.cmml" xref="S6.SS2.p3.5.m5.1.1.4">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.5.m5.1c">1\mbox{-}4</annotation></semantics></math> Nvidia 2080s. We only use more than one 2080 in scenarios where we want to observe how balancing the number of workers across GPU types affects performance. After the workers have completed their partial aggregation on the <em id="S6.SS2.p3.5.7" class="ltx_emph ltx_font_italic">CPU</em> of their respective node, all final aggregation steps happen on <em id="S6.SS2.p3.5.8" class="ltx_emph ltx_font_italic">node 0</em> on the <em id="S6.SS2.p3.5.9" class="ltx_emph ltx_font_italic">CPU</em>.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Framework Benchmark</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">The fundamental contribution of the design of our system is to overcome the pull-based queuing system to allow for fast FL simulation.
In this experiment, we aim to assess the speed of our solution, whatever the placement strategies adopted by the client allocator.
We compare to the other frameworks by measuring the throughput of the simulation since this describes the system’s speed as the number of clients the system can train per second.
It is calculated by dividing each round completion time by the number of clients trained.
We perform the three tasks under different homogeneous hardware settings having 1, 2 or 4 homogeneous GPUs.
For homogeneous hardware, different placement strategies perform similarly. Thus, the performance is mainly impacted by the system design.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Policies Benchmark</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">This work proposes different placement strategies to use in the simulation.
In this experiment, we aim to answer the question of how the choice of strategy impacts the performance of the simulation.
We measure the throughput of the FL simulation and compare different strategies against each other.
We are also interested in identifying how effective the strategies are in balancing the load between the workers.
To this aim, we measure the time difference between the first and last workers to finish processing their clients and compare the strategies against each other.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.2" class="ltx_p">The experiment is designed to be executed in the most challenging hardware setting, the heterogeneous one with two nodes having different GPU types.
First, we train all the tasks in the setting with one GPU per type for two GPUs.
We keep the number of clients per round to 100, except for Shakespeare, where we set <math id="S6.SS4.p2.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.SS4.p2.1.m1.1a"><mn id="S6.SS4.p2.1.m1.1.1" xref="S6.SS4.p2.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p2.1.m1.1b"><cn type="integer" id="S6.SS4.p2.1.m1.1.1.cmml" xref="S6.SS4.p2.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p2.1.m1.1c">10</annotation></semantics></math> clients for <math id="S6.SS4.p2.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S6.SS4.p2.2.m2.1a"><mn id="S6.SS4.p2.2.m2.1.1" xref="S6.SS4.p2.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p2.2.m2.1b"><cn type="integer" id="S6.SS4.p2.2.m2.1.1.cmml" xref="S6.SS4.p2.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p2.2.m2.1c">100</annotation></semantics></math> rounds.
Here, we compare the throughput of the experiments using different strategies against the fastest other framework.
Second, we train the Image Classification task in different heterogeneous hardware settings, increasing the number of GPUs belonging to the type that can host the lower number of workers.
This second set of experiments uses the two previous nodes, one always having one GPU and the other having 1, 2, 3, and then 4 GPUs.</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p">These settings reflect a typical situation a researcher could face: the available GPUs with more VRAM are few, but more GPUs with less VRAM are available.</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5. </span>System Scalability</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.11" class="ltx_p">Our main contribution in this paper is to allow efficient large-scale simulations at the scale of real-world applications. In this experiment, we aim to measure how our proposed method compares to other Placement Policies as we <em id="S6.SS5.p1.11.1" class="ltx_emph ltx_font_italic">increase the average number of clients per GPU</em>.
The first set of experiments considers the homogeneous scenario of four GPUs of the same type on the same node.
We fix the number of workers because the hardware is fixed, and we progressively increase the number of clients per round while keeping constant the overall number of clients trained.
In order to maintain the total number of trained clients to <math id="S6.SS5.p1.1.m1.1" class="ltx_Math" alttext="10\,000" display="inline"><semantics id="S6.SS5.p1.1.m1.1a"><mn id="S6.SS5.p1.1.m1.1.1" xref="S6.SS5.p1.1.m1.1.1.cmml">10 000</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.1.m1.1b"><cn type="integer" id="S6.SS5.p1.1.m1.1.1.cmml" xref="S6.SS5.p1.1.m1.1.1">10000</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.1.m1.1c">10\,000</annotation></semantics></math>, as it is in other experiments in this work, we choose the values for the number of clients per round to be <math id="S6.SS5.p1.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S6.SS5.p1.2.m2.1a"><mn id="S6.SS5.p1.2.m2.1.1" xref="S6.SS5.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.2.m2.1b"><cn type="integer" id="S6.SS5.p1.2.m2.1.1.cmml" xref="S6.SS5.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.2.m2.1c">100</annotation></semantics></math>, <math id="S6.SS5.p1.3.m3.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S6.SS5.p1.3.m3.1a"><mn id="S6.SS5.p1.3.m3.1.1" xref="S6.SS5.p1.3.m3.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.3.m3.1b"><cn type="integer" id="S6.SS5.p1.3.m3.1.1.cmml" xref="S6.SS5.p1.3.m3.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.3.m3.1c">200</annotation></semantics></math>, <math id="S6.SS5.p1.4.m4.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S6.SS5.p1.4.m4.1a"><mn id="S6.SS5.p1.4.m4.1.1" xref="S6.SS5.p1.4.m4.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.4.m4.1b"><cn type="integer" id="S6.SS5.p1.4.m4.1.1.cmml" xref="S6.SS5.p1.4.m4.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.4.m4.1c">400</annotation></semantics></math>, <math id="S6.SS5.p1.5.m5.1" class="ltx_Math" alttext="625" display="inline"><semantics id="S6.SS5.p1.5.m5.1a"><mn id="S6.SS5.p1.5.m5.1.1" xref="S6.SS5.p1.5.m5.1.1.cmml">625</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.5.m5.1b"><cn type="integer" id="S6.SS5.p1.5.m5.1.1.cmml" xref="S6.SS5.p1.5.m5.1.1">625</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.5.m5.1c">625</annotation></semantics></math>, <math id="S6.SS5.p1.6.m6.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S6.SS5.p1.6.m6.1a"><mn id="S6.SS5.p1.6.m6.1.1" xref="S6.SS5.p1.6.m6.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.6.m6.1b"><cn type="integer" id="S6.SS5.p1.6.m6.1.1.cmml" xref="S6.SS5.p1.6.m6.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.6.m6.1c">1000</annotation></semantics></math> training respectively for <math id="S6.SS5.p1.7.m7.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S6.SS5.p1.7.m7.1a"><mn id="S6.SS5.p1.7.m7.1.1" xref="S6.SS5.p1.7.m7.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.7.m7.1b"><cn type="integer" id="S6.SS5.p1.7.m7.1.1.cmml" xref="S6.SS5.p1.7.m7.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.7.m7.1c">100</annotation></semantics></math>, <math id="S6.SS5.p1.8.m8.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S6.SS5.p1.8.m8.1a"><mn id="S6.SS5.p1.8.m8.1.1" xref="S6.SS5.p1.8.m8.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.8.m8.1b"><cn type="integer" id="S6.SS5.p1.8.m8.1.1.cmml" xref="S6.SS5.p1.8.m8.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.8.m8.1c">50</annotation></semantics></math>, <math id="S6.SS5.p1.9.m9.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S6.SS5.p1.9.m9.1a"><mn id="S6.SS5.p1.9.m9.1.1" xref="S6.SS5.p1.9.m9.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.9.m9.1b"><cn type="integer" id="S6.SS5.p1.9.m9.1.1.cmml" xref="S6.SS5.p1.9.m9.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.9.m9.1c">25</annotation></semantics></math>, <math id="S6.SS5.p1.10.m10.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S6.SS5.p1.10.m10.1a"><mn id="S6.SS5.p1.10.m10.1.1" xref="S6.SS5.p1.10.m10.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.10.m10.1b"><cn type="integer" id="S6.SS5.p1.10.m10.1.1.cmml" xref="S6.SS5.p1.10.m10.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.10.m10.1c">16</annotation></semantics></math>, <math id="S6.SS5.p1.11.m11.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S6.SS5.p1.11.m11.1a"><mn id="S6.SS5.p1.11.m11.1.1" xref="S6.SS5.p1.11.m11.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.p1.11.m11.1b"><cn type="integer" id="S6.SS5.p1.11.m11.1.1.cmml" xref="S6.SS5.p1.11.m11.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.p1.11.m11.1c">10</annotation></semantics></math> rounds.
The workers receive longer lists of clients to train each round, while the total work is always the same.
We measure the system’s throughput for all the proposed client policies and then compare them against the fastest framework.</p>
</div>
<div id="S6.SS5.p2" class="ltx_para">
<p id="S6.SS5.p2.1" class="ltx_p">A second set of experiments considers the most constrained and heterogeneous scenario we had: two nodes, each of them having on GPU of a different type.
Similarly, we played with the number of clients each worker has to train in each round while keeping the total amount of work constant.
For each round, we measure the time difference between the first and last worker to finish processing their clients and the throughput.</p>
</div>
<div id="S6.SS5.p3" class="ltx_para">
<p id="S6.SS5.p3.1" class="ltx_p">We run the above experiments on the Image Classification task with the largest client population.
This setting reflects a common FL scenario where researchers want to investigate the benefits of sampling larger fleets of devices in one round while still constrained by the available hardware.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Evaluation</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this section, we describe the results of the experiments proposed in  <a href="#S6" title="6. Experimental Design ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6</span></a>.
We begin presenting the comparison of the performance of <em id="S7.p1.1.1" class="ltx_emph ltx_font_italic">Pollen</em> against other frameworks.
We continue focusing on the differences between the strategy proposed.
Furthermore, we extend the investigation of the strategies by evaluating them at scale.
Finally, a discussion about the proposed client placement’s limitations is presented.</p>
</div>
<figure id="S7.F6" class="ltx_figure"><img src="/html/2306.17453/assets/x6.png" id="S7.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="338" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S7.F6.4.2" class="ltx_text" style="font-size:90%;">The throughput (the greater, the better) of our client placement systems implemented in Flower compared with FedScale, Flute and standard Flower. Using resource-aware client placement, <em id="S7.F6.4.2.1" class="ltx_emph ltx_font_italic">Pollen</em> outperforms the fastest other framework by a factor 3x-5x, depending on the task. The values reported for our contribution have been chosen for the best-performing strategy that tends to be LB.</span></figcaption>
</figure>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Comparison between frameworks</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p"><em id="S7.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Pollen</em> proves to deliver faster FL simulations compared to Flower (version 1.1) <cite class="ltx_cite ltx_citemacro_citep">(Beutel et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> , Flute (commit@0e8762b) <cite class="ltx_cite ltx_citemacro_citep">(Dimitriadis et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite>, and FedScale (commit@9bfc029a3c)  <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite> in every setting we have tested.
This consistency demonstrates the superiority of a push-based allocation of clients across workers.
The results of the throughput of the simulation in a more accessible homogeneous setting put <em id="S7.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Pollen</em> on top of the classification since it outperforms all the other frameworks.
The example in <a href="#S7.F6" title="In 7. Evaluation ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows that, when using two GPUs of the same type, namely Nvidia A40, on a single node, <em id="S7.SS1.p1.1.3" class="ltx_emph ltx_font_italic">Pollen</em> has the highest throughput for all the tasks.
Compared to the fastest framework, the speed-up obtained is 3x on the Text Generation task, about 2x on the Image Classification task, and 5x on the Speech Recognition task.
It is noted that the performance of all the tasks is varied across the other frameworks except for the Speech Recognition one, in which they perform similarly.
We highlight that in every comparison in which the number of clients per round is fixed, the absolute time of the experiment is inversely proportional to the reported throughput.
In addition, since the Image Classification task has the lowest speed-up factor, we assume that to be the task using which further comparisons will be fairer.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p"><span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_bold">Implications:</span> <em id="S7.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Pollen</em> system outperforms all previous frameworks and can provide a great boost to the scalability of FL simulations thus allowing better FL methods to be developed for production systems.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>The Impact of Client Placement</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.2" class="ltx_p">Given that our design outperforms other frameworks in homogeneous settings, we evaluate the impact the client placement strategies in heterogeneous settings.
Two nodes were available in the setting, with one Nvidia A40 and one Nvidia RTX 2080 Ti, respectively.
The results regarding the throughput are shown in <a href="#S7.T1" title="In 7.2. The Impact of Client Placement ‣ 7. Evaluation ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
We can assume that different strategies deliver the same performance for the Text Generation task because distributing <math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mn id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><cn type="integer" id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">10</annotation></semantics></math> clients across <math id="S7.SS2.p1.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.SS2.p1.2.m2.1a"><mn id="S7.SS2.p1.2.m2.1.1" xref="S7.SS2.p1.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.2.m2.1b"><cn type="integer" id="S7.SS2.p1.2.m2.1.1.cmml" xref="S7.SS2.p1.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.2.m2.1c">10</annotation></semantics></math> workers is trivial.
The Image Classification task shows the biggest variation across strategies, where the LB strategy of <em id="S7.SS2.p1.2.1" class="ltx_emph ltx_font_italic">Pollen</em> outperforms the others.
We note that the RR strategy of <em id="S7.SS2.p1.2.2" class="ltx_emph ltx_font_italic">Pollen</em> underperforms other strategies when the placement is not trivial, presenting the most significant gap in the Image Classification task.
We argue that the peculiarity of the Speech Recognition dataset causes different strategies to have more minor variations in performance.
Furthermore, we highlight the general superiority of our method against the fastest framework in this challenging heterogeneous scenario.</p>
</div>
<figure id="S7.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S7.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S7.T1.4.2" class="ltx_text" style="font-size:90%;">The throughput (the greater, the better) measured in clients per second for three tasks. The proposed <em id="S7.T1.4.2.1" class="ltx_emph ltx_font_italic">Pollen</em>’s strategies are compared against FedScale in the most heterogeneous setting.</span></figcaption>
<div id="S7.T1.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:110.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.1pt,-19.1pt) scale(1.52985842775216,1.52985842775216) ;">
<table id="S7.T1.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T1.5.1.1.1" class="ltx_tr">
<th id="S7.T1.5.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S7.T1.5.1.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Datasets</span></th>
<th id="S7.T1.5.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T1.5.1.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedScale</span></th>
<th id="S7.T1.5.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T1.5.1.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">LB</span></th>
<th id="S7.T1.5.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T1.5.1.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">BU</span></th>
<th id="S7.T1.5.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T1.5.1.1.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">SRR</span></th>
<th id="S7.T1.5.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T1.5.1.1.1.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">RR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T1.5.1.2.1" class="ltx_tr">
<th id="S7.T1.5.1.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S7.T1.5.1.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Google Speech</span></th>
<td id="S7.T1.5.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S7.T1.5.1.2.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">3.6±0.5</span></td>
<td id="S7.T1.5.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S7.T1.5.1.2.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">4.7±0.4</span></td>
<td id="S7.T1.5.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S7.T1.5.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">5.4±0.2</span></td>
<td id="S7.T1.5.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S7.T1.5.1.2.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">5.0±0.2</span></td>
<td id="S7.T1.5.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S7.T1.5.1.2.1.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">5.2±0.3</span></td>
</tr>
<tr id="S7.T1.5.1.3.2" class="ltx_tr">
<th id="S7.T1.5.1.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S7.T1.5.1.3.2.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">OpenImage</span></th>
<td id="S7.T1.5.1.3.2.2" class="ltx_td ltx_align_left"><span id="S7.T1.5.1.3.2.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">3.6±0.5</span></td>
<td id="S7.T1.5.1.3.2.3" class="ltx_td ltx_align_left"><span id="S7.T1.5.1.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">6±1</span></td>
<td id="S7.T1.5.1.3.2.4" class="ltx_td ltx_align_left"><span id="S7.T1.5.1.3.2.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">4.7±0.5</span></td>
<td id="S7.T1.5.1.3.2.5" class="ltx_td ltx_align_left"><span id="S7.T1.5.1.3.2.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">5.2±0.8</span></td>
<td id="S7.T1.5.1.3.2.6" class="ltx_td ltx_align_left"><span id="S7.T1.5.1.3.2.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">4.1±0.9</span></td>
</tr>
<tr id="S7.T1.5.1.4.3" class="ltx_tr">
<th id="S7.T1.5.1.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb"><span id="S7.T1.5.1.4.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Shakespeare</span></th>
<td id="S7.T1.5.1.4.3.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S7.T1.5.1.4.3.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">2.3±0.8</span></td>
<td id="S7.T1.5.1.4.3.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S7.T1.5.1.4.3.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">10±3</span></td>
<td id="S7.T1.5.1.4.3.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S7.T1.5.1.4.3.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">10±4</span></td>
<td id="S7.T1.5.1.4.3.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S7.T1.5.1.4.3.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">10±4</span></td>
<td id="S7.T1.5.1.4.3.6" class="ltx_td ltx_align_left ltx_border_bb"><span id="S7.T1.5.1.4.3.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">11±4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.4" class="ltx_p">We gradually increased the number of available GPUs for the second set of experiments to evaluate the impact of different placement strategies.
In this experiment, we used the different strategy of <em id="S7.SS2.p2.4.1" class="ltx_emph ltx_font_italic">Pollen</em>.
We significantly increased the number of the GPU type that can host the lower amount of workers, namely the Nvidia RTX 2080 Ti.
As such, four settings are compared in this multi-node scenario: <math id="S7.SS2.p2.1.m1.2" class="ltx_Math" alttext="(1,1)" display="inline"><semantics id="S7.SS2.p2.1.m1.2a"><mrow id="S7.SS2.p2.1.m1.2.3.2" xref="S7.SS2.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S7.SS2.p2.1.m1.2.3.2.1" xref="S7.SS2.p2.1.m1.2.3.1.cmml">(</mo><mn id="S7.SS2.p2.1.m1.1.1" xref="S7.SS2.p2.1.m1.1.1.cmml">1</mn><mo id="S7.SS2.p2.1.m1.2.3.2.2" xref="S7.SS2.p2.1.m1.2.3.1.cmml">,</mo><mn id="S7.SS2.p2.1.m1.2.2" xref="S7.SS2.p2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S7.SS2.p2.1.m1.2.3.2.3" xref="S7.SS2.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.1.m1.2b"><interval closure="open" id="S7.SS2.p2.1.m1.2.3.1.cmml" xref="S7.SS2.p2.1.m1.2.3.2"><cn type="integer" id="S7.SS2.p2.1.m1.1.1.cmml" xref="S7.SS2.p2.1.m1.1.1">1</cn><cn type="integer" id="S7.SS2.p2.1.m1.2.2.cmml" xref="S7.SS2.p2.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.1.m1.2c">(1,1)</annotation></semantics></math>, <math id="S7.SS2.p2.2.m2.2" class="ltx_Math" alttext="(1,2)" display="inline"><semantics id="S7.SS2.p2.2.m2.2a"><mrow id="S7.SS2.p2.2.m2.2.3.2" xref="S7.SS2.p2.2.m2.2.3.1.cmml"><mo stretchy="false" id="S7.SS2.p2.2.m2.2.3.2.1" xref="S7.SS2.p2.2.m2.2.3.1.cmml">(</mo><mn id="S7.SS2.p2.2.m2.1.1" xref="S7.SS2.p2.2.m2.1.1.cmml">1</mn><mo id="S7.SS2.p2.2.m2.2.3.2.2" xref="S7.SS2.p2.2.m2.2.3.1.cmml">,</mo><mn id="S7.SS2.p2.2.m2.2.2" xref="S7.SS2.p2.2.m2.2.2.cmml">2</mn><mo stretchy="false" id="S7.SS2.p2.2.m2.2.3.2.3" xref="S7.SS2.p2.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.2.m2.2b"><interval closure="open" id="S7.SS2.p2.2.m2.2.3.1.cmml" xref="S7.SS2.p2.2.m2.2.3.2"><cn type="integer" id="S7.SS2.p2.2.m2.1.1.cmml" xref="S7.SS2.p2.2.m2.1.1">1</cn><cn type="integer" id="S7.SS2.p2.2.m2.2.2.cmml" xref="S7.SS2.p2.2.m2.2.2">2</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.2.m2.2c">(1,2)</annotation></semantics></math>, <math id="S7.SS2.p2.3.m3.2" class="ltx_Math" alttext="(1,3)" display="inline"><semantics id="S7.SS2.p2.3.m3.2a"><mrow id="S7.SS2.p2.3.m3.2.3.2" xref="S7.SS2.p2.3.m3.2.3.1.cmml"><mo stretchy="false" id="S7.SS2.p2.3.m3.2.3.2.1" xref="S7.SS2.p2.3.m3.2.3.1.cmml">(</mo><mn id="S7.SS2.p2.3.m3.1.1" xref="S7.SS2.p2.3.m3.1.1.cmml">1</mn><mo id="S7.SS2.p2.3.m3.2.3.2.2" xref="S7.SS2.p2.3.m3.2.3.1.cmml">,</mo><mn id="S7.SS2.p2.3.m3.2.2" xref="S7.SS2.p2.3.m3.2.2.cmml">3</mn><mo stretchy="false" id="S7.SS2.p2.3.m3.2.3.2.3" xref="S7.SS2.p2.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.3.m3.2b"><interval closure="open" id="S7.SS2.p2.3.m3.2.3.1.cmml" xref="S7.SS2.p2.3.m3.2.3.2"><cn type="integer" id="S7.SS2.p2.3.m3.1.1.cmml" xref="S7.SS2.p2.3.m3.1.1">1</cn><cn type="integer" id="S7.SS2.p2.3.m3.2.2.cmml" xref="S7.SS2.p2.3.m3.2.2">3</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.3.m3.2c">(1,3)</annotation></semantics></math>, <math id="S7.SS2.p2.4.m4.2" class="ltx_Math" alttext="(1,4)" display="inline"><semantics id="S7.SS2.p2.4.m4.2a"><mrow id="S7.SS2.p2.4.m4.2.3.2" xref="S7.SS2.p2.4.m4.2.3.1.cmml"><mo stretchy="false" id="S7.SS2.p2.4.m4.2.3.2.1" xref="S7.SS2.p2.4.m4.2.3.1.cmml">(</mo><mn id="S7.SS2.p2.4.m4.1.1" xref="S7.SS2.p2.4.m4.1.1.cmml">1</mn><mo id="S7.SS2.p2.4.m4.2.3.2.2" xref="S7.SS2.p2.4.m4.2.3.1.cmml">,</mo><mn id="S7.SS2.p2.4.m4.2.2" xref="S7.SS2.p2.4.m4.2.2.cmml">4</mn><mo stretchy="false" id="S7.SS2.p2.4.m4.2.3.2.3" xref="S7.SS2.p2.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.4.m4.2b"><interval closure="open" id="S7.SS2.p2.4.m4.2.3.1.cmml" xref="S7.SS2.p2.4.m4.2.3.2"><cn type="integer" id="S7.SS2.p2.4.m4.1.1.cmml" xref="S7.SS2.p2.4.m4.1.1">1</cn><cn type="integer" id="S7.SS2.p2.4.m4.2.2.cmml" xref="S7.SS2.p2.4.m4.2.2">4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.4.m4.2c">(1,4)</annotation></semantics></math>, where the first number refers to the available Nvidia A40s in the first node and the second to the available Nvidia RTX 2080 Ti in the second node.
The plot in <a href="#S7.F7" title="In 7.2. The Impact of Client Placement ‣ 7. Evaluation ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>
shows the throughput of different strategies over the Image Classification task.
It is worth noting that the number of clients per round has been kept constant during this particular experiment.
In this way, what is changing is the <em id="S7.SS2.p2.4.2" class="ltx_emph ltx_font_italic">density</em> of clients across workers, as having more GPUs available means having more workers.
The results demonstrate that the gain produced by using the LB strategy degrades as the density of clients across workers decreases.
This degradation is reflected in both the metrics taken into consideration, namely the difference in training time between the fastest and slowest workers (<a href="#S7.T2" title="In 7.2. The Impact of Client Placement ‣ 7. Evaluation ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>) and the throughput (<a href="#S7.F7" title="In 7.2. The Impact of Client Placement ‣ 7. Evaluation ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure id="S7.F7" class="ltx_figure"><img src="/html/2306.17453/assets/x7.png" id="S7.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S7.F7.3.2" class="ltx_text" style="font-size:90%;">
The throughput (the greater, the better) measured in clients per second of Round-Robin (RR), Sorted Round Robin (SRR), Batch-uniform (BU), and Learning-based (LB) client placement policies for different multi-node heterogeneous GPU configurations. The Image Classification task has been used.</span></figcaption>
</figure>
<figure id="S7.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S7.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>. </span><span id="S7.T2.3.2" class="ltx_text" style="font-size:90%;">Here the difference in training time between the fastest and slowest workers (the lower the better) measured in seconds for Round-Robin (RR), Sorted Round Robin (SRR), Batch-uniform (BU), and Learning-based (LB) client placement policies using different multi-node heterogeneous GPU configurations. The Image Classification task has been used.</span></figcaption>
<table id="S7.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T2.4.1.1" class="ltx_tr">
<th id="S7.T2.4.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S7.T2.4.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;"># GPUs in</span></th>
<th id="S7.T2.4.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S7.T2.4.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">BU</span></th>
<th id="S7.T2.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T2.4.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">LB</span></th>
<th id="S7.T2.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T2.4.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">RR</span></th>
<th id="S7.T2.4.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T2.4.1.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">SRR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T2.4.2.1" class="ltx_tr">
<th id="S7.T2.4.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row"><span id="S7.T2.4.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">nodes (0, 1)</span></th>
<th id="S7.T2.4.2.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="S7.T2.4.2.1.3" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S7.T2.4.2.1.4" class="ltx_td ltx_th ltx_th_column"></th>
<td id="S7.T2.4.2.1.5" class="ltx_td"></td>
</tr>
<tr id="S7.T2.4.3.2" class="ltx_tr">
<th id="S7.T2.4.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S7.T2.4.3.2.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">(1, 1)</span></th>
<th id="S7.T2.4.3.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S7.T2.4.3.2.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">23±3</span></th>
<td id="S7.T2.4.3.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S7.T2.4.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">10±4</span></td>
<td id="S7.T2.4.3.2.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S7.T2.4.3.2.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">30±7</span></td>
<td id="S7.T2.4.3.2.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S7.T2.4.3.2.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">26±5</span></td>
</tr>
<tr id="S7.T2.4.4.3" class="ltx_tr">
<th id="S7.T2.4.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S7.T2.4.4.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">(1, 2)</span></th>
<th id="S7.T2.4.4.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S7.T2.4.4.3.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">21±4</span></th>
<td id="S7.T2.4.4.3.3" class="ltx_td ltx_align_left"><span id="S7.T2.4.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">18±4</span></td>
<td id="S7.T2.4.4.3.4" class="ltx_td ltx_align_left"><span id="S7.T2.4.4.3.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">28±6</span></td>
<td id="S7.T2.4.4.3.5" class="ltx_td ltx_align_left"><span id="S7.T2.4.4.3.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">31±6</span></td>
</tr>
<tr id="S7.T2.4.5.4" class="ltx_tr">
<th id="S7.T2.4.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S7.T2.4.5.4.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">(1, 3)</span></th>
<th id="S7.T2.4.5.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S7.T2.4.5.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">17±4</span></th>
<td id="S7.T2.4.5.4.3" class="ltx_td ltx_align_left"><span id="S7.T2.4.5.4.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">18±4</span></td>
<td id="S7.T2.4.5.4.4" class="ltx_td ltx_align_left"><span id="S7.T2.4.5.4.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">23±6</span></td>
<td id="S7.T2.4.5.4.5" class="ltx_td ltx_align_left"><span id="S7.T2.4.5.4.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">23±5</span></td>
</tr>
<tr id="S7.T2.4.6.5" class="ltx_tr">
<th id="S7.T2.4.6.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb"><span id="S7.T2.4.6.5.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">(1, 4)</span></th>
<th id="S7.T2.4.6.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb"><span id="S7.T2.4.6.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">15±3</span></th>
<td id="S7.T2.4.6.5.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S7.T2.4.6.5.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">16±4</span></td>
<td id="S7.T2.4.6.5.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S7.T2.4.6.5.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">21±5</span></td>
<td id="S7.T2.4.6.5.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S7.T2.4.6.5.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">21±6</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p"><span id="S7.SS2.p3.1.1" class="ltx_text ltx_font_bold">Implications:</span> Intelligent client placement is highly beneficial for hardware configurations containing multiple GPU types. Given the difficulty of profiling and configuring a large-scale cluster with heterogeneous GPUs, the automatic nature of workload distribution in <em id="S7.SS2.p3.1.2" class="ltx_emph ltx_font_italic">Pollen</em> provides a great advantage.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Scalability of Placement Strategies</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Having already established the effectiveness of <em id="S7.SS3.p1.1.1" class="ltx_emph ltx_font_italic">Pollen</em> and analysed the difference between different client placement strategies, we are now concerned with the proposed method’s scalability.
We begin by comparing with FedScale in a homogeneous setting while increasing the <em id="S7.SS3.p1.1.2" class="ltx_emph ltx_font_italic">density</em> of clients across workers.
In this experiment, instead of increasing the number of available GPUs we increase the number of clients per round while keeping the total number of clients at <math id="S7.SS3.p1.1.m1.1" class="ltx_Math" alttext="10\,000" display="inline"><semantics id="S7.SS3.p1.1.m1.1a"><mn id="S7.SS3.p1.1.m1.1.1" xref="S7.SS3.p1.1.m1.1.1.cmml">10 000</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.1.m1.1b"><cn type="integer" id="S7.SS3.p1.1.m1.1.1.cmml" xref="S7.SS3.p1.1.m1.1.1">10000</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.1.m1.1c">10\,000</annotation></semantics></math>.
This represents a common setting for large-scale experiments in which the number of clients per round can be very high.
Such setting is often explored by theoretical works and it challenge in a simulation context.
As <a href="#S8.F8" title="In 8. Limitations ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a> shows, our push-based placement strategies increase in throughput as the number of clients per round increases.
However, the throughput of FedScale does not scale as the number of clients per round increases.
This is to be expected since as the number of clients per round increases, <em id="S7.SS3.p1.1.3" class="ltx_emph ltx_font_italic">Pollen</em> does fewer and fewer total communication steps.
On the other hand, FedScale does the same number of communication steps. We can also note that the exact placement strategy does not matter, even as the number of clients per round is scaled up, supporting our previous section results.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">In the case of heterogeneous GPUs, our results shown in <a href="#S8.T3" title="In 8. Limitations ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> indicates that the learning-based solution keeps its advantage over all others even as the number of clients increases. This is driven by the learning-based policy of properly distributing works across GPUs, even when many clients are involved in a round. The other placement strategies have a highly inconsistent ordering as they operate under the false assumption that workers should be treated equally. It is also worth noting that all the improvements in throughput we obtain are based on minimising the wait for the slowest worker at the end of a round. As such, while all placement strategies get an initial boost from increasing client density, once sufficient clients are available each round to keep all workers filled for most of the round, throughput stops increasing.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">We now turn our attention to <a href="#S8.T4" title="In 8. Limitations ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, which shows the difference between the fastest and slowest worker; we can observe the cause of this trend. When not accounting for hardware heterogeneity, the other policies reflect the speed difference between the GPUs the workers are placed on. It is clear that the learning-based solution minimises this gap relative to the other placement strategies.</p>
</div>
<div id="S7.SS3.p4" class="ltx_para">
<p id="S7.SS3.p4.1" class="ltx_p"><span id="S7.SS3.p4.1.1" class="ltx_text ltx_font_bold">Implications:</span> The benefits of client placement for heterogeneous GPU configurations persist as the number of clients processed in a round grows. Importantly, these benefits to be highly consistent after a sufficient number of clients per round is reached.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Limitations</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">While the results for the push-based client placement system we have presented are compelling and consistent, it does present several limitations, which we list below:</p>
<ul id="S8.I1" class="ltx_itemize">
<li id="S8.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i1.p1" class="ltx_para">
<p id="S8.I1.i1.p1.1" class="ltx_p">For homogeneous settings, the improvements are brought by the push-based design, as client placement decisions do not generally matter when using random sampling.</p>
</div>
</li>
<li id="S8.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i2.p1" class="ltx_para">
<p id="S8.I1.i2.p1.1" class="ltx_p">For heterogeneous settings, a very low number of clients relative to the number of workers does not allow for sufficient placement decisions. Thus, it is difficult for the learning-based method to balance load across workers.</p>
</div>
</li>
<li id="S8.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i3.p1" class="ltx_para">
<p id="S8.I1.i3.p1.1" class="ltx_p">For vast numbers of clients per round, all placement methods approach their maximum throughput for the task and cease providing further improvements.</p>
</div>
</li>
<li id="S8.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i4.p1" class="ltx_para">
<p id="S8.I1.i4.p1.1" class="ltx_p">The learning-based policy may not provide the theoretically optimal placement for two reasons. First, even when given a perfect prediction of training time for all clients by an oracle, the distribution of these clients over workers is still non-trivial. Second, the error in estimating each client may cause suboptimal placement decisions regardless of the number of available clients or placement strategy. As we observe from <a href="#S8.T4" title="In 8. Limitations ‣ High-throughput Simulation of Federated Learning via Resource-Aware Client Placement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, the gap between the fastest and slowest worker is always proportional to the number of clients in a fixed manner. This indicates a consistent estimation error.</p>
</div>
</li>
<li id="S8.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i5.p1" class="ltx_para">
<p id="S8.I1.i5.p1.1" class="ltx_p">The partial aggregation algorithm required to minimise communication is incompatible with some federated learning aggregation algorithms by default. For example, we do not currently support the adaptive optimisation proposed by <cite class="ltx_cite ltx_citemacro_citet">Reddi et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite></p>
</div>
</li>
</ul>
</div>
<figure id="S8.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S8.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>. </span><span id="S8.T3.4.2" class="ltx_text" style="font-size:90%;">The throughput (the greater, the better) for the Image Classification task using <em id="S8.T3.4.2.1" class="ltx_emph ltx_font_italic">Pollen</em>’s RR, SRR, BU, and LB client placement strategies with the most heterogeneous multi-node hardware configurations. The number of clients per round has been changed while keeping the total number of trained clients constant.</span></figcaption>
<table id="S8.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S8.T3.5.1.1" class="ltx_tr">
<th id="S8.T3.5.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S8.T3.5.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Num. Clients</span></th>
<th id="S8.T3.5.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S8.T3.5.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">BU</span></th>
<th id="S8.T3.5.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S8.T3.5.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">LB</span></th>
<th id="S8.T3.5.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S8.T3.5.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">RR</span></th>
<th id="S8.T3.5.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S8.T3.5.1.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">SRR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S8.T3.5.2.1" class="ltx_tr">
<th id="S8.T3.5.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row"><span id="S8.T3.5.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">per round</span></th>
<th id="S8.T3.5.2.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S8.T3.5.2.1.3" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S8.T3.5.2.1.4" class="ltx_td ltx_th ltx_th_column"></th>
<td id="S8.T3.5.2.1.5" class="ltx_td"></td>
</tr>
<tr id="S8.T3.5.3.2" class="ltx_tr">
<th id="S8.T3.5.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S8.T3.5.3.2.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">100</span></th>
<td id="S8.T3.5.3.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S8.T3.5.3.2.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">3.3±0.4</span></td>
<td id="S8.T3.5.3.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S8.T3.5.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">6±1</span></td>
<td id="S8.T3.5.3.2.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S8.T3.5.3.2.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">3.1±0.6</span></td>
<td id="S8.T3.5.3.2.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S8.T3.5.3.2.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">3.1±0.5</span></td>
</tr>
<tr id="S8.T3.5.4.3" class="ltx_tr">
<th id="S8.T3.5.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S8.T3.5.4.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">200</span></th>
<td id="S8.T3.5.4.3.2" class="ltx_td ltx_align_left"><span id="S8.T3.5.4.3.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">6.7±0.9</span></td>
<td id="S8.T3.5.4.3.3" class="ltx_td ltx_align_left"><span id="S8.T3.5.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">8±1</span></td>
<td id="S8.T3.5.4.3.4" class="ltx_td ltx_align_left"><span id="S8.T3.5.4.3.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">7±1</span></td>
<td id="S8.T3.5.4.3.5" class="ltx_td ltx_align_left"><span id="S8.T3.5.4.3.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">6±1</span></td>
</tr>
<tr id="S8.T3.5.5.4" class="ltx_tr">
<th id="S8.T3.5.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S8.T3.5.5.4.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">400</span></th>
<td id="S8.T3.5.5.4.2" class="ltx_td ltx_align_left"><span id="S8.T3.5.5.4.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">7.1±0.5</span></td>
<td id="S8.T3.5.5.4.3" class="ltx_td ltx_align_left"><span id="S8.T3.5.5.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">7.9±0.7</span></td>
<td id="S8.T3.5.5.4.4" class="ltx_td ltx_align_left"><span id="S8.T3.5.5.4.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">6.9±0.8</span></td>
<td id="S8.T3.5.5.4.5" class="ltx_td ltx_align_left"><span id="S8.T3.5.5.4.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">7.6±0.5</span></td>
</tr>
<tr id="S8.T3.5.6.5" class="ltx_tr">
<th id="S8.T3.5.6.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S8.T3.5.6.5.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">625</span></th>
<td id="S8.T3.5.6.5.2" class="ltx_td ltx_align_left"><span id="S8.T3.5.6.5.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">7.1±0.4</span></td>
<td id="S8.T3.5.6.5.3" class="ltx_td ltx_align_left"><span id="S8.T3.5.6.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">8.4±0.7</span></td>
<td id="S8.T3.5.6.5.4" class="ltx_td ltx_align_left"><span id="S8.T3.5.6.5.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">7.2±0.6</span></td>
<td id="S8.T3.5.6.5.5" class="ltx_td ltx_align_left"><span id="S8.T3.5.6.5.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">6.9±0.4</span></td>
</tr>
<tr id="S8.T3.5.7.6" class="ltx_tr">
<th id="S8.T3.5.7.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb"><span id="S8.T3.5.7.6.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">1000</span></th>
<td id="S8.T3.5.7.6.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S8.T3.5.7.6.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">7.0±0.4</span></td>
<td id="S8.T3.5.7.6.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S8.T3.5.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">8.3±0.9</span></td>
<td id="S8.T3.5.7.6.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S8.T3.5.7.6.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">7.2±0.5</span></td>
<td id="S8.T3.5.7.6.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S8.T3.5.7.6.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">7.4±0.3</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S8.F8" class="ltx_figure"><img src="/html/2306.17453/assets/x8.png" id="S8.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span id="S8.F8.3.2" class="ltx_text" style="font-size:90%;">Throughput of the simulation for different placement strategies compared against the fastest framework in performing the Image Classification task. The total number of trained clients has been kept constant to 10000, while the number of clients per round has been increased. The hardware setting used was homogeneous with 4 Nvidia A40.</span></figcaption>
</figure>
<figure id="S8.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S8.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>. </span><span id="S8.T4.3.2" class="ltx_text" style="font-size:90%;">Here the difference in training time between the fastest
and slowest workers (the lower the better) for Round-Robin (RR), Sorted Round Robin (SRR), Batch-uniform (BU), and Learning-based (LB) client placement policies using the most heterogeneous multi-node hardware configurations. The number of clients per round has been changed while keeping the total number of trained clients constant. The Image Classification task has been used.</span></figcaption>
<table id="S8.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S8.T4.4.1.1" class="ltx_tr">
<th id="S8.T4.4.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S8.T4.4.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Num. Clients</span></th>
<th id="S8.T4.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S8.T4.4.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">BU</span></th>
<th id="S8.T4.4.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S8.T4.4.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">LB</span></th>
<th id="S8.T4.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S8.T4.4.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">RR</span></th>
<th id="S8.T4.4.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S8.T4.4.1.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">SRR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S8.T4.4.2.1" class="ltx_tr">
<th id="S8.T4.4.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row"><span id="S8.T4.4.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">per round</span></th>
<th id="S8.T4.4.2.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S8.T4.4.2.1.3" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S8.T4.4.2.1.4" class="ltx_td ltx_th ltx_th_column"></th>
<td id="S8.T4.4.2.1.5" class="ltx_td"></td>
</tr>
<tr id="S8.T4.4.3.2" class="ltx_tr">
<th id="S8.T4.4.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S8.T4.4.3.2.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">100</span></th>
<td id="S8.T4.4.3.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S8.T4.4.3.2.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">23±3</span></td>
<td id="S8.T4.4.3.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S8.T4.4.3.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">10±4</span></td>
<td id="S8.T4.4.3.2.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S8.T4.4.3.2.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">30±7</span></td>
<td id="S8.T4.4.3.2.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S8.T4.4.3.2.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">26±5</span></td>
</tr>
<tr id="S8.T4.4.4.3" class="ltx_tr">
<th id="S8.T4.4.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S8.T4.4.4.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">200</span></th>
<td id="S8.T4.4.4.3.2" class="ltx_td ltx_align_left"><span id="S8.T4.4.4.3.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">24±3</span></td>
<td id="S8.T4.4.4.3.3" class="ltx_td ltx_align_left"><span id="S8.T4.4.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19±4</span></td>
<td id="S8.T4.4.4.3.4" class="ltx_td ltx_align_left"><span id="S8.T4.4.4.3.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">30±6</span></td>
<td id="S8.T4.4.4.3.5" class="ltx_td ltx_align_left"><span id="S8.T4.4.4.3.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">27±5</span></td>
</tr>
<tr id="S8.T4.4.5.4" class="ltx_tr">
<th id="S8.T4.4.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S8.T4.4.5.4.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">400</span></th>
<td id="S8.T4.4.5.4.2" class="ltx_td ltx_align_left"><span id="S8.T4.4.5.4.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">48±3</span></td>
<td id="S8.T4.4.5.4.3" class="ltx_td ltx_align_left"><span id="S8.T4.4.5.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">41±5</span></td>
<td id="S8.T4.4.5.4.4" class="ltx_td ltx_align_left"><span id="S8.T4.4.5.4.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">56±6</span></td>
<td id="S8.T4.4.5.4.5" class="ltx_td ltx_align_left"><span id="S8.T4.4.5.4.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">43±3</span></td>
</tr>
<tr id="S8.T4.4.6.5" class="ltx_tr">
<th id="S8.T4.4.6.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S8.T4.4.6.5.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">625</span></th>
<td id="S8.T4.4.6.5.2" class="ltx_td ltx_align_left"><span id="S8.T4.4.6.5.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">77±4</span></td>
<td id="S8.T4.4.6.5.3" class="ltx_td ltx_align_left"><span id="S8.T4.4.6.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">62±10</span></td>
<td id="S8.T4.4.6.5.4" class="ltx_td ltx_align_left"><span id="S8.T4.4.6.5.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">83±8</span></td>
<td id="S8.T4.4.6.5.5" class="ltx_td ltx_align_left"><span id="S8.T4.4.6.5.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">78±5</span></td>
</tr>
<tr id="S8.T4.4.7.6" class="ltx_tr">
<th id="S8.T4.4.7.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb"><span id="S8.T4.4.7.6.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">1000</span></th>
<td id="S8.T4.4.7.6.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S8.T4.4.7.6.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">125±7</span></td>
<td id="S8.T4.4.7.6.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S8.T4.4.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">106±18</span></td>
<td id="S8.T4.4.7.6.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S8.T4.4.7.6.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">134±11</span></td>
<td id="S8.T4.4.7.6.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S8.T4.4.7.6.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">118±6</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusion</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.2" class="ltx_p">In this work we have shown that the current <em id="S9.p1.2.1" class="ltx_emph ltx_font_italic">pull-based</em> approaches for client placement available in Federated Learning frameworks are incapable of exploiting both client and hardware heterogeneity. Based on this fact, we have proposed a resource-aware client placement algorithm which minimises communication costs between the server and workers responsible with training the clients. An extensive experimental evaluation has shown our proposed changes to bring improvements of <math id="S9.p1.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S9.p1.1.m1.1a"><mrow id="S9.p1.1.m1.1.1" xref="S9.p1.1.m1.1.1.cmml"><mn id="S9.p1.1.m1.1.1.2" xref="S9.p1.1.m1.1.1.2.cmml">50</mn><mo id="S9.p1.1.m1.1.1.1" xref="S9.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S9.p1.1.m1.1b"><apply id="S9.p1.1.m1.1.1.cmml" xref="S9.p1.1.m1.1.1"><csymbol cd="latexml" id="S9.p1.1.m1.1.1.1.cmml" xref="S9.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S9.p1.1.m1.1.1.2.cmml" xref="S9.p1.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S9.p1.1.m1.1c">50\%</annotation></semantics></math> to <math id="S9.p1.2.m2.1" class="ltx_Math" alttext="400\%" display="inline"><semantics id="S9.p1.2.m2.1a"><mrow id="S9.p1.2.m2.1.1" xref="S9.p1.2.m2.1.1.cmml"><mn id="S9.p1.2.m2.1.1.2" xref="S9.p1.2.m2.1.1.2.cmml">400</mn><mo id="S9.p1.2.m2.1.1.1" xref="S9.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S9.p1.2.m2.1b"><apply id="S9.p1.2.m2.1.1.cmml" xref="S9.p1.2.m2.1.1"><csymbol cd="latexml" id="S9.p1.2.m2.1.1.1.cmml" xref="S9.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S9.p1.2.m2.1.1.2.cmml" xref="S9.p1.2.m2.1.1.2">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S9.p1.2.m2.1c">400\%</annotation></semantics></math>. This significant improvement allows for the quick development of algorithms with downstream applications in training fleets of millions of edge-devices. Since our modifications have a relatively small footprint, we recommend that all active FL frameworks adopt a similar design for their simulation engines.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balakrishnan et al. [2022]</span>
<span class="ltx_bibblock">
Ravikumar Balakrishnan, Tian Li, Tianyi Zhou, Nageen Himayat, Virginia Smith,
and Jeff A. Bilmes.

</span>
<span class="ltx_bibblock">Diverse client selection for federated learning via submodular
maximization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=nwKXyFvaUm" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=nwKXyFvaUm</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al. [2020]</span>
<span class="ltx_bibblock">
Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, and
Nicholas D. Lane.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning research framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2007.14390, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2007.14390" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2007.14390</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. [2019]</span>
<span class="ltx_bibblock">
Kallista A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloé Kiddon, Jakub Konečný,
Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel
Ramage, and Jason Roselander.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock">In Ameet Talwalkar, Virginia Smith, and Matei Zaharia, editors,
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford,
CA, USA, March 31 - April 2, 2019</em>. mlsys.org, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlsys.org/book/271.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlsys.org/book/271.pdf</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. [2018]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konečný, H. Brendan
McMahan, Virginia Smith, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">LEAF: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1812.01097, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1812.01097" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1812.01097</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Charles et al. [2021]</span>
<span class="ltx_bibblock">
Zachary Charles, Zachary Garrett, Zhouyuan Huo, Sergei Shmulyian, and Virginia
Smith.

</span>
<span class="ltx_bibblock">On large-cohort training for federated learning.

</span>
<span class="ltx_bibblock">In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy
Liang, and Jennifer Wortman Vaughan, editors, <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 34: Annual Conference on Neural Information
Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual</em>, pages
20461–20475, 2021.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2021/hash/ab9ebd57177b5106ad7879f0896685d4-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2021/hash/ab9ebd57177b5106ad7879f0896685d4-Abstract.html</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. [2020]</span>
<span class="ltx_bibblock">
Yae Jee Cho, Jianyu Wang, and Gauri Joshi.

</span>
<span class="ltx_bibblock">Client selection in federated learning: Convergence analysis and
power-of-choice selection strategies.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2010.01243, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2010.01243" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2010.01243</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dimitriadis et al. [2022]</span>
<span class="ltx_bibblock">
Dimitrios Dimitriadis, Mirian Hipolito Garcia, Daniel Madrigal, Andre Manoel,
and Robert Sim.

</span>
<span class="ltx_bibblock">Flute: A scalable, extensible framework for high-performance
federated learning simulations, March 2022.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://www.microsoft.com/en-us/research/publication/flute-a-scalable-extensible-framework-for-high-performance-federated-learning-simulations/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.microsoft.com/en-us/research/publication/flute-a-scalable-extensible-framework-for-high-performance-federated-learning-simulations/</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2022]</span>
<span class="ltx_bibblock">
Wei Gao, Qinghao Hu, Zhisheng Ye, Peng Sun, Xiaolin Wang, Yingwei Luo, Tianwei
Zhang, and Yonggang Wen.

</span>
<span class="ltx_bibblock">Deep learning workload scheduling in GPU datacenters: Taxonomy,
challenges and vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2205.11913, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2205.11913</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2205.11913" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2205.11913</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google [2019]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Tensorflow federated, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. [2016]</span>
<span class="ltx_bibblock">
Song Han, Huizi Mao, and William J. Dally.

</span>
<span class="ltx_bibblock">Deep compression: Compressing deep neural network with pruning,
trained quantization and huffman coding.

</span>
<span class="ltx_bibblock">In Yoshua Bengio and Yann LeCun, editors, <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">4th International
Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings</em>, 2016.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1510.00149" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1510.00149</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2016]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</em>, pages
770–778. IEEE Computer Society, 2016.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2016.90</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPR.2016.90</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. [2021]</span>
<span class="ltx_bibblock">
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià
Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaïd
Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu,
Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub
Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh
Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng
Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu
Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Found. Trends Mach. Learn.</em>, 14(1-2):1–210, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1561/2200000083</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1561/2200000083" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1561/2200000083</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koupai et al. [2022]</span>
<span class="ltx_bibblock">
Armand K. Koupai, Mohammud Junaid Bocus, Raúl Santos-Rodríguez,
Robert J. Piechocki, and Ryan McConville.

</span>
<span class="ltx_bibblock">Self-supervised multimodal fusion transformer for passive activity
recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2209.03765, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2209.03765</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2209.03765" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2209.03765</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova et al. [2020]</span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin,
Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, Tom Duerig, and Vittorio Ferrari.

</span>
<span class="ltx_bibblock">The open images dataset V4.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Int. J. Comput. Vis.</em>, 128(7):1956–1981,
2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/s11263-020-01316-z</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1007/s11263-020-01316-z" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s11263-020-01316-z</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. [2020]</span>
<span class="ltx_bibblock">
HyeokHyen Kwon, Catherine Tong, Harish Haresamudram, Yan Gao, Gregory D. Abowd,
Nicholas D. Lane, and Thomas Plötz.

</span>
<span class="ltx_bibblock">Imutube: Automatic extraction of virtual on-body accelerometry from
video for human activity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>,
4(3):87:1–87:29, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3411841</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3411841" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3411841</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. [2022]</span>
<span class="ltx_bibblock">
Fan Lai, Yinwei Dai, Sanjay Sri Vallabh Singapuram, Jiachen Liu, Xiangfeng Zhu,
Harsha V. Madhyastha, and Mosharaf Chowdhury.

</span>
<span class="ltx_bibblock">Fedscale: Benchmarking model and system performance of federated
learning at scale.

</span>
<span class="ltx_bibblock">In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvári, Gang Niu, and Sivan Sabato, editors, <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International
Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,
Maryland, USA</em>, volume 162 of <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, pages 11814–11827. PMLR, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlr.press/v162/lai22a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v162/lai22a.html</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020a]</span>
<span class="ltx_bibblock">
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,
Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala.

</span>
<span class="ltx_bibblock">Pytorch distributed: Experiences on accelerating data parallel
training.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 13(12):3005–3018,
2020a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.14778/3415478.3415530</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://www.vldb.org/pvldb/vol13/p3005-li.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.vldb.org/pvldb/vol13/p3005-li.pdf</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020b]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Process. Mag.</em>, 37(3):50–60,
2020b.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/MSP.2020.2975749</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/MSP.2020.2975749" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/MSP.2020.2975749</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2022]</span>
<span class="ltx_bibblock">
Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and song han.

</span>
<span class="ltx_bibblock">On-device training under 256KB memory.

</span>
<span class="ltx_bibblock">In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
editors, <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=zGvRdBW06F5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=zGvRdBW06F5</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2018]</span>
<span class="ltx_bibblock">
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.

</span>
<span class="ltx_bibblock">Shufflenet V2: practical guidelines for efficient CNN
architecture design.

</span>
<span class="ltx_bibblock">In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
Weiss, editors, <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2018 - 15th European
Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIV</em>,
volume 11218 of <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 122–138.
Springer, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-030-01264-9“˙8</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1007/978-3-030-01264-9_8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-030-01264-9_8</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. [2017]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In Aarti Singh and Xiaojin (Jerry) Zhu, editors, <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 20th International Conference on Artificial Intelligence and Statistics,
AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA</em>, volume 54 of
<em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 1273–1282. PMLR,
2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moritz et al. [2018]</span>
<span class="ltx_bibblock">
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw,
Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and
Ion Stoica.

</span>
<span class="ltx_bibblock">Ray: A distributed framework for emerging AI applications.

</span>
<span class="ltx_bibblock">In Andrea C. Arpaci-Dusseau and Geoff Voelker, editors, <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">13th
USENIX Symposium on Operating Systems Design and Implementation, OSDI
2018, Carlsbad, CA, USA, October 8-10, 2018</em>, pages 561–577. USENIX
Association, 2018.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://www.usenix.org/conference/osdi18/presentation/nishihara" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/osdi18/presentation/nishihara</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishio and Yonetani [2019]</span>
<span class="ltx_bibblock">
Takayuki Nishio and Ryo Yonetani.

</span>
<span class="ltx_bibblock">Client selection for federated learning with heterogeneous resources
in mobile edge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Communications,
ICC 2019, Shanghai, China, May 20-24, 2019</em>, pages 1–7. IEEE, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICC.2019.8761315</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/ICC.2019.8761315" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICC.2019.8761315</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. [2021]</span>
<span class="ltx_bibblock">
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=LkFG3lB13U5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=LkFG3lB13U5</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sozinov et al. [2018]</span>
<span class="ltx_bibblock">
Konstantin Sozinov, Vladimir Vlassov, and Sarunas Girdzijauskas.

</span>
<span class="ltx_bibblock">Human activity recognition using federated learning.

</span>
<span class="ltx_bibblock">In Jinjun Chen and Laurence T. Yang, editors, <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE
International Conference on Parallel &amp; Distributed Processing with
Applications, Ubiquitous Computing &amp; Communications, Big Data &amp; Cloud
Computing, Social Computing &amp; Networking, Sustainable Computing &amp;
Communications, ISPA/IUCC/BDCloud/SocialCom/SustainCom 2018, Melbourne,
Australia, December 11-13, 2018</em>, pages 1103–1111. IEEE, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/BDCloud.2018.00164</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/BDCloud.2018.00164" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/BDCloud.2018.00164</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong et al. [2020]</span>
<span class="ltx_bibblock">
Catherine Tong, Shyam A. Tailor, and Nicholas D. Lane.

</span>
<span class="ltx_bibblock">Are accelerometers for activity recognition a dead-end?

</span>
<span class="ltx_bibblock">In Padmanabhan Pillai and Qin Lv, editors, <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">HotMobile ’20: The
21st International Workshop on Mobile Computing Systems and Applications,
Austin, TX, USA, March 3-4, 2020</em>, pages 39–44. ACM, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3376897.3377867</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3376897.3377867" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3376897.3377867</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023]</span>
<span class="ltx_bibblock">
Ewen Wang, Ajay Kannan, Yuefeng Liang, Boyi Chen, and Mosharaf Chowdhury.

</span>
<span class="ltx_bibblock">FLINT: A platform for federated learning integration.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.12862, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2302.12862</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2302.12862" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2302.12862</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warden [2018]</span>
<span class="ltx_bibblock">
Pete Warden.

</span>
<span class="ltx_bibblock">Speech commands: A dataset for limited-vocabulary speech
recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1804.03209, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1804.03209" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1804.03209</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. [2018]</span>
<span class="ltx_bibblock">
Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun
Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan
Yang, and Lidong Zhou.

</span>
<span class="ltx_bibblock">Gandiva: Introspective cluster scheduling for deep learning.

</span>
<span class="ltx_bibblock">In Andrea C. Arpaci-Dusseau and Geoff Voelker, editors, <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">13th
USENIX Symposium on Operating Systems Design and Implementation, OSDI
2018, Carlsbad, CA, USA, October 8-10, 2018</em>, pages 595–610. USENIX
Association, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.usenix.org/conference/osdi18/presentation/xiao" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/osdi18/presentation/xiao</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2020]</span>
<span class="ltx_bibblock">
Han Zhao, Weihao Cui, Quan Chen, Jingwen Leng, Kai Yu, Deze Zeng, Chao Li, and
Minyi Guo.

</span>
<span class="ltx_bibblock">CODA: improving resource utilization by slimming and co-locating
DNN and CPU jobs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">40th IEEE International Conference on Distributed
Computing Systems, ICDCS 2020, Singapore, November 29 - December 1, 2020</em>,
pages 853–863. IEEE, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICDCS47774.2020.00069</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1109/ICDCS47774.2020.00069" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICDCS47774.2020.00069</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.17452" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.17453" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.17453">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.17453" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.17454" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 21:11:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
