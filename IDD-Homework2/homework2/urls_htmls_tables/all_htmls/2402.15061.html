<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Fine-tuning Large Language Models for Domain-specific Machine Translation</title>
<!--Generated on Fri Feb 23 02:09:29 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.15061v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S1" title="1 Introduction ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S2" title="2 Related Work ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S3" title="3 Prompt-oriented Fine-tuning for LLMs ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Prompt-oriented Fine-tuning for LLMs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S3.SS1" title="3.1 Domain-specific Data Generation ‣ 3 Prompt-oriented Fine-tuning for LLMs ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Domain-specific Data Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S3.SS2" title="3.2 Zero-shot Prompting with Instructions ‣ 3 Prompt-oriented Fine-tuning for LLMs ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Zero-shot Prompting with Instructions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S3.SS3" title="3.3 Dictionary-based Prompting ‣ 3 Prompt-oriented Fine-tuning for LLMs ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Dictionary-based Prompting</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4" title="4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS1" title="4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS1.SSS1" title="4.1.1 Datasets ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS1.SSS2" title="4.1.2 Evaluation Metrics ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS1.SSS3" title="4.1.3 Implementation Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS2" title="4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Effects of Fine-tuning on MT Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS3" title="4.3 Effects of Instruction Templates on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Effects of Instruction Templates on MT Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS4" title="4.4 Basic Results ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Basic Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS5" title="4.5 Dictionary-based Prompt Strategy for MT ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Dictionary-based Prompt Strategy for MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS6" title="4.6 Fine-Tuning LLM on Domain-specific MT ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Fine-Tuning LLM on Domain-specific MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS7" title="4.7 Efficiency ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Efficiency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.SS8" title="4.8 Case Study ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.8 </span>Case Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S5" title="5 Conclusion ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2402.15061v1 [cs.CL] 23 Feb 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fine-tuning Large Language Models for Domain-specific Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jiawei Zheng<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mn id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><cn id="id1.1.m1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hanghai Hong<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id2.1.m1.1"><semantics id="id2.1.m1.1a"><msup id="id2.1.m1.1.1" xref="id2.1.m1.1.1.cmml"><mi id="id2.1.m1.1.1a" xref="id2.1.m1.1.1.cmml"></mi><mn id="id2.1.m1.1.1.1" xref="id2.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id2.1.m1.1b"><apply id="id2.1.m1.1.1.cmml" xref="id2.1.m1.1.1"><cn id="id2.1.m1.1.1.1.cmml" type="integer" xref="id2.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id2.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoli Wang<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id3.1.m1.1"><semantics id="id3.1.m1.1a"><msup id="id3.1.m1.1.1" xref="id3.1.m1.1.1.cmml"><mi id="id3.1.m1.1.1a" xref="id3.1.m1.1.1.cmml"></mi><mn id="id3.1.m1.1.1.1" xref="id3.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id3.1.m1.1b"><apply id="id3.1.m1.1.1.cmml" xref="id3.1.m1.1.1"><cn id="id3.1.m1.1.1.1.cmml" type="integer" xref="id3.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id3.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jingsong Su<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id4.1.m1.1"><semantics id="id4.1.m1.1a"><msup id="id4.1.m1.1.1" xref="id4.1.m1.1.1.cmml"><mi id="id4.1.m1.1.1a" xref="id4.1.m1.1.1.cmml"></mi><mn id="id4.1.m1.1.1.1" xref="id4.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id4.1.m1.1b"><apply id="id4.1.m1.1.1.cmml" xref="id4.1.m1.1.1"><cn id="id4.1.m1.1.1.1.cmml" type="integer" xref="id4.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id4.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yonggui Liang<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id5.1.m1.1"><semantics id="id5.1.m1.1a"><msup id="id5.1.m1.1.1" xref="id5.1.m1.1.1.cmml"><mi id="id5.1.m1.1.1a" xref="id5.1.m1.1.1.cmml"></mi><mn id="id5.1.m1.1.1.1" xref="id5.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id5.1.m1.1b"><apply id="id5.1.m1.1.1.cmml" xref="id5.1.m1.1.1"><cn id="id5.1.m1.1.1.1.cmml" type="integer" xref="id5.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id5.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>&amp;Shikai Wu<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id6.2.m2.1"><semantics id="id6.2.m2.1a"><msup id="id6.2.m2.1.1" xref="id6.2.m2.1.1.cmml"><mi id="id6.2.m2.1.1a" xref="id6.2.m2.1.1.cmml"></mi><mn id="id6.2.m2.1.1.1" xref="id6.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id6.2.m2.1b"><apply id="id6.2.m2.1.1.cmml" xref="id6.2.m2.1.1"><cn id="id6.2.m2.1.1.1.cmml" type="integer" xref="id6.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id6.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"/><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id7.3.m3.1"><semantics id="id7.3.m3.1a"><msup id="id7.3.m3.1.1" xref="id7.3.m3.1.1.cmml"><mi id="id7.3.m3.1.1a" xref="id7.3.m3.1.1.cmml"></mi><mn id="id7.3.m3.1.1.1" xref="id7.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id7.3.m3.1b"><apply id="id7.3.m3.1.1.cmml" xref="id7.3.m3.1.1"><cn id="id7.3.m3.1.1.1.cmml" type="integer" xref="id7.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id7.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>School of Informatics, Xiamen University
<br class="ltx_break"/><math alttext="{}^{2}" class="ltx_Math" display="inline" id="id8.4.m4.1"><semantics id="id8.4.m4.1a"><msup id="id8.4.m4.1.1" xref="id8.4.m4.1.1.cmml"><mi id="id8.4.m4.1.1a" xref="id8.4.m4.1.1.cmml"></mi><mn id="id8.4.m4.1.1.1" xref="id8.4.m4.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id8.4.m4.1b"><apply id="id8.4.m4.1.1.cmml" xref="id8.4.m4.1.1"><cn id="id8.4.m4.1.1.1.cmml" type="integer" xref="id8.4.m4.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.4.m4.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id8.4.m4.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>xFusion Digital Technologies Co., Ltd
<br class="ltx_break"/>{zhengjiawei, hanghaih}@stu.xmu.edu.cn,
{xlwang, jssu}@xmu.edu.cn,
<br class="ltx_break"/>{liangyonggui, wushikai}@xfusion.com
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Large language models (LLMs) have made significant progress in machine translation (MT). However, their potential in domain-specific MT remains under-explored. Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain-specific data often require high training costs for domain adaptation, and may weaken the zero-shot MT capabilities of LLMs due to over-specialization. The aforementioned methods can struggle to translate rare words in domain transfer scenarios. To address these challenges, this paper proposes a prompt-oriented fine-tuning method, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT tasks. First, we construct a task-specific mix-domain dataset, which is then used to fine-tune the LLM with LoRA. This can eliminate the need for input translation examples, post-processing, or over-specialization. By zero-shot prompting with instructions, we adapt the MT tasks to the target domain at inference time. To further elicit the MT capability for rare words, we construct new prompts by incorporating domain-specific bilingual vocabulary. We also conduct extensive experiments on both publicly available and self-constructed datasets. The results show that our LlamaIT can significantly enhance the domain-specific MT capabilities of the LLM, meanwhile preserving its zero-shot MT capabilities.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="440" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Results of the Llama2-7B are reported as chrF++(%), COMET(%) and BLEU(%) for the Chinese<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S0.F1.2.m1.1"><semantics id="S0.F1.2.m1.1b"><mo id="S0.F1.2.m1.1.1" stretchy="false" xref="S0.F1.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S0.F1.2.m1.1c"><ci id="S0.F1.2.m1.1.1.cmml" xref="S0.F1.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S0.F1.2.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S0.F1.2.m1.1e">→</annotation></semantics></math>English MT task in both general and specific domains. The model is fed with translation prompts, which are constructed by rephrasing inputs by using a prompt template of “<span class="ltx_text ltx_font_italic" id="S0.F1.4.1">Translate the following Chinese sentence into English: [input]</span>”, where the [input] represents the input sentences to be translated.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) pre-trained on a large amount of unlabeled corpora have been shown to perform few-shot learning remarkably well, which also enable prompting for machine translation (MT), even though they were not explicitly trained for the MT task <cite class="ltx_cite ltx_citemacro_cite">Brown <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib3" title="">2020</a>); Winata <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib38" title="">2021</a>); Chowdhery <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib5" title="">2023</a>)</cite>. However, their potential in domain-specific MT remains under explored. Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S0.F1" title="Figure 1 ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results of the Llama-7B <cite class="ltx_cite ltx_citemacro_cite">Touvron <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib33" title="">2023</a>)</cite> over three metrics of chrF++ <cite class="ltx_cite ltx_citemacro_cite">Popović (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib25" title="">2015</a>)</cite>, COMET <cite class="ltx_cite ltx_citemacro_cite">Rei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib26" title="">2022</a>)</cite> and BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib24" title="">2002</a>)</cite> for the Chinese<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.p1.1.m1.1"><semantics id="S1.p1.1.m1.1a"><mo id="S1.p1.1.m1.1.1" stretchy="false" xref="S1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><ci id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.p1.1.m1.1d">→</annotation></semantics></math>English MT task, by directly feeding it with task-specific prompts, which are constructed by rephrasing inputs with descriptive task instructions. We use two general-domain datasets of Flores-101 <cite class="ltx_cite ltx_citemacro_cite">Goyal <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib10" title="">2022</a>)</cite> and OPUS-100 <cite class="ltx_cite ltx_citemacro_cite">Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib44" title="">2020</a>)</cite>, and two domain-specific datasets of IT <cite class="ltx_cite ltx_citemacro_cite">Koehn and Knowles (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib16" title="">2017</a>)</cite> and Science <cite class="ltx_cite ltx_citemacro_cite">Tian <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib32" title="">2014</a>)</cite> for model evaluation. The results show that the pre-trained Llama2-7B performs much worse for MT in specific domains than that in general domains.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.2">To improve the domain-specific MT capability of LLMs, current works fall into two groups. Several works employ in-context learning by feeding an LLM with extra contexts by providing in-domain translation examples as a demonstration <cite class="ltx_cite ltx_citemacro_cite">Brown <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib3" title="">2020</a>); Vilar <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib34" title="">2023</a>); Moslem <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib21" title="">2023a</a>); Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib46" title="">2023</a>)</cite>. This can improve the MT quality and adherence to the domain style and terminology, without fine-tuning. However, their effectiveness highly depends on the quality of translation examples and they often require extra post-processing due to over-generation. Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of low-quality translation output from a pre-trained LLM, resulting in over-generation. Alternatively, other works fine-tune LLMs on domain-specific data <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib35" title="">2022a</a>); Moslem <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib22" title="">2023b</a>)</cite>, to improve their MT ability for domain adaptation. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, the LLM with fine-tuning produces much higher quality translation output than that of the pre-trained LLM. However, even though trained on a large amount of data, they can struggle to translate inputs with rare words, which are common in the domain transfer scenarios. They might weaken general-purpose MT capabilities due to over-specialization. Existing works also suffer from certain efficiency problems <cite class="ltx_cite ltx_citemacro_cite">Alves <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib2" title="">2023</a>)</cite>: (<math alttext="i" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">italic_i</annotation></semantics></math>) the in-context learning incurs inference costs which are greatly increased by processing extra contexts; (<math alttext="ii" class="ltx_Math" display="inline" id="S1.p2.2.m2.1"><semantics id="S1.p2.2.m2.1a"><mrow id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml"><mi id="S1.p2.2.m2.1.1.2" xref="S1.p2.2.m2.1.1.2.cmml">i</mi><mo id="S1.p2.2.m2.1.1.1" xref="S1.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S1.p2.2.m2.1.1.3" xref="S1.p2.2.m2.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><apply id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1"><times id="S1.p2.2.m2.1.1.1.cmml" xref="S1.p2.2.m2.1.1.1"></times><ci id="S1.p2.2.m2.1.1.2.cmml" xref="S1.p2.2.m2.1.1.2">𝑖</ci><ci id="S1.p2.2.m2.1.1.3.cmml" xref="S1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">ii</annotation><annotation encoding="application/x-llamapun" id="S1.p2.2.m2.1d">italic_i italic_i</annotation></semantics></math>) the fine-tuning solution requires multiple extra training for specific domains and yields a high training cost.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="137" id="S1.F2.g1" src="extracted/5426528/figs/exmaple-fineture-vs-context.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of illustrating an LLM on the Chinese<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.F2.2.m1.1"><semantics id="S1.F2.2.m1.1b"><mo id="S1.F2.2.m1.1.1" stretchy="false" xref="S1.F2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F2.2.m1.1c"><ci id="S1.F2.2.m1.1.1.cmml" xref="S1.F2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.2.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.F2.2.m1.1e">→</annotation></semantics></math>English translation task. We highlight the over-generation results with orange background from the pre-trained LLM without fine-tuning on the task-specific data. The correct rare or domain-specific words are highlighted with green background, while the mistranslated ones are marked in red. The fine-tuning significantly improves the MT capability of LLM.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the challenges, this paper proposes a novel prompt-oriented fine-tuning method, denoted as LlamaIT, for fine-tuning a general-purpose LLM for domain-specific MT tasks. We leverage the intuition that the MT tasks can be described via natural language instructions, like “<span class="ltx_text ltx_font_italic" id="S1.p3.1.1">Translate the following Chinese sentence into English: 左挂耳板到主板的左挂耳连接器（J6081）的低速信号线缆</span>”. We perform instruction tuning on a pre-trained LLM by using a self-constructed mix-domain translation dataset, which are expressed via natural language instructions. To elicit MT abilities for rare words, we further augment the dataset by rephrasing the source sentences for incorporating domain-specific vocabularies, like “<span class="ltx_text ltx_font_italic" id="S1.p3.1.2">左 mounting ear plate 到 mainboard 的左挂耳 connector （J6081）的低速信号线缆</span>”. Our main contributions are summarized as follows.</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">To eliminate the need for in-context examples or post-processing (See Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.F5" title="Figure 5 ‣ 4.1.3 Implementation Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>), we fine-tune a general-purpose LLM on the task-specific mix-domain data, which are described via translation instructions, to enhance the MT capabilities of the LLM for domain adaptation at inference time. The training on mix-domain data can repair the zero-shot MT capability of LLMs for general-purpose (See Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T4" title="Table 4 ‣ 4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>). This also naturally avoids the inference costs increased by processing input translation examples.
</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We perform the prompt-oriented fine-tuning with LoRA on a pre-trained LLM (like Llama2-7B), which can reduce the number of training parameters and thus reduces the cost of model fine-tuning (See Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T5" title="Table 5 ‣ 4.7 Efficiency ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T6" title="Table 6 ‣ 4.7 Efficiency ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We construct extra translation prompts by incorporating domain-specific bilingual vocabulary into the input source sentences, which highly improve the MT capability of the LLM for rare words (See Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T3" title="Table 3 ‣ 4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.2">Through comprehensive experiments, the results show that our LlamaIT can significantly enhance the MT abilities of the LLM, when applied to the targeted domain with rare words, for both Chinese<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.I1.i4.p1.1.m1.1"><semantics id="S1.I1.i4.p1.1.m1.1a"><mo id="S1.I1.i4.p1.1.m1.1.1" stretchy="false" xref="S1.I1.i4.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i4.p1.1.m1.1b"><ci id="S1.I1.i4.p1.1.m1.1.1.cmml" xref="S1.I1.i4.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i4.p1.1.m1.1d">→</annotation></semantics></math>English and English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.I1.i4.p1.2.m2.1"><semantics id="S1.I1.i4.p1.2.m2.1a"><mo id="S1.I1.i4.p1.2.m2.1.1" stretchy="false" xref="S1.I1.i4.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i4.p1.2.m2.1b"><ci id="S1.I1.i4.p1.2.m2.1.1.cmml" xref="S1.I1.i4.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i4.p1.2.m2.1d">→</annotation></semantics></math>Chinese MT tasks (See Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T4" title="Table 4 ‣ 4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Current LLM-based MT systems can follow into two groups: in-context learning and fine-tuning based methods.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.3">In-context learning feeds a language model with extra contexts to improve its MT capabilities, without fine-tuning <cite class="ltx_cite ltx_citemacro_cite">Brown <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib3" title="">2020</a>)</cite>. Existing works adopted similar methods to explore different settings for the contexts, such as zero-shot and few-shot examples. Several works focused on testing the MT capabilities of LLMs with simple prompts (zero-shot examples) via natural language descriptions of the MT task <cite class="ltx_cite ltx_citemacro_cite">Brown <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib3" title="">2020</a>); Winata <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib38" title="">2021</a>); Reynolds and McDonell (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib28" title="">2021</a>); Lin <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib19" title="">2022</a>); Scao <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib29" title="">2022</a>); Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib45" title="">2022</a>); Garcia and Firat (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib8" title="">2022</a>)</cite>. For instance, Reynolds et al. <cite class="ltx_cite ltx_citemacro_cite">Reynolds and McDonell (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib28" title="">2021</a>)</cite> tested different prompt templates for GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib3" title="">2020</a>)</cite>, while Garcia et al. <cite class="ltx_cite ltx_citemacro_cite">Garcia and Firat (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib8" title="">2022</a>)</cite> used prompts with mT5 <cite class="ltx_cite ltx_citemacro_cite">Xue <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib41" title="">2021</a>)</cite> to control the outputs. Other works investigated prompting strategies for identifying appropriate examples to enhance the MT capabilities of LLMs <cite class="ltx_cite ltx_citemacro_cite">Chowdhery <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib5" title="">2023</a>); Vilar <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib34" title="">2023</a>); Moslem <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib21" title="">2023a</a>); Agrawal <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib1" title="">2023</a>); Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib46" title="">2023</a>); Jiao <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib15" title="">2023b</a>); Hendy <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib11" title="">2023</a>)</cite>. Although moderate progress has been made, these methods suffer from several drawbacks: (<math alttext="i" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">italic_i</annotation></semantics></math>) their effectiveness is highly sensitive to the quality of examples (See Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T1" title="Table 1 ‣ 4.1.3 Implementation Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>); (<math alttext="ii" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mrow id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">i</mi><mo id="S2.p2.2.m2.1.1.1" xref="S2.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><times id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1.1"></times><ci id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">𝑖</ci><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">ii</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">italic_i italic_i</annotation></semantics></math>) extra post-processing is required due to over-generation (See Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>); (<math alttext="iii" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mrow id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">i</mi><mo id="S2.p2.3.m3.1.1.1" xref="S2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">i</mi><mo id="S2.p2.3.m3.1.1.1a" xref="S2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S2.p2.3.m3.1.1.4" xref="S2.p2.3.m3.1.1.4.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><times id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1.1"></times><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">𝑖</ci><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">𝑖</ci><ci id="S2.p2.3.m3.1.1.4.cmml" xref="S2.p2.3.m3.1.1.4">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">iii</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_i italic_i italic_i</annotation></semantics></math>) processing examples can increase inference costs <cite class="ltx_cite ltx_citemacro_cite">Alves <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib2" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Given labelled domain-specific data, fine-tuning can be an alternative to improve the MT capabilities of LLMs <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib35" title="">2022a</a>); Moslem <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib22" title="">2023b</a>); Li <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib18" title="">2023</a>); Alves <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib2" title="">2023</a>); Schioppa <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib30" title="">2023</a>); Iyer <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib13" title="">2023</a>); Jiao <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib14" title="">2023a</a>); Xu <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib40" title="">2023</a>)</cite>. Existing works enhanced MT via LLM training involving two ways: pre-training and fine-tuning. For instance, Schioppa et al. <cite class="ltx_cite ltx_citemacro_cite">Schioppa <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib30" title="">2023</a>)</cite> pre-trained an LLM for improving its zero-shot MT capabilities; while Li et al. <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib18" title="">2023</a>)</cite> fine-tuned LLMs on translation instructions. One recent work <cite class="ltx_cite ltx_citemacro_cite">Reinauer <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib27" title="">2023</a>)</cite> investigated pre-training or fine-tuning several typical neural MT models <cite class="ltx_cite ltx_citemacro_cite">Farajian <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib7" title="">2017</a>); Bulte and Tezcan (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib4" title="">2019</a>); Xu <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib39" title="">2020</a>)</cite>, by comparing against in-context learning of LLMs for domain adaptation. The fine-tuning methods are reported to outperform few-shot prompting and eliminate the need for in-context examples or post processing <cite class="ltx_cite ltx_citemacro_cite">Alves <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib2" title="">2023</a>)</cite>. However, they often require high computational cost for extra training on specific domains, and may weaken general-purpose MT capabilities of LLMs due to over-specialization.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Even though trained on large amount of data, these two groups of methods can struggle to translate inputs with rare words in domain transfer scenarios <cite class="ltx_cite ltx_citemacro_cite">Ghazvininejad <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib9" title="">2023</a>)</cite>. Therefore, several works focused on using the domain-specific vocabulary to supply translations in low-resource settings <cite class="ltx_cite ltx_citemacro_cite">Lu <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib20" title="">2023</a>); Ghazvininejad <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib9" title="">2023</a>); Moslem <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib23" title="">2023c</a>)</cite>, or chain-of-thought inspired prompts that elicit in-context example from the model itself <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib36" title="">2022b</a>); Lu <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib20" title="">2023</a>)</cite>. For instance, Ghazvininejad et al. <cite class="ltx_cite ltx_citemacro_cite">Ghazvininejad <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib9" title="">2023</a>)</cite> incorporated the additional dictionaries into zero-shot examples without model training, while Wei et al. <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib36" title="">2022b</a>)</cite> used a few chain-of-thought demonstrations which were provided as exemplars in prompting. However, these methods still suffer from the drawbacks listed with in-context learning methods.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">This paper takes full advantage of the above two groups of methods, by proposing a prompt-oriented fine-tuning method using LoRA, to fine-tune a general-purpose LLM on the task-specific mix-domain data, which are augmented by dictionary-level domain-specific prompting. We focus on exploring the impact of fine-tuning and zero-shot prompting for adapting LLMs to perform the domain-specific MT tasks.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="169" id="S2.F3.g1" src="extracted/5426528/figs/overview.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of our LlamaIT for performing prompt-oriented fine-tuning on a pre-trained LLM for the domain-specific MT task.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Prompt-oriented Fine-tuning for LLMs</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We propose to fine-tune a pre-trained LLM for adapting the domain transfer between the data used to pre-train the zero-shot model and that in the target domain. Our method overview is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S2.F3" title="Figure 3 ‣ 2 Related Work ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>. We first introduce how to generate the domain-specific data. Then, we present how to construct a task-specific dataset by zero-shot prompting with translation instructions. The dataset is further augmented by incorporating the domain-specific bilingual vocabulary and fed into our LlamaIT for performing prompt-oriented fine-tuning on a pre-trained LLM, which enhances its capabilities for the domain-specific MT tasks.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We select the Llama2-7B as the pre-trained model, which is open-source and commercially viable <cite class="ltx_cite ltx_citemacro_cite">Touvron <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib33" title="">2023</a>)</cite>. Its tokenizer uses the byte-pair encoding algorithm, which is implemented with SentencePiece <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib17" title="">2018</a>)</cite>. We choose the basic version of Llama2-7B without extra instruction fine-tuning due to its zero-shot ability. To save the model training cost, we adopt the LoRA, which is designed for resource-efficient fine-tuning of LLMs <cite class="ltx_cite ltx_citemacro_cite">Hu <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib12" title="">2021</a>)</cite>. It utilizes low-rank matrices to represent parameter updates, which reduce the number of trainable parameters, GPU memory usage, and incurring no additional inference cost, making it a practical choice to fine-tune LLMs.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="137" id="S3.F4.g1" src="extracted/5426528/figs/dict-prompt.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The illustration of two dictionary-based prompting methods</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Domain-specific Data Generation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">It is difficult to manually annotate large-scale task-specific data, as it requires expertise for writing the solutions to each task. Accordingly, we develop a self-constructed pipeline for generating high-quality bilingual translation pairs. We select the IT domain as a case study in our work, while our method can be easily applied to other domains. We first collect about <math alttext="100,000" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.2"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">100</mn><mo id="S3.SS1.p1.1.m1.2.3.2.1" xref="S3.SS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><list id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.2"><cn id="S3.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1">100</cn><cn id="S3.SS1.p1.1.m1.2.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">100,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.2d">100 , 000</annotation></semantics></math> IT product documentations from official websites of some well-known IT companies, half in Chinese and half in English. Then, we semantically segment the documents into textual sentences and sequentially align each pair of bilingual sentences. To ensure the high quality of translation data, we adopt the LLM with zero-shot capabilities for validating the translation pairs. For each pair, we use the Chinese sentences to create a translation instructions using our designed prompting templates (See Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T1" title="Table 1 ‣ 4.1.3 Implementation Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>), and directly feed them to the Baichuan-13B <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib42" title="">2023a</a>)</cite> for translation inference. The returned results are then used to compute BLEU scores with the English sentences of our generated pairs. For a low BLEU score that is not exceeding a given threshold value (This value is set as <math alttext="10" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">10</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">10</annotation></semantics></math> without loss of generality), our generated pair would require manual verification from expertise. As manual annotations are costly, we directly discard such pairs in this work. Consequently, low-quality and duplicate pairs are removed, resulting in <math alttext="200,000" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.2"><semantics id="S3.SS1.p1.3.m3.2a"><mrow id="S3.SS1.p1.3.m3.2.3.2" xref="S3.SS1.p1.3.m3.2.3.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">200</mn><mo id="S3.SS1.p1.3.m3.2.3.2.1" xref="S3.SS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.2b"><list id="S3.SS1.p1.3.m3.2.3.1.cmml" xref="S3.SS1.p1.3.m3.2.3.2"><cn id="S3.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1">200</cn><cn id="S3.SS1.p1.3.m3.2.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.2c">200,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.2d">200 , 000</annotation></semantics></math> high-quality Chinese-English translation pairs in the IT domain. We name this self-constructed domain-specific dataset as XFIT24-ORI.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Zero-shot Prompting with Instructions</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Fine-tuning an LLM directly on the XFIT24-ORI dataset can adapt its MT capabilities to the specific domain. However, its effectiveness is reported to be limited for MT tasks as it can struggle to respond to translation instructions without instruction-tuning <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib35" title="">2022a</a>)</cite>. Thus, the need arises to construct a task-specific dataset, that is described via natural language instructions. We conduct extensive experiments to explore 7 widely used MT instruction templates, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T1" title="Table 1 ‣ 4.1.3 Implementation Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>. We observed that prompting performance varies across templates, and task-specific templates mainly work when translating into languages for which LLMs are pre-trained on. An English template in a simple form like “<span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Translate the following [src] sentence into [tgt]: [input]</span>”, works best for MT. Particularly, we adopt this template to re-write the source sentences for all translation pairs in XFIT24-ORI, and name the resulting task-specific dataset as XFIT24-TSI. Fine-tuning an LLM on the XFIT24-TSI dataset can greatly enhance its MT capabilities by zero-shot prompting.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">In general, the pre-trained LLMs have acquired a certain level of cross-lingual representation abilities during pre-training. Through fine-tuning, we can refine these capabilities for domain-specific translation tasks. Therefore, zero-shot prompting is a simple but effective solution for this purpose. During fine-tuning, this method introduces explicit translation instructions to guide the LLM in performing specific translation tasks in the target domain. For instance, with a simple zero-shot prompt template like “<span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">Translate the following [src] sentence into [tgt]: [input]</span>”, the target is set to the correct translation from the training corpus. During training, the model generates corresponding translation outputs based on the instruction and input, and optimizes the training through the computation of the loss between the generated output and the target.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dictionary-based Prompting</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In domain-specific MT tasks, there exist numerous specific terms (words or phrases), which rarely appear in common contexts, resulting in the rare word problem for LLMs. To resolve this problem, we further augment the XFIT24-TSI dataset by rephrasing the source sentences for incorporating the domain-specific bilingual vocabulary, which enhances the MT capability of LLMs for supplying translations in low-resource settings.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.2">We construct a domain-specific dictionary containing <math alttext="324,785" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.2"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.3.2" xref="S3.SS3.p2.1.m1.2.3.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">324</mn><mo id="S3.SS3.p2.1.m1.2.3.2.1" xref="S3.SS3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml">785</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><list id="S3.SS3.p2.1.m1.2.3.1.cmml" xref="S3.SS3.p2.1.m1.2.3.2"><cn id="S3.SS3.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1">324</cn><cn id="S3.SS3.p2.1.m1.2.2.cmml" type="integer" xref="S3.SS3.p2.1.m1.2.2">785</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">324,785</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.2d">324 , 785</annotation></semantics></math> bilingual vocabulary terms in the IT domain. For incorporating the vocabulary, we explore two methods: The Chain-of-Dictionary uses a few chain-of-thought demonstrations as exemplars in prompting <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib36" title="">2022b</a>)</cite>, while the Dictionary-Rephrasing directly incorporates the vocabulary into zero-shot examples without model training <cite class="ltx_cite ltx_citemacro_cite">Ghazvininejad <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib9" title="">2023</a>)</cite>. Taking the Chinese<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mo id="S3.SS3.p2.2.m2.1.1" stretchy="false" xref="S3.SS3.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">→</annotation></semantics></math>English translation task as an example, the input Chinese sentences are rephrased by simply replacing the occurrences of specific terms that exist in the dictionary, with their target English translations. This also results in a mixed-language input for the LLM. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S3.F4" title="Figure 4 ‣ 3 Prompt-oriented Fine-tuning for LLMs ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, one source sentence on the XFIT24-TSI dataset as “<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.2.1">Translate the following Chinese sentence into English: 左挂耳板到主板的左挂耳连接器（J6081）的低速信号线缆</span>”, is rephrased into a new source sentence as “<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.2.2">左 mounting ear plate 到 mainboard 的左挂耳 connector （J6081）的低速信号线缆</span>”. Particularly, we rephrase the source sentences for all translation pairs in XFIT24-TSI, and name the resulting dataset as XFIT24-TSID. Fine-tuning an LLM on the XFIT24-TSID dataset can enhance its MT capabilities for rare words, by dictionary-based prompting (See Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T3" title="Table 3 ‣ 4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">So far, we have constructed a task-specific and domain-specific dataset for fine-tuning LLMs, to enhance their capabilities for domain-specific MT tasks with rare words. However, this method might weaken their general-purpose MT capabilities due to over-specialization. To solve this problem, we further augment our dataset by integrating several publicly available mix-domain datasets, including UM-Corpus <cite class="ltx_cite ltx_citemacro_cite">Tian <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib32" title="">2014</a>)</cite> and OPUS-100 <cite class="ltx_cite ltx_citemacro_cite">Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib44" title="">2020</a>)</cite>. We name the final task-specific mix-domain dataset as XFIT24. We observe that the training on mix-domain data can repair the zero-shot MT capability of LLMs for general-purpose (See Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T4" title="Table 4 ‣ 4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">We evaluate the proposed method in Chinese<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" stretchy="false" xref="S4.SS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">→</annotation></semantics></math>English and English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mo id="S4.SS1.p1.2.m2.1.1" stretchy="false" xref="S4.SS1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">→</annotation></semantics></math>Chinese translation directions. This paper trains the models on mix-domain translation datasets. We use the Llama2-7B as the main backbone model for the experiment, and evaluate the domain-specific MT performance with fine-tuning on the task-specific mix-domain dataset. For prompt-based translation prediction, we employ an English template in a simple form.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Datasets</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.3">For fine-tuning, we employ the self-constructed dataset XFIT24, which contains the IT domain translation data (In Section <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S3.SS1" title="3.1 Domain-specific Data Generation ‣ 3 Prompt-oriented Fine-tuning for LLMs ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>). It mainly consists of <math alttext="10,000" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.2"><semantics id="S4.SS1.SSS1.p1.1.m1.2a"><mrow id="S4.SS1.SSS1.p1.1.m1.2.3.2" xref="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml"><mn id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">10</mn><mo id="S4.SS1.SSS1.p1.1.m1.2.3.2.1" xref="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS1.p1.1.m1.2.2" xref="S4.SS1.SSS1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.2b"><list id="S4.SS1.SSS1.p1.1.m1.2.3.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.2.3.2"><cn id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.1.m1.1.1">10</cn><cn id="S4.SS1.SSS1.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.2c">10,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.1.m1.2d">10 , 000</annotation></semantics></math> English-Chinese translation pairs from publicly available datasets, and <math alttext="200,000" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.2.m2.2"><semantics id="S4.SS1.SSS1.p1.2.m2.2a"><mrow id="S4.SS1.SSS1.p1.2.m2.2.3.2" xref="S4.SS1.SSS1.p1.2.m2.2.3.1.cmml"><mn id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml">200</mn><mo id="S4.SS1.SSS1.p1.2.m2.2.3.2.1" xref="S4.SS1.SSS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS1.p1.2.m2.2.2" xref="S4.SS1.SSS1.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.2b"><list id="S4.SS1.SSS1.p1.2.m2.2.3.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.2.3.2"><cn id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.2.m2.1.1">200</cn><cn id="S4.SS1.SSS1.p1.2.m2.2.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.2c">200,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.2.m2.2d">200 , 000</annotation></semantics></math> self-constructed translation pairs. We use test datasets from two general-domain datasets of Flores-101 <cite class="ltx_cite ltx_citemacro_cite">Goyal <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib10" title="">2022</a>)</cite> and OPUS-100 <cite class="ltx_cite ltx_citemacro_cite">Zhang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib44" title="">2020</a>)</cite>, and two domain-specific datasets of IT <cite class="ltx_cite ltx_citemacro_cite">Koehn and Knowles (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib16" title="">2017</a>)</cite> and XFIT24 for translation inference. For the first three datasets, we use test datasets from the original papers. We randomly select <math alttext="3,000" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.3.m3.2"><semantics id="S4.SS1.SSS1.p1.3.m3.2a"><mrow id="S4.SS1.SSS1.p1.3.m3.2.3.2" xref="S4.SS1.SSS1.p1.3.m3.2.3.1.cmml"><mn id="S4.SS1.SSS1.p1.3.m3.1.1" xref="S4.SS1.SSS1.p1.3.m3.1.1.cmml">3</mn><mo id="S4.SS1.SSS1.p1.3.m3.2.3.2.1" xref="S4.SS1.SSS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS1.SSS1.p1.3.m3.2.2" xref="S4.SS1.SSS1.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.3.m3.2b"><list id="S4.SS1.SSS1.p1.3.m3.2.3.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.2.3.2"><cn id="S4.SS1.SSS1.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS1.SSS1.p1.3.m3.1.1">3</cn><cn id="S4.SS1.SSS1.p1.3.m3.2.2.cmml" type="integer" xref="S4.SS1.SSS1.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.3.m3.2c">3,000</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p1.3.m3.2d">3 , 000</annotation></semantics></math> domain-specific translation pairs from XFIT24 as the test set before fine-tuning, using the remaining pairs for training the LLM.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Evaluation Metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">We employed three widely used evaluation metrics, namely the character-based metric chrF++ <cite class="ltx_cite ltx_citemacro_cite">Popović (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib25" title="">2015</a>)</cite>, the word-based metric BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib24" title="">2002</a>)</cite>, and the reference-based metric COMET <cite class="ltx_cite ltx_citemacro_cite">Rei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib26" title="">2022</a>)</cite> for model evaluation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Implementation Details</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">In the fine-tuning experiment, the settings were as follows: a learning rate of 3e-4, a per-device training batch size of 2, weight decay of 0.00001, and a warmup ratio of 0.01. For efficient training, we employed the Deepspeed and Flash-Attention acceleration frameworks for fine-tuning with LoRA, the LoRA rank was set to 16. All experiments were conducted on 4 RTX 3090 for training and 2 A6000 GPUs.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.1.1">ID</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1">Templates (in English)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.1.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.1">Chinese</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.4.1">English</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.1">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.1">chrF++</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.3.1">COMET</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.4.1">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.5.1">chrF++</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.6.1">COMET</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.1">A</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.2">Translate the following[src]sentence into [tgt]:[input]</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.3.1">34.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.4">58.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.5.1">55.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.6.1">35.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.7.1">59.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.3.8"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.8.1">57.61</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4">
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.1">B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.2">Provide [tgt] translation for the [src] sentence:[input]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3">33.77</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.4.1">58.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.5">55.84</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.6">35.13</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.7">59.35</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.8">55.81</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5">
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.1">C</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.2">How to express [input] in [tgt] ?</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.3">33.46</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4">58.40</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5">55.75</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.6">34.90</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.7">59.45</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.8">56.36</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6">
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.1">D</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.2">What’s the [tgt] translation of [input]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.3">33.95</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.4">58.70</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5">55.71</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6">34.51</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.7">59.19</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.8">56.22</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7">
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.1">F</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.2">Generate [tgt] translation for [input]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.3">32.53</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.4">57.61</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.5">54.30</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6">34.73</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7">59.49</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.8">57.01</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8">
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.1">G</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.2">Express the following [src] sentence:[input] in [tgt]</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.3">33.16</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.4">58.39</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.5">55.52</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.6">34.59</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7">59.02</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8">56.79</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.9.1">H</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.9.2">How to say [input] in [tgt] ?</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.9.3">33.48</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.9.4">58.77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.9.5">55.74</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.9.6">35.19</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.9.7">59.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.9.8">57.26</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance over Chinese-to-English zero-shot prompting with different template and different template language on our IT domain-specific test dataset. [src], [tgt] and [input] represent the source language name, the target language name and the input sentences to be translated; <span class="ltx_text ltx_font_italic" id="S4.T1.4.1">English</span> and <span class="ltx_text ltx_font_italic" id="S4.T1.5.2">Chinese</span> represent the template language.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.F5.1" style="width:433.6pt;height:121.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.7pt,91.4pt) scale(1.00334445562696,0.399058975964965) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="421" id="S4.F5.1.g1" src="extracted/5426528/figs/length.png" width="598"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Length distribution of translation outputs on the XFIT24 test set. <span class="ltx_text ltx_font_italic" id="S4.F5.5.1">Pre-trained</span> and <span class="ltx_text ltx_font_italic" id="S4.F5.6.2">Fine-tuned</span> denote the outputs of the Llama2-7B base model and its fine-tuned model by performing zero-shot prompting with translation instructions, respectively. <span class="ltx_text ltx_font_italic" id="S4.F5.7.3">Target</span> refers to the ground-truth translations in XFIT24.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Effects of Fine-tuning on MT Tasks</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">To evaluate the effects of fine-tuning on MT tasks, we utilize the XFIT24 test set by rephrasing inputs with translation instructions. The instructions are fed into the pre-trained and fined-tuned Llama2-7B models, to generate the output. We show the length of the tokenized outputs in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.F5" title="Figure 5 ‣ 4.1.3 Implementation Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>. We observe that: (<math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_i</annotation></semantics></math>) the length distribution of the outputs generated by the fine-tuned model matches the distribution of the target; (<math alttext="ii" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">i</mi><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝑖</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">ii</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_i italic_i</annotation></semantics></math>) the pre-trained model can generate extra contexts besides the desired translation, that requires post-processing of the generated content. These findings indicate that the fine-tuned model no longer over-generates the translation output, avoiding the need for post-processing.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.1">
<tr class="ltx_tr" id="S4.T2.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.1">chrF++</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.1">COMET</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1">NLLB-600M*</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.2">23.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.3">53.01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.4">53.65</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3">
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.1">Google*</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.1">44.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.3.1">66.14</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.4.1">80.22</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.1">Aquila2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.2">15.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.3">34.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4">17.23</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5">
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.1">InternLM-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.2">15.88</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.3">37.07</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4">18.98</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6">
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.1">Baichuan2-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.2">20.56</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.3">45.80</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.4">28.42</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7">
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.1">Llama2-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.2">22.72</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.3">49.20</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.4">32.79</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8">
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.1">BLOOM-7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.2">24.89</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.3">51.22</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.4">36.74</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9">
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.1">BigTranslate</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.2">25.02</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.3">50.78</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.4">37.34</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10">
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.1">PolyLM-13B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.2">28.33</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.3">46.14</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.4">45.28</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.1">ChatGPT 3.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.2.1">34.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.3.1">55.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.11.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.4.1">57.80</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of Supervised Baseline Models and LLM base models in our XFIT24 test dataset. The * represents Supervised Baseline Models.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.1">
<tr class="ltx_tr" id="S4.T3.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1" style="font-size:90%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.1.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.1" style="font-size:90%;">Chinese → English</span><span class="ltx_text" id="S4.T3.1.1.2.2" style="font-size:90%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T3.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.1" style="font-size:90%;">English → Chinese</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.1.1" style="font-size:90%;">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.1" style="font-size:90%;">COMET</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.3.1" style="font-size:90%;">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.4.1" style="font-size:90%;">COMET</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1"><span class="ltx_text" id="S4.T3.1.3.1.1" style="font-size:90%;">No-Dict</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.2"><span class="ltx_text" id="S4.T3.1.3.2.1" style="font-size:90%;">35.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.3"><span class="ltx_text" id="S4.T3.1.3.3.1" style="font-size:90%;">58.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.4"><span class="ltx_text" id="S4.T3.1.3.4.1" style="font-size:90%;">50.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.5"><span class="ltx_text" id="S4.T3.1.3.5.1" style="font-size:90%;">80.78</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4">
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.1"><span class="ltx_text" id="S4.T3.1.4.1.1" style="font-size:90%;">Chain-of-Dict</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2"><span class="ltx_text" id="S4.T3.1.4.2.1" style="font-size:90%;">36.23</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3"><span class="ltx_text" id="S4.T3.1.4.3.1" style="font-size:90%;">58.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.4"><span class="ltx_text" id="S4.T3.1.4.4.1" style="font-size:90%;">52.07</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.5"><span class="ltx_text" id="S4.T3.1.4.5.1" style="font-size:90%;">81.26</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.5.1"><span class="ltx_text" id="S4.T3.1.5.1.1" style="font-size:90%;">Dict-Rephrasing</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.5.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.2.1" style="font-size:90%;">37.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.3.1" style="font-size:90%;">60.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.5.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.4.1" style="font-size:90%;">52.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.5.1" style="font-size:90%;">82.36</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance of different dictionary-based prompt methods on XFIT24 test dataset.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.4" style="width:433.6pt;height:87.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.2pt,10.8pt) scale(0.800133099671218,0.800133099671218) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.4.4">
<tr class="ltx_tr" id="S4.T4.4.4.5">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.4.4.5.1" rowspan="2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text" id="S4.T4.4.4.5.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.4.4.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.4.4.5.2.1">Flores-101</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.4.4.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.4.4.5.3.1">OPUS-100</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.4.4.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.4.4.5.4.1">IT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.4.4.5.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.4.4.5.5.1">XFIT24</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">COMET</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.6.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">COMET</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.6.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.6.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">COMET</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.6.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.6.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">COMET</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Llama2-7B<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><msup id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml"><mi id="S4.T4.1.1.1.1.m1.1.1a" xref="S4.T4.1.1.1.1.m1.1.1.cmml"></mi><mo id="S4.T4.1.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1"><ci id="S4.T4.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">25.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">49.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">24.99</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">8.42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">17.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">-28.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">22.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">32.79</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2.2">
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">LlamaIT<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.1.m1.1a"><msup id="S4.T4.2.2.2.1.m1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.cmml"><mi id="S4.T4.2.2.2.1.m1.1.1a" xref="S4.T4.2.2.2.1.m1.1.1.cmml"></mi><mo id="S4.T4.2.2.2.1.m1.1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><apply id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1"><ci id="S4.T4.2.2.2.1.m1.1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">22.04 <span class="ltx_text" id="S4.T4.2.2.2.2.1" style="color:#FF0000;">(↓3.34)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">45.49 <span class="ltx_text" id="S4.T4.2.2.2.3.1" style="color:#FF0000;">(↓4.41)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">35.91 <span class="ltx_text" id="S4.T4.2.2.2.4.1" style="color:#0000FF;">(↑10.92)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">31.27 <span class="ltx_text" id="S4.T4.2.2.2.5.1" style="color:#0000FF;">(↑22.85)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">36.24 <span class="ltx_text" id="S4.T4.2.2.2.6.1" style="color:#0000FF;">(↑19.05)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">61.97 <span class="ltx_text" id="S4.T4.2.2.2.7.1" style="color:#0000FF;">(↑90.90)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">55.16 <span class="ltx_text" id="S4.T4.2.2.2.8.1" style="color:#0000FF;">(↑32.44)</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">89.24 <span class="ltx_text" id="S4.T4.2.2.2.9.1" style="color:#0000FF;">(↑56.45)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Llama2-7B<math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.m1.1"><semantics id="S4.T4.3.3.3.1.m1.1a"><msup id="S4.T4.3.3.3.1.m1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.cmml"><mi id="S4.T4.3.3.3.1.m1.1.1a" xref="S4.T4.3.3.3.1.m1.1.1.cmml"></mi><mo id="S4.T4.3.3.3.1.m1.1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><apply id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1"><ci id="S4.T4.3.3.3.1.m1.1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">33.49</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">55.18</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">27.54</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">34.58</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">20.65</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">5.36</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">33.26</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.3.3.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">64.67</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">LlamaIT<math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S4.T4.4.4.4.1.m1.1"><semantics id="S4.T4.4.4.4.1.m1.1a"><msup id="S4.T4.4.4.4.1.m1.1.1" xref="S4.T4.4.4.4.1.m1.1.1.cmml"><mi id="S4.T4.4.4.4.1.m1.1.1a" xref="S4.T4.4.4.4.1.m1.1.1.cmml"></mi><mo id="S4.T4.4.4.4.1.m1.1.1.1" xref="S4.T4.4.4.4.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.1.m1.1b"><apply id="S4.T4.4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.4.1.m1.1.1"><ci id="S4.T4.4.4.4.1.m1.1.1.1.cmml" xref="S4.T4.4.4.4.1.m1.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">32.60 <span class="ltx_text" id="S4.T4.4.4.4.2.1" style="color:#FF0000;">(↓0.89)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">53.98 <span class="ltx_text" id="S4.T4.4.4.4.3.1" style="color:#FF0000;">(↓1.2)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">37.79 <span class="ltx_text" id="S4.T4.4.4.4.4.1" style="color:#0000FF;">(↑10.25)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">15.63 <span class="ltx_text" id="S4.T4.4.4.4.5.1" style="color:#FF0000;">(↓18.95)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">40.41 <span class="ltx_text" id="S4.T4.4.4.4.6.1" style="color:#0000FF;">(↑19.76)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.7" style="padding-top:2.5pt;padding-bottom:2.5pt;">72.98 <span class="ltx_text" id="S4.T4.4.4.4.7.1" style="color:#0000FF;">(↑67.62)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.8" style="padding-top:2.5pt;padding-bottom:2.5pt;">63.76 <span class="ltx_text" id="S4.T4.4.4.4.8.1" style="color:#0000FF;">(↑30.50)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.4.4.4.9" style="padding-top:2.5pt;padding-bottom:2.5pt;">101.71 <span class="ltx_text" id="S4.T4.4.4.4.9.1" style="color:#0000FF;">(↑37.04)</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The translation performance of LlamaIT on general domain and specific domain. The <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S4.T4.7.m1.1"><semantics id="S4.T4.7.m1.1b"><msup id="S4.T4.7.m1.1.1" xref="S4.T4.7.m1.1.1.cmml"><mi id="S4.T4.7.m1.1.1b" xref="S4.T4.7.m1.1.1.cmml"></mi><mo id="S4.T4.7.m1.1.1.1" xref="S4.T4.7.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T4.7.m1.1c"><apply id="S4.T4.7.m1.1.1.cmml" xref="S4.T4.7.m1.1.1"><ci id="S4.T4.7.m1.1.1.1.cmml" xref="S4.T4.7.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.m1.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.m1.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math> represents the translation direction is from Chinese to English, the <math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S4.T4.8.m2.1"><semantics id="S4.T4.8.m2.1b"><msup id="S4.T4.8.m2.1.1" xref="S4.T4.8.m2.1.1.cmml"><mi id="S4.T4.8.m2.1.1b" xref="S4.T4.8.m2.1.1.cmml"></mi><mo id="S4.T4.8.m2.1.1.1" xref="S4.T4.8.m2.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T4.8.m2.1c"><apply id="S4.T4.8.m2.1.1.cmml" xref="S4.T4.8.m2.1.1"><ci id="S4.T4.8.m2.1.1.1.cmml" xref="S4.T4.8.m2.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.m2.1d">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.m2.1e">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math> represents the translation direction from English to Chinese.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Effects of Instruction Templates on MT Tasks</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.2">We perform prompt-oriented fine-tuning by using supervision to teach an LLM to perform tasks described via natural language instructions, and the LLM will learn to follow instructions for MT tasks. Thus, the instruction templates directly affects the performance of the LLM. It is important to investigate the interesting questions that: (<math alttext="i" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_i</annotation></semantics></math>) which template should be used for the zero-shot prompting, and (<math alttext="ii" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">i</mi><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><times id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></times><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">𝑖</ci><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">ii</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">italic_i italic_i</annotation></semantics></math>) what language for the template is better for prompting?</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2">We compare 7 templates and evaluate them on the XFIT24 datasets covering two language
pairs (English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mo id="S4.SS3.p2.1.m1.1.1" stretchy="false" xref="S4.SS3.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">→</annotation></semantics></math>Chinese and Chinese<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mo id="S4.SS3.p2.2.m2.1.1" stretchy="false" xref="S4.SS3.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><ci id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">→</annotation></semantics></math>English). Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T1" title="Table 1 ‣ 4.1.3 Implementation Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results. The template affects zero-shot prompting quality substantially, and the simple template A in English specifying just the source and target language name achieves the best overall results. Thus, we use the template A in the following experiments. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T1" title="Table 1 ‣ 4.1.3 Implementation Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> also shows the prompting results of Chinese templates, which often under-perform their English counterparts. Overall, an English template works best on average.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Basic Results</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To explore the MT performance of the original language models, without fine-tuned on the specific domain, we conduct evaluation on the XFIT24 test set which are rephrased to instructions with Template A. We selected two supervised baseline models including NLLB <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib6" title="">2022</a>)</cite> and Google<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://translate.google.com/" title="">https://translate.google.com/</a></span></span></span> (The Google translator), along with eight LLMs including Llama2-7B, BLOOM-7B <cite class="ltx_cite ltx_citemacro_cite">Scao <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib29" title="">2022</a>)</cite>, Baichuan2-7B <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib42" title="">2023a</a>)</cite>, PolyLM-13B <cite class="ltx_cite ltx_citemacro_cite">Wei <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib37" title="">2023</a>)</cite>, BigTranslate <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib43" title="">2023b</a>)</cite>, Aquila2-7B<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/FlagAI-Open/Aquila2" title="">https://github.com/FlagAI-Open/Aquila2</a></span></span></span>, InterLM-7B <cite class="ltx_cite ltx_citemacro_cite">Team (<a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#bib.bib31" title="">2023</a>)</cite> and ChatGPT3.5<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com//" title="">https://chat.openai.com//</a></span></span></span>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.3">Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T2" title="Table 2 ‣ 4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results. we observed that: (<math alttext="i" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">italic_i</annotation></semantics></math>) The performance of LLMs in domain-specific MT tasks is less impressive, compared with the traditional supervision methods like Google. Among them, ChatGPT 3.5 demonstrates the best performance; (<math alttext="ii" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m2.1"><semantics id="S4.SS4.p2.2.m2.1a"><mrow id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mi id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">i</mi><mo id="S4.SS4.p2.2.m2.1.1.1" xref="S4.SS4.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><times id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1.1"></times><ci id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">𝑖</ci><ci id="S4.SS4.p2.2.m2.1.1.3.cmml" xref="S4.SS4.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">ii</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.2.m2.1d">italic_i italic_i</annotation></semantics></math>) The MT capability of traditional supervised methods surpass those of LLMs. With just 600 million parameters, the NLLB outperforms most 7B models. Similarly, the Google based on the supervised model, achieves the best results; (<math alttext="iii" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m3.1"><semantics id="S4.SS4.p2.3.m3.1a"><mrow id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mi id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">i</mi><mo id="S4.SS4.p2.3.m3.1.1.1" xref="S4.SS4.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p2.3.m3.1.1.3" xref="S4.SS4.p2.3.m3.1.1.3.cmml">i</mi><mo id="S4.SS4.p2.3.m3.1.1.1a" xref="S4.SS4.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p2.3.m3.1.1.4" xref="S4.SS4.p2.3.m3.1.1.4.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><times id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1.1"></times><ci id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">𝑖</ci><ci id="S4.SS4.p2.3.m3.1.1.3.cmml" xref="S4.SS4.p2.3.m3.1.1.3">𝑖</ci><ci id="S4.SS4.p2.3.m3.1.1.4.cmml" xref="S4.SS4.p2.3.m3.1.1.4">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">iii</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.3.m3.1d">italic_i italic_i italic_i</annotation></semantics></math>) The scale of model parameters in LLMs has a significant impact on the translation quality. As an example, the PolyLM with 13B parameters outperforms other 7B models by a considerable margin in all metrics.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Even though the models with larger-scale parameters show better performance, they are not practical choices for too high computational and resource cost. Accordingly, we focus on improving a model with 7B parameters with fine-tuning.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Dictionary-based Prompt Strategy for MT</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">We apply two dictionary-based prompting methods, as detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S3.SS3" title="3.3 Dictionary-based Prompting ‣ 3 Prompt-oriented Fine-tuning for LLMs ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a>, respectively to augment the XFIT24-TSI with vocabulary information. Noted that we remove mix-domain data from our dataset for purely evaluating the effects of dictionary-based prompting strategies. We fine-tune the Llama2-7B model using both methods and evaluate their performance on the XFIT24 test set. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T3" title="Table 3 ‣ 4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results. The Dictionary-Rephrasing method outperforms the Chain-of-Dictionary method on two translation directions. The results also highlight the positive impact of dictionary-based prompting on the capability of LLMs in domain-specific MT tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Fine-Tuning LLM on Domain-specific MT</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">As detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S3.SS1" title="3.1 Domain-specific Data Generation ‣ 3 Prompt-oriented Fine-tuning for LLMs ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we select the instruction template A described in English to generate instructions for inputs, and employ the Dictionary-Rephrasing strategy to re-write the inputs in our dataset. To repair the zero-shot capabilities of LLMs, we further integrate several publicly available general-domain datasets, to generate the final XFIT24 dataset for evaluation. We use test datasets from two general-domain datasets of Flores-101 and OPUS-100, and two domain-specific datasets of IT and XFIT24. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T4" title="Table 4 ‣ 4.2 Effects of Fine-tuning on MT Tasks ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results that our LlamaIT outperforms the base Llama2-7B by a significant margin on the domain-specific MT tasks. The results also show that fine-tuning can degrade the MT capability of LLMs in general domains as shown in the results on Flores-101, while mixing the dataset with general-domain data can repair the capability as shown in the results on OPUS-100.
</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Efficiency</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">Our LlamaIT method demonstrates time efficiency benefits through experiments on fine-tuning. In our LlamaIT, LoRA is employed to accelerate training by reducing parameters. We compared LoRA with full parameter fine-tuning on a 20,000-scale translation dataset, recording parameters and average epoch training time. Results in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T5" title="Table 5 ‣ 4.7 Efficiency ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> show that LoRA achieving a roughly 10-fold acceleration, significantly reducing the validation time.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.1" style="width:433.6pt;height:99.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(99.5pt,-22.9pt) scale(1.84780895477668,1.84780895477668) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.2.1">Parameters</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.3.1">Training Time(hours/epoch)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.1">LlamaIT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.2">20M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.2.3">4.20</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.3.1">Fine-Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.3.2">6,758M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.3.3">48.40</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparisons of trainable parameters and training time differences during the fine-tuning of Llama2-7B using Llama2IT and Full Parameter Fine-Tuning.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS7.p2">
<p class="ltx_p" id="S4.SS7.p2.1">We further verify that our LlamaIT with zero-shot prompting saves more training time compared to 1-shot and few-shot prompting methods (Table <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.T6" title="Table 6 ‣ 4.7 Efficiency ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>). It suggests that adopting LlamaIT effectively manages training time costs, offering an economical and viable option for experimental validation and model training while maintaining efficiency.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.1">
<tr class="ltx_tr" id="S4.T6.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.2.1">Training Time(hours/epoch)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1">LlamaIT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.2">4.20</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3">
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.1">1-shot Prompting</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2">5.46</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.4.1">3-shot Prompting</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.4.2">7.66</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Training time variations during the fine-tuning of Llama2-7B with 0-shot (LlamaIT), 1-shot, and 3-shot prompting.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="162" id="S4.F6.g1" src="extracted/5426528/figs/case-study.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The translation output cases of Llama2-7B fine-tuned with different prompt. The correct rare or domain-specific words are highlighted with green background, while the mistranslated ones are marked in pink.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Case Study</h3>
<div class="ltx_para" id="S4.SS8.p1">
<p class="ltx_p" id="S4.SS8.p1.1">To further explore the effect of each component in the translation capability, we conduct a case study. Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.15061v1#S4.F6" title="Figure 6 ‣ 4.7 Efficiency ‣ 4 Experiments ‣ Fine-tuning Large Language Models for Domain-specific Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a> shows the results. Based on the translation outputs in this example, three different fine-tuned templates effectively conveyed the meaning of the original sentence with slight differences in domain-specific terminology. For the No-Dict method, the translation omitted the word “mounting”. The Chain-of-Dict (Chain-of-Dictionary) method translated “mounting ear plate” as “mounting ear board”. Although the results seem to be correct, this suggests that the hints like <span class="ltx_text ltx_font_italic" id="S4.SS8.p1.1.1">“挂耳板” means “mounting ear plate”</span> provided by the chain-of-dictionary method sometimes do not affect the translation output of LLMs. The possible reason is that the model may struggle to understand long contexts, leading to failure of capturing the prompting hints of certain words. The Dict-Rephrasing (Dictionary-Rephrasing) method, by embedding corresponding translations of words directly in the prompt input, can accurately translate the domain-specific terms.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.4">By taking full advantages of both fine-tuning and in-context learning, this paper proposes a prompt-oriented fine-tuning method with LoRA, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT. We select the Llama2-7B as the backbone (Noted that our method can be directly applied to other LLMs), and conduct comprehensive experiments to evaluate our proposed method on four datasets in both general and specific domains. We observe that: (<math alttext="i" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">italic_i</annotation></semantics></math>) the prompt template selection has substantial impact on translation; (<math alttext="ii" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mi id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">i</mi><mo id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><times id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></times><ci id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">𝑖</ci><ci id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">ii</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">italic_i italic_i</annotation></semantics></math>) fine-tuning the LLM on task-specific data can significantly improve the MT capabilities of LLM, and eliminate the need for in-context examples and avoid over-generation; (<math alttext="iii" class="ltx_Math" display="inline" id="S5.p1.3.m3.1"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mi id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml">i</mi><mo id="S5.p1.3.m3.1.1.1" xref="S5.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3.cmml">i</mi><mo id="S5.p1.3.m3.1.1.1a" xref="S5.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.p1.3.m3.1.1.4" xref="S5.p1.3.m3.1.1.4.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><times id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1"></times><ci id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">𝑖</ci><ci id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3">𝑖</ci><ci id="S5.p1.3.m3.1.1.4.cmml" xref="S5.p1.3.m3.1.1.4">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">iii</annotation><annotation encoding="application/x-llamapun" id="S5.p1.3.m3.1d">italic_i italic_i italic_i</annotation></semantics></math>) training on mix-domain data can enhance the domain-specific MT capabilities of LLM, and preserve its high zero-shot MT capabilities; (<math alttext="iv" class="ltx_Math" display="inline" id="S5.p1.4.m4.1"><semantics id="S5.p1.4.m4.1a"><mrow id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml"><mi id="S5.p1.4.m4.1.1.2" xref="S5.p1.4.m4.1.1.2.cmml">i</mi><mo id="S5.p1.4.m4.1.1.1" xref="S5.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.p1.4.m4.1.1.3" xref="S5.p1.4.m4.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><apply id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1"><times id="S5.p1.4.m4.1.1.1.cmml" xref="S5.p1.4.m4.1.1.1"></times><ci id="S5.p1.4.m4.1.1.2.cmml" xref="S5.p1.4.m4.1.1.2">𝑖</ci><ci id="S5.p1.4.m4.1.1.3.cmml" xref="S5.p1.4.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">iv</annotation><annotation encoding="application/x-llamapun" id="S5.p1.4.m4.1d">italic_i italic_v</annotation></semantics></math>) translation prompts that incorporate in-domain vocabularies by simply rephrasing the source sentences outperforms that by using chain-of-thought demonstrations.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">In the future, we plan to address other issues on the domain-specific MT tasks using LLMs, like the translation stylistic problem and the data insufficient problem for document-level translation. We would also like to explore further how to improve the MT capability of LLM in low-resource languages or other specific domains.

</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal <span class="ltx_text ltx_font_italic" id="bib.bib1.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad.

</span>
<span class="ltx_bibblock">In-context examples selection for machine translation.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, <span class="ltx_text ltx_font_italic" id="bib.bib1.3.1">ACL Findings</span>, pages 8857–8873, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alves <span class="ltx_text ltx_font_italic" id="bib.bib2.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Duarte Alves, Nuno Guerreiro, João Alves, José Pombal, Ricardo Rei, José de Souza, Pierre Colombo, and Andre Martins.

</span>
<span class="ltx_bibblock">Steering large language models for machine translation with finetuning and in-context learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.3.1">EMNLP</span>, pages 11127–11148, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown <span class="ltx_text ltx_font_italic" id="bib.bib3.2.2.1">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.3.1">NIPS</span>, pages 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulte and Tezcan [2019]</span>
<span class="ltx_bibblock">
Bram Bulte and Arda Tezcan.

</span>
<span class="ltx_bibblock">Neural fuzzy repair: Integrating fuzzy matches into neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">ACL</span>, pages 1800–1809, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery <span class="ltx_text ltx_font_italic" id="bib.bib5.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1">Journal of Machine Learning Research</span>, 24(240):1–113, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà <span class="ltx_text ltx_font_italic" id="bib.bib6.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1">arXiv preprint arXiv:2207.04672</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farajian <span class="ltx_text ltx_font_italic" id="bib.bib7.2.2.1">et al.</span> [2017]</span>
<span class="ltx_bibblock">
M. Amin Farajian, Marco Turchi, Matteo Negri, and Marcello Federico.

</span>
<span class="ltx_bibblock">Multi-domain neural machine translation through unsupervised adaptation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.3.1">WMT</span>, pages 127–137, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia and Firat [2022]</span>
<span class="ltx_bibblock">
Xavier Garcia and Orhan Firat.

</span>
<span class="ltx_bibblock">Using natural language prompts for machine translation, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghazvininejad <span class="ltx_text ltx_font_italic" id="bib.bib9.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Dictionary-based phrase-level prompting of large language models for machine translation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal <span class="ltx_text ltx_font_italic" id="bib.bib10.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.

</span>
<span class="ltx_bibblock">The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.3.1">Transactions of the Association for Computational Linguistics</span>, 10:522–538, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy <span class="ltx_text ltx_font_italic" id="bib.bib11.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla.

</span>
<span class="ltx_bibblock">How good are gpt models at machine translation? a comprehensive evaluation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu <span class="ltx_text ltx_font_italic" id="bib.bib12.2.2.1">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer <span class="ltx_text ltx_font_italic" id="bib.bib13.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Vivek Iyer, Pinzhen Chen, and Alexandra Birch.

</span>
<span class="ltx_bibblock">Towards effective disambiguation for machine translation with large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.3.1">WMT</span>, pages 482–495, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao <span class="ltx_text ltx_font_italic" id="bib.bib14.2.2.1">et al.</span> [2023a]</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu.

</span>
<span class="ltx_bibblock">ParroT: Translating during chat using large language models tuned with human translation and feedback.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.3.1">EMNLP</span>, pages 15009–15020, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao <span class="ltx_text ltx_font_italic" id="bib.bib15.2.2.1">et al.</span> [2023b]</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu.

</span>
<span class="ltx_bibblock">Is chatgpt a good translator? a preliminary study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.3.1">ArXiv</span>, abs/2301.08745, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn and Knowles [2017]</span>
<span class="ltx_bibblock">
Philipp Koehn and Rebecca Knowles.

</span>
<span class="ltx_bibblock">Six challenges for neural machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1706.03872</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson [2018]</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">EMNLP: System Demonstrations</span>, pages 66–71, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span class="ltx_text ltx_font_italic" id="bib.bib18.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen.

</span>
<span class="ltx_bibblock">Eliciting the translation ability of large language models via multilingual finetuning with translation instructions, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin <span class="ltx_text ltx_font_italic" id="bib.bib19.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <span class="ltx_text ltx_font_italic" id="bib.bib19.3.1">EMNLP</span>, pages 9019–9052, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu <span class="ltx_text ltx_font_italic" id="bib.bib20.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and Furu Wei.

</span>
<span class="ltx_bibblock">Chain-of-dictionary prompting elicits translation in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2305.06575</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moslem <span class="ltx_text ltx_font_italic" id="bib.bib21.2.2.1">et al.</span> [2023a]</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way.

</span>
<span class="ltx_bibblock">Adaptive machine translation with large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.3.1">EAMT Annual Conference</span>, pages 227–237, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moslem <span class="ltx_text ltx_font_italic" id="bib.bib22.2.2.1">et al.</span> [2023b]</span>
<span class="ltx_bibblock">
Yasmin Moslem, Rejwanul Haque, and Andy Way.

</span>
<span class="ltx_bibblock">Fine-tuning large language models for adaptive machine translation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moslem <span class="ltx_text ltx_font_italic" id="bib.bib23.2.2.1">et al.</span> [2023c]</span>
<span class="ltx_bibblock">
Yasmin Moslem, Gianfranco Romani, Mahdi Molaei, John D. Kelleher, Rejwanul Haque, and Andy Way.

</span>
<span class="ltx_bibblock">Domain terminology integration into machine translation: Leveraging large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.3.1">WMT</span>, pages 902–911, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni <span class="ltx_text ltx_font_italic" id="bib.bib24.2.2.1">et al.</span> [2002]</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</span>, pages 311–318, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović [2015]</span>
<span class="ltx_bibblock">
Maja Popović.

</span>
<span class="ltx_bibblock">chrf: character n-gram f-score for automatic mt evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proceedings of the tenth workshop on statistical machine translation</span>, pages 392–395, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei <span class="ltx_text ltx_font_italic" id="bib.bib26.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Ricardo Rei, José GC De Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André FT Martins.

</span>
<span class="ltx_bibblock">Comet-22: Unbabel-ist 2022 submission for the metrics shared task.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.3.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</span>, pages 578–585, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reinauer <span class="ltx_text ltx_font_italic" id="bib.bib27.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Raphael Reinauer, Patrick Simianer, Kaden Uhlig, Johannes E. M. Mosig, and Joern Wuebker.

</span>
<span class="ltx_bibblock">Neural machine translation models can learn to be few-shot learners, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reynolds and McDonell [2021]</span>
<span class="ltx_bibblock">
Laria Reynolds and Kyle McDonell.

</span>
<span class="ltx_bibblock">Prompt programming for large language models: Beyond the few-shot paradigm.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">CHI EA</span>, pages 1–7, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao <span class="ltx_text ltx_font_italic" id="bib.bib29.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1">arXiv preprint arXiv:2211.05100</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schioppa <span class="ltx_text ltx_font_italic" id="bib.bib30.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Andrea Schioppa, Xavier Garcia, and Orhan Firat.

</span>
<span class="ltx_bibblock">Cross-lingual supervision improves large language models pre-training, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team [2023]</span>
<span class="ltx_bibblock">
InternLM Team.

</span>
<span class="ltx_bibblock">Internlm: A multilingual language model with progressively enhanced capabilities, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian <span class="ltx_text ltx_font_italic" id="bib.bib32.2.2.1">et al.</span> [2014]</span>
<span class="ltx_bibblock">
Liang Tian, Derek F Wong, Lidia S Chao, Paulo Quaresma, Francisco Oliveira, and Lu Yi.

</span>
<span class="ltx_bibblock">Um-corpus: A large english-chinese parallel corpus for statistical machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.3.1">LREC</span>, pages 1837–1842, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron <span class="ltx_text ltx_font_italic" id="bib.bib33.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.3.1">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar <span class="ltx_text ltx_font_italic" id="bib.bib34.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster.

</span>
<span class="ltx_bibblock">Prompting PaLM for translation: Assessing strategies and performance.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.3.1">ACL</span>, pages 15406–15427, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei <span class="ltx_text ltx_font_italic" id="bib.bib35.2.2.1">et al.</span> [2022a]</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.3.1">ICLR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei <span class="ltx_text ltx_font_italic" id="bib.bib36.2.2.1">et al.</span> [2022b]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.3.1">NIPS</span>, volume 35, pages 24824–24837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei <span class="ltx_text ltx_font_italic" id="bib.bib37.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al.

</span>
<span class="ltx_bibblock">Polylm: An open source polyglot large language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1">arXiv preprint arXiv:2307.06018</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata <span class="ltx_text ltx_font_italic" id="bib.bib38.2.2.1">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, and Pascale Fung.

</span>
<span class="ltx_bibblock">Language models are few-shot multilingual learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 1st Workshop on Multilingual Representation Learning</span>, ACL, pages 1–15, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu <span class="ltx_text ltx_font_italic" id="bib.bib39.2.2.1">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Jitao Xu, Josep Crego, and Jean Senellart.

</span>
<span class="ltx_bibblock">Boosting neural machine translation with similar translations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.3.1">ACL</span>, pages 1580–1590, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu <span class="ltx_text ltx_font_italic" id="bib.bib40.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla.

</span>
<span class="ltx_bibblock">A paradigm shift in machine translation: Boosting translation performance of large language models.

</span>
<span class="ltx_bibblock">volume abs/2309.11674, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue <span class="ltx_text ltx_font_italic" id="bib.bib41.2.2.1">et al.</span> [2021]</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel.

</span>
<span class="ltx_bibblock">mT5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span>, pages 483–498, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span class="ltx_text ltx_font_italic" id="bib.bib42.2.2.1">et al.</span> [2023a]</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al.

</span>
<span class="ltx_bibblock">Baichuan 2: Open large-scale language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.3.1">arXiv preprint arXiv:2309.10305</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span class="ltx_text ltx_font_italic" id="bib.bib43.2.2.1">et al.</span> [2023b]</span>
<span class="ltx_bibblock">
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong.

</span>
<span class="ltx_bibblock">Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.3.1">arXiv preprint arXiv:2305.18098</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang <span class="ltx_text ltx_font_italic" id="bib.bib44.2.2.1">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich.

</span>
<span class="ltx_bibblock">Improving massively multilingual neural machine translation and zero-shot translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.3.1">arXiv preprint arXiv:2004.11867</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang <span class="ltx_text ltx_font_italic" id="bib.bib45.2.2.1">et al.</span> [2022]</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang <span class="ltx_text ltx_font_italic" id="bib.bib46.2.2.1">et al.</span> [2023]</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Haddow, and Alexandra Birch.

</span>
<span class="ltx_bibblock">Prompting large language model for machine translation: A case study.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.3.1">ICML</span>, pages 41092–41110, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Feb 23 02:09:29 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
