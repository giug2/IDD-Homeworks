<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models</title>
<!--Generated on Fri Oct  4 10:46:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03311v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S1" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2.SS1" title="In 2 Related Work â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Large Language Models and Multi-Modality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2.SS2" title="In 2 Related Work â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Vector Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2.SS3" title="In 2 Related Work â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Human Motion Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S3" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MotionBase Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S4" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Scaling up Large Motion Model</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S4.SS1" title="In 4 Scaling up Large Motion Model â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Overall Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S4.SS2" title="In 4 Scaling up Large Motion Model â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>2D Lookup-free Motion Quantization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS1" title="In 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS2" title="In 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Discussion of Scaling up motion generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS3" title="In 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Discussion of Motion Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS4" title="In 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Limitation of Automated Metric</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S6" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Details of MoseBase</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.SS1" title="In Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Statistic Analyses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.SS2" title="In Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Prompt of Motion Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.SS3" title="In Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Word Distribution Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A2" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Overview of Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS1" title="In Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Ablation of Synthesis and Static Data?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS2" title="In Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Ablation of Different Encoder Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS3" title="In Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Ablation of Learning from Scratch vs. Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS4" title="In Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Ablation of Different Loss Calculation Strategies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS5" title="In Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.5 </span>Ablation of Motion Quantization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A4" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Additional Quantitative Results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ye Wang<sup class="ltx_sup" id="id12.12.id1"><span class="ltx_text ltx_font_italic" id="id12.12.id1.1">1</span></sup>, Â  Sipeng Zheng<sup class="ltx_sup" id="id13.13.id2"><span class="ltx_text ltx_font_italic" id="id13.13.id2.1">2</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>, Â Bin Cao<sup class="ltx_sup" id="id14.14.id3"><span class="ltx_text ltx_font_italic" id="id14.14.id3.1">2,3</span></sup>, Qianshan Wei<sup class="ltx_sup" id="id15.15.id4"><span class="ltx_text ltx_font_italic" id="id15.15.id4.1">4</span></sup>, Qin Jin<sup class="ltx_sup" id="id16.16.id5">1</sup>, Zongqing Lu<sup class="ltx_sup" id="id17.17.id6"><span class="ltx_text ltx_font_italic" id="id17.17.id6.1">5</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id18.18.id7">1</sup>Renmin University
<br class="ltx_break"/><sup class="ltx_sup" id="id19.19.id8">2</sup>Beijing Academy of Artificial Intelligence
<br class="ltx_break"/><sup class="ltx_sup" id="id20.20.id9">3</sup>Institute of Automation, Chinese Academy of Sciences
<br class="ltx_break"/><sup class="ltx_sup" id="id21.21.id10">4</sup>Southeast University
<br class="ltx_break"/><sup class="ltx_sup" id="id22.22.id11">5</sup>School of Computer Science, Peking University
</span><span class="ltx_author_notes">Equal contribution. Ye Wang works as an intern at BAAICorrespondence to Zongqing Lu &lt;zongqing.lu@pku.edu.cn&gt;.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id23.id1">Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted towards the development of large motion models.
Despite some progress, current state-of-the-art works remain far from achieving truly generalist models, largely due to the lack of large-scale, high-quality motion data.
To address this, we present MotionBase, the first million-level motion generation benchmark, offering 15 times the data volume of the previous largest dataset, and featuring multimodal data with hierarchically detailed text descriptions.
By leveraging this vast dataset, our large motion model demonstrates strong performance across a broad range of motions, including unseen ones.
Through systematic investigation, we underscore the importance of scaling both data and model size, with synthetic data and pseudo labels playing a crucial role in mitigating data acquisition costs.
Moreover, our research reveals the limitations of existing evaluation metrics, particularly in handling out-of-domain text instructions â€” an issue that has long been overlooked.
In addition to these, we introduce a novel 2D lookup-free approach for motion tokenization, which preserves motion information and expands codebook capacity, further enhancing the representative ability of large motion models.
The release of MotionBase and the insights gained from this study are expected to pave the way for the development of more powerful and versatile motion generation models.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Motion generation is an emerging field with diverse applications in video games, filmmaking, and robotics animation.
At the forefront of this area is text-to-motion generation (T2M)Â <cite class="ltx_cite ltx_citemacro_citep">(Ahn etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib1" title="">2018</a>; Ahuja &amp; Morency, <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib2" title="">2019</a>)</cite>, which plays a crucial role in translating natural language into human motions.
State-of-the-art T2M models typically rely on a combination of the motion quantization methods (e.g., VQÂ <cite class="ltx_cite ltx_citemacro_citep">(Van DenÂ Oord etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib48" title="">2017</a>)</cite>), along with a text encoder (e.g., CLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib39" title="">2021</a>)</cite>) and decoder (e.g., GPT-2Â <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib38" title="">2019</a>)</cite>) to generate motion sequences from detailed textual instructions.
Despite the availability of a few high-quality datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>; Lin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite> curated in recent years, their limited size restricts current methods to a narrow range of scenarios, creating performance bottlenecks when addressing diverse or unseen motions, as illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">1</span></a> (RIGHT).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The rapid advancement of large language models (LLMs)Â <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib46" title="">2023a</a>)</cite> in multimodal learning has been significantly bolstered by the availability of vast data resourcesÂ <cite class="ltx_cite ltx_citemacro_citep">(Zheng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib67" title="">2024</a>; Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib53" title="">2024</a>)</cite>.
In contrast, the volume of motion data remains considerably smaller than that of visual-text data, as illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">1</span></a> (LEFT).
This disparity primarily arises from the high costs associated with motion data collection, which often requires specialized wearable devices and substantial human labor for annotation.
Consequently, developing a state-of-the-art (SoTA) large motion model based on LLMs presents a significant challenge and remains an unresolved issue.
While some recent effortsÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib21" title="">2023</a>)</cite> have explored this direction, the effectiveness of large motion models has yet to be fully demonstrated.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we aim to address the question: â€œ<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p3.1.1">Can a large motion model be a promising direction for motion generation?</span>â€
To tackle this, we have developed a systematic data collection scheme that led to the creation of MotionBase, the first large-scale dataset containing over one million motion sequences â€” 15 times larger than the previous largest dataset.
This initiative provides a solid foundation for building robust, universally applicable large motion models and offers a comprehensive testbed for future research.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="276" id="S1.F1.g1" src="x1.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span class="ltx_text ltx_font_bold" id="S1.F1.6.1">LEFT</span>: Curves showing the effects of scaling up large motion models. MotionBase is the first large text-to-motion dataset comparable in scale to visual benchmarks like ImageNet.
<span class="ltx_text ltx_font_bold" id="S1.F1.7.2">RIGHT</span>: While existing models perform well on constrained datasets like <span class="ltx_text" id="S1.F1.8.3" style="color:#FF8000;">Motion-X</span> and <span class="ltx_text" id="S1.F1.9.4" style="color:#BF0040;">HumanML3D</span>, they struggle with out-of-domain concepts on <span class="ltx_text" id="S1.F1.10.5" style="color:#00FFFF;">MotionBase</span>, exhibiting limited generalization.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Building on the solid foundation of MotionBase, we can now conduct a comprehensive investigation into the effectiveness of large motion models.
This research aims to firstly identify key factors driving their advancement and offer valuable insights for future model design, including:
â¶ scaling both data and model size significantly reduces joint prediction errors on critical metrics while improving generalization to novel motions.
â· Despite observable domain gaps, synthetic and static data, as well as pseudo motion labels are becoming increasingly essential and effective, especially given the high cost of acquiring ground truth motion data.
â¸ Existing metrics show limitations when faced with out-of-domain text instructions.
Notably, the widely used metric, FID, fails to accurately capture the alignment between ground truth and generated motions.
Our findings highlight the need for a more robust and equitable evaluation framework that enhances open-set generalization.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.7">In addition to these factors, we argue that large motion models are further constrained by inadequate motion representation.
Most approaches rely on transforming motion into discrete tokens via vector quantization (VQ), which are then processed by autoregressive models to generate motion sequences.
While these methods have produced impressive results, they suffer from two major drawbacks.
â¶ <span class="ltx_text ltx_font_bold" id="S1.p5.7.1">Information loss</span>:
The current VQ process inevitably leads to the loss of critical information.
Given a motion clip with <math alttext="D" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mi id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">italic_D</annotation></semantics></math>-dimensional features <math alttext="\mathcal{M}=\{m_{1},m_{2},...,m_{T}\}" class="ltx_Math" display="inline" id="S1.p5.2.m2.4"><semantics id="S1.p5.2.m2.4a"><mrow id="S1.p5.2.m2.4.4" xref="S1.p5.2.m2.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S1.p5.2.m2.4.4.5" xref="S1.p5.2.m2.4.4.5.cmml">â„³</mi><mo id="S1.p5.2.m2.4.4.4" xref="S1.p5.2.m2.4.4.4.cmml">=</mo><mrow id="S1.p5.2.m2.4.4.3.3" xref="S1.p5.2.m2.4.4.3.4.cmml"><mo id="S1.p5.2.m2.4.4.3.3.4" stretchy="false" xref="S1.p5.2.m2.4.4.3.4.cmml">{</mo><msub id="S1.p5.2.m2.2.2.1.1.1" xref="S1.p5.2.m2.2.2.1.1.1.cmml"><mi id="S1.p5.2.m2.2.2.1.1.1.2" xref="S1.p5.2.m2.2.2.1.1.1.2.cmml">m</mi><mn id="S1.p5.2.m2.2.2.1.1.1.3" xref="S1.p5.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S1.p5.2.m2.4.4.3.3.5" xref="S1.p5.2.m2.4.4.3.4.cmml">,</mo><msub id="S1.p5.2.m2.3.3.2.2.2" xref="S1.p5.2.m2.3.3.2.2.2.cmml"><mi id="S1.p5.2.m2.3.3.2.2.2.2" xref="S1.p5.2.m2.3.3.2.2.2.2.cmml">m</mi><mn id="S1.p5.2.m2.3.3.2.2.2.3" xref="S1.p5.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S1.p5.2.m2.4.4.3.3.6" xref="S1.p5.2.m2.4.4.3.4.cmml">,</mo><mi id="S1.p5.2.m2.1.1" mathvariant="normal" xref="S1.p5.2.m2.1.1.cmml">â€¦</mi><mo id="S1.p5.2.m2.4.4.3.3.7" xref="S1.p5.2.m2.4.4.3.4.cmml">,</mo><msub id="S1.p5.2.m2.4.4.3.3.3" xref="S1.p5.2.m2.4.4.3.3.3.cmml"><mi id="S1.p5.2.m2.4.4.3.3.3.2" xref="S1.p5.2.m2.4.4.3.3.3.2.cmml">m</mi><mi id="S1.p5.2.m2.4.4.3.3.3.3" xref="S1.p5.2.m2.4.4.3.3.3.3.cmml">T</mi></msub><mo id="S1.p5.2.m2.4.4.3.3.8" stretchy="false" xref="S1.p5.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.4b"><apply id="S1.p5.2.m2.4.4.cmml" xref="S1.p5.2.m2.4.4"><eq id="S1.p5.2.m2.4.4.4.cmml" xref="S1.p5.2.m2.4.4.4"></eq><ci id="S1.p5.2.m2.4.4.5.cmml" xref="S1.p5.2.m2.4.4.5">â„³</ci><set id="S1.p5.2.m2.4.4.3.4.cmml" xref="S1.p5.2.m2.4.4.3.3"><apply id="S1.p5.2.m2.2.2.1.1.1.cmml" xref="S1.p5.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S1.p5.2.m2.2.2.1.1.1.1.cmml" xref="S1.p5.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S1.p5.2.m2.2.2.1.1.1.2.cmml" xref="S1.p5.2.m2.2.2.1.1.1.2">ğ‘š</ci><cn id="S1.p5.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S1.p5.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S1.p5.2.m2.3.3.2.2.2.cmml" xref="S1.p5.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S1.p5.2.m2.3.3.2.2.2.1.cmml" xref="S1.p5.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S1.p5.2.m2.3.3.2.2.2.2.cmml" xref="S1.p5.2.m2.3.3.2.2.2.2">ğ‘š</ci><cn id="S1.p5.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S1.p5.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1">â€¦</ci><apply id="S1.p5.2.m2.4.4.3.3.3.cmml" xref="S1.p5.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S1.p5.2.m2.4.4.3.3.3.1.cmml" xref="S1.p5.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S1.p5.2.m2.4.4.3.3.3.2.cmml" xref="S1.p5.2.m2.4.4.3.3.3.2">ğ‘š</ci><ci id="S1.p5.2.m2.4.4.3.3.3.3.cmml" xref="S1.p5.2.m2.4.4.3.3.3.3">ğ‘‡</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.4c">\mathcal{M}=\{m_{1},m_{2},...,m_{T}\}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.2.m2.4d">caligraphic_M = { italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_m start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }</annotation></semantics></math>, where <math alttext="m_{i}\in\mathbbm{R}^{D}" class="ltx_Math" display="inline" id="S1.p5.3.m3.1"><semantics id="S1.p5.3.m3.1a"><mrow id="S1.p5.3.m3.1.1" xref="S1.p5.3.m3.1.1.cmml"><msub id="S1.p5.3.m3.1.1.2" xref="S1.p5.3.m3.1.1.2.cmml"><mi id="S1.p5.3.m3.1.1.2.2" xref="S1.p5.3.m3.1.1.2.2.cmml">m</mi><mi id="S1.p5.3.m3.1.1.2.3" xref="S1.p5.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S1.p5.3.m3.1.1.1" xref="S1.p5.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S1.p5.3.m3.1.1.3" xref="S1.p5.3.m3.1.1.3.cmml"><mi id="S1.p5.3.m3.1.1.3.2" xref="S1.p5.3.m3.1.1.3.2.cmml">â„</mi><mi id="S1.p5.3.m3.1.1.3.3" xref="S1.p5.3.m3.1.1.3.3.cmml">D</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.3.m3.1b"><apply id="S1.p5.3.m3.1.1.cmml" xref="S1.p5.3.m3.1.1"><in id="S1.p5.3.m3.1.1.1.cmml" xref="S1.p5.3.m3.1.1.1"></in><apply id="S1.p5.3.m3.1.1.2.cmml" xref="S1.p5.3.m3.1.1.2"><csymbol cd="ambiguous" id="S1.p5.3.m3.1.1.2.1.cmml" xref="S1.p5.3.m3.1.1.2">subscript</csymbol><ci id="S1.p5.3.m3.1.1.2.2.cmml" xref="S1.p5.3.m3.1.1.2.2">ğ‘š</ci><ci id="S1.p5.3.m3.1.1.2.3.cmml" xref="S1.p5.3.m3.1.1.2.3">ğ‘–</ci></apply><apply id="S1.p5.3.m3.1.1.3.cmml" xref="S1.p5.3.m3.1.1.3"><csymbol cd="ambiguous" id="S1.p5.3.m3.1.1.3.1.cmml" xref="S1.p5.3.m3.1.1.3">superscript</csymbol><ci id="S1.p5.3.m3.1.1.3.2.cmml" xref="S1.p5.3.m3.1.1.3.2">â„</ci><ci id="S1.p5.3.m3.1.1.3.3.cmml" xref="S1.p5.3.m3.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.3.m3.1c">m_{i}\in\mathbbm{R}^{D}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.3.m3.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, VQ compresses it into a list of 1D embeddings of size <math alttext="\lfloor T/\alpha\rfloor\times d" class="ltx_Math" display="inline" id="S1.p5.4.m4.1"><semantics id="S1.p5.4.m4.1a"><mrow id="S1.p5.4.m4.1.1" xref="S1.p5.4.m4.1.1.cmml"><mrow id="S1.p5.4.m4.1.1.1.1" xref="S1.p5.4.m4.1.1.1.2.cmml"><mo id="S1.p5.4.m4.1.1.1.1.2" stretchy="false" xref="S1.p5.4.m4.1.1.1.2.1.cmml">âŒŠ</mo><mrow id="S1.p5.4.m4.1.1.1.1.1" xref="S1.p5.4.m4.1.1.1.1.1.cmml"><mi id="S1.p5.4.m4.1.1.1.1.1.2" xref="S1.p5.4.m4.1.1.1.1.1.2.cmml">T</mi><mo id="S1.p5.4.m4.1.1.1.1.1.1" xref="S1.p5.4.m4.1.1.1.1.1.1.cmml">/</mo><mi id="S1.p5.4.m4.1.1.1.1.1.3" xref="S1.p5.4.m4.1.1.1.1.1.3.cmml">Î±</mi></mrow><mo id="S1.p5.4.m4.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S1.p5.4.m4.1.1.1.2.1.cmml">âŒ‹</mo></mrow><mo id="S1.p5.4.m4.1.1.2" rspace="0.222em" xref="S1.p5.4.m4.1.1.2.cmml">Ã—</mo><mi id="S1.p5.4.m4.1.1.3" xref="S1.p5.4.m4.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.4.m4.1b"><apply id="S1.p5.4.m4.1.1.cmml" xref="S1.p5.4.m4.1.1"><times id="S1.p5.4.m4.1.1.2.cmml" xref="S1.p5.4.m4.1.1.2"></times><apply id="S1.p5.4.m4.1.1.1.2.cmml" xref="S1.p5.4.m4.1.1.1.1"><floor id="S1.p5.4.m4.1.1.1.2.1.cmml" xref="S1.p5.4.m4.1.1.1.1.2"></floor><apply id="S1.p5.4.m4.1.1.1.1.1.cmml" xref="S1.p5.4.m4.1.1.1.1.1"><divide id="S1.p5.4.m4.1.1.1.1.1.1.cmml" xref="S1.p5.4.m4.1.1.1.1.1.1"></divide><ci id="S1.p5.4.m4.1.1.1.1.1.2.cmml" xref="S1.p5.4.m4.1.1.1.1.1.2">ğ‘‡</ci><ci id="S1.p5.4.m4.1.1.1.1.1.3.cmml" xref="S1.p5.4.m4.1.1.1.1.1.3">ğ›¼</ci></apply></apply><ci id="S1.p5.4.m4.1.1.3.cmml" xref="S1.p5.4.m4.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.4.m4.1c">\lfloor T/\alpha\rfloor\times d</annotation><annotation encoding="application/x-llamapun" id="S1.p5.4.m4.1d">âŒŠ italic_T / italic_Î± âŒ‹ Ã— italic_d</annotation></semantics></math>, where <math alttext="\alpha" class="ltx_Math" display="inline" id="S1.p5.5.m5.1"><semantics id="S1.p5.5.m5.1a"><mi id="S1.p5.5.m5.1.1" xref="S1.p5.5.m5.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S1.p5.5.m5.1b"><ci id="S1.p5.5.m5.1.1.cmml" xref="S1.p5.5.m5.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.5.m5.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S1.p5.5.m5.1d">italic_Î±</annotation></semantics></math> is the temporal downsampling ratio and <math alttext="d" class="ltx_Math" display="inline" id="S1.p5.6.m6.1"><semantics id="S1.p5.6.m6.1a"><mi id="S1.p5.6.m6.1.1" xref="S1.p5.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.p5.6.m6.1b"><ci id="S1.p5.6.m6.1.1.cmml" xref="S1.p5.6.m6.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.6.m6.1c">d</annotation><annotation encoding="application/x-llamapun" id="S1.p5.6.m6.1d">italic_d</annotation></semantics></math> is the codebook dimension.
Unlike images, which consist of uniform RGB pixel values, each motion state <math alttext="m_{i}" class="ltx_Math" display="inline" id="S1.p5.7.m7.1"><semantics id="S1.p5.7.m7.1a"><msub id="S1.p5.7.m7.1.1" xref="S1.p5.7.m7.1.1.cmml"><mi id="S1.p5.7.m7.1.1.2" xref="S1.p5.7.m7.1.1.2.cmml">m</mi><mi id="S1.p5.7.m7.1.1.3" xref="S1.p5.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p5.7.m7.1b"><apply id="S1.p5.7.m7.1.1.cmml" xref="S1.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S1.p5.7.m7.1.1.1.cmml" xref="S1.p5.7.m7.1.1">subscript</csymbol><ci id="S1.p5.7.m7.1.1.2.cmml" xref="S1.p5.7.m7.1.1.2">ğ‘š</ci><ci id="S1.p5.7.m7.1.1.3.cmml" xref="S1.p5.7.m7.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.7.m7.1c">m_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.7.m7.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> contains a set of distinct features (e.g., joint position, velocity, foot-ground contact).
Using a single 1D embedding to represent such complex motion states is insufficient.
This not only results in the loss of vital information but also limits the modelâ€™s ability to flexibly generate motion at a part-level.
â· <span class="ltx_text ltx_font_bold" id="S1.p5.7.2">Limited Codebook Size: </span> Existing VQ are limited by a small codebook, meaning that all possible human motions must be selected from these limited options.
Consequently, these 1D embeddings fail to capture the vast diversity of human motion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To address this issue, we propose treating a motion clip as a 2D image with a single channel, represented as <math alttext="\mathcal{M}\in R^{T\times D\times 1}" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S1.p6.1.m1.1.1.2" xref="S1.p6.1.m1.1.1.2.cmml">â„³</mi><mo id="S1.p6.1.m1.1.1.1" xref="S1.p6.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S1.p6.1.m1.1.1.3" xref="S1.p6.1.m1.1.1.3.cmml"><mi id="S1.p6.1.m1.1.1.3.2" xref="S1.p6.1.m1.1.1.3.2.cmml">R</mi><mrow id="S1.p6.1.m1.1.1.3.3" xref="S1.p6.1.m1.1.1.3.3.cmml"><mi id="S1.p6.1.m1.1.1.3.3.2" xref="S1.p6.1.m1.1.1.3.3.2.cmml">T</mi><mo id="S1.p6.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S1.p6.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S1.p6.1.m1.1.1.3.3.3" xref="S1.p6.1.m1.1.1.3.3.3.cmml">D</mi><mo id="S1.p6.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S1.p6.1.m1.1.1.3.3.1.cmml">Ã—</mo><mn id="S1.p6.1.m1.1.1.3.3.4" xref="S1.p6.1.m1.1.1.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><apply id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"><in id="S1.p6.1.m1.1.1.1.cmml" xref="S1.p6.1.m1.1.1.1"></in><ci id="S1.p6.1.m1.1.1.2.cmml" xref="S1.p6.1.m1.1.1.2">â„³</ci><apply id="S1.p6.1.m1.1.1.3.cmml" xref="S1.p6.1.m1.1.1.3"><csymbol cd="ambiguous" id="S1.p6.1.m1.1.1.3.1.cmml" xref="S1.p6.1.m1.1.1.3">superscript</csymbol><ci id="S1.p6.1.m1.1.1.3.2.cmml" xref="S1.p6.1.m1.1.1.3.2">ğ‘…</ci><apply id="S1.p6.1.m1.1.1.3.3.cmml" xref="S1.p6.1.m1.1.1.3.3"><times id="S1.p6.1.m1.1.1.3.3.1.cmml" xref="S1.p6.1.m1.1.1.3.3.1"></times><ci id="S1.p6.1.m1.1.1.3.3.2.cmml" xref="S1.p6.1.m1.1.1.3.3.2">ğ‘‡</ci><ci id="S1.p6.1.m1.1.1.3.3.3.cmml" xref="S1.p6.1.m1.1.1.3.3.3">ğ·</ci><cn id="S1.p6.1.m1.1.1.3.3.4.cmml" type="integer" xref="S1.p6.1.m1.1.1.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\mathcal{M}\in R^{T\times D\times 1}</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">caligraphic_M âˆˆ italic_R start_POSTSUPERSCRIPT italic_T Ã— italic_D Ã— 1 end_POSTSUPERSCRIPT</annotation></semantics></math>.
By expanding the dimensionality of the motion clip from 1D to 2D, we enhance the encoderâ€™s capacity, improving its ability to represent complex motions while retaining more critical information after tokenization.
Although increasing the size of the codebook is a straightforward way to enhance its expressiveness, this approach often leads to â€œcodebook collapse," particularly when training samples are scarce.
To mitigate this, we introduce a finite scalar quantizing method inspired by Â <cite class="ltx_cite ltx_citemacro_citet">Mentzer etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib32" title="">2023</a>)</cite>, which enables learning a large motion vocabulary without requiring a lookup for corresponding tokens in the codebook for each entry.
As a result, we expand the motion codebook by at least two orders of magnitude, boosting its representational capacity while maintaining efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We summarize our main contributions as follows.
<span class="ltx_text ltx_font_bold" id="S1.p7.1.1">(1) MotionBase</span>: We introduce MotionBase, the first large-scale motion generation benchmark containing over one million motions with detailed textual descriptions, significantly advancing the capability to effectively train motion generation models.
<span class="ltx_text ltx_font_bold" id="S1.p7.1.2">(2) Key Insights</span>: Our research identifies critical factors affecting the effectiveness of large motion models, emphasizing the importance of scaling both data and model size. Additionally, we uncover limitations in the current evaluation metrics, particularly when handling diverse and unseen motions.
<span class="ltx_text ltx_font_bold" id="S1.p7.1.3">(3) Novel Motion Quantization</span>: We propose a novel motion quantization approach that represents motion clips as 2D images and constructs a finite-scale codebook without requiring token lookups. This method retains essential information and expands the capacity of the motion encoder, enhancing the ability of large motion models to leverage large-scale motion data.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Large Language Models and Multi-Modality</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Substantial advancements have been made in enhancing LLMsÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib5" title="">2020</a>; Raffel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib40" title="">2020</a>; Chowdhery etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib9" title="">2022</a>)</cite> with the ability to understand and respond to human instructions, through a technique known as instruction tuningÂ <cite class="ltx_cite ltx_citemacro_citep">(Ouyang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib34" title="">2022</a>)</cite>.
Recent research has extended these capabilities to the multimodal domainÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib56" title="">2023</a>; Zheng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib66" title="">2023</a>)</cite>, with notable work by <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib27" title="">2023</a>)</cite>, who pioneered visual instruction tuning to create a highly adaptable visual assistant.
Additionally, <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib23" title="">2023a</a>)</cite> integrated multimodal context directly into instruction data to further enhance model performance.
Subsequent studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib63" title="">2023b</a>; Zhao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib65" title="">2023</a>)</cite> expanded this research by scaling up instructional datasets and incorporating image-rich text.
Notably, <cite class="ltx_cite ltx_citemacro_cite">Dai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib11" title="">2023</a>)</cite> developed InstructBLIP, based on BLIP-2Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib24" title="">2023b</a>)</cite>, which features an advanced visual feature extraction mechanism to improve performance across vision-language tasks.
Despite these breakthroughs, the application of multimodal models to human motion remains less competitive compared to current state-of-the-art (SoTA) methods, although recent initiatives are beginning to explore this domainÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib21" title="">2023</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib64" title="">2024b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Vector Quantization</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Vector quantization (VQ) has been highly successful in generating high-quality imagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Van DenÂ Oord etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib48" title="">2017</a>)</cite> and videosÂ <cite class="ltx_cite ltx_citemacro_citep">(Gupta etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib20" title="">2022</a>; Yan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib55" title="">2021</a>)</cite>.
VQ-VAE first converts images into discrete representations and autoregressively models their distribution.
Building on this, <cite class="ltx_cite ltx_citemacro_cite">Lee etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib22" title="">2022</a>)</cite> introduced residual quantization (RQ), which encodes images into a stacked map of discrete codes, efficiently reducing the spatial resolution of features.
<cite class="ltx_cite ltx_citemacro_cite">You etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib57" title="">2022</a>)</cite> further developed hierarchical vector quantization (HQ), employing a pyramid scheme with two-level codes for image encoding.
Most existing motion generation approaches have adopted VQ or its variants to quantize human motions.
However, the small codebook size in traditional VQ methods limits their ability to generalize and accurately represent the diversity of human motions.
Although increasing the codebook size can improve representational capacity, it often leads to codebook collapse.
Recently, <cite class="ltx_cite ltx_citemacro_cite">Mentzer etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib32" title="">2023</a>)</cite> demonstrated that discrete codes can be obtained via scalar quantization, where each scalar entry is independently quantized to the nearest integer through rounding.
Similarly, <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib58" title="">2023</a>)</cite> introduced a lookup-free codebook that maps videos into compact discrete tokens, utilizing all codes without auxiliary losses and expanding the codebook size.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Human Motion Generation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The task of motion generation involves creating human motion based on various inputs, such as text descriptionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib18" title="">2022b</a>; Petrovich etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib36" title="">2022</a>)</cite>, action labelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Cervantes etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib7" title="">2022</a>; Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib16" title="">2020</a>)</cite> or motion prefixesÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib28" title="">2022</a>; Mao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib30" title="">2019</a>)</cite>.
Among these, text-to-motion (T2M) generation has received the most attention due to the ease and flexibility of using natural language as input.
Early approachesÂ <cite class="ltx_cite ltx_citemacro_citep">(Fragkiadaki etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib13" title="">2015</a>; Ghosh etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib14" title="">2017</a>; Gopalakrishnan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib15" title="">2019</a>)</cite> rely on deterministic motion modeling, which often produce averaged, blurry results.
To overcome this, researchers introduce stochastic methods using models like GANsÂ <cite class="ltx_cite ltx_citemacro_citep">(Cai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib6" title="">2018</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib51" title="">2020</a>)</cite> or VAEsÂ <cite class="ltx_cite ltx_citemacro_citep">(Aliakbarian etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib3" title="">2020</a>)</cite>.
For instance, T2M-GPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib60" title="">2023a</a>)</cite> extends the temporal VAE to capture the probabilistic relationship between text and motion.
More recently, <cite class="ltx_cite ltx_citemacro_cite">Guo etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib19" title="">2024</a>)</cite> proposed improving traditional vector quantization (VQ) by integrating residual quantization and a masked modeling framework.
To better align with a motion auto-encoder, MotionCLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Tevet etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib45" title="">2022</a>)</cite> incorporates CLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib39" title="">2021</a>)</cite> as the text encoder, bringing in more robust text priors.
Additionally, <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib64" title="">2024b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib21" title="">2023</a>)</cite> explored the development of unified models based on LLMs which accept multimodal conditions (e.g., vision, text, and pose), enabling the generation of subsequent, preceding, or â€œin-betweenâ€ motions.
Despite leveraging the power of LLMs, these large motion models remain limited to in-domain text instructions and do not yet perform as competitively as existing SoTA methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">In this work, we aim to bridge the gap between large language models and generalized, reliable large motion models.
To achieve this, We begin by introducing MotionBase â€” a novel, large-scale dataset designed to support extensive pretraining and comprehensive fair evaluation.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with existing human motion datasets. More details can be found in our appendix. In the table, B, H, and F refer to body, hand, and face, respectively. â€œpartâ€ indicates that the text captions include fine-grained descriptions of body parts, while â€œbodyâ€ means the descriptions are not as detailed. â€œmultiâ€ and â€œsingleâ€ specify whether the dataset contains multi-person scenarios or only single-person data. Our MotionBase is the largest motion generation dataset and benchmark to date, featuring at least 15Ã— more data than previous datasets, along with additional modalities.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.1" style="width:523.6pt;height:73.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.5pt,8.1pt) scale(0.82,0.82) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">SEQ NUMBER</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">MOTION</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">TEXT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">RGB</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">DEPTH</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">BBOX</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">PERSON</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">KITÂ <cite class="ltx_cite ltx_citemacro_citep">(Plappert etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib37" title="">2016</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">5.7K</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">body</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ—</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ—</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ—</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">single</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.1.4.2.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">HumanML3DÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">29.2K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">body</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ—</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ—</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ—</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">single</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.1.5.3.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">MotionXÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">81.1K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">B,H,F</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">body</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ—</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ—</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">single</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">MotionBase-V1</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<math alttext="&gt;" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><gt id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math>1M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">B,H</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">part</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">âœ“</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.1.1.1.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">multi</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MotionBase Dataset</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Data is the foundation of large motion models.
With advancements in fields like human pose detection, we are now able to extract high-quality motion sequences from vast amounts of online videos, including datasets like InternViDÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib50" title="">2023</a>)</cite> and WebVidÂ <cite class="ltx_cite ltx_citemacro_citep">(Bain etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib4" title="">2021</a>)</cite>.
In its initial public release, our MotionBase contains over one million motion clips, each annotated with fine-grained automatic pseudo labels.
A comparison with existing benchmarks is presented in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2.T1" title="Table 1 â€£ 2.3 Human Motion Generation â€£ 2 Related Work â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">1</span></a>.
Our data collection pipeline involves the following key steps in order.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">â¶ <span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Source Video Collection and Cleaning: </span>
We begin by collecting over 20 million videos from publicly available datasets and online platforms such as YouTube.
To ensure quality and relevance, we filter out videos that do not contain human figures.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">â· <span class="ltx_text ltx_font_bold" id="S3.p3.1.1">2D-3D Keypoint Estimation: </span>
Keypoints are essential for capturing the skeletal structure of human motion.
Initially, we estimate whole-body 2D keypoints with confidence scores using a pretrained modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib54" title="">2022</a>)</cite>.
To further enhance motion accuracy, we estimate precise 3D keypoints with another pretrained modelÂ <cite class="ltx_cite ltx_citemacro_citep">(SÃ¡rÃ¡ndi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib42" title="">2023</a>)</cite> trained on large 3D datasets,
Following the method of <cite class="ltx_cite ltx_citemacro_cite">Lin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>, we apply temporal smoothing and enforce 3D bone length constraints during triangulation, improving the stability and consistency of the keypoint estimations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">â¸ <span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Incorporating Additional Modalities: </span>
A comprehensive understanding of human motion benefits from the inclusion of diverse modalities such as RGB and depth data.
To enrich MotionBase, we provide annotations for these additional modalities.
Furthermore, MotionBase includes videos featuring multi-person scenarios, with each motion sequence grounded in its corresponding video through object-level bounding boxes.
Although this paper primarily focuses on the text-to-motion task, these additional modalities open avenues for future research in other areas.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">â¹ <span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Local-Global Pose Estimation: </span>
We begin by registering the body model SMPL-XÂ <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib35" title="">2019</a>)</cite> for each frame in MotionBase, which leverages keypoints based on progressive learning-based mesh fitting methodÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>.
Specifically, we predict SMPL-X parameters using a pretrained body mesh recovery method, OSXÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib25" title="">2023</a>)</cite>, followed by iterative optimization to fit the parameters to the target 2D and 3D joint positions.
After fitting, we apply global motion optimization based on <cite class="ltx_cite ltx_citemacro_cite">Yuan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib59" title="">2022</a>)</cite> to refine both global motions and camera poses simultaneously, ensuring alignment with the video evidence.
Finally, for motions with noisy or occluded input data, we reconstruct complete and plausible motions using RoHMÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib62" title="">2024a</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">âº <span class="ltx_text ltx_font_bold" id="S3.p6.1.1">Hierarchical Motion Descriptions: </span>
Existing motion benchmarks face inherent limitations in their text descriptions.
Previous studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> typically use a single sentence to describe whole-body motions, neglecting finer details of individual body parts, such as the arms or legs.
This approach restricts the ability of motion generation models to perform more nuanced body comprehension and flexible part-level motion control (e.g., raising only the left arm).
Moreover, the richness of text labels often varies across different motions; for example, a large portion of the Motion-X dataset provides only action labels.
In contrast, MotionBase offers hierarchical textual annotations for each video.
We carefully design a prompt format and use Gemini-1.5-proÂ <cite class="ltx_cite ltx_citemacro_citep">(Reid etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib41" title="">2024</a>)</cite> to generate detailed descriptions for individual body parts (e.g., left arm, right leg), assigning a dedicated sentence to each.
Additionally, we summarize the overall body movement in a paragraph containing 1â€“3 sentences, providing a more comprehensive description of the motion.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="512" id="S3.F2.g1" src="x2.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples from <span class="ltx_text ltx_font_bold" id="S3.F2.2.1">MotionBase</span>, which encompasses a diverse range of human motions, including both long-term clips and static snapshots. It features various scenes, ranging from outdoor environments to indoor settings, and includes both clean, single-person scenarios as well as crowded, multi-person scenes. Additionally, MotionBase comprises a mix of real-world data and synthetic data generated by game engines. For more details about MotionBase, please refer to AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1" title="Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Scaling up Large Motion Model</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overall Architecture</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.14">Similar to previous LLM-based multimodal models, we treat motion as a foreign language.
The overall framework is presented in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A2.F11" title="Figure 11 â€£ Appendix B Additional Overview of Model Architecture â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">11</span></a> in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A2" title="Appendix B Additional Overview of Model Architecture â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">B</span></a>.
Our large motion model, built on a pre-trained LLM, functions as a generative model that connects a motion tokenizer with the LLM backbone <math alttext="\Theta" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S4.SS1.p1.1.m1.1.1.cmml">Î˜</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">Î˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">roman_Î˜</annotation></semantics></math>.
The motion tokenizer encodes raw motion clip features <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">â„³</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">â„³</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">caligraphic_M</annotation></semantics></math> into token embeddings <math alttext="\mathcal{V}=\{v_{1},v_{2},...,v_{n}\}\in\mathbbm{R}^{n\times d}" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.4"><semantics id="S4.SS1.p1.3.m3.4a"><mrow id="S4.SS1.p1.3.m3.4.4" xref="S4.SS1.p1.3.m3.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.3.m3.4.4.5" xref="S4.SS1.p1.3.m3.4.4.5.cmml">ğ’±</mi><mo id="S4.SS1.p1.3.m3.4.4.6" xref="S4.SS1.p1.3.m3.4.4.6.cmml">=</mo><mrow id="S4.SS1.p1.3.m3.4.4.3.3" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml"><mo id="S4.SS1.p1.3.m3.4.4.3.3.4" stretchy="false" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">{</mo><msub id="S4.SS1.p1.3.m3.2.2.1.1.1" xref="S4.SS1.p1.3.m3.2.2.1.1.1.cmml"><mi id="S4.SS1.p1.3.m3.2.2.1.1.1.2" xref="S4.SS1.p1.3.m3.2.2.1.1.1.2.cmml">v</mi><mn id="S4.SS1.p1.3.m3.2.2.1.1.1.3" xref="S4.SS1.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p1.3.m3.4.4.3.3.5" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.3.m3.3.3.2.2.2" xref="S4.SS1.p1.3.m3.3.3.2.2.2.cmml"><mi id="S4.SS1.p1.3.m3.3.3.2.2.2.2" xref="S4.SS1.p1.3.m3.3.3.2.2.2.2.cmml">v</mi><mn id="S4.SS1.p1.3.m3.3.3.2.2.2.3" xref="S4.SS1.p1.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.3.m3.4.4.3.3.6" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><mi id="S4.SS1.p1.3.m3.1.1" mathvariant="normal" xref="S4.SS1.p1.3.m3.1.1.cmml">â€¦</mi><mo id="S4.SS1.p1.3.m3.4.4.3.3.7" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.3.m3.4.4.3.3.3" xref="S4.SS1.p1.3.m3.4.4.3.3.3.cmml"><mi id="S4.SS1.p1.3.m3.4.4.3.3.3.2" xref="S4.SS1.p1.3.m3.4.4.3.3.3.2.cmml">v</mi><mi id="S4.SS1.p1.3.m3.4.4.3.3.3.3" xref="S4.SS1.p1.3.m3.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.SS1.p1.3.m3.4.4.3.3.8" stretchy="false" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">}</mo></mrow><mo id="S4.SS1.p1.3.m3.4.4.7" xref="S4.SS1.p1.3.m3.4.4.7.cmml">âˆˆ</mo><msup id="S4.SS1.p1.3.m3.4.4.8" xref="S4.SS1.p1.3.m3.4.4.8.cmml"><mi id="S4.SS1.p1.3.m3.4.4.8.2" xref="S4.SS1.p1.3.m3.4.4.8.2.cmml">â„</mi><mrow id="S4.SS1.p1.3.m3.4.4.8.3" xref="S4.SS1.p1.3.m3.4.4.8.3.cmml"><mi id="S4.SS1.p1.3.m3.4.4.8.3.2" xref="S4.SS1.p1.3.m3.4.4.8.3.2.cmml">n</mi><mo id="S4.SS1.p1.3.m3.4.4.8.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.3.m3.4.4.8.3.1.cmml">Ã—</mo><mi id="S4.SS1.p1.3.m3.4.4.8.3.3" xref="S4.SS1.p1.3.m3.4.4.8.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.4b"><apply id="S4.SS1.p1.3.m3.4.4.cmml" xref="S4.SS1.p1.3.m3.4.4"><and id="S4.SS1.p1.3.m3.4.4a.cmml" xref="S4.SS1.p1.3.m3.4.4"></and><apply id="S4.SS1.p1.3.m3.4.4b.cmml" xref="S4.SS1.p1.3.m3.4.4"><eq id="S4.SS1.p1.3.m3.4.4.6.cmml" xref="S4.SS1.p1.3.m3.4.4.6"></eq><ci id="S4.SS1.p1.3.m3.4.4.5.cmml" xref="S4.SS1.p1.3.m3.4.4.5">ğ’±</ci><set id="S4.SS1.p1.3.m3.4.4.3.4.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3"><apply id="S4.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.2.2.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m3.2.2.1.1.1.2.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1.2">ğ‘£</ci><cn id="S4.SS1.p1.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.p1.3.m3.3.3.2.2.2.cmml" xref="S4.SS1.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.3.3.2.2.2.1.cmml" xref="S4.SS1.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.3.m3.3.3.2.2.2.2.cmml" xref="S4.SS1.p1.3.m3.3.3.2.2.2.2">ğ‘£</ci><cn id="S4.SS1.p1.3.m3.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS1.p1.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">â€¦</ci><apply id="S4.SS1.p1.3.m3.4.4.3.3.3.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.4.4.3.3.3.1.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.3.m3.4.4.3.3.3.2.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3.3.2">ğ‘£</ci><ci id="S4.SS1.p1.3.m3.4.4.3.3.3.3.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3.3.3">ğ‘›</ci></apply></set></apply><apply id="S4.SS1.p1.3.m3.4.4c.cmml" xref="S4.SS1.p1.3.m3.4.4"><in id="S4.SS1.p1.3.m3.4.4.7.cmml" xref="S4.SS1.p1.3.m3.4.4.7"></in><share href="https://arxiv.org/html/2410.03311v1#S4.SS1.p1.3.m3.4.4.3.cmml" id="S4.SS1.p1.3.m3.4.4d.cmml" xref="S4.SS1.p1.3.m3.4.4"></share><apply id="S4.SS1.p1.3.m3.4.4.8.cmml" xref="S4.SS1.p1.3.m3.4.4.8"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.4.4.8.1.cmml" xref="S4.SS1.p1.3.m3.4.4.8">superscript</csymbol><ci id="S4.SS1.p1.3.m3.4.4.8.2.cmml" xref="S4.SS1.p1.3.m3.4.4.8.2">â„</ci><apply id="S4.SS1.p1.3.m3.4.4.8.3.cmml" xref="S4.SS1.p1.3.m3.4.4.8.3"><times id="S4.SS1.p1.3.m3.4.4.8.3.1.cmml" xref="S4.SS1.p1.3.m3.4.4.8.3.1"></times><ci id="S4.SS1.p1.3.m3.4.4.8.3.2.cmml" xref="S4.SS1.p1.3.m3.4.4.8.3.2">ğ‘›</ci><ci id="S4.SS1.p1.3.m3.4.4.8.3.3.cmml" xref="S4.SS1.p1.3.m3.4.4.8.3.3">ğ‘‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.4c">\mathcal{V}=\{v_{1},v_{2},...,v_{n}\}\in\mathbbm{R}^{n\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.4d">caligraphic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_n</annotation></semantics></math> denotes the number of motion tokens and <math alttext="d" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m5.1"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.1d">italic_d</annotation></semantics></math> represents the dimensionality of each token.
To integrate motion tokens into the LLM framework, we incorporate <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.p1.6.m6.1"><semantics id="S4.SS1.p1.6.m6.1a"><mi id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.6.m6.1d">italic_K</annotation></semantics></math> discrete codes in the motion codebook as additional vocabulary for the LLM.
Additionally, we introduce two special tokens, <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.p1.7.m7.1"><semantics id="S4.SS1.p1.7.m7.1a"><mo id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><lt id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.7.m7.1d">&lt;</annotation></semantics></math>mot<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.p1.8.m8.1"><semantics id="S4.SS1.p1.8.m8.1a"><mo id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><gt id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.8.m8.1d">&gt;</annotation></semantics></math> and <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.p1.9.m9.1"><semantics id="S4.SS1.p1.9.m9.1a"><mo id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.1b"><lt id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.9.m9.1d">&lt;</annotation></semantics></math>/mot<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.p1.10.m10.1"><semantics id="S4.SS1.p1.10.m10.1a"><mo id="S4.SS1.p1.10.m10.1.1" xref="S4.SS1.p1.10.m10.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.10.m10.1b"><gt id="S4.SS1.p1.10.m10.1.1.cmml" xref="S4.SS1.p1.10.m10.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.10.m10.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.10.m10.1d">&gt;</annotation></semantics></math>, to signify the start and end of motion sequences within the input/output streams.
The LLM backbone <math alttext="\Theta" class="ltx_Math" display="inline" id="S4.SS1.p1.11.m11.1"><semantics id="S4.SS1.p1.11.m11.1a"><mi id="S4.SS1.p1.11.m11.1.1" mathvariant="normal" xref="S4.SS1.p1.11.m11.1.1.cmml">Î˜</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.11.m11.1b"><ci id="S4.SS1.p1.11.m11.1.1.cmml" xref="S4.SS1.p1.11.m11.1.1">Î˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.11.m11.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.11.m11.1d">roman_Î˜</annotation></semantics></math> is built on a decoder-only architecture using causal transformers.
The model generates outputs <math alttext="\mathcal{Y}=\{y_{1},y_{2},...,y_{m}\}" class="ltx_Math" display="inline" id="S4.SS1.p1.12.m12.4"><semantics id="S4.SS1.p1.12.m12.4a"><mrow id="S4.SS1.p1.12.m12.4.4" xref="S4.SS1.p1.12.m12.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.12.m12.4.4.5" xref="S4.SS1.p1.12.m12.4.4.5.cmml">ğ’´</mi><mo id="S4.SS1.p1.12.m12.4.4.4" xref="S4.SS1.p1.12.m12.4.4.4.cmml">=</mo><mrow id="S4.SS1.p1.12.m12.4.4.3.3" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml"><mo id="S4.SS1.p1.12.m12.4.4.3.3.4" stretchy="false" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">{</mo><msub id="S4.SS1.p1.12.m12.2.2.1.1.1" xref="S4.SS1.p1.12.m12.2.2.1.1.1.cmml"><mi id="S4.SS1.p1.12.m12.2.2.1.1.1.2" xref="S4.SS1.p1.12.m12.2.2.1.1.1.2.cmml">y</mi><mn id="S4.SS1.p1.12.m12.2.2.1.1.1.3" xref="S4.SS1.p1.12.m12.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p1.12.m12.4.4.3.3.5" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.12.m12.3.3.2.2.2" xref="S4.SS1.p1.12.m12.3.3.2.2.2.cmml"><mi id="S4.SS1.p1.12.m12.3.3.2.2.2.2" xref="S4.SS1.p1.12.m12.3.3.2.2.2.2.cmml">y</mi><mn id="S4.SS1.p1.12.m12.3.3.2.2.2.3" xref="S4.SS1.p1.12.m12.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.12.m12.4.4.3.3.6" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">,</mo><mi id="S4.SS1.p1.12.m12.1.1" mathvariant="normal" xref="S4.SS1.p1.12.m12.1.1.cmml">â€¦</mi><mo id="S4.SS1.p1.12.m12.4.4.3.3.7" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.12.m12.4.4.3.3.3" xref="S4.SS1.p1.12.m12.4.4.3.3.3.cmml"><mi id="S4.SS1.p1.12.m12.4.4.3.3.3.2" xref="S4.SS1.p1.12.m12.4.4.3.3.3.2.cmml">y</mi><mi id="S4.SS1.p1.12.m12.4.4.3.3.3.3" xref="S4.SS1.p1.12.m12.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S4.SS1.p1.12.m12.4.4.3.3.8" stretchy="false" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.12.m12.4b"><apply id="S4.SS1.p1.12.m12.4.4.cmml" xref="S4.SS1.p1.12.m12.4.4"><eq id="S4.SS1.p1.12.m12.4.4.4.cmml" xref="S4.SS1.p1.12.m12.4.4.4"></eq><ci id="S4.SS1.p1.12.m12.4.4.5.cmml" xref="S4.SS1.p1.12.m12.4.4.5">ğ’´</ci><set id="S4.SS1.p1.12.m12.4.4.3.4.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3"><apply id="S4.SS1.p1.12.m12.2.2.1.1.1.cmml" xref="S4.SS1.p1.12.m12.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.12.m12.2.2.1.1.1.1.cmml" xref="S4.SS1.p1.12.m12.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.12.m12.2.2.1.1.1.2.cmml" xref="S4.SS1.p1.12.m12.2.2.1.1.1.2">ğ‘¦</ci><cn id="S4.SS1.p1.12.m12.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.12.m12.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.p1.12.m12.3.3.2.2.2.cmml" xref="S4.SS1.p1.12.m12.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.12.m12.3.3.2.2.2.1.cmml" xref="S4.SS1.p1.12.m12.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.12.m12.3.3.2.2.2.2.cmml" xref="S4.SS1.p1.12.m12.3.3.2.2.2.2">ğ‘¦</ci><cn id="S4.SS1.p1.12.m12.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS1.p1.12.m12.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p1.12.m12.1.1.cmml" xref="S4.SS1.p1.12.m12.1.1">â€¦</ci><apply id="S4.SS1.p1.12.m12.4.4.3.3.3.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.12.m12.4.4.3.3.3.1.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.12.m12.4.4.3.3.3.2.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3.3.2">ğ‘¦</ci><ci id="S4.SS1.p1.12.m12.4.4.3.3.3.3.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3.3.3">ğ‘š</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.12.m12.4c">\mathcal{Y}=\{y_{1},y_{2},...,y_{m}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.12.m12.4d">caligraphic_Y = { italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }</annotation></semantics></math> in an auto-regressive manner, where <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S4.SS1.p1.13.m13.1"><semantics id="S4.SS1.p1.13.m13.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.13.m13.1.1" xref="S4.SS1.p1.13.m13.1.1.cmml">ğ’´</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.13.m13.1b"><ci id="S4.SS1.p1.13.m13.1.1.cmml" xref="S4.SS1.p1.13.m13.1.1">ğ’´</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.13.m13.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.13.m13.1d">caligraphic_Y</annotation></semantics></math> corresponds to the generated motion sequence based on the provided motion-text input tokens.
In this work, each motion-text pair in the MotionBase dataset is framed as an instruction-following instance <math alttext="\{\mathcal{X}_{Q},\mathcal{X}_{M}\}" class="ltx_Math" display="inline" id="S4.SS1.p1.14.m14.2"><semantics id="S4.SS1.p1.14.m14.2a"><mrow id="S4.SS1.p1.14.m14.2.2.2" xref="S4.SS1.p1.14.m14.2.2.3.cmml"><mo id="S4.SS1.p1.14.m14.2.2.2.3" stretchy="false" xref="S4.SS1.p1.14.m14.2.2.3.cmml">{</mo><msub id="S4.SS1.p1.14.m14.1.1.1.1" xref="S4.SS1.p1.14.m14.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.14.m14.1.1.1.1.2" xref="S4.SS1.p1.14.m14.1.1.1.1.2.cmml">ğ’³</mi><mi id="S4.SS1.p1.14.m14.1.1.1.1.3" xref="S4.SS1.p1.14.m14.1.1.1.1.3.cmml">Q</mi></msub><mo id="S4.SS1.p1.14.m14.2.2.2.4" xref="S4.SS1.p1.14.m14.2.2.3.cmml">,</mo><msub id="S4.SS1.p1.14.m14.2.2.2.2" xref="S4.SS1.p1.14.m14.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.14.m14.2.2.2.2.2" xref="S4.SS1.p1.14.m14.2.2.2.2.2.cmml">ğ’³</mi><mi id="S4.SS1.p1.14.m14.2.2.2.2.3" xref="S4.SS1.p1.14.m14.2.2.2.2.3.cmml">M</mi></msub><mo id="S4.SS1.p1.14.m14.2.2.2.5" stretchy="false" xref="S4.SS1.p1.14.m14.2.2.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.14.m14.2b"><set id="S4.SS1.p1.14.m14.2.2.3.cmml" xref="S4.SS1.p1.14.m14.2.2.2"><apply id="S4.SS1.p1.14.m14.1.1.1.1.cmml" xref="S4.SS1.p1.14.m14.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.14.m14.1.1.1.1.1.cmml" xref="S4.SS1.p1.14.m14.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.14.m14.1.1.1.1.2.cmml" xref="S4.SS1.p1.14.m14.1.1.1.1.2">ğ’³</ci><ci id="S4.SS1.p1.14.m14.1.1.1.1.3.cmml" xref="S4.SS1.p1.14.m14.1.1.1.1.3">ğ‘„</ci></apply><apply id="S4.SS1.p1.14.m14.2.2.2.2.cmml" xref="S4.SS1.p1.14.m14.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.14.m14.2.2.2.2.1.cmml" xref="S4.SS1.p1.14.m14.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.14.m14.2.2.2.2.2.cmml" xref="S4.SS1.p1.14.m14.2.2.2.2.2">ğ’³</ci><ci id="S4.SS1.p1.14.m14.2.2.2.2.3.cmml" xref="S4.SS1.p1.14.m14.2.2.2.2.3">ğ‘€</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.14.m14.2c">\{\mathcal{X}_{Q},\mathcal{X}_{M}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.14.m14.2d">{ caligraphic_X start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , caligraphic_X start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT }</annotation></semantics></math>, representing a question-answer interaction between the user and the motion model.
The entire instructional dataset adheres to this unified format.
To train our model, we optimize the negative log-likelihood over the predicted tokens which is defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(\Theta)=-\sum_{j=1}^{L}\log P_{\Theta}(y_{j}|desc,\hat{y}_{1:j-1})," class="ltx_Math" display="block" id="S4.E1.m1.2"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.1.1.3.2" xref="S4.E1.m1.2.2.1.1.3.2.cmml">â„’</mi><mo id="S4.E1.m1.2.2.1.1.3.1" xref="S4.E1.m1.2.2.1.1.3.1.cmml">â¢</mo><mrow id="S4.E1.m1.2.2.1.1.3.3.2" xref="S4.E1.m1.2.2.1.1.3.cmml"><mo id="S4.E1.m1.2.2.1.1.3.3.2.1" stretchy="false" xref="S4.E1.m1.2.2.1.1.3.cmml">(</mo><mi id="S4.E1.m1.1.1" mathvariant="normal" xref="S4.E1.m1.1.1.cmml">Î˜</mi><mo id="S4.E1.m1.2.2.1.1.3.3.2.2" stretchy="false" xref="S4.E1.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.cmml"><mo id="S4.E1.m1.2.2.1.1.1a" xref="S4.E1.m1.2.2.1.1.1.cmml">âˆ’</mo><mrow id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><munderover id="S4.E1.m1.2.2.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.2.cmml"><mo id="S4.E1.m1.2.2.1.1.1.1.2.2.2" movablelimits="false" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.2.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S4.E1.m1.2.2.1.1.1.1.2.2.3.1" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.2.2.1.1.1.1.2.2.3.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.2.2.1.1.1.1.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.3.cmml">L</mi></munderover><mrow id="S4.E1.m1.2.2.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.3.1" xref="S4.E1.m1.2.2.1.1.1.1.1.3.1.cmml">log</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.3a" lspace="0.167em" xref="S4.E1.m1.2.2.1.1.1.1.1.3.cmml">â¡</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.3.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml">P</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.3.2.3" mathvariant="normal" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml">Î˜</mi></msub></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml">y</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml">j</mi></msub><mo fence="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">d</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">e</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4.cmml">s</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1b" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5.cmml">c</mi></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml"><mover accent="true" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">y</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml">^</mo></mover><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">1</mn><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1" lspace="0.278em" rspace="0.278em" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">:</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.2.cmml">j</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.1.cmml">âˆ’</mo><mn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></mrow></msub></mrow></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1"><eq id="S4.E1.m1.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2"></eq><apply id="S4.E1.m1.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3"><times id="S4.E1.m1.2.2.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.3.1"></times><ci id="S4.E1.m1.2.2.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2">â„’</ci><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">Î˜</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"><minus id="S4.E1.m1.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1"></minus><apply id="S4.E1.m1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1"><apply id="S4.E1.m1.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3"><eq id="S4.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.2">ğ‘—</ci><cn id="S4.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.3">ğ¿</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2"></times><apply id="S4.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3"><log id="S4.E1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.1"></log><apply id="S4.E1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.2">ğ‘ƒ</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.3">Î˜</ci></apply></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2">ğ‘¦</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3">ğ‘—</ci></apply><list id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2"><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘‘</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘’</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4">ğ‘ </ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5">ğ‘</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2"><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.1">^</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.2">ğ‘¦</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3"><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1">:</ci><cn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" type="integer" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2">1</cn><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3"><minus id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.1"></minus><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.2">ğ‘—</ci><cn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.3.cmml" type="integer" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.3">1</cn></apply></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\mathcal{L}(\Theta)=-\sum_{j=1}^{L}\log P_{\Theta}(y_{j}|desc,\hat{y}_{1:j-1}),</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.2d">caligraphic_L ( roman_Î˜ ) = - âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT roman_log italic_P start_POSTSUBSCRIPT roman_Î˜ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_d italic_e italic_s italic_c , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT 1 : italic_j - 1 end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p1.19">where <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S4.SS1.p1.15.m1.1"><semantics id="S4.SS1.p1.15.m1.1a"><mover accent="true" id="S4.SS1.p1.15.m1.1.1" xref="S4.SS1.p1.15.m1.1.1.cmml"><mi id="S4.SS1.p1.15.m1.1.1.2" xref="S4.SS1.p1.15.m1.1.1.2.cmml">y</mi><mo id="S4.SS1.p1.15.m1.1.1.1" xref="S4.SS1.p1.15.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.15.m1.1b"><apply id="S4.SS1.p1.15.m1.1.1.cmml" xref="S4.SS1.p1.15.m1.1.1"><ci id="S4.SS1.p1.15.m1.1.1.1.cmml" xref="S4.SS1.p1.15.m1.1.1.1">^</ci><ci id="S4.SS1.p1.15.m1.1.1.2.cmml" xref="S4.SS1.p1.15.m1.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.15.m1.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.15.m1.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p1.16.m2.1"><semantics id="S4.SS1.p1.16.m2.1a"><mi id="S4.SS1.p1.16.m2.1.1" xref="S4.SS1.p1.16.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.16.m2.1b"><ci id="S4.SS1.p1.16.m2.1.1.cmml" xref="S4.SS1.p1.16.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.16.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.16.m2.1d">italic_y</annotation></semantics></math> denote the input and target token sequences, respectively. <math alttext="\Theta" class="ltx_Math" display="inline" id="S4.SS1.p1.17.m3.1"><semantics id="S4.SS1.p1.17.m3.1a"><mi id="S4.SS1.p1.17.m3.1.1" mathvariant="normal" xref="S4.SS1.p1.17.m3.1.1.cmml">Î˜</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.17.m3.1b"><ci id="S4.SS1.p1.17.m3.1.1.cmml" xref="S4.SS1.p1.17.m3.1.1">Î˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.17.m3.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.17.m3.1d">roman_Î˜</annotation></semantics></math> represents the model parameters, and <math alttext="L" class="ltx_Math" display="inline" id="S4.SS1.p1.18.m4.1"><semantics id="S4.SS1.p1.18.m4.1a"><mi id="S4.SS1.p1.18.m4.1.1" xref="S4.SS1.p1.18.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.18.m4.1b"><ci id="S4.SS1.p1.18.m4.1.1.cmml" xref="S4.SS1.p1.18.m4.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.18.m4.1c">L</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.18.m4.1d">italic_L</annotation></semantics></math> is the length of the target sequence.
The input description, <math alttext="desc" class="ltx_Math" display="inline" id="S4.SS1.p1.19.m5.1"><semantics id="S4.SS1.p1.19.m5.1a"><mrow id="S4.SS1.p1.19.m5.1.1" xref="S4.SS1.p1.19.m5.1.1.cmml"><mi id="S4.SS1.p1.19.m5.1.1.2" xref="S4.SS1.p1.19.m5.1.1.2.cmml">d</mi><mo id="S4.SS1.p1.19.m5.1.1.1" xref="S4.SS1.p1.19.m5.1.1.1.cmml">â¢</mo><mi id="S4.SS1.p1.19.m5.1.1.3" xref="S4.SS1.p1.19.m5.1.1.3.cmml">e</mi><mo id="S4.SS1.p1.19.m5.1.1.1a" xref="S4.SS1.p1.19.m5.1.1.1.cmml">â¢</mo><mi id="S4.SS1.p1.19.m5.1.1.4" xref="S4.SS1.p1.19.m5.1.1.4.cmml">s</mi><mo id="S4.SS1.p1.19.m5.1.1.1b" xref="S4.SS1.p1.19.m5.1.1.1.cmml">â¢</mo><mi id="S4.SS1.p1.19.m5.1.1.5" xref="S4.SS1.p1.19.m5.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.19.m5.1b"><apply id="S4.SS1.p1.19.m5.1.1.cmml" xref="S4.SS1.p1.19.m5.1.1"><times id="S4.SS1.p1.19.m5.1.1.1.cmml" xref="S4.SS1.p1.19.m5.1.1.1"></times><ci id="S4.SS1.p1.19.m5.1.1.2.cmml" xref="S4.SS1.p1.19.m5.1.1.2">ğ‘‘</ci><ci id="S4.SS1.p1.19.m5.1.1.3.cmml" xref="S4.SS1.p1.19.m5.1.1.3">ğ‘’</ci><ci id="S4.SS1.p1.19.m5.1.1.4.cmml" xref="S4.SS1.p1.19.m5.1.1.4">ğ‘ </ci><ci id="S4.SS1.p1.19.m5.1.1.5.cmml" xref="S4.SS1.p1.19.m5.1.1.5">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.19.m5.1c">desc</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.19.m5.1d">italic_d italic_e italic_s italic_c</annotation></semantics></math>, can be empty depending on the instruction provided.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>2D Lookup-free Motion Quantization</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.3">Similar to visual tokenization, motion tokenization is a process that compresses motion signals into a series of discrete tokens, typically involving an encoder <math alttext="\mathbbm{E}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">ğ”¼</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ”¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\mathbbm{E}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">blackboard_E</annotation></semantics></math>, a decoder <math alttext="\mathbbm{D}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">ğ”»</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">ğ”»</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\mathbbm{D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">blackboard_D</annotation></semantics></math> and a codebook <math alttext="\mathbbm{C}" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">â„‚</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">â„‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\mathbbm{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">blackboard_C</annotation></semantics></math>.
We propose a 2D lookup-free quantization method as a key component for building large motion models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.7"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.7.1">2D Motion Quantization. </span>
Traditional motion quantizers use 1D embeddings to represent motion at each timestamp, which inevitably results in the loss of crucial information.
Furthermore, this approach limits the quantizerâ€™s ability to generate and interpret part-level motions.
To address these limitations, we treat the motion sequence <math alttext="\mathcal{M}=\{m_{1},m_{2},...,m_{T}\}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.4"><semantics id="S4.SS2.p2.1.m1.4a"><mrow id="S4.SS2.p2.1.m1.4.4" xref="S4.SS2.p2.1.m1.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.1.m1.4.4.5" xref="S4.SS2.p2.1.m1.4.4.5.cmml">â„³</mi><mo id="S4.SS2.p2.1.m1.4.4.4" xref="S4.SS2.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS2.p2.1.m1.4.4.3.3" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml"><mo id="S4.SS2.p2.1.m1.4.4.3.3.4" stretchy="false" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">{</mo><msub id="S4.SS2.p2.1.m1.2.2.1.1.1" xref="S4.SS2.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.2.2.1.1.1.2" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2.cmml">m</mi><mn id="S4.SS2.p2.1.m1.2.2.1.1.1.3" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.5" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p2.1.m1.3.3.2.2.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.p2.1.m1.3.3.2.2.2.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2.cmml">m</mi><mn id="S4.SS2.p2.1.m1.3.3.2.2.2.3" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.6" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.SS2.p2.1.m1.1.1" mathvariant="normal" xref="S4.SS2.p2.1.m1.1.1.cmml">â€¦</mi><mo id="S4.SS2.p2.1.m1.4.4.3.3.7" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p2.1.m1.4.4.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.2" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2.cmml">m</mi><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.8" stretchy="false" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.4b"><apply id="S4.SS2.p2.1.m1.4.4.cmml" xref="S4.SS2.p2.1.m1.4.4"><eq id="S4.SS2.p2.1.m1.4.4.4.cmml" xref="S4.SS2.p2.1.m1.4.4.4"></eq><ci id="S4.SS2.p2.1.m1.4.4.5.cmml" xref="S4.SS2.p2.1.m1.4.4.5">â„³</ci><set id="S4.SS2.p2.1.m1.4.4.3.4.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3"><apply id="S4.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2">ğ‘š</ci><cn id="S4.SS2.p2.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p2.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p2.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2">ğ‘š</ci><cn id="S4.SS2.p2.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">â€¦</ci><apply id="S4.SS2.p2.1.m1.4.4.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2">ğ‘š</ci><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3">ğ‘‡</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.4c">\mathcal{M}=\{m_{1},m_{2},...,m_{T}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.4d">caligraphic_M = { italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_m start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }</annotation></semantics></math> as a single-channel image, representing each motin sequence as <math alttext="\mathcal{M}\in\mathbbm{R}^{T\times D\times 1}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">â„³</mi><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.2" xref="S4.SS2.p2.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S4.SS2.p2.2.m2.1.1.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.3.2" xref="S4.SS2.p2.2.m2.1.1.3.3.2.cmml">T</mi><mo id="S4.SS2.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p2.2.m2.1.1.3.3.1.cmml">Ã—</mo><mi id="S4.SS2.p2.2.m2.1.1.3.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.3.cmml">D</mi><mo id="S4.SS2.p2.2.m2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p2.2.m2.1.1.3.3.1.cmml">Ã—</mo><mn id="S4.SS2.p2.2.m2.1.1.3.3.4" xref="S4.SS2.p2.2.m2.1.1.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><in id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></in><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">â„³</ci><apply id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.2">â„</ci><apply id="S4.SS2.p2.2.m2.1.1.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3"><times id="S4.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3.1"></times><ci id="S4.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3.2">ğ‘‡</ci><ci id="S4.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3.3">ğ·</ci><cn id="S4.SS2.p2.2.m2.1.1.3.3.4.cmml" type="integer" xref="S4.SS2.p2.2.m2.1.1.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\mathcal{M}\in\mathbbm{R}^{T\times D\times 1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">caligraphic_M âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_T Ã— italic_D Ã— 1 end_POSTSUPERSCRIPT</annotation></semantics></math>.
Each motion embedding <math alttext="m_{i}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><msub id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">m</mi><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">ğ‘š</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">m_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is divided into <math alttext="P" class="ltx_Math" display="inline" id="S4.SS2.p2.4.m4.1"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.4.m4.1d">italic_P</annotation></semantics></math> components, capturing distinct features of motion, such as root orientation, joint rotation and foot contact.
Our motion encoder then converts <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S4.SS2.p2.5.m5.1"><semantics id="S4.SS2.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">â„³</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><ci id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">â„³</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.5.m5.1d">caligraphic_M</annotation></semantics></math> into a feature map <math alttext="\mathbbm{E}(\mathcal{M})\in\mathbbm{R}^{\lfloor T/\alpha\rfloor\times P\times d}" class="ltx_Math" display="inline" id="S4.SS2.p2.6.m6.2"><semantics id="S4.SS2.p2.6.m6.2a"><mrow id="S4.SS2.p2.6.m6.2.3" xref="S4.SS2.p2.6.m6.2.3.cmml"><mrow id="S4.SS2.p2.6.m6.2.3.2" xref="S4.SS2.p2.6.m6.2.3.2.cmml"><mi id="S4.SS2.p2.6.m6.2.3.2.2" xref="S4.SS2.p2.6.m6.2.3.2.2.cmml">ğ”¼</mi><mo id="S4.SS2.p2.6.m6.2.3.2.1" xref="S4.SS2.p2.6.m6.2.3.2.1.cmml">â¢</mo><mrow id="S4.SS2.p2.6.m6.2.3.2.3.2" xref="S4.SS2.p2.6.m6.2.3.2.cmml"><mo id="S4.SS2.p2.6.m6.2.3.2.3.2.1" stretchy="false" xref="S4.SS2.p2.6.m6.2.3.2.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.6.m6.2.2" xref="S4.SS2.p2.6.m6.2.2.cmml">â„³</mi><mo id="S4.SS2.p2.6.m6.2.3.2.3.2.2" stretchy="false" xref="S4.SS2.p2.6.m6.2.3.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p2.6.m6.2.3.1" xref="S4.SS2.p2.6.m6.2.3.1.cmml">âˆˆ</mo><msup id="S4.SS2.p2.6.m6.2.3.3" xref="S4.SS2.p2.6.m6.2.3.3.cmml"><mi id="S4.SS2.p2.6.m6.2.3.3.2" xref="S4.SS2.p2.6.m6.2.3.3.2.cmml">â„</mi><mrow id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml"><mrow id="S4.SS2.p2.6.m6.1.1.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.1.2.cmml"><mo id="S4.SS2.p2.6.m6.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p2.6.m6.1.1.1.1.2.1.cmml">âŒŠ</mo><mrow id="S4.SS2.p2.6.m6.1.1.1.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.1.1.1.1.2" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.2.cmml">T</mi><mo id="S4.SS2.p2.6.m6.1.1.1.1.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.1.cmml">/</mo><mi id="S4.SS2.p2.6.m6.1.1.1.1.1.1.3" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.3.cmml">Î±</mi></mrow><mo id="S4.SS2.p2.6.m6.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S4.SS2.p2.6.m6.1.1.1.1.2.1.cmml">âŒ‹</mo></mrow><mo id="S4.SS2.p2.6.m6.1.1.1.2" rspace="0.222em" xref="S4.SS2.p2.6.m6.1.1.1.2.cmml">Ã—</mo><mi id="S4.SS2.p2.6.m6.1.1.1.3" xref="S4.SS2.p2.6.m6.1.1.1.3.cmml">P</mi><mo id="S4.SS2.p2.6.m6.1.1.1.2a" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p2.6.m6.1.1.1.2.cmml">Ã—</mo><mi id="S4.SS2.p2.6.m6.1.1.1.4" xref="S4.SS2.p2.6.m6.1.1.1.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.2b"><apply id="S4.SS2.p2.6.m6.2.3.cmml" xref="S4.SS2.p2.6.m6.2.3"><in id="S4.SS2.p2.6.m6.2.3.1.cmml" xref="S4.SS2.p2.6.m6.2.3.1"></in><apply id="S4.SS2.p2.6.m6.2.3.2.cmml" xref="S4.SS2.p2.6.m6.2.3.2"><times id="S4.SS2.p2.6.m6.2.3.2.1.cmml" xref="S4.SS2.p2.6.m6.2.3.2.1"></times><ci id="S4.SS2.p2.6.m6.2.3.2.2.cmml" xref="S4.SS2.p2.6.m6.2.3.2.2">ğ”¼</ci><ci id="S4.SS2.p2.6.m6.2.2.cmml" xref="S4.SS2.p2.6.m6.2.2">â„³</ci></apply><apply id="S4.SS2.p2.6.m6.2.3.3.cmml" xref="S4.SS2.p2.6.m6.2.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.6.m6.2.3.3.1.cmml" xref="S4.SS2.p2.6.m6.2.3.3">superscript</csymbol><ci id="S4.SS2.p2.6.m6.2.3.3.2.cmml" xref="S4.SS2.p2.6.m6.2.3.3.2">â„</ci><apply id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1"><times id="S4.SS2.p2.6.m6.1.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.1.2"></times><apply id="S4.SS2.p2.6.m6.1.1.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1"><floor id="S4.SS2.p2.6.m6.1.1.1.1.2.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.2"></floor><apply id="S4.SS2.p2.6.m6.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1"><divide id="S4.SS2.p2.6.m6.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.1"></divide><ci id="S4.SS2.p2.6.m6.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S4.SS2.p2.6.m6.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.3">ğ›¼</ci></apply></apply><ci id="S4.SS2.p2.6.m6.1.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.1.3">ğ‘ƒ</ci><ci id="S4.SS2.p2.6.m6.1.1.1.4.cmml" xref="S4.SS2.p2.6.m6.1.1.1.4">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.2c">\mathbbm{E}(\mathcal{M})\in\mathbbm{R}^{\lfloor T/\alpha\rfloor\times P\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.6.m6.2d">blackboard_E ( caligraphic_M ) âˆˆ blackboard_R start_POSTSUPERSCRIPT âŒŠ italic_T / italic_Î± âŒ‹ Ã— italic_P Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.p2.7.m7.1"><semantics id="S4.SS2.p2.7.m7.1a"><mi id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><ci id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.7.m7.1d">italic_Î±</annotation></semantics></math> denotes the temporal downsampling ratio.
This approach ensures that each body part is tokenized separately, allowing for more granular, part-level motion encoding and decoding.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.12"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.12.2">Lookup-Free Quantization. </span>
Traditional motion quantizers are often constrained by small codebook sizes, restricting their ability to capture the full diversity of human motion.
A common approach is to expand the motion vocabulary.
However, excessively enlarging the codebook can result in â€œcodebook collapseâ€, where only a small subset of tokens in the codebook is used, offering minimal performance improvements.
In some cases, an overly large vocabulary can even degrade the modelâ€™s overall performance.
To address this issue, a more effective alternative is to reduce the dimensionality of code embeddingsÂ <cite class="ltx_cite ltx_citemacro_citep">(Mentzer etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib32" title="">2023</a>)</cite>, which limits the representational capacity of individual tokens and encourages more efficient learning across a larger vocabulary.
Similar to <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib58" title="">2023</a>)</cite>, we reduce the embedding dimension of the codebook to zero by replacing the codebook <math alttext="\mathbbm{C}\in\mathcal{R}^{K\times d}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">â„‚</mi><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.1.m1.1.1.3.2" xref="S4.SS2.p3.1.m1.1.1.3.2.cmml">â„›</mi><mrow id="S4.SS2.p3.1.m1.1.1.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.3.3.2" xref="S4.SS2.p3.1.m1.1.1.3.3.2.cmml">K</mi><mo id="S4.SS2.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p3.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S4.SS2.p3.1.m1.1.1.3.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><in id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></in><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">â„‚</ci><apply id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.2">â„›</ci><apply id="S4.SS2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3"><times id="S4.SS2.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.1"></times><ci id="S4.SS2.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.2">ğ¾</ci><ci id="S4.SS2.p3.1.m1.1.1.3.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathbbm{C}\in\mathcal{R}^{K\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">blackboard_C âˆˆ caligraphic_R start_POSTSUPERSCRIPT italic_K Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> with an integer set <math alttext="\mathbbm{C}" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">â„‚</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">â„‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\mathbbm{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">blackboard_C</annotation></semantics></math> with <math alttext="|\mathbbm{C}|=K" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mrow id="S4.SS2.p3.3.m3.1.2" xref="S4.SS2.p3.3.m3.1.2.cmml"><mrow id="S4.SS2.p3.3.m3.1.2.2.2" xref="S4.SS2.p3.3.m3.1.2.2.1.cmml"><mo id="S4.SS2.p3.3.m3.1.2.2.2.1" stretchy="false" xref="S4.SS2.p3.3.m3.1.2.2.1.1.cmml">|</mo><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">â„‚</mi><mo id="S4.SS2.p3.3.m3.1.2.2.2.2" stretchy="false" xref="S4.SS2.p3.3.m3.1.2.2.1.1.cmml">|</mo></mrow><mo id="S4.SS2.p3.3.m3.1.2.1" xref="S4.SS2.p3.3.m3.1.2.1.cmml">=</mo><mi id="S4.SS2.p3.3.m3.1.2.3" xref="S4.SS2.p3.3.m3.1.2.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.2.cmml" xref="S4.SS2.p3.3.m3.1.2"><eq id="S4.SS2.p3.3.m3.1.2.1.cmml" xref="S4.SS2.p3.3.m3.1.2.1"></eq><apply id="S4.SS2.p3.3.m3.1.2.2.1.cmml" xref="S4.SS2.p3.3.m3.1.2.2.2"><abs id="S4.SS2.p3.3.m3.1.2.2.1.1.cmml" xref="S4.SS2.p3.3.m3.1.2.2.2.1"></abs><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">â„‚</ci></apply><ci id="S4.SS2.p3.3.m3.1.2.3.cmml" xref="S4.SS2.p3.3.m3.1.2.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">|\mathbbm{C}|=K</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">| blackboard_C | = italic_K</annotation></semantics></math>.
Specifically, <math alttext="\mathbbm{C}" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mi id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">â„‚</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><ci id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">â„‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">\mathbbm{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">blackboard_C</annotation></semantics></math> is the Cartesian product of single-dimensional variables <math alttext="\mathbbm{C}=" class="ltx_Math" display="inline" id="S4.SS2.p3.5.m5.1"><semantics id="S4.SS2.p3.5.m5.1a"><mrow id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml"><mi id="S4.SS2.p3.5.m5.1.1.2" xref="S4.SS2.p3.5.m5.1.1.2.cmml">â„‚</mi><mo id="S4.SS2.p3.5.m5.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.cmml">=</mo><mi id="S4.SS2.p3.5.m5.1.1.3" xref="S4.SS2.p3.5.m5.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1"><eq id="S4.SS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1"></eq><ci id="S4.SS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2">â„‚</ci><csymbol cd="latexml" id="S4.SS2.p3.5.m5.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">\mathbbm{C}=</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.5.m5.1d">blackboard_C =</annotation></semantics></math>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.p3.6.1" style="width:11.7pt;height:9.899999999999999pt;vertical-align:-1.2pt;"><span class="ltx_transformed_inner" style="transform:translate(1.9pt,-1.5pt) scale(1.5,1.5) ;">
<span class="ltx_p" id="S4.SS2.p3.6.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.p3.6.1.1.m1.1"><semantics id="S4.SS2.p3.6.1.1.m1.1a"><mo id="S4.SS2.p3.6.1.1.m1.1.1" xref="S4.SS2.p3.6.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.1.1.m1.1b"><times id="S4.SS2.p3.6.1.1.m1.1.1.cmml" xref="S4.SS2.p3.6.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.6.1.1.m1.1d">Ã—</annotation></semantics></math></span>
</span></span><math alttext="{}_{i=1}^{d}C_{i}" class="ltx_Math" display="inline" id="S4.SS2.p3.7.m6.1"><semantics id="S4.SS2.p3.7.m6.1a"><mmultiscripts id="S4.SS2.p3.7.m6.1.1" xref="S4.SS2.p3.7.m6.1.1.cmml"><mi id="S4.SS2.p3.7.m6.1.1.2.2.2" xref="S4.SS2.p3.7.m6.1.1.2.2.2.cmml">C</mi><mi id="S4.SS2.p3.7.m6.1.1.2.2.3" xref="S4.SS2.p3.7.m6.1.1.2.2.3.cmml">i</mi><mrow id="S4.SS2.p3.7.m6.1.1a" xref="S4.SS2.p3.7.m6.1.1.cmml"></mrow><mprescripts id="S4.SS2.p3.7.m6.1.1b" xref="S4.SS2.p3.7.m6.1.1.cmml"></mprescripts><mrow id="S4.SS2.p3.7.m6.1.1.3" xref="S4.SS2.p3.7.m6.1.1.3.cmml"><mi id="S4.SS2.p3.7.m6.1.1.3.2" xref="S4.SS2.p3.7.m6.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.7.m6.1.1.3.1" xref="S4.SS2.p3.7.m6.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p3.7.m6.1.1.3.3" xref="S4.SS2.p3.7.m6.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.7.m6.1.1.2.3" xref="S4.SS2.p3.7.m6.1.1.2.3.cmml">d</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m6.1b"><apply id="S4.SS2.p3.7.m6.1.1.cmml" xref="S4.SS2.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m6.1.1.1.cmml" xref="S4.SS2.p3.7.m6.1.1">subscript</csymbol><apply id="S4.SS2.p3.7.m6.1.1.2.cmml" xref="S4.SS2.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m6.1.1.2.1.cmml" xref="S4.SS2.p3.7.m6.1.1">superscript</csymbol><apply id="S4.SS2.p3.7.m6.1.1.2.2.cmml" xref="S4.SS2.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m6.1.1.2.2.1.cmml" xref="S4.SS2.p3.7.m6.1.1">subscript</csymbol><ci id="S4.SS2.p3.7.m6.1.1.2.2.2.cmml" xref="S4.SS2.p3.7.m6.1.1.2.2.2">ğ¶</ci><ci id="S4.SS2.p3.7.m6.1.1.2.2.3.cmml" xref="S4.SS2.p3.7.m6.1.1.2.2.3">ğ‘–</ci></apply><ci id="S4.SS2.p3.7.m6.1.1.2.3.cmml" xref="S4.SS2.p3.7.m6.1.1.2.3">ğ‘‘</ci></apply><apply id="S4.SS2.p3.7.m6.1.1.3.cmml" xref="S4.SS2.p3.7.m6.1.1.3"><eq id="S4.SS2.p3.7.m6.1.1.3.1.cmml" xref="S4.SS2.p3.7.m6.1.1.3.1"></eq><ci id="S4.SS2.p3.7.m6.1.1.3.2.cmml" xref="S4.SS2.p3.7.m6.1.1.3.2">ğ‘–</ci><cn id="S4.SS2.p3.7.m6.1.1.3.3.cmml" type="integer" xref="S4.SS2.p3.7.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m6.1c">{}_{i=1}^{d}C_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.7.m6.1d">start_FLOATSUBSCRIPT italic_i = 1 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="C_{i}=\{-1,1\}" class="ltx_Math" display="inline" id="S4.SS2.p3.8.m7.2"><semantics id="S4.SS2.p3.8.m7.2a"><mrow id="S4.SS2.p3.8.m7.2.2" xref="S4.SS2.p3.8.m7.2.2.cmml"><msub id="S4.SS2.p3.8.m7.2.2.3" xref="S4.SS2.p3.8.m7.2.2.3.cmml"><mi id="S4.SS2.p3.8.m7.2.2.3.2" xref="S4.SS2.p3.8.m7.2.2.3.2.cmml">C</mi><mi id="S4.SS2.p3.8.m7.2.2.3.3" xref="S4.SS2.p3.8.m7.2.2.3.3.cmml">i</mi></msub><mo id="S4.SS2.p3.8.m7.2.2.2" xref="S4.SS2.p3.8.m7.2.2.2.cmml">=</mo><mrow id="S4.SS2.p3.8.m7.2.2.1.1" xref="S4.SS2.p3.8.m7.2.2.1.2.cmml"><mo id="S4.SS2.p3.8.m7.2.2.1.1.2" stretchy="false" xref="S4.SS2.p3.8.m7.2.2.1.2.cmml">{</mo><mrow id="S4.SS2.p3.8.m7.2.2.1.1.1" xref="S4.SS2.p3.8.m7.2.2.1.1.1.cmml"><mo id="S4.SS2.p3.8.m7.2.2.1.1.1a" xref="S4.SS2.p3.8.m7.2.2.1.1.1.cmml">âˆ’</mo><mn id="S4.SS2.p3.8.m7.2.2.1.1.1.2" xref="S4.SS2.p3.8.m7.2.2.1.1.1.2.cmml">1</mn></mrow><mo id="S4.SS2.p3.8.m7.2.2.1.1.3" xref="S4.SS2.p3.8.m7.2.2.1.2.cmml">,</mo><mn id="S4.SS2.p3.8.m7.1.1" xref="S4.SS2.p3.8.m7.1.1.cmml">1</mn><mo id="S4.SS2.p3.8.m7.2.2.1.1.4" stretchy="false" xref="S4.SS2.p3.8.m7.2.2.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m7.2b"><apply id="S4.SS2.p3.8.m7.2.2.cmml" xref="S4.SS2.p3.8.m7.2.2"><eq id="S4.SS2.p3.8.m7.2.2.2.cmml" xref="S4.SS2.p3.8.m7.2.2.2"></eq><apply id="S4.SS2.p3.8.m7.2.2.3.cmml" xref="S4.SS2.p3.8.m7.2.2.3"><csymbol cd="ambiguous" id="S4.SS2.p3.8.m7.2.2.3.1.cmml" xref="S4.SS2.p3.8.m7.2.2.3">subscript</csymbol><ci id="S4.SS2.p3.8.m7.2.2.3.2.cmml" xref="S4.SS2.p3.8.m7.2.2.3.2">ğ¶</ci><ci id="S4.SS2.p3.8.m7.2.2.3.3.cmml" xref="S4.SS2.p3.8.m7.2.2.3.3">ğ‘–</ci></apply><set id="S4.SS2.p3.8.m7.2.2.1.2.cmml" xref="S4.SS2.p3.8.m7.2.2.1.1"><apply id="S4.SS2.p3.8.m7.2.2.1.1.1.cmml" xref="S4.SS2.p3.8.m7.2.2.1.1.1"><minus id="S4.SS2.p3.8.m7.2.2.1.1.1.1.cmml" xref="S4.SS2.p3.8.m7.2.2.1.1.1"></minus><cn id="S4.SS2.p3.8.m7.2.2.1.1.1.2.cmml" type="integer" xref="S4.SS2.p3.8.m7.2.2.1.1.1.2">1</cn></apply><cn id="S4.SS2.p3.8.m7.1.1.cmml" type="integer" xref="S4.SS2.p3.8.m7.1.1">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m7.2c">C_{i}=\{-1,1\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.8.m7.2d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { - 1 , 1 }</annotation></semantics></math> and <math alttext="d" class="ltx_Math" display="inline" id="S4.SS2.p3.9.m8.1"><semantics id="S4.SS2.p3.9.m8.1a"><mi id="S4.SS2.p3.9.m8.1.1" xref="S4.SS2.p3.9.m8.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.9.m8.1b"><ci id="S4.SS2.p3.9.m8.1.1.cmml" xref="S4.SS2.p3.9.m8.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.9.m8.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.9.m8.1d">italic_d</annotation></semantics></math> is equal to <math alttext="\log_{2}K" class="ltx_Math" display="inline" id="S4.SS2.p3.10.m9.1"><semantics id="S4.SS2.p3.10.m9.1a"><mrow id="S4.SS2.p3.10.m9.1.1" xref="S4.SS2.p3.10.m9.1.1.cmml"><msub id="S4.SS2.p3.10.m9.1.1.1" xref="S4.SS2.p3.10.m9.1.1.1.cmml"><mi id="S4.SS2.p3.10.m9.1.1.1.2" xref="S4.SS2.p3.10.m9.1.1.1.2.cmml">log</mi><mn id="S4.SS2.p3.10.m9.1.1.1.3" xref="S4.SS2.p3.10.m9.1.1.1.3.cmml">2</mn></msub><mo id="S4.SS2.p3.10.m9.1.1a" lspace="0.167em" xref="S4.SS2.p3.10.m9.1.1.cmml">â¡</mo><mi id="S4.SS2.p3.10.m9.1.1.2" xref="S4.SS2.p3.10.m9.1.1.2.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.10.m9.1b"><apply id="S4.SS2.p3.10.m9.1.1.cmml" xref="S4.SS2.p3.10.m9.1.1"><apply id="S4.SS2.p3.10.m9.1.1.1.cmml" xref="S4.SS2.p3.10.m9.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.10.m9.1.1.1.1.cmml" xref="S4.SS2.p3.10.m9.1.1.1">subscript</csymbol><log id="S4.SS2.p3.10.m9.1.1.1.2.cmml" xref="S4.SS2.p3.10.m9.1.1.1.2"></log><cn id="S4.SS2.p3.10.m9.1.1.1.3.cmml" type="integer" xref="S4.SS2.p3.10.m9.1.1.1.3">2</cn></apply><ci id="S4.SS2.p3.10.m9.1.1.2.cmml" xref="S4.SS2.p3.10.m9.1.1.2">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.10.m9.1c">\log_{2}K</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.10.m9.1d">roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_K</annotation></semantics></math>.
Given a feature vector <math alttext="z\in\mathbbm{R}^{d}" class="ltx_Math" display="inline" id="S4.SS2.p3.11.m10.1"><semantics id="S4.SS2.p3.11.m10.1a"><mrow id="S4.SS2.p3.11.m10.1.1" xref="S4.SS2.p3.11.m10.1.1.cmml"><mi id="S4.SS2.p3.11.m10.1.1.2" xref="S4.SS2.p3.11.m10.1.1.2.cmml">z</mi><mo id="S4.SS2.p3.11.m10.1.1.1" xref="S4.SS2.p3.11.m10.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS2.p3.11.m10.1.1.3" xref="S4.SS2.p3.11.m10.1.1.3.cmml"><mi id="S4.SS2.p3.11.m10.1.1.3.2" xref="S4.SS2.p3.11.m10.1.1.3.2.cmml">â„</mi><mi id="S4.SS2.p3.11.m10.1.1.3.3" xref="S4.SS2.p3.11.m10.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.11.m10.1b"><apply id="S4.SS2.p3.11.m10.1.1.cmml" xref="S4.SS2.p3.11.m10.1.1"><in id="S4.SS2.p3.11.m10.1.1.1.cmml" xref="S4.SS2.p3.11.m10.1.1.1"></in><ci id="S4.SS2.p3.11.m10.1.1.2.cmml" xref="S4.SS2.p3.11.m10.1.1.2">ğ‘§</ci><apply id="S4.SS2.p3.11.m10.1.1.3.cmml" xref="S4.SS2.p3.11.m10.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.11.m10.1.1.3.1.cmml" xref="S4.SS2.p3.11.m10.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.11.m10.1.1.3.2.cmml" xref="S4.SS2.p3.11.m10.1.1.3.2">â„</ci><ci id="S4.SS2.p3.11.m10.1.1.3.3.cmml" xref="S4.SS2.p3.11.m10.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.11.m10.1c">z\in\mathbbm{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.11.m10.1d">italic_z âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, our quantizer <math alttext="Q(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.12.m11.1"><semantics id="S4.SS2.p3.12.m11.1a"><mrow id="S4.SS2.p3.12.m11.1.2" xref="S4.SS2.p3.12.m11.1.2.cmml"><mi id="S4.SS2.p3.12.m11.1.2.2" xref="S4.SS2.p3.12.m11.1.2.2.cmml">Q</mi><mo id="S4.SS2.p3.12.m11.1.2.1" xref="S4.SS2.p3.12.m11.1.2.1.cmml">â¢</mo><mrow id="S4.SS2.p3.12.m11.1.2.3.2" xref="S4.SS2.p3.12.m11.1.2.cmml"><mo id="S4.SS2.p3.12.m11.1.2.3.2.1" stretchy="false" xref="S4.SS2.p3.12.m11.1.2.cmml">(</mo><mo id="S4.SS2.p3.12.m11.1.1" lspace="0em" rspace="0em" xref="S4.SS2.p3.12.m11.1.1.cmml">â‹…</mo><mo id="S4.SS2.p3.12.m11.1.2.3.2.2" stretchy="false" xref="S4.SS2.p3.12.m11.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.12.m11.1b"><apply id="S4.SS2.p3.12.m11.1.2.cmml" xref="S4.SS2.p3.12.m11.1.2"><times id="S4.SS2.p3.12.m11.1.2.1.cmml" xref="S4.SS2.p3.12.m11.1.2.1"></times><ci id="S4.SS2.p3.12.m11.1.2.2.cmml" xref="S4.SS2.p3.12.m11.1.2.2">ğ‘„</ci><ci id="S4.SS2.p3.12.m11.1.1.cmml" xref="S4.SS2.p3.12.m11.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.12.m11.1c">Q(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.12.m11.1d">italic_Q ( â‹… )</annotation></semantics></math> converts each dimension of the quantized representation into:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q(z_{i})=\operatorname*{arg\,min}\nolimits_{c_{ik}}||z_{i}-c_{ik}||=-\mathbbm{%
1}\{z_{i}\leq 0\}+\mathbbm{1}\{z_{i}&gt;0\}," class="ltx_Math" display="block" id="S4.E2.m1.1"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.3.cmml">Q</mi><mo id="S4.E2.m1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E2.m1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.1.1.1.1.6" xref="S4.E2.m1.1.1.1.1.6.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml"><msub id="S4.E2.m1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.3.cmml"><mrow id="S4.E2.m1.1.1.1.1.2.3.2" xref="S4.E2.m1.1.1.1.1.2.3.2.cmml"><mi id="S4.E2.m1.1.1.1.1.2.3.2.2" xref="S4.E2.m1.1.1.1.1.2.3.2.2.cmml">arg</mi><mo id="S4.E2.m1.1.1.1.1.2.3.2.1" lspace="0.170em" xref="S4.E2.m1.1.1.1.1.2.3.2.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.2.3.2.3" xref="S4.E2.m1.1.1.1.1.2.3.2.3.cmml">min</mi></mrow><msub id="S4.E2.m1.1.1.1.1.2.3.3" xref="S4.E2.m1.1.1.1.1.2.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.3.3.2" xref="S4.E2.m1.1.1.1.1.2.3.3.2.cmml">c</mi><mrow id="S4.E2.m1.1.1.1.1.2.3.3.3" xref="S4.E2.m1.1.1.1.1.2.3.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.3.3.3.2" xref="S4.E2.m1.1.1.1.1.2.3.3.3.2.cmml">i</mi><mo id="S4.E2.m1.1.1.1.1.2.3.3.3.1" xref="S4.E2.m1.1.1.1.1.2.3.3.3.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.2.3.3.3.3" xref="S4.E2.m1.1.1.1.1.2.3.3.3.3.cmml">k</mi></mrow></msub></msub><mo id="S4.E2.m1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.2.cmml">â¢</mo><mrow id="S4.E2.m1.1.1.1.1.2.1.1" xref="S4.E2.m1.1.1.1.1.2.1.2.cmml"><mo id="S4.E2.m1.1.1.1.1.2.1.1.2" stretchy="false" xref="S4.E2.m1.1.1.1.1.2.1.2.1.cmml">â€–</mo><mrow id="S4.E2.m1.1.1.1.1.2.1.1.1" xref="S4.E2.m1.1.1.1.1.2.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.2.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.2.cmml">z</mi><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.2.1.1.1.1" xref="S4.E2.m1.1.1.1.1.2.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E2.m1.1.1.1.1.2.1.1.1.3" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.2.cmml">c</mi><mrow id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.2" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.2.cmml">i</mi><mo id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.1" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.1.cmml">â¢</mo><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.3" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.3.cmml">k</mi></mrow></msub></mrow><mo id="S4.E2.m1.1.1.1.1.2.1.1.3" stretchy="false" xref="S4.E2.m1.1.1.1.1.2.1.2.1.cmml">â€–</mo></mrow></mrow><mo id="S4.E2.m1.1.1.1.1.7" xref="S4.E2.m1.1.1.1.1.7.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.4" xref="S4.E2.m1.1.1.1.1.4.cmml"><mrow id="S4.E2.m1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.3.1.cmml"><mo id="S4.E2.m1.1.1.1.1.3.1a" xref="S4.E2.m1.1.1.1.1.3.1.cmml">âˆ’</mo><mrow id="S4.E2.m1.1.1.1.1.3.1.1" xref="S4.E2.m1.1.1.1.1.3.1.1.cmml"><mn id="S4.E2.m1.1.1.1.1.3.1.1.3" xref="S4.E2.m1.1.1.1.1.3.1.1.3.cmml">ğŸ™</mn><mo id="S4.E2.m1.1.1.1.1.3.1.1.2" xref="S4.E2.m1.1.1.1.1.3.1.1.2.cmml">â¢</mo><mrow id="S4.E2.m1.1.1.1.1.3.1.1.1.1" xref="S4.E2.m1.1.1.1.1.3.1.1.1.2.cmml"><mo id="S4.E2.m1.1.1.1.1.3.1.1.1.1.2" stretchy="false" xref="S4.E2.m1.1.1.1.1.3.1.1.1.2.cmml">{</mo><mrow id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.2.cmml">z</mi><mi id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.1.cmml">â‰¤</mo><mn id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.3.cmml">0</mn></mrow><mo id="S4.E2.m1.1.1.1.1.3.1.1.1.1.3" stretchy="false" xref="S4.E2.m1.1.1.1.1.3.1.1.1.2.cmml">}</mo></mrow></mrow></mrow><mo id="S4.E2.m1.1.1.1.1.4.3" xref="S4.E2.m1.1.1.1.1.4.3.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.4.2" xref="S4.E2.m1.1.1.1.1.4.2.cmml"><mn id="S4.E2.m1.1.1.1.1.4.2.3" xref="S4.E2.m1.1.1.1.1.4.2.3.cmml">ğŸ™</mn><mo id="S4.E2.m1.1.1.1.1.4.2.2" xref="S4.E2.m1.1.1.1.1.4.2.2.cmml">â¢</mo><mrow id="S4.E2.m1.1.1.1.1.4.2.1.1" xref="S4.E2.m1.1.1.1.1.4.2.1.2.cmml"><mo id="S4.E2.m1.1.1.1.1.4.2.1.1.2" stretchy="false" xref="S4.E2.m1.1.1.1.1.4.2.1.2.cmml">{</mo><mrow id="S4.E2.m1.1.1.1.1.4.2.1.1.1" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.2.cmml">z</mi><mi id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.4.2.1.1.1.1" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.1.cmml">&gt;</mo><mn id="S4.E2.m1.1.1.1.1.4.2.1.1.1.3" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.3.cmml">0</mn></mrow><mo id="S4.E2.m1.1.1.1.1.4.2.1.1.3" stretchy="false" xref="S4.E2.m1.1.1.1.1.4.2.1.2.cmml">}</mo></mrow></mrow></mrow></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><and id="S4.E2.m1.1.1.1.1a.cmml" xref="S4.E2.m1.1.1.1"></and><apply id="S4.E2.m1.1.1.1.1b.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.6.cmml" xref="S4.E2.m1.1.1.1.1.6"></eq><apply id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"><times id="S4.E2.m1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.2"></times><ci id="S4.E2.m1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.3">ğ‘„</ci><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2">ğ‘§</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><times id="S4.E2.m1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2"></times><apply id="S4.E2.m1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3">subscript</csymbol><apply id="S4.E2.m1.1.1.1.1.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2"><times id="S4.E2.m1.1.1.1.1.2.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2.1"></times><ci id="S4.E2.m1.1.1.1.1.2.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2.2">arg</ci><ci id="S4.E2.m1.1.1.1.1.2.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2.3">min</ci></apply><apply id="S4.E2.m1.1.1.1.1.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.2">ğ‘</ci><apply id="S4.E2.m1.1.1.1.1.2.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.3"><times id="S4.E2.m1.1.1.1.1.2.3.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.2.3.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.3.2">ğ‘–</ci><ci id="S4.E2.m1.1.1.1.1.2.3.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.3.3">ğ‘˜</ci></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1.2.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1"><csymbol cd="latexml" id="S4.E2.m1.1.1.1.1.2.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.2">norm</csymbol><apply id="S4.E2.m1.1.1.1.1.2.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1"><minus id="S4.E2.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.1"></minus><apply id="S4.E2.m1.1.1.1.1.2.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.2">ğ‘§</ci><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.3">ğ‘–</ci></apply><apply id="S4.E2.m1.1.1.1.1.2.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.2">ğ‘</ci><apply id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3"><times id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.2">ğ‘–</ci><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.3">ğ‘˜</ci></apply></apply></apply></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1c.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.7.cmml" xref="S4.E2.m1.1.1.1.1.7"></eq><share href="https://arxiv.org/html/2410.03311v1#S4.E2.m1.1.1.1.1.2.cmml" id="S4.E2.m1.1.1.1.1d.cmml" xref="S4.E2.m1.1.1.1"></share><apply id="S4.E2.m1.1.1.1.1.4.cmml" xref="S4.E2.m1.1.1.1.1.4"><plus id="S4.E2.m1.1.1.1.1.4.3.cmml" xref="S4.E2.m1.1.1.1.1.4.3"></plus><apply id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1"><minus id="S4.E2.m1.1.1.1.1.3.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1"></minus><apply id="S4.E2.m1.1.1.1.1.3.1.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1"><times id="S4.E2.m1.1.1.1.1.3.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.2"></times><cn id="S4.E2.m1.1.1.1.1.3.1.1.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.3.1.1.3">1</cn><set id="S4.E2.m1.1.1.1.1.3.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1"><apply id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1"><leq id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.1"></leq><apply id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.2">ğ‘§</ci><ci id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.3">ğ‘–</ci></apply><cn id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.3">0</cn></apply></set></apply></apply><apply id="S4.E2.m1.1.1.1.1.4.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2"><times id="S4.E2.m1.1.1.1.1.4.2.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2.2"></times><cn id="S4.E2.m1.1.1.1.1.4.2.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.4.2.3">1</cn><set id="S4.E2.m1.1.1.1.1.4.2.1.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1"><apply id="S4.E2.m1.1.1.1.1.4.2.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1"><gt id="S4.E2.m1.1.1.1.1.4.2.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.1"></gt><apply id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.2">ğ‘§</ci><ci id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.3">ğ‘–</ci></apply><cn id="S4.E2.m1.1.1.1.1.4.2.1.1.1.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.3">0</cn></apply></set></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">Q(z_{i})=\operatorname*{arg\,min}\nolimits_{c_{ik}}||z_{i}-c_{ik}||=-\mathbbm{%
1}\{z_{i}\leq 0\}+\mathbbm{1}\{z_{i}&gt;0\},</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.1d">italic_Q ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT | | italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_c start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT | | = - blackboard_1 { italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰¤ 0 } + blackboard_1 { italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &gt; 0 } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p3.16">where <math alttext="c_{ij}" class="ltx_Math" display="inline" id="S4.SS2.p3.13.m1.1"><semantics id="S4.SS2.p3.13.m1.1a"><msub id="S4.SS2.p3.13.m1.1.1" xref="S4.SS2.p3.13.m1.1.1.cmml"><mi id="S4.SS2.p3.13.m1.1.1.2" xref="S4.SS2.p3.13.m1.1.1.2.cmml">c</mi><mrow id="S4.SS2.p3.13.m1.1.1.3" xref="S4.SS2.p3.13.m1.1.1.3.cmml"><mi id="S4.SS2.p3.13.m1.1.1.3.2" xref="S4.SS2.p3.13.m1.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.13.m1.1.1.3.1" xref="S4.SS2.p3.13.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS2.p3.13.m1.1.1.3.3" xref="S4.SS2.p3.13.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.13.m1.1b"><apply id="S4.SS2.p3.13.m1.1.1.cmml" xref="S4.SS2.p3.13.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.13.m1.1.1.1.cmml" xref="S4.SS2.p3.13.m1.1.1">subscript</csymbol><ci id="S4.SS2.p3.13.m1.1.1.2.cmml" xref="S4.SS2.p3.13.m1.1.1.2">ğ‘</ci><apply id="S4.SS2.p3.13.m1.1.1.3.cmml" xref="S4.SS2.p3.13.m1.1.1.3"><times id="S4.SS2.p3.13.m1.1.1.3.1.cmml" xref="S4.SS2.p3.13.m1.1.1.3.1"></times><ci id="S4.SS2.p3.13.m1.1.1.3.2.cmml" xref="S4.SS2.p3.13.m1.1.1.3.2">ğ‘–</ci><ci id="S4.SS2.p3.13.m1.1.1.3.3.cmml" xref="S4.SS2.p3.13.m1.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.13.m1.1c">c_{ij}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.13.m1.1d">italic_c start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> denotes the <math alttext="j" class="ltx_Math" display="inline" id="S4.SS2.p3.14.m2.1"><semantics id="S4.SS2.p3.14.m2.1a"><mi id="S4.SS2.p3.14.m2.1.1" xref="S4.SS2.p3.14.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.14.m2.1b"><ci id="S4.SS2.p3.14.m2.1.1.cmml" xref="S4.SS2.p3.14.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.14.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.14.m2.1d">italic_j</annotation></semantics></math>-th value of <math alttext="C_{i}" class="ltx_Math" display="inline" id="S4.SS2.p3.15.m3.1"><semantics id="S4.SS2.p3.15.m3.1a"><msub id="S4.SS2.p3.15.m3.1.1" xref="S4.SS2.p3.15.m3.1.1.cmml"><mi id="S4.SS2.p3.15.m3.1.1.2" xref="S4.SS2.p3.15.m3.1.1.2.cmml">C</mi><mi id="S4.SS2.p3.15.m3.1.1.3" xref="S4.SS2.p3.15.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.15.m3.1b"><apply id="S4.SS2.p3.15.m3.1.1.cmml" xref="S4.SS2.p3.15.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.15.m3.1.1.1.cmml" xref="S4.SS2.p3.15.m3.1.1">subscript</csymbol><ci id="S4.SS2.p3.15.m3.1.1.2.cmml" xref="S4.SS2.p3.15.m3.1.1.2">ğ¶</ci><ci id="S4.SS2.p3.15.m3.1.1.3.cmml" xref="S4.SS2.p3.15.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.15.m3.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.15.m3.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. The token index is computed as <math alttext="Index(z)=\sum_{i=1}^{d}2^{i-1}\mathbbm{1}\{z_{i}&gt;0\}" class="ltx_Math" display="inline" id="S4.SS2.p3.16.m4.2"><semantics id="S4.SS2.p3.16.m4.2a"><mrow id="S4.SS2.p3.16.m4.2.2" xref="S4.SS2.p3.16.m4.2.2.cmml"><mrow id="S4.SS2.p3.16.m4.2.2.3" xref="S4.SS2.p3.16.m4.2.2.3.cmml"><mi id="S4.SS2.p3.16.m4.2.2.3.2" xref="S4.SS2.p3.16.m4.2.2.3.2.cmml">I</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p3.16.m4.2.2.3.3" xref="S4.SS2.p3.16.m4.2.2.3.3.cmml">n</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1a" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p3.16.m4.2.2.3.4" xref="S4.SS2.p3.16.m4.2.2.3.4.cmml">d</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1b" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p3.16.m4.2.2.3.5" xref="S4.SS2.p3.16.m4.2.2.3.5.cmml">e</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1c" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p3.16.m4.2.2.3.6" xref="S4.SS2.p3.16.m4.2.2.3.6.cmml">x</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1d" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">â¢</mo><mrow id="S4.SS2.p3.16.m4.2.2.3.7.2" xref="S4.SS2.p3.16.m4.2.2.3.cmml"><mo id="S4.SS2.p3.16.m4.2.2.3.7.2.1" stretchy="false" xref="S4.SS2.p3.16.m4.2.2.3.cmml">(</mo><mi id="S4.SS2.p3.16.m4.1.1" xref="S4.SS2.p3.16.m4.1.1.cmml">z</mi><mo id="S4.SS2.p3.16.m4.2.2.3.7.2.2" stretchy="false" xref="S4.SS2.p3.16.m4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p3.16.m4.2.2.2" rspace="0.111em" xref="S4.SS2.p3.16.m4.2.2.2.cmml">=</mo><mrow id="S4.SS2.p3.16.m4.2.2.1" xref="S4.SS2.p3.16.m4.2.2.1.cmml"><msubsup id="S4.SS2.p3.16.m4.2.2.1.2" xref="S4.SS2.p3.16.m4.2.2.1.2.cmml"><mo id="S4.SS2.p3.16.m4.2.2.1.2.2.2" xref="S4.SS2.p3.16.m4.2.2.1.2.2.2.cmml">âˆ‘</mo><mrow id="S4.SS2.p3.16.m4.2.2.1.2.2.3" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.cmml"><mi id="S4.SS2.p3.16.m4.2.2.1.2.2.3.2" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.2.cmml">i</mi><mo id="S4.SS2.p3.16.m4.2.2.1.2.2.3.1" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.1.cmml">=</mo><mn id="S4.SS2.p3.16.m4.2.2.1.2.2.3.3" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.16.m4.2.2.1.2.3" xref="S4.SS2.p3.16.m4.2.2.1.2.3.cmml">d</mi></msubsup><mrow id="S4.SS2.p3.16.m4.2.2.1.1" xref="S4.SS2.p3.16.m4.2.2.1.1.cmml"><msup id="S4.SS2.p3.16.m4.2.2.1.1.3" xref="S4.SS2.p3.16.m4.2.2.1.1.3.cmml"><mn id="S4.SS2.p3.16.m4.2.2.1.1.3.2" xref="S4.SS2.p3.16.m4.2.2.1.1.3.2.cmml">2</mn><mrow id="S4.SS2.p3.16.m4.2.2.1.1.3.3" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.cmml"><mi id="S4.SS2.p3.16.m4.2.2.1.1.3.3.2" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.2.cmml">i</mi><mo id="S4.SS2.p3.16.m4.2.2.1.1.3.3.1" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.1.cmml">âˆ’</mo><mn id="S4.SS2.p3.16.m4.2.2.1.1.3.3.3" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.3.cmml">1</mn></mrow></msup><mo id="S4.SS2.p3.16.m4.2.2.1.1.2" xref="S4.SS2.p3.16.m4.2.2.1.1.2.cmml">â¢</mo><mn id="S4.SS2.p3.16.m4.2.2.1.1.4" xref="S4.SS2.p3.16.m4.2.2.1.1.4.cmml">ğŸ™</mn><mo id="S4.SS2.p3.16.m4.2.2.1.1.2a" xref="S4.SS2.p3.16.m4.2.2.1.1.2.cmml">â¢</mo><mrow id="S4.SS2.p3.16.m4.2.2.1.1.1.1" xref="S4.SS2.p3.16.m4.2.2.1.1.1.2.cmml"><mo id="S4.SS2.p3.16.m4.2.2.1.1.1.1.2" stretchy="false" xref="S4.SS2.p3.16.m4.2.2.1.1.1.2.cmml">{</mo><mrow id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.cmml"><msub id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.2" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.2.cmml">z</mi><mi id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.3" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.1" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.1.cmml">&gt;</mo><mn id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.3" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.3.cmml">0</mn></mrow><mo id="S4.SS2.p3.16.m4.2.2.1.1.1.1.3" stretchy="false" xref="S4.SS2.p3.16.m4.2.2.1.1.1.2.cmml">}</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.16.m4.2b"><apply id="S4.SS2.p3.16.m4.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2"><eq id="S4.SS2.p3.16.m4.2.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2.2"></eq><apply id="S4.SS2.p3.16.m4.2.2.3.cmml" xref="S4.SS2.p3.16.m4.2.2.3"><times id="S4.SS2.p3.16.m4.2.2.3.1.cmml" xref="S4.SS2.p3.16.m4.2.2.3.1"></times><ci id="S4.SS2.p3.16.m4.2.2.3.2.cmml" xref="S4.SS2.p3.16.m4.2.2.3.2">ğ¼</ci><ci id="S4.SS2.p3.16.m4.2.2.3.3.cmml" xref="S4.SS2.p3.16.m4.2.2.3.3">ğ‘›</ci><ci id="S4.SS2.p3.16.m4.2.2.3.4.cmml" xref="S4.SS2.p3.16.m4.2.2.3.4">ğ‘‘</ci><ci id="S4.SS2.p3.16.m4.2.2.3.5.cmml" xref="S4.SS2.p3.16.m4.2.2.3.5">ğ‘’</ci><ci id="S4.SS2.p3.16.m4.2.2.3.6.cmml" xref="S4.SS2.p3.16.m4.2.2.3.6">ğ‘¥</ci><ci id="S4.SS2.p3.16.m4.1.1.cmml" xref="S4.SS2.p3.16.m4.1.1">ğ‘§</ci></apply><apply id="S4.SS2.p3.16.m4.2.2.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1"><apply id="S4.SS2.p3.16.m4.2.2.1.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m4.2.2.1.2.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2">superscript</csymbol><apply id="S4.SS2.p3.16.m4.2.2.1.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m4.2.2.1.2.2.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2">subscript</csymbol><sum id="S4.SS2.p3.16.m4.2.2.1.2.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.2.2"></sum><apply id="S4.SS2.p3.16.m4.2.2.1.2.2.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3"><eq id="S4.SS2.p3.16.m4.2.2.1.2.2.3.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.1"></eq><ci id="S4.SS2.p3.16.m4.2.2.1.2.2.3.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.2">ğ‘–</ci><cn id="S4.SS2.p3.16.m4.2.2.1.2.2.3.3.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S4.SS2.p3.16.m4.2.2.1.2.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.3">ğ‘‘</ci></apply><apply id="S4.SS2.p3.16.m4.2.2.1.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1"><times id="S4.SS2.p3.16.m4.2.2.1.1.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.2"></times><apply id="S4.SS2.p3.16.m4.2.2.1.1.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m4.2.2.1.1.3.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3">superscript</csymbol><cn id="S4.SS2.p3.16.m4.2.2.1.1.3.2.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.1.3.2">2</cn><apply id="S4.SS2.p3.16.m4.2.2.1.1.3.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3"><minus id="S4.SS2.p3.16.m4.2.2.1.1.3.3.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.1"></minus><ci id="S4.SS2.p3.16.m4.2.2.1.1.3.3.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.2">ğ‘–</ci><cn id="S4.SS2.p3.16.m4.2.2.1.1.3.3.3.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.3">1</cn></apply></apply><cn id="S4.SS2.p3.16.m4.2.2.1.1.4.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.1.4">1</cn><set id="S4.SS2.p3.16.m4.2.2.1.1.1.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1"><apply id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1"><gt id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.1"></gt><apply id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.2">ğ‘§</ci><ci id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.3">ğ‘–</ci></apply><cn id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.3">0</cn></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.16.m4.2c">Index(z)=\sum_{i=1}^{d}2^{i-1}\mathbbm{1}\{z_{i}&gt;0\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.16.m4.2d">italic_I italic_n italic_d italic_e italic_x ( italic_z ) = âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT blackboard_1 { italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &gt; 0 }</annotation></semantics></math>.
To train the tokenizer, we employ a standard combination of reconstruction, perceptual, and commitment losses, along with an entropy penalty to promote better codebook utilizationÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib58" title="">2023</a>)</cite>.
Importantly, we exclude the use of GAN loss, as it was found to negatively impact training stability.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Datasets.</span>
Our investigation first is conducted on the following text-to-motion datasets: HumanML3DÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> and Motion-XÂ <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>.
HumanML3D comprises 14,616 motion clips sourced from the AMASS datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Mahmood etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib29" title="">2019</a>)</cite>, paired with 44,970 textual descriptions.
Motion-X, a more recent dataset, includes approximately 81,000 motion clips.
To validate our conclusions on larger-scale data, we also carry out experiments on the proposed MotionBase dataset with two variants: MotionBase-0.5 and MotionBase-1.0.
MotionBase-0.5 contains 500,000 clips, while MotionBase-1.0 encompasses the full scope of our collected data, with over 1 million clips.
Following standard practice, each dataset is split into training, validation, and test sets in proportions of 85%, 5%, and 15%, respectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Evaluation Metrics. </span>
For the motion generation task, we employ the following metrics in our experiments following <cite class="ltx_cite ltx_citemacro_cite">Guo etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite>.
(1) Frechet Inception Distance (FID): This metric assesses overall motion quality by measuring the distributional difference between the high-level features of generated motions and real motions.
(2) Motion-retrieval Precision (R-Precision) and Multimodal Distance (MMDist): These metrics evaluate the semantic alignment between the textual input and generated motions.
R-Precision measures the top-1/2/3 retrieval accuracy, while MMDist computes the distance between matched text and motion pairs.
Additionally, we validate our motion tokenizer by conducting experiments on the motion reconstruction task.
This is measured using both Mean Per Joint Position Error (MPJPE) and FID.
MPJPE quantifies the average distance (in millimeters) between the predicted joint positions and the ground truth positions across all joints in the skeleton.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.6"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.6.1">Implementation Details. </span>
For the motion tokenizer, we implement a VQ codebook <math alttext="\mathbbm{C}\in\mathbbm{R}^{1024\times 512}" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">â„‚</mi><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml"><mi id="S5.SS1.p3.1.m1.1.1.3.2" xref="S5.SS1.p3.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S5.SS1.p3.1.m1.1.1.3.3" xref="S5.SS1.p3.1.m1.1.1.3.3.cmml"><mn id="S5.SS1.p3.1.m1.1.1.3.3.2" xref="S5.SS1.p3.1.m1.1.1.3.3.2.cmml">1024</mn><mo id="S5.SS1.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p3.1.m1.1.1.3.3.1.cmml">Ã—</mo><mn id="S5.SS1.p3.1.m1.1.1.3.3.3" xref="S5.SS1.p3.1.m1.1.1.3.3.3.cmml">512</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><in id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></in><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">â„‚</ci><apply id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.3.2.cmml" xref="S5.SS1.p3.1.m1.1.1.3.2">â„</ci><apply id="S5.SS1.p3.1.m1.1.1.3.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3"><times id="S5.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3.1"></times><cn id="S5.SS1.p3.1.m1.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1.3.3.2">1024</cn><cn id="S5.SS1.p3.1.m1.1.1.3.3.3.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1.3.3.3">512</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\mathbbm{C}\in\mathbbm{R}^{1024\times 512}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">blackboard_C âˆˆ blackboard_R start_POSTSUPERSCRIPT 1024 Ã— 512 end_POSTSUPERSCRIPT</annotation></semantics></math> with an embedding dimensionality of <math alttext="d=512" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">d</mi><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><eq id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></eq><ci id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">ğ‘‘</ci><cn id="S5.SS1.p3.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.p3.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">d=512</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">italic_d = 512</annotation></semantics></math>, and the resulting discrete codes are incorporated as additional vocabulary for the LLM.
In comparison, our lookup-free codebook has a size of <math alttext="2^{16}=16384" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3.1"><semantics id="S5.SS1.p3.3.m3.1a"><mrow id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><msup id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml"><mn id="S5.SS1.p3.3.m3.1.1.2.2" xref="S5.SS1.p3.3.m3.1.1.2.2.cmml">2</mn><mn id="S5.SS1.p3.3.m3.1.1.2.3" xref="S5.SS1.p3.3.m3.1.1.2.3.cmml">16</mn></msup><mo id="S5.SS1.p3.3.m3.1.1.1" xref="S5.SS1.p3.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.3.m3.1.1.3" xref="S5.SS1.p3.3.m3.1.1.3.cmml">16384</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><eq id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1.1"></eq><apply id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.1.1.2.1.cmml" xref="S5.SS1.p3.3.m3.1.1.2">superscript</csymbol><cn id="S5.SS1.p3.3.m3.1.1.2.2.cmml" type="integer" xref="S5.SS1.p3.3.m3.1.1.2.2">2</cn><cn id="S5.SS1.p3.3.m3.1.1.2.3.cmml" type="integer" xref="S5.SS1.p3.3.m3.1.1.2.3">16</cn></apply><cn id="S5.SS1.p3.3.m3.1.1.3.cmml" type="integer" xref="S5.SS1.p3.3.m3.1.1.3">16384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">2^{16}=16384</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m3.1d">2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT = 16384</annotation></semantics></math>, where the least frequently used tokens from the LLMâ€™s codebook are mapped to represent motion codes.
The motion encoder <math alttext="\mathbbm{E}" class="ltx_Math" display="inline" id="S5.SS1.p3.4.m4.1"><semantics id="S5.SS1.p3.4.m4.1a"><mi id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml">ğ”¼</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><ci id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">ğ”¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">\mathbbm{E}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.4.m4.1d">blackboard_E</annotation></semantics></math> operates with a temporal downsampling rate of <math alttext="\alpha=4" class="ltx_Math" display="inline" id="S5.SS1.p3.5.m5.1"><semantics id="S5.SS1.p3.5.m5.1a"><mrow id="S5.SS1.p3.5.m5.1.1" xref="S5.SS1.p3.5.m5.1.1.cmml"><mi id="S5.SS1.p3.5.m5.1.1.2" xref="S5.SS1.p3.5.m5.1.1.2.cmml">Î±</mi><mo id="S5.SS1.p3.5.m5.1.1.1" xref="S5.SS1.p3.5.m5.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.5.m5.1.1.3" xref="S5.SS1.p3.5.m5.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.5.m5.1b"><apply id="S5.SS1.p3.5.m5.1.1.cmml" xref="S5.SS1.p3.5.m5.1.1"><eq id="S5.SS1.p3.5.m5.1.1.1.cmml" xref="S5.SS1.p3.5.m5.1.1.1"></eq><ci id="S5.SS1.p3.5.m5.1.1.2.cmml" xref="S5.SS1.p3.5.m5.1.1.2">ğ›¼</ci><cn id="S5.SS1.p3.5.m5.1.1.3.cmml" type="integer" xref="S5.SS1.p3.5.m5.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.5.m5.1c">\alpha=4</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.5.m5.1d">italic_Î± = 4</annotation></semantics></math>.
We experiment with four LLM architectures to build our large motion model: GPT2-mediumÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib38" title="">2019</a>)</cite>, Llama-2-7b, Llama-2-13bÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib47" title="">2023b</a>)</cite>, and Llama3.1-8bÂ <cite class="ltx_cite ltx_citemacro_citep">(Dubey etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib12" title="">2024</a>)</cite>.
The motion tokenizer is trained with a learning rate of 1e-4 and a batch size of 256 over 300K iterations.
For training the large motion model, full parameter tuning is performed on 8<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.p3.6.m6.1"><semantics id="S5.SS1.p3.6.m6.1a"><mo id="S5.SS1.p3.6.m6.1.1" xref="S5.SS1.p3.6.m6.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.6.m6.1b"><times id="S5.SS1.p3.6.m6.1.1.cmml" xref="S5.SS1.p3.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.6.m6.1d">Ã—</annotation></semantics></math>A800 GPUs, with a batch size of 1024, over 300 epochs.
The learning rate is set to 2e-4 for GPT2-medium and 2e-5 for the Llama models.
Further details are provided in the appendix due to space limitation.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparisons under different model and data sizes. All experiments are conducted using the same pretrained VQ model for consistency. Additionally, we re-train the motion autoencoder and text encoder Â <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> separately on the Motion-X and MotionBase datasets, using their respective data to train the motion autoencoder for each datasetâ€™s evaluation.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.6" style="width:398.0pt;height:275.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.1pt,24.3pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.6.6.7.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="3" id="S5.T2.6.6.7.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_tt" colspan="3" id="S5.T2.6.6.7.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">Motion-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_tt" colspan="3" id="S5.T2.6.6.7.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">MotionBase</th>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.6.6.6.7" style="padding-left:10.0pt;padding-right:10.0pt;">Decoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.6.6.6.8" style="padding-left:10.0pt;padding-right:10.0pt;">#Inst.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.6.6.6.9" style="padding-left:10.0pt;padding-right:10.0pt;">#Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.1.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.m1.1"><semantics id="S5.T2.2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.2.m1.1.1" stretchy="false" xref="S5.T2.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.m1.1b"><ci id="S5.T2.2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.3.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.3.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T2.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.4.4.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.4.4.4.4.m1.1"><semantics id="S5.T2.4.4.4.4.m1.1a"><mo id="S5.T2.4.4.4.4.m1.1.1" stretchy="false" xref="S5.T2.4.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.4.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.5.5.5.5" style="padding-left:10.0pt;padding-right:10.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.5.5.5.5.m1.1"><semantics id="S5.T2.5.5.5.5.m1.1a"><mo id="S5.T2.5.5.5.5.m1.1.1" stretchy="false" xref="S5.T2.5.5.5.5.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.5.m1.1b"><ci id="S5.T2.5.5.5.5.m1.1.1.cmml" xref="S5.T2.5.5.5.5.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.5.5.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.6.6.6.6" style="padding-left:10.0pt;padding-right:10.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.6.6.6.6.m1.1"><semantics id="S5.T2.6.6.6.6.m1.1a"><mo id="S5.T2.6.6.6.6.m1.1.1" stretchy="false" xref="S5.T2.6.6.6.6.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.6.m1.1b"><ci id="S5.T2.6.6.6.6.m1.1.1.cmml" xref="S5.T2.6.6.6.6.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.6.6.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.6.6.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.8.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.8.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.8.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.8.1.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.206</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.8.1.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.402</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.8.1.6" style="padding-left:10.0pt;padding-right:10.0pt;">54.017</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.8.1.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.046</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.8.1.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.136</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.8.1.9" style="padding-left:10.0pt;padding-right:10.0pt;">173.275</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.9.2.1" style="padding-left:10.0pt;padding-right:10.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.9.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.9.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.9.2.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.468</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.9.2.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.791</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.9.2.6" style="padding-left:10.0pt;padding-right:10.0pt;">0.096</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.9.2.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.090</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.9.2.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.215</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.9.2.9" style="padding-left:10.0pt;padding-right:10.0pt;">251.358</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.10.3.1" style="padding-left:10.0pt;padding-right:10.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.10.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.10.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.358</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.618</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.10.3.6" style="padding-left:10.0pt;padding-right:10.0pt;">4.852</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.10.3.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.116</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.276</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3.9" style="padding-left:10.0pt;padding-right:10.0pt;">157.950</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.11.4.1" style="padding-left:10.0pt;padding-right:10.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4.2" style="padding-left:10.0pt;padding-right:10.0pt;">1M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.11.4.3" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.11.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.357</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.614</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.11.4.6" style="padding-left:10.0pt;padding-right:10.0pt;">5.083</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.11.4.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.118</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.269</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4.9" style="padding-left:10.0pt;padding-right:10.0pt;">121.917</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.12.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.12.5.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.12.5.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.12.5.3" style="padding-left:10.0pt;padding-right:10.0pt;">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.12.5.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.207</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.12.5.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.405</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.12.5.6" style="padding-left:10.0pt;padding-right:10.0pt;">53.354</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.12.5.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.042</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.12.5.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.123</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.12.5.9" style="padding-left:10.0pt;padding-right:10.0pt;">160.845</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.13.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.13.6.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.13.6.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.13.6.3" style="padding-left:10.0pt;padding-right:10.0pt;">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.13.6.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.471</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.13.6.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.794</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.13.6.6" style="padding-left:10.0pt;padding-right:10.0pt;">0.159</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.13.6.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.093</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.13.6.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.222</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.13.6.9" style="padding-left:10.0pt;padding-right:10.0pt;">253.289</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.14.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.14.7.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.14.7.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.14.7.3" style="padding-left:10.0pt;padding-right:10.0pt;">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.14.7.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.372</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.14.7.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.627</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.14.7.6" style="padding-left:10.0pt;padding-right:10.0pt;">4.908</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.14.7.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.125</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.14.7.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.272</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.14.7.9" style="padding-left:10.0pt;padding-right:10.0pt;">87.288</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.15.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.15.8.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.15.8.2" style="padding-left:10.0pt;padding-right:10.0pt;">1.0M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.15.8.3" style="padding-left:10.0pt;padding-right:10.0pt;">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.15.8.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.351</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.15.8.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.602</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.15.8.6" style="padding-left:10.0pt;padding-right:10.0pt;">5.582</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.15.8.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.125</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.15.8.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.267</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.15.8.9" style="padding-left:10.0pt;padding-right:10.0pt;">83.024</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.16.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.16.9.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.16.9.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.16.9.3" style="padding-left:10.0pt;padding-right:10.0pt;">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.16.9.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.217</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.16.9.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.418</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.16.9.6" style="padding-left:10.0pt;padding-right:10.0pt;">54.004</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.16.9.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.043</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.16.9.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.124</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.16.9.9" style="padding-left:10.0pt;padding-right:10.0pt;">162.102</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.17.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.17.10.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-3</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.17.10.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.17.10.3" style="padding-left:10.0pt;padding-right:10.0pt;">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.17.10.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.483</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.17.10.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.802</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.17.10.6" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.17.10.6.1">0.103</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.17.10.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.082</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.17.10.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.214</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.17.10.9" style="padding-left:10.0pt;padding-right:10.0pt;">249.790</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.18.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.18.11.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-3</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.18.11.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.18.11.3" style="padding-left:10.0pt;padding-right:10.0pt;">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.18.11.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.363</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.18.11.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.625</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.18.11.6" style="padding-left:10.0pt;padding-right:10.0pt;">4.798</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.18.11.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.121</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.18.11.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.264</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.18.11.9" style="padding-left:10.0pt;padding-right:10.0pt;">81.389</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.19.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.19.12.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-3</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.19.12.2" style="padding-left:10.0pt;padding-right:10.0pt;">1M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.19.12.3" style="padding-left:10.0pt;padding-right:10.0pt;">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.19.12.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.354</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.19.12.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.611</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.19.12.6" style="padding-left:10.0pt;padding-right:10.0pt;">5.100</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.19.12.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.129</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.19.12.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.270</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.19.12.9" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.19.12.9.1">68.083</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.20.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.20.13.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.20.13.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.20.13.3" style="padding-left:10.0pt;padding-right:10.0pt;">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.20.13.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.225</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.20.13.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.436</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.20.13.6" style="padding-left:10.0pt;padding-right:10.0pt;">53.447</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.20.13.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.045</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.20.13.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.125</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.20.13.9" style="padding-left:10.0pt;padding-right:10.0pt;">159.368</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.21.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.21.14.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.21.14.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.21.14.3" style="padding-left:10.0pt;padding-right:10.0pt;">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.21.14.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.21.14.4.1">0.486</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.21.14.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.21.14.5.1">0.805</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.21.14.6" style="padding-left:10.0pt;padding-right:10.0pt;">0.132</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.21.14.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.086</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.21.14.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.218</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.21.14.9" style="padding-left:10.0pt;padding-right:10.0pt;">249.868</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.22.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.22.15.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.22.15.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.22.15.3" style="padding-left:10.0pt;padding-right:10.0pt;">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.22.15.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.375</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.22.15.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.636</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.22.15.6" style="padding-left:10.0pt;padding-right:10.0pt;">4.792</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.22.15.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.116</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.22.15.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.267</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.22.15.9" style="padding-left:10.0pt;padding-right:10.0pt;">80.473</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.23.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T2.6.6.23.16.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.23.16.2" style="padding-left:10.0pt;padding-right:10.0pt;">1.0M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.6.6.23.16.3" style="padding-left:10.0pt;padding-right:10.0pt;">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T2.6.6.23.16.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.359</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.23.16.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.612</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.6.6.23.16.6" style="padding-left:10.0pt;padding-right:10.0pt;">5.370</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T2.6.6.23.16.7" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.23.16.7.1">0.131</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.23.16.8" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.23.16.8.1">0.277</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.23.16.9" style="padding-left:10.0pt;padding-right:10.0pt;">78.665</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Discussion of Scaling up motion generation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In this section, we investigate the impact of model size and data scale on motion generation performance.
We utilize the motion autoencoder Â <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> retrained on Motion-X and MotionBase datasets to evaluate performance on their respective test sets.
We categorize our training data into four scales: 0.02M (HumanML3D only), 0.08M (Motion-X only), 0.5M (MotionBase-0.5), and 1M (MotionBase-1.0).
To ensure fair comparison, we employ the same VQ as the motion tokenizer, maintaining consistency across experiments to validate our conclusions.</p>
</div>
<figure class="ltx_table ltx_align_floatright" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison with existing SoTA methods on the HumanML3D benchmark. Results marked with <math alttext="*" class="ltx_Math" display="inline" id="S5.T3.2.m1.1"><semantics id="S5.T3.2.m1.1b"><mo id="S5.T3.2.m1.1.1" xref="S5.T3.2.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.m1.1c"><times id="S5.T3.2.m1.1.1.cmml" xref="S5.T3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.m1.1e">âˆ—</annotation></semantics></math> represent values reproduced using the officially released code, while unmarked results are taken from the original papers.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.11" style="width:242.5pt;height:183.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.4pt,16.2pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.11.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.6.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.6.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.6.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">Decoder</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.3.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.3.1.1.1.m1.1"><semantics id="S5.T3.3.1.1.1.m1.1a"><mo id="S5.T3.3.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.3.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.1.1.1.m1.1b"><ci id="S5.T3.3.1.1.1.m1.1.1.cmml" xref="S5.T3.3.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.4.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.4.2.2.2.m1.1"><semantics id="S5.T3.4.2.2.2.m1.1a"><mo id="S5.T3.4.2.2.2.m1.1.1" stretchy="false" xref="S5.T3.4.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.2.2.2.m1.1b"><ci id="S5.T3.4.2.2.2.m1.1.1.cmml" xref="S5.T3.4.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.2.2.2.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.5.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.5.3.3.3.m1.1"><semantics id="S5.T3.5.3.3.3.m1.1a"><mo id="S5.T3.5.3.3.3.m1.1.1" stretchy="false" xref="S5.T3.5.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.3.3.3.m1.1b"><ci id="S5.T3.5.3.3.3.m1.1.1.cmml" xref="S5.T3.5.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.3.3.3.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.6.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">MMDist <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.6.4.4.4.m1.1"><semantics id="S5.T3.6.4.4.4.m1.1a"><mo id="S5.T3.6.4.4.4.m1.1.1" stretchy="false" xref="S5.T3.6.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.4.4.4.m1.1b"><ci id="S5.T3.6.4.4.4.m1.1.1.cmml" xref="S5.T3.6.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.4.4.4.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.9.10.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">MLD</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.9.10.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">-</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.10.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.481</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.10.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.772</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.10.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.473</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.10.1.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.196</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.11.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionDiffuse</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.11.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">-</th>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.11.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.491</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.11.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.782</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.11.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.630</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.11.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.113</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.9.12.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">T2M-GPT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.9.12.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.12.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.492</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.12.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.775</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.12.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.12.3.5.1">0.141</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.12.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.121</td>
</tr>
<tr class="ltx_tr" id="S5.T3.7.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.7.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.7.5.5.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.7.5.5.1.1.1">1,âˆ—</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.7.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">T5</th>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.409</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.667</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.162</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.992</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.8.6.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.8.6.6.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.8.6.6.1.1.1">1</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.8.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">T5</th>
<td class="ltx_td ltx_align_center" id="S5.T3.8.6.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.492</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.6.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.778</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.6.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.232</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.6.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.096</td>
</tr>
<tr class="ltx_tr" id="S5.T3.9.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.9.7.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.9.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.9.7.7.1.1.1">2,âˆ—</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.9.7.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-2-13B</th>
<td class="ltx_td ltx_align_center" id="S5.T3.9.7.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.367</td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.7.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.654</td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.7.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.571</td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.7.7.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.981</td>
</tr>
<tr class="ltx_tr" id="S5.T3.10.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.10.8.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.10.8.8.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.10.8.8.1.1.1">2,âˆ—</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.10.8.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-1-13B</th>
<td class="ltx_td ltx_align_center" id="S5.T3.10.8.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.363</td>
<td class="ltx_td ltx_align_center" id="S5.T3.10.8.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.633</td>
<td class="ltx_td ltx_align_center" id="S5.T3.10.8.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.592</td>
<td class="ltx_td ltx_align_center" id="S5.T3.10.8.8.6" style="padding-left:3.0pt;padding-right:3.0pt;">4.029</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.11.9.9.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.11.9.9.1.1.1">2</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.9.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-1-13B</th>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.411</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.696</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.9.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.542</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.9.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.584</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.13.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionLLM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.13.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">Gemma-2b</th>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.13.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.482</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.13.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.770</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.13.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.491</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.13.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.138</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.14.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">AvatarGPT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.14.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-1-13B</th>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.14.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.389</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.14.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.623</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.14.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.567</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.14.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T3.11.9.15.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T3.11.9.15.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-2-13B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.9.15.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.15.6.3.1">0.519</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.9.15.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.15.6.4.1">0.803</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.9.15.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.166</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.9.15.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.15.6.6.1">2.964</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Does increasing model size benefit motion generation? </span>
Yes.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T2" title="Table 2 â€£ 5.1 Experimental Setup â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">2</span></a>, our results demonstrate that increasing model size leads to significant performance improvements when provided with the same amount of training data.
Specifically, Llama2-13b outperforms Llama2-7b, which in turn surpasses GPT2-medium, illustrating a clear trend of performance gains as model capacity increases.
This suggests that models with larger size are better equipped to capture diverse, complex patterns and relationships within human motions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Does increasing data scale benefit motion generation?</span>
Yes.
In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T2" title="Table 2 â€£ 5.1 Experimental Setup â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">2</span></a>, when using the same foundation model, increasing the scale of training data leads to substantial performance gains on the MotionBase test set, aligning with our expected scaling laws.
This improvement is particularly pronounced in the R-precision metric, emphasizing the critical role of data scale in enhancing semantic alignment between generated motions and text prompts.
However, contrary to our expectations, we observe a noticeable performance decline on the Motion-X test set if not trained on Motion-X (0.08M).
We attribute this to the limitations of the retrieval-based evaluation model, as discussed in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS4" title="5.4 Limitation of Automated Metric â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_align_right ltx_img_landscape" height="254" id="S5.F3.g1" src="x3.png" width="343"/>
<figcaption class="ltx_caption ltx_align_right"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Training curves with Y-axis denoting R@1 retrieval accuracy. All these models are trained for 300 epochs at most and are evaluated every 1000 steps.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Does the large motion model perform SoTA competitively?</span>
We evaluate our large motion model on the widely adopted HumanML3D benchmark.
We compare its performance against a variety of SoTA approaches.
This includes diffusion-based methods such as MLDÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib8" title="">2023</a>)</cite> and MotionDiffuseÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib61" title="">2022</a>)</cite>, as well as the GPT-based T2M-GPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib60" title="">2023a</a>)</cite>.
We also compare against LLM fine-tuning methods like MotionGPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib21" title="">2023</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib64" title="">2024b</a>)</cite>, MotionLLMÂ <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib52" title="">2024</a>)</cite>, and AvatarGPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib68" title="">2024</a>)</cite>.
As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T3" title="Table 3 â€£ 5.2 Discussion of Scaling up motion generation â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">3</span></a>, our model, which utilizes Llama-2-13B as the decoder and calculates the loss over the entire concatenated sequence of input text, achieves SOTA performance.
Our large motion model significantly outperforms other LLM-based methods such as MotionGPT and AvatarGPT, as well as the earlier T2M-GPT.
In particular, we observe substantial improvements in key metrics such as R@1, R@3, and MMDist, highlighting our modelâ€™s ability to generate motion sequences that are better aligned with text descriptions and of higher quality.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Slow convergence of large motion models.</span>
To evaluate the convergence speed of large motion models, we train GPT-2, Llama2-7b, and Llama3.1-8b for 300 epochs on Motion-X.
The training curve of with R@1 performance is illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.F3" title="Figure 3 â€£ 5.2 Discussion of Scaling up motion generation â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">3</span></a>.
We obverse that all large motion models nearly converge by 200 epochs, with larger models converging faster.
Initializing these models with pre-trained weights proves beneficial for speeding up convergence.
Compared to large multimodal models like LLaVAÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib27" title="">2023</a>)</cite>, large motion models require more epochs to capture the complex representations of motion sequences.
We attribute the slow convergence of these models to the limited representation capacity of the motion tokenizer, which contains only 512 motion tokens.
This suggests the need to optimize the motion tokenizer and expand its representation space.
To address this, we explore 2D-LFQ quantization method as a promising alternative.</p>
</div>
<figure class="ltx_table ltx_align_floatright" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation of the effectiveness of synthetic data and static data.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.3" style="width:185.4pt;height:68.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.4pt,6.1pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.3.3.3.4">TRAIN SET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.2.m1.1"><semantics id="S5.T4.2.2.2.2.m1.1a"><mo id="S5.T4.2.2.2.2.m1.1.1" stretchy="false" xref="S5.T4.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.m1.1b"><ci id="S5.T4.2.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.2.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.3.3.3">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.3.3.3.3.m1.1"><semantics id="S5.T4.3.3.3.3.m1.1a"><mo id="S5.T4.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T4.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.3.m1.1b"><ci id="S5.T4.3.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T4.3.3.4.1.1">w/o static &amp; syn</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.2">0.101</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.3">0.231</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.4">261.325</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.3.3.5.2.1">w/o static</th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.2">0.110</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.3">0.248</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.4">286.809</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T4.3.3.6.3.1">MotionBase</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.6.3.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.6.3.2.1">0.118</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.6.3.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.6.3.3.1">0.269</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.6.3.4"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.6.3.4.1">121.917</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p6.1.1">Does Static and Synthetic Data help?</span>
Yes, the addition of static image data and synthesized data both contribute to improvements, as illustrated in TableÂ <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:syn_and_static_part</span>, more analysis can be found in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS1" title="C.1 Ablation of Synthesis and Static Data? â€£ Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">C.1</span></a>.</p>
</div>
<figure class="ltx_table ltx_align_floatright" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation of out-of-domain evaluation on UNSEEN-90K dataset, where <math alttext="\#N" class="ltx_Math" display="inline" id="S5.T5.3.m1.1"><semantics id="S5.T5.3.m1.1b"><mrow id="S5.T5.3.m1.1.1" xref="S5.T5.3.m1.1.1.cmml"><mi id="S5.T5.3.m1.1.1.2" mathvariant="normal" xref="S5.T5.3.m1.1.1.2.cmml">#</mi><mo id="S5.T5.3.m1.1.1.1" xref="S5.T5.3.m1.1.1.1.cmml">â¢</mo><mi id="S5.T5.3.m1.1.1.3" xref="S5.T5.3.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.3.m1.1c"><apply id="S5.T5.3.m1.1.1.cmml" xref="S5.T5.3.m1.1.1"><times id="S5.T5.3.m1.1.1.1.cmml" xref="S5.T5.3.m1.1.1.1"></times><ci id="S5.T5.3.m1.1.1.2.cmml" xref="S5.T5.3.m1.1.1.2">#</ci><ci id="S5.T5.3.m1.1.1.3.cmml" xref="S5.T5.3.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.m1.1d">\#N</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.m1.1e"># italic_N</annotation></semantics></math> denotes we use <math alttext="N" class="ltx_Math" display="inline" id="S5.T5.4.m2.1"><semantics id="S5.T5.4.m2.1b"><mi id="S5.T5.4.m2.1.1" xref="S5.T5.4.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T5.4.m2.1c"><ci id="S5.T5.4.m2.1.1.cmml" xref="S5.T5.4.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.m2.1d">N</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.m2.1e">italic_N</annotation></semantics></math> subsets of MotionBase for training.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.7" style="width:214.7pt;height:68.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.9pt,6.1pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.7.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.7.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.7.3.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">TRAIN SET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.5.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.5.1.1.1.m1.1"><semantics id="S5.T5.5.1.1.1.m1.1a"><mo id="S5.T5.5.1.1.1.m1.1.1" stretchy="false" xref="S5.T5.5.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.1.1.1.m1.1b"><ci id="S5.T5.5.1.1.1.m1.1.1.cmml" xref="S5.T5.5.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.6.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.6.2.2.2.m1.1"><semantics id="S5.T5.6.2.2.2.m1.1a"><mo id="S5.T5.6.2.2.2.m1.1.1" stretchy="false" xref="S5.T5.6.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.2.2.2.m1.1b"><ci id="S5.T5.6.2.2.2.m1.1.1.cmml" xref="S5.T5.6.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.2.2.2.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.7.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T5.7.3.3.3.m1.1"><semantics id="S5.T5.7.3.3.3.m1.1a"><mo id="S5.T5.7.3.3.3.m1.1.1" stretchy="false" xref="S5.T5.7.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T5.7.3.3.3.m1.1b"><ci id="S5.T5.7.3.3.3.m1.1.1.cmml" xref="S5.T5.7.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.7.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T5.7.3.4.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">HumanML3D</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.3.4.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.0264</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.3.4.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.0832</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.3.4.1.4" style="padding-left:10.0pt;padding-right:10.0pt;">257.563</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T5.7.3.5.2.1" style="padding-left:10.0pt;padding-right:10.0pt;">MotionX</th>
<td class="ltx_td ltx_align_center" id="S5.T5.7.3.5.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.0224</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.3.5.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.0705</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.3.5.2.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.7.3.5.2.4.1">246.220</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.3.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T5.7.3.6.3.1" style="padding-left:10.0pt;padding-right:10.0pt;">MotionBase-#38</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.3.6.3.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.7.3.6.3.2.1">0.0761</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.3.6.3.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.7.3.6.3.3.1">0.2090</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.3.6.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">263.539</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p7.1.1">Do large motion models outperform in out-of-distribution setup? </span>
Yes.
We present the results in TableÂ <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:out_of_distribute</span>.
This ablation is essential for further validating the generalization capabilities of large motion models, as the improvements observed in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T2" title="Table 2 â€£ 5.1 Experimental Setup â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">2</span></a> may stem from the inclusion of additional in-domain data from Motion-X.
In this setup, we select four subsets from MotionBase, comprising 90K samples (UNSEEN-90K), for evaluation, while the remaining 38 subsets are used for training.
This ensures that the test set consists entirely of out-of-domain (OOD) samples.
We compare the performance of models trained on HumanML3D, MotionX, and Motion-#38, all utilizing the GPT2-medium architecture, where <math alttext="\#N" class="ltx_Math" display="inline" id="S5.SS2.p7.1.m1.1"><semantics id="S5.SS2.p7.1.m1.1a"><mrow id="S5.SS2.p7.1.m1.1.1" xref="S5.SS2.p7.1.m1.1.1.cmml"><mi id="S5.SS2.p7.1.m1.1.1.2" mathvariant="normal" xref="S5.SS2.p7.1.m1.1.1.2.cmml">#</mi><mo id="S5.SS2.p7.1.m1.1.1.1" xref="S5.SS2.p7.1.m1.1.1.1.cmml">â¢</mo><mi id="S5.SS2.p7.1.m1.1.1.3" xref="S5.SS2.p7.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.1.m1.1b"><apply id="S5.SS2.p7.1.m1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1"><times id="S5.SS2.p7.1.m1.1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1.1"></times><ci id="S5.SS2.p7.1.m1.1.1.2.cmml" xref="S5.SS2.p7.1.m1.1.1.2">#</ci><ci id="S5.SS2.p7.1.m1.1.1.3.cmml" xref="S5.SS2.p7.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.1.m1.1c">\#N</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.1.m1.1d"># italic_N</annotation></semantics></math> denotes the number of training subsets.
All models are trained using the GPT2-medium.
The results on the OOD test set clearly demonstrate that the model trained on MotionBase significantly outperforms those trained on HumanML3D and MotionX, particularly in terms of R@1 and R@3 metrics.
These findings strongly highlight the superior generalization ability of large motion models when handling unseen OOD data, especially when trained on diverse, large-scale datasets.
However, we once again observe unexpected results with the FID metric, which will be discussed further in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS4" title="5.4 Limitation of Automated Metric â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S5.F4.g1" src="x4.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Comparison with different motion quantization on Motion-X (<span class="ltx_text ltx_font_bold" id="S5.F4.5.1">left</span>) and MotionBase (<span class="ltx_text ltx_font_bold" id="S5.F4.6.2">right</span>). Note that we only show MPJPE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.F4.2.m1.1"><semantics id="S5.F4.2.m1.1b"><mo id="S5.F4.2.m1.1.1" stretchy="false" xref="S5.F4.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.F4.2.m1.1c"><ci id="S5.F4.2.m1.1.1.cmml" xref="S5.F4.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.m1.1e">â†“</annotation></semantics></math>) results here. FID results is shown in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS5" title="C.5 Ablation of Motion Quantization â€£ Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">C.5</span></a>.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Discussion of Motion Quantization</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.3">In this section, we investigate the impact of different motion quantization methods.
We compare our proposed 2D lookup-free quantization (2D-LFQ) against two commonly used approaches: residual vector quantization (RVQ) and vector quantization (VQ), across various codebook sizes ranging from <math alttext="2^{8}" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><msup id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mn id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">2</mn><mn id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">8</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">superscript</csymbol><cn id="S5.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1.2">2</cn><cn id="S5.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">2^{8}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="2^{16}" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><msup id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">2</mn><mn id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">16</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">superscript</csymbol><cn id="S5.SS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS3.p1.2.m2.1.1.2">2</cn><cn id="S5.SS3.p1.2.m2.1.1.3.cmml" type="integer" xref="S5.SS3.p1.2.m2.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">2^{16}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT</annotation></semantics></math>.
The number of parameters for RVQ/VQ and 2D-LFQ are 19.43M and 108.35M, respectively.
As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.F4" title="Figure 4 â€£ 5.2 Discussion of Scaling up motion generation â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">4</span></a>, 2D-LFQ demonstrates significant improvements over both RVQ and VQ. Notably, as the codebook size increases, 2D-LFQ continues to enhance performance, while RVQ and VQ experience diminishing returns or performance degradation with larger codebooks.
Our deeper analysis attributes these gains to better codebook utilization by 2D-LFQ.
FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.F5" title="Figure 5 â€£ 5.3 Discussion of Motion Quantization â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates that the utilization rates for VQ and RVQ begin to decline once the codebook size exceeds <math alttext="2^{10}" class="ltx_Math" display="inline" id="S5.SS3.p1.3.m3.1"><semantics id="S5.SS3.p1.3.m3.1a"><msup id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mn id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">2</mn><mn id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml">10</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">superscript</csymbol><cn id="S5.SS3.p1.3.m3.1.1.2.cmml" type="integer" xref="S5.SS3.p1.3.m3.1.1.2">2</cn><cn id="S5.SS3.p1.3.m3.1.1.3.cmml" type="integer" xref="S5.SS3.p1.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">2^{10}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.3.m3.1d">2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math>, which corresponds to the peak performance for these methods, whereas the utilization of 2D-LFQ continues to increase with larger codebooks.
Additionally, we conduct further experiments to validate the benefits of 2D motion encoding in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS5" title="C.5 Ablation of Motion Quantization â€£ Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">C.5</span></a>.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="256" id="S5.F5.g1" src="x5.png" width="342"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of codebook utilization for different motion quantization methods.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Limitation of Automated Metric</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">As mentioned earlier, the FID scores in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T2" title="Table 2 â€£ 5.1 Experimental Setup â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">2</span></a> and TableÂ <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:out_of_distribute</span> yield unexpected results.
Specifically, when evaluating on Motion-X and UNSEEN-90K, FID achieves its best performance when trained on Motion-X, significantly outperforming both the smaller HumanML3D and the larger-scale MotionBase.
In this section, we aim to investigate this anomaly.
FID, a standard metric widely used for generation tasks, is typically measured by a pretrained evaluator.
In traditional image generation, FID is calculated using a well-trained, robust visual encoder like InceptionNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Szegedy etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib43" title="">2015</a>)</cite>, which is trained on millions of images.
However, the evaluator currently used to compute FID for motion generation is a simple motion autoencoder with a very small parameter scaleÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite>.
Since this motion autoencoder is trained on limited data consisting of only 20K motions, we argue that it may lack the generalization needed for robust performance, leading to difficulties in reliably capturing the complex semantic alignment between text and motion.Similar unexpected results occur in motion reconstruction as well.
As show in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T6" title="Table 6 â€£ 5.4 Limitation of Automated Metric â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">6</span></a>, the FID score on HumanML3D is two orders of magnitude higher when comparing 2D-LFQ and VQ-VAE, despite the former achieving a much lower MPJPE.
When tested on MotionBase, 2D-LFQ obtains the highest FID score even while achieving the best MPJPE.
We observe the same issue with other metrics like MMDist, as discussed in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS1" title="C.1 Ablation of Synthesis and Static Data? â€£ Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">C.1</span></a>.
Notably, <cite class="ltx_cite ltx_citemacro_cite">Voas etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib49" title="">2023</a>)</cite> have mentioned that existing metrics are sensitive to the quality of the embedding space and do not always align with human perception.
These findings highlight the need for a more robust and fair evaluation metric for large motion models moving forward.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Robustness investigation of the evaluation metrics on the motion reconstruction task.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.2" style="width:348.4pt;height:76.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.7pt,6.8pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.2.2.3.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T6.2.2.3.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="S5.T6.2.2.3.1.2">HumanML3D</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="S5.T6.2.2.3.1.3">Motion-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="S5.T6.2.2.3.1.4">MotionBase</th>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.3">Tokenizer</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.4">#Num.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.2.2.2.5">#Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.1.1">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T6.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.m1.1b"><ci id="S5.T6.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.2.2.2.2">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T6.2.2.2.2.m1.1"><semantics id="S5.T6.2.2.2.2.m1.1a"><mo id="S5.T6.2.2.2.2.m1.1.1" stretchy="false" xref="S5.T6.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.2.m1.1b"><ci id="S5.T6.2.2.2.2.m1.1.1.cmml" xref="S5.T6.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.2.2.2.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.6">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.2.2.2.7">MPJPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.8">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.9">MPJPE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.2.2.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.1">VQ-VAE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.2">512</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.4.1.3">19.43M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.4">0.078</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.4.1.5">69.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.6">0.852</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.4.1.7">106.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.8">4.366</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.9">123.6</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2.5.2">
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.1">RQ-VAE</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.2">512</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.2.5.2.3">19.43M</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.4"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.5.2.4.1">0.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.2.5.2.5"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.5.2.5.1">37.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.6">0.568</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.2.5.2.7">56.9</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.8"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.5.2.8.1">4.026</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.9">78.2</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2.6.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.1">2D-LFQ</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.2">16384</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.2.2.6.3.3">108.35M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.4"><span class="ltx_text" id="S5.T6.2.2.6.3.4.1" style="color:#FF0000;">1.769</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.2.2.6.3.5">45.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.6"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.6.3.6.1">0.295</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.2.2.6.3.7"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.6.3.7.1">54.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.8"><span class="ltx_text" id="S5.T6.2.2.6.3.8.1" style="color:#FF0000;">7.853</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.9"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.6.3.9.1">64.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we explore how to advance the field of large-scale motion generation.
To this end, we introduce a large-scale motion dataset named MotionBase, which includes detailed text descriptions and rich modality annotations, providing a strong foundation for effectively training large motion models.
Our research highlights key findings, such as the impact of scaling both data and model size.
Additionally, we identify potential limitations in the current evaluation metrics, particularly when assessing diverse and unseen motions.
To enhances the benefits large motion models can derive from extensive motion data, we propose a novel motion quantization approach that treats motion clips as 2D images and constructs a finite-scale codebook, eliminating the need for token lookups.
We hope that this research offers valuable direction for future work in large-scale motion generation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn etÂ al. (2018)</span>
<span class="ltx_bibblock">
Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and Songhwai Oh.

</span>
<span class="ltx_bibblock">Text2action: Generative adversarial synthesis from language to action.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2018 IEEE International Conference on Robotics and Automation (ICRA)</em>, pp.Â  5915â€“5920. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahuja &amp; Morency (2019)</span>
<span class="ltx_bibblock">
Chaitanya Ahuja and Louis-Philippe Morency.

</span>
<span class="ltx_bibblock">Language2pose: Natural language grounded pose forecasting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2019 International Conference on 3D Vision (3DV)</em>, pp.Â  719â€“728. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aliakbarian etÂ al. (2020)</span>
<span class="ltx_bibblock">
Sadegh Aliakbarian, FatemehÂ Sadat Saleh, Mathieu Salzmann, Lars Petersson, and Stephen Gould.

</span>
<span class="ltx_bibblock">A stochastic conditioning scheme for diverse human motion prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  5223â€“5232, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain etÂ al. (2021)</span>
<span class="ltx_bibblock">
Max Bain, Arsha Nagrani, GÃ¼l Varol, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Frozen in time: A joint video and image encoder for end-to-end retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.Â  1728â€“1738, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in neural information processing systems</em>, 33:1877â€“1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al. (2018)</span>
<span class="ltx_bibblock">
Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung Tang.

</span>
<span class="ltx_bibblock">Deep video generation, prediction and completion of human action sequences.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, pp.Â  366â€“382, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cervantes etÂ al. (2022)</span>
<span class="ltx_bibblock">
Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi Shinoda.

</span>
<span class="ltx_bibblock">Implicit neural representations for variable length human motion generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">European Conference on Computer Vision</em>, pp.Â  356â€“372. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu.

</span>
<span class="ltx_bibblock">Executing your commands via motion diffusion in latent space.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  18000â€“18010, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, HyungÂ Won Chung, Charles Sutton, Sebastian Gehrmann, etÂ al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, and Chi-Keung Tang.

</span>
<span class="ltx_bibblock">Haa500: Human-centric atomic action dataset with curated videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.Â  13465â€“13474, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony MengÂ Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2305.06500</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey etÂ al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, etÂ al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fragkiadaki etÂ al. (2015)</span>
<span class="ltx_bibblock">
Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik.

</span>
<span class="ltx_bibblock">Recurrent network models for human dynamics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE international conference on computer vision</em>, pp.Â  4346â€“4354, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh etÂ al. (2017)</span>
<span class="ltx_bibblock">
Partha Ghosh, Jie Song, Emre Aksan, and Otmar Hilliges.

</span>
<span class="ltx_bibblock">Learning human motion models for long-term predictions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">2017 International Conference on 3D Vision (3DV)</em>, pp.Â  458â€“466. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gopalakrishnan etÂ al. (2019)</span>
<span class="ltx_bibblock">
Anand Gopalakrishnan, Ankur Mali, Dan Kifer, Lee Giles, and AlexanderÂ G Ororbia.

</span>
<span class="ltx_bibblock">A neural temporal model for human motion prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  12116â€“12125, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2020)</span>
<span class="ltx_bibblock">
Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and LiÂ Cheng.

</span>
<span class="ltx_bibblock">Action2motion: Conditioned generation of 3d human motions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 28th ACM International Conference on Multimedia</em>, pp.Â  2021â€“2029, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and LiÂ Cheng.

</span>
<span class="ltx_bibblock">Generating diverse and natural 3d human motions from text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  5152â€“5161, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Chuan Guo, Xinxin Zuo, Sen Wang, and LiÂ Cheng.

</span>
<span class="ltx_bibblock">Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">European Conference on Computer Vision</em>, pp.Â  580â€“597. Springer, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2024)</span>
<span class="ltx_bibblock">
Chuan Guo, Yuxuan Mu, MuhammadÂ Gohar Javed, Sen Wang, and LiÂ Cheng.

</span>
<span class="ltx_bibblock">Momask: Generative masked modeling of 3d human motions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  1900â€“1910, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta etÂ al. (2022)</span>
<span class="ltx_bibblock">
Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto MartÃ­n-MartÃ­n, and LiÂ Fei-Fei.

</span>
<span class="ltx_bibblock">Maskvit: Masked visual pre-training for video prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2206.11894</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen.

</span>
<span class="ltx_bibblock">Motiongpt: Human motion as a foreign language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in Neural Information Processing Systems</em>, 36:20067â€“20079, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al. (2022)</span>
<span class="ltx_bibblock">
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han.

</span>
<span class="ltx_bibblock">Autoregressive image generation using residual quantization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  11523â€“11532, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023a)</span>
<span class="ltx_bibblock">
BoÂ Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Mimic-it: Multi-modal in-context instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2306.05425</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2301.12597</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and YuÂ Li.

</span>
<span class="ltx_bibblock">One-stage 3d whole-body mesh recovery with component aware transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  21159â€“21168, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang.

</span>
<span class="ltx_bibblock">Motion-x: A large-scale 3d expressive whole-body human motion dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and YongÂ Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2304.08485</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhenguang Liu, Shuang Wu, Shuyuan Jin, Shouling Ji, QiÂ Liu, Shijian Lu, and LiÂ Cheng.

</span>
<span class="ltx_bibblock">Investigating pose representations and motion contexts modeling for 3d motion prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, 45(1):681â€“697, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahmood etÂ al. (2019)</span>
<span class="ltx_bibblock">
Naureen Mahmood, Nima Ghorbani, NikolausÂ F Troje, Gerard Pons-Moll, and MichaelÂ J Black.

</span>
<span class="ltx_bibblock">Amass: Archive of motion capture as surface shapes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.Â  5442â€“5451, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao etÂ al. (2019)</span>
<span class="ltx_bibblock">
Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong Li.

</span>
<span class="ltx_bibblock">Learning trajectory dependencies for human motion prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.Â  9489â€“9497, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta etÂ al. (2017)</span>
<span class="ltx_bibblock">
Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Monocular 3d human pose estimation in the wild using improved cnn supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">2017 international conference on 3D vision (3DV)</em>, pp.Â  506â€“516. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mentzer etÂ al. (2023)</span>
<span class="ltx_bibblock">
Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen.

</span>
<span class="ltx_bibblock">Finite scalar quantization: Vq-vae made simple.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2309.15505</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4o mini: advancing cost-efficient intelligence.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/" title="">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Advances in neural information processing systems</em>, 35:27730â€“27744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos etÂ al. (2019)</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, AhmedÂ AA Osman, Dimitrios Tzionas, and MichaelÂ J Black.

</span>
<span class="ltx_bibblock">Expressive body capture: 3d hands, face, and body from a single image.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  10975â€“10985, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrovich etÂ al. (2022)</span>
<span class="ltx_bibblock">
Mathis Petrovich, MichaelÂ J Black, and GÃ¼l Varol.

</span>
<span class="ltx_bibblock">Temos: Generating diverse human motions from textual descriptions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">European Conference on Computer Vision</em>, pp.Â  480â€“497. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plappert etÂ al. (2016)</span>
<span class="ltx_bibblock">
Matthias Plappert, Christian Mandery, and Tamim Asfour.

</span>
<span class="ltx_bibblock">The kit motion-language dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Big data</em>, 4(4):236â€“252, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, etÂ al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">OpenAI blog</em>, 1(8):9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">International conference on machine learning</em>, pp.Â  8748â€“8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel etÂ al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">The Journal of Machine Learning Research</em>, 21(1):5485â€“5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid etÂ al. (2024)</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, etÂ al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SÃ¡rÃ¡ndi etÂ al. (2023)</span>
<span class="ltx_bibblock">
IstvÃ¡n SÃ¡rÃ¡ndi, Alexander Hermans, and Bastian Leibe.

</span>
<span class="ltx_bibblock">Learning 3d human pose estimation from dozens of datasets using a geometry-aware autoencoder to bridge between skeleton formats.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pp.Â  2956â€“2966, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy etÂ al. (2015)</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.Â  1â€“9, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taheri etÂ al. (2020)</span>
<span class="ltx_bibblock">
Omid Taheri, Nima Ghorbani, MichaelÂ J Black, and Dimitrios Tzionas.

</span>
<span class="ltx_bibblock">Grab: A dataset of whole-body human grasping of objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part IV 16</em>, pp.Â  581â€“600. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tevet etÂ al. (2022)</span>
<span class="ltx_bibblock">
Guy Tevet, Brian Gordon, Amir Hertz, AmitÂ H Bermano, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Motionclip: Exposing human motion generation to clip space.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">European Conference on Computer Vision</em>, pp.Â  358â€“374. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, etÂ al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van DenÂ Oord etÂ al. (2017)</span>
<span class="ltx_bibblock">
Aaron Van DenÂ Oord, Oriol Vinyals, etÂ al.

</span>
<span class="ltx_bibblock">Neural discrete representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voas etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jordan Voas, Yili Wang, Qixing Huang, and Raymond Mooney.

</span>
<span class="ltx_bibblock">What is the best automated metric for text to motion generation?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">SIGGRAPH Asia 2023 Conference Papers</em>, pp.Â  1â€“11, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
YiÂ Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, etÂ al.

</span>
<span class="ltx_bibblock">Internvid: A large-scale video-text dataset for multimodal understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2307.06942</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan Zhou, Junsong Yuan, and Changyou Chen.

</span>
<span class="ltx_bibblock">Learning diverse stochastic human-action generators by learning smooth latent transitions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volumeÂ 34, pp.Â  12281â€“12288, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2024)</span>
<span class="ltx_bibblock">
QiÂ Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, and Chi-Keung Tang.

</span>
<span class="ltx_bibblock">Motionllm: Multimodal motion-language learning with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2405.17013</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Boshen Xu, Ziheng Wang, Yang Du, Sipeng Zheng, Zhinan Song, and Qin Jin.

</span>
<span class="ltx_bibblock">Egonce++: Do egocentric video-language models really understand hand-object interactions?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2405.17719</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Vitpose: Simple vision transformer baselines for human pose estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Advances in Neural Information Processing Systems</em>, 35:38571â€“38584, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas.

</span>
<span class="ltx_bibblock">Videogpt: Video generation using vq-vae and transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2104.10157</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, etÂ al.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2304.14178</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tackgeun You, Saehoon Kim, Chiheon Kim, Doyup Lee, and Bohyung Han.

</span>
<span class="ltx_bibblock">Locally hierarchical auto-regressive modeling for image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in Neural Information Processing Systems</em>, 35:16360â€“16372, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Lijun Yu, JosÃ© Lezama, NiteshÂ B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, AlexanderÂ G Hauptmann, etÂ al.

</span>
<span class="ltx_bibblock">Language model beats diffusionâ€“tokenizer is key to visual generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2310.05737</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al. (2022)</span>
<span class="ltx_bibblock">
YeÂ Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz.

</span>
<span class="ltx_bibblock">Glamr: Global occlusion-aware human mesh recovery with dynamic cameras.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  11038â€“11049, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, XiÂ Shen, and Ying Shan.

</span>
<span class="ltx_bibblock">Generating human motion from textual descriptions with discrete representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.Â  14730â€“14740, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Motiondiffuse: Text-driven human motion generation with diffusion model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2208.15001</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Siwei Zhang, BharatÂ Lal Bhatnagar, Yuanlu Xu, Alexander Winkler, Petr Kadlecek, Siyu Tang, and Federica Bogo.

</span>
<span class="ltx_bibblock">Rohm: Robust human motion reconstruction via diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  14606â€“14617, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2306.17107</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Yaqi Zhang, DiÂ Huang, Bin Liu, Shixiang Tang, Yan Lu, LuÂ Chen, Lei Bai, QiÂ Chu, Nenghai Yu, and Wanli Ouyang.

</span>
<span class="ltx_bibblock">Motiongpt: Finetuned llms are general-purpose motion generators.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volumeÂ 38, pp.Â  7368â€“7376, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2023)</span>
<span class="ltx_bibblock">
BoÂ Zhao, Boya Wu, and Tiejun Huang.

</span>
<span class="ltx_bibblock">Svit: Scaling up visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2307.04087</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Sipeng Zheng, Yicheng Feng, Zongqing Lu, etÂ al.

</span>
<span class="ltx_bibblock">Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Sipeng Zheng, Bohan Zhou, Yicheng Feng, YeÂ Wang, and Zongqing Lu.

</span>
<span class="ltx_bibblock">Unicode: Learning a unified codebook for multimodal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2403.09072</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zixiang Zhou, YuÂ Wan, and Baoyuan Wang.

</span>
<span class="ltx_bibblock">Avatargpt: All-in-one framework for motion understanding planning generation and beyond.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.Â  1357â€“1366, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\appendixpage</span>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Details of MoseBase</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we provide more details about <span class="ltx_text ltx_font_bold" id="A1.p1.1.1">Motionbase</span> that are not included in the main paper due to spatial limitations.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Statistic Analyses</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">MotionBase contains over 1 million motion sequences from 42 different public datasets and web videos on the Internet.
Subsets of MotionX, including Animation, Perform, Dance, Aist, Kungfu, GRABÂ <cite class="ltx_cite ltx_citemacro_citep">(Taheri etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib44" title="">2020</a>)</cite>, Music, Idea400Â <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>, HAA500Â <cite class="ltx_cite ltx_citemacro_citep">(Chung etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib10" title="">2021</a>)</cite>, Game Motion, and Fitness, are included in MotionBase.
Recognizing the high cost of collecting and annotating videos, we also see the untapped potential of images for motion understanding.
Consequently, MotionBase incorporates image data by repeating each image across 64 frames and treating it as a motion sequence.
For the datasets with long-range videos, such as MPI-INF-3DHPÂ <cite class="ltx_cite ltx_citemacro_citep">(Mehta etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib31" title="">2017</a>)</cite>, we segment the footage into sub-clips with random durations ranging from 10 seconds to one minute.
FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F6" title="Figure 6 â€£ A.1 Statistic Analyses â€£ Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">6</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F7" title="Figure 7 â€£ A.1 Statistic Analyses â€£ Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">7</span></a> illustrate the scale and length distributions of MotionBase.</p>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="575" id="A1.F6.g1" src="x6.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The scale distribution of motion sequences across subsets of MotionBase.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="576" id="A1.F7.g1" src="x7.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The length distribution across different subsets of MotionBase</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1123" id="A1.F8.g1" src="x8.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Prompt examples to label human motions in the video. We use Gemini-1.5-pro and GPT-4o-mini to generate motion descriptions for the video and image data, respectively. We provide â€œwhole-bodyâ€ (UP) and â€œpart-levelâ€ (DOWN) labels for each sample in the dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Prompt of Motion Description</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In this paper, we use Gemini-1.5-proÂ <cite class="ltx_cite ltx_citemacro_citep">(Reid etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib41" title="">2024</a>)</cite> and GPT-4o-miniÂ <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib33" title="">2024</a>)</cite> as large multimodal models (LMM) to generate textual annotations for video and image data, respectively.
For each person-centric sample, we first crop and track the personâ€™s body using the corresponding bounding box(es).
The LMM is then tasked with focusing on the personâ€™s physical movements and positions in the global space to generate detailed descriptions.
Unlike previous datasets, we provide more granular motion descriptions by dividing the body into upper and lower sections, prompting the LMM to generate part-specific descriptions (â€œpart-levelâ€).
Additionally, an overall summary of the entire bodyâ€™s movement (â€œwhole-bodyâ€) is also produced.
FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F8" title="Figure 8 â€£ A.1 Statistic Analyses â€£ Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates the prompt used to caption human motion sequences in MotionBase.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Word Distribution Analysis</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">To further explore the annotated motion text, we generate word clouds from the entire text corpus in MotionBase.
Since the annotations in MotionBase consist of both whole-body and part-level descriptions, we create separate word clouds for general labels and more detailed annotations, as shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F9" title="Figure 9 â€£ A.3 Word Distribution Analysis â€£ Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">9</span></a> and FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F10" title="Figure 10 â€£ A.3 Word Distribution Analysis â€£ Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">10</span></a>, respectively.
In FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F9" title="Figure 9 â€£ A.3 Word Distribution Analysis â€£ Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">9</span></a>, we observe that the whole-body annotations primarily highlight high-level motion activities, such as standing, sitting, and walking.
In contrast, FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F10" title="Figure 10 â€£ A.3 Word Distribution Analysis â€£ Appendix A Additional Details of MoseBase â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">10</span></a> shows that part-level annotations focus more on specific body movements, including the torso, shoulders, legs, and arms.
We believe that this hierarchical structure of annotations will enhance the understanding of motion.</p>
</div>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="361" id="A1.F9.g1" src="x9.png" width="722"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Word cloud of whole-body textual annotation in MotionBase.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="363" id="A1.F10.g1" src="x10.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Word cloud of part-level textual annotation in MotionBase.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Overview of Model Architecture</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Due to space limitations in the main paper, we provide the overview of our model architecture in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A2.F11" title="Figure 11 â€£ Appendix B Additional Overview of Model Architecture â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">11</span></a> in this appendix.
Following most LMMs, our large motion model consists of two stages: pre-training and fine-tuning.
During the pre-training stage, we train a motion encoder, a motion decoder, and a motion codebook to represent motions using discrete tokens.
With this motion tokenizer, we fine-tune an autoregressive language model to predict motion tokens.
In the inference stage, the input text is processed by the language model to generate motion tokens in an autoregressive manner, which are then decoded into natural motion by the pre-trained motion decoder.</p>
</div>
<figure class="ltx_figure" id="A2.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="175" id="A2.F11.g1" src="x11.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Overview of the large motion model, which can be divided into two stages. In the first stage(<span class="ltx_text ltx_font_bold" id="A2.F11.3.1">left</span>), we pre-train a motion VQ-VAE to quantify motion sequences into tokens. In the second stage(<span class="ltx_text ltx_font_bold" id="A2.F11.4.2">right</span>), we fine-tune an autoregressive language model to predict motion tokens. </figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Experimental Results</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In this section, we provide more experimental analysis which can not be presented in our main paper due to space limitation.</p>
</div>
<figure class="ltx_table" id="A3.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Ablation of the effectiveness of synthetic data and static data.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T7.4" style="width:270.8pt;height:76.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.1pt,2.0pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T7.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T7.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T7.4.4.4.5" style="padding-left:7.0pt;padding-right:7.0pt;">TRAIN SET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.1.1.1.1" style="padding-left:7.0pt;padding-right:7.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T7.1.1.1.1.m1.1"><semantics id="A3.T7.1.1.1.1.m1.1a"><mo id="A3.T7.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T7.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T7.1.1.1.1.m1.1b"><ci id="A3.T7.1.1.1.1.m1.1.1.cmml" xref="A3.T7.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.1.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.2.2.2.2" style="padding-left:7.0pt;padding-right:7.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T7.2.2.2.2.m1.1"><semantics id="A3.T7.2.2.2.2.m1.1a"><mo id="A3.T7.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T7.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T7.2.2.2.2.m1.1b"><ci id="A3.T7.2.2.2.2.m1.1.1.cmml" xref="A3.T7.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.2.2.2.2.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.3.3.3.3" style="padding-left:7.0pt;padding-right:7.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T7.3.3.3.3.m1.1"><semantics id="A3.T7.3.3.3.3.m1.1a"><mo id="A3.T7.3.3.3.3.m1.1.1" stretchy="false" xref="A3.T7.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T7.3.3.3.3.m1.1b"><ci id="A3.T7.3.3.3.3.m1.1.1.cmml" xref="A3.T7.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.4.4.4.4" style="padding-left:7.0pt;padding-right:7.0pt;">MMDist <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T7.4.4.4.4.m1.1"><semantics id="A3.T7.4.4.4.4.m1.1a"><mo id="A3.T7.4.4.4.4.m1.1.1" stretchy="false" xref="A3.T7.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T7.4.4.4.4.m1.1b"><ci id="A3.T7.4.4.4.4.m1.1.1.cmml" xref="A3.T7.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.4.4.4.4.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T7.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T7.4.4.5.1.1" style="padding-left:7.0pt;padding-right:7.0pt;">w/o static &amp; syn</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.4.5.1.2" style="padding-left:7.0pt;padding-right:7.0pt;">0.101</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.4.5.1.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.231</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.4.5.1.4" style="padding-left:7.0pt;padding-right:7.0pt;">261.325</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.4.5.1.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.4.4.5.1.5.1">5.201</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.4.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T7.4.4.6.2.1" style="padding-left:7.0pt;padding-right:7.0pt;">w/o static</th>
<td class="ltx_td ltx_align_center" id="A3.T7.4.4.6.2.2" style="padding-left:7.0pt;padding-right:7.0pt;">0.110</td>
<td class="ltx_td ltx_align_center" id="A3.T7.4.4.6.2.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.248</td>
<td class="ltx_td ltx_align_center" id="A3.T7.4.4.6.2.4" style="padding-left:7.0pt;padding-right:7.0pt;">286.809</td>
<td class="ltx_td ltx_align_center" id="A3.T7.4.4.6.2.5" style="padding-left:7.0pt;padding-right:7.0pt;">5.213</td>
</tr>
<tr class="ltx_tr" id="A3.T7.4.4.7.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T7.4.4.7.3.1" style="padding-left:7.0pt;padding-right:7.0pt;">MotionBase</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.4.4.7.3.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.4.4.7.3.2.1">0.118</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.4.4.7.3.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.4.4.7.3.3.1">0.269</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.4.4.7.3.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.4.4.7.3.4.1">121.917</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.4.4.7.3.5" style="padding-left:7.0pt;padding-right:7.0pt;">7.644</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Ablation of Synthesis and Static Data?</h3>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">To assess the effectiveness of synthetic and static data, we conduct a series of ablation experiments.
We train GPT2-medium on three variations of MotionBase: without synthetic data, without image data, and without both synthetic data and image data.
The model is trained for 300 epochs with a learning rate of 2e-4.
Performance is tested on the Motion-X test set using the VQ-VAE and retrieval model trained on MotionBase, with results shown in TableÂ <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:syn_and_static</span>.
Our findings indicate that incorporating both static data (i.e., image data) and synthetic data leads to performance improvements in terms of R-Precision.
Additionally, we observe that the trend of MMDist is opposite to that of R-Precision.
This could be attributed to MMDistâ€™s sensitivity to the quality of the embedding space.
When the motion and text encoders have limited capacity, this metric may struggle to discern the quality of generated motions.
This phenomenon highlights the importance of developing more robust evaluation metrics and models.</p>
</div>
<figure class="ltx_table" id="A3.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison of evaluations using different encoder models.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T8.6" style="width:368.5pt;height:171pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.7pt,4.5pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T8.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T8.6.6.7.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="3" id="A3.T8.6.6.7.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_tt" colspan="3" id="A3.T8.6.6.7.1.2">EM_Humanml3d</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_tt" colspan="3" id="A3.T8.6.6.7.1.3">EM_Motion-X</th>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.6.6.6.7">Decoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.6.6.6.8">#Inst.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T8.6.6.6.9">#Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.1.1.1.1">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T8.1.1.1.1.m1.1"><semantics id="A3.T8.1.1.1.1.m1.1a"><mo id="A3.T8.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T8.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T8.1.1.1.1.m1.1b"><ci id="A3.T8.1.1.1.1.m1.1.1.cmml" xref="A3.T8.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.1.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.2.2.2.2">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T8.2.2.2.2.m1.1"><semantics id="A3.T8.2.2.2.2.m1.1a"><mo id="A3.T8.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T8.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T8.2.2.2.2.m1.1b"><ci id="A3.T8.2.2.2.2.m1.1.1.cmml" xref="A3.T8.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.2.2.2.2.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T8.3.3.3.3">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T8.3.3.3.3.m1.1"><semantics id="A3.T8.3.3.3.3.m1.1a"><mo id="A3.T8.3.3.3.3.m1.1.1" stretchy="false" xref="A3.T8.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T8.3.3.3.3.m1.1b"><ci id="A3.T8.3.3.3.3.m1.1.1.cmml" xref="A3.T8.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.4.4.4.4">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T8.4.4.4.4.m1.1"><semantics id="A3.T8.4.4.4.4.m1.1a"><mo id="A3.T8.4.4.4.4.m1.1.1" stretchy="false" xref="A3.T8.4.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T8.4.4.4.4.m1.1b"><ci id="A3.T8.4.4.4.4.m1.1.1.cmml" xref="A3.T8.4.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.4.4.4.4.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.5.5.5.5">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T8.5.5.5.5.m1.1"><semantics id="A3.T8.5.5.5.5.m1.1a"><mo id="A3.T8.5.5.5.5.m1.1.1" stretchy="false" xref="A3.T8.5.5.5.5.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T8.5.5.5.5.m1.1b"><ci id="A3.T8.5.5.5.5.m1.1.1.cmml" xref="A3.T8.5.5.5.5.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.5.5.5.5.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.6.6.6.6">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T8.6.6.6.6.m1.1"><semantics id="A3.T8.6.6.6.6.m1.1a"><mo id="A3.T8.6.6.6.6.m1.1.1" stretchy="false" xref="A3.T8.6.6.6.6.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T8.6.6.6.6.m1.1b"><ci id="A3.T8.6.6.6.6.m1.1.1.cmml" xref="A3.T8.6.6.6.6.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.6.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.6.6.6.6.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T8.6.6.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.8.1.1">GPT-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.8.1.2">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.8.1.3">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.8.1.4">0.466</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.8.1.5">0.752</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.8.1.6"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.8.1.6.1">0.101</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.8.1.7">0.358</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.8.1.8">0.651</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.8.1.9"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.8.1.9.1">0.050</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.9.2.1">GPT-2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.9.2.2">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.9.2.3">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.9.2.4">0.462</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.9.2.5">0.744</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.9.2.6">0.208</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.9.2.7">0.362</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.9.2.8">0.656</td>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.9.2.9">0.754</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.10.3.1">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.10.3.2">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.10.3.3">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.10.3.4">0.497</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.10.3.5">0.778</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.10.3.6">0.214</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.10.3.7">0.378</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.10.3.8">0.671</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.10.3.9">0.122</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.11.4.1">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.11.4.2">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.11.4.3">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.11.4.4">0.474</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.11.4.5">0.758</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.11.4.6">0.452</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.11.4.7">0.376</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.11.4.8">0.673</td>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.11.4.9">0.518</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.12.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.12.5.1">LLaMA-3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.12.5.2">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.12.5.3">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.12.5.4">0.500</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.12.5.5">0.783</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.12.5.6">0.173</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.12.5.7">0.380</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.12.5.8">0.675</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.12.5.9">0.094</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.13.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.13.6.1">LLaMA-3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.13.6.2">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.13.6.3">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.13.6.4">0.499</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.13.6.5">0.786</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.13.6.6">0.264</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.13.6.7">0.393</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.13.6.8">0.696</td>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.13.6.9">0.591</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.14.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.14.7.1">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.14.7.2">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.14.7.3">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.14.7.4"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.14.7.4.1">0.519</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.14.7.5"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.14.7.5.1">0.803</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.14.7.6">0.166</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.14.7.7">0.395</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.14.7.8">0.695</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.14.7.9">0.105</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.15.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T8.6.6.15.8.1">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.6.6.15.8.2">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T8.6.6.15.8.3">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T8.6.6.15.8.4">0.504</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.6.6.15.8.5">0.790</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T8.6.6.15.8.6">0.393</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T8.6.6.15.8.7"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.15.8.7.1">0.400</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.6.6.15.8.8"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.15.8.8.1">0.700</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.6.6.15.8.9">0.637</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Ablation of Different Encoder Models</h3>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.T8" title="Table 8 â€£ C.1 Ablation of Synthesis and Static Data? â€£ Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">8</span></a> presents the evaluation results on the HumanML3D test set using different encoder models (EM).
We employ the same dual-encoder architectureÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> but trained it on two distinct datasets: HumanML3D and Motion-X, where HumanML3D is a subset of Motion-X.
The results highlight the limited generalization ability of the encoder model.
When using the model trained on the larger Motion-X dataset, performance metrics on HumanML3D decrease.
This suggests that training on the broader Motion-X dataset negatively impacts R-Precision performance on the HumanML3D subset.
Furthermore, when the encoder model is trained on Motion-X, increasing the training data size for the text-to-motion model leads to significant performance gains.
Conversely, when using the encoder model trained on HumanML3D, the performance of the text-to-motion model degrades as the training data size increases.
This might be attributed to inherent limitations in the encoder model itself.</p>
</div>
<figure class="ltx_table" id="A3.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Comparison between fine-tuning and learning from scratch on the Motion-X test set.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T9.4" style="width:279.4pt;height:94pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.4pt,2.5pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T9.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T9.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T9.4.4.4.5">#Inst</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A3.T9.4.4.4.6">From Sctrach</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T9.1.1.1.1">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T9.1.1.1.1.m1.1"><semantics id="A3.T9.1.1.1.1.m1.1a"><mo id="A3.T9.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T9.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T9.1.1.1.1.m1.1b"><ci id="A3.T9.1.1.1.1.m1.1.1.cmml" xref="A3.T9.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T9.1.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T9.2.2.2.2">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T9.2.2.2.2.m1.1"><semantics id="A3.T9.2.2.2.2.m1.1a"><mo id="A3.T9.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T9.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T9.2.2.2.2.m1.1b"><ci id="A3.T9.2.2.2.2.m1.1.1.cmml" xref="A3.T9.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T9.2.2.2.2.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T9.3.3.3.3">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T9.3.3.3.3.m1.1"><semantics id="A3.T9.3.3.3.3.m1.1a"><mo id="A3.T9.3.3.3.3.m1.1.1" stretchy="false" xref="A3.T9.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T9.3.3.3.3.m1.1b"><ci id="A3.T9.3.3.3.3.m1.1.1.cmml" xref="A3.T9.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T9.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T9.4.4.4.4">MMDist <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T9.4.4.4.4.m1.1"><semantics id="A3.T9.4.4.4.4.m1.1a"><mo id="A3.T9.4.4.4.4.m1.1.1" stretchy="false" xref="A3.T9.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T9.4.4.4.4.m1.1b"><ci id="A3.T9.4.4.4.4.m1.1.1.cmml" xref="A3.T9.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T9.4.4.4.4.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T9.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.4.4.5.1.1">0.02M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T9.4.4.5.1.2">Yes</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.5.1.3">0.035</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.5.1.4">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.5.1.5">16.904</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.5.1.6">9.280</td>
</tr>
<tr class="ltx_tr" id="A3.T9.4.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.4.4.6.2.1">0.02M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T9.4.4.6.2.2">No</th>
<td class="ltx_td ltx_align_center" id="A3.T9.4.4.6.2.3">0.206</td>
<td class="ltx_td ltx_align_center" id="A3.T9.4.4.6.2.4">0.402</td>
<td class="ltx_td ltx_align_center" id="A3.T9.4.4.6.2.5">54.017</td>
<td class="ltx_td ltx_align_center" id="A3.T9.4.4.6.2.6">8.218</td>
</tr>
<tr class="ltx_tr" id="A3.T9.4.4.7.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.4.4.7.3.1">0.08M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T9.4.4.7.3.2">Yes</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.7.3.3">0.460</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.7.3.4">0.782</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.7.3.5">0.113</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.7.3.6">2.862</td>
</tr>
<tr class="ltx_tr" id="A3.T9.4.4.8.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T9.4.4.8.4.1">0.08M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A3.T9.4.4.8.4.2">No</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.4.4.8.4.3">0.468</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.4.4.8.4.4">0.791</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.4.4.8.4.5">0.096</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.4.4.8.4.6">2.798</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Ablation of Learning from Scratch vs. Fine-tuning</h3>
<div class="ltx_para ltx_noindent" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">We compare the performance of fine-tuning GPT-2 against training it from scratch (random initialization).
The results show that fine-tuned models consistently outperform those trained from scratch, particularly when trained on HumanML3D and evaluated on MotionX.
The improvement of pretrained LLM highlights the importance of text pre-training in enhancing the modelâ€™s understanding of text descriptions and improving its generalization capabilities.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Ablation of Different Loss Calculation Strategies</h3>
<figure class="ltx_table" id="A3.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Results of different loss calculation methods on the HumanML3D test set.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T10.4" style="width:253.8pt;height:59.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.7pt,1.6pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T10.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T10.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T10.4.4.4.5">Loss Calculation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T10.1.1.1.1.m1.1"><semantics id="A3.T10.1.1.1.1.m1.1a"><mo id="A3.T10.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T10.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T10.1.1.1.1.m1.1b"><ci id="A3.T10.1.1.1.1.m1.1.1.cmml" xref="A3.T10.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.1.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.2.2.2.2">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T10.2.2.2.2.m1.1"><semantics id="A3.T10.2.2.2.2.m1.1a"><mo id="A3.T10.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T10.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A3.T10.2.2.2.2.m1.1b"><ci id="A3.T10.2.2.2.2.m1.1.1.cmml" xref="A3.T10.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.2.2.2.2.m1.1d">â†‘</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.3.3.3.3">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T10.3.3.3.3.m1.1"><semantics id="A3.T10.3.3.3.3.m1.1a"><mo id="A3.T10.3.3.3.3.m1.1.1" stretchy="false" xref="A3.T10.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T10.3.3.3.3.m1.1b"><ci id="A3.T10.3.3.3.3.m1.1.1.cmml" xref="A3.T10.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.4.4.4.4">MMDist <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T10.4.4.4.4.m1.1"><semantics id="A3.T10.4.4.4.4.m1.1a"><mo id="A3.T10.4.4.4.4.m1.1.1" stretchy="false" xref="A3.T10.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T10.4.4.4.4.m1.1b"><ci id="A3.T10.4.4.4.4.m1.1.1.cmml" xref="A3.T10.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.4.4.4.4.m1.1d">â†“</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T10.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T10.4.4.5.1.1">Motion Seq Loss</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.4.4.5.1.2">0.388</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.4.4.5.1.3">0.650</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.4.4.5.1.4">0.680</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.4.4.5.1.5">3.919</td>
</tr>
<tr class="ltx_tr" id="A3.T10.4.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T10.4.4.6.2.1">Whole Seq Loss</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.4.4.6.2.2">0.466</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.4.4.6.2.3">0.752</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.4.4.6.2.4">0.101</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.4.4.6.2.5">3.234</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="A3.SS4.p1">
<p class="ltx_p" id="A3.SS4.p1.1">We also investigate the impact of different loss calculation strategies on model performance:
We compare two strategies:
1) calculating the loss solely on the output motion tokens, and
2) calculating the loss on both the input text and the output motion tokens.
As shown in TableÂ <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:training_obj</span>, our results indicate that the second strategy yields better performance.
This improvement compared to the first alternative is likely due to the strategyâ€™s ability to prevent catastrophic forgetting of text understanding.
Additionally, it helps mitigate overfitting to motion patterns in the training data, thereby enhancing the modelâ€™s generalization ability.</p>
</div>
<figure class="ltx_figure" id="A3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="A3.F12.g1" src="x12.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Comparison with different motion quantization on the Motion-X (<span class="ltx_text ltx_font_bold" id="A3.F12.5.1">left</span>) and MotionBase dataset (<span class="ltx_text ltx_font_bold" id="A3.F12.6.2">right</span>). The Y-axis denotes FID (<math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.F12.2.m1.1"><semantics id="A3.F12.2.m1.1b"><mo id="A3.F12.2.m1.1.1" stretchy="false" xref="A3.F12.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.F12.2.m1.1c"><ci id="A3.F12.2.m1.1.1.cmml" xref="A3.F12.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F12.2.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.F12.2.m1.1e">â†“</annotation></semantics></math>).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.5 </span>Ablation of Motion Quantization</h3>
<div class="ltx_para ltx_noindent" id="A3.SS5.p1">
<p class="ltx_p" id="A3.SS5.p1.1">First, we provide additional FID results on Motion-X in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.F12" title="Figure 12 â€£ C.4 Ablation of Different Loss Calculation Strategies â€£ Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">12</span></a>.
It is worth noting that while our motion quantizer performs worse than RQ-VAE on the smaller HumanML3D dataset, it surpasses both VQ and RQ when evaluated on the larger Motion-X and MotionBase benchmarks, as can be seen in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T6" title="Table 6 â€£ 5.4 Limitation of Automated Metric â€£ 5 Experiments â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">6</span></a>.
This suggests that our approach offers a greater advantage when applied to larger datasets, highlighting its improved generalization compared to previous methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS5.p2">
<p class="ltx_p" id="A3.SS5.p2.1">To further validate the effectiveness of our 2D quantization strategy, we compare the 2D-LFQ method with its 1D counterpart (which is identical to VQ except for the quantization strategy).
The results, shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.T11" title="Table 11 â€£ C.5 Ablation of Motion Quantization â€£ Appendix C Additional Experimental Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">11</span></a>, demonstrate that 2D quantization in LFQ significantly outperforms the 1D version.
This highlights the superior ability of 2D quantization to enhance the representational capacity of the motion tokenizer.</p>
</div>
<figure class="ltx_table" id="A3.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Ablation of 2D motion quantization vs. its 1D version.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T11.2" style="width:352.6pt;height:61.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.1pt,5.4pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T11.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T11.2.2.3.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A3.T11.2.2.3.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="A3.T11.2.2.3.1.2">HumanML3D</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="A3.T11.2.2.3.1.3">Motion-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="A3.T11.2.2.3.1.4">MotionBase</th>
</tr>
<tr class="ltx_tr" id="A3.T11.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.3">Tokenizer</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.4">#Num.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T11.2.2.2.5">#Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.1.1.1.1">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T11.1.1.1.1.m1.1"><semantics id="A3.T11.1.1.1.1.m1.1a"><mo id="A3.T11.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T11.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T11.1.1.1.1.m1.1b"><ci id="A3.T11.1.1.1.1.m1.1.1.cmml" xref="A3.T11.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T11.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T11.1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T11.2.2.2.2">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T11.2.2.2.2.m1.1"><semantics id="A3.T11.2.2.2.2.m1.1a"><mo id="A3.T11.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T11.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A3.T11.2.2.2.2.m1.1b"><ci id="A3.T11.2.2.2.2.m1.1.1.cmml" xref="A3.T11.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T11.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T11.2.2.2.2.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.6">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T11.2.2.2.7">MPJPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.8">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.9">MPJPE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T11.2.2.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.1">1D-LFQ</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.2">16384</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T11.2.2.4.1.3">19.43M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.4">3.85</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T11.2.2.4.1.5">52.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.6">2.783</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T11.2.2.4.1.7">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.8">10.358</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.9">80.1</td>
</tr>
<tr class="ltx_tr" id="A3.T11.2.2.5.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.1">2D-LFQ</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.2">16384</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T11.2.2.5.2.3">108.35M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.4"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.4.1">1.769</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T11.2.2.5.2.5"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.5.1">45.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.6"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.6.1">0.295</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T11.2.2.5.2.7"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.7.1">54.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.8"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.8.1">7.853</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.9"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.9.1">64.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Quantitative Results</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We provide some examples to visualize the human motions predicted by our large motion model trained on MotionBase, as illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A4.F13" title="Figure 13 â€£ Appendix D Additional Quantitative Results â€£ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">13</span></a>.
As can be seen, our large motion model is capable of generating motion sequences that align well with the input texts, demonstrating the effectiveness of the MotionBase dataset.</p>
</div>
<figure class="ltx_figure" id="A4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="580" id="A4.F13.g1" src="x13.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Quantitative examples of motions generated by our large motion model.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 10:46:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
