<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models</title>
<!--Generated on Fri Oct  4 10:46:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03311v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S1" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2.SS1" title="In 2 Related Work ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Large Language Models and Multi-Modality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2.SS2" title="In 2 Related Work ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Vector Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2.SS3" title="In 2 Related Work ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Human Motion Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S3" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MotionBase Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S4" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Scaling up Large Motion Model</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S4.SS1" title="In 4 Scaling up Large Motion Model ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Overall Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S4.SS2" title="In 4 Scaling up Large Motion Model ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>2D Lookup-free Motion Quantization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS1" title="In 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS2" title="In 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Discussion of Scaling up motion generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS3" title="In 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Discussion of Motion Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS4" title="In 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Limitation of Automated Metric</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S6" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Details of MoseBase</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.SS1" title="In Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Statistic Analyses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.SS2" title="In Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Prompt of Motion Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.SS3" title="In Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Word Distribution Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A2" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Overview of Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS1" title="In Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Ablation of Synthesis and Static Data?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS2" title="In Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Ablation of Different Encoder Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS3" title="In Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Ablation of Learning from Scratch vs. Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS4" title="In Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Ablation of Different Loss Calculation Strategies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS5" title="In Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.5 </span>Ablation of Motion Quantization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A4" title="In Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Additional Quantitative Results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ye Wang<sup class="ltx_sup" id="id12.12.id1"><span class="ltx_text ltx_font_italic" id="id12.12.id1.1">1</span></sup>,   Sipeng Zheng<sup class="ltx_sup" id="id13.13.id2"><span class="ltx_text ltx_font_italic" id="id13.13.id2.1">2</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>,  Bin Cao<sup class="ltx_sup" id="id14.14.id3"><span class="ltx_text ltx_font_italic" id="id14.14.id3.1">2,3</span></sup>, Qianshan Wei<sup class="ltx_sup" id="id15.15.id4"><span class="ltx_text ltx_font_italic" id="id15.15.id4.1">4</span></sup>, Qin Jin<sup class="ltx_sup" id="id16.16.id5">1</sup>, Zongqing Lu<sup class="ltx_sup" id="id17.17.id6"><span class="ltx_text ltx_font_italic" id="id17.17.id6.1">5</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id18.18.id7">1</sup>Renmin University
<br class="ltx_break"/><sup class="ltx_sup" id="id19.19.id8">2</sup>Beijing Academy of Artificial Intelligence
<br class="ltx_break"/><sup class="ltx_sup" id="id20.20.id9">3</sup>Institute of Automation, Chinese Academy of Sciences
<br class="ltx_break"/><sup class="ltx_sup" id="id21.21.id10">4</sup>Southeast University
<br class="ltx_break"/><sup class="ltx_sup" id="id22.22.id11">5</sup>School of Computer Science, Peking University
</span><span class="ltx_author_notes">Equal contribution. Ye Wang works as an intern at BAAICorrespondence to Zongqing Lu &lt;zongqing.lu@pku.edu.cn&gt;.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id23.id1">Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted towards the development of large motion models.
Despite some progress, current state-of-the-art works remain far from achieving truly generalist models, largely due to the lack of large-scale, high-quality motion data.
To address this, we present MotionBase, the first million-level motion generation benchmark, offering 15 times the data volume of the previous largest dataset, and featuring multimodal data with hierarchically detailed text descriptions.
By leveraging this vast dataset, our large motion model demonstrates strong performance across a broad range of motions, including unseen ones.
Through systematic investigation, we underscore the importance of scaling both data and model size, with synthetic data and pseudo labels playing a crucial role in mitigating data acquisition costs.
Moreover, our research reveals the limitations of existing evaluation metrics, particularly in handling out-of-domain text instructions — an issue that has long been overlooked.
In addition to these, we introduce a novel 2D lookup-free approach for motion tokenization, which preserves motion information and expands codebook capacity, further enhancing the representative ability of large motion models.
The release of MotionBase and the insights gained from this study are expected to pave the way for the development of more powerful and versatile motion generation models.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Motion generation is an emerging field with diverse applications in video games, filmmaking, and robotics animation.
At the forefront of this area is text-to-motion generation (T2M) <cite class="ltx_cite ltx_citemacro_citep">(Ahn et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib1" title="">2018</a>; Ahuja &amp; Morency, <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib2" title="">2019</a>)</cite>, which plays a crucial role in translating natural language into human motions.
State-of-the-art T2M models typically rely on a combination of the motion quantization methods (e.g., VQ <cite class="ltx_cite ltx_citemacro_citep">(Van Den Oord et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib48" title="">2017</a>)</cite>), along with a text encoder (e.g., CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib39" title="">2021</a>)</cite>) and decoder (e.g., GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib38" title="">2019</a>)</cite>) to generate motion sequences from detailed textual instructions.
Despite the availability of a few high-quality datasets <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>; Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite> curated in recent years, their limited size restricts current methods to a narrow range of scenarios, creating performance bottlenecks when addressing diverse or unseen motions, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">1</span></a> (RIGHT).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The rapid advancement of large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib46" title="">2023a</a>)</cite> in multimodal learning has been significantly bolstered by the availability of vast data resources <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib67" title="">2024</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib53" title="">2024</a>)</cite>.
In contrast, the volume of motion data remains considerably smaller than that of visual-text data, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">1</span></a> (LEFT).
This disparity primarily arises from the high costs associated with motion data collection, which often requires specialized wearable devices and substantial human labor for annotation.
Consequently, developing a state-of-the-art (SoTA) large motion model based on LLMs presents a significant challenge and remains an unresolved issue.
While some recent efforts <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib21" title="">2023</a>)</cite> have explored this direction, the effectiveness of large motion models has yet to be fully demonstrated.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we aim to address the question: “<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p3.1.1">Can a large motion model be a promising direction for motion generation?</span>”
To tackle this, we have developed a systematic data collection scheme that led to the creation of MotionBase, the first large-scale dataset containing over one million motion sequences — 15 times larger than the previous largest dataset.
This initiative provides a solid foundation for building robust, universally applicable large motion models and offers a comprehensive testbed for future research.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="276" id="S1.F1.g1" src="x1.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span class="ltx_text ltx_font_bold" id="S1.F1.6.1">LEFT</span>: Curves showing the effects of scaling up large motion models. MotionBase is the first large text-to-motion dataset comparable in scale to visual benchmarks like ImageNet.
<span class="ltx_text ltx_font_bold" id="S1.F1.7.2">RIGHT</span>: While existing models perform well on constrained datasets like <span class="ltx_text" id="S1.F1.8.3" style="color:#FF8000;">Motion-X</span> and <span class="ltx_text" id="S1.F1.9.4" style="color:#BF0040;">HumanML3D</span>, they struggle with out-of-domain concepts on <span class="ltx_text" id="S1.F1.10.5" style="color:#00FFFF;">MotionBase</span>, exhibiting limited generalization.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Building on the solid foundation of MotionBase, we can now conduct a comprehensive investigation into the effectiveness of large motion models.
This research aims to firstly identify key factors driving their advancement and offer valuable insights for future model design, including:
❶ scaling both data and model size significantly reduces joint prediction errors on critical metrics while improving generalization to novel motions.
❷ Despite observable domain gaps, synthetic and static data, as well as pseudo motion labels are becoming increasingly essential and effective, especially given the high cost of acquiring ground truth motion data.
❸ Existing metrics show limitations when faced with out-of-domain text instructions.
Notably, the widely used metric, FID, fails to accurately capture the alignment between ground truth and generated motions.
Our findings highlight the need for a more robust and equitable evaluation framework that enhances open-set generalization.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.7">In addition to these factors, we argue that large motion models are further constrained by inadequate motion representation.
Most approaches rely on transforming motion into discrete tokens via vector quantization (VQ), which are then processed by autoregressive models to generate motion sequences.
While these methods have produced impressive results, they suffer from two major drawbacks.
❶ <span class="ltx_text ltx_font_bold" id="S1.p5.7.1">Information loss</span>:
The current VQ process inevitably leads to the loss of critical information.
Given a motion clip with <math alttext="D" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mi id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">italic_D</annotation></semantics></math>-dimensional features <math alttext="\mathcal{M}=\{m_{1},m_{2},...,m_{T}\}" class="ltx_Math" display="inline" id="S1.p5.2.m2.4"><semantics id="S1.p5.2.m2.4a"><mrow id="S1.p5.2.m2.4.4" xref="S1.p5.2.m2.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S1.p5.2.m2.4.4.5" xref="S1.p5.2.m2.4.4.5.cmml">ℳ</mi><mo id="S1.p5.2.m2.4.4.4" xref="S1.p5.2.m2.4.4.4.cmml">=</mo><mrow id="S1.p5.2.m2.4.4.3.3" xref="S1.p5.2.m2.4.4.3.4.cmml"><mo id="S1.p5.2.m2.4.4.3.3.4" stretchy="false" xref="S1.p5.2.m2.4.4.3.4.cmml">{</mo><msub id="S1.p5.2.m2.2.2.1.1.1" xref="S1.p5.2.m2.2.2.1.1.1.cmml"><mi id="S1.p5.2.m2.2.2.1.1.1.2" xref="S1.p5.2.m2.2.2.1.1.1.2.cmml">m</mi><mn id="S1.p5.2.m2.2.2.1.1.1.3" xref="S1.p5.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S1.p5.2.m2.4.4.3.3.5" xref="S1.p5.2.m2.4.4.3.4.cmml">,</mo><msub id="S1.p5.2.m2.3.3.2.2.2" xref="S1.p5.2.m2.3.3.2.2.2.cmml"><mi id="S1.p5.2.m2.3.3.2.2.2.2" xref="S1.p5.2.m2.3.3.2.2.2.2.cmml">m</mi><mn id="S1.p5.2.m2.3.3.2.2.2.3" xref="S1.p5.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S1.p5.2.m2.4.4.3.3.6" xref="S1.p5.2.m2.4.4.3.4.cmml">,</mo><mi id="S1.p5.2.m2.1.1" mathvariant="normal" xref="S1.p5.2.m2.1.1.cmml">…</mi><mo id="S1.p5.2.m2.4.4.3.3.7" xref="S1.p5.2.m2.4.4.3.4.cmml">,</mo><msub id="S1.p5.2.m2.4.4.3.3.3" xref="S1.p5.2.m2.4.4.3.3.3.cmml"><mi id="S1.p5.2.m2.4.4.3.3.3.2" xref="S1.p5.2.m2.4.4.3.3.3.2.cmml">m</mi><mi id="S1.p5.2.m2.4.4.3.3.3.3" xref="S1.p5.2.m2.4.4.3.3.3.3.cmml">T</mi></msub><mo id="S1.p5.2.m2.4.4.3.3.8" stretchy="false" xref="S1.p5.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.4b"><apply id="S1.p5.2.m2.4.4.cmml" xref="S1.p5.2.m2.4.4"><eq id="S1.p5.2.m2.4.4.4.cmml" xref="S1.p5.2.m2.4.4.4"></eq><ci id="S1.p5.2.m2.4.4.5.cmml" xref="S1.p5.2.m2.4.4.5">ℳ</ci><set id="S1.p5.2.m2.4.4.3.4.cmml" xref="S1.p5.2.m2.4.4.3.3"><apply id="S1.p5.2.m2.2.2.1.1.1.cmml" xref="S1.p5.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S1.p5.2.m2.2.2.1.1.1.1.cmml" xref="S1.p5.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S1.p5.2.m2.2.2.1.1.1.2.cmml" xref="S1.p5.2.m2.2.2.1.1.1.2">𝑚</ci><cn id="S1.p5.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S1.p5.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S1.p5.2.m2.3.3.2.2.2.cmml" xref="S1.p5.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S1.p5.2.m2.3.3.2.2.2.1.cmml" xref="S1.p5.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S1.p5.2.m2.3.3.2.2.2.2.cmml" xref="S1.p5.2.m2.3.3.2.2.2.2">𝑚</ci><cn id="S1.p5.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S1.p5.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1">…</ci><apply id="S1.p5.2.m2.4.4.3.3.3.cmml" xref="S1.p5.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S1.p5.2.m2.4.4.3.3.3.1.cmml" xref="S1.p5.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S1.p5.2.m2.4.4.3.3.3.2.cmml" xref="S1.p5.2.m2.4.4.3.3.3.2">𝑚</ci><ci id="S1.p5.2.m2.4.4.3.3.3.3.cmml" xref="S1.p5.2.m2.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.4c">\mathcal{M}=\{m_{1},m_{2},...,m_{T}\}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.2.m2.4d">caligraphic_M = { italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_m start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }</annotation></semantics></math>, where <math alttext="m_{i}\in\mathbbm{R}^{D}" class="ltx_Math" display="inline" id="S1.p5.3.m3.1"><semantics id="S1.p5.3.m3.1a"><mrow id="S1.p5.3.m3.1.1" xref="S1.p5.3.m3.1.1.cmml"><msub id="S1.p5.3.m3.1.1.2" xref="S1.p5.3.m3.1.1.2.cmml"><mi id="S1.p5.3.m3.1.1.2.2" xref="S1.p5.3.m3.1.1.2.2.cmml">m</mi><mi id="S1.p5.3.m3.1.1.2.3" xref="S1.p5.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S1.p5.3.m3.1.1.1" xref="S1.p5.3.m3.1.1.1.cmml">∈</mo><msup id="S1.p5.3.m3.1.1.3" xref="S1.p5.3.m3.1.1.3.cmml"><mi id="S1.p5.3.m3.1.1.3.2" xref="S1.p5.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S1.p5.3.m3.1.1.3.3" xref="S1.p5.3.m3.1.1.3.3.cmml">D</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.3.m3.1b"><apply id="S1.p5.3.m3.1.1.cmml" xref="S1.p5.3.m3.1.1"><in id="S1.p5.3.m3.1.1.1.cmml" xref="S1.p5.3.m3.1.1.1"></in><apply id="S1.p5.3.m3.1.1.2.cmml" xref="S1.p5.3.m3.1.1.2"><csymbol cd="ambiguous" id="S1.p5.3.m3.1.1.2.1.cmml" xref="S1.p5.3.m3.1.1.2">subscript</csymbol><ci id="S1.p5.3.m3.1.1.2.2.cmml" xref="S1.p5.3.m3.1.1.2.2">𝑚</ci><ci id="S1.p5.3.m3.1.1.2.3.cmml" xref="S1.p5.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S1.p5.3.m3.1.1.3.cmml" xref="S1.p5.3.m3.1.1.3"><csymbol cd="ambiguous" id="S1.p5.3.m3.1.1.3.1.cmml" xref="S1.p5.3.m3.1.1.3">superscript</csymbol><ci id="S1.p5.3.m3.1.1.3.2.cmml" xref="S1.p5.3.m3.1.1.3.2">ℝ</ci><ci id="S1.p5.3.m3.1.1.3.3.cmml" xref="S1.p5.3.m3.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.3.m3.1c">m_{i}\in\mathbbm{R}^{D}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.3.m3.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, VQ compresses it into a list of 1D embeddings of size <math alttext="\lfloor T/\alpha\rfloor\times d" class="ltx_Math" display="inline" id="S1.p5.4.m4.1"><semantics id="S1.p5.4.m4.1a"><mrow id="S1.p5.4.m4.1.1" xref="S1.p5.4.m4.1.1.cmml"><mrow id="S1.p5.4.m4.1.1.1.1" xref="S1.p5.4.m4.1.1.1.2.cmml"><mo id="S1.p5.4.m4.1.1.1.1.2" stretchy="false" xref="S1.p5.4.m4.1.1.1.2.1.cmml">⌊</mo><mrow id="S1.p5.4.m4.1.1.1.1.1" xref="S1.p5.4.m4.1.1.1.1.1.cmml"><mi id="S1.p5.4.m4.1.1.1.1.1.2" xref="S1.p5.4.m4.1.1.1.1.1.2.cmml">T</mi><mo id="S1.p5.4.m4.1.1.1.1.1.1" xref="S1.p5.4.m4.1.1.1.1.1.1.cmml">/</mo><mi id="S1.p5.4.m4.1.1.1.1.1.3" xref="S1.p5.4.m4.1.1.1.1.1.3.cmml">α</mi></mrow><mo id="S1.p5.4.m4.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S1.p5.4.m4.1.1.1.2.1.cmml">⌋</mo></mrow><mo id="S1.p5.4.m4.1.1.2" rspace="0.222em" xref="S1.p5.4.m4.1.1.2.cmml">×</mo><mi id="S1.p5.4.m4.1.1.3" xref="S1.p5.4.m4.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.4.m4.1b"><apply id="S1.p5.4.m4.1.1.cmml" xref="S1.p5.4.m4.1.1"><times id="S1.p5.4.m4.1.1.2.cmml" xref="S1.p5.4.m4.1.1.2"></times><apply id="S1.p5.4.m4.1.1.1.2.cmml" xref="S1.p5.4.m4.1.1.1.1"><floor id="S1.p5.4.m4.1.1.1.2.1.cmml" xref="S1.p5.4.m4.1.1.1.1.2"></floor><apply id="S1.p5.4.m4.1.1.1.1.1.cmml" xref="S1.p5.4.m4.1.1.1.1.1"><divide id="S1.p5.4.m4.1.1.1.1.1.1.cmml" xref="S1.p5.4.m4.1.1.1.1.1.1"></divide><ci id="S1.p5.4.m4.1.1.1.1.1.2.cmml" xref="S1.p5.4.m4.1.1.1.1.1.2">𝑇</ci><ci id="S1.p5.4.m4.1.1.1.1.1.3.cmml" xref="S1.p5.4.m4.1.1.1.1.1.3">𝛼</ci></apply></apply><ci id="S1.p5.4.m4.1.1.3.cmml" xref="S1.p5.4.m4.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.4.m4.1c">\lfloor T/\alpha\rfloor\times d</annotation><annotation encoding="application/x-llamapun" id="S1.p5.4.m4.1d">⌊ italic_T / italic_α ⌋ × italic_d</annotation></semantics></math>, where <math alttext="\alpha" class="ltx_Math" display="inline" id="S1.p5.5.m5.1"><semantics id="S1.p5.5.m5.1a"><mi id="S1.p5.5.m5.1.1" xref="S1.p5.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S1.p5.5.m5.1b"><ci id="S1.p5.5.m5.1.1.cmml" xref="S1.p5.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.5.m5.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S1.p5.5.m5.1d">italic_α</annotation></semantics></math> is the temporal downsampling ratio and <math alttext="d" class="ltx_Math" display="inline" id="S1.p5.6.m6.1"><semantics id="S1.p5.6.m6.1a"><mi id="S1.p5.6.m6.1.1" xref="S1.p5.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.p5.6.m6.1b"><ci id="S1.p5.6.m6.1.1.cmml" xref="S1.p5.6.m6.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.6.m6.1c">d</annotation><annotation encoding="application/x-llamapun" id="S1.p5.6.m6.1d">italic_d</annotation></semantics></math> is the codebook dimension.
Unlike images, which consist of uniform RGB pixel values, each motion state <math alttext="m_{i}" class="ltx_Math" display="inline" id="S1.p5.7.m7.1"><semantics id="S1.p5.7.m7.1a"><msub id="S1.p5.7.m7.1.1" xref="S1.p5.7.m7.1.1.cmml"><mi id="S1.p5.7.m7.1.1.2" xref="S1.p5.7.m7.1.1.2.cmml">m</mi><mi id="S1.p5.7.m7.1.1.3" xref="S1.p5.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p5.7.m7.1b"><apply id="S1.p5.7.m7.1.1.cmml" xref="S1.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S1.p5.7.m7.1.1.1.cmml" xref="S1.p5.7.m7.1.1">subscript</csymbol><ci id="S1.p5.7.m7.1.1.2.cmml" xref="S1.p5.7.m7.1.1.2">𝑚</ci><ci id="S1.p5.7.m7.1.1.3.cmml" xref="S1.p5.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.7.m7.1c">m_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.p5.7.m7.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> contains a set of distinct features (e.g., joint position, velocity, foot-ground contact).
Using a single 1D embedding to represent such complex motion states is insufficient.
This not only results in the loss of vital information but also limits the model’s ability to flexibly generate motion at a part-level.
❷ <span class="ltx_text ltx_font_bold" id="S1.p5.7.2">Limited Codebook Size: </span> Existing VQ are limited by a small codebook, meaning that all possible human motions must be selected from these limited options.
Consequently, these 1D embeddings fail to capture the vast diversity of human motion.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To address this issue, we propose treating a motion clip as a 2D image with a single channel, represented as <math alttext="\mathcal{M}\in R^{T\times D\times 1}" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S1.p6.1.m1.1.1.2" xref="S1.p6.1.m1.1.1.2.cmml">ℳ</mi><mo id="S1.p6.1.m1.1.1.1" xref="S1.p6.1.m1.1.1.1.cmml">∈</mo><msup id="S1.p6.1.m1.1.1.3" xref="S1.p6.1.m1.1.1.3.cmml"><mi id="S1.p6.1.m1.1.1.3.2" xref="S1.p6.1.m1.1.1.3.2.cmml">R</mi><mrow id="S1.p6.1.m1.1.1.3.3" xref="S1.p6.1.m1.1.1.3.3.cmml"><mi id="S1.p6.1.m1.1.1.3.3.2" xref="S1.p6.1.m1.1.1.3.3.2.cmml">T</mi><mo id="S1.p6.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S1.p6.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S1.p6.1.m1.1.1.3.3.3" xref="S1.p6.1.m1.1.1.3.3.3.cmml">D</mi><mo id="S1.p6.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S1.p6.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S1.p6.1.m1.1.1.3.3.4" xref="S1.p6.1.m1.1.1.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><apply id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"><in id="S1.p6.1.m1.1.1.1.cmml" xref="S1.p6.1.m1.1.1.1"></in><ci id="S1.p6.1.m1.1.1.2.cmml" xref="S1.p6.1.m1.1.1.2">ℳ</ci><apply id="S1.p6.1.m1.1.1.3.cmml" xref="S1.p6.1.m1.1.1.3"><csymbol cd="ambiguous" id="S1.p6.1.m1.1.1.3.1.cmml" xref="S1.p6.1.m1.1.1.3">superscript</csymbol><ci id="S1.p6.1.m1.1.1.3.2.cmml" xref="S1.p6.1.m1.1.1.3.2">𝑅</ci><apply id="S1.p6.1.m1.1.1.3.3.cmml" xref="S1.p6.1.m1.1.1.3.3"><times id="S1.p6.1.m1.1.1.3.3.1.cmml" xref="S1.p6.1.m1.1.1.3.3.1"></times><ci id="S1.p6.1.m1.1.1.3.3.2.cmml" xref="S1.p6.1.m1.1.1.3.3.2">𝑇</ci><ci id="S1.p6.1.m1.1.1.3.3.3.cmml" xref="S1.p6.1.m1.1.1.3.3.3">𝐷</ci><cn id="S1.p6.1.m1.1.1.3.3.4.cmml" type="integer" xref="S1.p6.1.m1.1.1.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\mathcal{M}\in R^{T\times D\times 1}</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">caligraphic_M ∈ italic_R start_POSTSUPERSCRIPT italic_T × italic_D × 1 end_POSTSUPERSCRIPT</annotation></semantics></math>.
By expanding the dimensionality of the motion clip from 1D to 2D, we enhance the encoder’s capacity, improving its ability to represent complex motions while retaining more critical information after tokenization.
Although increasing the size of the codebook is a straightforward way to enhance its expressiveness, this approach often leads to “codebook collapse," particularly when training samples are scarce.
To mitigate this, we introduce a finite scalar quantizing method inspired by  <cite class="ltx_cite ltx_citemacro_citet">Mentzer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib32" title="">2023</a>)</cite>, which enables learning a large motion vocabulary without requiring a lookup for corresponding tokens in the codebook for each entry.
As a result, we expand the motion codebook by at least two orders of magnitude, boosting its representational capacity while maintaining efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We summarize our main contributions as follows.
<span class="ltx_text ltx_font_bold" id="S1.p7.1.1">(1) MotionBase</span>: We introduce MotionBase, the first large-scale motion generation benchmark containing over one million motions with detailed textual descriptions, significantly advancing the capability to effectively train motion generation models.
<span class="ltx_text ltx_font_bold" id="S1.p7.1.2">(2) Key Insights</span>: Our research identifies critical factors affecting the effectiveness of large motion models, emphasizing the importance of scaling both data and model size. Additionally, we uncover limitations in the current evaluation metrics, particularly when handling diverse and unseen motions.
<span class="ltx_text ltx_font_bold" id="S1.p7.1.3">(3) Novel Motion Quantization</span>: We propose a novel motion quantization approach that represents motion clips as 2D images and constructs a finite-scale codebook without requiring token lookups. This method retains essential information and expands the capacity of the motion encoder, enhancing the ability of large motion models to leverage large-scale motion data.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Large Language Models and Multi-Modality</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Substantial advancements have been made in enhancing LLMs <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib5" title="">2020</a>; Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib40" title="">2020</a>; Chowdhery et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib9" title="">2022</a>)</cite> with the ability to understand and respond to human instructions, through a technique known as instruction tuning <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib34" title="">2022</a>)</cite>.
Recent research has extended these capabilities to the multimodal domain <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib56" title="">2023</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib66" title="">2023</a>)</cite>, with notable work by <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib27" title="">2023</a>)</cite>, who pioneered visual instruction tuning to create a highly adaptable visual assistant.
Additionally, <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib23" title="">2023a</a>)</cite> integrated multimodal context directly into instruction data to further enhance model performance.
Subsequent studies <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib63" title="">2023b</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib65" title="">2023</a>)</cite> expanded this research by scaling up instructional datasets and incorporating image-rich text.
Notably, <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib11" title="">2023</a>)</cite> developed InstructBLIP, based on BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib24" title="">2023b</a>)</cite>, which features an advanced visual feature extraction mechanism to improve performance across vision-language tasks.
Despite these breakthroughs, the application of multimodal models to human motion remains less competitive compared to current state-of-the-art (SoTA) methods, although recent initiatives are beginning to explore this domain <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib21" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib64" title="">2024b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Vector Quantization</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Vector quantization (VQ) has been highly successful in generating high-quality images <cite class="ltx_cite ltx_citemacro_citep">(Van Den Oord et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib48" title="">2017</a>)</cite> and videos <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib20" title="">2022</a>; Yan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib55" title="">2021</a>)</cite>.
VQ-VAE first converts images into discrete representations and autoregressively models their distribution.
Building on this, <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib22" title="">2022</a>)</cite> introduced residual quantization (RQ), which encodes images into a stacked map of discrete codes, efficiently reducing the spatial resolution of features.
<cite class="ltx_cite ltx_citemacro_cite">You et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib57" title="">2022</a>)</cite> further developed hierarchical vector quantization (HQ), employing a pyramid scheme with two-level codes for image encoding.
Most existing motion generation approaches have adopted VQ or its variants to quantize human motions.
However, the small codebook size in traditional VQ methods limits their ability to generalize and accurately represent the diversity of human motions.
Although increasing the codebook size can improve representational capacity, it often leads to codebook collapse.
Recently, <cite class="ltx_cite ltx_citemacro_cite">Mentzer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib32" title="">2023</a>)</cite> demonstrated that discrete codes can be obtained via scalar quantization, where each scalar entry is independently quantized to the nearest integer through rounding.
Similarly, <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib58" title="">2023</a>)</cite> introduced a lookup-free codebook that maps videos into compact discrete tokens, utilizing all codes without auxiliary losses and expanding the codebook size.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Human Motion Generation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The task of motion generation involves creating human motion based on various inputs, such as text descriptions <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib18" title="">2022b</a>; Petrovich et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib36" title="">2022</a>)</cite>, action labels <cite class="ltx_cite ltx_citemacro_citep">(Cervantes et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib7" title="">2022</a>; Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib16" title="">2020</a>)</cite> or motion prefixes <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib28" title="">2022</a>; Mao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib30" title="">2019</a>)</cite>.
Among these, text-to-motion (T2M) generation has received the most attention due to the ease and flexibility of using natural language as input.
Early approaches <cite class="ltx_cite ltx_citemacro_citep">(Fragkiadaki et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib13" title="">2015</a>; Ghosh et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib14" title="">2017</a>; Gopalakrishnan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib15" title="">2019</a>)</cite> rely on deterministic motion modeling, which often produce averaged, blurry results.
To overcome this, researchers introduce stochastic methods using models like GANs <cite class="ltx_cite ltx_citemacro_citep">(Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib6" title="">2018</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib51" title="">2020</a>)</cite> or VAEs <cite class="ltx_cite ltx_citemacro_citep">(Aliakbarian et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib3" title="">2020</a>)</cite>.
For instance, T2M-GPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib60" title="">2023a</a>)</cite> extends the temporal VAE to capture the probabilistic relationship between text and motion.
More recently, <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib19" title="">2024</a>)</cite> proposed improving traditional vector quantization (VQ) by integrating residual quantization and a masked modeling framework.
To better align with a motion auto-encoder, MotionCLIP <cite class="ltx_cite ltx_citemacro_citep">(Tevet et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib45" title="">2022</a>)</cite> incorporates CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib39" title="">2021</a>)</cite> as the text encoder, bringing in more robust text priors.
Additionally, <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib64" title="">2024b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib21" title="">2023</a>)</cite> explored the development of unified models based on LLMs which accept multimodal conditions (e.g., vision, text, and pose), enabling the generation of subsequent, preceding, or “in-between” motions.
Despite leveraging the power of LLMs, these large motion models remain limited to in-domain text instructions and do not yet perform as competitively as existing SoTA methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">In this work, we aim to bridge the gap between large language models and generalized, reliable large motion models.
To achieve this, We begin by introducing MotionBase — a novel, large-scale dataset designed to support extensive pretraining and comprehensive fair evaluation.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison with existing human motion datasets. More details can be found in our appendix. In the table, B, H, and F refer to body, hand, and face, respectively. “part” indicates that the text captions include fine-grained descriptions of body parts, while “body” means the descriptions are not as detailed. “multi” and “single” specify whether the dataset contains multi-person scenarios or only single-person data. Our MotionBase is the largest motion generation dataset and benchmark to date, featuring at least 15× more data than previous datasets, along with additional modalities.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.1" style="width:523.6pt;height:73.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.5pt,8.1pt) scale(0.82,0.82) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">SEQ NUMBER</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">MOTION</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">TEXT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">RGB</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">DEPTH</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.2.1.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">BBOX</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">PERSON</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">KIT <cite class="ltx_cite ltx_citemacro_citep">(Plappert et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib37" title="">2016</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">5.7K</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">body</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">single</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.1.4.2.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">HumanML3D <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">29.2K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">body</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.4.2.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">single</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.1.5.3.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">MotionX <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">81.1K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">B,H,F</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">body</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.1.5.3.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">single</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2" style="padding-top:0.25pt;padding-bottom:0.25pt;">MotionBase-V1</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<math alttext="&gt;" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><gt id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math>1M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3" style="padding-top:0.25pt;padding-bottom:0.25pt;">B,H</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.4" style="padding-top:0.25pt;padding-bottom:0.25pt;">part</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.5" style="padding-top:0.25pt;padding-bottom:0.25pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.6" style="padding-top:0.25pt;padding-bottom:0.25pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T1.1.1.1.7" style="padding-top:0.25pt;padding-bottom:0.25pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.1.1.1.8" style="padding-top:0.25pt;padding-bottom:0.25pt;">multi</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MotionBase Dataset</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Data is the foundation of large motion models.
With advancements in fields like human pose detection, we are now able to extract high-quality motion sequences from vast amounts of online videos, including datasets like InternViD <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib50" title="">2023</a>)</cite> and WebVid <cite class="ltx_cite ltx_citemacro_citep">(Bain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib4" title="">2021</a>)</cite>.
In its initial public release, our MotionBase contains over one million motion clips, each annotated with fine-grained automatic pseudo labels.
A comparison with existing benchmarks is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S2.T1" title="Table 1 ‣ 2.3 Human Motion Generation ‣ 2 Related Work ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">1</span></a>.
Our data collection pipeline involves the following key steps in order.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">❶ <span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Source Video Collection and Cleaning: </span>
We begin by collecting over 20 million videos from publicly available datasets and online platforms such as YouTube.
To ensure quality and relevance, we filter out videos that do not contain human figures.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">❷ <span class="ltx_text ltx_font_bold" id="S3.p3.1.1">2D-3D Keypoint Estimation: </span>
Keypoints are essential for capturing the skeletal structure of human motion.
Initially, we estimate whole-body 2D keypoints with confidence scores using a pretrained model <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib54" title="">2022</a>)</cite>.
To further enhance motion accuracy, we estimate precise 3D keypoints with another pretrained model <cite class="ltx_cite ltx_citemacro_citep">(Sárándi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib42" title="">2023</a>)</cite> trained on large 3D datasets,
Following the method of <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>, we apply temporal smoothing and enforce 3D bone length constraints during triangulation, improving the stability and consistency of the keypoint estimations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">❸ <span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Incorporating Additional Modalities: </span>
A comprehensive understanding of human motion benefits from the inclusion of diverse modalities such as RGB and depth data.
To enrich MotionBase, we provide annotations for these additional modalities.
Furthermore, MotionBase includes videos featuring multi-person scenarios, with each motion sequence grounded in its corresponding video through object-level bounding boxes.
Although this paper primarily focuses on the text-to-motion task, these additional modalities open avenues for future research in other areas.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">❹ <span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Local-Global Pose Estimation: </span>
We begin by registering the body model SMPL-X <cite class="ltx_cite ltx_citemacro_citep">(Pavlakos et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib35" title="">2019</a>)</cite> for each frame in MotionBase, which leverages keypoints based on progressive learning-based mesh fitting method <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>.
Specifically, we predict SMPL-X parameters using a pretrained body mesh recovery method, OSX <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib25" title="">2023</a>)</cite>, followed by iterative optimization to fit the parameters to the target 2D and 3D joint positions.
After fitting, we apply global motion optimization based on <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib59" title="">2022</a>)</cite> to refine both global motions and camera poses simultaneously, ensuring alignment with the video evidence.
Finally, for motions with noisy or occluded input data, we reconstruct complete and plausible motions using RoHM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib62" title="">2024a</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">❺ <span class="ltx_text ltx_font_bold" id="S3.p6.1.1">Hierarchical Motion Descriptions: </span>
Existing motion benchmarks face inherent limitations in their text descriptions.
Previous studies <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> typically use a single sentence to describe whole-body motions, neglecting finer details of individual body parts, such as the arms or legs.
This approach restricts the ability of motion generation models to perform more nuanced body comprehension and flexible part-level motion control (e.g., raising only the left arm).
Moreover, the richness of text labels often varies across different motions; for example, a large portion of the Motion-X dataset provides only action labels.
In contrast, MotionBase offers hierarchical textual annotations for each video.
We carefully design a prompt format and use Gemini-1.5-pro <cite class="ltx_cite ltx_citemacro_citep">(Reid et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib41" title="">2024</a>)</cite> to generate detailed descriptions for individual body parts (e.g., left arm, right leg), assigning a dedicated sentence to each.
Additionally, we summarize the overall body movement in a paragraph containing 1–3 sentences, providing a more comprehensive description of the motion.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="512" id="S3.F2.g1" src="x2.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples from <span class="ltx_text ltx_font_bold" id="S3.F2.2.1">MotionBase</span>, which encompasses a diverse range of human motions, including both long-term clips and static snapshots. It features various scenes, ranging from outdoor environments to indoor settings, and includes both clean, single-person scenarios as well as crowded, multi-person scenes. Additionally, MotionBase comprises a mix of real-world data and synthetic data generated by game engines. For more details about MotionBase, please refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1" title="Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Scaling up Large Motion Model</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Overall Architecture</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.14">Similar to previous LLM-based multimodal models, we treat motion as a foreign language.
The overall framework is presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A2.F11" title="Figure 11 ‣ Appendix B Additional Overview of Model Architecture ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">11</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A2" title="Appendix B Additional Overview of Model Architecture ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">B</span></a>.
Our large motion model, built on a pre-trained LLM, functions as a generative model that connects a motion tokenizer with the LLM backbone <math alttext="\Theta" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S4.SS1.p1.1.m1.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">roman_Θ</annotation></semantics></math>.
The motion tokenizer encodes raw motion clip features <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">caligraphic_M</annotation></semantics></math> into token embeddings <math alttext="\mathcal{V}=\{v_{1},v_{2},...,v_{n}\}\in\mathbbm{R}^{n\times d}" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.4"><semantics id="S4.SS1.p1.3.m3.4a"><mrow id="S4.SS1.p1.3.m3.4.4" xref="S4.SS1.p1.3.m3.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.3.m3.4.4.5" xref="S4.SS1.p1.3.m3.4.4.5.cmml">𝒱</mi><mo id="S4.SS1.p1.3.m3.4.4.6" xref="S4.SS1.p1.3.m3.4.4.6.cmml">=</mo><mrow id="S4.SS1.p1.3.m3.4.4.3.3" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml"><mo id="S4.SS1.p1.3.m3.4.4.3.3.4" stretchy="false" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">{</mo><msub id="S4.SS1.p1.3.m3.2.2.1.1.1" xref="S4.SS1.p1.3.m3.2.2.1.1.1.cmml"><mi id="S4.SS1.p1.3.m3.2.2.1.1.1.2" xref="S4.SS1.p1.3.m3.2.2.1.1.1.2.cmml">v</mi><mn id="S4.SS1.p1.3.m3.2.2.1.1.1.3" xref="S4.SS1.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p1.3.m3.4.4.3.3.5" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.3.m3.3.3.2.2.2" xref="S4.SS1.p1.3.m3.3.3.2.2.2.cmml"><mi id="S4.SS1.p1.3.m3.3.3.2.2.2.2" xref="S4.SS1.p1.3.m3.3.3.2.2.2.2.cmml">v</mi><mn id="S4.SS1.p1.3.m3.3.3.2.2.2.3" xref="S4.SS1.p1.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.3.m3.4.4.3.3.6" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><mi id="S4.SS1.p1.3.m3.1.1" mathvariant="normal" xref="S4.SS1.p1.3.m3.1.1.cmml">…</mi><mo id="S4.SS1.p1.3.m3.4.4.3.3.7" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.3.m3.4.4.3.3.3" xref="S4.SS1.p1.3.m3.4.4.3.3.3.cmml"><mi id="S4.SS1.p1.3.m3.4.4.3.3.3.2" xref="S4.SS1.p1.3.m3.4.4.3.3.3.2.cmml">v</mi><mi id="S4.SS1.p1.3.m3.4.4.3.3.3.3" xref="S4.SS1.p1.3.m3.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S4.SS1.p1.3.m3.4.4.3.3.8" stretchy="false" xref="S4.SS1.p1.3.m3.4.4.3.4.cmml">}</mo></mrow><mo id="S4.SS1.p1.3.m3.4.4.7" xref="S4.SS1.p1.3.m3.4.4.7.cmml">∈</mo><msup id="S4.SS1.p1.3.m3.4.4.8" xref="S4.SS1.p1.3.m3.4.4.8.cmml"><mi id="S4.SS1.p1.3.m3.4.4.8.2" xref="S4.SS1.p1.3.m3.4.4.8.2.cmml">ℝ</mi><mrow id="S4.SS1.p1.3.m3.4.4.8.3" xref="S4.SS1.p1.3.m3.4.4.8.3.cmml"><mi id="S4.SS1.p1.3.m3.4.4.8.3.2" xref="S4.SS1.p1.3.m3.4.4.8.3.2.cmml">n</mi><mo id="S4.SS1.p1.3.m3.4.4.8.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.3.m3.4.4.8.3.1.cmml">×</mo><mi id="S4.SS1.p1.3.m3.4.4.8.3.3" xref="S4.SS1.p1.3.m3.4.4.8.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.4b"><apply id="S4.SS1.p1.3.m3.4.4.cmml" xref="S4.SS1.p1.3.m3.4.4"><and id="S4.SS1.p1.3.m3.4.4a.cmml" xref="S4.SS1.p1.3.m3.4.4"></and><apply id="S4.SS1.p1.3.m3.4.4b.cmml" xref="S4.SS1.p1.3.m3.4.4"><eq id="S4.SS1.p1.3.m3.4.4.6.cmml" xref="S4.SS1.p1.3.m3.4.4.6"></eq><ci id="S4.SS1.p1.3.m3.4.4.5.cmml" xref="S4.SS1.p1.3.m3.4.4.5">𝒱</ci><set id="S4.SS1.p1.3.m3.4.4.3.4.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3"><apply id="S4.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.2.2.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m3.2.2.1.1.1.2.cmml" xref="S4.SS1.p1.3.m3.2.2.1.1.1.2">𝑣</ci><cn id="S4.SS1.p1.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.p1.3.m3.3.3.2.2.2.cmml" xref="S4.SS1.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.3.3.2.2.2.1.cmml" xref="S4.SS1.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.3.m3.3.3.2.2.2.2.cmml" xref="S4.SS1.p1.3.m3.3.3.2.2.2.2">𝑣</ci><cn id="S4.SS1.p1.3.m3.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS1.p1.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">…</ci><apply id="S4.SS1.p1.3.m3.4.4.3.3.3.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.4.4.3.3.3.1.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.3.m3.4.4.3.3.3.2.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3.3.2">𝑣</ci><ci id="S4.SS1.p1.3.m3.4.4.3.3.3.3.cmml" xref="S4.SS1.p1.3.m3.4.4.3.3.3.3">𝑛</ci></apply></set></apply><apply id="S4.SS1.p1.3.m3.4.4c.cmml" xref="S4.SS1.p1.3.m3.4.4"><in id="S4.SS1.p1.3.m3.4.4.7.cmml" xref="S4.SS1.p1.3.m3.4.4.7"></in><share href="https://arxiv.org/html/2410.03311v1#S4.SS1.p1.3.m3.4.4.3.cmml" id="S4.SS1.p1.3.m3.4.4d.cmml" xref="S4.SS1.p1.3.m3.4.4"></share><apply id="S4.SS1.p1.3.m3.4.4.8.cmml" xref="S4.SS1.p1.3.m3.4.4.8"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.4.4.8.1.cmml" xref="S4.SS1.p1.3.m3.4.4.8">superscript</csymbol><ci id="S4.SS1.p1.3.m3.4.4.8.2.cmml" xref="S4.SS1.p1.3.m3.4.4.8.2">ℝ</ci><apply id="S4.SS1.p1.3.m3.4.4.8.3.cmml" xref="S4.SS1.p1.3.m3.4.4.8.3"><times id="S4.SS1.p1.3.m3.4.4.8.3.1.cmml" xref="S4.SS1.p1.3.m3.4.4.8.3.1"></times><ci id="S4.SS1.p1.3.m3.4.4.8.3.2.cmml" xref="S4.SS1.p1.3.m3.4.4.8.3.2">𝑛</ci><ci id="S4.SS1.p1.3.m3.4.4.8.3.3.cmml" xref="S4.SS1.p1.3.m3.4.4.8.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.4c">\mathcal{V}=\{v_{1},v_{2},...,v_{n}\}\in\mathbbm{R}^{n\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.4d">caligraphic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_n</annotation></semantics></math> denotes the number of motion tokens and <math alttext="d" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m5.1"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.1d">italic_d</annotation></semantics></math> represents the dimensionality of each token.
To integrate motion tokens into the LLM framework, we incorporate <math alttext="K" class="ltx_Math" display="inline" id="S4.SS1.p1.6.m6.1"><semantics id="S4.SS1.p1.6.m6.1a"><mi id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><ci id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.6.m6.1d">italic_K</annotation></semantics></math> discrete codes in the motion codebook as additional vocabulary for the LLM.
Additionally, we introduce two special tokens, <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.p1.7.m7.1"><semantics id="S4.SS1.p1.7.m7.1a"><mo id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><lt id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.7.m7.1d">&lt;</annotation></semantics></math>mot<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.p1.8.m8.1"><semantics id="S4.SS1.p1.8.m8.1a"><mo id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><gt id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.8.m8.1d">&gt;</annotation></semantics></math> and <math alttext="&lt;" class="ltx_Math" display="inline" id="S4.SS1.p1.9.m9.1"><semantics id="S4.SS1.p1.9.m9.1a"><mo id="S4.SS1.p1.9.m9.1.1" xref="S4.SS1.p1.9.m9.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.9.m9.1b"><lt id="S4.SS1.p1.9.m9.1.1.cmml" xref="S4.SS1.p1.9.m9.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.9.m9.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.9.m9.1d">&lt;</annotation></semantics></math>/mot<math alttext="&gt;" class="ltx_Math" display="inline" id="S4.SS1.p1.10.m10.1"><semantics id="S4.SS1.p1.10.m10.1a"><mo id="S4.SS1.p1.10.m10.1.1" xref="S4.SS1.p1.10.m10.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.10.m10.1b"><gt id="S4.SS1.p1.10.m10.1.1.cmml" xref="S4.SS1.p1.10.m10.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.10.m10.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.10.m10.1d">&gt;</annotation></semantics></math>, to signify the start and end of motion sequences within the input/output streams.
The LLM backbone <math alttext="\Theta" class="ltx_Math" display="inline" id="S4.SS1.p1.11.m11.1"><semantics id="S4.SS1.p1.11.m11.1a"><mi id="S4.SS1.p1.11.m11.1.1" mathvariant="normal" xref="S4.SS1.p1.11.m11.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.11.m11.1b"><ci id="S4.SS1.p1.11.m11.1.1.cmml" xref="S4.SS1.p1.11.m11.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.11.m11.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.11.m11.1d">roman_Θ</annotation></semantics></math> is built on a decoder-only architecture using causal transformers.
The model generates outputs <math alttext="\mathcal{Y}=\{y_{1},y_{2},...,y_{m}\}" class="ltx_Math" display="inline" id="S4.SS1.p1.12.m12.4"><semantics id="S4.SS1.p1.12.m12.4a"><mrow id="S4.SS1.p1.12.m12.4.4" xref="S4.SS1.p1.12.m12.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.12.m12.4.4.5" xref="S4.SS1.p1.12.m12.4.4.5.cmml">𝒴</mi><mo id="S4.SS1.p1.12.m12.4.4.4" xref="S4.SS1.p1.12.m12.4.4.4.cmml">=</mo><mrow id="S4.SS1.p1.12.m12.4.4.3.3" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml"><mo id="S4.SS1.p1.12.m12.4.4.3.3.4" stretchy="false" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">{</mo><msub id="S4.SS1.p1.12.m12.2.2.1.1.1" xref="S4.SS1.p1.12.m12.2.2.1.1.1.cmml"><mi id="S4.SS1.p1.12.m12.2.2.1.1.1.2" xref="S4.SS1.p1.12.m12.2.2.1.1.1.2.cmml">y</mi><mn id="S4.SS1.p1.12.m12.2.2.1.1.1.3" xref="S4.SS1.p1.12.m12.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.p1.12.m12.4.4.3.3.5" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.12.m12.3.3.2.2.2" xref="S4.SS1.p1.12.m12.3.3.2.2.2.cmml"><mi id="S4.SS1.p1.12.m12.3.3.2.2.2.2" xref="S4.SS1.p1.12.m12.3.3.2.2.2.2.cmml">y</mi><mn id="S4.SS1.p1.12.m12.3.3.2.2.2.3" xref="S4.SS1.p1.12.m12.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.12.m12.4.4.3.3.6" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">,</mo><mi id="S4.SS1.p1.12.m12.1.1" mathvariant="normal" xref="S4.SS1.p1.12.m12.1.1.cmml">…</mi><mo id="S4.SS1.p1.12.m12.4.4.3.3.7" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">,</mo><msub id="S4.SS1.p1.12.m12.4.4.3.3.3" xref="S4.SS1.p1.12.m12.4.4.3.3.3.cmml"><mi id="S4.SS1.p1.12.m12.4.4.3.3.3.2" xref="S4.SS1.p1.12.m12.4.4.3.3.3.2.cmml">y</mi><mi id="S4.SS1.p1.12.m12.4.4.3.3.3.3" xref="S4.SS1.p1.12.m12.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S4.SS1.p1.12.m12.4.4.3.3.8" stretchy="false" xref="S4.SS1.p1.12.m12.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.12.m12.4b"><apply id="S4.SS1.p1.12.m12.4.4.cmml" xref="S4.SS1.p1.12.m12.4.4"><eq id="S4.SS1.p1.12.m12.4.4.4.cmml" xref="S4.SS1.p1.12.m12.4.4.4"></eq><ci id="S4.SS1.p1.12.m12.4.4.5.cmml" xref="S4.SS1.p1.12.m12.4.4.5">𝒴</ci><set id="S4.SS1.p1.12.m12.4.4.3.4.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3"><apply id="S4.SS1.p1.12.m12.2.2.1.1.1.cmml" xref="S4.SS1.p1.12.m12.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.12.m12.2.2.1.1.1.1.cmml" xref="S4.SS1.p1.12.m12.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.12.m12.2.2.1.1.1.2.cmml" xref="S4.SS1.p1.12.m12.2.2.1.1.1.2">𝑦</ci><cn id="S4.SS1.p1.12.m12.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.12.m12.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.p1.12.m12.3.3.2.2.2.cmml" xref="S4.SS1.p1.12.m12.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.12.m12.3.3.2.2.2.1.cmml" xref="S4.SS1.p1.12.m12.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.12.m12.3.3.2.2.2.2.cmml" xref="S4.SS1.p1.12.m12.3.3.2.2.2.2">𝑦</ci><cn id="S4.SS1.p1.12.m12.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS1.p1.12.m12.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.p1.12.m12.1.1.cmml" xref="S4.SS1.p1.12.m12.1.1">…</ci><apply id="S4.SS1.p1.12.m12.4.4.3.3.3.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.12.m12.4.4.3.3.3.1.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.12.m12.4.4.3.3.3.2.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3.3.2">𝑦</ci><ci id="S4.SS1.p1.12.m12.4.4.3.3.3.3.cmml" xref="S4.SS1.p1.12.m12.4.4.3.3.3.3">𝑚</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.12.m12.4c">\mathcal{Y}=\{y_{1},y_{2},...,y_{m}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.12.m12.4d">caligraphic_Y = { italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }</annotation></semantics></math> in an auto-regressive manner, where <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S4.SS1.p1.13.m13.1"><semantics id="S4.SS1.p1.13.m13.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.13.m13.1.1" xref="S4.SS1.p1.13.m13.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.13.m13.1b"><ci id="S4.SS1.p1.13.m13.1.1.cmml" xref="S4.SS1.p1.13.m13.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.13.m13.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.13.m13.1d">caligraphic_Y</annotation></semantics></math> corresponds to the generated motion sequence based on the provided motion-text input tokens.
In this work, each motion-text pair in the MotionBase dataset is framed as an instruction-following instance <math alttext="\{\mathcal{X}_{Q},\mathcal{X}_{M}\}" class="ltx_Math" display="inline" id="S4.SS1.p1.14.m14.2"><semantics id="S4.SS1.p1.14.m14.2a"><mrow id="S4.SS1.p1.14.m14.2.2.2" xref="S4.SS1.p1.14.m14.2.2.3.cmml"><mo id="S4.SS1.p1.14.m14.2.2.2.3" stretchy="false" xref="S4.SS1.p1.14.m14.2.2.3.cmml">{</mo><msub id="S4.SS1.p1.14.m14.1.1.1.1" xref="S4.SS1.p1.14.m14.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.14.m14.1.1.1.1.2" xref="S4.SS1.p1.14.m14.1.1.1.1.2.cmml">𝒳</mi><mi id="S4.SS1.p1.14.m14.1.1.1.1.3" xref="S4.SS1.p1.14.m14.1.1.1.1.3.cmml">Q</mi></msub><mo id="S4.SS1.p1.14.m14.2.2.2.4" xref="S4.SS1.p1.14.m14.2.2.3.cmml">,</mo><msub id="S4.SS1.p1.14.m14.2.2.2.2" xref="S4.SS1.p1.14.m14.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.14.m14.2.2.2.2.2" xref="S4.SS1.p1.14.m14.2.2.2.2.2.cmml">𝒳</mi><mi id="S4.SS1.p1.14.m14.2.2.2.2.3" xref="S4.SS1.p1.14.m14.2.2.2.2.3.cmml">M</mi></msub><mo id="S4.SS1.p1.14.m14.2.2.2.5" stretchy="false" xref="S4.SS1.p1.14.m14.2.2.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.14.m14.2b"><set id="S4.SS1.p1.14.m14.2.2.3.cmml" xref="S4.SS1.p1.14.m14.2.2.2"><apply id="S4.SS1.p1.14.m14.1.1.1.1.cmml" xref="S4.SS1.p1.14.m14.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.14.m14.1.1.1.1.1.cmml" xref="S4.SS1.p1.14.m14.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.14.m14.1.1.1.1.2.cmml" xref="S4.SS1.p1.14.m14.1.1.1.1.2">𝒳</ci><ci id="S4.SS1.p1.14.m14.1.1.1.1.3.cmml" xref="S4.SS1.p1.14.m14.1.1.1.1.3">𝑄</ci></apply><apply id="S4.SS1.p1.14.m14.2.2.2.2.cmml" xref="S4.SS1.p1.14.m14.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.14.m14.2.2.2.2.1.cmml" xref="S4.SS1.p1.14.m14.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.14.m14.2.2.2.2.2.cmml" xref="S4.SS1.p1.14.m14.2.2.2.2.2">𝒳</ci><ci id="S4.SS1.p1.14.m14.2.2.2.2.3.cmml" xref="S4.SS1.p1.14.m14.2.2.2.2.3">𝑀</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.14.m14.2c">\{\mathcal{X}_{Q},\mathcal{X}_{M}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.14.m14.2d">{ caligraphic_X start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , caligraphic_X start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT }</annotation></semantics></math>, representing a question-answer interaction between the user and the motion model.
The entire instructional dataset adheres to this unified format.
To train our model, we optimize the negative log-likelihood over the predicted tokens which is defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(\Theta)=-\sum_{j=1}^{L}\log P_{\Theta}(y_{j}|desc,\hat{y}_{1:j-1})," class="ltx_Math" display="block" id="S4.E1.m1.2"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.1.1.3.2" xref="S4.E1.m1.2.2.1.1.3.2.cmml">ℒ</mi><mo id="S4.E1.m1.2.2.1.1.3.1" xref="S4.E1.m1.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S4.E1.m1.2.2.1.1.3.3.2" xref="S4.E1.m1.2.2.1.1.3.cmml"><mo id="S4.E1.m1.2.2.1.1.3.3.2.1" stretchy="false" xref="S4.E1.m1.2.2.1.1.3.cmml">(</mo><mi id="S4.E1.m1.1.1" mathvariant="normal" xref="S4.E1.m1.1.1.cmml">Θ</mi><mo id="S4.E1.m1.2.2.1.1.3.3.2.2" stretchy="false" xref="S4.E1.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.cmml"><mo id="S4.E1.m1.2.2.1.1.1a" xref="S4.E1.m1.2.2.1.1.1.cmml">−</mo><mrow id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><munderover id="S4.E1.m1.2.2.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.2.cmml"><mo id="S4.E1.m1.2.2.1.1.1.1.2.2.2" movablelimits="false" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.2.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S4.E1.m1.2.2.1.1.1.1.2.2.3.1" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.2.2.1.1.1.1.2.2.3.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.2.2.1.1.1.1.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.3.cmml">L</mi></munderover><mrow id="S4.E1.m1.2.2.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.3.1" xref="S4.E1.m1.2.2.1.1.1.1.1.3.1.cmml">log</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.3a" lspace="0.167em" xref="S4.E1.m1.2.2.1.1.1.1.1.3.cmml">⁡</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.3.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml">P</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.3.2.3" mathvariant="normal" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml">Θ</mi></msub></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml">y</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml">j</mi></msub><mo fence="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">d</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">e</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4.cmml">s</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1b" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5.cmml">c</mi></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml"><mover accent="true" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">y</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml">^</mo></mover><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">1</mn><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1" lspace="0.278em" rspace="0.278em" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">:</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.2.cmml">j</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.1.cmml">−</mo><mn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></mrow></msub></mrow></mrow><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1"><eq id="S4.E1.m1.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2"></eq><apply id="S4.E1.m1.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3"><times id="S4.E1.m1.2.2.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.3.1"></times><ci id="S4.E1.m1.2.2.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2">ℒ</ci><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">Θ</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"><minus id="S4.E1.m1.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1"></minus><apply id="S4.E1.m1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1"><apply id="S4.E1.m1.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3"><eq id="S4.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.2">𝑗</ci><cn id="S4.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.3">𝐿</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2"></times><apply id="S4.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3"><log id="S4.E1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.1"></log><apply id="S4.E1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.2">𝑃</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.3">Θ</ci></apply></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2">𝑦</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3">𝑗</ci></apply><list id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2"><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝑑</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">𝑒</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4">𝑠</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5">𝑐</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2"><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.1">^</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.2">𝑦</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3"><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1">:</ci><cn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" type="integer" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2">1</cn><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3"><minus id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.1"></minus><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.2">𝑗</ci><cn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.3.cmml" type="integer" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.3">1</cn></apply></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\mathcal{L}(\Theta)=-\sum_{j=1}^{L}\log P_{\Theta}(y_{j}|desc,\hat{y}_{1:j-1}),</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.2d">caligraphic_L ( roman_Θ ) = - ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT roman_log italic_P start_POSTSUBSCRIPT roman_Θ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_d italic_e italic_s italic_c , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT 1 : italic_j - 1 end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p1.19">where <math alttext="\hat{y}" class="ltx_Math" display="inline" id="S4.SS1.p1.15.m1.1"><semantics id="S4.SS1.p1.15.m1.1a"><mover accent="true" id="S4.SS1.p1.15.m1.1.1" xref="S4.SS1.p1.15.m1.1.1.cmml"><mi id="S4.SS1.p1.15.m1.1.1.2" xref="S4.SS1.p1.15.m1.1.1.2.cmml">y</mi><mo id="S4.SS1.p1.15.m1.1.1.1" xref="S4.SS1.p1.15.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.15.m1.1b"><apply id="S4.SS1.p1.15.m1.1.1.cmml" xref="S4.SS1.p1.15.m1.1.1"><ci id="S4.SS1.p1.15.m1.1.1.1.cmml" xref="S4.SS1.p1.15.m1.1.1.1">^</ci><ci id="S4.SS1.p1.15.m1.1.1.2.cmml" xref="S4.SS1.p1.15.m1.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.15.m1.1c">\hat{y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.15.m1.1d">over^ start_ARG italic_y end_ARG</annotation></semantics></math> and <math alttext="y" class="ltx_Math" display="inline" id="S4.SS1.p1.16.m2.1"><semantics id="S4.SS1.p1.16.m2.1a"><mi id="S4.SS1.p1.16.m2.1.1" xref="S4.SS1.p1.16.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.16.m2.1b"><ci id="S4.SS1.p1.16.m2.1.1.cmml" xref="S4.SS1.p1.16.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.16.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.16.m2.1d">italic_y</annotation></semantics></math> denote the input and target token sequences, respectively. <math alttext="\Theta" class="ltx_Math" display="inline" id="S4.SS1.p1.17.m3.1"><semantics id="S4.SS1.p1.17.m3.1a"><mi id="S4.SS1.p1.17.m3.1.1" mathvariant="normal" xref="S4.SS1.p1.17.m3.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.17.m3.1b"><ci id="S4.SS1.p1.17.m3.1.1.cmml" xref="S4.SS1.p1.17.m3.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.17.m3.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.17.m3.1d">roman_Θ</annotation></semantics></math> represents the model parameters, and <math alttext="L" class="ltx_Math" display="inline" id="S4.SS1.p1.18.m4.1"><semantics id="S4.SS1.p1.18.m4.1a"><mi id="S4.SS1.p1.18.m4.1.1" xref="S4.SS1.p1.18.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.18.m4.1b"><ci id="S4.SS1.p1.18.m4.1.1.cmml" xref="S4.SS1.p1.18.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.18.m4.1c">L</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.18.m4.1d">italic_L</annotation></semantics></math> is the length of the target sequence.
The input description, <math alttext="desc" class="ltx_Math" display="inline" id="S4.SS1.p1.19.m5.1"><semantics id="S4.SS1.p1.19.m5.1a"><mrow id="S4.SS1.p1.19.m5.1.1" xref="S4.SS1.p1.19.m5.1.1.cmml"><mi id="S4.SS1.p1.19.m5.1.1.2" xref="S4.SS1.p1.19.m5.1.1.2.cmml">d</mi><mo id="S4.SS1.p1.19.m5.1.1.1" xref="S4.SS1.p1.19.m5.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p1.19.m5.1.1.3" xref="S4.SS1.p1.19.m5.1.1.3.cmml">e</mi><mo id="S4.SS1.p1.19.m5.1.1.1a" xref="S4.SS1.p1.19.m5.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p1.19.m5.1.1.4" xref="S4.SS1.p1.19.m5.1.1.4.cmml">s</mi><mo id="S4.SS1.p1.19.m5.1.1.1b" xref="S4.SS1.p1.19.m5.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p1.19.m5.1.1.5" xref="S4.SS1.p1.19.m5.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.19.m5.1b"><apply id="S4.SS1.p1.19.m5.1.1.cmml" xref="S4.SS1.p1.19.m5.1.1"><times id="S4.SS1.p1.19.m5.1.1.1.cmml" xref="S4.SS1.p1.19.m5.1.1.1"></times><ci id="S4.SS1.p1.19.m5.1.1.2.cmml" xref="S4.SS1.p1.19.m5.1.1.2">𝑑</ci><ci id="S4.SS1.p1.19.m5.1.1.3.cmml" xref="S4.SS1.p1.19.m5.1.1.3">𝑒</ci><ci id="S4.SS1.p1.19.m5.1.1.4.cmml" xref="S4.SS1.p1.19.m5.1.1.4">𝑠</ci><ci id="S4.SS1.p1.19.m5.1.1.5.cmml" xref="S4.SS1.p1.19.m5.1.1.5">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.19.m5.1c">desc</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.19.m5.1d">italic_d italic_e italic_s italic_c</annotation></semantics></math>, can be empty depending on the instruction provided.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>2D Lookup-free Motion Quantization</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.3">Similar to visual tokenization, motion tokenization is a process that compresses motion signals into a series of discrete tokens, typically involving an encoder <math alttext="\mathbbm{E}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">𝔼</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝔼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\mathbbm{E}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">blackboard_E</annotation></semantics></math>, a decoder <math alttext="\mathbbm{D}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">𝔻</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">𝔻</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\mathbbm{D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">blackboard_D</annotation></semantics></math> and a codebook <math alttext="\mathbbm{C}" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">ℂ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">ℂ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\mathbbm{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">blackboard_C</annotation></semantics></math>.
We propose a 2D lookup-free quantization method as a key component for building large motion models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.7"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.7.1">2D Motion Quantization. </span>
Traditional motion quantizers use 1D embeddings to represent motion at each timestamp, which inevitably results in the loss of crucial information.
Furthermore, this approach limits the quantizer’s ability to generate and interpret part-level motions.
To address these limitations, we treat the motion sequence <math alttext="\mathcal{M}=\{m_{1},m_{2},...,m_{T}\}" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.4"><semantics id="S4.SS2.p2.1.m1.4a"><mrow id="S4.SS2.p2.1.m1.4.4" xref="S4.SS2.p2.1.m1.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.1.m1.4.4.5" xref="S4.SS2.p2.1.m1.4.4.5.cmml">ℳ</mi><mo id="S4.SS2.p2.1.m1.4.4.4" xref="S4.SS2.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS2.p2.1.m1.4.4.3.3" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml"><mo id="S4.SS2.p2.1.m1.4.4.3.3.4" stretchy="false" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">{</mo><msub id="S4.SS2.p2.1.m1.2.2.1.1.1" xref="S4.SS2.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.2.2.1.1.1.2" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2.cmml">m</mi><mn id="S4.SS2.p2.1.m1.2.2.1.1.1.3" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.5" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p2.1.m1.3.3.2.2.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.p2.1.m1.3.3.2.2.2.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2.cmml">m</mi><mn id="S4.SS2.p2.1.m1.3.3.2.2.2.3" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.6" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><mi id="S4.SS2.p2.1.m1.1.1" mathvariant="normal" xref="S4.SS2.p2.1.m1.1.1.cmml">…</mi><mo id="S4.SS2.p2.1.m1.4.4.3.3.7" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p2.1.m1.4.4.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.2" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2.cmml">m</mi><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo id="S4.SS2.p2.1.m1.4.4.3.3.8" stretchy="false" xref="S4.SS2.p2.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.4b"><apply id="S4.SS2.p2.1.m1.4.4.cmml" xref="S4.SS2.p2.1.m1.4.4"><eq id="S4.SS2.p2.1.m1.4.4.4.cmml" xref="S4.SS2.p2.1.m1.4.4.4"></eq><ci id="S4.SS2.p2.1.m1.4.4.5.cmml" xref="S4.SS2.p2.1.m1.4.4.5">ℳ</ci><set id="S4.SS2.p2.1.m1.4.4.3.4.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3"><apply id="S4.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2">𝑚</ci><cn id="S4.SS2.p2.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS2.p2.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p2.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2">𝑚</ci><cn id="S4.SS2.p2.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">…</ci><apply id="S4.SS2.p2.1.m1.4.4.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2">𝑚</ci><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.4c">\mathcal{M}=\{m_{1},m_{2},...,m_{T}\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.4d">caligraphic_M = { italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_m start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }</annotation></semantics></math> as a single-channel image, representing each motin sequence as <math alttext="\mathcal{M}\in\mathbbm{R}^{T\times D\times 1}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">ℳ</mi><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.2" xref="S4.SS2.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS2.p2.2.m2.1.1.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.3.2" xref="S4.SS2.p2.2.m2.1.1.3.3.2.cmml">T</mi><mo id="S4.SS2.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S4.SS2.p2.2.m2.1.1.3.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.3.cmml">D</mi><mo id="S4.SS2.p2.2.m2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p2.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S4.SS2.p2.2.m2.1.1.3.3.4" xref="S4.SS2.p2.2.m2.1.1.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><in id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></in><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">ℳ</ci><apply id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S4.SS2.p2.2.m2.1.1.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3"><times id="S4.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3.1"></times><ci id="S4.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3.2">𝑇</ci><ci id="S4.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3.3">𝐷</ci><cn id="S4.SS2.p2.2.m2.1.1.3.3.4.cmml" type="integer" xref="S4.SS2.p2.2.m2.1.1.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\mathcal{M}\in\mathbbm{R}^{T\times D\times 1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">caligraphic_M ∈ blackboard_R start_POSTSUPERSCRIPT italic_T × italic_D × 1 end_POSTSUPERSCRIPT</annotation></semantics></math>.
Each motion embedding <math alttext="m_{i}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3.1"><semantics id="S4.SS2.p2.3.m3.1a"><msub id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">m</mi><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝑚</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">m_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.3.m3.1d">italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is divided into <math alttext="P" class="ltx_Math" display="inline" id="S4.SS2.p2.4.m4.1"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.4.m4.1d">italic_P</annotation></semantics></math> components, capturing distinct features of motion, such as root orientation, joint rotation and foot contact.
Our motion encoder then converts <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S4.SS2.p2.5.m5.1"><semantics id="S4.SS2.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><ci id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.5.m5.1d">caligraphic_M</annotation></semantics></math> into a feature map <math alttext="\mathbbm{E}(\mathcal{M})\in\mathbbm{R}^{\lfloor T/\alpha\rfloor\times P\times d}" class="ltx_Math" display="inline" id="S4.SS2.p2.6.m6.2"><semantics id="S4.SS2.p2.6.m6.2a"><mrow id="S4.SS2.p2.6.m6.2.3" xref="S4.SS2.p2.6.m6.2.3.cmml"><mrow id="S4.SS2.p2.6.m6.2.3.2" xref="S4.SS2.p2.6.m6.2.3.2.cmml"><mi id="S4.SS2.p2.6.m6.2.3.2.2" xref="S4.SS2.p2.6.m6.2.3.2.2.cmml">𝔼</mi><mo id="S4.SS2.p2.6.m6.2.3.2.1" xref="S4.SS2.p2.6.m6.2.3.2.1.cmml">⁢</mo><mrow id="S4.SS2.p2.6.m6.2.3.2.3.2" xref="S4.SS2.p2.6.m6.2.3.2.cmml"><mo id="S4.SS2.p2.6.m6.2.3.2.3.2.1" stretchy="false" xref="S4.SS2.p2.6.m6.2.3.2.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p2.6.m6.2.2" xref="S4.SS2.p2.6.m6.2.2.cmml">ℳ</mi><mo id="S4.SS2.p2.6.m6.2.3.2.3.2.2" stretchy="false" xref="S4.SS2.p2.6.m6.2.3.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p2.6.m6.2.3.1" xref="S4.SS2.p2.6.m6.2.3.1.cmml">∈</mo><msup id="S4.SS2.p2.6.m6.2.3.3" xref="S4.SS2.p2.6.m6.2.3.3.cmml"><mi id="S4.SS2.p2.6.m6.2.3.3.2" xref="S4.SS2.p2.6.m6.2.3.3.2.cmml">ℝ</mi><mrow id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml"><mrow id="S4.SS2.p2.6.m6.1.1.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.1.2.cmml"><mo id="S4.SS2.p2.6.m6.1.1.1.1.1.2" stretchy="false" xref="S4.SS2.p2.6.m6.1.1.1.1.2.1.cmml">⌊</mo><mrow id="S4.SS2.p2.6.m6.1.1.1.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p2.6.m6.1.1.1.1.1.1.2" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.2.cmml">T</mi><mo id="S4.SS2.p2.6.m6.1.1.1.1.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.1.cmml">/</mo><mi id="S4.SS2.p2.6.m6.1.1.1.1.1.1.3" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo id="S4.SS2.p2.6.m6.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S4.SS2.p2.6.m6.1.1.1.1.2.1.cmml">⌋</mo></mrow><mo id="S4.SS2.p2.6.m6.1.1.1.2" rspace="0.222em" xref="S4.SS2.p2.6.m6.1.1.1.2.cmml">×</mo><mi id="S4.SS2.p2.6.m6.1.1.1.3" xref="S4.SS2.p2.6.m6.1.1.1.3.cmml">P</mi><mo id="S4.SS2.p2.6.m6.1.1.1.2a" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p2.6.m6.1.1.1.2.cmml">×</mo><mi id="S4.SS2.p2.6.m6.1.1.1.4" xref="S4.SS2.p2.6.m6.1.1.1.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.2b"><apply id="S4.SS2.p2.6.m6.2.3.cmml" xref="S4.SS2.p2.6.m6.2.3"><in id="S4.SS2.p2.6.m6.2.3.1.cmml" xref="S4.SS2.p2.6.m6.2.3.1"></in><apply id="S4.SS2.p2.6.m6.2.3.2.cmml" xref="S4.SS2.p2.6.m6.2.3.2"><times id="S4.SS2.p2.6.m6.2.3.2.1.cmml" xref="S4.SS2.p2.6.m6.2.3.2.1"></times><ci id="S4.SS2.p2.6.m6.2.3.2.2.cmml" xref="S4.SS2.p2.6.m6.2.3.2.2">𝔼</ci><ci id="S4.SS2.p2.6.m6.2.2.cmml" xref="S4.SS2.p2.6.m6.2.2">ℳ</ci></apply><apply id="S4.SS2.p2.6.m6.2.3.3.cmml" xref="S4.SS2.p2.6.m6.2.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.6.m6.2.3.3.1.cmml" xref="S4.SS2.p2.6.m6.2.3.3">superscript</csymbol><ci id="S4.SS2.p2.6.m6.2.3.3.2.cmml" xref="S4.SS2.p2.6.m6.2.3.3.2">ℝ</ci><apply id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1"><times id="S4.SS2.p2.6.m6.1.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.1.2"></times><apply id="S4.SS2.p2.6.m6.1.1.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1"><floor id="S4.SS2.p2.6.m6.1.1.1.1.2.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.2"></floor><apply id="S4.SS2.p2.6.m6.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1"><divide id="S4.SS2.p2.6.m6.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.1"></divide><ci id="S4.SS2.p2.6.m6.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.2">𝑇</ci><ci id="S4.SS2.p2.6.m6.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.1.1.1.1.3">𝛼</ci></apply></apply><ci id="S4.SS2.p2.6.m6.1.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.1.3">𝑃</ci><ci id="S4.SS2.p2.6.m6.1.1.1.4.cmml" xref="S4.SS2.p2.6.m6.1.1.1.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.2c">\mathbbm{E}(\mathcal{M})\in\mathbbm{R}^{\lfloor T/\alpha\rfloor\times P\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.6.m6.2d">blackboard_E ( caligraphic_M ) ∈ blackboard_R start_POSTSUPERSCRIPT ⌊ italic_T / italic_α ⌋ × italic_P × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.p2.7.m7.1"><semantics id="S4.SS2.p2.7.m7.1a"><mi id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><ci id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.7.m7.1d">italic_α</annotation></semantics></math> denotes the temporal downsampling ratio.
This approach ensures that each body part is tokenized separately, allowing for more granular, part-level motion encoding and decoding.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.12"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.12.2">Lookup-Free Quantization. </span>
Traditional motion quantizers are often constrained by small codebook sizes, restricting their ability to capture the full diversity of human motion.
A common approach is to expand the motion vocabulary.
However, excessively enlarging the codebook can result in “codebook collapse”, where only a small subset of tokens in the codebook is used, offering minimal performance improvements.
In some cases, an overly large vocabulary can even degrade the model’s overall performance.
To address this issue, a more effective alternative is to reduce the dimensionality of code embeddings <cite class="ltx_cite ltx_citemacro_citep">(Mentzer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib32" title="">2023</a>)</cite>, which limits the representational capacity of individual tokens and encourages more efficient learning across a larger vocabulary.
Similar to <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib58" title="">2023</a>)</cite>, we reduce the embedding dimension of the codebook to zero by replacing the codebook <math alttext="\mathbbm{C}\in\mathcal{R}^{K\times d}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">ℂ</mi><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p3.1.m1.1.1.3.2" xref="S4.SS2.p3.1.m1.1.1.3.2.cmml">ℛ</mi><mrow id="S4.SS2.p3.1.m1.1.1.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.3.3.2" xref="S4.SS2.p3.1.m1.1.1.3.3.2.cmml">K</mi><mo id="S4.SS2.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p3.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S4.SS2.p3.1.m1.1.1.3.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><in id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></in><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">ℂ</ci><apply id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.2">ℛ</ci><apply id="S4.SS2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3"><times id="S4.SS2.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.1"></times><ci id="S4.SS2.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.2">𝐾</ci><ci id="S4.SS2.p3.1.m1.1.1.3.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathbbm{C}\in\mathcal{R}^{K\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">blackboard_C ∈ caligraphic_R start_POSTSUPERSCRIPT italic_K × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> with an integer set <math alttext="\mathbbm{C}" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">ℂ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">ℂ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\mathbbm{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">blackboard_C</annotation></semantics></math> with <math alttext="|\mathbbm{C}|=K" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mrow id="S4.SS2.p3.3.m3.1.2" xref="S4.SS2.p3.3.m3.1.2.cmml"><mrow id="S4.SS2.p3.3.m3.1.2.2.2" xref="S4.SS2.p3.3.m3.1.2.2.1.cmml"><mo id="S4.SS2.p3.3.m3.1.2.2.2.1" stretchy="false" xref="S4.SS2.p3.3.m3.1.2.2.1.1.cmml">|</mo><mi id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml">ℂ</mi><mo id="S4.SS2.p3.3.m3.1.2.2.2.2" stretchy="false" xref="S4.SS2.p3.3.m3.1.2.2.1.1.cmml">|</mo></mrow><mo id="S4.SS2.p3.3.m3.1.2.1" xref="S4.SS2.p3.3.m3.1.2.1.cmml">=</mo><mi id="S4.SS2.p3.3.m3.1.2.3" xref="S4.SS2.p3.3.m3.1.2.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.2.cmml" xref="S4.SS2.p3.3.m3.1.2"><eq id="S4.SS2.p3.3.m3.1.2.1.cmml" xref="S4.SS2.p3.3.m3.1.2.1"></eq><apply id="S4.SS2.p3.3.m3.1.2.2.1.cmml" xref="S4.SS2.p3.3.m3.1.2.2.2"><abs id="S4.SS2.p3.3.m3.1.2.2.1.1.cmml" xref="S4.SS2.p3.3.m3.1.2.2.2.1"></abs><ci id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">ℂ</ci></apply><ci id="S4.SS2.p3.3.m3.1.2.3.cmml" xref="S4.SS2.p3.3.m3.1.2.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">|\mathbbm{C}|=K</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">| blackboard_C | = italic_K</annotation></semantics></math>.
Specifically, <math alttext="\mathbbm{C}" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mi id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">ℂ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><ci id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">ℂ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">\mathbbm{C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">blackboard_C</annotation></semantics></math> is the Cartesian product of single-dimensional variables <math alttext="\mathbbm{C}=" class="ltx_Math" display="inline" id="S4.SS2.p3.5.m5.1"><semantics id="S4.SS2.p3.5.m5.1a"><mrow id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml"><mi id="S4.SS2.p3.5.m5.1.1.2" xref="S4.SS2.p3.5.m5.1.1.2.cmml">ℂ</mi><mo id="S4.SS2.p3.5.m5.1.1.1" xref="S4.SS2.p3.5.m5.1.1.1.cmml">=</mo><mi id="S4.SS2.p3.5.m5.1.1.3" xref="S4.SS2.p3.5.m5.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1"><eq id="S4.SS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1.1"></eq><ci id="S4.SS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2">ℂ</ci><csymbol cd="latexml" id="S4.SS2.p3.5.m5.1.1.3.cmml" xref="S4.SS2.p3.5.m5.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">\mathbbm{C}=</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.5.m5.1d">blackboard_C =</annotation></semantics></math>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.p3.6.1" style="width:11.7pt;height:9.899999999999999pt;vertical-align:-1.2pt;"><span class="ltx_transformed_inner" style="transform:translate(1.9pt,-1.5pt) scale(1.5,1.5) ;">
<span class="ltx_p" id="S4.SS2.p3.6.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.p3.6.1.1.m1.1"><semantics id="S4.SS2.p3.6.1.1.m1.1a"><mo id="S4.SS2.p3.6.1.1.m1.1.1" xref="S4.SS2.p3.6.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.1.1.m1.1b"><times id="S4.SS2.p3.6.1.1.m1.1.1.cmml" xref="S4.SS2.p3.6.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.6.1.1.m1.1d">×</annotation></semantics></math></span>
</span></span><math alttext="{}_{i=1}^{d}C_{i}" class="ltx_Math" display="inline" id="S4.SS2.p3.7.m6.1"><semantics id="S4.SS2.p3.7.m6.1a"><mmultiscripts id="S4.SS2.p3.7.m6.1.1" xref="S4.SS2.p3.7.m6.1.1.cmml"><mi id="S4.SS2.p3.7.m6.1.1.2.2.2" xref="S4.SS2.p3.7.m6.1.1.2.2.2.cmml">C</mi><mi id="S4.SS2.p3.7.m6.1.1.2.2.3" xref="S4.SS2.p3.7.m6.1.1.2.2.3.cmml">i</mi><mrow id="S4.SS2.p3.7.m6.1.1a" xref="S4.SS2.p3.7.m6.1.1.cmml"></mrow><mprescripts id="S4.SS2.p3.7.m6.1.1b" xref="S4.SS2.p3.7.m6.1.1.cmml"></mprescripts><mrow id="S4.SS2.p3.7.m6.1.1.3" xref="S4.SS2.p3.7.m6.1.1.3.cmml"><mi id="S4.SS2.p3.7.m6.1.1.3.2" xref="S4.SS2.p3.7.m6.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.7.m6.1.1.3.1" xref="S4.SS2.p3.7.m6.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p3.7.m6.1.1.3.3" xref="S4.SS2.p3.7.m6.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.7.m6.1.1.2.3" xref="S4.SS2.p3.7.m6.1.1.2.3.cmml">d</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m6.1b"><apply id="S4.SS2.p3.7.m6.1.1.cmml" xref="S4.SS2.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m6.1.1.1.cmml" xref="S4.SS2.p3.7.m6.1.1">subscript</csymbol><apply id="S4.SS2.p3.7.m6.1.1.2.cmml" xref="S4.SS2.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m6.1.1.2.1.cmml" xref="S4.SS2.p3.7.m6.1.1">superscript</csymbol><apply id="S4.SS2.p3.7.m6.1.1.2.2.cmml" xref="S4.SS2.p3.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.7.m6.1.1.2.2.1.cmml" xref="S4.SS2.p3.7.m6.1.1">subscript</csymbol><ci id="S4.SS2.p3.7.m6.1.1.2.2.2.cmml" xref="S4.SS2.p3.7.m6.1.1.2.2.2">𝐶</ci><ci id="S4.SS2.p3.7.m6.1.1.2.2.3.cmml" xref="S4.SS2.p3.7.m6.1.1.2.2.3">𝑖</ci></apply><ci id="S4.SS2.p3.7.m6.1.1.2.3.cmml" xref="S4.SS2.p3.7.m6.1.1.2.3">𝑑</ci></apply><apply id="S4.SS2.p3.7.m6.1.1.3.cmml" xref="S4.SS2.p3.7.m6.1.1.3"><eq id="S4.SS2.p3.7.m6.1.1.3.1.cmml" xref="S4.SS2.p3.7.m6.1.1.3.1"></eq><ci id="S4.SS2.p3.7.m6.1.1.3.2.cmml" xref="S4.SS2.p3.7.m6.1.1.3.2">𝑖</ci><cn id="S4.SS2.p3.7.m6.1.1.3.3.cmml" type="integer" xref="S4.SS2.p3.7.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m6.1c">{}_{i=1}^{d}C_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.7.m6.1d">start_FLOATSUBSCRIPT italic_i = 1 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="C_{i}=\{-1,1\}" class="ltx_Math" display="inline" id="S4.SS2.p3.8.m7.2"><semantics id="S4.SS2.p3.8.m7.2a"><mrow id="S4.SS2.p3.8.m7.2.2" xref="S4.SS2.p3.8.m7.2.2.cmml"><msub id="S4.SS2.p3.8.m7.2.2.3" xref="S4.SS2.p3.8.m7.2.2.3.cmml"><mi id="S4.SS2.p3.8.m7.2.2.3.2" xref="S4.SS2.p3.8.m7.2.2.3.2.cmml">C</mi><mi id="S4.SS2.p3.8.m7.2.2.3.3" xref="S4.SS2.p3.8.m7.2.2.3.3.cmml">i</mi></msub><mo id="S4.SS2.p3.8.m7.2.2.2" xref="S4.SS2.p3.8.m7.2.2.2.cmml">=</mo><mrow id="S4.SS2.p3.8.m7.2.2.1.1" xref="S4.SS2.p3.8.m7.2.2.1.2.cmml"><mo id="S4.SS2.p3.8.m7.2.2.1.1.2" stretchy="false" xref="S4.SS2.p3.8.m7.2.2.1.2.cmml">{</mo><mrow id="S4.SS2.p3.8.m7.2.2.1.1.1" xref="S4.SS2.p3.8.m7.2.2.1.1.1.cmml"><mo id="S4.SS2.p3.8.m7.2.2.1.1.1a" xref="S4.SS2.p3.8.m7.2.2.1.1.1.cmml">−</mo><mn id="S4.SS2.p3.8.m7.2.2.1.1.1.2" xref="S4.SS2.p3.8.m7.2.2.1.1.1.2.cmml">1</mn></mrow><mo id="S4.SS2.p3.8.m7.2.2.1.1.3" xref="S4.SS2.p3.8.m7.2.2.1.2.cmml">,</mo><mn id="S4.SS2.p3.8.m7.1.1" xref="S4.SS2.p3.8.m7.1.1.cmml">1</mn><mo id="S4.SS2.p3.8.m7.2.2.1.1.4" stretchy="false" xref="S4.SS2.p3.8.m7.2.2.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m7.2b"><apply id="S4.SS2.p3.8.m7.2.2.cmml" xref="S4.SS2.p3.8.m7.2.2"><eq id="S4.SS2.p3.8.m7.2.2.2.cmml" xref="S4.SS2.p3.8.m7.2.2.2"></eq><apply id="S4.SS2.p3.8.m7.2.2.3.cmml" xref="S4.SS2.p3.8.m7.2.2.3"><csymbol cd="ambiguous" id="S4.SS2.p3.8.m7.2.2.3.1.cmml" xref="S4.SS2.p3.8.m7.2.2.3">subscript</csymbol><ci id="S4.SS2.p3.8.m7.2.2.3.2.cmml" xref="S4.SS2.p3.8.m7.2.2.3.2">𝐶</ci><ci id="S4.SS2.p3.8.m7.2.2.3.3.cmml" xref="S4.SS2.p3.8.m7.2.2.3.3">𝑖</ci></apply><set id="S4.SS2.p3.8.m7.2.2.1.2.cmml" xref="S4.SS2.p3.8.m7.2.2.1.1"><apply id="S4.SS2.p3.8.m7.2.2.1.1.1.cmml" xref="S4.SS2.p3.8.m7.2.2.1.1.1"><minus id="S4.SS2.p3.8.m7.2.2.1.1.1.1.cmml" xref="S4.SS2.p3.8.m7.2.2.1.1.1"></minus><cn id="S4.SS2.p3.8.m7.2.2.1.1.1.2.cmml" type="integer" xref="S4.SS2.p3.8.m7.2.2.1.1.1.2">1</cn></apply><cn id="S4.SS2.p3.8.m7.1.1.cmml" type="integer" xref="S4.SS2.p3.8.m7.1.1">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m7.2c">C_{i}=\{-1,1\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.8.m7.2d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { - 1 , 1 }</annotation></semantics></math> and <math alttext="d" class="ltx_Math" display="inline" id="S4.SS2.p3.9.m8.1"><semantics id="S4.SS2.p3.9.m8.1a"><mi id="S4.SS2.p3.9.m8.1.1" xref="S4.SS2.p3.9.m8.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.9.m8.1b"><ci id="S4.SS2.p3.9.m8.1.1.cmml" xref="S4.SS2.p3.9.m8.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.9.m8.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.9.m8.1d">italic_d</annotation></semantics></math> is equal to <math alttext="\log_{2}K" class="ltx_Math" display="inline" id="S4.SS2.p3.10.m9.1"><semantics id="S4.SS2.p3.10.m9.1a"><mrow id="S4.SS2.p3.10.m9.1.1" xref="S4.SS2.p3.10.m9.1.1.cmml"><msub id="S4.SS2.p3.10.m9.1.1.1" xref="S4.SS2.p3.10.m9.1.1.1.cmml"><mi id="S4.SS2.p3.10.m9.1.1.1.2" xref="S4.SS2.p3.10.m9.1.1.1.2.cmml">log</mi><mn id="S4.SS2.p3.10.m9.1.1.1.3" xref="S4.SS2.p3.10.m9.1.1.1.3.cmml">2</mn></msub><mo id="S4.SS2.p3.10.m9.1.1a" lspace="0.167em" xref="S4.SS2.p3.10.m9.1.1.cmml">⁡</mo><mi id="S4.SS2.p3.10.m9.1.1.2" xref="S4.SS2.p3.10.m9.1.1.2.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.10.m9.1b"><apply id="S4.SS2.p3.10.m9.1.1.cmml" xref="S4.SS2.p3.10.m9.1.1"><apply id="S4.SS2.p3.10.m9.1.1.1.cmml" xref="S4.SS2.p3.10.m9.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.10.m9.1.1.1.1.cmml" xref="S4.SS2.p3.10.m9.1.1.1">subscript</csymbol><log id="S4.SS2.p3.10.m9.1.1.1.2.cmml" xref="S4.SS2.p3.10.m9.1.1.1.2"></log><cn id="S4.SS2.p3.10.m9.1.1.1.3.cmml" type="integer" xref="S4.SS2.p3.10.m9.1.1.1.3">2</cn></apply><ci id="S4.SS2.p3.10.m9.1.1.2.cmml" xref="S4.SS2.p3.10.m9.1.1.2">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.10.m9.1c">\log_{2}K</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.10.m9.1d">roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_K</annotation></semantics></math>.
Given a feature vector <math alttext="z\in\mathbbm{R}^{d}" class="ltx_Math" display="inline" id="S4.SS2.p3.11.m10.1"><semantics id="S4.SS2.p3.11.m10.1a"><mrow id="S4.SS2.p3.11.m10.1.1" xref="S4.SS2.p3.11.m10.1.1.cmml"><mi id="S4.SS2.p3.11.m10.1.1.2" xref="S4.SS2.p3.11.m10.1.1.2.cmml">z</mi><mo id="S4.SS2.p3.11.m10.1.1.1" xref="S4.SS2.p3.11.m10.1.1.1.cmml">∈</mo><msup id="S4.SS2.p3.11.m10.1.1.3" xref="S4.SS2.p3.11.m10.1.1.3.cmml"><mi id="S4.SS2.p3.11.m10.1.1.3.2" xref="S4.SS2.p3.11.m10.1.1.3.2.cmml">ℝ</mi><mi id="S4.SS2.p3.11.m10.1.1.3.3" xref="S4.SS2.p3.11.m10.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.11.m10.1b"><apply id="S4.SS2.p3.11.m10.1.1.cmml" xref="S4.SS2.p3.11.m10.1.1"><in id="S4.SS2.p3.11.m10.1.1.1.cmml" xref="S4.SS2.p3.11.m10.1.1.1"></in><ci id="S4.SS2.p3.11.m10.1.1.2.cmml" xref="S4.SS2.p3.11.m10.1.1.2">𝑧</ci><apply id="S4.SS2.p3.11.m10.1.1.3.cmml" xref="S4.SS2.p3.11.m10.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.11.m10.1.1.3.1.cmml" xref="S4.SS2.p3.11.m10.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.11.m10.1.1.3.2.cmml" xref="S4.SS2.p3.11.m10.1.1.3.2">ℝ</ci><ci id="S4.SS2.p3.11.m10.1.1.3.3.cmml" xref="S4.SS2.p3.11.m10.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.11.m10.1c">z\in\mathbbm{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.11.m10.1d">italic_z ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, our quantizer <math alttext="Q(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p3.12.m11.1"><semantics id="S4.SS2.p3.12.m11.1a"><mrow id="S4.SS2.p3.12.m11.1.2" xref="S4.SS2.p3.12.m11.1.2.cmml"><mi id="S4.SS2.p3.12.m11.1.2.2" xref="S4.SS2.p3.12.m11.1.2.2.cmml">Q</mi><mo id="S4.SS2.p3.12.m11.1.2.1" xref="S4.SS2.p3.12.m11.1.2.1.cmml">⁢</mo><mrow id="S4.SS2.p3.12.m11.1.2.3.2" xref="S4.SS2.p3.12.m11.1.2.cmml"><mo id="S4.SS2.p3.12.m11.1.2.3.2.1" stretchy="false" xref="S4.SS2.p3.12.m11.1.2.cmml">(</mo><mo id="S4.SS2.p3.12.m11.1.1" lspace="0em" rspace="0em" xref="S4.SS2.p3.12.m11.1.1.cmml">⋅</mo><mo id="S4.SS2.p3.12.m11.1.2.3.2.2" stretchy="false" xref="S4.SS2.p3.12.m11.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.12.m11.1b"><apply id="S4.SS2.p3.12.m11.1.2.cmml" xref="S4.SS2.p3.12.m11.1.2"><times id="S4.SS2.p3.12.m11.1.2.1.cmml" xref="S4.SS2.p3.12.m11.1.2.1"></times><ci id="S4.SS2.p3.12.m11.1.2.2.cmml" xref="S4.SS2.p3.12.m11.1.2.2">𝑄</ci><ci id="S4.SS2.p3.12.m11.1.1.cmml" xref="S4.SS2.p3.12.m11.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.12.m11.1c">Q(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.12.m11.1d">italic_Q ( ⋅ )</annotation></semantics></math> converts each dimension of the quantized representation into:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q(z_{i})=\operatorname*{arg\,min}\nolimits_{c_{ik}}||z_{i}-c_{ik}||=-\mathbbm{%
1}\{z_{i}\leq 0\}+\mathbbm{1}\{z_{i}&gt;0\}," class="ltx_Math" display="block" id="S4.E2.m1.1"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.3.cmml">Q</mi><mo id="S4.E2.m1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E2.m1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S4.E2.m1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.1.1.1.1.6" xref="S4.E2.m1.1.1.1.1.6.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml"><msub id="S4.E2.m1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.3.cmml"><mrow id="S4.E2.m1.1.1.1.1.2.3.2" xref="S4.E2.m1.1.1.1.1.2.3.2.cmml"><mi id="S4.E2.m1.1.1.1.1.2.3.2.2" xref="S4.E2.m1.1.1.1.1.2.3.2.2.cmml">arg</mi><mo id="S4.E2.m1.1.1.1.1.2.3.2.1" lspace="0.170em" xref="S4.E2.m1.1.1.1.1.2.3.2.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.2.3.2.3" xref="S4.E2.m1.1.1.1.1.2.3.2.3.cmml">min</mi></mrow><msub id="S4.E2.m1.1.1.1.1.2.3.3" xref="S4.E2.m1.1.1.1.1.2.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.3.3.2" xref="S4.E2.m1.1.1.1.1.2.3.3.2.cmml">c</mi><mrow id="S4.E2.m1.1.1.1.1.2.3.3.3" xref="S4.E2.m1.1.1.1.1.2.3.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.3.3.3.2" xref="S4.E2.m1.1.1.1.1.2.3.3.3.2.cmml">i</mi><mo id="S4.E2.m1.1.1.1.1.2.3.3.3.1" xref="S4.E2.m1.1.1.1.1.2.3.3.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.2.3.3.3.3" xref="S4.E2.m1.1.1.1.1.2.3.3.3.3.cmml">k</mi></mrow></msub></msub><mo id="S4.E2.m1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.2.cmml">⁢</mo><mrow id="S4.E2.m1.1.1.1.1.2.1.1" xref="S4.E2.m1.1.1.1.1.2.1.2.cmml"><mo id="S4.E2.m1.1.1.1.1.2.1.1.2" stretchy="false" xref="S4.E2.m1.1.1.1.1.2.1.2.1.cmml">‖</mo><mrow id="S4.E2.m1.1.1.1.1.2.1.1.1" xref="S4.E2.m1.1.1.1.1.2.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.2.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.2.cmml">z</mi><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.2.1.1.1.1" xref="S4.E2.m1.1.1.1.1.2.1.1.1.1.cmml">−</mo><msub id="S4.E2.m1.1.1.1.1.2.1.1.1.3" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.2.cmml">c</mi><mrow id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.2" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.2.cmml">i</mi><mo id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.1" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.3" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.3.cmml">k</mi></mrow></msub></mrow><mo id="S4.E2.m1.1.1.1.1.2.1.1.3" stretchy="false" xref="S4.E2.m1.1.1.1.1.2.1.2.1.cmml">‖</mo></mrow></mrow><mo id="S4.E2.m1.1.1.1.1.7" xref="S4.E2.m1.1.1.1.1.7.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.4" xref="S4.E2.m1.1.1.1.1.4.cmml"><mrow id="S4.E2.m1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.3.1.cmml"><mo id="S4.E2.m1.1.1.1.1.3.1a" xref="S4.E2.m1.1.1.1.1.3.1.cmml">−</mo><mrow id="S4.E2.m1.1.1.1.1.3.1.1" xref="S4.E2.m1.1.1.1.1.3.1.1.cmml"><mn id="S4.E2.m1.1.1.1.1.3.1.1.3" xref="S4.E2.m1.1.1.1.1.3.1.1.3.cmml">𝟙</mn><mo id="S4.E2.m1.1.1.1.1.3.1.1.2" xref="S4.E2.m1.1.1.1.1.3.1.1.2.cmml">⁢</mo><mrow id="S4.E2.m1.1.1.1.1.3.1.1.1.1" xref="S4.E2.m1.1.1.1.1.3.1.1.1.2.cmml"><mo id="S4.E2.m1.1.1.1.1.3.1.1.1.1.2" stretchy="false" xref="S4.E2.m1.1.1.1.1.3.1.1.1.2.cmml">{</mo><mrow id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.2.cmml">z</mi><mi id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.1.cmml">≤</mo><mn id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.3.cmml">0</mn></mrow><mo id="S4.E2.m1.1.1.1.1.3.1.1.1.1.3" stretchy="false" xref="S4.E2.m1.1.1.1.1.3.1.1.1.2.cmml">}</mo></mrow></mrow></mrow><mo id="S4.E2.m1.1.1.1.1.4.3" xref="S4.E2.m1.1.1.1.1.4.3.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.4.2" xref="S4.E2.m1.1.1.1.1.4.2.cmml"><mn id="S4.E2.m1.1.1.1.1.4.2.3" xref="S4.E2.m1.1.1.1.1.4.2.3.cmml">𝟙</mn><mo id="S4.E2.m1.1.1.1.1.4.2.2" xref="S4.E2.m1.1.1.1.1.4.2.2.cmml">⁢</mo><mrow id="S4.E2.m1.1.1.1.1.4.2.1.1" xref="S4.E2.m1.1.1.1.1.4.2.1.2.cmml"><mo id="S4.E2.m1.1.1.1.1.4.2.1.1.2" stretchy="false" xref="S4.E2.m1.1.1.1.1.4.2.1.2.cmml">{</mo><mrow id="S4.E2.m1.1.1.1.1.4.2.1.1.1" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.2.cmml">z</mi><mi id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.4.2.1.1.1.1" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.1.cmml">&gt;</mo><mn id="S4.E2.m1.1.1.1.1.4.2.1.1.1.3" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.3.cmml">0</mn></mrow><mo id="S4.E2.m1.1.1.1.1.4.2.1.1.3" stretchy="false" xref="S4.E2.m1.1.1.1.1.4.2.1.2.cmml">}</mo></mrow></mrow></mrow></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><and id="S4.E2.m1.1.1.1.1a.cmml" xref="S4.E2.m1.1.1.1"></and><apply id="S4.E2.m1.1.1.1.1b.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.6.cmml" xref="S4.E2.m1.1.1.1.1.6"></eq><apply id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"><times id="S4.E2.m1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.2"></times><ci id="S4.E2.m1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.3">𝑄</ci><apply id="S4.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><times id="S4.E2.m1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2"></times><apply id="S4.E2.m1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3">subscript</csymbol><apply id="S4.E2.m1.1.1.1.1.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2"><times id="S4.E2.m1.1.1.1.1.2.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2.1"></times><ci id="S4.E2.m1.1.1.1.1.2.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2.2">arg</ci><ci id="S4.E2.m1.1.1.1.1.2.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.2.3">min</ci></apply><apply id="S4.E2.m1.1.1.1.1.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.2">𝑐</ci><apply id="S4.E2.m1.1.1.1.1.2.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.3"><times id="S4.E2.m1.1.1.1.1.2.3.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.2.3.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.3.2">𝑖</ci><ci id="S4.E2.m1.1.1.1.1.2.3.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3.3.3.3">𝑘</ci></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1.2.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1"><csymbol cd="latexml" id="S4.E2.m1.1.1.1.1.2.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.2">norm</csymbol><apply id="S4.E2.m1.1.1.1.1.2.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1"><minus id="S4.E2.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.1"></minus><apply id="S4.E2.m1.1.1.1.1.2.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.2">𝑧</ci><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E2.m1.1.1.1.1.2.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.2">𝑐</ci><apply id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3"><times id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.2">𝑖</ci><ci id="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.1.1.1.3.3.3">𝑘</ci></apply></apply></apply></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1c.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.7.cmml" xref="S4.E2.m1.1.1.1.1.7"></eq><share href="https://arxiv.org/html/2410.03311v1#S4.E2.m1.1.1.1.1.2.cmml" id="S4.E2.m1.1.1.1.1d.cmml" xref="S4.E2.m1.1.1.1"></share><apply id="S4.E2.m1.1.1.1.1.4.cmml" xref="S4.E2.m1.1.1.1.1.4"><plus id="S4.E2.m1.1.1.1.1.4.3.cmml" xref="S4.E2.m1.1.1.1.1.4.3"></plus><apply id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1"><minus id="S4.E2.m1.1.1.1.1.3.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1"></minus><apply id="S4.E2.m1.1.1.1.1.3.1.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1"><times id="S4.E2.m1.1.1.1.1.3.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.2"></times><cn id="S4.E2.m1.1.1.1.1.3.1.1.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.3.1.1.3">1</cn><set id="S4.E2.m1.1.1.1.1.3.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1"><apply id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1"><leq id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.1"></leq><apply id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.2">𝑧</ci><ci id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.2.3">𝑖</ci></apply><cn id="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.3.1.1.1.1.1.3">0</cn></apply></set></apply></apply><apply id="S4.E2.m1.1.1.1.1.4.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2"><times id="S4.E2.m1.1.1.1.1.4.2.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2.2"></times><cn id="S4.E2.m1.1.1.1.1.4.2.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.4.2.3">1</cn><set id="S4.E2.m1.1.1.1.1.4.2.1.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1"><apply id="S4.E2.m1.1.1.1.1.4.2.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1"><gt id="S4.E2.m1.1.1.1.1.4.2.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.1"></gt><apply id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.2">𝑧</ci><ci id="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.2.3">𝑖</ci></apply><cn id="S4.E2.m1.1.1.1.1.4.2.1.1.1.3.cmml" type="integer" xref="S4.E2.m1.1.1.1.1.4.2.1.1.1.3">0</cn></apply></set></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">Q(z_{i})=\operatorname*{arg\,min}\nolimits_{c_{ik}}||z_{i}-c_{ik}||=-\mathbbm{%
1}\{z_{i}\leq 0\}+\mathbbm{1}\{z_{i}&gt;0\},</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.1d">italic_Q ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT | | italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_c start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT | | = - blackboard_1 { italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≤ 0 } + blackboard_1 { italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &gt; 0 } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p3.16">where <math alttext="c_{ij}" class="ltx_Math" display="inline" id="S4.SS2.p3.13.m1.1"><semantics id="S4.SS2.p3.13.m1.1a"><msub id="S4.SS2.p3.13.m1.1.1" xref="S4.SS2.p3.13.m1.1.1.cmml"><mi id="S4.SS2.p3.13.m1.1.1.2" xref="S4.SS2.p3.13.m1.1.1.2.cmml">c</mi><mrow id="S4.SS2.p3.13.m1.1.1.3" xref="S4.SS2.p3.13.m1.1.1.3.cmml"><mi id="S4.SS2.p3.13.m1.1.1.3.2" xref="S4.SS2.p3.13.m1.1.1.3.2.cmml">i</mi><mo id="S4.SS2.p3.13.m1.1.1.3.1" xref="S4.SS2.p3.13.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.p3.13.m1.1.1.3.3" xref="S4.SS2.p3.13.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.13.m1.1b"><apply id="S4.SS2.p3.13.m1.1.1.cmml" xref="S4.SS2.p3.13.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.13.m1.1.1.1.cmml" xref="S4.SS2.p3.13.m1.1.1">subscript</csymbol><ci id="S4.SS2.p3.13.m1.1.1.2.cmml" xref="S4.SS2.p3.13.m1.1.1.2">𝑐</ci><apply id="S4.SS2.p3.13.m1.1.1.3.cmml" xref="S4.SS2.p3.13.m1.1.1.3"><times id="S4.SS2.p3.13.m1.1.1.3.1.cmml" xref="S4.SS2.p3.13.m1.1.1.3.1"></times><ci id="S4.SS2.p3.13.m1.1.1.3.2.cmml" xref="S4.SS2.p3.13.m1.1.1.3.2">𝑖</ci><ci id="S4.SS2.p3.13.m1.1.1.3.3.cmml" xref="S4.SS2.p3.13.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.13.m1.1c">c_{ij}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.13.m1.1d">italic_c start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> denotes the <math alttext="j" class="ltx_Math" display="inline" id="S4.SS2.p3.14.m2.1"><semantics id="S4.SS2.p3.14.m2.1a"><mi id="S4.SS2.p3.14.m2.1.1" xref="S4.SS2.p3.14.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.14.m2.1b"><ci id="S4.SS2.p3.14.m2.1.1.cmml" xref="S4.SS2.p3.14.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.14.m2.1c">j</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.14.m2.1d">italic_j</annotation></semantics></math>-th value of <math alttext="C_{i}" class="ltx_Math" display="inline" id="S4.SS2.p3.15.m3.1"><semantics id="S4.SS2.p3.15.m3.1a"><msub id="S4.SS2.p3.15.m3.1.1" xref="S4.SS2.p3.15.m3.1.1.cmml"><mi id="S4.SS2.p3.15.m3.1.1.2" xref="S4.SS2.p3.15.m3.1.1.2.cmml">C</mi><mi id="S4.SS2.p3.15.m3.1.1.3" xref="S4.SS2.p3.15.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.15.m3.1b"><apply id="S4.SS2.p3.15.m3.1.1.cmml" xref="S4.SS2.p3.15.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.15.m3.1.1.1.cmml" xref="S4.SS2.p3.15.m3.1.1">subscript</csymbol><ci id="S4.SS2.p3.15.m3.1.1.2.cmml" xref="S4.SS2.p3.15.m3.1.1.2">𝐶</ci><ci id="S4.SS2.p3.15.m3.1.1.3.cmml" xref="S4.SS2.p3.15.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.15.m3.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.15.m3.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. The token index is computed as <math alttext="Index(z)=\sum_{i=1}^{d}2^{i-1}\mathbbm{1}\{z_{i}&gt;0\}" class="ltx_Math" display="inline" id="S4.SS2.p3.16.m4.2"><semantics id="S4.SS2.p3.16.m4.2a"><mrow id="S4.SS2.p3.16.m4.2.2" xref="S4.SS2.p3.16.m4.2.2.cmml"><mrow id="S4.SS2.p3.16.m4.2.2.3" xref="S4.SS2.p3.16.m4.2.2.3.cmml"><mi id="S4.SS2.p3.16.m4.2.2.3.2" xref="S4.SS2.p3.16.m4.2.2.3.2.cmml">I</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">⁢</mo><mi id="S4.SS2.p3.16.m4.2.2.3.3" xref="S4.SS2.p3.16.m4.2.2.3.3.cmml">n</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1a" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">⁢</mo><mi id="S4.SS2.p3.16.m4.2.2.3.4" xref="S4.SS2.p3.16.m4.2.2.3.4.cmml">d</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1b" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">⁢</mo><mi id="S4.SS2.p3.16.m4.2.2.3.5" xref="S4.SS2.p3.16.m4.2.2.3.5.cmml">e</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1c" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">⁢</mo><mi id="S4.SS2.p3.16.m4.2.2.3.6" xref="S4.SS2.p3.16.m4.2.2.3.6.cmml">x</mi><mo id="S4.SS2.p3.16.m4.2.2.3.1d" xref="S4.SS2.p3.16.m4.2.2.3.1.cmml">⁢</mo><mrow id="S4.SS2.p3.16.m4.2.2.3.7.2" xref="S4.SS2.p3.16.m4.2.2.3.cmml"><mo id="S4.SS2.p3.16.m4.2.2.3.7.2.1" stretchy="false" xref="S4.SS2.p3.16.m4.2.2.3.cmml">(</mo><mi id="S4.SS2.p3.16.m4.1.1" xref="S4.SS2.p3.16.m4.1.1.cmml">z</mi><mo id="S4.SS2.p3.16.m4.2.2.3.7.2.2" stretchy="false" xref="S4.SS2.p3.16.m4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p3.16.m4.2.2.2" rspace="0.111em" xref="S4.SS2.p3.16.m4.2.2.2.cmml">=</mo><mrow id="S4.SS2.p3.16.m4.2.2.1" xref="S4.SS2.p3.16.m4.2.2.1.cmml"><msubsup id="S4.SS2.p3.16.m4.2.2.1.2" xref="S4.SS2.p3.16.m4.2.2.1.2.cmml"><mo id="S4.SS2.p3.16.m4.2.2.1.2.2.2" xref="S4.SS2.p3.16.m4.2.2.1.2.2.2.cmml">∑</mo><mrow id="S4.SS2.p3.16.m4.2.2.1.2.2.3" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.cmml"><mi id="S4.SS2.p3.16.m4.2.2.1.2.2.3.2" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.2.cmml">i</mi><mo id="S4.SS2.p3.16.m4.2.2.1.2.2.3.1" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.1.cmml">=</mo><mn id="S4.SS2.p3.16.m4.2.2.1.2.2.3.3" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p3.16.m4.2.2.1.2.3" xref="S4.SS2.p3.16.m4.2.2.1.2.3.cmml">d</mi></msubsup><mrow id="S4.SS2.p3.16.m4.2.2.1.1" xref="S4.SS2.p3.16.m4.2.2.1.1.cmml"><msup id="S4.SS2.p3.16.m4.2.2.1.1.3" xref="S4.SS2.p3.16.m4.2.2.1.1.3.cmml"><mn id="S4.SS2.p3.16.m4.2.2.1.1.3.2" xref="S4.SS2.p3.16.m4.2.2.1.1.3.2.cmml">2</mn><mrow id="S4.SS2.p3.16.m4.2.2.1.1.3.3" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.cmml"><mi id="S4.SS2.p3.16.m4.2.2.1.1.3.3.2" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.2.cmml">i</mi><mo id="S4.SS2.p3.16.m4.2.2.1.1.3.3.1" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.1.cmml">−</mo><mn id="S4.SS2.p3.16.m4.2.2.1.1.3.3.3" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.3.cmml">1</mn></mrow></msup><mo id="S4.SS2.p3.16.m4.2.2.1.1.2" xref="S4.SS2.p3.16.m4.2.2.1.1.2.cmml">⁢</mo><mn id="S4.SS2.p3.16.m4.2.2.1.1.4" xref="S4.SS2.p3.16.m4.2.2.1.1.4.cmml">𝟙</mn><mo id="S4.SS2.p3.16.m4.2.2.1.1.2a" xref="S4.SS2.p3.16.m4.2.2.1.1.2.cmml">⁢</mo><mrow id="S4.SS2.p3.16.m4.2.2.1.1.1.1" xref="S4.SS2.p3.16.m4.2.2.1.1.1.2.cmml"><mo id="S4.SS2.p3.16.m4.2.2.1.1.1.1.2" stretchy="false" xref="S4.SS2.p3.16.m4.2.2.1.1.1.2.cmml">{</mo><mrow id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.cmml"><msub id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.cmml"><mi id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.2" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.2.cmml">z</mi><mi id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.3" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.1" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.1.cmml">&gt;</mo><mn id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.3" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.3.cmml">0</mn></mrow><mo id="S4.SS2.p3.16.m4.2.2.1.1.1.1.3" stretchy="false" xref="S4.SS2.p3.16.m4.2.2.1.1.1.2.cmml">}</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.16.m4.2b"><apply id="S4.SS2.p3.16.m4.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2"><eq id="S4.SS2.p3.16.m4.2.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2.2"></eq><apply id="S4.SS2.p3.16.m4.2.2.3.cmml" xref="S4.SS2.p3.16.m4.2.2.3"><times id="S4.SS2.p3.16.m4.2.2.3.1.cmml" xref="S4.SS2.p3.16.m4.2.2.3.1"></times><ci id="S4.SS2.p3.16.m4.2.2.3.2.cmml" xref="S4.SS2.p3.16.m4.2.2.3.2">𝐼</ci><ci id="S4.SS2.p3.16.m4.2.2.3.3.cmml" xref="S4.SS2.p3.16.m4.2.2.3.3">𝑛</ci><ci id="S4.SS2.p3.16.m4.2.2.3.4.cmml" xref="S4.SS2.p3.16.m4.2.2.3.4">𝑑</ci><ci id="S4.SS2.p3.16.m4.2.2.3.5.cmml" xref="S4.SS2.p3.16.m4.2.2.3.5">𝑒</ci><ci id="S4.SS2.p3.16.m4.2.2.3.6.cmml" xref="S4.SS2.p3.16.m4.2.2.3.6">𝑥</ci><ci id="S4.SS2.p3.16.m4.1.1.cmml" xref="S4.SS2.p3.16.m4.1.1">𝑧</ci></apply><apply id="S4.SS2.p3.16.m4.2.2.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1"><apply id="S4.SS2.p3.16.m4.2.2.1.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m4.2.2.1.2.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2">superscript</csymbol><apply id="S4.SS2.p3.16.m4.2.2.1.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m4.2.2.1.2.2.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2">subscript</csymbol><sum id="S4.SS2.p3.16.m4.2.2.1.2.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.2.2"></sum><apply id="S4.SS2.p3.16.m4.2.2.1.2.2.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3"><eq id="S4.SS2.p3.16.m4.2.2.1.2.2.3.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.1"></eq><ci id="S4.SS2.p3.16.m4.2.2.1.2.2.3.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.2">𝑖</ci><cn id="S4.SS2.p3.16.m4.2.2.1.2.2.3.3.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S4.SS2.p3.16.m4.2.2.1.2.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.2.3">𝑑</ci></apply><apply id="S4.SS2.p3.16.m4.2.2.1.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1"><times id="S4.SS2.p3.16.m4.2.2.1.1.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.2"></times><apply id="S4.SS2.p3.16.m4.2.2.1.1.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m4.2.2.1.1.3.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3">superscript</csymbol><cn id="S4.SS2.p3.16.m4.2.2.1.1.3.2.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.1.3.2">2</cn><apply id="S4.SS2.p3.16.m4.2.2.1.1.3.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3"><minus id="S4.SS2.p3.16.m4.2.2.1.1.3.3.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.1"></minus><ci id="S4.SS2.p3.16.m4.2.2.1.1.3.3.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.2">𝑖</ci><cn id="S4.SS2.p3.16.m4.2.2.1.1.3.3.3.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.1.3.3.3">1</cn></apply></apply><cn id="S4.SS2.p3.16.m4.2.2.1.1.4.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.1.4">1</cn><set id="S4.SS2.p3.16.m4.2.2.1.1.1.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1"><apply id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1"><gt id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.1"></gt><apply id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.1.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.2.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.2">𝑧</ci><ci id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.3.cmml" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.2.3">𝑖</ci></apply><cn id="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS2.p3.16.m4.2.2.1.1.1.1.1.3">0</cn></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.16.m4.2c">Index(z)=\sum_{i=1}^{d}2^{i-1}\mathbbm{1}\{z_{i}&gt;0\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.16.m4.2d">italic_I italic_n italic_d italic_e italic_x ( italic_z ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_i - 1 end_POSTSUPERSCRIPT blackboard_1 { italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &gt; 0 }</annotation></semantics></math>.
To train the tokenizer, we employ a standard combination of reconstruction, perceptual, and commitment losses, along with an entropy penalty to promote better codebook utilization <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib58" title="">2023</a>)</cite>.
Importantly, we exclude the use of GAN loss, as it was found to negatively impact training stability.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Datasets.</span>
Our investigation first is conducted on the following text-to-motion datasets: HumanML3D <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> and Motion-X <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>.
HumanML3D comprises 14,616 motion clips sourced from the AMASS dataset <cite class="ltx_cite ltx_citemacro_citep">(Mahmood et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib29" title="">2019</a>)</cite>, paired with 44,970 textual descriptions.
Motion-X, a more recent dataset, includes approximately 81,000 motion clips.
To validate our conclusions on larger-scale data, we also carry out experiments on the proposed MotionBase dataset with two variants: MotionBase-0.5 and MotionBase-1.0.
MotionBase-0.5 contains 500,000 clips, while MotionBase-1.0 encompasses the full scope of our collected data, with over 1 million clips.
Following standard practice, each dataset is split into training, validation, and test sets in proportions of 85%, 5%, and 15%, respectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Evaluation Metrics. </span>
For the motion generation task, we employ the following metrics in our experiments following <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite>.
(1) Frechet Inception Distance (FID): This metric assesses overall motion quality by measuring the distributional difference between the high-level features of generated motions and real motions.
(2) Motion-retrieval Precision (R-Precision) and Multimodal Distance (MMDist): These metrics evaluate the semantic alignment between the textual input and generated motions.
R-Precision measures the top-1/2/3 retrieval accuracy, while MMDist computes the distance between matched text and motion pairs.
Additionally, we validate our motion tokenizer by conducting experiments on the motion reconstruction task.
This is measured using both Mean Per Joint Position Error (MPJPE) and FID.
MPJPE quantifies the average distance (in millimeters) between the predicted joint positions and the ground truth positions across all joints in the skeleton.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.6"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.6.1">Implementation Details. </span>
For the motion tokenizer, we implement a VQ codebook <math alttext="\mathbbm{C}\in\mathbbm{R}^{1024\times 512}" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">ℂ</mi><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml"><mi id="S5.SS1.p3.1.m1.1.1.3.2" xref="S5.SS1.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S5.SS1.p3.1.m1.1.1.3.3" xref="S5.SS1.p3.1.m1.1.1.3.3.cmml"><mn id="S5.SS1.p3.1.m1.1.1.3.3.2" xref="S5.SS1.p3.1.m1.1.1.3.3.2.cmml">1024</mn><mo id="S5.SS1.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p3.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S5.SS1.p3.1.m1.1.1.3.3.3" xref="S5.SS1.p3.1.m1.1.1.3.3.3.cmml">512</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><in id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></in><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">ℂ</ci><apply id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.3.2.cmml" xref="S5.SS1.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S5.SS1.p3.1.m1.1.1.3.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3"><times id="S5.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3.1"></times><cn id="S5.SS1.p3.1.m1.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1.3.3.2">1024</cn><cn id="S5.SS1.p3.1.m1.1.1.3.3.3.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1.3.3.3">512</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\mathbbm{C}\in\mathbbm{R}^{1024\times 512}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">blackboard_C ∈ blackboard_R start_POSTSUPERSCRIPT 1024 × 512 end_POSTSUPERSCRIPT</annotation></semantics></math> with an embedding dimensionality of <math alttext="d=512" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">d</mi><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><eq id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></eq><ci id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">𝑑</ci><cn id="S5.SS1.p3.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.p3.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">d=512</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">italic_d = 512</annotation></semantics></math>, and the resulting discrete codes are incorporated as additional vocabulary for the LLM.
In comparison, our lookup-free codebook has a size of <math alttext="2^{16}=16384" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3.1"><semantics id="S5.SS1.p3.3.m3.1a"><mrow id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><msup id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml"><mn id="S5.SS1.p3.3.m3.1.1.2.2" xref="S5.SS1.p3.3.m3.1.1.2.2.cmml">2</mn><mn id="S5.SS1.p3.3.m3.1.1.2.3" xref="S5.SS1.p3.3.m3.1.1.2.3.cmml">16</mn></msup><mo id="S5.SS1.p3.3.m3.1.1.1" xref="S5.SS1.p3.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.3.m3.1.1.3" xref="S5.SS1.p3.3.m3.1.1.3.cmml">16384</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><eq id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1.1"></eq><apply id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.1.1.2.1.cmml" xref="S5.SS1.p3.3.m3.1.1.2">superscript</csymbol><cn id="S5.SS1.p3.3.m3.1.1.2.2.cmml" type="integer" xref="S5.SS1.p3.3.m3.1.1.2.2">2</cn><cn id="S5.SS1.p3.3.m3.1.1.2.3.cmml" type="integer" xref="S5.SS1.p3.3.m3.1.1.2.3">16</cn></apply><cn id="S5.SS1.p3.3.m3.1.1.3.cmml" type="integer" xref="S5.SS1.p3.3.m3.1.1.3">16384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">2^{16}=16384</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m3.1d">2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT = 16384</annotation></semantics></math>, where the least frequently used tokens from the LLM’s codebook are mapped to represent motion codes.
The motion encoder <math alttext="\mathbbm{E}" class="ltx_Math" display="inline" id="S5.SS1.p3.4.m4.1"><semantics id="S5.SS1.p3.4.m4.1a"><mi id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml">𝔼</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><ci id="S5.SS1.p3.4.m4.1.1.cmml" xref="S5.SS1.p3.4.m4.1.1">𝔼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">\mathbbm{E}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.4.m4.1d">blackboard_E</annotation></semantics></math> operates with a temporal downsampling rate of <math alttext="\alpha=4" class="ltx_Math" display="inline" id="S5.SS1.p3.5.m5.1"><semantics id="S5.SS1.p3.5.m5.1a"><mrow id="S5.SS1.p3.5.m5.1.1" xref="S5.SS1.p3.5.m5.1.1.cmml"><mi id="S5.SS1.p3.5.m5.1.1.2" xref="S5.SS1.p3.5.m5.1.1.2.cmml">α</mi><mo id="S5.SS1.p3.5.m5.1.1.1" xref="S5.SS1.p3.5.m5.1.1.1.cmml">=</mo><mn id="S5.SS1.p3.5.m5.1.1.3" xref="S5.SS1.p3.5.m5.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.5.m5.1b"><apply id="S5.SS1.p3.5.m5.1.1.cmml" xref="S5.SS1.p3.5.m5.1.1"><eq id="S5.SS1.p3.5.m5.1.1.1.cmml" xref="S5.SS1.p3.5.m5.1.1.1"></eq><ci id="S5.SS1.p3.5.m5.1.1.2.cmml" xref="S5.SS1.p3.5.m5.1.1.2">𝛼</ci><cn id="S5.SS1.p3.5.m5.1.1.3.cmml" type="integer" xref="S5.SS1.p3.5.m5.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.5.m5.1c">\alpha=4</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.5.m5.1d">italic_α = 4</annotation></semantics></math>.
We experiment with four LLM architectures to build our large motion model: GPT2-medium <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib38" title="">2019</a>)</cite>, Llama-2-7b, Llama-2-13b <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib47" title="">2023b</a>)</cite>, and Llama3.1-8b <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib12" title="">2024</a>)</cite>.
The motion tokenizer is trained with a learning rate of 1e-4 and a batch size of 256 over 300K iterations.
For training the large motion model, full parameter tuning is performed on 8<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.p3.6.m6.1"><semantics id="S5.SS1.p3.6.m6.1a"><mo id="S5.SS1.p3.6.m6.1.1" xref="S5.SS1.p3.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.6.m6.1b"><times id="S5.SS1.p3.6.m6.1.1.cmml" xref="S5.SS1.p3.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.6.m6.1d">×</annotation></semantics></math>A800 GPUs, with a batch size of 1024, over 300 epochs.
The learning rate is set to 2e-4 for GPT2-medium and 2e-5 for the Llama models.
Further details are provided in the appendix due to space limitation.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparisons under different model and data sizes. All experiments are conducted using the same pretrained VQ model for consistency. Additionally, we re-train the motion autoencoder and text encoder  <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> separately on the Motion-X and MotionBase datasets, using their respective data to train the motion autoencoder for each dataset’s evaluation.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.6" style="width:398.0pt;height:275.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.1pt,24.3pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.6.6.7.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="3" id="S5.T2.6.6.7.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_tt" colspan="3" id="S5.T2.6.6.7.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">Motion-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_tt" colspan="3" id="S5.T2.6.6.7.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">MotionBase</th>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.6.6.6.7" style="padding-left:10.0pt;padding-right:10.0pt;">Decoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.6.6.6.8" style="padding-left:10.0pt;padding-right:10.0pt;">#Inst.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.6.6.6.9" style="padding-left:10.0pt;padding-right:10.0pt;">#Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.1.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.m1.1"><semantics id="S5.T2.2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.2.m1.1.1" stretchy="false" xref="S5.T2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.m1.1b"><ci id="S5.T2.2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T2.3.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.3.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T2.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.4.4.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.4.4.4.4.m1.1"><semantics id="S5.T2.4.4.4.4.m1.1a"><mo id="S5.T2.4.4.4.4.m1.1.1" stretchy="false" xref="S5.T2.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.5.5.5.5" style="padding-left:10.0pt;padding-right:10.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.5.5.5.5.m1.1"><semantics id="S5.T2.5.5.5.5.m1.1a"><mo id="S5.T2.5.5.5.5.m1.1.1" stretchy="false" xref="S5.T2.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.5.m1.1b"><ci id="S5.T2.5.5.5.5.m1.1.1.cmml" xref="S5.T2.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.5.5.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.6.6.6.6" style="padding-left:10.0pt;padding-right:10.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.6.6.6.6.m1.1"><semantics id="S5.T2.6.6.6.6.m1.1a"><mo id="S5.T2.6.6.6.6.m1.1.1" stretchy="false" xref="S5.T2.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.6.m1.1b"><ci id="S5.T2.6.6.6.6.m1.1.1.cmml" xref="S5.T2.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.6.6.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.6.6.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.8.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.8.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.8.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.8.1.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.206</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.8.1.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.402</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.8.1.6" style="padding-left:10.0pt;padding-right:10.0pt;">54.017</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.8.1.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.046</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.8.1.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.136</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.8.1.9" style="padding-left:10.0pt;padding-right:10.0pt;">173.275</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.9.2.1" style="padding-left:10.0pt;padding-right:10.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.9.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.9.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.9.2.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.468</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.9.2.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.791</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.9.2.6" style="padding-left:10.0pt;padding-right:10.0pt;">0.096</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.9.2.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.090</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.9.2.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.215</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.9.2.9" style="padding-left:10.0pt;padding-right:10.0pt;">251.358</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.10.3.1" style="padding-left:10.0pt;padding-right:10.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.10.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.10.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.358</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.618</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.10.3.6" style="padding-left:10.0pt;padding-right:10.0pt;">4.852</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.10.3.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.116</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.276</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.10.3.9" style="padding-left:10.0pt;padding-right:10.0pt;">157.950</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.11.4.1" style="padding-left:10.0pt;padding-right:10.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4.2" style="padding-left:10.0pt;padding-right:10.0pt;">1M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.11.4.3" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.11.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.357</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.614</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.11.4.6" style="padding-left:10.0pt;padding-right:10.0pt;">5.083</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.11.4.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.118</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.269</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.11.4.9" style="padding-left:10.0pt;padding-right:10.0pt;">121.917</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.12.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.12.5.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.12.5.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.12.5.3" style="padding-left:10.0pt;padding-right:10.0pt;">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.12.5.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.207</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.12.5.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.405</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.12.5.6" style="padding-left:10.0pt;padding-right:10.0pt;">53.354</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.12.5.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.042</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.12.5.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.123</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.12.5.9" style="padding-left:10.0pt;padding-right:10.0pt;">160.845</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.13.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.13.6.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.13.6.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.13.6.3" style="padding-left:10.0pt;padding-right:10.0pt;">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.13.6.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.471</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.13.6.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.794</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.13.6.6" style="padding-left:10.0pt;padding-right:10.0pt;">0.159</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.13.6.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.093</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.13.6.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.222</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.13.6.9" style="padding-left:10.0pt;padding-right:10.0pt;">253.289</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.14.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.14.7.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.14.7.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.14.7.3" style="padding-left:10.0pt;padding-right:10.0pt;">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.14.7.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.372</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.14.7.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.627</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.14.7.6" style="padding-left:10.0pt;padding-right:10.0pt;">4.908</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.14.7.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.125</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.14.7.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.272</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.14.7.9" style="padding-left:10.0pt;padding-right:10.0pt;">87.288</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.15.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.15.8.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.15.8.2" style="padding-left:10.0pt;padding-right:10.0pt;">1.0M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.15.8.3" style="padding-left:10.0pt;padding-right:10.0pt;">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.15.8.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.351</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.15.8.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.602</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.15.8.6" style="padding-left:10.0pt;padding-right:10.0pt;">5.582</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.15.8.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.125</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.15.8.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.267</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.15.8.9" style="padding-left:10.0pt;padding-right:10.0pt;">83.024</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.16.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.16.9.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.16.9.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.16.9.3" style="padding-left:10.0pt;padding-right:10.0pt;">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.16.9.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.217</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.16.9.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.418</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.16.9.6" style="padding-left:10.0pt;padding-right:10.0pt;">54.004</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.16.9.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.043</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.16.9.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.124</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.16.9.9" style="padding-left:10.0pt;padding-right:10.0pt;">162.102</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.17.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.17.10.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-3</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.17.10.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.17.10.3" style="padding-left:10.0pt;padding-right:10.0pt;">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.17.10.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.483</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.17.10.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.802</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.17.10.6" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.17.10.6.1">0.103</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.17.10.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.082</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.17.10.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.214</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.17.10.9" style="padding-left:10.0pt;padding-right:10.0pt;">249.790</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.18.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.18.11.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-3</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.18.11.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.18.11.3" style="padding-left:10.0pt;padding-right:10.0pt;">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.18.11.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.363</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.18.11.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.625</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.18.11.6" style="padding-left:10.0pt;padding-right:10.0pt;">4.798</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.18.11.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.121</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.18.11.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.264</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.18.11.9" style="padding-left:10.0pt;padding-right:10.0pt;">81.389</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.19.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.19.12.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-3</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.19.12.2" style="padding-left:10.0pt;padding-right:10.0pt;">1M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.19.12.3" style="padding-left:10.0pt;padding-right:10.0pt;">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.19.12.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.354</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.19.12.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.611</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.19.12.6" style="padding-left:10.0pt;padding-right:10.0pt;">5.100</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.19.12.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.129</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.19.12.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.270</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.19.12.9" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.19.12.9.1">68.083</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.20.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.20.13.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.20.13.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.20.13.3" style="padding-left:10.0pt;padding-right:10.0pt;">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.20.13.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.225</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.20.13.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.436</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.6.6.20.13.6" style="padding-left:10.0pt;padding-right:10.0pt;">53.447</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.6.6.20.13.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.045</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.20.13.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.125</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.6.6.20.13.9" style="padding-left:10.0pt;padding-right:10.0pt;">159.368</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.21.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.21.14.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.21.14.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.21.14.3" style="padding-left:10.0pt;padding-right:10.0pt;">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.21.14.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.21.14.4.1">0.486</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.21.14.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.21.14.5.1">0.805</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.21.14.6" style="padding-left:10.0pt;padding-right:10.0pt;">0.132</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.21.14.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.086</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.21.14.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.218</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.21.14.9" style="padding-left:10.0pt;padding-right:10.0pt;">249.868</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.22.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.22.15.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.22.15.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.5M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.22.15.3" style="padding-left:10.0pt;padding-right:10.0pt;">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.22.15.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.375</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.22.15.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.636</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.6.6.22.15.6" style="padding-left:10.0pt;padding-right:10.0pt;">4.792</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T2.6.6.22.15.7" style="padding-left:10.0pt;padding-right:10.0pt;">0.116</th>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.22.15.8" style="padding-left:10.0pt;padding-right:10.0pt;">0.267</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.22.15.9" style="padding-left:10.0pt;padding-right:10.0pt;">80.473</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6.23.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T2.6.6.23.16.1" style="padding-left:10.0pt;padding-right:10.0pt;">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.23.16.2" style="padding-left:10.0pt;padding-right:10.0pt;">1.0M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.6.6.23.16.3" style="padding-left:10.0pt;padding-right:10.0pt;">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T2.6.6.23.16.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.359</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.23.16.5" style="padding-left:10.0pt;padding-right:10.0pt;">0.612</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.6.6.23.16.6" style="padding-left:10.0pt;padding-right:10.0pt;">5.370</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T2.6.6.23.16.7" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.23.16.7.1">0.131</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.23.16.8" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.6.6.23.16.8.1">0.277</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.6.6.23.16.9" style="padding-left:10.0pt;padding-right:10.0pt;">78.665</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Discussion of Scaling up motion generation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In this section, we investigate the impact of model size and data scale on motion generation performance.
We utilize the motion autoencoder  <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> retrained on Motion-X and MotionBase datasets to evaluate performance on their respective test sets.
We categorize our training data into four scales: 0.02M (HumanML3D only), 0.08M (Motion-X only), 0.5M (MotionBase-0.5), and 1M (MotionBase-1.0).
To ensure fair comparison, we employ the same VQ as the motion tokenizer, maintaining consistency across experiments to validate our conclusions.</p>
</div>
<figure class="ltx_table ltx_align_floatright" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison with existing SoTA methods on the HumanML3D benchmark. Results marked with <math alttext="*" class="ltx_Math" display="inline" id="S5.T3.2.m1.1"><semantics id="S5.T3.2.m1.1b"><mo id="S5.T3.2.m1.1.1" xref="S5.T3.2.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.m1.1c"><times id="S5.T3.2.m1.1.1.cmml" xref="S5.T3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.m1.1e">∗</annotation></semantics></math> represent values reproduced using the officially released code, while unmarked results are taken from the original papers.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.11" style="width:242.5pt;height:183.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.4pt,16.2pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.11.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.6.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.6.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.6.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">Decoder</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.3.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.3.1.1.1.m1.1"><semantics id="S5.T3.3.1.1.1.m1.1a"><mo id="S5.T3.3.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.1.1.1.m1.1b"><ci id="S5.T3.3.1.1.1.m1.1.1.cmml" xref="S5.T3.3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.1.1.1.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.4.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.4.2.2.2.m1.1"><semantics id="S5.T3.4.2.2.2.m1.1a"><mo id="S5.T3.4.2.2.2.m1.1.1" stretchy="false" xref="S5.T3.4.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.2.2.2.m1.1b"><ci id="S5.T3.4.2.2.2.m1.1.1.cmml" xref="S5.T3.4.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.2.2.2.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.5.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.5.3.3.3.m1.1"><semantics id="S5.T3.5.3.3.3.m1.1a"><mo id="S5.T3.5.3.3.3.m1.1.1" stretchy="false" xref="S5.T3.5.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.3.3.3.m1.1b"><ci id="S5.T3.5.3.3.3.m1.1.1.cmml" xref="S5.T3.5.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.3.3.3.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.6.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">MMDist <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.6.4.4.4.m1.1"><semantics id="S5.T3.6.4.4.4.m1.1a"><mo id="S5.T3.6.4.4.4.m1.1.1" stretchy="false" xref="S5.T3.6.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.4.4.4.m1.1b"><ci id="S5.T3.6.4.4.4.m1.1.1.cmml" xref="S5.T3.6.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.4.4.4.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.9.10.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">MLD</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.9.10.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">-</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.10.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.481</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.10.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.772</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.10.1.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.473</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.10.1.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.196</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.11.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionDiffuse</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.11.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">-</th>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.11.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.491</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.11.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.782</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.11.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.630</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.11.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.113</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.9.12.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">T2M-GPT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.9.12.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.12.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.492</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.12.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.775</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.12.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.12.3.5.1">0.141</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.9.12.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.121</td>
</tr>
<tr class="ltx_tr" id="S5.T3.7.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.7.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.7.5.5.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.7.5.5.1.1.1">1,∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.7.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">T5</th>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.409</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.667</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.162</td>
<td class="ltx_td ltx_align_center" id="S5.T3.7.5.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.992</td>
</tr>
<tr class="ltx_tr" id="S5.T3.8.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.8.6.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.8.6.6.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.8.6.6.1.1.1">1</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.8.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">T5</th>
<td class="ltx_td ltx_align_center" id="S5.T3.8.6.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.492</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.6.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.778</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.6.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.232</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.6.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.096</td>
</tr>
<tr class="ltx_tr" id="S5.T3.9.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.9.7.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.9.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.9.7.7.1.1.1">2,∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.9.7.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-2-13B</th>
<td class="ltx_td ltx_align_center" id="S5.T3.9.7.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.367</td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.7.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.654</td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.7.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.571</td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.7.7.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.981</td>
</tr>
<tr class="ltx_tr" id="S5.T3.10.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.10.8.8.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.10.8.8.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.10.8.8.1.1.1">2,∗</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.10.8.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-1-13B</th>
<td class="ltx_td ltx_align_center" id="S5.T3.10.8.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.363</td>
<td class="ltx_td ltx_align_center" id="S5.T3.10.8.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.633</td>
<td class="ltx_td ltx_align_center" id="S5.T3.10.8.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.592</td>
<td class="ltx_td ltx_align_center" id="S5.T3.10.8.8.6" style="padding-left:3.0pt;padding-right:3.0pt;">4.029</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.9.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionGPT<sup class="ltx_sup" id="S5.T3.11.9.9.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.11.9.9.1.1.1">2</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.9.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-1-13B</th>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.9.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.411</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.9.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.696</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.9.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.542</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.9.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.584</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.13.4.1" style="padding-left:3.0pt;padding-right:3.0pt;">MotionLLM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.13.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">Gemma-2b</th>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.13.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.482</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.13.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.770</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.13.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.491</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.13.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">3.138</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.14.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">AvatarGPT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.11.9.14.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-1-13B</th>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.14.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">0.389</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.14.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">0.623</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.14.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.567</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.9.14.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.11.9.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T3.11.9.15.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T3.11.9.15.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">Llama-2-13B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.9.15.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.15.6.3.1">0.519</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.9.15.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.15.6.4.1">0.803</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.9.15.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">0.166</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T3.11.9.15.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.11.9.15.6.6.1">2.964</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Does increasing model size benefit motion generation? </span>
Yes.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T2" title="Table 2 ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">2</span></a>, our results demonstrate that increasing model size leads to significant performance improvements when provided with the same amount of training data.
Specifically, Llama2-13b outperforms Llama2-7b, which in turn surpasses GPT2-medium, illustrating a clear trend of performance gains as model capacity increases.
This suggests that models with larger size are better equipped to capture diverse, complex patterns and relationships within human motions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Does increasing data scale benefit motion generation?</span>
Yes.
In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T2" title="Table 2 ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">2</span></a>, when using the same foundation model, increasing the scale of training data leads to substantial performance gains on the MotionBase test set, aligning with our expected scaling laws.
This improvement is particularly pronounced in the R-precision metric, emphasizing the critical role of data scale in enhancing semantic alignment between generated motions and text prompts.
However, contrary to our expectations, we observe a noticeable performance decline on the Motion-X test set if not trained on Motion-X (0.08M).
We attribute this to the limitations of the retrieval-based evaluation model, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS4" title="5.4 Limitation of Automated Metric ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_align_right ltx_img_landscape" height="254" id="S5.F3.g1" src="x3.png" width="343"/>
<figcaption class="ltx_caption ltx_align_right"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Training curves with Y-axis denoting R@1 retrieval accuracy. All these models are trained for 300 epochs at most and are evaluated every 1000 steps.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Does the large motion model perform SoTA competitively?</span>
We evaluate our large motion model on the widely adopted HumanML3D benchmark.
We compare its performance against a variety of SoTA approaches.
This includes diffusion-based methods such as MLD <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib8" title="">2023</a>)</cite> and MotionDiffuse <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib61" title="">2022</a>)</cite>, as well as the GPT-based T2M-GPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib60" title="">2023a</a>)</cite>.
We also compare against LLM fine-tuning methods like MotionGPT <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib21" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib64" title="">2024b</a>)</cite>, MotionLLM <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib52" title="">2024</a>)</cite>, and AvatarGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib68" title="">2024</a>)</cite>.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T3" title="Table 3 ‣ 5.2 Discussion of Scaling up motion generation ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">3</span></a>, our model, which utilizes Llama-2-13B as the decoder and calculates the loss over the entire concatenated sequence of input text, achieves SOTA performance.
Our large motion model significantly outperforms other LLM-based methods such as MotionGPT and AvatarGPT, as well as the earlier T2M-GPT.
In particular, we observe substantial improvements in key metrics such as R@1, R@3, and MMDist, highlighting our model’s ability to generate motion sequences that are better aligned with text descriptions and of higher quality.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Slow convergence of large motion models.</span>
To evaluate the convergence speed of large motion models, we train GPT-2, Llama2-7b, and Llama3.1-8b for 300 epochs on Motion-X.
The training curve of with R@1 performance is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.F3" title="Figure 3 ‣ 5.2 Discussion of Scaling up motion generation ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">3</span></a>.
We obverse that all large motion models nearly converge by 200 epochs, with larger models converging faster.
Initializing these models with pre-trained weights proves beneficial for speeding up convergence.
Compared to large multimodal models like LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib27" title="">2023</a>)</cite>, large motion models require more epochs to capture the complex representations of motion sequences.
We attribute the slow convergence of these models to the limited representation capacity of the motion tokenizer, which contains only 512 motion tokens.
This suggests the need to optimize the motion tokenizer and expand its representation space.
To address this, we explore 2D-LFQ quantization method as a promising alternative.</p>
</div>
<figure class="ltx_table ltx_align_floatright" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation of the effectiveness of synthetic data and static data.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.3" style="width:185.4pt;height:68.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.4pt,6.1pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.3.3.3.4">TRAIN SET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.2.m1.1"><semantics id="S5.T4.2.2.2.2.m1.1a"><mo id="S5.T4.2.2.2.2.m1.1.1" stretchy="false" xref="S5.T4.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.m1.1b"><ci id="S5.T4.2.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.3.3.3">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.3.3.3.3.m1.1"><semantics id="S5.T4.3.3.3.3.m1.1a"><mo id="S5.T4.3.3.3.3.m1.1.1" stretchy="false" xref="S5.T4.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.3.m1.1b"><ci id="S5.T4.3.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T4.3.3.4.1.1">w/o static &amp; syn</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.2">0.101</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.3">0.231</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.3.4.1.4">261.325</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.3.3.5.2.1">w/o static</th>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.2">0.110</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.3">0.248</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.5.2.4">286.809</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T4.3.3.6.3.1">MotionBase</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.6.3.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.6.3.2.1">0.118</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.6.3.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.6.3.3.1">0.269</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.6.3.4"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.6.3.4.1">121.917</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p6.1.1">Does Static and Synthetic Data help?</span>
Yes, the addition of static image data and synthesized data both contribute to improvements, as illustrated in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:syn_and_static_part</span>, more analysis can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS1" title="C.1 Ablation of Synthesis and Static Data? ‣ Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">C.1</span></a>.</p>
</div>
<figure class="ltx_table ltx_align_floatright" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation of out-of-domain evaluation on UNSEEN-90K dataset, where <math alttext="\#N" class="ltx_Math" display="inline" id="S5.T5.3.m1.1"><semantics id="S5.T5.3.m1.1b"><mrow id="S5.T5.3.m1.1.1" xref="S5.T5.3.m1.1.1.cmml"><mi id="S5.T5.3.m1.1.1.2" mathvariant="normal" xref="S5.T5.3.m1.1.1.2.cmml">#</mi><mo id="S5.T5.3.m1.1.1.1" xref="S5.T5.3.m1.1.1.1.cmml">⁢</mo><mi id="S5.T5.3.m1.1.1.3" xref="S5.T5.3.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.3.m1.1c"><apply id="S5.T5.3.m1.1.1.cmml" xref="S5.T5.3.m1.1.1"><times id="S5.T5.3.m1.1.1.1.cmml" xref="S5.T5.3.m1.1.1.1"></times><ci id="S5.T5.3.m1.1.1.2.cmml" xref="S5.T5.3.m1.1.1.2">#</ci><ci id="S5.T5.3.m1.1.1.3.cmml" xref="S5.T5.3.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.m1.1d">\#N</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.m1.1e"># italic_N</annotation></semantics></math> denotes we use <math alttext="N" class="ltx_Math" display="inline" id="S5.T5.4.m2.1"><semantics id="S5.T5.4.m2.1b"><mi id="S5.T5.4.m2.1.1" xref="S5.T5.4.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T5.4.m2.1c"><ci id="S5.T5.4.m2.1.1.cmml" xref="S5.T5.4.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.m2.1d">N</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.m2.1e">italic_N</annotation></semantics></math> subsets of MotionBase for training.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.7" style="width:214.7pt;height:68.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.9pt,6.1pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.7.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.7.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.7.3.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">TRAIN SET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.5.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.5.1.1.1.m1.1"><semantics id="S5.T5.5.1.1.1.m1.1a"><mo id="S5.T5.5.1.1.1.m1.1.1" stretchy="false" xref="S5.T5.5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.1.1.1.m1.1b"><ci id="S5.T5.5.1.1.1.m1.1.1.cmml" xref="S5.T5.5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.6.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.6.2.2.2.m1.1"><semantics id="S5.T5.6.2.2.2.m1.1a"><mo id="S5.T5.6.2.2.2.m1.1.1" stretchy="false" xref="S5.T5.6.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.2.2.2.m1.1b"><ci id="S5.T5.6.2.2.2.m1.1.1.cmml" xref="S5.T5.6.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.7.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T5.7.3.3.3.m1.1"><semantics id="S5.T5.7.3.3.3.m1.1a"><mo id="S5.T5.7.3.3.3.m1.1.1" stretchy="false" xref="S5.T5.7.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.7.3.3.3.m1.1b"><ci id="S5.T5.7.3.3.3.m1.1.1.cmml" xref="S5.T5.7.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.7.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T5.7.3.4.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">HumanML3D</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.3.4.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.0264</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.3.4.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.0832</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.7.3.4.1.4" style="padding-left:10.0pt;padding-right:10.0pt;">257.563</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T5.7.3.5.2.1" style="padding-left:10.0pt;padding-right:10.0pt;">MotionX</th>
<td class="ltx_td ltx_align_center" id="S5.T5.7.3.5.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">0.0224</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.3.5.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.0705</td>
<td class="ltx_td ltx_align_center" id="S5.T5.7.3.5.2.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.7.3.5.2.4.1">246.220</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.3.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T5.7.3.6.3.1" style="padding-left:10.0pt;padding-right:10.0pt;">MotionBase-#38</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.3.6.3.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.7.3.6.3.2.1">0.0761</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.3.6.3.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.7.3.6.3.3.1">0.2090</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.3.6.3.4" style="padding-left:10.0pt;padding-right:10.0pt;">263.539</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p7.1.1">Do large motion models outperform in out-of-distribution setup? </span>
Yes.
We present the results in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:out_of_distribute</span>.
This ablation is essential for further validating the generalization capabilities of large motion models, as the improvements observed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T2" title="Table 2 ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">2</span></a> may stem from the inclusion of additional in-domain data from Motion-X.
In this setup, we select four subsets from MotionBase, comprising 90K samples (UNSEEN-90K), for evaluation, while the remaining 38 subsets are used for training.
This ensures that the test set consists entirely of out-of-domain (OOD) samples.
We compare the performance of models trained on HumanML3D, MotionX, and Motion-#38, all utilizing the GPT2-medium architecture, where <math alttext="\#N" class="ltx_Math" display="inline" id="S5.SS2.p7.1.m1.1"><semantics id="S5.SS2.p7.1.m1.1a"><mrow id="S5.SS2.p7.1.m1.1.1" xref="S5.SS2.p7.1.m1.1.1.cmml"><mi id="S5.SS2.p7.1.m1.1.1.2" mathvariant="normal" xref="S5.SS2.p7.1.m1.1.1.2.cmml">#</mi><mo id="S5.SS2.p7.1.m1.1.1.1" xref="S5.SS2.p7.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p7.1.m1.1.1.3" xref="S5.SS2.p7.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.1.m1.1b"><apply id="S5.SS2.p7.1.m1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1"><times id="S5.SS2.p7.1.m1.1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1.1"></times><ci id="S5.SS2.p7.1.m1.1.1.2.cmml" xref="S5.SS2.p7.1.m1.1.1.2">#</ci><ci id="S5.SS2.p7.1.m1.1.1.3.cmml" xref="S5.SS2.p7.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.1.m1.1c">\#N</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.1.m1.1d"># italic_N</annotation></semantics></math> denotes the number of training subsets.
All models are trained using the GPT2-medium.
The results on the OOD test set clearly demonstrate that the model trained on MotionBase significantly outperforms those trained on HumanML3D and MotionX, particularly in terms of R@1 and R@3 metrics.
These findings strongly highlight the superior generalization ability of large motion models when handling unseen OOD data, especially when trained on diverse, large-scale datasets.
However, we once again observe unexpected results with the FID metric, which will be discussed further in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.SS4" title="5.4 Limitation of Automated Metric ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S5.F4.g1" src="x4.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Comparison with different motion quantization on Motion-X (<span class="ltx_text ltx_font_bold" id="S5.F4.5.1">left</span>) and MotionBase (<span class="ltx_text ltx_font_bold" id="S5.F4.6.2">right</span>). Note that we only show MPJPE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.F4.2.m1.1"><semantics id="S5.F4.2.m1.1b"><mo id="S5.F4.2.m1.1.1" stretchy="false" xref="S5.F4.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.F4.2.m1.1c"><ci id="S5.F4.2.m1.1.1.cmml" xref="S5.F4.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.m1.1e">↓</annotation></semantics></math>) results here. FID results is shown in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS5" title="C.5 Ablation of Motion Quantization ‣ Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">C.5</span></a>.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Discussion of Motion Quantization</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.3">In this section, we investigate the impact of different motion quantization methods.
We compare our proposed 2D lookup-free quantization (2D-LFQ) against two commonly used approaches: residual vector quantization (RVQ) and vector quantization (VQ), across various codebook sizes ranging from <math alttext="2^{8}" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><msup id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mn id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">2</mn><mn id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">8</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">superscript</csymbol><cn id="S5.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1.2">2</cn><cn id="S5.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">2^{8}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="2^{16}" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><msup id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">2</mn><mn id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">16</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">superscript</csymbol><cn id="S5.SS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.SS3.p1.2.m2.1.1.2">2</cn><cn id="S5.SS3.p1.2.m2.1.1.3.cmml" type="integer" xref="S5.SS3.p1.2.m2.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">2^{16}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT</annotation></semantics></math>.
The number of parameters for RVQ/VQ and 2D-LFQ are 19.43M and 108.35M, respectively.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.F4" title="Figure 4 ‣ 5.2 Discussion of Scaling up motion generation ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">4</span></a>, 2D-LFQ demonstrates significant improvements over both RVQ and VQ. Notably, as the codebook size increases, 2D-LFQ continues to enhance performance, while RVQ and VQ experience diminishing returns or performance degradation with larger codebooks.
Our deeper analysis attributes these gains to better codebook utilization by 2D-LFQ.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.F5" title="Figure 5 ‣ 5.3 Discussion of Motion Quantization ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates that the utilization rates for VQ and RVQ begin to decline once the codebook size exceeds <math alttext="2^{10}" class="ltx_Math" display="inline" id="S5.SS3.p1.3.m3.1"><semantics id="S5.SS3.p1.3.m3.1a"><msup id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mn id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">2</mn><mn id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml">10</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">superscript</csymbol><cn id="S5.SS3.p1.3.m3.1.1.2.cmml" type="integer" xref="S5.SS3.p1.3.m3.1.1.2">2</cn><cn id="S5.SS3.p1.3.m3.1.1.3.cmml" type="integer" xref="S5.SS3.p1.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">2^{10}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.3.m3.1d">2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math>, which corresponds to the peak performance for these methods, whereas the utilization of 2D-LFQ continues to increase with larger codebooks.
Additionally, we conduct further experiments to validate the benefits of 2D motion encoding in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS5" title="C.5 Ablation of Motion Quantization ‣ Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">C.5</span></a>.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="256" id="S5.F5.g1" src="x5.png" width="342"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of codebook utilization for different motion quantization methods.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Limitation of Automated Metric</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">As mentioned earlier, the FID scores in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T2" title="Table 2 ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:out_of_distribute</span> yield unexpected results.
Specifically, when evaluating on Motion-X and UNSEEN-90K, FID achieves its best performance when trained on Motion-X, significantly outperforming both the smaller HumanML3D and the larger-scale MotionBase.
In this section, we aim to investigate this anomaly.
FID, a standard metric widely used for generation tasks, is typically measured by a pretrained evaluator.
In traditional image generation, FID is calculated using a well-trained, robust visual encoder like InceptionNet <cite class="ltx_cite ltx_citemacro_citep">(Szegedy et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib43" title="">2015</a>)</cite>, which is trained on millions of images.
However, the evaluator currently used to compute FID for motion generation is a simple motion autoencoder with a very small parameter scale <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite>.
Since this motion autoencoder is trained on limited data consisting of only 20K motions, we argue that it may lack the generalization needed for robust performance, leading to difficulties in reliably capturing the complex semantic alignment between text and motion.Similar unexpected results occur in motion reconstruction as well.
As show in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T6" title="Table 6 ‣ 5.4 Limitation of Automated Metric ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">6</span></a>, the FID score on HumanML3D is two orders of magnitude higher when comparing 2D-LFQ and VQ-VAE, despite the former achieving a much lower MPJPE.
When tested on MotionBase, 2D-LFQ obtains the highest FID score even while achieving the best MPJPE.
We observe the same issue with other metrics like MMDist, as discussed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.SS1" title="C.1 Ablation of Synthesis and Static Data? ‣ Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">C.1</span></a>.
Notably, <cite class="ltx_cite ltx_citemacro_cite">Voas et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib49" title="">2023</a>)</cite> have mentioned that existing metrics are sensitive to the quality of the embedding space and do not always align with human perception.
These findings highlight the need for a more robust and fair evaluation metric for large motion models moving forward.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Robustness investigation of the evaluation metrics on the motion reconstruction task.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.2" style="width:348.4pt;height:76.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.7pt,6.8pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.2.2.3.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T6.2.2.3.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="S5.T6.2.2.3.1.2">HumanML3D</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="S5.T6.2.2.3.1.3">Motion-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="S5.T6.2.2.3.1.4">MotionBase</th>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.3">Tokenizer</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.4">#Num.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.2.2.2.5">#Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.1.1">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T6.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.m1.1b"><ci id="S5.T6.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.2.2.2.2">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T6.2.2.2.2.m1.1"><semantics id="S5.T6.2.2.2.2.m1.1a"><mo id="S5.T6.2.2.2.2.m1.1.1" stretchy="false" xref="S5.T6.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.2.m1.1b"><ci id="S5.T6.2.2.2.2.m1.1.1.cmml" xref="S5.T6.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.6">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.2.2.2.7">MPJPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.8">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.2.2.2.9">MPJPE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.2.2.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.1">VQ-VAE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.2">512</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.4.1.3">19.43M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.4">0.078</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.4.1.5">69.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.6">0.852</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.4.1.7">106.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.8">4.366</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.2.4.1.9">123.6</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2.5.2">
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.1">RQ-VAE</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.2">512</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.2.5.2.3">19.43M</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.4"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.5.2.4.1">0.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.2.5.2.5"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.5.2.5.1">37.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.6">0.568</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.2.2.5.2.7">56.9</td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.8"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.5.2.8.1">4.026</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.2.2.5.2.9">78.2</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2.6.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.1">2D-LFQ</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.2">16384</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.2.2.6.3.3">108.35M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.4"><span class="ltx_text" id="S5.T6.2.2.6.3.4.1" style="color:#FF0000;">1.769</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.2.2.6.3.5">45.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.6"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.6.3.6.1">0.295</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.2.2.6.3.7"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.6.3.7.1">54.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.8"><span class="ltx_text" id="S5.T6.2.2.6.3.8.1" style="color:#FF0000;">7.853</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.2.6.3.9"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.6.3.9.1">64.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we explore how to advance the field of large-scale motion generation.
To this end, we introduce a large-scale motion dataset named MotionBase, which includes detailed text descriptions and rich modality annotations, providing a strong foundation for effectively training large motion models.
Our research highlights key findings, such as the impact of scaling both data and model size.
Additionally, we identify potential limitations in the current evaluation metrics, particularly when assessing diverse and unseen motions.
To enhances the benefits large motion models can derive from extensive motion data, we propose a novel motion quantization approach that treats motion clips as 2D images and constructs a finite-scale codebook, eliminating the need for token lookups.
We hope that this research offers valuable direction for future work in large-scale motion generation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn et al. (2018)</span>
<span class="ltx_bibblock">
Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and Songhwai Oh.

</span>
<span class="ltx_bibblock">Text2action: Generative adversarial synthesis from language to action.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2018 IEEE International Conference on Robotics and Automation (ICRA)</em>, pp.  5915–5920. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahuja &amp; Morency (2019)</span>
<span class="ltx_bibblock">
Chaitanya Ahuja and Louis-Philippe Morency.

</span>
<span class="ltx_bibblock">Language2pose: Natural language grounded pose forecasting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2019 International Conference on 3D Vision (3DV)</em>, pp.  719–728. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aliakbarian et al. (2020)</span>
<span class="ltx_bibblock">
Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, and Stephen Gould.

</span>
<span class="ltx_bibblock">A stochastic conditioning scheme for diverse human motion prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  5223–5232, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain et al. (2021)</span>
<span class="ltx_bibblock">
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Frozen in time: A joint video and image encoder for end-to-end retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  1728–1738, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2018)</span>
<span class="ltx_bibblock">
Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung Tang.

</span>
<span class="ltx_bibblock">Deep video generation, prediction and completion of human action sequences.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, pp.  366–382, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cervantes et al. (2022)</span>
<span class="ltx_bibblock">
Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi Shinoda.

</span>
<span class="ltx_bibblock">Implicit neural representations for variable length human motion generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">European Conference on Computer Vision</em>, pp.  356–372. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu.

</span>
<span class="ltx_bibblock">Executing your commands via motion diffusion in latent space.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  18000–18010, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2021)</span>
<span class="ltx_bibblock">
Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, and Chi-Keung Tang.

</span>
<span class="ltx_bibblock">Haa500: Human-centric atomic action dataset with curated videos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  13465–13474, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2305.06500</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fragkiadaki et al. (2015)</span>
<span class="ltx_bibblock">
Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik.

</span>
<span class="ltx_bibblock">Recurrent network models for human dynamics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE international conference on computer vision</em>, pp.  4346–4354, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et al. (2017)</span>
<span class="ltx_bibblock">
Partha Ghosh, Jie Song, Emre Aksan, and Otmar Hilliges.

</span>
<span class="ltx_bibblock">Learning human motion models for long-term predictions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">2017 International Conference on 3D Vision (3DV)</em>, pp.  458–466. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gopalakrishnan et al. (2019)</span>
<span class="ltx_bibblock">
Anand Gopalakrishnan, Ankur Mali, Dan Kifer, Lee Giles, and Alexander G Ororbia.

</span>
<span class="ltx_bibblock">A neural temporal model for human motion prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  12116–12125, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2020)</span>
<span class="ltx_bibblock">
Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng.

</span>
<span class="ltx_bibblock">Action2motion: Conditioned generation of 3d human motions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 28th ACM International Conference on Multimedia</em>, pp.  2021–2029, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2022a)</span>
<span class="ltx_bibblock">
Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng.

</span>
<span class="ltx_bibblock">Generating diverse and natural 3d human motions from text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  5152–5161, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2022b)</span>
<span class="ltx_bibblock">
Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng.

</span>
<span class="ltx_bibblock">Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">European Conference on Computer Vision</em>, pp.  580–597. Springer, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2024)</span>
<span class="ltx_bibblock">
Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng.

</span>
<span class="ltx_bibblock">Momask: Generative masked modeling of 3d human motions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  1900–1910, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2022)</span>
<span class="ltx_bibblock">
Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Martín-Martín, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Maskvit: Masked visual pre-training for video prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2206.11894</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen.

</span>
<span class="ltx_bibblock">Motiongpt: Human motion as a foreign language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in Neural Information Processing Systems</em>, 36:20067–20079, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han.

</span>
<span class="ltx_bibblock">Autoregressive image generation using residual quantization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  11523–11532, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Mimic-it: Multi-modal in-context instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2306.05425</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2301.12597</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li.

</span>
<span class="ltx_bibblock">One-stage 3d whole-body mesh recovery with component aware transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  21159–21168, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang.

</span>
<span class="ltx_bibblock">Motion-x: A large-scale 3d expressive whole-body human motion dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2304.08485</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Zhenguang Liu, Shuang Wu, Shuyuan Jin, Shouling Ji, Qi Liu, Shijian Lu, and Li Cheng.

</span>
<span class="ltx_bibblock">Investigating pose representations and motion contexts modeling for 3d motion prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, 45(1):681–697, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahmood et al. (2019)</span>
<span class="ltx_bibblock">
Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black.

</span>
<span class="ltx_bibblock">Amass: Archive of motion capture as surface shapes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  5442–5451, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2019)</span>
<span class="ltx_bibblock">
Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong Li.

</span>
<span class="ltx_bibblock">Learning trajectory dependencies for human motion prediction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, pp.  9489–9497, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. (2017)</span>
<span class="ltx_bibblock">
Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt.

</span>
<span class="ltx_bibblock">Monocular 3d human pose estimation in the wild using improved cnn supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">2017 international conference on 3D vision (3DV)</em>, pp.  506–516. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mentzer et al. (2023)</span>
<span class="ltx_bibblock">
Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen.

</span>
<span class="ltx_bibblock">Finite scalar quantization: Vq-vae made simple.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2309.15505</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4o mini: advancing cost-efficient intelligence.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/" title="">https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Advances in neural information processing systems</em>, 35:27730–27744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlakos et al. (2019)</span>
<span class="ltx_bibblock">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black.

</span>
<span class="ltx_bibblock">Expressive body capture: 3d hands, face, and body from a single image.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  10975–10985, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrovich et al. (2022)</span>
<span class="ltx_bibblock">
Mathis Petrovich, Michael J Black, and Gül Varol.

</span>
<span class="ltx_bibblock">Temos: Generating diverse human motions from textual descriptions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">European Conference on Computer Vision</em>, pp.  480–497. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plappert et al. (2016)</span>
<span class="ltx_bibblock">
Matthias Plappert, Christian Mandery, and Tamim Asfour.

</span>
<span class="ltx_bibblock">The kit motion-language dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Big data</em>, 4(4):236–252, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">OpenAI blog</em>, 1(8):9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">International conference on machine learning</em>, pp.  8748–8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et al. (2024)</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sárándi et al. (2023)</span>
<span class="ltx_bibblock">
István Sárándi, Alexander Hermans, and Bastian Leibe.

</span>
<span class="ltx_bibblock">Learning 3d human pose estimation from dozens of datasets using a geometry-aware autoencoder to bridge between skeleton formats.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pp.  2956–2966, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szegedy et al. (2015)</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  1–9, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taheri et al. (2020)</span>
<span class="ltx_bibblock">
Omid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios Tzionas.

</span>
<span class="ltx_bibblock">Grab: A dataset of whole-body human grasping of objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16</em>, pp.  581–600. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tevet et al. (2022)</span>
<span class="ltx_bibblock">
Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Motionclip: Exposing human motion generation to clip space.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">European Conference on Computer Vision</em>, pp.  358–374. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Den Oord et al. (2017)</span>
<span class="ltx_bibblock">
Aaron Van Den Oord, Oriol Vinyals, et al.

</span>
<span class="ltx_bibblock">Neural discrete representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voas et al. (2023)</span>
<span class="ltx_bibblock">
Jordan Voas, Yili Wang, Qixing Huang, and Raymond Mooney.

</span>
<span class="ltx_bibblock">What is the best automated metric for text to motion generation?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">SIGGRAPH Asia 2023 Conference Papers</em>, pp.  1–11, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al.

</span>
<span class="ltx_bibblock">Internvid: A large-scale video-text dataset for multimodal understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2307.06942</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan Zhou, Junsong Yuan, and Changyou Chen.

</span>
<span class="ltx_bibblock">Learning diverse stochastic human-action generators by learning smooth latent transitions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volume 34, pp.  12281–12288, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Qi Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, and Chi-Keung Tang.

</span>
<span class="ltx_bibblock">Motionllm: Multimodal motion-language learning with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2405.17013</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Boshen Xu, Ziheng Wang, Yang Du, Sipeng Zheng, Zhinan Song, and Qin Jin.

</span>
<span class="ltx_bibblock">Egonce++: Do egocentric video-language models really understand hand-object interactions?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2405.17719</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2022)</span>
<span class="ltx_bibblock">
Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Vitpose: Simple vision transformer baselines for human pose estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Advances in Neural Information Processing Systems</em>, 35:38571–38584, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2021)</span>
<span class="ltx_bibblock">
Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas.

</span>
<span class="ltx_bibblock">Videogpt: Video generation using vq-vae and transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2104.10157</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2304.14178</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et al. (2022)</span>
<span class="ltx_bibblock">
Tackgeun You, Saehoon Kim, Chiheon Kim, Doyup Lee, and Bohyung Han.

</span>
<span class="ltx_bibblock">Locally hierarchical auto-regressive modeling for image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in Neural Information Processing Systems</em>, 35:16360–16372, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Lijun Yu, José Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al.

</span>
<span class="ltx_bibblock">Language model beats diffusion–tokenizer is key to visual generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2310.05737</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2022)</span>
<span class="ltx_bibblock">
Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz.

</span>
<span class="ltx_bibblock">Glamr: Global occlusion-aware human mesh recovery with dynamic cameras.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  11038–11049, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan.

</span>
<span class="ltx_bibblock">Generating human motion from textual descriptions with discrete representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  14730–14740, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Motiondiffuse: Text-driven human motion generation with diffusion model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2208.15001</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Siwei Zhang, Bharat Lal Bhatnagar, Yuanlu Xu, Alexander Winkler, Petr Kadlecek, Siyu Tang, and Federica Bogo.

</span>
<span class="ltx_bibblock">Rohm: Robust human motion reconstruction via diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  14606–14617, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2306.17107</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang.

</span>
<span class="ltx_bibblock">Motiongpt: Finetuned llms are general-purpose motion generators.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 38, pp.  7368–7376, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Bo Zhao, Boya Wu, and Tiejun Huang.

</span>
<span class="ltx_bibblock">Svit: Scaling up visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2307.04087</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Sipeng Zheng, Yicheng Feng, Zongqing Lu, et al.

</span>
<span class="ltx_bibblock">Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2024)</span>
<span class="ltx_bibblock">
Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, and Zongqing Lu.

</span>
<span class="ltx_bibblock">Unicode: Learning a unified codebook for multimodal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2403.09072</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
Zixiang Zhou, Yu Wan, and Baoyuan Wang.

</span>
<span class="ltx_bibblock">Avatargpt: All-in-one framework for motion understanding planning generation and beyond.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  1357–1366, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\appendixpage</span>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Details of MoseBase</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this section, we provide more details about <span class="ltx_text ltx_font_bold" id="A1.p1.1.1">Motionbase</span> that are not included in the main paper due to spatial limitations.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Statistic Analyses</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">MotionBase contains over 1 million motion sequences from 42 different public datasets and web videos on the Internet.
Subsets of MotionX, including Animation, Perform, Dance, Aist, Kungfu, GRAB <cite class="ltx_cite ltx_citemacro_citep">(Taheri et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib44" title="">2020</a>)</cite>, Music, Idea400 <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib26" title="">2024</a>)</cite>, HAA500 <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib10" title="">2021</a>)</cite>, Game Motion, and Fitness, are included in MotionBase.
Recognizing the high cost of collecting and annotating videos, we also see the untapped potential of images for motion understanding.
Consequently, MotionBase incorporates image data by repeating each image across 64 frames and treating it as a motion sequence.
For the datasets with long-range videos, such as MPI-INF-3DHP <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib31" title="">2017</a>)</cite>, we segment the footage into sub-clips with random durations ranging from 10 seconds to one minute.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F6" title="Figure 6 ‣ A.1 Statistic Analyses ‣ Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">6</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F7" title="Figure 7 ‣ A.1 Statistic Analyses ‣ Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">7</span></a> illustrate the scale and length distributions of MotionBase.</p>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="575" id="A1.F6.g1" src="x6.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The scale distribution of motion sequences across subsets of MotionBase.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="576" id="A1.F7.g1" src="x7.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The length distribution across different subsets of MotionBase</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1123" id="A1.F8.g1" src="x8.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Prompt examples to label human motions in the video. We use Gemini-1.5-pro and GPT-4o-mini to generate motion descriptions for the video and image data, respectively. We provide “whole-body” (UP) and “part-level” (DOWN) labels for each sample in the dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Prompt of Motion Description</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In this paper, we use Gemini-1.5-pro <cite class="ltx_cite ltx_citemacro_citep">(Reid et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib41" title="">2024</a>)</cite> and GPT-4o-mini <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib33" title="">2024</a>)</cite> as large multimodal models (LMM) to generate textual annotations for video and image data, respectively.
For each person-centric sample, we first crop and track the person’s body using the corresponding bounding box(es).
The LMM is then tasked with focusing on the person’s physical movements and positions in the global space to generate detailed descriptions.
Unlike previous datasets, we provide more granular motion descriptions by dividing the body into upper and lower sections, prompting the LMM to generate part-specific descriptions (“part-level”).
Additionally, an overall summary of the entire body’s movement (“whole-body”) is also produced.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F8" title="Figure 8 ‣ A.1 Statistic Analyses ‣ Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates the prompt used to caption human motion sequences in MotionBase.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Word Distribution Analysis</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">To further explore the annotated motion text, we generate word clouds from the entire text corpus in MotionBase.
Since the annotations in MotionBase consist of both whole-body and part-level descriptions, we create separate word clouds for general labels and more detailed annotations, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F9" title="Figure 9 ‣ A.3 Word Distribution Analysis ‣ Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">9</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F10" title="Figure 10 ‣ A.3 Word Distribution Analysis ‣ Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">10</span></a>, respectively.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F9" title="Figure 9 ‣ A.3 Word Distribution Analysis ‣ Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">9</span></a>, we observe that the whole-body annotations primarily highlight high-level motion activities, such as standing, sitting, and walking.
In contrast, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A1.F10" title="Figure 10 ‣ A.3 Word Distribution Analysis ‣ Appendix A Additional Details of MoseBase ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">10</span></a> shows that part-level annotations focus more on specific body movements, including the torso, shoulders, legs, and arms.
We believe that this hierarchical structure of annotations will enhance the understanding of motion.</p>
</div>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="361" id="A1.F9.g1" src="x9.png" width="722"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Word cloud of whole-body textual annotation in MotionBase.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="363" id="A1.F10.g1" src="x10.png" width="723"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Word cloud of part-level textual annotation in MotionBase.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Overview of Model Architecture</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Due to space limitations in the main paper, we provide the overview of our model architecture in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A2.F11" title="Figure 11 ‣ Appendix B Additional Overview of Model Architecture ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">11</span></a> in this appendix.
Following most LMMs, our large motion model consists of two stages: pre-training and fine-tuning.
During the pre-training stage, we train a motion encoder, a motion decoder, and a motion codebook to represent motions using discrete tokens.
With this motion tokenizer, we fine-tune an autoregressive language model to predict motion tokens.
In the inference stage, the input text is processed by the language model to generate motion tokens in an autoregressive manner, which are then decoded into natural motion by the pre-trained motion decoder.</p>
</div>
<figure class="ltx_figure" id="A2.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="175" id="A2.F11.g1" src="x11.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Overview of the large motion model, which can be divided into two stages. In the first stage(<span class="ltx_text ltx_font_bold" id="A2.F11.3.1">left</span>), we pre-train a motion VQ-VAE to quantify motion sequences into tokens. In the second stage(<span class="ltx_text ltx_font_bold" id="A2.F11.4.2">right</span>), we fine-tune an autoregressive language model to predict motion tokens. </figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Experimental Results</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In this section, we provide more experimental analysis which can not be presented in our main paper due to space limitation.</p>
</div>
<figure class="ltx_table" id="A3.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Ablation of the effectiveness of synthetic data and static data.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T7.4" style="width:270.8pt;height:76.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.1pt,2.0pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T7.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T7.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T7.4.4.4.5" style="padding-left:7.0pt;padding-right:7.0pt;">TRAIN SET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.1.1.1.1" style="padding-left:7.0pt;padding-right:7.0pt;">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T7.1.1.1.1.m1.1"><semantics id="A3.T7.1.1.1.1.m1.1a"><mo id="A3.T7.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T7.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T7.1.1.1.1.m1.1b"><ci id="A3.T7.1.1.1.1.m1.1.1.cmml" xref="A3.T7.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.2.2.2.2" style="padding-left:7.0pt;padding-right:7.0pt;">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T7.2.2.2.2.m1.1"><semantics id="A3.T7.2.2.2.2.m1.1a"><mo id="A3.T7.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T7.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T7.2.2.2.2.m1.1b"><ci id="A3.T7.2.2.2.2.m1.1.1.cmml" xref="A3.T7.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.3.3.3.3" style="padding-left:7.0pt;padding-right:7.0pt;">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T7.3.3.3.3.m1.1"><semantics id="A3.T7.3.3.3.3.m1.1a"><mo id="A3.T7.3.3.3.3.m1.1.1" stretchy="false" xref="A3.T7.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T7.3.3.3.3.m1.1b"><ci id="A3.T7.3.3.3.3.m1.1.1.cmml" xref="A3.T7.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.4.4.4.4" style="padding-left:7.0pt;padding-right:7.0pt;">MMDist <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T7.4.4.4.4.m1.1"><semantics id="A3.T7.4.4.4.4.m1.1a"><mo id="A3.T7.4.4.4.4.m1.1.1" stretchy="false" xref="A3.T7.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T7.4.4.4.4.m1.1b"><ci id="A3.T7.4.4.4.4.m1.1.1.cmml" xref="A3.T7.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.4.4.4.4.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T7.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T7.4.4.5.1.1" style="padding-left:7.0pt;padding-right:7.0pt;">w/o static &amp; syn</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.4.5.1.2" style="padding-left:7.0pt;padding-right:7.0pt;">0.101</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.4.5.1.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.231</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.4.5.1.4" style="padding-left:7.0pt;padding-right:7.0pt;">261.325</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.4.4.5.1.5" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.4.4.5.1.5.1">5.201</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.4.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T7.4.4.6.2.1" style="padding-left:7.0pt;padding-right:7.0pt;">w/o static</th>
<td class="ltx_td ltx_align_center" id="A3.T7.4.4.6.2.2" style="padding-left:7.0pt;padding-right:7.0pt;">0.110</td>
<td class="ltx_td ltx_align_center" id="A3.T7.4.4.6.2.3" style="padding-left:7.0pt;padding-right:7.0pt;">0.248</td>
<td class="ltx_td ltx_align_center" id="A3.T7.4.4.6.2.4" style="padding-left:7.0pt;padding-right:7.0pt;">286.809</td>
<td class="ltx_td ltx_align_center" id="A3.T7.4.4.6.2.5" style="padding-left:7.0pt;padding-right:7.0pt;">5.213</td>
</tr>
<tr class="ltx_tr" id="A3.T7.4.4.7.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T7.4.4.7.3.1" style="padding-left:7.0pt;padding-right:7.0pt;">MotionBase</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.4.4.7.3.2" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.4.4.7.3.2.1">0.118</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.4.4.7.3.3" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.4.4.7.3.3.1">0.269</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.4.4.7.3.4" style="padding-left:7.0pt;padding-right:7.0pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.4.4.7.3.4.1">121.917</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.4.4.7.3.5" style="padding-left:7.0pt;padding-right:7.0pt;">7.644</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Ablation of Synthesis and Static Data?</h3>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">To assess the effectiveness of synthetic and static data, we conduct a series of ablation experiments.
We train GPT2-medium on three variations of MotionBase: without synthetic data, without image data, and without both synthetic data and image data.
The model is trained for 300 epochs with a learning rate of 2e-4.
Performance is tested on the Motion-X test set using the VQ-VAE and retrieval model trained on MotionBase, with results shown in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:syn_and_static</span>.
Our findings indicate that incorporating both static data (i.e., image data) and synthetic data leads to performance improvements in terms of R-Precision.
Additionally, we observe that the trend of MMDist is opposite to that of R-Precision.
This could be attributed to MMDist’s sensitivity to the quality of the embedding space.
When the motion and text encoders have limited capacity, this metric may struggle to discern the quality of generated motions.
This phenomenon highlights the importance of developing more robust evaluation metrics and models.</p>
</div>
<figure class="ltx_table" id="A3.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison of evaluations using different encoder models.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T8.6" style="width:368.5pt;height:171pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.7pt,4.5pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T8.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T8.6.6.7.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="3" id="A3.T8.6.6.7.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_tt" colspan="3" id="A3.T8.6.6.7.1.2">EM_Humanml3d</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_tt" colspan="3" id="A3.T8.6.6.7.1.3">EM_Motion-X</th>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.6.6.6.7">Decoder</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.6.6.6.8">#Inst.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T8.6.6.6.9">#Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.1.1.1.1">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T8.1.1.1.1.m1.1"><semantics id="A3.T8.1.1.1.1.m1.1a"><mo id="A3.T8.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T8.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T8.1.1.1.1.m1.1b"><ci id="A3.T8.1.1.1.1.m1.1.1.cmml" xref="A3.T8.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.2.2.2.2">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T8.2.2.2.2.m1.1"><semantics id="A3.T8.2.2.2.2.m1.1a"><mo id="A3.T8.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T8.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T8.2.2.2.2.m1.1b"><ci id="A3.T8.2.2.2.2.m1.1.1.cmml" xref="A3.T8.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T8.3.3.3.3">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T8.3.3.3.3.m1.1"><semantics id="A3.T8.3.3.3.3.m1.1a"><mo id="A3.T8.3.3.3.3.m1.1.1" stretchy="false" xref="A3.T8.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T8.3.3.3.3.m1.1b"><ci id="A3.T8.3.3.3.3.m1.1.1.cmml" xref="A3.T8.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T8.4.4.4.4">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T8.4.4.4.4.m1.1"><semantics id="A3.T8.4.4.4.4.m1.1a"><mo id="A3.T8.4.4.4.4.m1.1.1" stretchy="false" xref="A3.T8.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T8.4.4.4.4.m1.1b"><ci id="A3.T8.4.4.4.4.m1.1.1.cmml" xref="A3.T8.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.5.5.5.5">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T8.5.5.5.5.m1.1"><semantics id="A3.T8.5.5.5.5.m1.1a"><mo id="A3.T8.5.5.5.5.m1.1.1" stretchy="false" xref="A3.T8.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T8.5.5.5.5.m1.1b"><ci id="A3.T8.5.5.5.5.m1.1.1.cmml" xref="A3.T8.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.5.5.5.5.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T8.6.6.6.6">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T8.6.6.6.6.m1.1"><semantics id="A3.T8.6.6.6.6.m1.1a"><mo id="A3.T8.6.6.6.6.m1.1.1" stretchy="false" xref="A3.T8.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T8.6.6.6.6.m1.1b"><ci id="A3.T8.6.6.6.6.m1.1.1.cmml" xref="A3.T8.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T8.6.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T8.6.6.6.6.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T8.6.6.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.8.1.1">GPT-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.8.1.2">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.8.1.3">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.8.1.4">0.466</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.8.1.5">0.752</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.8.1.6"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.8.1.6.1">0.101</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.8.1.7">0.358</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.8.1.8">0.651</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.8.1.9"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.8.1.9.1">0.050</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.9.2.1">GPT-2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.9.2.2">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.9.2.3">700M</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.9.2.4">0.462</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.9.2.5">0.744</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.9.2.6">0.208</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.9.2.7">0.362</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.9.2.8">0.656</td>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.9.2.9">0.754</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.10.3.1">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.10.3.2">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.10.3.3">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.10.3.4">0.497</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.10.3.5">0.778</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.10.3.6">0.214</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.10.3.7">0.378</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.10.3.8">0.671</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.10.3.9">0.122</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.11.4.1">LLaMA-2</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.11.4.2">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.11.4.3">7B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.11.4.4">0.474</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.11.4.5">0.758</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.11.4.6">0.452</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.11.4.7">0.376</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.11.4.8">0.673</td>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.11.4.9">0.518</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.12.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.12.5.1">LLaMA-3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.12.5.2">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.12.5.3">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.12.5.4">0.500</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.12.5.5">0.783</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.12.5.6">0.173</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.12.5.7">0.380</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.12.5.8">0.675</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.12.5.9">0.094</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.13.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.13.6.1">LLaMA-3</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.13.6.2">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.13.6.3">8B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.13.6.4">0.499</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.13.6.5">0.786</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T8.6.6.13.6.6">0.264</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T8.6.6.13.6.7">0.393</th>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.13.6.8">0.696</td>
<td class="ltx_td ltx_align_center" id="A3.T8.6.6.13.6.9">0.591</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.14.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.14.7.1">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.14.7.2">0.02M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.14.7.3">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.14.7.4"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.14.7.4.1">0.519</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.14.7.5"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.14.7.5.1">0.803</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T8.6.6.14.7.6">0.166</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T8.6.6.14.7.7">0.395</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.14.7.8">0.695</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.6.6.14.7.9">0.105</td>
</tr>
<tr class="ltx_tr" id="A3.T8.6.6.15.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T8.6.6.15.8.1">LLaMA-2</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.6.6.15.8.2">0.08M</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T8.6.6.15.8.3">13B</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T8.6.6.15.8.4">0.504</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.6.6.15.8.5">0.790</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T8.6.6.15.8.6">0.393</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T8.6.6.15.8.7"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.15.8.7.1">0.400</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.6.6.15.8.8"><span class="ltx_text ltx_font_bold" id="A3.T8.6.6.15.8.8.1">0.700</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.6.6.15.8.9">0.637</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Ablation of Different Encoder Models</h3>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.T8" title="Table 8 ‣ C.1 Ablation of Synthesis and Static Data? ‣ Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">8</span></a> presents the evaluation results on the HumanML3D test set using different encoder models (EM).
We employ the same dual-encoder architecture <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#bib.bib17" title="">2022a</a>)</cite> but trained it on two distinct datasets: HumanML3D and Motion-X, where HumanML3D is a subset of Motion-X.
The results highlight the limited generalization ability of the encoder model.
When using the model trained on the larger Motion-X dataset, performance metrics on HumanML3D decrease.
This suggests that training on the broader Motion-X dataset negatively impacts R-Precision performance on the HumanML3D subset.
Furthermore, when the encoder model is trained on Motion-X, increasing the training data size for the text-to-motion model leads to significant performance gains.
Conversely, when using the encoder model trained on HumanML3D, the performance of the text-to-motion model degrades as the training data size increases.
This might be attributed to inherent limitations in the encoder model itself.</p>
</div>
<figure class="ltx_table" id="A3.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Comparison between fine-tuning and learning from scratch on the Motion-X test set.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T9.4" style="width:279.4pt;height:94pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.4pt,2.5pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T9.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T9.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T9.4.4.4.5">#Inst</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A3.T9.4.4.4.6">From Sctrach</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T9.1.1.1.1">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T9.1.1.1.1.m1.1"><semantics id="A3.T9.1.1.1.1.m1.1a"><mo id="A3.T9.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T9.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T9.1.1.1.1.m1.1b"><ci id="A3.T9.1.1.1.1.m1.1.1.cmml" xref="A3.T9.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T9.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T9.2.2.2.2">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T9.2.2.2.2.m1.1"><semantics id="A3.T9.2.2.2.2.m1.1a"><mo id="A3.T9.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T9.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T9.2.2.2.2.m1.1b"><ci id="A3.T9.2.2.2.2.m1.1.1.cmml" xref="A3.T9.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T9.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T9.3.3.3.3">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T9.3.3.3.3.m1.1"><semantics id="A3.T9.3.3.3.3.m1.1a"><mo id="A3.T9.3.3.3.3.m1.1.1" stretchy="false" xref="A3.T9.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T9.3.3.3.3.m1.1b"><ci id="A3.T9.3.3.3.3.m1.1.1.cmml" xref="A3.T9.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T9.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T9.4.4.4.4">MMDist <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T9.4.4.4.4.m1.1"><semantics id="A3.T9.4.4.4.4.m1.1a"><mo id="A3.T9.4.4.4.4.m1.1.1" stretchy="false" xref="A3.T9.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T9.4.4.4.4.m1.1b"><ci id="A3.T9.4.4.4.4.m1.1.1.cmml" xref="A3.T9.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T9.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T9.4.4.4.4.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T9.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.4.4.5.1.1">0.02M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T9.4.4.5.1.2">Yes</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.5.1.3">0.035</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.5.1.4">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.5.1.5">16.904</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.5.1.6">9.280</td>
</tr>
<tr class="ltx_tr" id="A3.T9.4.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T9.4.4.6.2.1">0.02M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T9.4.4.6.2.2">No</th>
<td class="ltx_td ltx_align_center" id="A3.T9.4.4.6.2.3">0.206</td>
<td class="ltx_td ltx_align_center" id="A3.T9.4.4.6.2.4">0.402</td>
<td class="ltx_td ltx_align_center" id="A3.T9.4.4.6.2.5">54.017</td>
<td class="ltx_td ltx_align_center" id="A3.T9.4.4.6.2.6">8.218</td>
</tr>
<tr class="ltx_tr" id="A3.T9.4.4.7.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T9.4.4.7.3.1">0.08M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T9.4.4.7.3.2">Yes</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.7.3.3">0.460</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.7.3.4">0.782</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.7.3.5">0.113</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.4.4.7.3.6">2.862</td>
</tr>
<tr class="ltx_tr" id="A3.T9.4.4.8.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T9.4.4.8.4.1">0.08M</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A3.T9.4.4.8.4.2">No</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.4.4.8.4.3">0.468</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.4.4.8.4.4">0.791</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.4.4.8.4.5">0.096</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.4.4.8.4.6">2.798</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Ablation of Learning from Scratch vs. Fine-tuning</h3>
<div class="ltx_para ltx_noindent" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">We compare the performance of fine-tuning GPT-2 against training it from scratch (random initialization).
The results show that fine-tuned models consistently outperform those trained from scratch, particularly when trained on HumanML3D and evaluated on MotionX.
The improvement of pretrained LLM highlights the importance of text pre-training in enhancing the model’s understanding of text descriptions and improving its generalization capabilities.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Ablation of Different Loss Calculation Strategies</h3>
<figure class="ltx_table" id="A3.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Results of different loss calculation methods on the HumanML3D test set.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T10.4" style="width:253.8pt;height:59.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.7pt,1.6pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T10.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T10.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T10.4.4.4.5">Loss Calculation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1">R@1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T10.1.1.1.1.m1.1"><semantics id="A3.T10.1.1.1.1.m1.1a"><mo id="A3.T10.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T10.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T10.1.1.1.1.m1.1b"><ci id="A3.T10.1.1.1.1.m1.1.1.cmml" xref="A3.T10.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.2.2.2.2">R@3 <math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T10.2.2.2.2.m1.1"><semantics id="A3.T10.2.2.2.2.m1.1a"><mo id="A3.T10.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T10.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T10.2.2.2.2.m1.1b"><ci id="A3.T10.2.2.2.2.m1.1.1.cmml" xref="A3.T10.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.3.3.3.3">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T10.3.3.3.3.m1.1"><semantics id="A3.T10.3.3.3.3.m1.1a"><mo id="A3.T10.3.3.3.3.m1.1.1" stretchy="false" xref="A3.T10.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T10.3.3.3.3.m1.1b"><ci id="A3.T10.3.3.3.3.m1.1.1.cmml" xref="A3.T10.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.3.3.3.3.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.4.4.4.4">MMDist <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T10.4.4.4.4.m1.1"><semantics id="A3.T10.4.4.4.4.m1.1a"><mo id="A3.T10.4.4.4.4.m1.1.1" stretchy="false" xref="A3.T10.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T10.4.4.4.4.m1.1b"><ci id="A3.T10.4.4.4.4.m1.1.1.cmml" xref="A3.T10.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.4.4.4.4.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T10.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T10.4.4.5.1.1">Motion Seq Loss</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.4.4.5.1.2">0.388</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.4.4.5.1.3">0.650</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.4.4.5.1.4">0.680</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.4.4.5.1.5">3.919</td>
</tr>
<tr class="ltx_tr" id="A3.T10.4.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T10.4.4.6.2.1">Whole Seq Loss</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.4.4.6.2.2">0.466</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.4.4.6.2.3">0.752</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.4.4.6.2.4">0.101</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.4.4.6.2.5">3.234</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="A3.SS4.p1">
<p class="ltx_p" id="A3.SS4.p1.1">We also investigate the impact of different loss calculation strategies on model performance:
We compare two strategies:
1) calculating the loss solely on the output motion tokens, and
2) calculating the loss on both the input text and the output motion tokens.
As shown in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:training_obj</span>, our results indicate that the second strategy yields better performance.
This improvement compared to the first alternative is likely due to the strategy’s ability to prevent catastrophic forgetting of text understanding.
Additionally, it helps mitigate overfitting to motion patterns in the training data, thereby enhancing the model’s generalization ability.</p>
</div>
<figure class="ltx_figure" id="A3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="A3.F12.g1" src="x12.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Comparison with different motion quantization on the Motion-X (<span class="ltx_text ltx_font_bold" id="A3.F12.5.1">left</span>) and MotionBase dataset (<span class="ltx_text ltx_font_bold" id="A3.F12.6.2">right</span>). The Y-axis denotes FID (<math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.F12.2.m1.1"><semantics id="A3.F12.2.m1.1b"><mo id="A3.F12.2.m1.1.1" stretchy="false" xref="A3.F12.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.F12.2.m1.1c"><ci id="A3.F12.2.m1.1.1.cmml" xref="A3.F12.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.F12.2.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.F12.2.m1.1e">↓</annotation></semantics></math>).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.5 </span>Ablation of Motion Quantization</h3>
<div class="ltx_para ltx_noindent" id="A3.SS5.p1">
<p class="ltx_p" id="A3.SS5.p1.1">First, we provide additional FID results on Motion-X in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.F12" title="Figure 12 ‣ C.4 Ablation of Different Loss Calculation Strategies ‣ Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">12</span></a>.
It is worth noting that while our motion quantizer performs worse than RQ-VAE on the smaller HumanML3D dataset, it surpasses both VQ and RQ when evaluated on the larger Motion-X and MotionBase benchmarks, as can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#S5.T6" title="Table 6 ‣ 5.4 Limitation of Automated Metric ‣ 5 Experiments ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">6</span></a>.
This suggests that our approach offers a greater advantage when applied to larger datasets, highlighting its improved generalization compared to previous methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS5.p2">
<p class="ltx_p" id="A3.SS5.p2.1">To further validate the effectiveness of our 2D quantization strategy, we compare the 2D-LFQ method with its 1D counterpart (which is identical to VQ except for the quantization strategy).
The results, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A3.T11" title="Table 11 ‣ C.5 Ablation of Motion Quantization ‣ Appendix C Additional Experimental Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">11</span></a>, demonstrate that 2D quantization in LFQ significantly outperforms the 1D version.
This highlights the superior ability of 2D quantization to enhance the representational capacity of the motion tokenizer.</p>
</div>
<figure class="ltx_table" id="A3.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Ablation of 2D motion quantization vs. its 1D version.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T11.2" style="width:352.6pt;height:61.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.1pt,5.4pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T11.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T11.2.2.3.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A3.T11.2.2.3.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="A3.T11.2.2.3.1.2">HumanML3D</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="A3.T11.2.2.3.1.3">Motion-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2" id="A3.T11.2.2.3.1.4">MotionBase</th>
</tr>
<tr class="ltx_tr" id="A3.T11.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.3">Tokenizer</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.4">#Num.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T11.2.2.2.5">#Param.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.1.1.1.1">FID <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T11.1.1.1.1.m1.1"><semantics id="A3.T11.1.1.1.1.m1.1a"><mo id="A3.T11.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T11.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T11.1.1.1.1.m1.1b"><ci id="A3.T11.1.1.1.1.m1.1.1.cmml" xref="A3.T11.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T11.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T11.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T11.2.2.2.2">MPJPE <math alttext="\downarrow" class="ltx_Math" display="inline" id="A3.T11.2.2.2.2.m1.1"><semantics id="A3.T11.2.2.2.2.m1.1a"><mo id="A3.T11.2.2.2.2.m1.1.1" stretchy="false" xref="A3.T11.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A3.T11.2.2.2.2.m1.1b"><ci id="A3.T11.2.2.2.2.m1.1.1.cmml" xref="A3.T11.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T11.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T11.2.2.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.6">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A3.T11.2.2.2.7">MPJPE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.8">FID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T11.2.2.2.9">MPJPE</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T11.2.2.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.1">1D-LFQ</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.2">16384</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T11.2.2.4.1.3">19.43M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.4">3.85</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T11.2.2.4.1.5">52.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.6">2.783</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T11.2.2.4.1.7">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.8">10.358</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T11.2.2.4.1.9">80.1</td>
</tr>
<tr class="ltx_tr" id="A3.T11.2.2.5.2">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.1">2D-LFQ</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.2">16384</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T11.2.2.5.2.3">108.35M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.4"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.4.1">1.769</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T11.2.2.5.2.5"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.5.1">45.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.6"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.6.1">0.295</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T11.2.2.5.2.7"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.7.1">54.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.8"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.8.1">7.853</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T11.2.2.5.2.9"><span class="ltx_text ltx_font_bold" id="A3.T11.2.2.5.2.9.1">64.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Additional Quantitative Results</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We provide some examples to visualize the human motions predicted by our large motion model trained on MotionBase, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03311v1#A4.F13" title="Figure 13 ‣ Appendix D Additional Quantitative Results ‣ Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models"><span class="ltx_text ltx_ref_tag">13</span></a>.
As can be seen, our large motion model is capable of generating motion sequences that align well with the input texts, demonstrating the effectiveness of the MotionBase dataset.</p>
</div>
<figure class="ltx_figure" id="A4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="580" id="A4.F13.g1" src="x13.png" width="761"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Quantitative examples of motions generated by our large motion model.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct  4 10:46:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
