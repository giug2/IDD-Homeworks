<article class="ltx_document ltx_authors_1line" lang="en">
 <h1 class="ltx_title ltx_title_document">
  Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Xiaoxue Cheng
    <sup class="ltx_sup" id="id1.1.id1">
     1
    </sup>
    ,
Junyi Li
    <sup class="ltx_sup" id="id2.2.id2">
     1,3
    </sup>
    ,
Wayne Xin Zhao
    <sup class="ltx_sup" id="id3.3.id3">
     1
    </sup>
    ,
Hongzhi Zhang
    <sup class="ltx_sup" id="id4.4.id4">
     4
    </sup>
    ,
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id5.5.id5">
     Fuzheng Zhang
     <sup class="ltx_sup" id="id5.5.id5.1">
      <span class="ltx_text ltx_font_medium" id="id5.5.id5.1.1">
       4
      </span>
     </sup>
     ,
Di Zhang
     <sup class="ltx_sup" id="id5.5.id5.2">
      <span class="ltx_text ltx_font_medium" id="id5.5.id5.2.1">
       4
      </span>
     </sup>
     ,
Kun Gai
     <sup class="ltx_sup" id="id5.5.id5.3">
      <span class="ltx_text ltx_font_medium" id="id5.5.id5.3.1">
       4
      </span>
     </sup>
    </span>
    and
    <span class="ltx_text ltx_font_bold" id="id6.6.id6">
     Ji-Rong Wen
     <sup class="ltx_sup" id="id6.6.id6.1">
      <span class="ltx_text ltx_font_medium" id="id6.6.id6.1.1">
       1,2
      </span>
     </sup>
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id7.7.id7">
     1
    </sup>
    Gaoling School of Artificial Intelligence, Renmin University of China
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id8.8.id8">
     2
    </sup>
    School of Information, Renmin University of China
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id9.9.id9">
     3
    </sup>
    DIRO, Université de Montréal
    <sup class="ltx_sup" id="id10.10.id10">
     4
    </sup>
    Kuaishou
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id11.11.id11">
     chengxiaoxue@ruc.edu.cn
    </span>
    <span class="ltx_text ltx_font_typewriter" id="id12.12.id12">
     lijunyi@ruc.edu.cn
    </span>
    <span class="ltx_text ltx_font_typewriter" id="id13.13.id13">
     batmanfly@gmail.com
    </span>
    <br class="ltx_break"/>
   </span>
   <span class="ltx_author_notes">
    Corresponding author
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id14.id1">
   <span class="ltx_text" id="id14.id1.1">
    Hallucination detection is a challenging task for large language models (LLMs), and existing studies heavily rely on powerful closed-source LLMs such as GPT-4. In this paper, we propose an autonomous LLM-based agent framework, called
    <span class="ltx_text ltx_font_bold" id="id14.id1.1.1">
     HaluAgent
    </span>
    , which enables relatively smaller LLMs
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       “smaller” in this work is relative to larger language models (
       <em class="ltx_emph ltx_font_italic" id="footnote1.1">
        e.g.,
       </em>
       over 100B).
      </span>
     </span>
    </span>
    (
    <em class="ltx_emph ltx_font_italic" id="id14.id1.1.2">
     e.g.,
    </em>
    Baichuan2-Chat 7B) to actively select suitable tools for detecting multiple hallucination types such as text, code, and mathematical expression. In HaluAgent, we integrate the LLM, multi-functional toolbox, and design a fine-grained three-stage detection framework along with memory mechanism.
To facilitate the effectiveness of HaluAgent, we leverage existing Chinese and English datasets to synthesize detection trajectories for fine-tuning, which endows HaluAgent with the capability for bilingual hallucination detection.
Extensive experiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent can perform hallucination detection on various types of tasks and datasets, achieving performance comparable to or even higher than GPT-4 without tool enhancements on both in-domain and out-of-domain datasets.
We release our dataset and code at
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/RUCAIBox/HaluAgent" target="_blank" title="">
     https://github.com/RUCAIBox/HaluAgent
    </a>
    .
   </span>
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Recently, large language models (LLMs)
    <cite class="ltx_cite ltx_citemacro_cite">
     Zhao et al. (
     <a class="ltx_ref" href="#bib.bib39" title="">
      2023
     </a>
     )
    </cite>
    have demonstrated exceptional capabilities across a variety of tasks within the field of natural language processing. However,
    <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">
     hallucination
    </em>
    <cite class="ltx_cite ltx_citemacro_cite">
     Ji et al. (
     <a class="ltx_ref" href="#bib.bib12" title="">
      2023
     </a>
     ); Rawte et al. (
     <a class="ltx_ref" href="#bib.bib26" title="">
      2023
     </a>
     ); Zhang et al. (
     <a class="ltx_ref" href="#bib.bib38" title="">
      2023
     </a>
     ); Huang et al. (
     <a class="ltx_ref" href="#bib.bib11" title="">
      2023
     </a>
     ); Ye et al. (
     <a class="ltx_ref" href="#bib.bib35" title="">
      2023
     </a>
     )
    </cite>
    in text generated by LLMs remains an underlying concern, impeding the application of LLMs in real-world scenarios
    <cite class="ltx_cite ltx_citemacro_cite">
     Kaddour et al. (
     <a class="ltx_ref" href="#bib.bib15" title="">
      2023
     </a>
     )
    </cite>
    .
    <cite class="ltx_cite ltx_citemacro_citet">
     Xu et al. (
     <a class="ltx_ref" href="#bib.bib31" title="">
      2024
     </a>
     )
    </cite>
    have indicated that, despite the existence of some effective hallucination mitigation strategies, the occurrence of hallucinations in LLMs is inevitable. Therefore, reliable and effective hallucination detection methods are necessary and urgent.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Existing hallucination detection methods can be roughly categorized into two primary approaches.
One line of work relies on the internal knowledge of LLMs to directly identify hallucinations via prompts
    <cite class="ltx_cite ltx_citemacro_cite">
     Li et al. (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     ); Lei et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     )
    </cite>
    or evaluating the semantic consistency among multiple responses to the same question generated by LLMs
    <cite class="ltx_cite ltx_citemacro_cite">
     Manakul et al. (
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     )
    </cite>
    .
However, these methods are usually constrained not only by LLMs’ internal knowledge but also by their abilities to utilize knowledge.
Another line of work extends the detection ability of LLMs by employing external tools (
    <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">
     e.g.,
    </em>
    search engine) to obtain supporting evidence for hallucination detection
    <cite class="ltx_cite ltx_citemacro_cite">
     Chern et al. (
     <a class="ltx_ref" href="#bib.bib5" title="">
      2023
     </a>
     ); Wei et al. (
     <a class="ltx_ref" href="#bib.bib30" title="">
      2024
     </a>
     ); Min et al. (
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023
     </a>
     )
    </cite>
    . However, these approaches mostly depend on closed-source powerful LLMs such as GPT-4. Moreover, their detection process is usually pre-determined by human, making it difficult for the model to autonomously and effectively execute the hallucination detection.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    To address these issues, in this paper, we propose the
    <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">
     HaluAgent
    </span>
    , an autonomous LLM-based agent framework for hallucination detection. This agent is based on smaller open-source models (
    <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">
     i.e.,
    </em>
    Baichuan2-Chat 7B and 13B
    <cite class="ltx_cite ltx_citemacro_cite">
     Yang et al. (
     <a class="ltx_ref" href="#bib.bib32" title="">
      2023a
     </a>
     )
    </cite>
    ) and capable of
    <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">
     bilingual
    </em>
    hallucination detection in Chinese and English. The motivations are twofold: (1) designing autonomous detection agents that can actively make decisions and judgements, without human
assistance; (2) enabling relatively smaller models to effectively perform complex detection, without reliance on close-sourced LLM APIs. To achieve this, we make three major technical contributions. First, we extend the LLM’s capability to detect a broader range of hallucination forms such as text, code, math expression, or their combination by curating a multi-functional toolbox, in contrast to previous work only focused on textual hallucination or limited tools
    <cite class="ltx_cite ltx_citemacro_cite">
     Manakul et al. (
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     ); Min et al. (
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023
     </a>
     )
    </cite>
    . Second, we design a fine-grained three-stage detection framework along with memory mechanism, including sentence segmentation, tool selection and verification, and reflection. Third, we leverage existing hallucination datasets to synthesize detection trajectories for fine-tuning the LLM, where we first employ GPT-4 to execute the above three stages until obtaining detection results consistent with the ground-truth label and then synthesize the instruction data. We compare HaluAgent and previous work in Table
    <a class="ltx_ref" href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    To verify the effectiveness, We evaluate HaluAgent on both in-domain and out-of-domain datasets at response- and sentence-level granularities. After fine-tuning with only 2K trajectories, the detection performance of HaluAgent has been improved significantly (
    <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">
     e.g.,
    </em>
    overall accuracy increases from 46.44% to 79.70% in four in-domain datasets and from 49.50% to 78.43% in two out-of-domain datasets), reaching a level comparable to or even higher than GPT-4 without tool enhancement. In the sentence-level detection experiments, HaluAgent also achieved substantial improvements, particularly with F1 scores on the math and science datasets increasing from 19.51% to 68.80% and from 17.54% to 94.16%, respectively.
   </p>
  </div>
  <figure class="ltx_table" id="S1.T1">
   <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T1.1" style="width:212.5pt;height:71.9pt;vertical-align:-0.0pt;">
    <span class="ltx_transformed_inner" style="transform:translate(-61.4pt,20.8pt) scale(0.633783343706868,0.633783343706868) ;">
     <table class="ltx_tabular ltx_align_middle" id="S1.T1.1.1">
      <tr class="ltx_tr" id="S1.T1.1.1.1">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S1.T1.1.1.1.1" style="padding:0.75pt 2.0pt;">
        <span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.1.1">
         Methods
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.1.2" style="padding:0.75pt 2.0pt;">
        <span class="ltx_text" id="S1.T1.1.1.1.2.1">
        </span>
        <span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.2.2">
         <span class="ltx_text" id="S1.T1.1.1.1.2.2.1">
          <span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.1.2.2.1.1">
           <span class="ltx_tr" id="S1.T1.1.1.1.2.2.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.2.2.1.1.1.1" style="padding:0.75pt 2.0pt;">
             Base
            </span>
           </span>
           <span class="ltx_tr" id="S1.T1.1.1.1.2.2.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.2.2.1.1.2.1" style="padding:0.75pt 2.0pt;">
             Model
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_text" id="S1.T1.1.1.1.2.2.2">
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.1.3" style="padding:0.75pt 2.0pt;">
        <span class="ltx_text" id="S1.T1.1.1.1.3.1">
        </span>
        <span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.3.2">
         <span class="ltx_text" id="S1.T1.1.1.1.3.2.1">
          <span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.1.3.2.1.1">
           <span class="ltx_tr" id="S1.T1.1.1.1.3.2.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.3.2.1.1.1.1" style="padding:0.75pt 2.0pt;">
             Task
            </span>
           </span>
           <span class="ltx_tr" id="S1.T1.1.1.1.3.2.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.3.2.1.1.2.1" style="padding:0.75pt 2.0pt;">
             Agnostic
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_text" id="S1.T1.1.1.1.3.2.2">
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.1.4" style="padding:0.75pt 2.0pt;">
        <span class="ltx_text" id="S1.T1.1.1.1.4.1">
        </span>
        <span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.4.2">
         <span class="ltx_text" id="S1.T1.1.1.1.4.2.1">
          <span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.1.4.2.1.1">
           <span class="ltx_tr" id="S1.T1.1.1.1.4.2.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.4.2.1.1.1.1" style="padding:0.75pt 2.0pt;">
             Tool
            </span>
           </span>
           <span class="ltx_tr" id="S1.T1.1.1.1.4.2.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.4.2.1.1.2.1" style="padding:0.75pt 2.0pt;">
             Usage
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_text" id="S1.T1.1.1.1.4.2.2">
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.1.5" style="padding:0.75pt 2.0pt;">
        <span class="ltx_text" id="S1.T1.1.1.1.5.1">
        </span>
        <span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.5.2">
         <span class="ltx_text" id="S1.T1.1.1.1.5.2.1">
          <span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.1.5.2.1.1">
           <span class="ltx_tr" id="S1.T1.1.1.1.5.2.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.5.2.1.1.1.1" style="padding:0.75pt 2.0pt;">
             Fine
            </span>
           </span>
           <span class="ltx_tr" id="S1.T1.1.1.1.5.2.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.5.2.1.1.2.1" style="padding:0.75pt 2.0pt;">
             Grained
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_text" id="S1.T1.1.1.1.5.2.2">
         </span>
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.1.1.1.6" style="padding:0.75pt 2.0pt;">
        <span class="ltx_text" id="S1.T1.1.1.1.6.1">
        </span>
        <span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.6.2">
         <span class="ltx_text" id="S1.T1.1.1.1.6.2.1">
          <span class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.1.6.2.1.1">
           <span class="ltx_tr" id="S1.T1.1.1.1.6.2.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.6.2.1.1.1.1" style="padding:0.75pt 2.0pt;">
             Extensi-
            </span>
           </span>
           <span class="ltx_tr" id="S1.T1.1.1.1.6.2.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S1.T1.1.1.1.6.2.1.1.2.1" style="padding:0.75pt 2.0pt;">
             bility
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_text" id="S1.T1.1.1.1.6.2.2">
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S1.T1.1.1.2">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S1.T1.1.1.2.1" style="padding:0.75pt 2.0pt;">
        SelfCheckGPT
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.2.2" style="padding:0.75pt 2.0pt;">
        ChatGPT
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.2.3" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.2.3.1">
         \usym
        </span>
        2717
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.2.4" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.2.4.1">
         \usym
        </span>
        2717
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.2.5" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.2.5.1">
         \usym
        </span>
        2717
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.2.6" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.2.6.1">
         \usym
        </span>
        2717
       </td>
      </tr>
      <tr class="ltx_tr" id="S1.T1.1.1.3">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.1.1.3.1" style="padding:0.75pt 2.0pt;">
        SAFE
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.3.2" style="padding:0.75pt 2.0pt;">
        GPT-4
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.3.3" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.3.3.1">
         \usym
        </span>
        2717
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.3.4" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.3.4.1">
         \usym
        </span>
        2713
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.3.5" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.3.5.1">
         \usym
        </span>
        2713
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.3.6" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.3.6.1">
         \usym
        </span>
        2717
       </td>
      </tr>
      <tr class="ltx_tr" id="S1.T1.1.1.4">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S1.T1.1.1.4.1" style="padding:0.75pt 2.0pt;">
        FacTool
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.2" style="padding:0.75pt 2.0pt;">
        GPT-4
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.3" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.4.3.1">
         \usym
        </span>
        2717
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.4" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.4.4.1">
         \usym
        </span>
        2713
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.5" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.4.5.1">
         \usym
        </span>
        2713
       </td>
       <td class="ltx_td ltx_align_center" id="S1.T1.1.1.4.6" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.4.6.1">
         \usym
        </span>
        2717
       </td>
      </tr>
      <tr class="ltx_tr" id="S1.T1.1.1.5">
       <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S1.T1.1.1.5.1" style="padding:0.75pt 2.0pt;">
        HaluAgent
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.1.5.2" style="padding:0.75pt 2.0pt;">
        Baichuan2-Chat
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.1.5.3" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.5.3.1">
         \usym
        </span>
        2713
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.1.5.4" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.5.4.1">
         \usym
        </span>
        2713
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.1.5.5" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.5.5.1">
         \usym
        </span>
        2713
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.1.1.5.6" style="padding:0.75pt 2.0pt;">
        <span class="ltx_ERROR undefined" id="S1.T1.1.1.5.6.1">
         \usym
        </span>
        2713
       </td>
      </tr>
     </table>
    </span>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 1:
    </span>
    Comparison of different methods.
    <span class="ltx_text ltx_font_bold" id="S1.T1.5.1">
     Task Agnostic
    </span>
    means whether the method is designed to specific tasks;
    <span class="ltx_text ltx_font_bold" id="S1.T1.6.2">
     Fine Grained
    </span>
    describes whether providing detailed hallucination sentences;
    <span class="ltx_text ltx_font_bold" id="S1.T1.7.3">
     Extensibility
    </span>
    means whether the method can extend to more tasks and tools.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Approach
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    In this section, we introduce
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">
     HaluAgent
    </span>
    , our proposed autonomous agent for detecting hallucinations across various text types. The core of our HaluAgent framework is a well-instructed LLM, which can autonomously leverage tools to detect a broader range of hallucination types. First, we define several tasks for hallucination detection and then design a toolbox with supporting tools to extend the LLM’s capability. To enable step-by-step detection, we design a three-stage detection framework equipped with memory mechanism. Finally, we synthesize high-quality detection trajectory data to fine-tune open-source LLMs. We present the overall architecture of HaluAgent in Figure
    <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2 Approach ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    .
   </p>
  </div>
  <figure class="ltx_figure" id="S2.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="176" id="S2.F1.g1" src="/html/2406.11277/assets/x1.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    The overview of our proposed HaluAgent. The left part shows the process of fine-tuning open-source models and detecting hallucinations. The right part illustrates the hallucination detection pipeline of HaluAgent.
   </figcaption>
  </figure>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Task Definition
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     Hallucination refers to seemingly plausible yet factually unsupported content
     <cite class="ltx_cite ltx_citemacro_cite">
      Huang et al. (
      <a class="ltx_ref" href="#bib.bib11" title="">
       2023
      </a>
      )
     </cite>
     . Unlike previous work that mostly focused on detecting text-based hallucinations
     <cite class="ltx_cite ltx_citemacro_cite">
      Manakul et al. (
      <a class="ltx_ref" href="#bib.bib22" title="">
       2023
      </a>
      ); Yehuda et al. (
      <a class="ltx_ref" href="#bib.bib36" title="">
       2024
      </a>
      )
     </cite>
     , we consider a broader range of hallucination forms such as text, code, and mathematical expression.
Hence, in this work, we conduct hallucination detection in five tasks as:
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">
      Knowledge-based QA
     </span>
     involves generating answers to the input question
     <cite class="ltx_cite ltx_citemacro_cite">
      Lan et al. (
      <a class="ltx_ref" href="#bib.bib16" title="">
       2021
      </a>
      )
     </cite>
     , and we aim to identify misinformation from the answer text such as incorrect historical dates, misattributed quotes, or false scientific facts.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p3">
    <p class="ltx_p" id="S2.SS1.p3.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">
      Conditional Text Generation
     </span>
     focuses on tasks with specific requirements given in the input instructions
     <cite class="ltx_cite ltx_citemacro_cite">
      Dathathri et al. (
      <a class="ltx_ref" href="#bib.bib7" title="">
       2020
      </a>
      )
     </cite>
     such as generating text with particular length, format, or translating
a paragraph into special language. We aim to detect hallucinations that deviate from the given instructions or include irrelevant information.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p4">
    <p class="ltx_p" id="S2.SS1.p4.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p4.1.1">
      Semantic Consistency
     </span>
     is not specific to any particular type of text or task. We define hallucination in semantic consistency as those irrelevant or self-contradictory content in the responses.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p5">
    <p class="ltx_p" id="S2.SS1.p5.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p5.1.1">
      Math Problem Solving
     </span>
     is related to generating a series of mathematical expressions to solve the math problem
     <cite class="ltx_cite ltx_citemacro_cite">
      Hendrycks et al. (
      <a class="ltx_ref" href="#bib.bib10" title="">
       2021
      </a>
      )
     </cite>
     , while the expressions may contain computational errors,
     <em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.2">
      e.g.,
     </em>
     arithmetic or calculation mistakes.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p6">
    <p class="ltx_p" id="S2.SS1.p6.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p6.1.1">
      Code Generation
     </span>
     aims to generate code snippets for the input query
     <cite class="ltx_cite ltx_citemacro_cite">
      Chen et al. (
      <a class="ltx_ref" href="#bib.bib2" title="">
       2021
      </a>
      )
     </cite>
     . We define hallucinations as code snippets that are syntactically incorrect, fail to execute as intended, or contain logical flaws and missing dependencies.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p7">
    <p class="ltx_p" id="S2.SS1.p7.1">
     Note that in real-world scenarios the responses from LLMs may contain a mixture of these hallucination types, and we aim to develop a general and versatile hallucination detection agent that can deal with a broader range of hallucinations.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Toolbox for Hallucination Detection
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     Since detecting texts with a mix of hallucination types is challenging, we design a comprehensive toolbox to enhance the hallucination detection capability of LLMs following previous work
     <cite class="ltx_cite ltx_citemacro_cite">
      Chern et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      ); Gou et al. (
      <a class="ltx_ref" href="#bib.bib9" title="">
       2024
      </a>
      )
     </cite>
     . Based on the hallucination types discussed in Section
     <a class="ltx_ref" href="#S2.SS1" title="2.1 Task Definition ‣ 2 Approach ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       2.1
      </span>
     </a>
     , we incorporate five types of external tools,
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">
      i.e.,
     </em>
     search engine, calculator, code interpreter, condition verifier, and semantic checker, and two internal system tools:
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">
      Search Engine
     </span>
     is utilized to retrieve supporting evidence from the web for identifying factually incorrect content, defined as
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.2">
      web_search
     </em>
     .
We use Google Programmable Search Engine API
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        https://developers.google.com/custom-search
       </span>
      </span>
     </span>
     to implement this tool.
Considering the relevance between retrieved documents and the output text, we only use the top-5 documents as the most relevant retrieval results for hallucination detection.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p3">
    <p class="ltx_p" id="S2.SS2.p3.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">
      Calculator
     </span>
     is used to verify inaccurate mathematical calculations in the model responses, targeting math-related hallucinations, defined as
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.2">
      calculator
     </em>
     .
We implement calculator via the scientific computing library SymPy
     <cite class="ltx_cite ltx_citemacro_cite">
      Meurer et al. (
      <a class="ltx_ref" href="#bib.bib23" title="">
       2017
      </a>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p4">
    <p class="ltx_p" id="S2.SS2.p4.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">
      Code Interpreter
     </span>
     can be used to validate code snippets by executing the snippets in a programming environment, defined as
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.2">
      code_interpreter
     </em>
     , ensuring that the code is syntactically correct and functions as intended.
We implement the code interpreter tool following CRITIC
     <cite class="ltx_cite ltx_citemacro_cite">
      Gou et al. (
      <a class="ltx_ref" href="#bib.bib9" title="">
       2024
      </a>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p5">
    <p class="ltx_p" id="S2.SS2.p5.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p5.1.1">
      Condition Verifier
     </span>
     aims to detect hallucinations for conditional text generation by assessing whether the text is consistent with the given condition. For example, we utilize word counter (
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p5.1.2">
      word_counter
     </em>
     ) to compute the number of words in a length-constrained generation scenario.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p6">
    <p class="ltx_p" id="S2.SS2.p6.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p6.1.1">
      Semantic Checker
     </span>
     mainly addresses hallucination types such as irrelevant responses and self-contradictions, which is defined as
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p6.1.2">
      match
     </em>
     .
We leverage GPT-4 to examine the consistency and relevance of semantics, primarily handling the semantic matching scenario.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p7">
    <p class="ltx_p" id="S2.SS2.p7.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p7.1.1">
      System Tools
     </span>
     are developed to support the basic manipulation operations for hallucination detection, including sentence segmentation (
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p7.1.2">
      split_text
     </em>
     ) and returning detection results (
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p7.1.3">
      get_answer
     </em>
     ).
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p8">
    <p class="ltx_p" id="S2.SS2.p8.1">
     All tools are defined as functions in a unified way and we present the whole toolbox in Table
     <a class="ltx_ref" href="#A1.T5" title="Table 5 ‣ A.2 Toolbox ‣ Appendix A HaluAgent Framework ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     in Appendix
     <a class="ltx_ref" href="#A1.SS2" title="A.2 Toolbox ‣ Appendix A HaluAgent Framework ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       A.2
      </span>
     </a>
     . It is worth noting that the toolbox is not limited to the existing tools and can be easily extended,
     <em class="ltx_emph ltx_font_italic" id="S2.SS2.p8.1.1">
      e.g.,
     </em>
     adding more task-specific tools.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    HaluAgent Framework
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     Inspired by prior work on complex reasoning
     <cite class="ltx_cite ltx_citemacro_cite">
      Jiang et al. (
      <a class="ltx_ref" href="#bib.bib14" title="">
       2024
      </a>
      )
     </cite>
     , we consider the hallucination detection process as an agent task.
Specifically, HaluAgent includes three stages: sentence segmentation, tool selection, and reflection, along with memory mechanism. Below, we outline the workflow and components of our HaluAgent framework.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS3.p2">
    <p class="ltx_p" id="S2.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">
      Sentence Segmentation.
     </span>
     Since the responses of LLMs are usually the combination of facts, opinions, and various types of texts, we first segment the responses into several independent detection units. To be specific, we utilize the system tool,
     <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.2">
      split_text
     </span>
     , to perform sentence segmentation. This tool requires the agent to split the input text into a set of sentences and complete any incomplete semantic information within the sentences,
     <em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.3">
      e.g.,
     </em>
     pronouns and omitted content.
Sentence segmentation reduces the complexity of hallucination detection tasks and allows HaluAgent to tailor its detection strategies for individual sentences, ensuring more accurate and fine-grained results by minimizing interference from unrelated content.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS3.p3">
    <p class="ltx_p" id="S2.SS3.p3.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">
      Tool Selection and Verification.
     </span>
     Next, HaluAgent verifies each sentence separately by selecting the appropriate tool from the toolbox based on the type of sentence content (
     <em class="ltx_emph ltx_font_italic" id="S2.SS3.p3.1.2">
      e.g.,
     </em>
     <span class="ltx_text ltx_font_italic" id="S2.SS3.p3.1.3">
      web_search
     </span>
     for fact statements and
     <span class="ltx_text ltx_font_italic" id="S2.SS3.p3.1.4">
      calculator
     </span>
     for mathematical expressions). HaluAgent compares each sentence with the execution results of the selected tool to identify any inconsistencies or inaccuracies, thereby detecting hallucinations.
To prevent the agent from forgetting intermediate detection results and support the subsequent reflection, HaluAgent stores the detection result of each sentence as a triple,
     <em class="ltx_emph ltx_font_italic" id="S2.SS3.p3.1.5">
      i.e.,
     </em>
     <em class="ltx_emph ltx_font_italic" id="S2.SS3.p3.1.6">
      (sentence, hallucination label, supporting evidence)
     </em>
     . If the sentence is identified as containing hallucinations, it will be labeled “1”, otherwise “0”. These useful information will be stored based on a memory mechanism,
allowing HaluAgent to refer to historical results and maintain a consistent understanding of the text veracity throughout the detection process.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS3.p4">
    <p class="ltx_p" id="S2.SS3.p4.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">
      Reflection.
     </span>
     After individually examining all the sentences, HaluAgent can obtain preliminary detection results. However, due to the limited capacity of each tool and potential errors in their outputs, these detection results might not be fully accurate.
To address this, HaluAgent performs the final reflection to double-check whether the previous detection results are correct from
     <em class="ltx_emph ltx_font_italic" id="S2.SS3.p4.1.2">
      local
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S2.SS3.p4.1.3">
      global
     </em>
     perspectives. At the local level, HaluAgent will match each sentence with the corresponding evidence to ensure the local correctness of hallucination detection. However, different sentences may influence each other. Therefore, at the global level, HaluAgent will determine whether the current sentence is incorrect based on the context of other sentences.
For instance, if the calculation result in a preceding sentence is incorrect, any subsequent steps based on this result should be considered incorrect, even if these steps are correct when checked in isolation.
Any detection mistakes will be corrected and the detection results (
     <em class="ltx_emph ltx_font_italic" id="S2.SS3.p4.1.4">
      i.e.,
     </em>
     hallucination label and supporting evidence) stored in memory are updated accordingly.
After reflection, HaluAgent invokes a system tool,
     <span class="ltx_text ltx_font_italic" id="S2.SS3.p4.1.5">
      get_answer
     </span>
     , to output the final detection result. If any hallucinations are detected, HaluAgent outputs the specific sentences and supporting evidence from the detection tools.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS3.p5">
    <p class="ltx_p" id="S2.SS3.p5.1">
     Throughout the above process, HaluAgent first segments the input text into a set of semantically complete sentences, then selects tools to check each sentence individually, and finally reflects on the detection results to further correct mistakes. To support this process, we use memory mechanism to store useful information such as historical detection trajectories and current detection results.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.4
    </span>
    Bilingual Agent Tuning
   </h3>
   <div class="ltx_para" id="S2.SS4.p1">
    <p class="ltx_p" id="S2.SS4.p1.1">
     Previous studies mostly depended on closed-source LLMs (
     <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.1">
      e.g.,
     </em>
     GPT-4) to detect hallucinations
     <cite class="ltx_cite ltx_citemacro_cite">
      Chern et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      ); Wei et al. (
      <a class="ltx_ref" href="#bib.bib30" title="">
       2024
      </a>
      )
     </cite>
     . To empower smaller language models as effective hallucination detectors, we aim to perform supervised fine-tuning on smaller LLMs (
     <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.2">
      e.g.,
     </em>
     Baichuan2-Chat 7B). Given the powerful agent capability, we leverage GPT-4 to synthesize high-quality hallucination detection trajectory data in Chinese and English following the detection framework in Section
     <a class="ltx_ref" href="#S2.SS3" title="2.3 HaluAgent Framework ‣ 2 Approach ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       2.3
      </span>
     </a>
     .
    </p>
   </div>
   <section class="ltx_subsubsection" id="S2.SS4.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.4.1
     </span>
     Trajectory Generation
    </h4>
    <figure class="ltx_table" id="S2.T2">
     <table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.1">
      <tr class="ltx_tr" id="S2.T2.1.1">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S2.T2.1.1.1">
        Datasets
       </td>
       <td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.1.1.2">
        #Train
       </td>
       <td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.1.1.3">
        #Trajectory
       </td>
       <td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.1.1.4">
        #Test
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T2.1.2">
       <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.1.2.1">
        WebQA
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.2.2">
        900
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.2.3">
        675
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.2.4">
        100
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T2.1.3">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.3.1">
        Ape210K
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.3.2">
        500
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.3.3">
        334
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.3.4">
        100
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T2.1.4">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.4.1">
        HumanEval
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.4.2">
        100
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.4.3">
        100
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.4.4">
        63
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T2.1.5">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.5.1">
        WordCnt
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.5.2">
        100
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.5.3">
        100
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.5.4">
        100
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T2.1.6">
       <td class="ltx_td ltx_align_left ltx_border_r" id="S2.T2.1.6.1">
        HaluEval-QA
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.6.2">
        900
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.6.3">
        808
       </td>
       <td class="ltx_td ltx_align_center" id="S2.T2.1.6.4">
        100
       </td>
      </tr>
      <tr class="ltx_tr" id="S2.T2.1.7">
       <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S2.T2.1.7.1">
        All
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.1.7.2">
        2500
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.1.7.3">
        2017
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T2.1.7.4">
        463
       </td>
      </tr>
     </table>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_table">
       Table 2:
      </span>
      Statistics of synthetic detection trajectories.
     </figcaption>
    </figure>
    <div class="ltx_para ltx_noindent" id="S2.SS4.SSS1.p1">
     <p class="ltx_p" id="S2.SS4.SSS1.p1.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS1.p1.1.1">
       Data Source.
      </span>
      To curate high-quality trajectory data covering diverse hallucination types, we select and construct five datasets,
      <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS1.p1.1.2">
       i.e.,
      </em>
      HaluEval
      <cite class="ltx_cite ltx_citemacro_cite">
       Li et al. (
       <a class="ltx_ref" href="#bib.bib19" title="">
        2023
       </a>
       )
      </cite>
      and WebQA
      <cite class="ltx_cite ltx_citemacro_cite">
       Li et al. (
       <a class="ltx_ref" href="#bib.bib20" title="">
        2016
       </a>
       )
      </cite>
      for knowledge-based QA, Ape210K
      <cite class="ltx_cite ltx_citemacro_cite">
       Zhao et al. (
       <a class="ltx_ref" href="#bib.bib40" title="">
        2020
       </a>
       )
      </cite>
      for math word problem, HumanEval
      <cite class="ltx_cite ltx_citemacro_cite">
       Chen et al. (
       <a class="ltx_ref" href="#bib.bib2" title="">
        2021
       </a>
       )
      </cite>
      for code generation, and WordCnt for conditional text generation. Among them, HaluEval and HumanEval are English datasets, and WebQA, Ape210K, and WordCnt are Chinese datasets, which enable bilingual detection capabilities of HaluAgent. In our experiments, we use GPT-4 to synthesize WordCnt which is targeted at generating text with a specified length. Besides, we obtain the ground-truth hallucination labels for WebQA and Ape210K by ChatGPT and human annotators.
We provide details of each dataset in Appendix
      <a class="ltx_ref" href="#A1.SS3" title="A.3 Data Source ‣ Appendix A HaluAgent Framework ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
       <span class="ltx_text ltx_ref_tag">
        A.3
       </span>
      </a>
      .
     </p>
    </div>
    <div class="ltx_para ltx_noindent" id="S2.SS4.SSS1.p2">
     <p class="ltx_p" id="S2.SS4.SSS1.p2.1">
      <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS1.p2.1.1">
       Trajectory Format.
      </span>
      Based on our datasets, we employ GPT-4 to execute the detection process in Section
      <a class="ltx_ref" href="#S2.SS3" title="2.3 HaluAgent Framework ‣ 2 Approach ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
       <span class="ltx_text ltx_ref_tag">
        2.3
       </span>
      </a>
      and generate corresponding trajectory data following the ReAct format
      <cite class="ltx_cite ltx_citemacro_cite">
       Yao et al. (
       <a class="ltx_ref" href="#bib.bib34" title="">
        2023
       </a>
       )
      </cite>
      . We begin by feeding the detection instruction and the text to be detected as input for GPT-4. At each turn, the agent receives an
      <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS1.p2.1.2">
       observation
      </em>
      , makes its plans and thoughts as
      <span class="ltx_text ltx_font_italic" id="S2.SS4.SSS1.p2.1.3">
       thought
      </span>
      , and invokes corresponding tools through
      <span class="ltx_text ltx_font_italic" id="S2.SS4.SSS1.p2.1.4">
       action
      </span>
      . The results from these tools are formulated as a new observation for the next turn. By iterating the above process, we can obtain a complete detection trajectory comprising the input instruction, detected text, intermediate steps (
      <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS1.p2.1.5">
       i.e.,
      </em>
      observations, thoughts, actions), and the final detection result. To ensure the accuracy of the trajectories, we remove those samples that include wrong tool invocation, formatting errors, and inconsistency between the detection result and the ground-truth hallucination label. Finally, we produce 2,017 high-quality trajectories for supervised fine-tuning. We present an example in Figure
      <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.3 Sentence-level Detection ‣ 3 Experiments ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
       <span class="ltx_text ltx_ref_tag">
        3
       </span>
      </a>
      and the statistics of trajectory data in Table
      <a class="ltx_ref" href="#S2.T2" title="Table 2 ‣ 2.4.1 Trajectory Generation ‣ 2.4 Bilingual Agent Tuning ‣ 2 Approach ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      .
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S2.SS4.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      2.4.2
     </span>
     Trajectory Tuning
    </h4>
    <div class="ltx_para" id="S2.SS4.SSS2.p1">
     <p class="ltx_p" id="S2.SS4.SSS2.p1.13">
      Based on the above formatted bilingual trajectory data, we perform supervised fine-tuning on Baichuan2-Chat 7B and 13B
      <cite class="ltx_cite ltx_citemacro_cite">
       Yang et al. (
       <a class="ltx_ref" href="#bib.bib32" title="">
        2023a
       </a>
       )
      </cite>
      , which are much smaller than the backbone models in previous studies
      <cite class="ltx_cite ltx_citemacro_cite">
       Chern et al. (
       <a class="ltx_ref" href="#bib.bib5" title="">
        2023
       </a>
       ); Wei et al. (
       <a class="ltx_ref" href="#bib.bib30" title="">
        2024
       </a>
       )
      </cite>
      .
Formally, the hallucination detection trajectory for each sample can be represented as
      <math alttext="\langle o_{0},t_{1},a_{1},o_{1},t_{2},a_{2},\ldots,o_{n-1},t_{n},a_{n},o_{n}\rangle" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.1.m1.11">
       <semantics id="S2.SS4.SSS2.p1.1.m1.11a">
        <mrow id="S2.SS4.SSS2.p1.1.m1.11.11.10" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.11" stretchy="false" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ⟨
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.2.2.1.1" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.2" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.2.cmml">
           o
          </mi>
          <mn id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.3" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.3.cmml">
           0
          </mn>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.12" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.3.3.2.2" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.cmml">
           t
          </mi>
          <mn id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.3" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.3.cmml">
           1
          </mn>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.13" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.4.4.3.3" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.2" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.2.cmml">
           a
          </mi>
          <mn id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.cmml">
           1
          </mn>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.14" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.5.5.4.4" xref="S2.SS4.SSS2.p1.1.m1.5.5.4.4.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.5.5.4.4.2" xref="S2.SS4.SSS2.p1.1.m1.5.5.4.4.2.cmml">
           o
          </mi>
          <mn id="S2.SS4.SSS2.p1.1.m1.5.5.4.4.3" xref="S2.SS4.SSS2.p1.1.m1.5.5.4.4.3.cmml">
           1
          </mn>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.15" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.6.6.5.5" xref="S2.SS4.SSS2.p1.1.m1.6.6.5.5.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.6.6.5.5.2" xref="S2.SS4.SSS2.p1.1.m1.6.6.5.5.2.cmml">
           t
          </mi>
          <mn id="S2.SS4.SSS2.p1.1.m1.6.6.5.5.3" xref="S2.SS4.SSS2.p1.1.m1.6.6.5.5.3.cmml">
           2
          </mn>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.16" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.7.7.6.6" xref="S2.SS4.SSS2.p1.1.m1.7.7.6.6.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.7.7.6.6.2" xref="S2.SS4.SSS2.p1.1.m1.7.7.6.6.2.cmml">
           a
          </mi>
          <mn id="S2.SS4.SSS2.p1.1.m1.7.7.6.6.3" xref="S2.SS4.SSS2.p1.1.m1.7.7.6.6.3.cmml">
           2
          </mn>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.17" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <mi id="S2.SS4.SSS2.p1.1.m1.1.1" mathvariant="normal" xref="S2.SS4.SSS2.p1.1.m1.1.1.cmml">
          …
         </mi>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.18" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.8.8.7.7" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.2" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.2.cmml">
           o
          </mi>
          <mrow id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.cmml">
           <mi id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.2" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.2.cmml">
            n
           </mi>
           <mo id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.1" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.1.cmml">
            −
           </mo>
           <mn id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.3" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.3.cmml">
            1
           </mn>
          </mrow>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.19" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.9.9.8.8" xref="S2.SS4.SSS2.p1.1.m1.9.9.8.8.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.9.9.8.8.2" xref="S2.SS4.SSS2.p1.1.m1.9.9.8.8.2.cmml">
           t
          </mi>
          <mi id="S2.SS4.SSS2.p1.1.m1.9.9.8.8.3" xref="S2.SS4.SSS2.p1.1.m1.9.9.8.8.3.cmml">
           n
          </mi>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.20" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.10.10.9.9" xref="S2.SS4.SSS2.p1.1.m1.10.10.9.9.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.10.10.9.9.2" xref="S2.SS4.SSS2.p1.1.m1.10.10.9.9.2.cmml">
           a
          </mi>
          <mi id="S2.SS4.SSS2.p1.1.m1.10.10.9.9.3" xref="S2.SS4.SSS2.p1.1.m1.10.10.9.9.3.cmml">
           n
          </mi>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.21" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ,
         </mo>
         <msub id="S2.SS4.SSS2.p1.1.m1.11.11.10.10" xref="S2.SS4.SSS2.p1.1.m1.11.11.10.10.cmml">
          <mi id="S2.SS4.SSS2.p1.1.m1.11.11.10.10.2" xref="S2.SS4.SSS2.p1.1.m1.11.11.10.10.2.cmml">
           o
          </mi>
          <mi id="S2.SS4.SSS2.p1.1.m1.11.11.10.10.3" xref="S2.SS4.SSS2.p1.1.m1.11.11.10.10.3.cmml">
           n
          </mi>
         </msub>
         <mo id="S2.SS4.SSS2.p1.1.m1.11.11.10.22" stretchy="false" xref="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml">
          ⟩
         </mo>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.1.m1.11b">
         <list id="S2.SS4.SSS2.p1.1.m1.11.11.11.cmml" xref="S2.SS4.SSS2.p1.1.m1.11.11.10">
          <apply id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.2">
            𝑜
           </ci>
           <cn id="S2.SS4.SSS2.p1.1.m1.2.2.1.1.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.1.m1.2.2.1.1.3">
            0
           </cn>
          </apply>
          <apply id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.2">
            𝑡
           </ci>
           <cn id="S2.SS4.SSS2.p1.1.m1.3.3.2.2.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.1.m1.3.3.2.2.3">
            1
           </cn>
          </apply>
          <apply id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.2">
            𝑎
           </ci>
           <cn id="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.1.m1.4.4.3.3.3">
            1
           </cn>
          </apply>
          <apply id="S2.SS4.SSS2.p1.1.m1.5.5.4.4.cmml" xref="S2.SS4.SSS2.p1.1.m1.5.5.4.4">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.5.5.4.4.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.5.5.4.4">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.5.5.4.4.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.5.5.4.4.2">
            𝑜
           </ci>
           <cn id="S2.SS4.SSS2.p1.1.m1.5.5.4.4.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.1.m1.5.5.4.4.3">
            1
           </cn>
          </apply>
          <apply id="S2.SS4.SSS2.p1.1.m1.6.6.5.5.cmml" xref="S2.SS4.SSS2.p1.1.m1.6.6.5.5">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.6.6.5.5.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.6.6.5.5">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.6.6.5.5.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.6.6.5.5.2">
            𝑡
           </ci>
           <cn id="S2.SS4.SSS2.p1.1.m1.6.6.5.5.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.1.m1.6.6.5.5.3">
            2
           </cn>
          </apply>
          <apply id="S2.SS4.SSS2.p1.1.m1.7.7.6.6.cmml" xref="S2.SS4.SSS2.p1.1.m1.7.7.6.6">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.7.7.6.6.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.7.7.6.6">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.7.7.6.6.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.7.7.6.6.2">
            𝑎
           </ci>
           <cn id="S2.SS4.SSS2.p1.1.m1.7.7.6.6.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.1.m1.7.7.6.6.3">
            2
           </cn>
          </apply>
          <ci id="S2.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1">
           …
          </ci>
          <apply id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.cmml" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.2">
            𝑜
           </ci>
           <apply id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3">
            <minus id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.1">
            </minus>
            <ci id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.2">
             𝑛
            </ci>
            <cn id="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.1.m1.8.8.7.7.3.3">
             1
            </cn>
           </apply>
          </apply>
          <apply id="S2.SS4.SSS2.p1.1.m1.9.9.8.8.cmml" xref="S2.SS4.SSS2.p1.1.m1.9.9.8.8">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.9.9.8.8.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.9.9.8.8">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.9.9.8.8.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.9.9.8.8.2">
            𝑡
           </ci>
           <ci id="S2.SS4.SSS2.p1.1.m1.9.9.8.8.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.9.9.8.8.3">
            𝑛
           </ci>
          </apply>
          <apply id="S2.SS4.SSS2.p1.1.m1.10.10.9.9.cmml" xref="S2.SS4.SSS2.p1.1.m1.10.10.9.9">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.10.10.9.9.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.10.10.9.9">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.10.10.9.9.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.10.10.9.9.2">
            𝑎
           </ci>
           <ci id="S2.SS4.SSS2.p1.1.m1.10.10.9.9.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.10.10.9.9.3">
            𝑛
           </ci>
          </apply>
          <apply id="S2.SS4.SSS2.p1.1.m1.11.11.10.10.cmml" xref="S2.SS4.SSS2.p1.1.m1.11.11.10.10">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.1.m1.11.11.10.10.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.11.11.10.10">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.1.m1.11.11.10.10.2.cmml" xref="S2.SS4.SSS2.p1.1.m1.11.11.10.10.2">
            𝑜
           </ci>
           <ci id="S2.SS4.SSS2.p1.1.m1.11.11.10.10.3.cmml" xref="S2.SS4.SSS2.p1.1.m1.11.11.10.10.3">
            𝑛
           </ci>
          </apply>
         </list>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.1.m1.11c">
         \langle o_{0},t_{1},a_{1},o_{1},t_{2},a_{2},\ldots,o_{n-1},t_{n},a_{n},o_{n}\rangle
        </annotation>
       </semantics>
      </math>
      , where
      <math alttext="o_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.2.m2.1">
       <semantics id="S2.SS4.SSS2.p1.2.m2.1a">
        <msub id="S2.SS4.SSS2.p1.2.m2.1.1" xref="S2.SS4.SSS2.p1.2.m2.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.2.m2.1.1.2" xref="S2.SS4.SSS2.p1.2.m2.1.1.2.cmml">
          o
         </mi>
         <mi id="S2.SS4.SSS2.p1.2.m2.1.1.3" xref="S2.SS4.SSS2.p1.2.m2.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.2.m2.1b">
         <apply id="S2.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1.2">
           𝑜
          </ci>
          <ci id="S2.SS4.SSS2.p1.2.m2.1.1.3.cmml" xref="S2.SS4.SSS2.p1.2.m2.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.2.m2.1c">
         o_{i}
        </annotation>
       </semantics>
      </math>
      ,
      <math alttext="t_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.3.m3.1">
       <semantics id="S2.SS4.SSS2.p1.3.m3.1a">
        <msub id="S2.SS4.SSS2.p1.3.m3.1.1" xref="S2.SS4.SSS2.p1.3.m3.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.3.m3.1.1.2" xref="S2.SS4.SSS2.p1.3.m3.1.1.2.cmml">
          t
         </mi>
         <mi id="S2.SS4.SSS2.p1.3.m3.1.1.3" xref="S2.SS4.SSS2.p1.3.m3.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.3.m3.1b">
         <apply id="S2.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.3.m3.1.1.1.cmml" xref="S2.SS4.SSS2.p1.3.m3.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.3.m3.1.1.2.cmml" xref="S2.SS4.SSS2.p1.3.m3.1.1.2">
           𝑡
          </ci>
          <ci id="S2.SS4.SSS2.p1.3.m3.1.1.3.cmml" xref="S2.SS4.SSS2.p1.3.m3.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.3.m3.1c">
         t_{i}
        </annotation>
       </semantics>
      </math>
      , and
      <math alttext="a_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.4.m4.1">
       <semantics id="S2.SS4.SSS2.p1.4.m4.1a">
        <msub id="S2.SS4.SSS2.p1.4.m4.1.1" xref="S2.SS4.SSS2.p1.4.m4.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.4.m4.1.1.2" xref="S2.SS4.SSS2.p1.4.m4.1.1.2.cmml">
          a
         </mi>
         <mi id="S2.SS4.SSS2.p1.4.m4.1.1.3" xref="S2.SS4.SSS2.p1.4.m4.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.4.m4.1b">
         <apply id="S2.SS4.SSS2.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS2.p1.4.m4.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.4.m4.1.1.1.cmml" xref="S2.SS4.SSS2.p1.4.m4.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.4.m4.1.1.2.cmml" xref="S2.SS4.SSS2.p1.4.m4.1.1.2">
           𝑎
          </ci>
          <ci id="S2.SS4.SSS2.p1.4.m4.1.1.3.cmml" xref="S2.SS4.SSS2.p1.4.m4.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.4.m4.1c">
         a_{i}
        </annotation>
       </semantics>
      </math>
      denote the observation, thought, and action at the
      <math alttext="i" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.5.m5.1">
       <semantics id="S2.SS4.SSS2.p1.5.m5.1a">
        <mi id="S2.SS4.SSS2.p1.5.m5.1.1" xref="S2.SS4.SSS2.p1.5.m5.1.1.cmml">
         i
        </mi>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.5.m5.1b">
         <ci id="S2.SS4.SSS2.p1.5.m5.1.1.cmml" xref="S2.SS4.SSS2.p1.5.m5.1.1">
          𝑖
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.5.m5.1c">
         i
        </annotation>
       </semantics>
      </math>
      -th turn, respectively. Specifically,
      <math alttext="o_{0}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.6.m6.1">
       <semantics id="S2.SS4.SSS2.p1.6.m6.1a">
        <msub id="S2.SS4.SSS2.p1.6.m6.1.1" xref="S2.SS4.SSS2.p1.6.m6.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.6.m6.1.1.2" xref="S2.SS4.SSS2.p1.6.m6.1.1.2.cmml">
          o
         </mi>
         <mn id="S2.SS4.SSS2.p1.6.m6.1.1.3" xref="S2.SS4.SSS2.p1.6.m6.1.1.3.cmml">
          0
         </mn>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.6.m6.1b">
         <apply id="S2.SS4.SSS2.p1.6.m6.1.1.cmml" xref="S2.SS4.SSS2.p1.6.m6.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.6.m6.1.1.1.cmml" xref="S2.SS4.SSS2.p1.6.m6.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.6.m6.1.1.2.cmml" xref="S2.SS4.SSS2.p1.6.m6.1.1.2">
           𝑜
          </ci>
          <cn id="S2.SS4.SSS2.p1.6.m6.1.1.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.6.m6.1.1.3">
           0
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.6.m6.1c">
         o_{0}
        </annotation>
       </semantics>
      </math>
      denotes the initial observation consisting of the input instruction and detected text, and
      <math alttext="o_{n}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.7.m7.1">
       <semantics id="S2.SS4.SSS2.p1.7.m7.1a">
        <msub id="S2.SS4.SSS2.p1.7.m7.1.1" xref="S2.SS4.SSS2.p1.7.m7.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.7.m7.1.1.2" xref="S2.SS4.SSS2.p1.7.m7.1.1.2.cmml">
          o
         </mi>
         <mi id="S2.SS4.SSS2.p1.7.m7.1.1.3" xref="S2.SS4.SSS2.p1.7.m7.1.1.3.cmml">
          n
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.7.m7.1b">
         <apply id="S2.SS4.SSS2.p1.7.m7.1.1.cmml" xref="S2.SS4.SSS2.p1.7.m7.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.7.m7.1.1.1.cmml" xref="S2.SS4.SSS2.p1.7.m7.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.7.m7.1.1.2.cmml" xref="S2.SS4.SSS2.p1.7.m7.1.1.2">
           𝑜
          </ci>
          <ci id="S2.SS4.SSS2.p1.7.m7.1.1.3.cmml" xref="S2.SS4.SSS2.p1.7.m7.1.1.3">
           𝑛
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.7.m7.1c">
         o_{n}
        </annotation>
       </semantics>
      </math>
      denotes the final detection result. At each turn, based on the historical trajectory
      <math alttext="c_{i}=\langle o_{0},t_{1},a_{1},\ldots,o_{i-1}\rangle" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.8.m8.5">
       <semantics id="S2.SS4.SSS2.p1.8.m8.5a">
        <mrow id="S2.SS4.SSS2.p1.8.m8.5.5" xref="S2.SS4.SSS2.p1.8.m8.5.5.cmml">
         <msub id="S2.SS4.SSS2.p1.8.m8.5.5.6" xref="S2.SS4.SSS2.p1.8.m8.5.5.6.cmml">
          <mi id="S2.SS4.SSS2.p1.8.m8.5.5.6.2" xref="S2.SS4.SSS2.p1.8.m8.5.5.6.2.cmml">
           c
          </mi>
          <mi id="S2.SS4.SSS2.p1.8.m8.5.5.6.3" xref="S2.SS4.SSS2.p1.8.m8.5.5.6.3.cmml">
           i
          </mi>
         </msub>
         <mo id="S2.SS4.SSS2.p1.8.m8.5.5.5" xref="S2.SS4.SSS2.p1.8.m8.5.5.5.cmml">
          =
         </mo>
         <mrow id="S2.SS4.SSS2.p1.8.m8.5.5.4.4" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.5.cmml">
          <mo id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.5" stretchy="false" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.5.cmml">
           ⟨
          </mo>
          <msub id="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1" xref="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.cmml">
           <mi id="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.2" xref="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.2.cmml">
            o
           </mi>
           <mn id="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.3" xref="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.3.cmml">
            0
           </mn>
          </msub>
          <mo id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.6" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.5.cmml">
           ,
          </mo>
          <msub id="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2" xref="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.cmml">
           <mi id="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.2" xref="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.2.cmml">
            t
           </mi>
           <mn id="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.3" xref="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.3.cmml">
            1
           </mn>
          </msub>
          <mo id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.7" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.5.cmml">
           ,
          </mo>
          <msub id="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3" xref="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.cmml">
           <mi id="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.2" xref="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.2.cmml">
            a
           </mi>
           <mn id="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.3" xref="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.3.cmml">
            1
           </mn>
          </msub>
          <mo id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.8" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.5.cmml">
           ,
          </mo>
          <mi id="S2.SS4.SSS2.p1.8.m8.1.1" mathvariant="normal" xref="S2.SS4.SSS2.p1.8.m8.1.1.cmml">
           …
          </mi>
          <mo id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.9" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.5.cmml">
           ,
          </mo>
          <msub id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.cmml">
           <mi id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.2" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.2.cmml">
            o
           </mi>
           <mrow id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.cmml">
            <mi id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.2" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.2.cmml">
             i
            </mi>
            <mo id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.1" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.1.cmml">
             −
            </mo>
            <mn id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.3" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.3.cmml">
             1
            </mn>
           </mrow>
          </msub>
          <mo id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.10" stretchy="false" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.5.cmml">
           ⟩
          </mo>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.8.m8.5b">
         <apply id="S2.SS4.SSS2.p1.8.m8.5.5.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5">
          <eq id="S2.SS4.SSS2.p1.8.m8.5.5.5.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.5">
          </eq>
          <apply id="S2.SS4.SSS2.p1.8.m8.5.5.6.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.6">
           <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.8.m8.5.5.6.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.6">
            subscript
           </csymbol>
           <ci id="S2.SS4.SSS2.p1.8.m8.5.5.6.2.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.6.2">
            𝑐
           </ci>
           <ci id="S2.SS4.SSS2.p1.8.m8.5.5.6.3.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.6.3">
            𝑖
           </ci>
          </apply>
          <list id="S2.SS4.SSS2.p1.8.m8.5.5.4.5.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4">
           <apply id="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1">
            <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1">
             subscript
            </csymbol>
            <ci id="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.2.cmml" xref="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.2">
             𝑜
            </ci>
            <cn id="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.8.m8.2.2.1.1.1.3">
             0
            </cn>
           </apply>
           <apply id="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.cmml" xref="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2">
            <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2">
             subscript
            </csymbol>
            <ci id="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.2.cmml" xref="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.2">
             𝑡
            </ci>
            <cn id="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.8.m8.3.3.2.2.2.3">
             1
            </cn>
           </apply>
           <apply id="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.cmml" xref="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3">
            <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3">
             subscript
            </csymbol>
            <ci id="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.2.cmml" xref="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.2">
             𝑎
            </ci>
            <cn id="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.8.m8.4.4.3.3.3.3">
             1
            </cn>
           </apply>
           <ci id="S2.SS4.SSS2.p1.8.m8.1.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.1.1">
            …
           </ci>
           <apply id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4">
            <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4">
             subscript
            </csymbol>
            <ci id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.2.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.2">
             𝑜
            </ci>
            <apply id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3">
             <minus id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.1.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.1">
             </minus>
             <ci id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.2.cmml" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.2">
              𝑖
             </ci>
             <cn id="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.3.cmml" type="integer" xref="S2.SS4.SSS2.p1.8.m8.5.5.4.4.4.3.3">
              1
             </cn>
            </apply>
           </apply>
          </list>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.8.m8.5c">
         c_{i}=\langle o_{0},t_{1},a_{1},\ldots,o_{i-1}\rangle
        </annotation>
       </semantics>
      </math>
      , the agent aims to generate thought
      <math alttext="t_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.9.m9.1">
       <semantics id="S2.SS4.SSS2.p1.9.m9.1a">
        <msub id="S2.SS4.SSS2.p1.9.m9.1.1" xref="S2.SS4.SSS2.p1.9.m9.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.9.m9.1.1.2" xref="S2.SS4.SSS2.p1.9.m9.1.1.2.cmml">
          t
         </mi>
         <mi id="S2.SS4.SSS2.p1.9.m9.1.1.3" xref="S2.SS4.SSS2.p1.9.m9.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.9.m9.1b">
         <apply id="S2.SS4.SSS2.p1.9.m9.1.1.cmml" xref="S2.SS4.SSS2.p1.9.m9.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.9.m9.1.1.1.cmml" xref="S2.SS4.SSS2.p1.9.m9.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.9.m9.1.1.2.cmml" xref="S2.SS4.SSS2.p1.9.m9.1.1.2">
           𝑡
          </ci>
          <ci id="S2.SS4.SSS2.p1.9.m9.1.1.3.cmml" xref="S2.SS4.SSS2.p1.9.m9.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.9.m9.1c">
         t_{i}
        </annotation>
       </semantics>
      </math>
      and action
      <math alttext="a_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.10.m10.1">
       <semantics id="S2.SS4.SSS2.p1.10.m10.1a">
        <msub id="S2.SS4.SSS2.p1.10.m10.1.1" xref="S2.SS4.SSS2.p1.10.m10.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.10.m10.1.1.2" xref="S2.SS4.SSS2.p1.10.m10.1.1.2.cmml">
          a
         </mi>
         <mi id="S2.SS4.SSS2.p1.10.m10.1.1.3" xref="S2.SS4.SSS2.p1.10.m10.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.10.m10.1b">
         <apply id="S2.SS4.SSS2.p1.10.m10.1.1.cmml" xref="S2.SS4.SSS2.p1.10.m10.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.10.m10.1.1.1.cmml" xref="S2.SS4.SSS2.p1.10.m10.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.10.m10.1.1.2.cmml" xref="S2.SS4.SSS2.p1.10.m10.1.1.2">
           𝑎
          </ci>
          <ci id="S2.SS4.SSS2.p1.10.m10.1.1.3.cmml" xref="S2.SS4.SSS2.p1.10.m10.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.10.m10.1c">
         a_{i}
        </annotation>
       </semantics>
      </math>
      .
Therefore, during the trajectory fine-tuning process, we only compute the cross-entropy loss for
      <math alttext="t_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.11.m11.1">
       <semantics id="S2.SS4.SSS2.p1.11.m11.1a">
        <msub id="S2.SS4.SSS2.p1.11.m11.1.1" xref="S2.SS4.SSS2.p1.11.m11.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.11.m11.1.1.2" xref="S2.SS4.SSS2.p1.11.m11.1.1.2.cmml">
          t
         </mi>
         <mi id="S2.SS4.SSS2.p1.11.m11.1.1.3" xref="S2.SS4.SSS2.p1.11.m11.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.11.m11.1b">
         <apply id="S2.SS4.SSS2.p1.11.m11.1.1.cmml" xref="S2.SS4.SSS2.p1.11.m11.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.11.m11.1.1.1.cmml" xref="S2.SS4.SSS2.p1.11.m11.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.11.m11.1.1.2.cmml" xref="S2.SS4.SSS2.p1.11.m11.1.1.2">
           𝑡
          </ci>
          <ci id="S2.SS4.SSS2.p1.11.m11.1.1.3.cmml" xref="S2.SS4.SSS2.p1.11.m11.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.11.m11.1c">
         t_{i}
        </annotation>
       </semantics>
      </math>
      and
      <math alttext="a_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.12.m12.1">
       <semantics id="S2.SS4.SSS2.p1.12.m12.1a">
        <msub id="S2.SS4.SSS2.p1.12.m12.1.1" xref="S2.SS4.SSS2.p1.12.m12.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.12.m12.1.1.2" xref="S2.SS4.SSS2.p1.12.m12.1.1.2.cmml">
          a
         </mi>
         <mi id="S2.SS4.SSS2.p1.12.m12.1.1.3" xref="S2.SS4.SSS2.p1.12.m12.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.12.m12.1b">
         <apply id="S2.SS4.SSS2.p1.12.m12.1.1.cmml" xref="S2.SS4.SSS2.p1.12.m12.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.12.m12.1.1.1.cmml" xref="S2.SS4.SSS2.p1.12.m12.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.12.m12.1.1.2.cmml" xref="S2.SS4.SSS2.p1.12.m12.1.1.2">
           𝑎
          </ci>
          <ci id="S2.SS4.SSS2.p1.12.m12.1.1.3.cmml" xref="S2.SS4.SSS2.p1.12.m12.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.12.m12.1c">
         a_{i}
        </annotation>
       </semantics>
      </math>
      while masking
      <math alttext="o_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p1.13.m13.1">
       <semantics id="S2.SS4.SSS2.p1.13.m13.1a">
        <msub id="S2.SS4.SSS2.p1.13.m13.1.1" xref="S2.SS4.SSS2.p1.13.m13.1.1.cmml">
         <mi id="S2.SS4.SSS2.p1.13.m13.1.1.2" xref="S2.SS4.SSS2.p1.13.m13.1.1.2.cmml">
          o
         </mi>
         <mi id="S2.SS4.SSS2.p1.13.m13.1.1.3" xref="S2.SS4.SSS2.p1.13.m13.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.13.m13.1b">
         <apply id="S2.SS4.SSS2.p1.13.m13.1.1.cmml" xref="S2.SS4.SSS2.p1.13.m13.1.1">
          <csymbol cd="ambiguous" id="S2.SS4.SSS2.p1.13.m13.1.1.1.cmml" xref="S2.SS4.SSS2.p1.13.m13.1.1">
           subscript
          </csymbol>
          <ci id="S2.SS4.SSS2.p1.13.m13.1.1.2.cmml" xref="S2.SS4.SSS2.p1.13.m13.1.1.2">
           𝑜
          </ci>
          <ci id="S2.SS4.SSS2.p1.13.m13.1.1.3.cmml" xref="S2.SS4.SSS2.p1.13.m13.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.13.m13.1c">
         o_{i}
        </annotation>
       </semantics>
      </math>
      :
     </p>
     <table class="ltx_equation ltx_eqn_table" id="S2.E1">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="\mathcal{L}=-\log\sum_{i=1}^{n}\text{Pr}(t_{i},a_{i}|c_{i})." class="ltx_Math" display="block" id="S2.E1.m1.1">
          <semantics id="S2.E1.m1.1a">
           <mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">
            <mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">
             <mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.4.cmml">
              ℒ
             </mi>
             <mo id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml">
              =
             </mo>
             <mrow id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">
              <mo id="S2.E1.m1.1.1.1.1.2a" rspace="0.167em" xref="S2.E1.m1.1.1.1.1.2.cmml">
               −
              </mo>
              <mrow id="S2.E1.m1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.2.2.cmml">
               <mi id="S2.E1.m1.1.1.1.1.2.2.4" xref="S2.E1.m1.1.1.1.1.2.2.4.cmml">
                log
               </mi>
               <mo id="S2.E1.m1.1.1.1.1.2.2.3" lspace="0em" rspace="0em" xref="S2.E1.m1.1.1.1.1.2.2.3.cmml">
                ​
               </mo>
               <mrow id="S2.E1.m1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.2.2.2.cmml">
                <munderover id="S2.E1.m1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.1.1.1.1.2.2.2.3.cmml">
                 <mo id="S2.E1.m1.1.1.1.1.2.2.2.3.2.2" movablelimits="false" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.2.cmml">
                  ∑
                 </mo>
                 <mrow id="S2.E1.m1.1.1.1.1.2.2.2.3.2.3" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.cmml">
                  <mi id="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.2" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.2.cmml">
                   i
                  </mi>
                  <mo id="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.1" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.1.cmml">
                   =
                  </mo>
                  <mn id="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.3" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.3.cmml">
                   1
                  </mn>
                 </mrow>
                 <mi id="S2.E1.m1.1.1.1.1.2.2.2.3.3" xref="S2.E1.m1.1.1.1.1.2.2.2.3.3.cmml">
                  n
                 </mi>
                </munderover>
                <mrow id="S2.E1.m1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.1.1.1.1.2.2.2.2.cmml">
                 <mtext id="S2.E1.m1.1.1.1.1.2.2.2.2.4" xref="S2.E1.m1.1.1.1.1.2.2.2.2.4a.cmml">
                  Pr
                 </mtext>
                 <mo id="S2.E1.m1.1.1.1.1.2.2.2.2.3" lspace="0em" rspace="0em" xref="S2.E1.m1.1.1.1.1.2.2.2.2.3.cmml">
                  ​
                 </mo>
                 <mrow id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml">
                  <mo id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.3" stretchy="false" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml">
                   (
                  </mo>
                  <msub id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                   <mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">
                    t
                   </mi>
                   <mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                    i
                   </mi>
                  </msub>
                  <mo id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.4" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml">
                   ,
                  </mo>
                  <mrow id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.cmml">
                   <msub id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.cmml">
                    <mi id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.cmml">
                     a
                    </mi>
                    <mi id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3.cmml">
                     i
                    </mi>
                   </msub>
                   <mo fence="false" id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.1" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.1.cmml">
                    |
                   </mo>
                   <msub id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml">
                    <mi id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.2" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.2.cmml">
                     c
                    </mi>
                    <mi id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.3" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.3.cmml">
                     i
                    </mi>
                   </msub>
                  </mrow>
                  <mo id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.5" stretchy="false" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml">
                   )
                  </mo>
                 </mrow>
                </mrow>
               </mrow>
              </mrow>
             </mrow>
            </mrow>
            <mo id="S2.E1.m1.1.1.1.2" lspace="0em" xref="S2.E1.m1.1.1.1.1.cmml">
             .
            </mo>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b">
            <apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1">
             <eq id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3">
             </eq>
             <ci id="S2.E1.m1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.4">
              ℒ
             </ci>
             <apply id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2">
              <minus id="S2.E1.m1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2">
              </minus>
              <apply id="S2.E1.m1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2">
               <times id="S2.E1.m1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.3">
               </times>
               <log id="S2.E1.m1.1.1.1.1.2.2.4.cmml" xref="S2.E1.m1.1.1.1.1.2.2.4">
               </log>
               <apply id="S2.E1.m1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2">
                <apply id="S2.E1.m1.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3">
                 <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3">
                  superscript
                 </csymbol>
                 <apply id="S2.E1.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3">
                  <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.2.3.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3">
                   subscript
                  </csymbol>
                  <sum id="S2.E1.m1.1.1.1.1.2.2.2.3.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.2">
                  </sum>
                  <apply id="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.3">
                   <eq id="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.1">
                   </eq>
                   <ci id="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.2">
                    𝑖
                   </ci>
                   <cn id="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.2.2.2.3.2.3.3">
                    1
                   </cn>
                  </apply>
                 </apply>
                 <ci id="S2.E1.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.3.3">
                  𝑛
                 </ci>
                </apply>
                <apply id="S2.E1.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2">
                 <times id="S2.E1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.3">
                 </times>
                 <ci id="S2.E1.m1.1.1.1.1.2.2.2.2.4a.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.4">
                  <mtext id="S2.E1.m1.1.1.1.1.2.2.2.2.4.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.4">
                   Pr
                  </mtext>
                 </ci>
                 <interval closure="open" id="S2.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2">
                  <apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1">
                   <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1">
                    subscript
                   </csymbol>
                   <ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2">
                    𝑡
                   </ci>
                   <ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3">
                    𝑖
                   </ci>
                  </apply>
                  <apply id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2">
                   <csymbol cd="latexml" id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.1">
                    conditional
                   </csymbol>
                   <apply id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2">
                    <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2">
                     subscript
                    </csymbol>
                    <ci id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.2">
                     𝑎
                    </ci>
                    <ci id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.2.3">
                     𝑖
                    </ci>
                   </apply>
                   <apply id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3">
                    <csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3">
                     subscript
                    </csymbol>
                    <ci id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.2">
                     𝑐
                    </ci>
                    <ci id="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2.2.2.2.2.3.3">
                     𝑖
                    </ci>
                   </apply>
                  </apply>
                 </interval>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="S2.E1.m1.1c">
            \mathcal{L}=-\log\sum_{i=1}^{n}\text{Pr}(t_{i},a_{i}|c_{i}).
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
        <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
         <span class="ltx_tag ltx_tag_equation ltx_align_right">
          (1)
         </span>
        </td>
       </tr>
      </tbody>
     </table>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S2.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.5
    </span>
    Comparison to Previous Work
   </h3>
   <div class="ltx_para" id="S2.SS5.p1">
    <p class="ltx_p" id="S2.SS5.p1.1">
     To clarify the differences between HaluAgent and other hallucination detection methods, we aim to address the following two questions:
    </p>
   </div>
   <div class="ltx_para" id="S2.SS5.p2">
    <p class="ltx_p" id="S2.SS5.p2.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS5.p2.1.1">
      What are the benefits of designing an autonomous hallucination detection agent?
     </span>
     Hallucination detection is fundamental to related research. Most previous work either relied on the internal knowledge of LLMs (might be limited)
     <cite class="ltx_cite ltx_citemacro_cite">
      Cohen et al. (
      <a class="ltx_ref" href="#bib.bib6" title="">
       2023
      </a>
      )
     </cite>
     or performed coarse-grained detection process
     <cite class="ltx_cite ltx_citemacro_cite">
      Manakul et al. (
      <a class="ltx_ref" href="#bib.bib22" title="">
       2023
      </a>
      )
     </cite>
     . Designing an agent for hallucination detection offers a flexible alternative that can effectively extend the detection capability of LLMs through tool utilization.
Existing agent-based detection methods do not consider tool utilization or only employ limited tools such as search engine for hallucination detection in long texts
     <cite class="ltx_cite ltx_citemacro_cite">
      Lei et al. (
      <a class="ltx_ref" href="#bib.bib17" title="">
       2023
      </a>
      ); Wei et al. (
      <a class="ltx_ref" href="#bib.bib30" title="">
       2024
      </a>
      )
     </cite>
     .
In contrast, HaluAgent develops a comprehensive and extensible toolbox and performs fine-grained reasoning process for hallucination detection, providing a more adaptable and robust solution.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS5.p3">
    <p class="ltx_p" id="S2.SS5.p3.1">
     •
     <span class="ltx_text ltx_font_bold" id="S2.SS5.p3.1.1">
      Can smaller language models perform well in challenging hallucination detection?
     </span>
     Existing methods
     <cite class="ltx_cite ltx_citemacro_cite">
      Chern et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      ); Manakul et al. (
      <a class="ltx_ref" href="#bib.bib22" title="">
       2023
      </a>
      ); Dhuliawala et al. (
      <a class="ltx_ref" href="#bib.bib8" title="">
       2023
      </a>
      )
     </cite>
     heavily depended on powerful closed-source LLMs such as GPT-4, which leads to high computational costs and poses unavoidable limitations for the practical deployment of these technologies. Moreover, relying on closed-source models makes the detection results difficult to reproduce.
Our work demonstrates that by incorporating the agent capabilities and tool integration, smaller language models can also effectively handle challenging hallucination detection tasks.
This way can provide a more viable and economical choice, significantly reducing the need for closed-source LLMs.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Experimental Setup
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">
      In-domain/Out-of-domain Tasks.
     </span>
     We evaluate HaluAgent on both in-domain and out-of-domain datasets. As described in Section
     <a class="ltx_ref" href="#S2.T2" title="Table 2 ‣ 2.4.1 Trajectory Generation ‣ 2.4 Bilingual Agent Tuning ‣ 2 Approach ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     , we select HaluEval-QA, WebQA, Ape210K, HumanEval, and WordCnt as in-domain datasets. For out-of-domain datasets, we use a Chinese dataset, HalluQA
     <cite class="ltx_cite ltx_citemacro_cite">
      Cheng et al. (
      <a class="ltx_ref" href="#bib.bib4" title="">
       2023
      </a>
      )
     </cite>
     , and an English dataset, HaluEval 2.0
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib18" title="">
       2024
      </a>
      )
     </cite>
     , which cover diverse hallucination detection scenarios including knowledge, math, science texts.
All datasets are associated with ground-truth response-level hallucination labels. We present the details in Appendix
     <a class="ltx_ref" href="#A2.SS1" title="B.1 Out-of-domain Datasets ‣ Appendix B Experiment Setup ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       B.1
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">
      Sentence-level Tasks.
     </span>
     HaluAgent performs fine-grained hallucination detection by segmenting sentences and provides sentence-level detection results. We use FacTool
     <cite class="ltx_cite ltx_citemacro_cite">
      Chern et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2023
      </a>
      )
     </cite>
     with annotated claim-level hallucination labels to evaluate the fine-grained detection capability. We consider each claim as a sentence and concatenate all claims as the input text. FacTool contains five sub-datasets: Chinese-QA, KB-QA, math problems, code generation, and scientific literature review. Since the code generation sub-dataset is collected from HumanEval and overlaps with our training data, we conduct sentence-level detection experiments on the other four sub-datasets.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">
      Baselines.
     </span>
     Unlike previous work relied on powerful closed-source models like GPT-4, HaluAgent is built upon relatively smaller language model by performing fine-tuning on hallucination detection trajectory data.
Hence, we compare HaluAgent with two kinds of baselines: (1) Closed-source models:
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.2">
      GPT-4 prompt
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.3">
      GPT-4 pipeline
     </em>
     employ GPT-4 with simple task description prompts and our proposed detection pipeline, respectively; (2) Open-source models:
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.4">
      Baichuan2-Chat (7B and 13B)
     </em>
     , which is the backbone model of HaluAgent. For other detection methods without comparison in this work, they are either implemented based on ChatGPT/GPT-4 (an unfair comparison) or focus solely on specific tasks, making it difficult to adapt to other tasks, as shown in Table
     <a class="ltx_ref" href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">
      Metrics.
     </span>
     Hallucination detection is essentially a binary classification task. Consequently, we adopt
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.2">
      Accuracy
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.3">
      F1 score
     </em>
     as metrics for response-level detection evaluation. Considering the imbalanced hallucination data distribution at the sentence level, we adopt
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.4">
      Accuracy
     </em>
     ,
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.5">
      Precision
     </em>
     ,
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.6">
      Recall
     </em>
     , and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.7">
      F1 score
     </em>
     for sentence-level detection.
    </p>
   </div>
   <figure class="ltx_table" id="S3.T3">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:433.6pt;height:152.6pt;vertical-align:-0.8pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-66.0pt,23.1pt) scale(0.766675670387833,0.766675670387833) ;">
      <table class="ltx_tabular ltx_align_middle" id="S3.T3.1.1">
       <tr class="ltx_tr" id="S3.T3.1.1.1">
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.1" rowspan="2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text" id="S3.T3.1.1.1.1.1">
          Types
         </span>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T3.1.1.1.2" rowspan="2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text" id="S3.T3.1.1.1.2.1">
          Datasets
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T3.1.1.1.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         GPT-4
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T3.1.1.1.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Baichuan2-Chat
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T3.1.1.1.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         HaluAgent
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.2">
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         prompt
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         pipeline
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         7B
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         13B
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         7B
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.1.1.2.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         13B
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.3">
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1" rowspan="6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text" id="S3.T3.1.1.3.1.1">
          <span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.3.1.1.1">
           <span class="ltx_tr" id="S3.T3.1.1.3.1.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.1.1.1.1.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
             In-domain
            </span>
           </span>
           <span class="ltx_tr" id="S3.T3.1.1.3.1.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.3.1.1.1.2.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
             Datasets
            </span>
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S3.T3.1.1.3.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         WebQA
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         82.00/35.71
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.3.4.1">
          91.00
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.3.4.2">
          57.14
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         51.00/14.04
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         54.00/
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.3.6.1">
          61.67
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         80.00/54.55
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.1.1.3.8" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.3.8.1">
          82.83
         </span>
         /51.43
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.4">
        <td class="ltx_td ltx_nopad_l ltx_align_left" id="S3.T3.1.1.4.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Ape210K
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         72.33/74.21
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.4.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.4.3.1">
          76.63
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.4.3.2">
          75.10
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         49.00/7.27
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.4.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         51.33/58.29
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         72.00/72.55
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.4.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.4.7.1">
          73.40
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.4.7.2">
          73.68
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.5">
        <td class="ltx_td ltx_nopad_l ltx_align_left" id="S3.T3.1.1.5.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         HumanEval
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         71.43/79.07
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.5.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.5.3.1">
          93.44
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.5.3.2">
          94.12
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         34.92/49.38
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.5.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         47.62/19.51
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.6.1">
          93.44
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.6.2">
          94.12
         </span>
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.5.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.7.1">
          93.44
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.7.2">
          94.12
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.6">
        <td class="ltx_td ltx_nopad_l ltx_align_left" id="S3.T3.1.1.6.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         WordCnt
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         56.00/66.15
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.6.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.6.3.1">
          100.00
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.6.3.2">
          100.00
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         43.00/16.00
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.6.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         46.00/59.70
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.6.6.1">
          100.00
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.6.6.2">
          100.00
         </span>
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.6.7.1">
          100.00
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.6.7.2">
          100.00
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.7">
        <td class="ltx_td ltx_nopad_l ltx_align_left" id="S3.T3.1.1.7.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         HaluEval-QA
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         62.00/42.42
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.7.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.7.3.1">
          77.53
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.7.3.2">
          75.61
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         53.19/46.34
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.7.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         60.00/67.74
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         67.00/67.33
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.7.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.7.7.1">
          71.00
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.7.7.2">
          72.38
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.8">
        <td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S3.T3.1.1.8.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Overall
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.8.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         69.76/71.66
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.8.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.8.3.1">
          85.10
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.8.3.2">
          83.12
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.8.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         46.44/32.20
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.8.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         52.70/55.58
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.8.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         79.70/79.50
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.1.1.8.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.8.7.1">
          81.86
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.8.7.2">
          80.69
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.9">
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.9.1" rowspan="3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text" id="S3.T3.1.1.9.1.1">
          <span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.9.1.1.1">
           <span class="ltx_tr" id="S3.T3.1.1.9.1.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.9.1.1.1.1.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
             Out-of-domain
            </span>
           </span>
           <span class="ltx_tr" id="S3.T3.1.1.9.1.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.9.1.1.1.2.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
             Datasets
            </span>
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S3.T3.1.1.9.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         HalluQA
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         61.00/74.84
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.9.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.9.4.1">
          85.11
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.9.4.2">
          89.23
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         33.00/12.99
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.9.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         56.00/67.16
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         67.48/76.09
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.1.1.9.8" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.8.1">
          78.16
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.9.8.2">
          83.75
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.10">
        <td class="ltx_td ltx_nopad_l ltx_align_left" id="S3.T3.1.1.10.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         HaluEval 2.0
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         63.00/74.13
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.10.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.10.3.1">
          85.71
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.10.3.2">
          87.36
         </span>
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         54.00/69.33
        </td>
        <td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.1.1.10.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         43.00/46.73
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         75.00/76.19
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.10.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.10.7.1">
          79.00
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.10.7.2">
          78.79
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T3.1.1.11">
        <td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb ltx_border_t" id="S3.T3.1.1.11.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Overall
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.11.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         62.00/74.50
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.1.11.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.11.3.1">
          85.25
         </span>
         /
         <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.1.1.11.3.2">
          88.76
         </span>
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.11.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         43.50/50.22
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.1.11.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         49.50/58.10
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.11.6" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         69.97/76.12
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.11.7" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.11.7.1">
          78.43
         </span>
         /
         <span class="ltx_text ltx_font_bold" id="S3.T3.1.1.11.7.2">
          82.45
         </span>
        </td>
       </tr>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     Evaluation results at Accuracy and F1 score on in-domain and out-of-domain datasets.
     <span class="ltx_text ltx_font_bold" id="S3.T3.4.1">
      Bold
     </span>
     denotes the best methods among open-source models;
     <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T3.5.2">
      underline
     </span>
     denotes the best methods among closed-source models.
    </figcaption>
   </figure>
   <figure class="ltx_table" id="S3.T4">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T4.1" style="width:433.6pt;height:95.1pt;vertical-align:-0.7pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-72.5pt,15.8pt) scale(0.749413316829798,0.749413316829798) ;">
      <table class="ltx_tabular ltx_align_middle" id="S3.T4.1.1">
       <tr class="ltx_tr" id="S3.T4.1.1.1">
        <td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T4.1.1.1.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Models
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Chinese-QA
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         KB-QA
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Math
        </td>
        <td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Science
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T4.1.1.2">
        <td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.1.1.2.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         GPT4-prompt
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.2.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         54.93/59.40/33.17/42.57
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.2.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         75.97/39.29/50.00/44.00
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.2.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         55.27/49.25/23.74/32.04
        </td>
        <td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.2.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         59.14/64.71/81.82/72.26
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T4.1.1.3">
        <td class="ltx_td ltx_align_left" id="S3.T4.1.1.3.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         GPT4-pipeline
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.3.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         79.76/64.71/50.00/56.41
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.3.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         84.12/85.31/93.21/89.09
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.3.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         91.61/79.66/78.33/78.99
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.3.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         95.72/96.77/98.04/97.40
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T4.1.1.4">
        <td class="ltx_td ltx_align_left" id="S3.T4.1.1.4.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Baichuan2-Chat 7B
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.4.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         65.15/15.02/29.17/19.83
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.4.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         49.77/38.78/19.59/26.03
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.4.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         68.07/17.91/21.43/19.51
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.4.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         24.19/9.80/83.33/17.54
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T4.1.1.5">
        <td class="ltx_td ltx_align_left" id="S3.T4.1.1.5.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         Baichuan2-Chat 13B
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.5.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         56.82/21.70/22.67/22.17
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.5.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         65.41/30.77/30.00/30.38
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.5.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         70.97/13.85/20.93/16.67
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.5.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         18.82/1.31/100.00/2.58
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T4.1.1.6">
        <td class="ltx_td ltx_align_left" id="S3.T4.1.1.6.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         HaluAgent-7B
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.6.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         73.57/32.96/55.97/41.49
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.6.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         83.33/41.82/82.14/55.42
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.6.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         85.39/81.13/59.72/68.80
        </td>
        <td class="ltx_td ltx_align_center" id="S3.T4.1.1.6.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         90.27/94.77/93.55/94.16
        </td>
       </tr>
       <tr class="ltx_tr" id="S3.T4.1.1.7">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.1.1.7.1" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         HaluAgent-13B
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.1.7.2" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         75.47/33.80/56.25/42.23
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.1.7.3" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         81.97/62.07/85.71/72.00
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.1.7.4" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         87.50/64.52/72.73/68.38
        </td>
        <td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.1.7.5" style="padding-top:0.75pt;padding-bottom:0.75pt;">
         92.39/96.05/94.81/95.42
        </td>
       </tr>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 4:
     </span>
     Evaluation results of sentence-level detection on the four subsets of FacTool.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Response-level Detection
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     We present the evaluation results on in-domain and out-of-domain datasets in Table
     <a class="ltx_ref" href="#S3.T3" title="Table 3 ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     First, by comparing GPT-4 and Baichuan2-Chat, we can observe a large performance gap between closed-source and open-source models when detecting hallucinations solely based on their internal knowledge. In Table
     <a class="ltx_ref" href="#S3.T3" title="Table 3 ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     ,
GPT-4 achieves detection accuracies of 82.00% and 72.33% on WebQA and Ape210K respectively, whereas Baichuan2-Chat only achieves 51.00% and 49.00%.
This underscores the considerable difference in hallucination detection capabilities between these models.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     Second, implementing a fine-grained detection framework and incorporating extensive tools can enhance the hallucination detection performance of LLMs across various tasks. Compared to GPT-4 prompt, GPT-4 pipeline consistently yields better detection results across all datasets.
Especially for tasks that require tools to detect the hallucinations such as WordCnt, our tool-assistant framework boosts the detection accuracy of GPT-4 from 56.00% to 100.00%. This demonstrates that guiding the model to utilize appropriate tools tailored to the specific text is an effective strategy to enhance hallucination detection capabilities.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p4">
    <p class="ltx_p" id="S3.SS2.p4.1">
     Finally, based on trajectory fine-tuning, smaller open-source models can be effective autonomous agents for hallucination detection, narrowing the gap with closed-source models. As can be observed from Table
     <a class="ltx_ref" href="#S3.T3" title="Table 3 ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     , HaluAgent consistently improves detection performance across in-domain datasets compared to Baichuan2-Chat.
For instance, on the WebQA dataset, HaluAgent 7B and 13B improve the detection accuracy from 51.00%, 60.00% to 80.00%, 71.00%, respectively.
Furthermore, substantial performance improvements are observed on out-of-domain datasets, with accuracy increasing from 33.00% to 67.48% (7B) for HalluQA and from 43.00% to 79.00% (13B) for HaluEval 2.0.
With tool utilization and agent capabilities, HaluAgent achieves performance comparable to or even higher than GPT-4 prompt across all tasks, showing strong generalization capability.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Sentence-level Detection
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     In our framework, HaluAgent is capable of detecting hallucinations for each individual sentence. Thus, we evaluate the accuracy of HaluAgent in identifying hallucinations at the sentence level.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS3.p2">
    <p class="ltx_p" id="S3.SS3.p2.1">
     The sentence-level detection results are shown in Table
     <a class="ltx_ref" href="#S3.T4" title="Table 4 ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     . As we can see, HaluAgent achieves much higher F1 score compared to Baichuan2-Chat, especially for those tasks where the detection results can be precisely judged via tools. For example, HaluAgent 13B achieves an accuracy of 87.50% on the math dataset, and F1 score of 95.42% on the scientific literature review dataset, approaching the performance of the GPT-4 pipeline. We attribute this improvement to the fine-grained design of the HaluAgent framework and its capability to store and reflect on detection results for each sentence.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F2">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="129" id="S3.F2.g1" src="/html/2406.11277/assets/x2.png" width="221"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 2:
     </span>
     The usage rate of new tools and the proportion of successful detection.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S3.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="215" id="S3.F3.g1" src="/html/2406.11277/assets/x3.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     Case study between GPT-4 with a simple prompt and single tool, and the HaluAgent framework.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S3.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.4
    </span>
    Further Analysis
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS4.p1">
    <p class="ltx_p" id="S3.SS4.p1.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS4.p1.1.1">
      Extensibility Study.
     </span>
     To verify the extensibility of our HaluAgent framework, we introduce new tools to the fine-tuned models and test their abilities to use these tools for hallucination detection. Specifically, we incorporate a translator and a calendar tool into the HaluAgent toolbox for handling texts related to translation and date calculations. We provide instructions and two examples as in-context demonstrations to guide the model to use these new tools.
For evaluation, we employ ChatGPT to generate 100 samples involving translation and date calculation. Instruction and dataset details can be found in Appendix
     <a class="ltx_ref" href="#A2.SS3" title="B.3 Implementation Details of Scalability Study ‣ Appendix B Experiment Setup ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       B.3
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS4.p2">
    <p class="ltx_p" id="S3.SS4.p2.1">
     We measure the proportion of correct tool usage and successful task completion rate by HaluAgent in Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.3 Sentence-level Detection ‣ 3 Experiments ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     . As we can see, HaluAgent 7B and 13B achieve a usage rate of over 95% for the translator tool, and the usage rate for the calendar tool is 100%. Due to the models’ inherent multilingual capabilities, they sometimes directly assess the accuracy of the translation result instead of invoking the translator tool. With the assistance of the appropriate tools, HaluAgent achieves a success rate of 100% in hallucination detection for date calculation and over 90% success rate for translation task. These experimental results indicate that with instructions and demonstrations, HaluAgent can effectively use new tools to complete hallucination detection tasks without additional fine-tuning.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS4.p3">
    <p class="ltx_p" id="S3.SS4.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">
      Case Study.
     </span>
     To qualitatively demonstrate the effectiveness of HaluAgent, we present a case study about average speed calculation, which involves both commonsense knowledge and mathematical calculations.
We compare HaluAgent with two hallucination detection methods based on GPT-4 with simple prompts and search engine tool, as shown in Figure
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.3 Sentence-level Detection ‣ 3 Experiments ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
As we can see, GPT-4 fails to detect the minor calculation errors (
     <em class="ltx_emph ltx_font_italic" id="S3.SS4.p3.1.2">
      i.e.,
     </em>
     <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.3">
      3 hours + 0.5 hours + 1.5 hours = 4 hours
     </span>
     ) when only provided with a simple prompt of task description.
Meanwhile, GPT-4 with a search engine tool can only verify the formula for average speed in the response but cannot check the correctness of each calculation step. Consequently, both methods yield incorrect detection results.
In contrast, HaluAgent autonomously plans and selects suitable tools for different parts of the text (
     <em class="ltx_emph ltx_font_italic" id="S3.SS4.p3.1.4">
      i.e.,
     </em>
     search engine for knowledge checks and calculator for calculation checks), allowing for accurate verification of the content. This highlights HaluAgent’s capability to effectively verify complex texts with mixed types of hallucinations.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Related Work
  </h2>
  <div class="ltx_para ltx_noindent" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">
     Hallucination Detection.
    </span>
    Hallucination Detection in LLMs is a pivotal concern due to its role in identifying inaccuracies or falsehoods within model responses. Most existing methods for hallucination detection are implemented based on powerful LLMs
    <cite class="ltx_cite ltx_citemacro_cite">
     Luo et al. (
     <a class="ltx_ref" href="#bib.bib21" title="">
      2024
     </a>
     )
    </cite>
    . One category of these methods relies on the internal knowledge and consistency of LLMs to detect hallucinations, by breaking down the detection process
    <cite class="ltx_cite ltx_citemacro_cite">
     Dhuliawala et al. (
     <a class="ltx_ref" href="#bib.bib8" title="">
      2023
     </a>
     )
    </cite>
    or comparing multiple responses to the same query
    <cite class="ltx_cite ltx_citemacro_cite">
     Manakul et al. (
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     )
    </cite>
    .
However, these approaches are inherently limited by the knowledge boundaries of LLMs. Another category of methods leverages external tools for hallucination detection
    <cite class="ltx_cite ltx_citemacro_cite">
     Chern et al. (
     <a class="ltx_ref" href="#bib.bib5" title="">
      2023
     </a>
     ); Wei et al. (
     <a class="ltx_ref" href="#bib.bib30" title="">
      2024
     </a>
     )
    </cite>
    .
While tool utilization can compensate for the knowledge limitations of LLMs, existing methods often require manually designing specific tools tailored to particular text types. In contrast, HaluAgent is equipped with a versatile and expandable toolbox, enabling small language models to autonomously select appropriate tools for fine-grained detection.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    <span class="ltx_text ltx_font_bold" id="S4.p2.1.1">
     Agent Tuning.
    </span>
    Recently, the surprising planning and reasoning capabilities of LLMs have inspired research into their application as agents for specific tasks.
Previous research
    <cite class="ltx_cite ltx_citemacro_cite">
     Yao et al. (
     <a class="ltx_ref" href="#bib.bib34" title="">
      2023
     </a>
     ); Nakano et al. (
     <a class="ltx_ref" href="#bib.bib25" title="">
      2021
     </a>
     ); Singh et al. (
     <a class="ltx_ref" href="#bib.bib28" title="">
      2023
     </a>
     ); Yang et al. (
     <a class="ltx_ref" href="#bib.bib33" title="">
      2023b
     </a>
     )
    </cite>
    has predominantly relied on prompting to use LLMs as agents, but this method demands advanced instruction-following capabilities that open-source LLMs typically do not match with API-based LLMs.
To break this restriction, AgentTuning
    <cite class="ltx_cite ltx_citemacro_cite">
     Zeng et al. (
     <a class="ltx_ref" href="#bib.bib37" title="">
      2023
     </a>
     )
    </cite>
    first fine-tunes open-source LLMs on agent interaction trajectories generated by powerful LLMs. Moreover, Agent-FLAN
    <cite class="ltx_cite ltx_citemacro_cite">
     Chen et al. (
     <a class="ltx_ref" href="#bib.bib3" title="">
      2024
     </a>
     )
    </cite>
    goes a step further by decomposing and redesigning the training corpus.
Besides general agents, KG-Agent
    <cite class="ltx_cite ltx_citemacro_cite">
     Jiang et al. (
     <a class="ltx_ref" href="#bib.bib14" title="">
      2024
     </a>
     )
    </cite>
    fine-tunes LLaMA-7B model to achieve an agent specialized in reasoning over knowledge graphs. Similarly, HaluAgent is fine-tuned on trajectory data of hallucination detection tasks, thereby enhancing the detection capabilities of open-source LLMs.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this work, we proposed an autonomous agent framework, HaluAgent, which is capable of bilingual hallucination detection in Chinese and English. In our approach, we first curated a multi-functional toolbox to extend the LLM’s capability to detect hallucinations. Next, we designed a fine-grained three-stage detection framework along with memory mechanism, including sentence segmentation, tool selection and verification, and reflection.
Then, we leveraged existing datasets to synthesize detection trajectories by employing GPT-4 to execute the detection process following the HaluAgent framework.
Finally, we fine-tuned Baichuan2-Chat 7B and 13B on the synthesized trajectories. The HaluAgent models achieved notable improvements on both in-domain and out-of-domain datasets, with performance comparable to or even higher than GPT-4 without tool enhancements.
Due to its high flexibility and adaptability, HaluAgent can effectively serve as a hallucination detector when human users interact with LLMs in real-world scenarios. In future work, we will extend our method to deal with more types of tools and hallucinations.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="Sx1">
  <h2 class="ltx_title ltx_title_section">
   Limitations
  </h2>
  <div class="ltx_para" id="Sx1.p1">
   <p class="ltx_p" id="Sx1.p1.1">
    Although HaluAgent significantly enhances the performance of small open-source models in hallucination detection tasks, our approach still has some limitations.
First, we use only Baichuan2-Chat as the backbone LLM and do not compare it with other models of comparable parameter size, such as Llama2-7B
    <cite class="ltx_cite ltx_citemacro_cite">
     Touvron et al. (
     <a class="ltx_ref" href="#bib.bib29" title="">
      2023
     </a>
     )
    </cite>
    , Mistral-7B
    <cite class="ltx_cite ltx_citemacro_cite">
     Jiang et al. (
     <a class="ltx_ref" href="#bib.bib13" title="">
      2023
     </a>
     )
    </cite>
    , and Qwen-7B
    <cite class="ltx_cite ltx_citemacro_cite">
     Bai et al. (
     <a class="ltx_ref" href="#bib.bib1" title="">
      2023
     </a>
     )
    </cite>
    .
Second, our work focuses on hallucination detection tasks and does not propose corresponding mitigation strategies.
Third, we focus on detecting hallucinations that contain errors and contradictions, lacking consideration for hallucinations related to identity recognition and ethical issues.
Finally, the training process of HaluAgent uses only correct trajectory data for supervised fine-tuning, without fully leveraging failed trajectory data or incorporating other types of data.
In the future, we will further refine the HaluAgent framework to cover more hallucination types and fully leverage failed data to train the model. We also consider developing improved mitigation strategies based on HaluAgent.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bai et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.16609" target="_blank" title="">
      Qwen technical report
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      CoRR
     </em>
     , abs/2309.16609.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2107.03374" target="_blank" title="">
      Evaluating large language models trained on code
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      CoRR
     </em>
     , abs/2107.03374.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2403.12881" target="_blank" title="">
      Agent-flan: Designing data and methods of effective agent tuning for large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      CoRR
     </em>
     , abs/2403.12881.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cheng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, and Xipeng Qiu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.03368" target="_blank" title="">
      Evaluating hallucinations in chinese large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      CoRR
     </em>
     , abs/2310.03368.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chern et al. (2023)
    </span>
    <span class="ltx_bibblock">
     I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.13528" target="_blank" title="">
      Factool: Factuality detection in generative AI - A tool augmented framework for multi-task and multi-domain scenarios
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      CoRR
     </em>
     , abs/2307.13528.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Cohen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.EMNLP-MAIN.778" target="_blank" title="">
      LM vs LM: detecting factual errors via cross examination
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023
     </em>
     , pages 12621–12640. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dathathri et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=H1edEyBKDS" target="_blank" title="">
      Plug and play language models: A simple approach to controlled text generation
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020
     </em>
     . OpenReview.net.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dhuliawala et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.11495" target="_blank" title="">
      Chain-of-verification reduces hallucination in large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      CoRR
     </em>
     , abs/2309.11495.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gou et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Sx038qxjek" target="_blank" title="">
      CRITIC: Large language models can self-correct with tool-interactive critiquing
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hendrycks et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.
    </span>
    <span class="ltx_bibblock">
     Measuring mathematical problem solving with the math dataset.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.05232" target="_blank" title="">
      A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      CoRR
     </em>
     , abs/2311.05232.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ji et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023.
    </span>
    <span class="ltx_bibblock">
     Survey of hallucination in natural language generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      ACM Computing Surveys
     </em>
     , 55(12):1–38.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Mistral 7b.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      arXiv preprint arXiv:2310.06825
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and Ji-Rong Wen. 2024.
    </span>
    <span class="ltx_bibblock">
     Kg-agent: An efficient autonomous agent framework for complex reasoning over knowledge graph.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      CoRR
     </em>
     , abs/2402.11163.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kaddour et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.10169" target="_blank" title="">
      Challenges and applications of large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      CoRR
     </em>
     , abs/2307.10169.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lan et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.24963/IJCAI.2021/611" target="_blank" title="">
      A survey on complex knowledge base question answering: Methods, challenges and solutions
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021
     </em>
     , pages 4483–4491. ijcai.org.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lei et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily Ching, and Eslam Kamal. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.03951" target="_blank" title="">
      Chain of natural language inference for reducing large language model ungrounded hallucinations
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      CoRR
     </em>
     , abs/2310.03951.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.03205" target="_blank" title="">
      The dawn after the dark: An empirical study on factuality hallucination in large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      CoRR
     </em>
     , abs/2401.03205.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023.
    </span>
    <span class="ltx_bibblock">
     Halueval: A large-scale hallucination evaluation benchmark for large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 6449–6464.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2016)
    </span>
    <span class="ltx_bibblock">
     Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. 2016.
    </span>
    <span class="ltx_bibblock">
     Dataset and neural recurrent sequence labeling model for open-domain factoid question answering.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      arXiv preprint arXiv:1607.06275
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Luo et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Junliang Luo, Tianyu Li, Di Wu, Michael Jenkin, Steve Liu, and Gregory Dudek. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.08358" target="_blank" title="">
      Hallucination detection and hallucination mitigation: An investigation
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      CoRR
     </em>
     , abs/2401.08358.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Manakul et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.EMNLP-MAIN.557" target="_blank" title="">
      Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023
     </em>
     , pages 9004–9017. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Meurer et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondřej Čertík, Sergey B. Kirpichev, Matthew Rocklin, Amit Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Štěpán Roučka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony Scopatz. 2017.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.7717/peerj-cs.103" target="_blank" title="">
      Sympy: symbolic computing in python
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      PeerJ Computer Science
     </em>
     , 3:e103.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Min et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
    </span>
    <span class="ltx_bibblock">
     Factscore: Fine-grained atomic evaluation of factual precision in long form text generation.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
     </em>
     , pages 12076–12100.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakano et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2112.09332" target="_blank" title="">
      Webgpt: Browser-assisted question-answering with human feedback
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      CoRR
     </em>
     , abs/2112.09332.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rawte et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Vipula Rawte, Amit P. Sheth, and Amitava Das. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.05922" target="_blank" title="">
      A survey of hallucination in large foundation models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      CoRR
     </em>
     , abs/2309.05922.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rozière et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2308.12950" target="_blank" title="">
      Code llama: Open foundation models for code
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      CoRR
     </em>
     , abs/2308.12950.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Singh et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.
    </span>
    <span class="ltx_bibblock">
     Progprompt: Generating situated robot task plans using large language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      2023 IEEE International Conference on Robotics and Automation (ICRA)
     </em>
     , pages 11523–11530. IEEE.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open foundation and fine-tuned chat models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      arXiv preprint arXiv:2307.09288
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2403.18802" target="_blank" title="">
      Long-form factuality in large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      CoRR
     </em>
     , abs/2403.18802.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Ziwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.11817" target="_blank" title="">
      Hallucination is inevitable: An innate limitation of large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      CoRR
     </em>
     , abs/2401.11817.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. 2023a.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.10305" target="_blank" title="">
      Baichuan 2: Open large-scale language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      CoRR
     </em>
     , abs/2309.10305.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2303.11381" target="_blank" title="">
      MM-REACT: prompting chatgpt for multimodal reasoning and action
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      CoRR
     </em>
     , abs/2303.11381.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      International Conference on Learning Representations (ICLR)
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ye et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.06794" target="_blank" title="">
      Cognitive mirage: A review of hallucinations in large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      CoRR
     </em>
     , abs/2309.06794.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yehuda et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, and Noam Koenigstein. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2403.02889" target="_blank" title="">
      In search of truth: An interrogation approach to hallucination detection
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      CoRR
     </em>
     , abs/2403.02889.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zeng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.12823" target="_blank" title="">
      Agenttuning: Enabling generalized agent abilities for llms
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      CoRR
     </em>
     , abs/2310.12823.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.01219" target="_blank" title="">
      Siren’s song in the AI ocean: A survey on hallucination in large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      CoRR
     </em>
     , abs/2309.01219.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2303.18223" target="_blank" title="">
      A survey of large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      CoRR
     </em>
     , abs/2303.18223.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming Liu. 2020.
    </span>
    <span class="ltx_bibblock">
     Ape210k: A large-scale and template-rich dataset of math word problems.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      arXiv preprint arXiv:2009.11506
     </em>
     .
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   HaluAgent Framework
  </h2>
  <section class="ltx_subsection" id="A1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.1
    </span>
    Instructions
   </h3>
   <div class="ltx_para" id="A1.SS1.p1">
    <p class="ltx_p" id="A1.SS1.p1.1">
     During the trajectory generation phase, we provide detailed instructions for the HaluAgent framework to prompt GPT-4 to generate the required detection trajectories. The English and Chinese prompts are shown in Figure
     <a class="ltx_ref" href="#A1.F4" title="Figure 4 ‣ A.3 Data Source ‣ Appendix A HaluAgent Framework ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     and Figure
     <a class="ltx_ref" href="#A1.F5" title="Figure 5 ‣ A.3 Data Source ‣ Appendix A HaluAgent Framework ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     , respectively.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.2
    </span>
    Toolbox
   </h3>
   <div class="ltx_para" id="A1.SS2.p1">
    <p class="ltx_p" id="A1.SS2.p1.1">
     HaluAgent includes both verification tools and system tools. The tool names and their usage instructions are summarized in Table
     <a class="ltx_ref" href="#A1.T5" title="Table 5 ‣ A.2 Toolbox ‣ Appendix A HaluAgent Framework ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_table" id="A1.T5">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T5.7" style="width:433.6pt;height:291.7pt;vertical-align:-1.0pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-7.1pt,4.8pt) scale(0.968201962282408,0.968201962282408) ;">
      <table class="ltx_tabular ltx_align_middle" id="A1.T5.7.7">
       <tr class="ltx_tr" id="A1.T5.7.7.8">
        <td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.7.7.8.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_text ltx_font_bold" id="A1.T5.7.7.8.1.1">
          Category
         </span>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T5.7.7.8.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_text ltx_font_bold" id="A1.T5.7.7.8.2.1">
          Tool Name
         </span>
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" id="A1.T5.7.7.8.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.8.3.1">
          <span class="ltx_p" id="A1.T5.7.7.8.3.1.1" style="width:284.5pt;">
           <span class="ltx_text ltx_font_bold" id="A1.T5.7.7.8.3.1.1.1">
            Tool Usage Instruction
           </span>
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.1.1.1">
        <td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.1.2" rowspan="10" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_text" id="A1.T5.1.1.1.2.1">
          <span class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.1.2.1.1">
           <span class="ltx_tr" id="A1.T5.1.1.1.2.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.2.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
             <span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.2.1.1.1.1.1">
              Verification
             </span>
            </span>
           </span>
           <span class="ltx_tr" id="A1.T5.1.1.1.2.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.2.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
             <span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.2.1.1.2.1.1">
              Tools
             </span>
            </span>
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.1.1.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         web_search
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="A1.T5.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.1.1.1.1.1">
          <span class="ltx_p" id="A1.T5.1.1.1.1.1.1" style="width:284.5pt;">
           <span class="ltx_text ltx_font_italic" id="A1.T5.1.1.1.1.1.1.1">
            Input:
           </span>
           sentence: str
           <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.1.1.m1.1">
            <semantics id="A1.T5.1.1.1.1.1.1.m1.1a">
             <mo id="A1.T5.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.1.1.1.1.1.1.m1.1.1.cmml">
              →
             </mo>
             <annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.1.1.m1.1b">
              <ci id="A1.T5.1.1.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.1.1.m1.1.1">
               →
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.1.1.m1.1c">
              \rightarrow
             </annotation>
            </semantics>
           </math>
           <span class="ltx_text ltx_font_italic" id="A1.T5.1.1.1.1.1.1.2">
            Output:
           </span>
           fact
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.7.7.9">
        <td class="ltx_td" id="A1.T5.7.7.9.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="A1.T5.7.7.9.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.9.2.1">
          <span class="ltx_p" id="A1.T5.7.7.9.2.1.1" style="width:284.5pt;">
           Conduct a web search and return factual information.
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.2.2.2">
        <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.2.2.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         calculator
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="A1.T5.2.2.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.2.2.2.1.1">
          <span class="ltx_p" id="A1.T5.2.2.2.1.1.1" style="width:284.5pt;">
           <span class="ltx_text ltx_font_italic" id="A1.T5.2.2.2.1.1.1.1">
            Input:
           </span>
           formula: str
           <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.T5.2.2.2.1.1.1.m1.1">
            <semantics id="A1.T5.2.2.2.1.1.1.m1.1a">
             <mo id="A1.T5.2.2.2.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.2.2.2.1.1.1.m1.1.1.cmml">
              →
             </mo>
             <annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.1.1.1.m1.1b">
              <ci id="A1.T5.2.2.2.1.1.1.m1.1.1.cmml" xref="A1.T5.2.2.2.1.1.1.m1.1.1">
               →
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A1.T5.2.2.2.1.1.1.m1.1c">
              \rightarrow
             </annotation>
            </semantics>
           </math>
           <span class="ltx_text ltx_font_italic" id="A1.T5.2.2.2.1.1.1.2">
            Output:
           </span>
           result
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.7.7.10">
        <td class="ltx_td" id="A1.T5.7.7.10.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="A1.T5.7.7.10.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.10.2.1">
          <span class="ltx_p" id="A1.T5.7.7.10.2.1.1" style="width:284.5pt;">
           Perform calculations based on the input formula and return the result.
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.3.3.3">
        <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.3.3.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         code_interpreter
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="A1.T5.3.3.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.3.3.3.1.1">
          <span class="ltx_p" id="A1.T5.3.3.3.1.1.1" style="width:284.5pt;">
           <span class="ltx_text ltx_font_italic" id="A1.T5.3.3.3.1.1.1.1">
            Input:
           </span>
           code: str
           <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.T5.3.3.3.1.1.1.m1.1">
            <semantics id="A1.T5.3.3.3.1.1.1.m1.1a">
             <mo id="A1.T5.3.3.3.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.3.3.3.1.1.1.m1.1.1.cmml">
              →
             </mo>
             <annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.1.1.1.m1.1b">
              <ci id="A1.T5.3.3.3.1.1.1.m1.1.1.cmml" xref="A1.T5.3.3.3.1.1.1.m1.1.1">
               →
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A1.T5.3.3.3.1.1.1.m1.1c">
              \rightarrow
             </annotation>
            </semantics>
           </math>
           <span class="ltx_text ltx_font_italic" id="A1.T5.3.3.3.1.1.1.2">
            Output:
           </span>
           label
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.7.7.11">
        <td class="ltx_td" id="A1.T5.7.7.11.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="A1.T5.7.7.11.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.11.2.1">
          <span class="ltx_p" id="A1.T5.7.7.11.2.1.1" style="width:284.5pt;">
           Execute code and return a label indicating whether the execution was successful.
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.4.4.4">
        <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.4.4.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         word_count
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="A1.T5.4.4.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.4.4.4.1.1">
          <span class="ltx_p" id="A1.T5.4.4.4.1.1.1" style="width:284.5pt;">
           <span class="ltx_text ltx_font_italic" id="A1.T5.4.4.4.1.1.1.1">
            Input:
           </span>
           length: int, text: str
           <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.T5.4.4.4.1.1.1.m1.1">
            <semantics id="A1.T5.4.4.4.1.1.1.m1.1a">
             <mo id="A1.T5.4.4.4.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.4.4.4.1.1.1.m1.1.1.cmml">
              →
             </mo>
             <annotation-xml encoding="MathML-Content" id="A1.T5.4.4.4.1.1.1.m1.1b">
              <ci id="A1.T5.4.4.4.1.1.1.m1.1.1.cmml" xref="A1.T5.4.4.4.1.1.1.m1.1.1">
               →
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A1.T5.4.4.4.1.1.1.m1.1c">
              \rightarrow
             </annotation>
            </semantics>
           </math>
           <span class="ltx_text ltx_font_italic" id="A1.T5.4.4.4.1.1.1.2">
            Output:
           </span>
           count, label
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.7.7.12">
        <td class="ltx_td" id="A1.T5.7.7.12.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="A1.T5.7.7.12.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.12.2.1">
          <span class="ltx_p" id="A1.T5.7.7.12.2.1.1" style="width:284.5pt;">
           Count words in the text and provide a label indicating whether the requirement is met.
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.5.5.5">
        <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.5.5.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         match
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="A1.T5.5.5.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.5.5.5.1.1">
          <span class="ltx_p" id="A1.T5.5.5.5.1.1.1" style="width:284.5pt;">
           <span class="ltx_text ltx_font_italic" id="A1.T5.5.5.5.1.1.1.1">
            Input:
           </span>
           sentence: str, context: str
           <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.T5.5.5.5.1.1.1.m1.1">
            <semantics id="A1.T5.5.5.5.1.1.1.m1.1a">
             <mo id="A1.T5.5.5.5.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.5.5.5.1.1.1.m1.1.1.cmml">
              →
             </mo>
             <annotation-xml encoding="MathML-Content" id="A1.T5.5.5.5.1.1.1.m1.1b">
              <ci id="A1.T5.5.5.5.1.1.1.m1.1.1.cmml" xref="A1.T5.5.5.5.1.1.1.m1.1.1">
               →
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A1.T5.5.5.5.1.1.1.m1.1c">
              \rightarrow
             </annotation>
            </semantics>
           </math>
           <span class="ltx_text ltx_font_italic" id="A1.T5.5.5.5.1.1.1.2">
            Output:
           </span>
           label
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.7.7.13">
        <td class="ltx_td" id="A1.T5.7.7.13.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="A1.T5.7.7.13.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.13.2.1">
          <span class="ltx_p" id="A1.T5.7.7.13.2.1.1" style="width:284.5pt;">
           Match a sentence with the given context and return a label indicating whether semantic matching is successful.
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.6.6.6">
        <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.6.6.6.2" rowspan="4" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_text" id="A1.T5.6.6.6.2.1">
          <span class="ltx_tabular ltx_align_middle" id="A1.T5.6.6.6.2.1.1">
           <span class="ltx_tr" id="A1.T5.6.6.6.2.1.1.1">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.6.6.6.2.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
             <span class="ltx_text ltx_font_bold" id="A1.T5.6.6.6.2.1.1.1.1.1">
              System
             </span>
            </span>
           </span>
           <span class="ltx_tr" id="A1.T5.6.6.6.2.1.1.2">
            <span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.6.6.6.2.1.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
             <span class="ltx_text ltx_font_bold" id="A1.T5.6.6.6.2.1.1.2.1.1">
              Tools
             </span>
            </span>
           </span>
          </span>
         </span>
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.6.6.6.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         split_text
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="A1.T5.6.6.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.6.6.6.1.1">
          <span class="ltx_p" id="A1.T5.6.6.6.1.1.1" style="width:284.5pt;">
           <span class="ltx_text ltx_font_italic" id="A1.T5.6.6.6.1.1.1.1">
            Input:
           </span>
           text: str
           <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.T5.6.6.6.1.1.1.m1.1">
            <semantics id="A1.T5.6.6.6.1.1.1.m1.1a">
             <mo id="A1.T5.6.6.6.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.6.6.6.1.1.1.m1.1.1.cmml">
              →
             </mo>
             <annotation-xml encoding="MathML-Content" id="A1.T5.6.6.6.1.1.1.m1.1b">
              <ci id="A1.T5.6.6.6.1.1.1.m1.1.1.cmml" xref="A1.T5.6.6.6.1.1.1.m1.1.1">
               →
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A1.T5.6.6.6.1.1.1.m1.1c">
              \rightarrow
             </annotation>
            </semantics>
           </math>
           <span class="ltx_text ltx_font_italic" id="A1.T5.6.6.6.1.1.1.2">
            Output:
           </span>
           sentences
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.7.7.14">
        <td class="ltx_td" id="A1.T5.7.7.14.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="A1.T5.7.7.14.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.14.2.1">
          <span class="ltx_p" id="A1.T5.7.7.14.2.1.1" style="width:284.5pt;">
           Split text into individual sentences.
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.7.7.7">
        <td class="ltx_td ltx_align_left ltx_border_t" id="A1.T5.7.7.7.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         get_answer
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="A1.T5.7.7.7.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.7.1.1">
          <span class="ltx_p" id="A1.T5.7.7.7.1.1.1" style="width:284.5pt;">
           <span class="ltx_text ltx_font_italic" id="A1.T5.7.7.7.1.1.1.1">
            Input:
           </span>
           <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A1.T5.7.7.7.1.1.1.m1.1">
            <semantics id="A1.T5.7.7.7.1.1.1.m1.1a">
             <mo id="A1.T5.7.7.7.1.1.1.m1.1.1" stretchy="false" xref="A1.T5.7.7.7.1.1.1.m1.1.1.cmml">
              →
             </mo>
             <annotation-xml encoding="MathML-Content" id="A1.T5.7.7.7.1.1.1.m1.1b">
              <ci id="A1.T5.7.7.7.1.1.1.m1.1.1.cmml" xref="A1.T5.7.7.7.1.1.1.m1.1.1">
               →
              </ci>
             </annotation-xml>
             <annotation encoding="application/x-tex" id="A1.T5.7.7.7.1.1.1.m1.1c">
              \rightarrow
             </annotation>
            </semantics>
           </math>
           <span class="ltx_text ltx_font_italic" id="A1.T5.7.7.7.1.1.1.2">
            Output:
           </span>
           result(, evidence)
          </span>
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="A1.T5.7.7.15">
        <td class="ltx_td ltx_border_bb" id="A1.T5.7.7.15.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
        </td>
        <td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="A1.T5.7.7.15.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
         <span class="ltx_inline-block ltx_align_top" id="A1.T5.7.7.15.2.1">
          <span class="ltx_p" id="A1.T5.7.7.15.2.1.1" style="width:284.5pt;">
           Return the detection answer with optional evidence.
          </span>
         </span>
        </td>
       </tr>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 5:
     </span>
     Instructions of the toolbox in HaluAgent.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="A1.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.3
    </span>
    Data Source
   </h3>
   <div class="ltx_para" id="A1.SS3.p1">
    <p class="ltx_p" id="A1.SS3.p1.1">
     We introduce the data sources for trajectory generation here, which include five datasets: HaluEval, WebQA, Ape210K, HumanEval, and WordCnt.
    </p>
   </div>
   <div class="ltx_para" id="A1.SS3.p2">
    <p class="ltx_p" id="A1.SS3.p2.1">
     •
     <span class="ltx_text ltx_font_bold" id="A1.SS3.p2.1.1">
      HaluEval
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
      )
     </cite>
     is a benchmark for evaluating hallucinations in LLMs across various tasks.
We select 1,000 samples from the QA subset, with 900 samples used for trajectory generation and 100 samples for testing.
    </p>
   </div>
   <div class="ltx_para" id="A1.SS3.p3">
    <p class="ltx_p" id="A1.SS3.p3.1">
     •
     <span class="ltx_text ltx_font_bold" id="A1.SS3.p3.1.1">
      WebQA
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib20" title="">
       2016
      </a>
      )
     </cite>
     is a QA dataset collected from the Baidu Zhidao platform.
This dataset contains a large number of questions and corresponding answers.
We sample 1,000 questions from the training set of WebQA and generate answers for each question using ChatGPT. Then, human annotators evaluate these answers by comparing them to the correct ones to determine if they contain hallucinations.
We use 900 questions for generating detection trajectories and reserve 100 samples as the testset.
    </p>
   </div>
   <div class="ltx_para" id="A1.SS3.p4">
    <p class="ltx_p" id="A1.SS3.p4.1">
     •
     <span class="ltx_text ltx_font_bold" id="A1.SS3.p4.1.1">
      Ape210K
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Zhao et al. (
      <a class="ltx_ref" href="#bib.bib40" title="">
       2020
      </a>
      )
     </cite>
     is a large-scale math word problem dataset, containing 210,000 Chinese primary school-level math problems.
Each problem includes a gold answer and the equations needed to derive the answer. Solving Ape210K requires not only natural language understanding but also commonsense knowledge.
We select 700 data samples from the training set of Ape210K for mathematical calculation. As with the WebQA processing method, we first use ChatGPT to generate answers for each question, and then human annotators label the responses.
We reserve 100 data samples as the testset, and use the remaining 600 samples for trajectory generation.
    </p>
   </div>
   <div class="ltx_para" id="A1.SS3.p5">
    <p class="ltx_p" id="A1.SS3.p5.1">
     •
     <span class="ltx_text ltx_font_bold" id="A1.SS3.p5.1.1">
      HumanEval
     </span>
     <cite class="ltx_cite ltx_citemacro_cite">
      Chen et al. (
      <a class="ltx_ref" href="#bib.bib2" title="">
       2021
      </a>
      )
     </cite>
     dataset contains 164 programming problems, including function names, comments, specific implementations, and multiple unit tests. It is widely used to test the code generation capabilities of LLMs. We use CodeLlama-34b-Instruct-hf
     <cite class="ltx_cite ltx_citemacro_cite">
      Rozière et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2023
      </a>
      )
     </cite>
     to generate code for the HumanEval dataset, using 100 samples for trajectory generation and the rest for testing.
    </p>
   </div>
   <div class="ltx_para" id="A1.SS3.p6">
    <p class="ltx_p" id="A1.SS3.p6.1">
     •
     <span class="ltx_text ltx_font_bold" id="A1.SS3.p6.1.1">
      WordCnt
     </span>
     is a newly constructed dataset, representing conditional text generation tasks in scenarios when users interact with LLMs. Specifically, we create WordCnt by prompting GPT-4 to generate a set of text generation instructions with specific length requirements and the corresponding responses for each instruction. WordCnt consists of 200 samples, with 100 samples used for trajectory generation and 100 samples for testing.
    </p>
   </div>
   <figure class="ltx_figure" id="A1.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="684" id="A1.F4.g1" src="/html/2406.11277/assets/x4.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     Instructions of HaluAgent framework in English.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A1.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="704" id="A1.F5.g1" src="/html/2406.11277/assets/x5.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     Instructions of HaluAgent framework in Chinese.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Experiment Setup
  </h2>
  <section class="ltx_subsection" id="A2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.1
    </span>
    Out-of-domain Datasets
   </h3>
   <div class="ltx_para" id="A2.SS1.p1">
    <p class="ltx_p" id="A2.SS1.p1.1">
     We present the details of the out-of-domain datasets here.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p2">
    <p class="ltx_p" id="A2.SS1.p2.1">
     •
     <span class="ltx_text ltx_font_bold" id="A2.SS1.p2.1.1">
      HalluQA
     </span>
     is a Chinese Hallucination Question-Answering benchmark covering misleading questions like identity awareness and knowledge-based questions. Each question includes one correct answer and several answers with hallucinations. We conduct experiments on 206 knowledge-related samples from the dataset, randomly selecting one answer from the correct answer and hallucinated answers for evaluation.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p3">
    <p class="ltx_p" id="A2.SS1.p3.1">
     •
     <span class="ltx_text ltx_font_bold" id="A2.SS1.p3.1.1">
      HaluEval 2.0
     </span>
     is a hallucination evaluation benchmark that contains large-scale questions from five domains: biomedicine, finance, science, education, and open domain. We evenly sample 100 of HaluEval 2.0 for hallucination detection evaluation.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.2
    </span>
    Baselines
   </h3>
   <div class="ltx_para" id="A2.SS2.p1">
    <p class="ltx_p" id="A2.SS2.p1.1">
     In the experiment section, we compare the hallucination detection performance of GPT-4, Baichuan2-Chat, and HaluAgent. The detailed baseline settings are explained below.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS2.p2">
    <p class="ltx_p" id="A2.SS2.p2.1">
     •
     <span class="ltx_text ltx_font_bold" id="A2.SS2.p2.1.1">
      GPT-4 prompt
     </span>
     involves providing GPT-4 with a simple description of the hallucination detection task, enabling the model to determine whether there are hallucinations in the text. The model’s response is either “Yes” or “No”, indicating the presence or absence of hallucinations. The detailed prompt is shown in Figure
     <a class="ltx_ref" href="#A2.F6" title="Figure 6 ‣ B.2 Baselines ‣ Appendix B Experiment Setup ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="A2.SS2.p3">
    <p class="ltx_p" id="A2.SS2.p3.1">
     •
     <span class="ltx_text ltx_font_bold" id="A2.SS2.p3.1.1">
      GPT-4 pipeline
     </span>
     guides GPT-4 through the hallucination detection process following the HaluAgent framework, which includes steps such as sentence segmentation and tool invocation. In addition to providing a “Yes” or “No” answer, it identifies the location of the hallucinations and provides supporting evidence. The instructions are shown in Figure
     <a class="ltx_ref" href="#A1.F4" title="Figure 4 ‣ A.3 Data Source ‣ Appendix A HaluAgent Framework ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     and Figure
     <a class="ltx_ref" href="#A1.F5" title="Figure 5 ‣ A.3 Data Source ‣ Appendix A HaluAgent Framework ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="A2.SS2.p4">
    <p class="ltx_p" id="A2.SS2.p4.1">
     •
     <span class="ltx_text ltx_font_bold" id="A2.SS2.p4.1.1">
      Baichuan2-Chat (7B and 13B)
     </span>
     do not have the capability to follow the HaluAgent detection framework. Therefore, we evaluate these models using the same simple hallucination detection prompt as used for GPT-4.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS2.p5">
    <p class="ltx_p" id="A2.SS2.p5.1">
     •
     <span class="ltx_text ltx_font_bold" id="A2.SS2.p5.1.1">
      HaluAgent (7B and 13B)
     </span>
     are models fine-tuned with trajectory data. We evaluate them using HaluAgent instructions in a zero-shot setting, similar to the evaluation of the GPT-4 pipeline.
    </p>
   </div>
   <figure class="ltx_figure" id="A2.F6">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="119" id="A2.F6.g1" src="/html/2406.11277/assets/x6.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     Simple description of the hallucination detection task in English and Chinese.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="A2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.3
    </span>
    Implementation Details of Scalability Study
   </h3>
   <section class="ltx_subsubsection" id="A2.SS3.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      B.3.1
     </span>
     Constructed Dataset
    </h4>
    <div class="ltx_para" id="A2.SS3.SSS1.p1">
     <p class="ltx_p" id="A2.SS3.SSS1.p1.1">
      We construct a new dataset for translation and data calculation tasks via ChatGPT by providing examples. Our goal is to create a dataset specifically for evaluating these new tools, so the questions in the dataset are straightforward. We present some examples below:
     </p>
     <p class="ltx_p ltx_align_center" id="A2.SS3.SSS1.p1.2">
      •
      <span class="ltx_text ltx_font_italic" id="A2.SS3.SSS1.p1.2.1">
       Translate the following Spanish into Chinese: ¡Hola! ¿Cómo estás?
      </span>
     </p>
     <p class="ltx_p ltx_align_center" id="A2.SS3.SSS1.p1.3">
      •
      <span class="ltx_text ltx_font_italic" id="A2.SS3.SSS1.p1.3.1">
       How many days are there from 2014-02-06 to 2014-05-21?
      </span>
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="A2.SS3.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      B.3.2
     </span>
     Instructions of New Tools
    </h4>
    <div class="ltx_para" id="A2.SS3.SSS2.p1">
     <p class="ltx_p" id="A2.SS3.SSS2.p1.1">
      To guide HaluAgent in using the new tools, we include descriptions and usage examples of these tools in the instructions. The detailed prompt is shown in Figure
      <a class="ltx_ref" href="#A2.F7" title="Figure 7 ‣ B.3.2 Instructions of New Tools ‣ B.3 Implementation Details of Scalability Study ‣ Appendix B Experiment Setup ‣ Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector">
       <span class="ltx_text ltx_ref_tag">
        7
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="A2.F7">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="396" id="A2.F7.g1" src="/html/2406.11277/assets/x7.png" width="415"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 7:
      </span>
      Description and usage example of new tools.
     </figcaption>
    </figure>
    <div class="ltx_pagination ltx_role_newpage">
    </div>
   </section>
  </section>
 </section>
</article>
