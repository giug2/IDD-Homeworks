<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.12389] Track Boosting and Synthetic Data Aided Drone Detection</title><meta property="og:description" content="This is the paper for the first place winning solution of the Drone vs. Bird Challenge, organized by AVSS 2021. As the usage of drones increases with lowered costs and improved drone technology, drone detection emerges…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Track Boosting and Synthetic Data Aided Drone Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Track Boosting and Synthetic Data Aided Drone Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.12389">

<!--Generated on Sat Mar  2 02:38:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\UseRawInputEncoding</span>
</div>
<h1 class="ltx_title ltx_title_document">Track Boosting and Synthetic Data Aided Drone Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fatih Cagatay Akyon, Ogulcan Eryuksel, Kamil Anil Ozfuttu, Sinan Onur Altinuc 
<br class="ltx_break">OBSS AI
<br class="ltx_break">OBSS Technology, Ankara, Turkey 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{fatih.akyon, ogulcan.eryuksel, anil.ozfuttu, sinan.altinuc}@obss.com.tr</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">This is the paper for the first place winning solution of the Drone vs. Bird Challenge, organized by AVSS 2021. As the usage of drones increases with lowered costs and improved drone technology, drone detection emerges as a vital object detection task. However, detecting distant drones under unfavorable conditions, namely weak contrast, long-range, low visibility, requires effective algorithms. Our method approaches the drone detection problem by fine-tuning a YOLOv5 model with real and synthetically generated data using a Kalman-based object tracker to boost detection confidence. Our results indicate that augmenting the real data with an optimal subset of synthetic data can increase the performance. Moreover, temporal information gathered by object tracking methods can increase performance further.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><span id="footnotex1.1" class="ltx_text" style="font-size:80%;">978-1-6654-3396-9/21/$31.00 ©2021 IEEE </span>
</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Initially being used for military applications, the use of drones has been extended to multiple application fields, including traffic and weather monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, smart agriculture monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and many more <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Furthermore, with the COVID-19 pandemic, there has been a radical increase in the use of drones not only for autonomous delivery of essential grocery and medical supplies but also to enforce social distancing. Nowadays, small quadcopters can be easily purchased on the Internet at low prices, which brings unprecedented opportunities but also poses several threats in terms of safety, privacy, and security <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The Drone vs. Bird Detection Challenge was launched in 2017, during the first edition of the International Workshop on Small-Drone Surveillance, Detection and Counteraction Techniques (WOSDETC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> as part of the 14th edition of the IEEE International Conference on Advanced Video and Signal based Surveillance (AVSS). This challenge aims to address the technical issues of discriminating between drones and birds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Given their characteristics, in fact, drones can be easily confused with birds, particularly at long distances, which makes the surveillance task even more challenging. The use of video analytics can solve the issue, but effective algorithms are needed that can operate under unfavorable conditions, namely weak contrast, long-range, low visibility, etc.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To overcome these issues, firstly, we use synthetic data selectively to enrich the dataset. Secondly, we make use of the Kalman filter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> based object tracking method to track objects across time to eliminate false positives and enhance detection performance. Lastly, we propose a track boosting method for boosting the confidence scores of detections based on track statistics.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In recent years, the application of deep learning-based detection methods has led to excellent results for a wide range of applications, including drone detection. However, due to the absence of large amounts of drone detection datasets, a two-staged detection strategy has been proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. First, the authors examined the suitability of different flying object detection techniques, i.e., frame differencing and background subtraction techniques, locally adaptive change detection, and object proposal techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, to extract region candidates in video data from static and moving cameras. In the second stage, a small CNN classification network is applied to distinguish each candidate region into drone and clutter categories.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, Gagné and Mercier (referred to as Alexis team) proposed a drone detection approach based on YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and taking a single RGB frame as input. By integrating an image tiling strategy, this approach is able to detect small drones in high-resolution images successfully. Alexis Team leveraged the public PyTorch implementation of YOLOv3 with Spatial Pyramid Pooling (YOLOv3-SPP) made available by Ultralytics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Spatial Pyramid Pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is a simple technique for which the input features are processed by pooling layers of different sizes in parallel and then concatenated to generate fixed-length feature vectors. Moreover, EagleDrone Team proposed a YOLOv5 based drone detection modality with a linear sampling-based data sub-sampling method. They propose to set the sampling probabilities using calculated loss per image. In addition to that, they utilize an ESRGAN based super-resolution technique to detect small and low-resolution drones.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Technique</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Detection Model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The proposed technique focuses on a combination of two methods to improve the accuracy of drone detection performance using YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> object detection method. YOLOv5 is selected because of its speed and performance on object detection tasks. In addition, it supports anchor optimization, which is proven to improve performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and feature pyramids <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> that handle objects at different scales.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Synthetic Data</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2111.12389/assets/synthetic_drones.jpg" id="S3.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="155" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Samples from synthetically generated drones images.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The use of synthetic data in deep learning appears helpful in scenarios where data is scarce or unavailable. Although synthetic data alone cannot show the same performance as real data, it has been seen that it increases performance when used alongside real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Since there is no general method for creating synthetic data, each problem requires a unique approach. For the drone tracking problem, a method for creating labeled, randomized compositions by positioning 3D drone objects in front of 2D backgrounds were designed. This method was chosen because it is challenging to create a 3D randomized environment for the drone detection problem, and a location-independent object such as a drone can be used appropriately with 2D backgrounds. To generate the dataset, 3D drone models were rendered with various conditions such as position, rotation and lighting, and post-process effects on the randomized background images. Some samples from the synthetically generated dataset can be seen in Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.2 Synthetic Data ‣ 3 Proposed Technique ‣ Track Boosting and Synthetic Data Aided Drone Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The most complex challenge encountered in the method is developing a solution to bridge the so-called ”domain gap”. To achieve this, experiments were conducted on the properties of the generated synthetic data, and the properties that make it similar to real data were investigated. In line with these studies, we created datasets with varying sample sizes that were optimized based on the features discovered. Finally, a series of experiments were conducted; first, a dataset was used with mixing synthetic and real data; second, a model trained on a synthetic dataset was used as a backbone for real data training.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">Data</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Technique</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP-s</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP-m</th>
<th id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">mAP-l</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Combined Dataset</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">YOLOv5 + Tracker + Track Boosting</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.3.1" class="ltx_text ltx_font_bold">79.4</span></td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.4.1" class="ltx_text ltx_font_bold">86.2</span></td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.5.1" class="ltx_text ltx_font_bold">72.7</span></td>
<td id="S3.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.6.1" class="ltx_text ltx_font_bold">70.3</span></td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Real Dataset</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center">YOLOv5 + Tracker + Track Boosting</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center">76.1</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center">86.6</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center">67.6</td>
<td id="S3.T1.1.3.2.6" class="ltx_td ltx_align_center">43.0</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic Dataset</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center">YOLOv5 + Tracker + Track Boosting</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center">60.9</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center">69.3</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center">58.3</td>
<td id="S3.T1.1.4.3.6" class="ltx_td ltx_align_center">41.3</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Combined Dataset</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">YOLOv5 + Tracker</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">78.8</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">85.7</td>
<td id="S3.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">71.6</td>
<td id="S3.T1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t">65.3</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Real Dataset</th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center">YOLOv5 + Tracker</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center">74.6</td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center">86.2</td>
<td id="S3.T1.1.6.5.5" class="ltx_td ltx_align_center">66.9</td>
<td id="S3.T1.1.6.5.6" class="ltx_td ltx_align_center">33.0</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic Dataset</th>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center">YOLOv5 + Tracker</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center">56.6</td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center">65.4</td>
<td id="S3.T1.1.7.6.5" class="ltx_td ltx_align_center">55.7</td>
<td id="S3.T1.1.7.6.6" class="ltx_td ltx_align_center">36.0</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Combined</th>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">YOLOv5</td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">78.1</td>
<td id="S3.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">84.8</td>
<td id="S3.T1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">72.0</td>
<td id="S3.T1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t">65.2</td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<th id="S3.T1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Real Dataset</th>
<td id="S3.T1.1.9.8.2" class="ltx_td ltx_align_center">YOLOv5</td>
<td id="S3.T1.1.9.8.3" class="ltx_td ltx_align_center">74.6</td>
<td id="S3.T1.1.9.8.4" class="ltx_td ltx_align_center">86.0</td>
<td id="S3.T1.1.9.8.5" class="ltx_td ltx_align_center">67.1</td>
<td id="S3.T1.1.9.8.6" class="ltx_td ltx_align_center">33.4</td>
</tr>
<tr id="S3.T1.1.10.9" class="ltx_tr">
<th id="S3.T1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic Dataset</th>
<td id="S3.T1.1.10.9.2" class="ltx_td ltx_align_center">YOLOv5</td>
<td id="S3.T1.1.10.9.3" class="ltx_td ltx_align_center">55.6</td>
<td id="S3.T1.1.10.9.4" class="ltx_td ltx_align_center">63.0</td>
<td id="S3.T1.1.10.9.5" class="ltx_td ltx_align_center">55.6</td>
<td id="S3.T1.1.10.9.6" class="ltx_td ltx_align_center">35.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Fine-tuning results for synthetic data augmentation and track boosting technique. In ‘Data‘ column, ‘Combined‘ means synthetic samples are used in training together with real data. In ‘Technique‘ column, ‘Y‘ means YOLOv5m6 model is used as detector and ‘T‘ means Kalman based tracker is applied on top of model detections. ‘mAP‘ corresponds to mean average precision at 0.50 threshold. mAP-s, mAP-m and mAP-l corresponds to small, medium and large object detection accuracies, respectively.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Object Tracking And Tracker Based Confidence Boosting</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Object tracking algorithms are used to provide continuity of object detections over time. While tracking the objects is not directly required, it provides temporal information about the objects in the video that can further improve performance.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Tracker Method</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">A simple Kalman-based tracking method is applied <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> over the predictions of the object detection network. Kalman-based tracking uses a position and velocity-based process, object detection methods as measurements, and a hit counter mechanism. The tracking parameters are optimized for drone tracking with possibly moving cameras.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p">One benefit of using a tracker system is that for a track to be formed successfully, the object detection model needs to provide a consistent stream of predictions close to the object’s predicted location. Therefore false positives occurring at random positions usually fail to build up the necessary hit count to form a track, as shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.3.1 Tracker Method ‣ 3.3 Object Tracking And Tracker Based Confidence Boosting ‣ 3 Proposed Technique ‣ Track Boosting and Synthetic Data Aided Drone Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This has a positive impact on the mAP score.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2111.12389/assets/false-positives.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Tracker results and false positive detections on a frame. Drone in the frame has a tracking id of 1 that shows tracker tracks the frame. The other red boxes on the clouds are false positives that can be filtered out by the tracker.</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Track Boosting Method</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">The tracks provide a spatiotemporal dimension and continuity over the predictions. This information is used in various ways to improve performance.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">However, tracking can provide object predictions where the object detection method provides no predictions; tracker-only predictions are not very useful compared to detection predictions due to their low IoU rate and test set not having annotations for occluded objects. Furthermore, in conducted experiments, including the tracker predictions had a negative impact on the mAP score. Therefore only detections from the object detection method are used in the Tracker Boosting Method.</p>
</div>
<div id="S3.SS3.SSS2.p3" class="ltx_para">
<p id="S3.SS3.SSS2.p3.5" class="ltx_p">Also, object detection model confidence may vary significantly with moving objects as the object moves or changes orientation over time. With the track information provided by the tracking algorithm, we increased the confidence of the predictions within a track by averaging the max confidence score in the track with the confidence provided by the object detection algorithm as shown in Eq. (<a href="#S3.E1" title="In 3.3.2 Track Boosting Method ‣ 3.3 Object Tracking And Tracker Based Confidence Boosting ‣ 3 Proposed Technique ‣ Track Boosting and Synthetic Data Aided Drone Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) where <math id="S3.SS3.SSS2.p3.1.m1.2" class="ltx_Math" alttext="{S^{\prime}_{i,j}}" display="inline"><semantics id="S3.SS3.SSS2.p3.1.m1.2a"><msubsup id="S3.SS3.SSS2.p3.1.m1.2.3" xref="S3.SS3.SSS2.p3.1.m1.2.3.cmml"><mi id="S3.SS3.SSS2.p3.1.m1.2.3.2.2" xref="S3.SS3.SSS2.p3.1.m1.2.3.2.2.cmml">S</mi><mrow id="S3.SS3.SSS2.p3.1.m1.2.2.2.4" xref="S3.SS3.SSS2.p3.1.m1.2.2.2.3.cmml"><mi id="S3.SS3.SSS2.p3.1.m1.1.1.1.1" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS3.SSS2.p3.1.m1.2.2.2.4.1" xref="S3.SS3.SSS2.p3.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS3.SSS2.p3.1.m1.2.2.2.2" xref="S3.SS3.SSS2.p3.1.m1.2.2.2.2.cmml">j</mi></mrow><mo id="S3.SS3.SSS2.p3.1.m1.2.3.2.3" xref="S3.SS3.SSS2.p3.1.m1.2.3.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p3.1.m1.2b"><apply id="S3.SS3.SSS2.p3.1.m1.2.3.cmml" xref="S3.SS3.SSS2.p3.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p3.1.m1.2.3.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.2.3">subscript</csymbol><apply id="S3.SS3.SSS2.p3.1.m1.2.3.2.cmml" xref="S3.SS3.SSS2.p3.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p3.1.m1.2.3.2.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.2.3">superscript</csymbol><ci id="S3.SS3.SSS2.p3.1.m1.2.3.2.2.cmml" xref="S3.SS3.SSS2.p3.1.m1.2.3.2.2">𝑆</ci><ci id="S3.SS3.SSS2.p3.1.m1.2.3.2.3.cmml" xref="S3.SS3.SSS2.p3.1.m1.2.3.2.3">′</ci></apply><list id="S3.SS3.SSS2.p3.1.m1.2.2.2.3.cmml" xref="S3.SS3.SSS2.p3.1.m1.2.2.2.4"><ci id="S3.SS3.SSS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p3.1.m1.1.1.1.1">𝑖</ci><ci id="S3.SS3.SSS2.p3.1.m1.2.2.2.2.cmml" xref="S3.SS3.SSS2.p3.1.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p3.1.m1.2c">{S^{\prime}_{i,j}}</annotation></semantics></math> is the score for a prediction with <math id="S3.SS3.SSS2.p3.2.m2.1" class="ltx_Math" alttext="{i}" display="inline"><semantics id="S3.SS3.SSS2.p3.2.m2.1a"><mi id="S3.SS3.SSS2.p3.2.m2.1.1" xref="S3.SS3.SSS2.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p3.2.m2.1b"><ci id="S3.SS3.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p3.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p3.2.m2.1c">{i}</annotation></semantics></math> as track number and <math id="S3.SS3.SSS2.p3.3.m3.1" class="ltx_Math" alttext="{j}" display="inline"><semantics id="S3.SS3.SSS2.p3.3.m3.1a"><mi id="S3.SS3.SSS2.p3.3.m3.1.1" xref="S3.SS3.SSS2.p3.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p3.3.m3.1b"><ci id="S3.SS3.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p3.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p3.3.m3.1c">{j}</annotation></semantics></math> as the position in the track and <math id="S3.SS3.SSS2.p3.4.m4.1" class="ltx_Math" alttext="{s_{i}}" display="inline"><semantics id="S3.SS3.SSS2.p3.4.m4.1a"><msub id="S3.SS3.SSS2.p3.4.m4.1.1" xref="S3.SS3.SSS2.p3.4.m4.1.1.cmml"><mi id="S3.SS3.SSS2.p3.4.m4.1.1.2" xref="S3.SS3.SSS2.p3.4.m4.1.1.2.cmml">s</mi><mi id="S3.SS3.SSS2.p3.4.m4.1.1.3" xref="S3.SS3.SSS2.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p3.4.m4.1b"><apply id="S3.SS3.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS3.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p3.4.m4.1.1.1.cmml" xref="S3.SS3.SSS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p3.4.m4.1.1.2.cmml" xref="S3.SS3.SSS2.p3.4.m4.1.1.2">𝑠</ci><ci id="S3.SS3.SSS2.p3.4.m4.1.1.3.cmml" xref="S3.SS3.SSS2.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p3.4.m4.1c">{s_{i}}</annotation></semantics></math> is the vector of scores for track <math id="S3.SS3.SSS2.p3.5.m5.1" class="ltx_Math" alttext="{i}" display="inline"><semantics id="S3.SS3.SSS2.p3.5.m5.1a"><mi id="S3.SS3.SSS2.p3.5.m5.1.1" xref="S3.SS3.SSS2.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p3.5.m5.1b"><ci id="S3.SS3.SSS2.p3.5.m5.1.1.cmml" xref="S3.SS3.SSS2.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p3.5.m5.1c">{i}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.SSS2.p4" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="S^{\prime}_{i,j}={\frac{S_{i,j}+\max(s_{i})}{2}}" display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.7" xref="S3.E1.m1.6.7.cmml"><msubsup id="S3.E1.m1.6.7.2" xref="S3.E1.m1.6.7.2.cmml"><mi id="S3.E1.m1.6.7.2.2.2" xref="S3.E1.m1.6.7.2.2.2.cmml">S</mi><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.2.4.1" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">j</mi></mrow><mo id="S3.E1.m1.6.7.2.2.3" xref="S3.E1.m1.6.7.2.2.3.cmml">′</mo></msubsup><mo id="S3.E1.m1.6.7.1" xref="S3.E1.m1.6.7.1.cmml">=</mo><mfrac id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mrow id="S3.E1.m1.6.6.4" xref="S3.E1.m1.6.6.4.cmml"><msub id="S3.E1.m1.6.6.4.6" xref="S3.E1.m1.6.6.4.6.cmml"><mi id="S3.E1.m1.6.6.4.6.2" xref="S3.E1.m1.6.6.4.6.2.cmml">S</mi><mrow id="S3.E1.m1.4.4.2.2.2.4" xref="S3.E1.m1.4.4.2.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.4.4.2.2.2.4.1" xref="S3.E1.m1.4.4.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.4.4.2.2.2.2" xref="S3.E1.m1.4.4.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.E1.m1.6.6.4.5" xref="S3.E1.m1.6.6.4.5.cmml">+</mo><mrow id="S3.E1.m1.6.6.4.4.1" xref="S3.E1.m1.6.6.4.4.2.cmml"><mi id="S3.E1.m1.5.5.3.3" xref="S3.E1.m1.5.5.3.3.cmml">max</mi><mo id="S3.E1.m1.6.6.4.4.1a" xref="S3.E1.m1.6.6.4.4.2.cmml">⁡</mo><mrow id="S3.E1.m1.6.6.4.4.1.1" xref="S3.E1.m1.6.6.4.4.2.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.4.4.1.1.2" xref="S3.E1.m1.6.6.4.4.2.cmml">(</mo><msub id="S3.E1.m1.6.6.4.4.1.1.1" xref="S3.E1.m1.6.6.4.4.1.1.1.cmml"><mi id="S3.E1.m1.6.6.4.4.1.1.1.2" xref="S3.E1.m1.6.6.4.4.1.1.1.2.cmml">s</mi><mi id="S3.E1.m1.6.6.4.4.1.1.1.3" xref="S3.E1.m1.6.6.4.4.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.6.6.4.4.1.1.3" xref="S3.E1.m1.6.6.4.4.2.cmml">)</mo></mrow></mrow></mrow><mn id="S3.E1.m1.6.6.6" xref="S3.E1.m1.6.6.6.cmml">2</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.7.cmml" xref="S3.E1.m1.6.7"><eq id="S3.E1.m1.6.7.1.cmml" xref="S3.E1.m1.6.7.1"></eq><apply id="S3.E1.m1.6.7.2.cmml" xref="S3.E1.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.7.2.1.cmml" xref="S3.E1.m1.6.7.2">subscript</csymbol><apply id="S3.E1.m1.6.7.2.2.cmml" xref="S3.E1.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.7.2.2.1.cmml" xref="S3.E1.m1.6.7.2">superscript</csymbol><ci id="S3.E1.m1.6.7.2.2.2.cmml" xref="S3.E1.m1.6.7.2.2.2">𝑆</ci><ci id="S3.E1.m1.6.7.2.2.3.cmml" xref="S3.E1.m1.6.7.2.2.3">′</ci></apply><list id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑖</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">𝑗</ci></list></apply><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><divide id="S3.E1.m1.6.6.5.cmml" xref="S3.E1.m1.6.6"></divide><apply id="S3.E1.m1.6.6.4.cmml" xref="S3.E1.m1.6.6.4"><plus id="S3.E1.m1.6.6.4.5.cmml" xref="S3.E1.m1.6.6.4.5"></plus><apply id="S3.E1.m1.6.6.4.6.cmml" xref="S3.E1.m1.6.6.4.6"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.6.1.cmml" xref="S3.E1.m1.6.6.4.6">subscript</csymbol><ci id="S3.E1.m1.6.6.4.6.2.cmml" xref="S3.E1.m1.6.6.4.6.2">𝑆</ci><list id="S3.E1.m1.4.4.2.2.2.3.cmml" xref="S3.E1.m1.4.4.2.2.2.4"><ci id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1">𝑖</ci><ci id="S3.E1.m1.4.4.2.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2">𝑗</ci></list></apply><apply id="S3.E1.m1.6.6.4.4.2.cmml" xref="S3.E1.m1.6.6.4.4.1"><max id="S3.E1.m1.5.5.3.3.cmml" xref="S3.E1.m1.5.5.3.3"></max><apply id="S3.E1.m1.6.6.4.4.1.1.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.4.4.1.1.1.1.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1">subscript</csymbol><ci id="S3.E1.m1.6.6.4.4.1.1.1.2.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.2">𝑠</ci><ci id="S3.E1.m1.6.6.4.4.1.1.1.3.cmml" xref="S3.E1.m1.6.6.4.4.1.1.1.3">𝑖</ci></apply></apply></apply><cn type="integer" id="S3.E1.m1.6.6.6.cmml" xref="S3.E1.m1.6.6.6">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">S^{\prime}_{i,j}={\frac{S_{i,j}+\max(s_{i})}{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2111.12389/assets/conf_increase_v2.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="269" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Plot of the confidence scores of a track. Red lines are the confidence scores of object detection and tracking. Blue lines are the results of the confidence increasing algorithm. In areas marked with orange, object detector failed to find the object. </figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Used Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Before conducting the experiments, we randomly selected 15 of the videos from drone-vs-bird set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> as the validation set. Then extracted the frames from training and validation videos. We observed that using a subset of the training frames instead of the whole set prevents over-fitting resulting in improved accuracy. Therefore, uniformly sampled <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="10^{4}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msup id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">10</mn><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">10</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">10^{4}</annotation></semantics></math> frames from the drone vs. bird training is used as training data in experiments. This dataset will be referred as ”Real Dataset”.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Synthetic datasets with different features were generated using the method mentioned in sec. <a href="#S3.SS2" title="3.2 Synthetic Data ‣ 3 Proposed Technique ‣ Track Boosting and Synthetic Data Aided Drone Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.These datasets with special features are:</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<dl id="S4.I1" class="ltx_description">
<dt id="S4.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix1.1.1.m1.1b"><mo id="S4.I1.ix1.1.1.m1.1.1" xref="S4.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.1.1.m1.1c"><ci id="S4.I1.ix1.1.1.m1.1.1.cmml" xref="S4.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math><span id="S4.I1.ix1.3.3.1" class="ltx_text ltx_font_bold"> Original</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix1.p1" class="ltx_para">
<p id="S4.I1.ix1.p1.1" class="ltx_p">Original rendered image without spesific features</p>
</div>
</dd>
<dt id="S4.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix2.1.1.m1.1b"><mo id="S4.I1.ix2.1.1.m1.1.1" xref="S4.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.1.1.m1.1c"><ci id="S4.I1.ix2.1.1.m1.1.1.cmml" xref="S4.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math><span id="S4.I1.ix2.3.3.1" class="ltx_text ltx_font_bold"> Noise</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix2.p1" class="ltx_para">
<p id="S4.I1.ix2.p1.1" class="ltx_p">Image rendered with film grain noise post processing effect</p>
</div>
</dd>
<dt id="S4.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix3.1.1.m1.1b"><mo id="S4.I1.ix3.1.1.m1.1.1" xref="S4.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix3.1.1.m1.1c"><ci id="S4.I1.ix3.1.1.m1.1.1.cmml" xref="S4.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math><span id="S4.I1.ix3.3.3.1" class="ltx_text ltx_font_bold"> Optimal drone sizes</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix3.p1" class="ltx_para">
<p id="S4.I1.ix3.p1.1" class="ltx_p">Drones sized according to normal distribution</p>
</div>
</dd>
<dt id="S4.I1.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix4.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix4.1.1.m1.1b"><mo id="S4.I1.ix4.1.1.m1.1.1" xref="S4.I1.ix4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix4.1.1.m1.1c"><ci id="S4.I1.ix4.1.1.m1.1.1.cmml" xref="S4.I1.ix4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix4.1.1.m1.1d">\bullet</annotation></semantics></math><span id="S4.I1.ix4.3.3.1" class="ltx_text ltx_font_bold"> Blur</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix4.p1" class="ltx_para">
<p id="S4.I1.ix4.p1.1" class="ltx_p">Rendered image with Gaussian Blur optimized for backgrounds</p>
</div>
</dd>
</dl>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">The dataset generated with <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="10^{4}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><msup id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mn id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">10</mn><mn id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">10</cn><cn type="integer" id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">10^{4}</annotation></semantics></math> images by optimizing these features is called ”Synthetic Dataset”.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.2" class="ltx_p">To perform experiments in which synthetic data will be used alongside real data, we created a combined dataset of <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="10\,500" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><mn id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">10 500</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><cn type="integer" id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">10500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">10\,500</annotation></semantics></math> images which will be referred as ”Combined Dataset”. This dataset includes all <math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="10^{4}" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><msup id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml"><mn id="S4.SS1.p5.2.m2.1.1.2" xref="S4.SS1.p5.2.m2.1.1.2.cmml">10</mn><mn id="S4.SS1.p5.2.m2.1.1.3" xref="S4.SS1.p5.2.m2.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><apply id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.2.m2.1.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p5.2.m2.1.1.2.cmml" xref="S4.SS1.p5.2.m2.1.1.2">10</cn><cn type="integer" id="S4.SS1.p5.2.m2.1.1.3.cmml" xref="S4.SS1.p5.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">10^{4}</annotation></semantics></math> samples from ”Real Dataset” and an optimal sub-sample of 500 images from ’Synthetic Dataset’. The reason to include just a small subset of the synthetic images is to avoid the domain gap mentioned in sec. <a href="#S3.SS2" title="3.2 Synthetic Data ‣ 3 Proposed Technique ‣ Track Boosting and Synthetic Data Aided Drone Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">Moreover, we have also trained models with a combined dataset containing images from mav-vid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and realworld-uav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> drone detection datasets. However, since their distributions differ from the drone-vs-bird dataset, our drone-vs-bird validation accuracy dropped, so we do not mention combined dataset training results in this paper.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Firstly, a series of experiments were performed on various synthetic datasets to find the optimal synthetic features. All experiments were performed with COCO pretrained YOLOv5m6 model of 1333 input image size, 8 batch size, and 10 epochs setup and with datasets described in <a href="#S4.SS1" title="4.1 Used Datasets ‣ 4 Experiments ‣ Track Boosting and Synthetic Data Aided Drone Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. For data sampling, inference and evaluation, our open source vision package SAHI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is utilized. As seen in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.2 Results ‣ 4 Experiments ‣ Track Boosting and Synthetic Data Aided Drone Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, experiments revealed that the most important property is the amount of blur applied to the image. In addition, it was understood that the distribution of drone sizes and noise effect similar to film grain significantly affect the performance. The results of the synthetic data experiments are used to create the optimized synthetic dataset known as ’Synthetic Dataset’. The training results obtained with the ’Synthetic Dataset’ show that with the correct features, a training set consisting of only synthetic images gives us acceptable results considering no real images were required in the process.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2111.12389/assets/synth_chart.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="370" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Evaluation result for different synthetic features</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For the real data (drone-vs-bird) experiments, COCO pretrained YOLOv5m6 model is fine-tuned on drone-vs-bird training split with 1333 input image size, 6 batch size for 10 epochs. During inference, vanilla YOLOv5 detection results are taken as a baseline. Then Kalman filter-based tracker is applied on top of model detections. Norfair package <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is utilized for the Kalman filter implementations with measurement (R) and process (Q) uncertainty parameters of 0.2 and 1, respectively. Lastly, the track boosting technique is applied to the tracker output for further performance improvement.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">As seen in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Synthetic Data ‣ 3 Proposed Technique ‣ Track Boosting and Synthetic Data Aided Drone Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, real data gives better results than synthetic data; however, augmenting real data with synthetically generated data improves the validation results by up to 4.2 AP in all scenarios. Moreover, by applying a Kalman filter-based tracker, base results can be improved by up to 1 AP. More importantly, applying the track boosting method on top of a tracker provides us with an additional 1.5 and 0.6 AP improvement in real and combined dataset experiments, respectively.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Our results show that a YOLOv5 model fine-tuned only on synthetically generated images can achieve acceptable performance on drone detection tasks. Moreover, mixing an optimal subset of synthetic data yielded much better results than using real and synthetic images by themselves. Usage of the tracker improves upon the object detection performance in all cases. This improvement may be a result of filling out missing frames and eliminating the false positives by the tracker’s internal mechanism. These results can be further improved by adjusting the frame predictions using the track information. Using the maximum confidence value in a track as a reference value, the overall mAP score increased. In cases where both the tracking algorithm and the object detection provide a prediction using the prediction from the object detection model results in improved accuracy as well.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
F. C. Akyon, S. O. Altinuc, and A. Temizel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Slicing aided hyper inference and fine-tuning for small object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.06934</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
J. Alori, A. Descoins, B. Ríos, and A. Castro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">tryolabs/norfair: v0.3.1.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tryolabs/norfair" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/tryolabs/norfair</a><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
A. Coluccia, A. Fascista, A. Schumann, L. Sommer, A. Dimou, D. Zarpalas,
M. Méndez, D. De la Iglesia, I. González, J.-P. Mercier, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Drone vs. bird detection: Deep learning algorithms and results from a
grand challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 21(8):2824, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
A. Coluccia, A. Fascista, A. Schumann, L. Sommer, M. Ghenescu, T. Piatrik,
G. De Cubber, M. Nalamati, A. Kapoor, M. Saqib, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Drone-vs-bird detection challenge at ieee avss2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 16th IEEE International Conference on Advanced Video and
Signal Based Surveillance (AVSS)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 1–7. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
M. Elloumi, R. Dhaou, B. Escrig, H. Idoudi, and L. A. Saidane.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Monitoring road traffic with a uav-based system.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE Wireless Communications and Networking Conference
(WCNC)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 1–6. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Spatial pyramid pooling in deep convolutional networks for visual
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">,
37(9):1904–1916, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
T. Humphreys.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Statement on the security threat posed by unmanned aerial systems and
possible countermeasures.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Oversight and Management Efficiency Subcommittee, Homeland
Security Committee, Washington, DC, US House</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
G. Jocher, Y. Kwon, J. Veitch-Michaelis, Marc, G. Bianconi, F. Baltacı,
D. Suess, W. Xinyu, T. M. Shead, T. Havlik, P. Skalski, J. Hu, F. Reveriano,
Falak, and D. Kendall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">ultralytics/yolov3: 43.1map@0.5:0.95 on coco2014.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ultralytics/yolov3" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/ultralytics/yolov3</a><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
G. Jocher, A. Stoken, J. Borovec, NanoCode012, ChristopherSTAN, L. Changyu,
Laughing, tkianai, A. Hogan, lorenzomammana, yxNONG, AlexWang1900,
L. Diaconu, Marc, wanghaoyang0106, ml5ah, Doug, F. Ingham, Frederik, Guilhen,
Hatovix, J. Poznanski, J. Fang, L. Yu, changyu98, M. Wang, N. Gupta,
O. Akhtar, PetrDvoracek, and P. Rai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">ultralytics/yolov5: v3.1 - bug fixes and performance improvements.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ultralytics/yolov5" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/ultralytics/yolov5</a><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">, Oct. 2020.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 2117–2125, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
T. Müller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Robust drone detection for day/night counter-uav with static vis and
swir cameras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Ground/Air Multisensor Interoperability, Integration, and
Networking for Persistent ISR VIII</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, volume 10190, page 1019018.
International Society for Optics and Photonics, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
M.  . Pawełczyk and M. Wojtyra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Real world object detection dataset for quadcopter unmanned aerial
vehicle detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 8:174394–174409, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
J. Redmon and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Yolov3: An incremental improvement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.02767</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
A. Rodriguez-Ramos, J. Rodriguez-Vazquez, C. Sampedro, and P. Campoy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Adaptive inattentional framework for video object detection with
reward-conditional training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 8:124451–124466, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil,
N. S. Othman, A. Khreishah, and M. Guizani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Unmanned aerial vehicles (uavs): A survey on civil applications and
key research challenges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Ieee Access</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 7:48572–48634, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
L. Sommer, A. Schumann, T. Müller, T. Schuchert, and J. Beyerer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Flying object detection for automatic uav recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 14th IEEE International Conference on Advanced Video and
Signal Based Surveillance (AVSS)</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 1–6. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
P. Tokekar, J. Vander Hook, D. Mulla, and V. Isler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Sensor planning for a symbiotic uav and ugv system for precision
agriculture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Robotics</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 32(6):1498–1511, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
G. Welch, G. Bishop, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">An introduction to the kalman filter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">1995.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
J. Wojtanowski, M. Zygmunt, T. Drozd, M. Jakubaszek, M. Życzkowski, and
M. Muzal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Distinguishing drones from birds in a uav searching laser scanner
based on echo depolarization measurement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 21(16):5597, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.12388" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.12389" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.12389">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.12389" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.12390" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 02:38:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
