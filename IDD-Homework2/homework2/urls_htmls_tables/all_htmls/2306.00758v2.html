<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.00758] LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing</title><meta property="og:description" content="VQA methods in  remote sensing (RS) aim to answer natural language questions with respect to an RS image.
Most of the existing methods require a large amount of computational resources, which limits their application iâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.00758">

<!--Generated on Thu Feb 29 02:57:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.3" class="ltx_p"><sup id="p1.3.1" class="ltx_sup">1</sup>Technische UniversitÃ¤t Berlin, Germany
<sup id="p1.3.2" class="ltx_sup">2</sup>Zalando SE, Berlin, Germany
<br class="ltx_break"><sup id="p1.3.3" class="ltx_sup">3</sup>BIFOLD â€“ Berlin Institute for the Foundations of Learning and Data, Germany

</p>
</div>
<h1 class="ltx_title ltx_title_document">LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<span id="id5.id1" class="ltx_ERROR undefined">\Ac</span>
<p id="id6.id2" class="ltx_p">VQA methods in  <span title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">remote sensing</span></span> (<abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>) aim to answer natural language questions with respect to an <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> image.
Most of the existing methods require a large amount of computational resources, which limits their application in operational scenarios in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
To address this issue, in this paper we present an effective  <span title="lightweight transformer-based  in " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">lightweight transformer-based <abbr title="" class="ltx_glossaryref"></abbr> in <abbr title="" class="ltx_glossaryref"></abbr></span></span> (<abbr title="lightweight transformer-based  in " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LiT-4-RSVQA</span></abbr>) architecture for efficient and accurate  <span title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">visual question answering</span></span> (<abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr>) in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
Our architecture consists of:
i) a lightweight text encoder module;
ii) a lightweight image encoder module;
iii) a fusion module;
and iv) a classification module.
The experimental results obtained on a <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> benchmark dataset demonstrate that our proposed <abbr title="lightweight transformer-based  in " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LiT-4-RSVQA</span></abbr> architecture provides accurate <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> results while significantly reducing the computational requirements on the executing hardware. Our code is publicly available at <a target="_blank" href="https://git.tu-berlin.de/rsim/lit4rsvqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://git.tu-berlin.de/rsim/lit4rsvqa</a>.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work</span></span></span>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p2.1.1.1" class="ltx_text ltx_font_upright">â€”â€‰</span></span>
visual question answering, natural language processing, lightweight transformer, remote sensing.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As a result of the increased volume of <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> image archives and the amount of information that can be extracted from them, the development of <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> methods has recently become an important research topic in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
In the task of <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>, the user asks a question to a system in natural language concerning the content of <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> images.
During the last years, several <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> methods have been presented in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
As an example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite> defines the <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> task in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> as a classification problem.
In this work, a ResNet-152 model is employed to extract features from <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> images, while skip-thought vectors are used to extract features from the text data.
Then, the multi-modal features obtained through point-wise multiplication are classified using a  <span title="multi-layer perceptron" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">multi-layer perceptron</span></span> (<abbr title="multi-layer perceptron" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MLP</span></abbr>) as classification head.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>, the first large-scale <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> benchmark dataset for <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> based on low- and high-resolution <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> images and OpenStreetMap data is also introduced.
In the <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> method presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>, the  <span title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">recurrent neural network</span></span> (<abbr title="recurrent neural network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RNN</span></abbr>) based skip-thought vectors model for text feature extraction used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite> is replaced with the attention-based <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>.
For the feature fusion, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite> uses  <span title="multi-modal tucker fusion for " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">multi-modal tucker fusion for <abbr title="" class="ltx_glossaryref"></abbr></span></span> (<abbr title="multi-modal tucker fusion for " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MUTAN</span></abbr>), which enables a richer and more meaningful interaction between features compared to a simple point-wise operation.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>, a multi-modal transformer-based  <span title="Visual  () fusion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Visual<abbr title="" class="ltx_glossaryref"></abbr> fusion</span></span> (<abbr title="Visual  () fusion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VBFusion</span></abbr>) architecture is proposed.
<abbr title="Visual  () fusion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VBFusion</span></abbr> uses a ResNet-152 architecture as a region proposal system and a Visual<abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite> model to learn the joint representation of image and text modalities instead of simply combining modality-specific representations. A similar approach is employed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>, utilizing  <span title="multi-modal " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">multi-modal <abbr title="" class="ltx_glossaryref"></abbr></span></span> (<abbr title="multi-modal " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MMBERT</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>.
This method also uses a ResNet-152 as a feature encoder.
Instead of different regions of the input image, the backbone extracts five input representations at different resolutions and combines image and text features with a shallow <abbr title="multi-modal " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MMBERT</span></abbr> encoder.
A comprehensive review of the current state-of-the-art of image-language models in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">All of the above-mentioned architectures provide good <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> accuracies.
However, they are associated with high computational complexity and  <span title="parameter count" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">parameter count</span></span> (<abbr title="parameter count" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">PC</span></abbr>), and thus have limited capability to be applied for operational <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> applications in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
As an example, <abbr title="Visual  () fusion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VBFusion</span></abbr> has a <abbr title="parameter count" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">PC</span></abbr> of 277 million and requires about 184 billion <span title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">floating point operations</span></span> per forward pass.
To overcome these limitations, in this paper, we investigate the effectiveness of lightweight transformer-based models for <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> problems in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> that require few parameters and are thus associated with low computational requirements.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.00758/assets/figures/LIT4EOVQA.drawio_v7.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="489" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span>Illustration of the <abbr title="lightweight transformer-based  in " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LiT-4-RSVQA</span></abbr> architecture.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Lightweight Transformer-based Models for VQA</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.8" class="ltx_p">Given a triplet <math id="S2.p1.1.m1.3" class="ltx_Math" alttext="(\mathbf{I},\mathbf{Q},\mathbf{A})" display="inline"><semantics id="S2.p1.1.m1.3a"><mrow id="S2.p1.1.m1.3.4.2" xref="S2.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="S2.p1.1.m1.3.4.2.1" xref="S2.p1.1.m1.3.4.1.cmml">(</mo><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">ğˆ</mi><mo id="S2.p1.1.m1.3.4.2.2" xref="S2.p1.1.m1.3.4.1.cmml">,</mo><mi id="S2.p1.1.m1.2.2" xref="S2.p1.1.m1.2.2.cmml">ğ</mi><mo id="S2.p1.1.m1.3.4.2.3" xref="S2.p1.1.m1.3.4.1.cmml">,</mo><mi id="S2.p1.1.m1.3.3" xref="S2.p1.1.m1.3.3.cmml">ğ€</mi><mo stretchy="false" id="S2.p1.1.m1.3.4.2.4" xref="S2.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.3b"><vector id="S2.p1.1.m1.3.4.1.cmml" xref="S2.p1.1.m1.3.4.2"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">ğˆ</ci><ci id="S2.p1.1.m1.2.2.cmml" xref="S2.p1.1.m1.2.2">ğ</ci><ci id="S2.p1.1.m1.3.3.cmml" xref="S2.p1.1.m1.3.3">ğ€</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.3c">(\mathbf{I},\mathbf{Q},\mathbf{A})</annotation></semantics></math> of an image, question, and answer, a <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> system provides the answer <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{A}" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">ğ€</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğ€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\mathbf{A}</annotation></semantics></math> given the image <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathbf{I}</annotation></semantics></math> and the question <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{Q}" display="inline"><semantics id="S2.p1.4.m4.1a"><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">\mathbf{Q}</annotation></semantics></math> as the input.
In line with the <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> literature, we consider <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> as a classification problem, where <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{A}" display="inline"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">ğ€</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğ€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\mathbf{A}</annotation></semantics></math> is one of <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="n_{A}" display="inline"><semantics id="S2.p1.6.m6.1a"><msub id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml"><mi id="S2.p1.6.m6.1.1.2" xref="S2.p1.6.m6.1.1.2.cmml">n</mi><mi id="S2.p1.6.m6.1.1.3" xref="S2.p1.6.m6.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p1.6.m6.1.1.1.cmml" xref="S2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.p1.6.m6.1.1.2.cmml" xref="S2.p1.6.m6.1.1.2">ğ‘›</ci><ci id="S2.p1.6.m6.1.1.3.cmml" xref="S2.p1.6.m6.1.1.3">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">n_{A}</annotation></semantics></math> predefined cases.
In this paper, we study lightweight models for <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
To this end, we introduce the <abbr title="lightweight transformer-based  in " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LiT-4-RSVQA</span></abbr> architecture that consists of four modules:
i) a lightweight encoder module for the text modality;
ii) a lightweight encoder module for the image modality;
iii) a fusion module;
and iv) a classification module (see <a href="#S1.F1" title="In 1 Introduction â€£ LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>).
In detail, the text encoder module is realized as one lightweight transformer (<abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr><math id="S2.p1.7.m7.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p1.7.m7.1a"><msub id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml"><mi id="S2.p1.7.m7.1.1a" xref="S2.p1.7.m7.1.1.cmml"></mi><mtext id="S2.p1.7.m7.1.1.1" xref="S2.p1.7.m7.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><apply id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1"><ci id="S2.p1.7.m7.1.1.1a.cmml" xref="S2.p1.7.m7.1.1.1"><mtext mathsize="70%" id="S2.p1.7.m7.1.1.1.cmml" xref="S2.p1.7.m7.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">{}_{\text{TINY}}</annotation></semantics></math>), while the image encoder module is comprised of one of three lightweight image transformers:
i)Â  <span title="data-efficient vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">data-efficient vision transformer</span></span> (<abbr title="data-efficient vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">Deit Tiny</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>;
ii)Â  <span title="mobile vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">mobile vision transformer</span></span> (<abbr title="mobile vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MobileViT-S</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>;
or iii)Â  <span title="cross-covariance image transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">cross-covariance image transformer</span></span> (<abbr title="cross-covariance image transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">XCiT Nano</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite>.
We select these models based on their proven success from the literature and investigate their effectiveness in <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
To the best of our knowledge, they are introduced for the first time in <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> problems in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
We combine BERT<math id="S2.p1.8.m8.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p1.8.m8.1a"><msub id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml"><mi id="S2.p1.8.m8.1.1a" xref="S2.p1.8.m8.1.1.cmml"></mi><mtext id="S2.p1.8.m8.1.1.1" xref="S2.p1.8.m8.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><apply id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1"><ci id="S2.p1.8.m8.1.1.1a.cmml" xref="S2.p1.8.m8.1.1.1"><mtext mathsize="70%" id="S2.p1.8.m8.1.1.1.cmml" xref="S2.p1.8.m8.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">{}_{\text{TINY}}</annotation></semantics></math> with one of the above-mentioned image encoders, resulting in three <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> model configurations.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In general, text and image transformers are trained using  <span title="multi-head self-attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">multi-head self-attention</span></span> (<abbr title="multi-head self-attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSA</span></abbr>), which is a mechanism that enables the model to focus on different parts (tokens) of an input sequence.
Attention computes scores between each pair of input tokens through matrix multiplications and softmax operations, resulting in weights determining how much attention should be given to each token.
The attention weights are utilized to prioritize relevant parts of the input, enabling the model to focus on the most essential tokens.
The attention operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite> can be defined as:</p>
</div>
<div id="S2.p3" class="ltx_para">
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.7" class="ltx_Math" alttext="\displaystyle\operatorname{Attention}(Q,K,V)=\operatorname{Softmax}\left(\frac{QK^{\text{T}}}{\sqrt{d}}\right)V," display="inline"><semantics id="S2.E1.m1.7a"><mrow id="S2.E1.m1.7.7.1" xref="S2.E1.m1.7.7.1.1.cmml"><mrow id="S2.E1.m1.7.7.1.1" xref="S2.E1.m1.7.7.1.1.cmml"><mrow id="S2.E1.m1.7.7.1.1.2.2" xref="S2.E1.m1.7.7.1.1.2.1.cmml"><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">Attention</mi><mo id="S2.E1.m1.7.7.1.1.2.2a" xref="S2.E1.m1.7.7.1.1.2.1.cmml">â¡</mo><mrow id="S2.E1.m1.7.7.1.1.2.2.1" xref="S2.E1.m1.7.7.1.1.2.1.cmml"><mo stretchy="false" id="S2.E1.m1.7.7.1.1.2.2.1.1" xref="S2.E1.m1.7.7.1.1.2.1.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">Q</mi><mo id="S2.E1.m1.7.7.1.1.2.2.1.2" xref="S2.E1.m1.7.7.1.1.2.1.cmml">,</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">K</mi><mo id="S2.E1.m1.7.7.1.1.2.2.1.3" xref="S2.E1.m1.7.7.1.1.2.1.cmml">,</mo><mi id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">V</mi><mo stretchy="false" id="S2.E1.m1.7.7.1.1.2.2.1.4" xref="S2.E1.m1.7.7.1.1.2.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.1.1.1" xref="S2.E1.m1.7.7.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.7.7.1.1.3" xref="S2.E1.m1.7.7.1.1.3.cmml"><mrow id="S2.E1.m1.7.7.1.1.3.2.2" xref="S2.E1.m1.7.7.1.1.3.2.1.cmml"><mi id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml">Softmax</mi><mo id="S2.E1.m1.7.7.1.1.3.2.2a" xref="S2.E1.m1.7.7.1.1.3.2.1.cmml">â¡</mo><mrow id="S2.E1.m1.7.7.1.1.3.2.2.1" xref="S2.E1.m1.7.7.1.1.3.2.1.cmml"><mo id="S2.E1.m1.7.7.1.1.3.2.2.1.1" xref="S2.E1.m1.7.7.1.1.3.2.1.cmml">(</mo><mstyle displaystyle="true" id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml"><mfrac id="S2.E1.m1.6.6a" xref="S2.E1.m1.6.6.cmml"><mrow id="S2.E1.m1.6.6.2" xref="S2.E1.m1.6.6.2.cmml"><mi id="S2.E1.m1.6.6.2.2" xref="S2.E1.m1.6.6.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.2.1" xref="S2.E1.m1.6.6.2.1.cmml">â€‹</mo><msup id="S2.E1.m1.6.6.2.3" xref="S2.E1.m1.6.6.2.3.cmml"><mi id="S2.E1.m1.6.6.2.3.2" xref="S2.E1.m1.6.6.2.3.2.cmml">K</mi><mtext id="S2.E1.m1.6.6.2.3.3" xref="S2.E1.m1.6.6.2.3.3a.cmml">T</mtext></msup></mrow><msqrt id="S2.E1.m1.6.6.3" xref="S2.E1.m1.6.6.3.cmml"><mi id="S2.E1.m1.6.6.3.2" xref="S2.E1.m1.6.6.3.2.cmml">d</mi></msqrt></mfrac></mstyle><mo id="S2.E1.m1.7.7.1.1.3.2.2.1.2" xref="S2.E1.m1.7.7.1.1.3.2.1.cmml">)</mo></mrow></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.1.1.3.1" xref="S2.E1.m1.7.7.1.1.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.7.7.1.1.3.3" xref="S2.E1.m1.7.7.1.1.3.3.cmml">V</mi></mrow></mrow><mo id="S2.E1.m1.7.7.1.2" xref="S2.E1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.7b"><apply id="S2.E1.m1.7.7.1.1.cmml" xref="S2.E1.m1.7.7.1"><eq id="S2.E1.m1.7.7.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1"></eq><apply id="S2.E1.m1.7.7.1.1.2.1.cmml" xref="S2.E1.m1.7.7.1.1.2.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">Attention</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ‘„</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">ğ¾</ci><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">ğ‘‰</ci></apply><apply id="S2.E1.m1.7.7.1.1.3.cmml" xref="S2.E1.m1.7.7.1.1.3"><times id="S2.E1.m1.7.7.1.1.3.1.cmml" xref="S2.E1.m1.7.7.1.1.3.1"></times><apply id="S2.E1.m1.7.7.1.1.3.2.1.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2"><ci id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5">Softmax</ci><apply id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6"><divide id="S2.E1.m1.6.6.1.cmml" xref="S2.E1.m1.6.6"></divide><apply id="S2.E1.m1.6.6.2.cmml" xref="S2.E1.m1.6.6.2"><times id="S2.E1.m1.6.6.2.1.cmml" xref="S2.E1.m1.6.6.2.1"></times><ci id="S2.E1.m1.6.6.2.2.cmml" xref="S2.E1.m1.6.6.2.2">ğ‘„</ci><apply id="S2.E1.m1.6.6.2.3.cmml" xref="S2.E1.m1.6.6.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.2.3.1.cmml" xref="S2.E1.m1.6.6.2.3">superscript</csymbol><ci id="S2.E1.m1.6.6.2.3.2.cmml" xref="S2.E1.m1.6.6.2.3.2">ğ¾</ci><ci id="S2.E1.m1.6.6.2.3.3a.cmml" xref="S2.E1.m1.6.6.2.3.3"><mtext mathsize="70%" id="S2.E1.m1.6.6.2.3.3.cmml" xref="S2.E1.m1.6.6.2.3.3">T</mtext></ci></apply></apply><apply id="S2.E1.m1.6.6.3.cmml" xref="S2.E1.m1.6.6.3"><root id="S2.E1.m1.6.6.3a.cmml" xref="S2.E1.m1.6.6.3"></root><ci id="S2.E1.m1.6.6.3.2.cmml" xref="S2.E1.m1.6.6.3.2">ğ‘‘</ci></apply></apply></apply><ci id="S2.E1.m1.7.7.1.1.3.3.cmml" xref="S2.E1.m1.7.7.1.1.3.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.7c">\displaystyle\operatorname{Attention}(Q,K,V)=\operatorname{Softmax}\left(\frac{QK^{\text{T}}}{\sqrt{d}}\right)V,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p3.15" class="ltx_p">where <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">Q</annotation></semantics></math>, <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.p3.2.m2.1a"><mi id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">K</annotation></semantics></math>, and <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">V</annotation></semantics></math> are query, key, and value, respectively.
For self-attention, they are defined as <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="Q=XW_{Q}" display="inline"><semantics id="S2.p3.4.m4.1a"><mrow id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml"><mi id="S2.p3.4.m4.1.1.2" xref="S2.p3.4.m4.1.1.2.cmml">Q</mi><mo id="S2.p3.4.m4.1.1.1" xref="S2.p3.4.m4.1.1.1.cmml">=</mo><mrow id="S2.p3.4.m4.1.1.3" xref="S2.p3.4.m4.1.1.3.cmml"><mi id="S2.p3.4.m4.1.1.3.2" xref="S2.p3.4.m4.1.1.3.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S2.p3.4.m4.1.1.3.1" xref="S2.p3.4.m4.1.1.3.1.cmml">â€‹</mo><msub id="S2.p3.4.m4.1.1.3.3" xref="S2.p3.4.m4.1.1.3.3.cmml"><mi id="S2.p3.4.m4.1.1.3.3.2" xref="S2.p3.4.m4.1.1.3.3.2.cmml">W</mi><mi id="S2.p3.4.m4.1.1.3.3.3" xref="S2.p3.4.m4.1.1.3.3.3.cmml">Q</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"><eq id="S2.p3.4.m4.1.1.1.cmml" xref="S2.p3.4.m4.1.1.1"></eq><ci id="S2.p3.4.m4.1.1.2.cmml" xref="S2.p3.4.m4.1.1.2">ğ‘„</ci><apply id="S2.p3.4.m4.1.1.3.cmml" xref="S2.p3.4.m4.1.1.3"><times id="S2.p3.4.m4.1.1.3.1.cmml" xref="S2.p3.4.m4.1.1.3.1"></times><ci id="S2.p3.4.m4.1.1.3.2.cmml" xref="S2.p3.4.m4.1.1.3.2">ğ‘‹</ci><apply id="S2.p3.4.m4.1.1.3.3.cmml" xref="S2.p3.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.p3.4.m4.1.1.3.3.1.cmml" xref="S2.p3.4.m4.1.1.3.3">subscript</csymbol><ci id="S2.p3.4.m4.1.1.3.3.2.cmml" xref="S2.p3.4.m4.1.1.3.3.2">ğ‘Š</ci><ci id="S2.p3.4.m4.1.1.3.3.3.cmml" xref="S2.p3.4.m4.1.1.3.3.3">ğ‘„</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">Q=XW_{Q}</annotation></semantics></math>, <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="K=XW_{K}" display="inline"><semantics id="S2.p3.5.m5.1a"><mrow id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><mi id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml">K</mi><mo id="S2.p3.5.m5.1.1.1" xref="S2.p3.5.m5.1.1.1.cmml">=</mo><mrow id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml"><mi id="S2.p3.5.m5.1.1.3.2" xref="S2.p3.5.m5.1.1.3.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S2.p3.5.m5.1.1.3.1" xref="S2.p3.5.m5.1.1.3.1.cmml">â€‹</mo><msub id="S2.p3.5.m5.1.1.3.3" xref="S2.p3.5.m5.1.1.3.3.cmml"><mi id="S2.p3.5.m5.1.1.3.3.2" xref="S2.p3.5.m5.1.1.3.3.2.cmml">W</mi><mi id="S2.p3.5.m5.1.1.3.3.3" xref="S2.p3.5.m5.1.1.3.3.3.cmml">K</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><eq id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1.1"></eq><ci id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2">ğ¾</ci><apply id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3"><times id="S2.p3.5.m5.1.1.3.1.cmml" xref="S2.p3.5.m5.1.1.3.1"></times><ci id="S2.p3.5.m5.1.1.3.2.cmml" xref="S2.p3.5.m5.1.1.3.2">ğ‘‹</ci><apply id="S2.p3.5.m5.1.1.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.3.3.1.cmml" xref="S2.p3.5.m5.1.1.3.3">subscript</csymbol><ci id="S2.p3.5.m5.1.1.3.3.2.cmml" xref="S2.p3.5.m5.1.1.3.3.2">ğ‘Š</ci><ci id="S2.p3.5.m5.1.1.3.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3.3">ğ¾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">K=XW_{K}</annotation></semantics></math> and <math id="S2.p3.6.m6.1" class="ltx_Math" alttext="V=XW_{V}" display="inline"><semantics id="S2.p3.6.m6.1a"><mrow id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mi id="S2.p3.6.m6.1.1.2" xref="S2.p3.6.m6.1.1.2.cmml">V</mi><mo id="S2.p3.6.m6.1.1.1" xref="S2.p3.6.m6.1.1.1.cmml">=</mo><mrow id="S2.p3.6.m6.1.1.3" xref="S2.p3.6.m6.1.1.3.cmml"><mi id="S2.p3.6.m6.1.1.3.2" xref="S2.p3.6.m6.1.1.3.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S2.p3.6.m6.1.1.3.1" xref="S2.p3.6.m6.1.1.3.1.cmml">â€‹</mo><msub id="S2.p3.6.m6.1.1.3.3" xref="S2.p3.6.m6.1.1.3.3.cmml"><mi id="S2.p3.6.m6.1.1.3.3.2" xref="S2.p3.6.m6.1.1.3.3.2.cmml">W</mi><mi id="S2.p3.6.m6.1.1.3.3.3" xref="S2.p3.6.m6.1.1.3.3.3.cmml">V</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><eq id="S2.p3.6.m6.1.1.1.cmml" xref="S2.p3.6.m6.1.1.1"></eq><ci id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2">ğ‘‰</ci><apply id="S2.p3.6.m6.1.1.3.cmml" xref="S2.p3.6.m6.1.1.3"><times id="S2.p3.6.m6.1.1.3.1.cmml" xref="S2.p3.6.m6.1.1.3.1"></times><ci id="S2.p3.6.m6.1.1.3.2.cmml" xref="S2.p3.6.m6.1.1.3.2">ğ‘‹</ci><apply id="S2.p3.6.m6.1.1.3.3.cmml" xref="S2.p3.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S2.p3.6.m6.1.1.3.3.1.cmml" xref="S2.p3.6.m6.1.1.3.3">subscript</csymbol><ci id="S2.p3.6.m6.1.1.3.3.2.cmml" xref="S2.p3.6.m6.1.1.3.3.2">ğ‘Š</ci><ci id="S2.p3.6.m6.1.1.3.3.3.cmml" xref="S2.p3.6.m6.1.1.3.3.3">ğ‘‰</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">V=XW_{V}</annotation></semantics></math>, where <math id="S2.p3.7.m7.1" class="ltx_Math" alttext="W_{Q}" display="inline"><semantics id="S2.p3.7.m7.1a"><msub id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml"><mi id="S2.p3.7.m7.1.1.2" xref="S2.p3.7.m7.1.1.2.cmml">W</mi><mi id="S2.p3.7.m7.1.1.3" xref="S2.p3.7.m7.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><apply id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p3.7.m7.1.1.1.cmml" xref="S2.p3.7.m7.1.1">subscript</csymbol><ci id="S2.p3.7.m7.1.1.2.cmml" xref="S2.p3.7.m7.1.1.2">ğ‘Š</ci><ci id="S2.p3.7.m7.1.1.3.cmml" xref="S2.p3.7.m7.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">W_{Q}</annotation></semantics></math>, <math id="S2.p3.8.m8.1" class="ltx_Math" alttext="W_{K}" display="inline"><semantics id="S2.p3.8.m8.1a"><msub id="S2.p3.8.m8.1.1" xref="S2.p3.8.m8.1.1.cmml"><mi id="S2.p3.8.m8.1.1.2" xref="S2.p3.8.m8.1.1.2.cmml">W</mi><mi id="S2.p3.8.m8.1.1.3" xref="S2.p3.8.m8.1.1.3.cmml">K</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.8.m8.1b"><apply id="S2.p3.8.m8.1.1.cmml" xref="S2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.1.cmml" xref="S2.p3.8.m8.1.1">subscript</csymbol><ci id="S2.p3.8.m8.1.1.2.cmml" xref="S2.p3.8.m8.1.1.2">ğ‘Š</ci><ci id="S2.p3.8.m8.1.1.3.cmml" xref="S2.p3.8.m8.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.8.m8.1c">W_{K}</annotation></semantics></math> and <math id="S2.p3.9.m9.1" class="ltx_Math" alttext="W_{V}" display="inline"><semantics id="S2.p3.9.m9.1a"><msub id="S2.p3.9.m9.1.1" xref="S2.p3.9.m9.1.1.cmml"><mi id="S2.p3.9.m9.1.1.2" xref="S2.p3.9.m9.1.1.2.cmml">W</mi><mi id="S2.p3.9.m9.1.1.3" xref="S2.p3.9.m9.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.9.m9.1b"><apply id="S2.p3.9.m9.1.1.cmml" xref="S2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p3.9.m9.1.1.1.cmml" xref="S2.p3.9.m9.1.1">subscript</csymbol><ci id="S2.p3.9.m9.1.1.2.cmml" xref="S2.p3.9.m9.1.1.2">ğ‘Š</ci><ci id="S2.p3.9.m9.1.1.3.cmml" xref="S2.p3.9.m9.1.1.3">ğ‘‰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.9.m9.1c">W_{V}</annotation></semantics></math> are linear projection matrices, and <math id="S2.p3.10.m10.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.p3.10.m10.1a"><mi id="S2.p3.10.m10.1.1" xref="S2.p3.10.m10.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.p3.10.m10.1b"><ci id="S2.p3.10.m10.1.1.cmml" xref="S2.p3.10.m10.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.10.m10.1c">X</annotation></semantics></math> is a sequence of <math id="S2.p3.11.m11.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.p3.11.m11.1a"><mi id="S2.p3.11.m11.1.1" xref="S2.p3.11.m11.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p3.11.m11.1b"><ci id="S2.p3.11.m11.1.1.cmml" xref="S2.p3.11.m11.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.11.m11.1c">d</annotation></semantics></math>-dimensional embeddings of the input.
For normalization, the attention weights <math id="S2.p3.12.m12.1" class="ltx_Math" alttext="QK^{\text{T}}" display="inline"><semantics id="S2.p3.12.m12.1a"><mrow id="S2.p3.12.m12.1.1" xref="S2.p3.12.m12.1.1.cmml"><mi id="S2.p3.12.m12.1.1.2" xref="S2.p3.12.m12.1.1.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S2.p3.12.m12.1.1.1" xref="S2.p3.12.m12.1.1.1.cmml">â€‹</mo><msup id="S2.p3.12.m12.1.1.3" xref="S2.p3.12.m12.1.1.3.cmml"><mi id="S2.p3.12.m12.1.1.3.2" xref="S2.p3.12.m12.1.1.3.2.cmml">K</mi><mtext id="S2.p3.12.m12.1.1.3.3" xref="S2.p3.12.m12.1.1.3.3a.cmml">T</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.12.m12.1b"><apply id="S2.p3.12.m12.1.1.cmml" xref="S2.p3.12.m12.1.1"><times id="S2.p3.12.m12.1.1.1.cmml" xref="S2.p3.12.m12.1.1.1"></times><ci id="S2.p3.12.m12.1.1.2.cmml" xref="S2.p3.12.m12.1.1.2">ğ‘„</ci><apply id="S2.p3.12.m12.1.1.3.cmml" xref="S2.p3.12.m12.1.1.3"><csymbol cd="ambiguous" id="S2.p3.12.m12.1.1.3.1.cmml" xref="S2.p3.12.m12.1.1.3">superscript</csymbol><ci id="S2.p3.12.m12.1.1.3.2.cmml" xref="S2.p3.12.m12.1.1.3.2">ğ¾</ci><ci id="S2.p3.12.m12.1.1.3.3a.cmml" xref="S2.p3.12.m12.1.1.3.3"><mtext mathsize="70%" id="S2.p3.12.m12.1.1.3.3.cmml" xref="S2.p3.12.m12.1.1.3.3">T</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.12.m12.1c">QK^{\text{T}}</annotation></semantics></math> are scaled by the square root of the embedding dimension <math id="S2.p3.13.m13.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.p3.13.m13.1a"><mi id="S2.p3.13.m13.1.1" xref="S2.p3.13.m13.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p3.13.m13.1b"><ci id="S2.p3.13.m13.1.1.cmml" xref="S2.p3.13.m13.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.13.m13.1c">d</annotation></semantics></math>.
To facilitate more diverse features, attention is computed in <math id="S2.p3.14.m14.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.p3.14.m14.1a"><mi id="S2.p3.14.m14.1.1" xref="S2.p3.14.m14.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.p3.14.m14.1b"><ci id="S2.p3.14.m14.1.1.cmml" xref="S2.p3.14.m14.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.14.m14.1c">A</annotation></semantics></math> parallel attention heads.
<math id="S2.p3.15.m15.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.p3.15.m15.1a"><mi id="S2.p3.15.m15.1.1" xref="S2.p3.15.m15.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.p3.15.m15.1b"><ci id="S2.p3.15.m15.1.1.cmml" xref="S2.p3.15.m15.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.15.m15.1c">L</annotation></semantics></math> transformer block layers are stacked to create a transformer network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>, <a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.10" class="ltx_p">To obtain representations for the text modality, we utilize <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr><math id="S2.p4.1.m1.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p4.1.m1.1a"><msub id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml"><mi id="S2.p4.1.m1.1.1a" xref="S2.p4.1.m1.1.1.cmml"></mi><mtext id="S2.p4.1.m1.1.1.1" xref="S2.p4.1.m1.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><apply id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1"><ci id="S2.p4.1.m1.1.1.1a.cmml" xref="S2.p4.1.m1.1.1.1"><mtext mathsize="70%" id="S2.p4.1.m1.1.1.1.cmml" xref="S2.p4.1.m1.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">{}_{\text{TINY}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>, which is a distilled version of <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>. <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr><math id="S2.p4.2.m2.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p4.2.m2.1a"><msub id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml"><mi id="S2.p4.2.m2.1.1a" xref="S2.p4.2.m2.1.1.cmml"></mi><mtext id="S2.p4.2.m2.1.1.1" xref="S2.p4.2.m2.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.1b"><apply id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1"><ci id="S2.p4.2.m2.1.1.1a.cmml" xref="S2.p4.2.m2.1.1.1"><mtext mathsize="70%" id="S2.p4.2.m2.1.1.1.cmml" xref="S2.p4.2.m2.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.1c">{}_{\text{TINY}}</annotation></semantics></math> uses the same general architecture as <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr> but with fewer layers <math id="S2.p4.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.p4.3.m3.1a"><mi id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><ci id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">L</annotation></semantics></math>, attention heads <math id="S2.p4.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.p4.4.m4.1a"><mi id="S2.p4.4.m4.1.1" xref="S2.p4.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.p4.4.m4.1b"><ci id="S2.p4.4.m4.1.1.cmml" xref="S2.p4.4.m4.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m4.1c">A</annotation></semantics></math>, and a smaller embedding dimension <math id="S2.p4.5.m5.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.p4.5.m5.1a"><mi id="S2.p4.5.m5.1.1" xref="S2.p4.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p4.5.m5.1b"><ci id="S2.p4.5.m5.1.1.cmml" xref="S2.p4.5.m5.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.5.m5.1c">d</annotation></semantics></math>.
It uses <abbr title="multi-head self-attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSA</span></abbr> and a  <span title="feed-forward network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">feed-forward network</span></span> (<abbr title="feed-forward network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFN</span></abbr>) to extract features from text tokens of the question input <math id="S2.p4.6.m6.1" class="ltx_Math" alttext="\mathbf{Q}" display="inline"><semantics id="S2.p4.6.m6.1a"><mi id="S2.p4.6.m6.1.1" xref="S2.p4.6.m6.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.p4.6.m6.1b"><ci id="S2.p4.6.m6.1.1.cmml" xref="S2.p4.6.m6.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.6.m6.1c">\mathbf{Q}</annotation></semantics></math>.
<abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr><math id="S2.p4.7.m7.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p4.7.m7.1a"><msub id="S2.p4.7.m7.1.1" xref="S2.p4.7.m7.1.1.cmml"><mi id="S2.p4.7.m7.1.1a" xref="S2.p4.7.m7.1.1.cmml"></mi><mtext id="S2.p4.7.m7.1.1.1" xref="S2.p4.7.m7.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.7.m7.1b"><apply id="S2.p4.7.m7.1.1.cmml" xref="S2.p4.7.m7.1.1"><ci id="S2.p4.7.m7.1.1.1a.cmml" xref="S2.p4.7.m7.1.1.1"><mtext mathsize="70%" id="S2.p4.7.m7.1.1.1.cmml" xref="S2.p4.7.m7.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.7.m7.1c">{}_{\text{TINY}}</annotation></semantics></math> is pre-trained using a three-step method:
1) a large teacher model with more layers, attention heads, and larger embedding dimension is trained using  <span title="masked language modeling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">masked language modeling</span></span> (<abbr title="masked language modeling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MLM</span></abbr>) and next sentence objectives;
2) <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr><math id="S2.p4.8.m8.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p4.8.m8.1a"><msub id="S2.p4.8.m8.1.1" xref="S2.p4.8.m8.1.1.cmml"><mi id="S2.p4.8.m8.1.1a" xref="S2.p4.8.m8.1.1.cmml"></mi><mtext id="S2.p4.8.m8.1.1.1" xref="S2.p4.8.m8.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.8.m8.1b"><apply id="S2.p4.8.m8.1.1.cmml" xref="S2.p4.8.m8.1.1"><ci id="S2.p4.8.m8.1.1.1a.cmml" xref="S2.p4.8.m8.1.1.1"><mtext mathsize="70%" id="S2.p4.8.m8.1.1.1.cmml" xref="S2.p4.8.m8.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.8.m8.1c">{}_{\text{TINY}}</annotation></semantics></math> is pre-trained with <abbr title="masked language modeling" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MLM</span></abbr>;
3) <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr><math id="S2.p4.9.m9.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p4.9.m9.1a"><msub id="S2.p4.9.m9.1.1" xref="S2.p4.9.m9.1.1.cmml"><mi id="S2.p4.9.m9.1.1a" xref="S2.p4.9.m9.1.1.cmml"></mi><mtext id="S2.p4.9.m9.1.1.1" xref="S2.p4.9.m9.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.9.m9.1b"><apply id="S2.p4.9.m9.1.1.cmml" xref="S2.p4.9.m9.1.1"><ci id="S2.p4.9.m9.1.1.1a.cmml" xref="S2.p4.9.m9.1.1.1"><mtext mathsize="70%" id="S2.p4.9.m9.1.1.1.cmml" xref="S2.p4.9.m9.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.9.m9.1c">{}_{\text{TINY}}</annotation></semantics></math> is additionally pre-trained using knowledge distillation of the large teacher model from the first step.
During the last step, the student model BERT<math id="S2.p4.10.m10.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p4.10.m10.1a"><msub id="S2.p4.10.m10.1.1" xref="S2.p4.10.m10.1.1.cmml"><mi id="S2.p4.10.m10.1.1a" xref="S2.p4.10.m10.1.1.cmml"></mi><mtext id="S2.p4.10.m10.1.1.1" xref="S2.p4.10.m10.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p4.10.m10.1b"><apply id="S2.p4.10.m10.1.1.cmml" xref="S2.p4.10.m10.1.1"><ci id="S2.p4.10.m10.1.1.1a.cmml" xref="S2.p4.10.m10.1.1.1"><mtext mathsize="70%" id="S2.p4.10.m10.1.1.1.cmml" xref="S2.p4.10.m10.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.10.m10.1c">{}_{\text{TINY}}</annotation></semantics></math> learns from the soft labels produced by the teacher model.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.4" class="ltx_p">For the image feature extraction, <abbr title="data-efficient vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">Deit Tiny</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite> uses <abbr title="multi-head self-attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSA</span></abbr> and a <abbr title="feed-forward network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FFN</span></abbr>.
It extracts token embeddings from <math id="S2.p5.1.m1.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S2.p5.1.m1.1a"><mi id="S2.p5.1.m1.1.1" xref="S2.p5.1.m1.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S2.p5.1.m1.1b"><ci id="S2.p5.1.m1.1.1.cmml" xref="S2.p5.1.m1.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.1.m1.1c">\mathbf{I}</annotation></semantics></math> by splitting the image into patches following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>.
Similar to <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr><math id="S2.p5.2.m2.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S2.p5.2.m2.1a"><msub id="S2.p5.2.m2.1.1" xref="S2.p5.2.m2.1.1.cmml"><mi id="S2.p5.2.m2.1.1a" xref="S2.p5.2.m2.1.1.cmml"></mi><mtext id="S2.p5.2.m2.1.1.1" xref="S2.p5.2.m2.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.p5.2.m2.1b"><apply id="S2.p5.2.m2.1.1.cmml" xref="S2.p5.2.m2.1.1"><ci id="S2.p5.2.m2.1.1.1a.cmml" xref="S2.p5.2.m2.1.1.1"><mtext mathsize="70%" id="S2.p5.2.m2.1.1.1.cmml" xref="S2.p5.2.m2.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.2.m2.1c">{}_{\text{TINY}}</annotation></semantics></math>, it uses a reduced number of attention heads <math id="S2.p5.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.p5.3.m3.1a"><mi id="S2.p5.3.m3.1.1" xref="S2.p5.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.p5.3.m3.1b"><ci id="S2.p5.3.m3.1.1.cmml" xref="S2.p5.3.m3.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.3.m3.1c">A</annotation></semantics></math> and a smaller embedding dimension <math id="S2.p5.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.p5.4.m4.1a"><mi id="S2.p5.4.m4.1.1" xref="S2.p5.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p5.4.m4.1b"><ci id="S2.p5.4.m4.1.1.cmml" xref="S2.p5.4.m4.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p5.4.m4.1c">d</annotation></semantics></math> to reduce the number of parameters and increase the throughput of the model.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><abbr title="mobile vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MobileViT-S</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite> uses <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="n\times n" display="inline"><semantics id="S2.p6.1.m1.1a"><mrow id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml"><mi id="S2.p6.1.m1.1.1.2" xref="S2.p6.1.m1.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S2.p6.1.m1.1.1.1" xref="S2.p6.1.m1.1.1.1.cmml">Ã—</mo><mi id="S2.p6.1.m1.1.1.3" xref="S2.p6.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><apply id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1"><times id="S2.p6.1.m1.1.1.1.cmml" xref="S2.p6.1.m1.1.1.1"></times><ci id="S2.p6.1.m1.1.1.2.cmml" xref="S2.p6.1.m1.1.1.2">ğ‘›</ci><ci id="S2.p6.1.m1.1.1.3.cmml" xref="S2.p6.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">n\times n</annotation></semantics></math> and point-wise convolutions to encode local information about a pixel and project the information into a higher dimensional space.
The resulting tensors are reshaped (unfolded) into non-overlapping patches (tokens) similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>, and attention is applied to the tokens to model long-range interactions.
Afterwards, the tokens are reshaped into their original tensor dimensions (folding), which is possible as the previous unfolding operation keeps the pixel and patch order intact.
This restores the original pixel location and allows for all pixels to encode information about all other pixels without the need for a large number of tokens in the attention operation.
Combined with dimensionality-reducing MobileNet-blocks, the unfolding-attention-folding operations reduce the latency and the number of parameters.
</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p"><abbr title="cross-covariance image transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">XCiT Nano</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> changes the order of matrix operations in <abbr title="multi-head self-attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSA</span></abbr> and thus greatly reduces the cost of the attention operation.
This  <span title="cross-covariance attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">cross-covariance attention</span></span> (<abbr title="cross-covariance attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">XCA</span></abbr>) between keys and queries is calculated between channels instead of tokens, significantly reducing the number of
calculations required.
<abbr title="cross-covariance attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">XCA</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> can be defined as:</p>
</div>
<div id="S2.p8" class="ltx_para">
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1.6" class="ltx_Math" alttext="\displaystyle\operatorname{Attention}_{\text{XC}}(Q,K,V)=V\operatorname{Softmax}\left(\cfrac{\hat{K}^{\text{T}}\hat{Q}}{\tau}\right)," display="inline"><semantics id="S2.E2.m1.6a"><mrow id="S2.E2.m1.6.6.1" xref="S2.E2.m1.6.6.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1" xref="S2.E2.m1.6.6.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.2.cmml"><msub id="S2.E2.m1.6.6.1.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.1.1.cmml"><mi id="S2.E2.m1.6.6.1.1.1.1.1.2" xref="S2.E2.m1.6.6.1.1.1.1.1.2.cmml">Attention</mi><mtext id="S2.E2.m1.6.6.1.1.1.1.1.3" xref="S2.E2.m1.6.6.1.1.1.1.1.3a.cmml">XC</mtext></msub><mo id="S2.E2.m1.6.6.1.1.1.1a" xref="S2.E2.m1.6.6.1.1.1.2.cmml">â¡</mo><mrow id="S2.E2.m1.6.6.1.1.1.1.2" xref="S2.E2.m1.6.6.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.1.1.1.1.2.1" xref="S2.E2.m1.6.6.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">Q</mi><mo id="S2.E2.m1.6.6.1.1.1.1.2.2" xref="S2.E2.m1.6.6.1.1.1.2.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">K</mi><mo id="S2.E2.m1.6.6.1.1.1.1.2.3" xref="S2.E2.m1.6.6.1.1.1.2.cmml">,</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">V</mi><mo stretchy="false" id="S2.E2.m1.6.6.1.1.1.1.2.4" xref="S2.E2.m1.6.6.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.6.6.1.1.2" xref="S2.E2.m1.6.6.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.6.6.1.1.3" xref="S2.E2.m1.6.6.1.1.3.cmml"><mi id="S2.E2.m1.6.6.1.1.3.2" xref="S2.E2.m1.6.6.1.1.3.2.cmml">V</mi><mo lspace="0.167em" rspace="0em" id="S2.E2.m1.6.6.1.1.3.1" xref="S2.E2.m1.6.6.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E2.m1.6.6.1.1.3.3.2" xref="S2.E2.m1.6.6.1.1.3.3.1.cmml"><mi id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">Softmax</mi><mo id="S2.E2.m1.6.6.1.1.3.3.2a" xref="S2.E2.m1.6.6.1.1.3.3.1.cmml">â¡</mo><mrow id="S2.E2.m1.6.6.1.1.3.3.2.1" xref="S2.E2.m1.6.6.1.1.3.3.1.cmml"><mo id="S2.E2.m1.6.6.1.1.3.3.2.1.1" xref="S2.E2.m1.6.6.1.1.3.3.1.cmml">(</mo><mstyle displaystyle="true" id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml"><mfrac id="S2.E2.m1.5.5a" xref="S2.E2.m1.5.5.cmml"><mrow id="S2.E2.m1.5.5.2" xref="S2.E2.m1.5.5.2.cmml"><msup id="S2.E2.m1.5.5.2.2" xref="S2.E2.m1.5.5.2.2.cmml"><mover accent="true" id="S2.E2.m1.5.5.2.2.2" xref="S2.E2.m1.5.5.2.2.2.cmml"><mi id="S2.E2.m1.5.5.2.2.2.2" xref="S2.E2.m1.5.5.2.2.2.2.cmml">K</mi><mo id="S2.E2.m1.5.5.2.2.2.1" xref="S2.E2.m1.5.5.2.2.2.1.cmml">^</mo></mover><mtext id="S2.E2.m1.5.5.2.2.3" xref="S2.E2.m1.5.5.2.2.3a.cmml">T</mtext></msup><mo lspace="0em" rspace="0em" id="S2.E2.m1.5.5.2.1" xref="S2.E2.m1.5.5.2.1.cmml">â€‹</mo><mover accent="true" id="S2.E2.m1.5.5.2.3" xref="S2.E2.m1.5.5.2.3.cmml"><mi id="S2.E2.m1.5.5.2.3.2" xref="S2.E2.m1.5.5.2.3.2.cmml">Q</mi><mo id="S2.E2.m1.5.5.2.3.1" xref="S2.E2.m1.5.5.2.3.1.cmml">^</mo></mover></mrow><mi id="S2.E2.m1.5.5.3" xref="S2.E2.m1.5.5.3.cmml">Ï„</mi></mfrac></mstyle><mo id="S2.E2.m1.6.6.1.1.3.3.2.1.2" xref="S2.E2.m1.6.6.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.6.6.1.2" xref="S2.E2.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.6b"><apply id="S2.E2.m1.6.6.1.1.cmml" xref="S2.E2.m1.6.6.1"><eq id="S2.E2.m1.6.6.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.2"></eq><apply id="S2.E2.m1.6.6.1.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.1.1"><apply id="S2.E2.m1.6.6.1.1.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.6.6.1.1.1.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.6.6.1.1.1.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.2">Attention</ci><ci id="S2.E2.m1.6.6.1.1.1.1.1.3a.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.3"><mtext mathsize="70%" id="S2.E2.m1.6.6.1.1.1.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.3">XC</mtext></ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ğ‘„</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ğ¾</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">ğ‘‰</ci></apply><apply id="S2.E2.m1.6.6.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.3"><times id="S2.E2.m1.6.6.1.1.3.1.cmml" xref="S2.E2.m1.6.6.1.1.3.1"></times><ci id="S2.E2.m1.6.6.1.1.3.2.cmml" xref="S2.E2.m1.6.6.1.1.3.2">ğ‘‰</ci><apply id="S2.E2.m1.6.6.1.1.3.3.1.cmml" xref="S2.E2.m1.6.6.1.1.3.3.2"><ci id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">Softmax</ci><apply id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5"><csymbol cd="latexml" id="S2.E2.m1.5.5.1.cmml" xref="S2.E2.m1.5.5">continued-fraction</csymbol><apply id="S2.E2.m1.5.5.2.cmml" xref="S2.E2.m1.5.5.2"><times id="S2.E2.m1.5.5.2.1.cmml" xref="S2.E2.m1.5.5.2.1"></times><apply id="S2.E2.m1.5.5.2.2.cmml" xref="S2.E2.m1.5.5.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.2.2.1.cmml" xref="S2.E2.m1.5.5.2.2">superscript</csymbol><apply id="S2.E2.m1.5.5.2.2.2.cmml" xref="S2.E2.m1.5.5.2.2.2"><ci id="S2.E2.m1.5.5.2.2.2.1.cmml" xref="S2.E2.m1.5.5.2.2.2.1">^</ci><ci id="S2.E2.m1.5.5.2.2.2.2.cmml" xref="S2.E2.m1.5.5.2.2.2.2">ğ¾</ci></apply><ci id="S2.E2.m1.5.5.2.2.3a.cmml" xref="S2.E2.m1.5.5.2.2.3"><mtext mathsize="70%" id="S2.E2.m1.5.5.2.2.3.cmml" xref="S2.E2.m1.5.5.2.2.3">T</mtext></ci></apply><apply id="S2.E2.m1.5.5.2.3.cmml" xref="S2.E2.m1.5.5.2.3"><ci id="S2.E2.m1.5.5.2.3.1.cmml" xref="S2.E2.m1.5.5.2.3.1">^</ci><ci id="S2.E2.m1.5.5.2.3.2.cmml" xref="S2.E2.m1.5.5.2.3.2">ğ‘„</ci></apply></apply><ci id="S2.E2.m1.5.5.3.cmml" xref="S2.E2.m1.5.5.3">ğœ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.6c">\displaystyle\operatorname{Attention}_{\text{XC}}(Q,K,V)=V\operatorname{Softmax}\left(\cfrac{\hat{K}^{\text{T}}\hat{Q}}{\tau}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.p8.5" class="ltx_p">where <math id="S2.p8.1.m1.1" class="ltx_Math" alttext="\hat{K}" display="inline"><semantics id="S2.p8.1.m1.1a"><mover accent="true" id="S2.p8.1.m1.1.1" xref="S2.p8.1.m1.1.1.cmml"><mi id="S2.p8.1.m1.1.1.2" xref="S2.p8.1.m1.1.1.2.cmml">K</mi><mo id="S2.p8.1.m1.1.1.1" xref="S2.p8.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.p8.1.m1.1b"><apply id="S2.p8.1.m1.1.1.cmml" xref="S2.p8.1.m1.1.1"><ci id="S2.p8.1.m1.1.1.1.cmml" xref="S2.p8.1.m1.1.1.1">^</ci><ci id="S2.p8.1.m1.1.1.2.cmml" xref="S2.p8.1.m1.1.1.2">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.1.m1.1c">\hat{K}</annotation></semantics></math> and <math id="S2.p8.2.m2.1" class="ltx_Math" alttext="\hat{Q}" display="inline"><semantics id="S2.p8.2.m2.1a"><mover accent="true" id="S2.p8.2.m2.1.1" xref="S2.p8.2.m2.1.1.cmml"><mi id="S2.p8.2.m2.1.1.2" xref="S2.p8.2.m2.1.1.2.cmml">Q</mi><mo id="S2.p8.2.m2.1.1.1" xref="S2.p8.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.p8.2.m2.1b"><apply id="S2.p8.2.m2.1.1.cmml" xref="S2.p8.2.m2.1.1"><ci id="S2.p8.2.m2.1.1.1.cmml" xref="S2.p8.2.m2.1.1.1">^</ci><ci id="S2.p8.2.m2.1.1.2.cmml" xref="S2.p8.2.m2.1.1.2">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.2.m2.1c">\hat{Q}</annotation></semantics></math> are the normalized <math id="S2.p8.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.p8.3.m3.1a"><mi id="S2.p8.3.m3.1.1" xref="S2.p8.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.p8.3.m3.1b"><ci id="S2.p8.3.m3.1.1.cmml" xref="S2.p8.3.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.3.m3.1c">K</annotation></semantics></math> and <math id="S2.p8.4.m4.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.p8.4.m4.1a"><mi id="S2.p8.4.m4.1.1" xref="S2.p8.4.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.p8.4.m4.1b"><ci id="S2.p8.4.m4.1.1.cmml" xref="S2.p8.4.m4.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.4.m4.1c">Q</annotation></semantics></math>, respectively, and <math id="S2.p8.5.m5.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S2.p8.5.m5.1a"><mi id="S2.p8.5.m5.1.1" xref="S2.p8.5.m5.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S2.p8.5.m5.1b"><ci id="S2.p8.5.m5.1.1.cmml" xref="S2.p8.5.m5.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.5.m5.1c">\tau</annotation></semantics></math> is a learnable temperature parameter.
As this change removes explicit communication between patches, a block of convolutions, batch-norm, and non-linearity is added after each <abbr title="cross-covariance attention" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">XCA</span></abbr> block to re-introduce information exchange.
For further detailed information on the considered models, we refer the reader to their respective papers mentioned above.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.9" class="ltx_p">The feature fusion module consists of two linear projections and a modality combination.
The projections map the two modalities with dimensions <math id="S2.p9.1.m1.1" class="ltx_Math" alttext="d_{t}" display="inline"><semantics id="S2.p9.1.m1.1a"><msub id="S2.p9.1.m1.1.1" xref="S2.p9.1.m1.1.1.cmml"><mi id="S2.p9.1.m1.1.1.2" xref="S2.p9.1.m1.1.1.2.cmml">d</mi><mi id="S2.p9.1.m1.1.1.3" xref="S2.p9.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p9.1.m1.1b"><apply id="S2.p9.1.m1.1.1.cmml" xref="S2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p9.1.m1.1.1.1.cmml" xref="S2.p9.1.m1.1.1">subscript</csymbol><ci id="S2.p9.1.m1.1.1.2.cmml" xref="S2.p9.1.m1.1.1.2">ğ‘‘</ci><ci id="S2.p9.1.m1.1.1.3.cmml" xref="S2.p9.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.1.m1.1c">d_{t}</annotation></semantics></math> and <math id="S2.p9.2.m2.1" class="ltx_Math" alttext="d_{v}" display="inline"><semantics id="S2.p9.2.m2.1a"><msub id="S2.p9.2.m2.1.1" xref="S2.p9.2.m2.1.1.cmml"><mi id="S2.p9.2.m2.1.1.2" xref="S2.p9.2.m2.1.1.2.cmml">d</mi><mi id="S2.p9.2.m2.1.1.3" xref="S2.p9.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p9.2.m2.1b"><apply id="S2.p9.2.m2.1.1.cmml" xref="S2.p9.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p9.2.m2.1.1.1.cmml" xref="S2.p9.2.m2.1.1">subscript</csymbol><ci id="S2.p9.2.m2.1.1.2.cmml" xref="S2.p9.2.m2.1.1.2">ğ‘‘</ci><ci id="S2.p9.2.m2.1.1.3.cmml" xref="S2.p9.2.m2.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.2.m2.1c">d_{v}</annotation></semantics></math> into a common dimension <math id="S2.p9.3.m3.1" class="ltx_Math" alttext="d_{f}" display="inline"><semantics id="S2.p9.3.m3.1a"><msub id="S2.p9.3.m3.1.1" xref="S2.p9.3.m3.1.1.cmml"><mi id="S2.p9.3.m3.1.1.2" xref="S2.p9.3.m3.1.1.2.cmml">d</mi><mi id="S2.p9.3.m3.1.1.3" xref="S2.p9.3.m3.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p9.3.m3.1b"><apply id="S2.p9.3.m3.1.1.cmml" xref="S2.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p9.3.m3.1.1.1.cmml" xref="S2.p9.3.m3.1.1">subscript</csymbol><ci id="S2.p9.3.m3.1.1.2.cmml" xref="S2.p9.3.m3.1.1.2">ğ‘‘</ci><ci id="S2.p9.3.m3.1.1.3.cmml" xref="S2.p9.3.m3.1.1.3">ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.3.m3.1c">d_{f}</annotation></semantics></math>, where <math id="S2.p9.4.m4.1" class="ltx_Math" alttext="d_{t}" display="inline"><semantics id="S2.p9.4.m4.1a"><msub id="S2.p9.4.m4.1.1" xref="S2.p9.4.m4.1.1.cmml"><mi id="S2.p9.4.m4.1.1.2" xref="S2.p9.4.m4.1.1.2.cmml">d</mi><mi id="S2.p9.4.m4.1.1.3" xref="S2.p9.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p9.4.m4.1b"><apply id="S2.p9.4.m4.1.1.cmml" xref="S2.p9.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p9.4.m4.1.1.1.cmml" xref="S2.p9.4.m4.1.1">subscript</csymbol><ci id="S2.p9.4.m4.1.1.2.cmml" xref="S2.p9.4.m4.1.1.2">ğ‘‘</ci><ci id="S2.p9.4.m4.1.1.3.cmml" xref="S2.p9.4.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.4.m4.1c">d_{t}</annotation></semantics></math> and <math id="S2.p9.5.m5.1" class="ltx_Math" alttext="d_{v}" display="inline"><semantics id="S2.p9.5.m5.1a"><msub id="S2.p9.5.m5.1.1" xref="S2.p9.5.m5.1.1.cmml"><mi id="S2.p9.5.m5.1.1.2" xref="S2.p9.5.m5.1.1.2.cmml">d</mi><mi id="S2.p9.5.m5.1.1.3" xref="S2.p9.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p9.5.m5.1b"><apply id="S2.p9.5.m5.1.1.cmml" xref="S2.p9.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p9.5.m5.1.1.1.cmml" xref="S2.p9.5.m5.1.1">subscript</csymbol><ci id="S2.p9.5.m5.1.1.2.cmml" xref="S2.p9.5.m5.1.1.2">ğ‘‘</ci><ci id="S2.p9.5.m5.1.1.3.cmml" xref="S2.p9.5.m5.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.5.m5.1c">d_{v}</annotation></semantics></math> denote the dimensions of the flattened output of the text and image encoder modules, respectively.
The value of <math id="S2.p9.6.m6.1" class="ltx_Math" alttext="d_{v}" display="inline"><semantics id="S2.p9.6.m6.1a"><msub id="S2.p9.6.m6.1.1" xref="S2.p9.6.m6.1.1.cmml"><mi id="S2.p9.6.m6.1.1.2" xref="S2.p9.6.m6.1.1.2.cmml">d</mi><mi id="S2.p9.6.m6.1.1.3" xref="S2.p9.6.m6.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p9.6.m6.1b"><apply id="S2.p9.6.m6.1.1.cmml" xref="S2.p9.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p9.6.m6.1.1.1.cmml" xref="S2.p9.6.m6.1.1">subscript</csymbol><ci id="S2.p9.6.m6.1.1.2.cmml" xref="S2.p9.6.m6.1.1.2">ğ‘‘</ci><ci id="S2.p9.6.m6.1.1.3.cmml" xref="S2.p9.6.m6.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.6.m6.1c">d_{v}</annotation></semantics></math> differs depending on the used lightweight transformer.
The projected features are then element-wise multiplied as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>.
The classification module is defined as an <abbr title="multi-layer perceptron" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MLP</span></abbr> projection head.
After training of the proposed architecture, the <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> system can be used to generate an answer <math id="S2.p9.7.m7.1" class="ltx_Math" alttext="\mathbf{A}" display="inline"><semantics id="S2.p9.7.m7.1a"><mi id="S2.p9.7.m7.1.1" xref="S2.p9.7.m7.1.1.cmml">ğ€</mi><annotation-xml encoding="MathML-Content" id="S2.p9.7.m7.1b"><ci id="S2.p9.7.m7.1.1.cmml" xref="S2.p9.7.m7.1.1">ğ€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.7.m7.1c">\mathbf{A}</annotation></semantics></math> for a given input image <math id="S2.p9.8.m8.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S2.p9.8.m8.1a"><mi id="S2.p9.8.m8.1.1" xref="S2.p9.8.m8.1.1.cmml">ğˆ</mi><annotation-xml encoding="MathML-Content" id="S2.p9.8.m8.1b"><ci id="S2.p9.8.m8.1.1.cmml" xref="S2.p9.8.m8.1.1">ğˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.8.m8.1c">\mathbf{I}</annotation></semantics></math> based on a natural language question <math id="S2.p9.9.m9.1" class="ltx_Math" alttext="\mathbf{Q}" display="inline"><semantics id="S2.p9.9.m9.1a"><mi id="S2.p9.9.m9.1.1" xref="S2.p9.9.m9.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.p9.9.m9.1b"><ci id="S2.p9.9.m9.1.1.cmml" xref="S2.p9.9.m9.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p9.9.m9.1c">\mathbf{Q}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.3" class="ltx_p">The experiments were conducted on the RSVQAxBEN benchmark dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>, which contains almost 15 million image/question/answer triplets extracted from the BigEarthNet-S2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>.
The questions provided in this dataset are about: i) the presence of one or more specific  <span title="land use/land cover" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">land use/land cover</span></span> (<abbr title="land use/land cover" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LULC</span></abbr>) classes, where the answers are associated to <span id="S3.p1.3.1" class="ltx_text ltx_inline-quote ltx_outerquote">â€œYes/Noâ€</span> (called <em id="S3.p1.3.2" class="ltx_emph ltx_font_italic">Yes/No</em>);
and ii) the type of the <abbr title="land use/land cover" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LULC</span></abbr> classes, where the answers are one or more <abbr title="land use/land cover" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LULC</span></abbr> class names (called <em id="S3.p1.3.3" class="ltx_emph ltx_font_italic"><abbr title="land use/land cover" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LULC</span></abbr></em>).
In this paper, we used all available Sentinel-2 bands with <math id="S3.p1.1.m1.3" class="ltx_Math" alttext="10\text{\,}\mathrm{m}" display="inline"><semantics id="S3.p1.1.m1.3a"><mrow id="S3.p1.1.m1.3.3" xref="S3.p1.1.m1.3.3.cmml"><mn id="S3.p1.1.m1.1.1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.1.cmml">10</mn><mtext id="S3.p1.1.m1.2.2.2.2.2.2" xref="S3.p1.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.p1.1.m1.3.3.3.3.3.3" xref="S3.p1.1.m1.3.3.3.3.3.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.3b"><apply id="S3.p1.1.m1.3.3.cmml" xref="S3.p1.1.m1.3.3"><csymbol cd="latexml" id="S3.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.p1.1.m1.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1.1.1.1">10</cn><csymbol cd="latexml" id="S3.p1.1.m1.3.3.3.3.3.3.cmml" xref="S3.p1.1.m1.3.3.3.3.3.3">meter</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.3c">10\text{\,}\mathrm{m}</annotation></semantics></math> and <math id="S3.p1.2.m2.3" class="ltx_Math" alttext="20\text{\,}\mathrm{m}" display="inline"><semantics id="S3.p1.2.m2.3a"><mrow id="S3.p1.2.m2.3.3" xref="S3.p1.2.m2.3.3.cmml"><mn id="S3.p1.2.m2.1.1.1.1.1.1" xref="S3.p1.2.m2.1.1.1.1.1.1.cmml">20</mn><mtext id="S3.p1.2.m2.2.2.2.2.2.2" xref="S3.p1.2.m2.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.p1.2.m2.3.3.3.3.3.3" xref="S3.p1.2.m2.3.3.3.3.3.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.3b"><apply id="S3.p1.2.m2.3.3.cmml" xref="S3.p1.2.m2.3.3"><csymbol cd="latexml" id="S3.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.p1.2.m2.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1">20</cn><csymbol cd="latexml" id="S3.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.p1.2.m2.3.3.3.3.3.3">meter</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.3c">20\text{\,}\mathrm{m}</annotation></semantics></math> spatial resolution included in the BigEarthNet-S2 dataset, as suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>. We restricted the model output to the <math id="S3.p1.3.m3.2" class="ltx_Math" alttext="n_{A}=1,000" display="inline"><semantics id="S3.p1.3.m3.2a"><mrow id="S3.p1.3.m3.2.3" xref="S3.p1.3.m3.2.3.cmml"><msub id="S3.p1.3.m3.2.3.2" xref="S3.p1.3.m3.2.3.2.cmml"><mi id="S3.p1.3.m3.2.3.2.2" xref="S3.p1.3.m3.2.3.2.2.cmml">n</mi><mi id="S3.p1.3.m3.2.3.2.3" xref="S3.p1.3.m3.2.3.2.3.cmml">A</mi></msub><mo id="S3.p1.3.m3.2.3.1" xref="S3.p1.3.m3.2.3.1.cmml">=</mo><mrow id="S3.p1.3.m3.2.3.3.2" xref="S3.p1.3.m3.2.3.3.1.cmml"><mn id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">1</mn><mo id="S3.p1.3.m3.2.3.3.2.1" xref="S3.p1.3.m3.2.3.3.1.cmml">,</mo><mn id="S3.p1.3.m3.2.2" xref="S3.p1.3.m3.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.2b"><apply id="S3.p1.3.m3.2.3.cmml" xref="S3.p1.3.m3.2.3"><eq id="S3.p1.3.m3.2.3.1.cmml" xref="S3.p1.3.m3.2.3.1"></eq><apply id="S3.p1.3.m3.2.3.2.cmml" xref="S3.p1.3.m3.2.3.2"><csymbol cd="ambiguous" id="S3.p1.3.m3.2.3.2.1.cmml" xref="S3.p1.3.m3.2.3.2">subscript</csymbol><ci id="S3.p1.3.m3.2.3.2.2.cmml" xref="S3.p1.3.m3.2.3.2.2">ğ‘›</ci><ci id="S3.p1.3.m3.2.3.2.3.cmml" xref="S3.p1.3.m3.2.3.2.3">ğ´</ci></apply><list id="S3.p1.3.m3.2.3.3.1.cmml" xref="S3.p1.3.m3.2.3.3.2"><cn type="integer" id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">1</cn><cn type="integer" id="S3.p1.3.m3.2.2.cmml" xref="S3.p1.3.m3.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.2c">n_{A}=1,000</annotation></semantics></math> most frequent answers.
We use the train/validation/test split as proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.3" class="ltx_p">In the experiments, we analyze our results between each other and compare them with those obtained from:
i) VBFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>;
and ii)  <span title="Deit3Base + TINY" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Deit3<math class="ltx_Math" alttext="{}_{\text{Base}}" display="inline"><semantics><msub><mi></mi><mtext>Base</mtext></msub><annotation-xml encoding="MathML-Content"><apply><ci><mtext mathsize="70%">Base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">{}_{\text{Base}}</annotation></semantics></math> + <abbr title="" class="ltx_glossaryref"></abbr><math class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics><msub><mi></mi><mtext>TINY</mtext></msub><annotation-xml encoding="MathML-Content"><apply><ci><mtext mathsize="70%">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex">{}_{\text{TINY}}</annotation></semantics></math></span></span> (<abbr title="Deit3Base + TINY" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DBBT</span></abbr>), which is a <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> model that exploits the large transformer Deit3<math id="S3.p2.1.m1.1" class="ltx_Math" alttext="{}_{\text{Base}}" display="inline"><semantics id="S3.p2.1.m1.1a"><msub id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1a" xref="S3.p2.1.m1.1.1.cmml"></mi><mtext id="S3.p2.1.m1.1.1.1" xref="S3.p2.1.m1.1.1.1a.cmml">Base</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><ci id="S3.p2.1.m1.1.1.1a.cmml" xref="S3.p2.1.m1.1.1.1"><mtext mathsize="70%" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1">Base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">{}_{\text{Base}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite> as image encoder and BERT<math id="S3.p2.2.m2.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1a" xref="S3.p2.2.m2.1.1.cmml"></mi><mtext id="S3.p2.2.m2.1.1.1" xref="S3.p2.2.m2.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><ci id="S3.p2.2.m2.1.1.1a.cmml" xref="S3.p2.2.m2.1.1.1"><mtext mathsize="70%" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">{}_{\text{TINY}}</annotation></semantics></math> as a text encoder.
Each model (except VBFusion, for which we used the same implementation proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>) is evaluated under two training regimes:
i) the image encoder is pre-trained for 100 epochs on BigEarthNet-S2, and fine-tuned for additional ten epochs on the RSVQAxBEN dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>;
and ii) the full <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> network is trained in an end-to-end fashion for ten epochs.
In both cases, the text encoders use pre-trained weights from Huggingface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite>.
Both training regimes use a linear-warmup-cosine-annuling learning rate schedule with a learning rate of <math id="S3.p2.3.m3.3" class="ltx_Math" alttext="5\text{\times}{10}^{-4}" display="inline"><semantics id="S3.p2.3.m3.3a"><mrow id="S3.p2.3.m3.3.3.3" xref="S3.p2.3.m3.3.3.3.cmml"><mn id="S3.p2.3.m3.1.1.1.1.1.1.1" xref="S3.p2.3.m3.3.3.3.cmml">5</mn><mtext id="S3.p2.3.m3.2.2.2.2.2.2.2" xref="S3.p2.3.m3.3.3.3.cmml">Ã—</mtext><msup id="S3.p2.3.m3.3.3.3.3.3.3.3" xref="S3.p2.3.m3.3.3.3.cmml"><mn id="S3.p2.3.m3.3.3.3.3.3.3.3.2" xref="S3.p2.3.m3.3.3.3.cmml">10</mn><mrow id="S3.p2.3.m3.3.3.3.3.3.3.3.3.2" xref="S3.p2.3.m3.3.3.3.cmml"><mo id="S3.p2.3.m3.3.3.3.3.3.3.3.3.2a" xref="S3.p2.3.m3.3.3.3.cmml">âˆ’</mo><mn id="S3.p2.3.m3.3.3.3.3.3.3.3.3.2.2" xref="S3.p2.3.m3.3.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.3b"><csymbol cd="latexml" id="S3.p2.3.m3.3.3.3.cmml" xref="S3.p2.3.m3.3.3.3">5E-4</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.3c">5\text{\times}{10}^{-4}</annotation></semantics></math> after 10,000 warm-up steps with batch size and dropout set to 512 and 0.25, respectively.
All models are trained on a single A100 GPU with matrix multiplication precision set to <span id="S3.p2.3.1" class="ltx_text ltx_inline-quote ltx_outerquote">â€œmediumâ€</span> in Pytorch 1.13.1.
To evaluate the results, the models are compared in terms of their:
i) accuracy on the two question types Yes/No and <abbr title="land use/land cover" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LULC</span></abbr>;
ii)  <span title="overall accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">overall accuracy</span></span> (<abbr title="overall accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OA</span></abbr>), which is the micro average of all answer classes;
iii)  <span title="average accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">average accuracy</span></span> (<abbr title="average accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AA</span></abbr>), which is the macro average of the two aforementioned question types;
iv)  <span title="parameter count" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">parameter count</span></span> (<abbr title="parameter count" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">PC</span></abbr>);
and v)  <span title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">floating point operations</span></span> (<abbr title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FLOPs</span></abbr>).</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.4" class="ltx_p"><a href="#S3.T1" title="In 3 Experimental Results â€£ LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> shows the experimental results.
From the table, one can see that all models within our <abbr title="lightweight transformer-based  in " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LiT-4-RSVQA</span></abbr> architecture provide competitive accuracies with significantly reduced computational complexity compared to both baseline models.
Among the models in <abbr title="lightweight transformer-based  in " class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">LiT-4-RSVQA</span></abbr>, the configuration with pre-trained <abbr title="cross-covariance image transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">XCiT Nano</span></abbr> achieves the highest accuracy with less than one-tenth of the number of parameters and one-seventh of the computational effort measured in <abbr title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FLOPs</span></abbr> compared to <abbr title="Deit3Base + TINY" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DBBT</span></abbr>.
With an <abbr title="average accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AA</span></abbr> of <math id="S3.p3.1.m1.3" class="ltx_Math" alttext="64.61\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S3.p3.1.m1.3a"><mrow id="S3.p3.1.m1.3.3" xref="S3.p3.1.m1.3.3.cmml"><mn id="S3.p3.1.m1.1.1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.1.1.cmml">64.61</mn><mtext id="S3.p3.1.m1.2.2.2.2.2.2" xref="S3.p3.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.p3.1.m1.3.3.3.3.3.3" xref="S3.p3.1.m1.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.3b"><apply id="S3.p3.1.m1.3.3.cmml" xref="S3.p3.1.m1.3.3"><csymbol cd="latexml" id="S3.p3.1.m1.2.2.2.2.2.2.cmml" xref="S3.p3.1.m1.2.2.2.2.2.2">times</csymbol><cn type="float" id="S3.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1">64.61</cn><csymbol cd="latexml" id="S3.p3.1.m1.3.3.3.3.3.3.cmml" xref="S3.p3.1.m1.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.3c">64.61\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>, it is almost <math id="S3.p3.2.m2.3" class="ltx_Math" alttext="9\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S3.p3.2.m2.3a"><mrow id="S3.p3.2.m2.3.3" xref="S3.p3.2.m2.3.3.cmml"><mn id="S3.p3.2.m2.1.1.1.1.1.1" xref="S3.p3.2.m2.1.1.1.1.1.1.cmml">9</mn><mtext id="S3.p3.2.m2.2.2.2.2.2.2" xref="S3.p3.2.m2.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.p3.2.m2.3.3.3.3.3.3" xref="S3.p3.2.m2.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.3b"><apply id="S3.p3.2.m2.3.3.cmml" xref="S3.p3.2.m2.3.3"><csymbol cd="latexml" id="S3.p3.2.m2.2.2.2.2.2.2.cmml" xref="S3.p3.2.m2.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S3.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1.1.1.1">9</cn><csymbol cd="latexml" id="S3.p3.2.m2.3.3.3.3.3.3.cmml" xref="S3.p3.2.m2.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.3c">9\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math> better than <abbr title="Visual  () fusion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VBFusion</span></abbr> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite> and more than <math id="S3.p3.3.m3.3" class="ltx_Math" alttext="2.5\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S3.p3.3.m3.3a"><mrow id="S3.p3.3.m3.3.3" xref="S3.p3.3.m3.3.3.cmml"><mn id="S3.p3.3.m3.1.1.1.1.1.1" xref="S3.p3.3.m3.1.1.1.1.1.1.cmml">2.5</mn><mtext id="S3.p3.3.m3.2.2.2.2.2.2" xref="S3.p3.3.m3.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.p3.3.m3.3.3.3.3.3.3" xref="S3.p3.3.m3.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.3b"><apply id="S3.p3.3.m3.3.3.cmml" xref="S3.p3.3.m3.3.3"><csymbol cd="latexml" id="S3.p3.3.m3.2.2.2.2.2.2.cmml" xref="S3.p3.3.m3.2.2.2.2.2.2">times</csymbol><cn type="float" id="S3.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.p3.3.m3.1.1.1.1.1.1">2.5</cn><csymbol cd="latexml" id="S3.p3.3.m3.3.3.3.3.3.3.cmml" xref="S3.p3.3.m3.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.3c">2.5\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math> better than <abbr title="Deit3Base + TINY" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">DBBT</span></abbr>.
However, it is worth noting that <abbr title="cross-covariance image transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">XCiT Nano</span></abbr> performs worst of all compared configurations when trained end-to-end.
In terms of <abbr title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FLOPs</span></abbr>, the <abbr title="data-efficient vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">Deit Tiny</span></abbr>-based configuration demands the fewest computational resources.
It uses less than <math id="S3.p3.4.m4.3" class="ltx_Math" alttext="10\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S3.p3.4.m4.3a"><mrow id="S3.p3.4.m4.3.3" xref="S3.p3.4.m4.3.3.cmml"><mn id="S3.p3.4.m4.1.1.1.1.1.1" xref="S3.p3.4.m4.1.1.1.1.1.1.cmml">10</mn><mtext id="S3.p3.4.m4.2.2.2.2.2.2" xref="S3.p3.4.m4.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.p3.4.m4.3.3.3.3.3.3" xref="S3.p3.4.m4.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.3b"><apply id="S3.p3.4.m4.3.3.cmml" xref="S3.p3.4.m4.3.3"><csymbol cd="latexml" id="S3.p3.4.m4.2.2.2.2.2.2.cmml" xref="S3.p3.4.m4.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S3.p3.4.m4.1.1.1.1.1.1.cmml" xref="S3.p3.4.m4.1.1.1.1.1.1">10</cn><csymbol cd="latexml" id="S3.p3.4.m4.3.3.3.3.3.3.cmml" xref="S3.p3.4.m4.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.3c">10\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math> of the <abbr title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FLOPs</span></abbr> in comparison with the DBBT and more than 600 times fewer <abbr title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FLOPs</span></abbr> when compared with <abbr title="Visual  () fusion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VBFusion</span></abbr>.
However, the accuracy of this model is higher in all evaluated metrics than the respective DBBT model and <abbr title="Visual  () fusion" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VBFusion</span></abbr>.
In addition, one can observe that:
i) models with pre-trained image encoders perform better than end-to-end trained networks;
and ii) better-performing pre-trained models do not correspond to better-performing end-to-end trained models, highlighting the advantage of using a pre-trained image encoder.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.27.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Accuracy per question type as well as  <span title="average accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">average accuracy</span></span> (<abbr title="average accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AA</span></abbr>) and  <span title="overall accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">overall accuracy</span></span> (<abbr title="overall accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OA</span></abbr>) obtained on the RSVQAxBEN dataset. <span id="S3.T1.28.2" class="ltx_ERROR undefined">\Acf</span>*PC and  <span title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">floating point operations</span></span> (<abbr title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FLOPs</span></abbr>) are given in millions and billions, respectively. The image encoders use all available Sentinel-2 bands with 10m and 20m spatial resolution. Results are averaged over three runs, with standard deviations provided in brackets.
âœ“<math id="S3.T1.3.m1.1" class="ltx_Math" alttext="{}^{\mbox{\tiny{\char 115}}}" display="inline"><semantics id="S3.T1.3.m1.1b"><msup id="S3.T1.3.m1.1.1" xref="S3.T1.3.m1.1.1.cmml"><mi id="S3.T1.3.m1.1.1b" xref="S3.T1.3.m1.1.1.cmml"></mi><mtext mathsize="90%" id="S3.T1.3.m1.1.1.1" xref="S3.T1.3.m1.1.1.1a.cmml">â–²</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.T1.3.m1.1c"><apply id="S3.T1.3.m1.1.1.cmml" xref="S3.T1.3.m1.1.1"><ci id="S3.T1.3.m1.1.1.1a.cmml" xref="S3.T1.3.m1.1.1.1"><mtext mathsize="63%" id="S3.T1.3.m1.1.1.1.cmml" xref="S3.T1.3.m1.1.1.1">â–²</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.m1.1d">{}^{\mbox{\tiny{\char 115}}}</annotation></semantics></math>Â Pre-trainedÂ onÂ ImageNet; âœ“<sup id="S3.T1.29.3" class="ltx_sup">âˆ—</sup>Â pre-trainedÂ onÂ BigEarthNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>. </figcaption>
<table id="S3.T1.9" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.9.6.1" class="ltx_tr">
<td id="S3.T1.9.6.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S3.T1.9.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Architecture</span></td>
<td id="S3.T1.9.6.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S3.T1.9.6.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Encoder</span></td>
<td id="S3.T1.9.6.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S3.T1.9.6.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Fusion</span></td>
<td id="S3.T1.9.6.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S3.T1.9.6.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question Type</span></td>
<td id="S3.T1.9.6.1.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S3.T1.9.6.1.5.1" class="ltx_text" style="font-size:80%;"><abbr title="average accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AA</span></abbr></span></td>
<td id="S3.T1.9.6.1.6" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S3.T1.9.6.1.6.1" class="ltx_text" style="font-size:80%;"><abbr title="overall accuracy" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">OA</span></abbr></span></td>
<td id="S3.T1.9.6.1.7" class="ltx_td ltx_align_right ltx_border_tt" rowspan="2"><span id="S3.T1.9.6.1.7.1" class="ltx_text" style="font-size:80%;"><abbr title="parameter count" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">PC</span></abbr></span></td>
<td id="S3.T1.9.6.1.8" class="ltx_td ltx_align_right ltx_border_tt" rowspan="2"><span id="S3.T1.9.6.1.8.1" class="ltx_text" style="font-size:80%;"><abbr title="floating point operation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">FLOPs</span></abbr></span></td>
</tr>
<tr id="S3.T1.9.7.2" class="ltx_tr">
<td id="S3.T1.9.7.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.9.7.2.1.1" class="ltx_text" style="font-size:80%;">Image</span></td>
<td id="S3.T1.9.7.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.7.2.2.1" class="ltx_text" style="font-size:80%;">pre-</span></td>
<td id="S3.T1.9.7.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.7.2.3.1" class="ltx_text" style="font-size:80%;">Text</span></td>
<td id="S3.T1.9.7.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.7.2.4.1" class="ltx_text" style="font-size:80%;">LULC</span></td>
<td id="S3.T1.9.7.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.7.2.5.1" class="ltx_text" style="font-size:80%;">Yes/No</span></td>
</tr>
<tr id="S3.T1.9.8.3" class="ltx_tr">
<td id="S3.T1.9.8.3.1" class="ltx_td"></td>
<td id="S3.T1.9.8.3.2" class="ltx_td"></td>
<td id="S3.T1.9.8.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.9.8.3.3.1" class="ltx_text" style="font-size:80%;">trained</span></td>
<td id="S3.T1.9.8.3.4" class="ltx_td"></td>
<td id="S3.T1.9.8.3.5" class="ltx_td"></td>
<td id="S3.T1.9.8.3.6" class="ltx_td"></td>
<td id="S3.T1.9.8.3.7" class="ltx_td"></td>
<td id="S3.T1.9.8.3.8" class="ltx_td"></td>
<td id="S3.T1.9.8.3.9" class="ltx_td"></td>
<td id="S3.T1.9.8.3.10" class="ltx_td"></td>
<td id="S3.T1.9.8.3.11" class="ltx_td"></td>
</tr>
<tr id="S3.T1.5.1" class="ltx_tr">
<td id="S3.T1.5.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T1.5.1.2.1" class="ltx_text" style="font-size:80%;">VBFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite></span></td>
<td id="S3.T1.5.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T1.5.1.3.1" class="ltx_text" style="font-size:80%;">ResNet-152</span></td>
<td id="S3.T1.5.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S3.T1.5.1.1.1" class="ltx_text" style="font-size:80%;">âœ“</span><math id="S3.T1.5.1.1.m1.1" class="ltx_Math" alttext="\mathrlap{{}^{\mbox{\tiny{\char 115}}}}" display="inline"><semantics id="S3.T1.5.1.1.m1.1a"><msup id="S3.T1.5.1.1.m1.1.1" xref="S3.T1.5.1.1.m1.1.1.cmml"><mi id="S3.T1.5.1.1.m1.1.1a" xref="S3.T1.5.1.1.m1.1.1.cmml"></mi><mtext mathsize="71%" id="S3.T1.5.1.1.m1.1.1.1" xref="S3.T1.5.1.1.m1.1.1.1a.cmml">â–²</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.T1.5.1.1.m1.1b"><apply id="S3.T1.5.1.1.m1.1.1.cmml" xref="S3.T1.5.1.1.m1.1.1"><ci id="S3.T1.5.1.1.m1.1.1.1a.cmml" xref="S3.T1.5.1.1.m1.1.1.1"><mtext mathsize="50%" id="S3.T1.5.1.1.m1.1.1.1.cmml" xref="S3.T1.5.1.1.m1.1.1.1">â–²</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.1.1.m1.1c">\mathrlap{{}^{\mbox{\tiny{\char 115}}}}</annotation></semantics></math>
</td>
<td id="S3.T1.5.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.5.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T1.5.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.5.1.5.1" class="ltx_text" style="font-size:80%;">VisualBert</span></td>
<td id="S3.T1.5.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.5.1.6.1" class="ltx_text" style="font-size:80%;">26.26</span></td>
<td id="S3.T1.5.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.5.1.7.1" class="ltx_text" style="font-size:80%;">86.56</span></td>
<td id="S3.T1.5.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.5.1.8.1" class="ltx_text" style="font-size:80%;">55.80</span></td>
<td id="S3.T1.5.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.5.1.9.1" class="ltx_text" style="font-size:80%;">76.10</span></td>
<td id="S3.T1.5.1.10" class="ltx_td ltx_align_right ltx_border_tt"><span id="S3.T1.5.1.10.1" class="ltx_text" style="font-size:80%;">277.0</span></td>
<td id="S3.T1.5.1.11" class="ltx_td ltx_align_right ltx_border_tt"><span id="S3.T1.5.1.11.1" class="ltx_text" style="font-size:80%;">183.9</span></td>
</tr>
<tr id="S3.T1.6.2" class="ltx_tr">
<td id="S3.T1.6.2.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S3.T1.6.2.2.1" class="ltx_text" style="font-size:80%;">DBBT</span></td>
<td id="S3.T1.6.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.6.2.3.1" class="ltx_text" style="font-size:80%;">Deit3 Base</span></td>
<td id="S3.T1.6.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T1.6.2.1.1" class="ltx_text" style="font-size:80%;">âœ“</span><math id="S3.T1.6.2.1.m1.1" class="ltx_Math" alttext="\mathrlap{{}^{*}}" display="inline"><semantics id="S3.T1.6.2.1.m1.1a"><msup id="S3.T1.6.2.1.m1.1.1" xref="S3.T1.6.2.1.m1.1.1.cmml"><mi id="S3.T1.6.2.1.m1.1.1a" xref="S3.T1.6.2.1.m1.1.1.cmml"></mi><mo mathsize="80%" id="S3.T1.6.2.1.m1.1.1.1" xref="S3.T1.6.2.1.m1.1.1.1.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.6.2.1.m1.1b"><apply id="S3.T1.6.2.1.m1.1.1.cmml" xref="S3.T1.6.2.1.m1.1.1"><times id="S3.T1.6.2.1.m1.1.1.1.cmml" xref="S3.T1.6.2.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.2.1.m1.1c">\mathrlap{{}^{*}}</annotation></semantics></math>
</td>
<td id="S3.T1.6.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.6.2.4.1" class="ltx_text" style="font-size:80%;">Bert Tiny</span></td>
<td id="S3.T1.6.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.6.2.5.1" class="ltx_text" style="font-size:80%;">Simple</span></td>
<td id="S3.T1.6.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.6.2.6.1" class="ltx_text" style="font-size:80%;">36.26 (1.10)</span></td>
<td id="S3.T1.6.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.6.2.7.1" class="ltx_text" style="font-size:80%;">87.83 (0.30)</span></td>
<td id="S3.T1.6.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.6.2.8.1" class="ltx_text" style="font-size:80%;">62.04 (0.70)</span></td>
<td id="S3.T1.6.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.6.2.9.1" class="ltx_text" style="font-size:80%;">79.15 (0.43)</span></td>
<td id="S3.T1.6.2.10" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.6.2.10.1" class="ltx_text" style="font-size:80%;">92.6</span></td>
<td id="S3.T1.6.2.11" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.6.2.11.1" class="ltx_text" style="font-size:80%;">4.4</span></td>
</tr>
<tr id="S3.T1.9.9.4" class="ltx_tr">
<td id="S3.T1.9.9.4.1" class="ltx_td ltx_align_left"><span id="S3.T1.9.9.4.1.1" class="ltx_text" style="font-size:80%;">Deit3 Base</span></td>
<td id="S3.T1.9.9.4.2" class="ltx_td ltx_align_center"><span id="S3.T1.9.9.4.2.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S3.T1.9.9.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.9.9.4.3.1" class="ltx_text" style="font-size:80%;">Bert Tiny</span></td>
<td id="S3.T1.9.9.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.9.9.4.4.1" class="ltx_text" style="font-size:80%;">Simple</span></td>
<td id="S3.T1.9.9.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.9.9.4.5.1" class="ltx_text" style="font-size:80%;">29.53 (3.10)</span></td>
<td id="S3.T1.9.9.4.6" class="ltx_td ltx_align_center"><span id="S3.T1.9.9.4.6.1" class="ltx_text" style="font-size:80%;">85.21 (1.39)</span></td>
<td id="S3.T1.9.9.4.7" class="ltx_td ltx_align_center"><span id="S3.T1.9.9.4.7.1" class="ltx_text" style="font-size:80%;">57.36 (2.24)</span></td>
<td id="S3.T1.9.9.4.8" class="ltx_td ltx_align_center"><span id="S3.T1.9.9.4.8.1" class="ltx_text" style="font-size:80%;">75.84 (1.67)</span></td>
<td id="S3.T1.9.9.4.9" class="ltx_td ltx_align_right"><span id="S3.T1.9.9.4.9.1" class="ltx_text" style="font-size:80%;">92.6</span></td>
<td id="S3.T1.9.9.4.10" class="ltx_td ltx_align_right"><span id="S3.T1.9.9.4.10.1" class="ltx_text" style="font-size:80%;">4.4</span></td>
</tr>
<tr id="S3.T1.7.3" class="ltx_tr">
<td id="S3.T1.7.3.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="6"><span id="S3.T1.7.3.2.1" class="ltx_text" style="font-size:80%;">LiT-4-RSVQA</span></td>
<td id="S3.T1.7.3.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.7.3.3.1" class="ltx_text" style="font-size:80%;">Deit Tiny</span></td>
<td id="S3.T1.7.3.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T1.7.3.1.1" class="ltx_text" style="font-size:80%;">âœ“</span><math id="S3.T1.7.3.1.m1.1" class="ltx_Math" alttext="\mathrlap{{}^{*}}" display="inline"><semantics id="S3.T1.7.3.1.m1.1a"><msup id="S3.T1.7.3.1.m1.1.1" xref="S3.T1.7.3.1.m1.1.1.cmml"><mi id="S3.T1.7.3.1.m1.1.1a" xref="S3.T1.7.3.1.m1.1.1.cmml"></mi><mo mathsize="80%" id="S3.T1.7.3.1.m1.1.1.1" xref="S3.T1.7.3.1.m1.1.1.1.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.7.3.1.m1.1b"><apply id="S3.T1.7.3.1.m1.1.1.cmml" xref="S3.T1.7.3.1.m1.1.1"><times id="S3.T1.7.3.1.m1.1.1.1.cmml" xref="S3.T1.7.3.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.3.1.m1.1c">\mathrlap{{}^{*}}</annotation></semantics></math>
</td>
<td id="S3.T1.7.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.3.4.1" class="ltx_text" style="font-size:80%;">Bert Tiny</span></td>
<td id="S3.T1.7.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.3.5.1" class="ltx_text" style="font-size:80%;">Simple</span></td>
<td id="S3.T1.7.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.3.6.1" class="ltx_text" style="font-size:80%;">38.37 (1.17)</span></td>
<td id="S3.T1.7.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.3.7.1" class="ltx_text" style="font-size:80%;">88.65 (0.39)</span></td>
<td id="S3.T1.7.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.3.8.1" class="ltx_text" style="font-size:80%;">63.51 (0.64)</span></td>
<td id="S3.T1.7.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.3.9.1" class="ltx_text" style="font-size:80%;">80.19 (0.39)</span></td>
<td id="S3.T1.7.3.10" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.7.3.10.1" class="ltx_text" style="font-size:80%;">11.0</span></td>
<td id="S3.T1.7.3.11" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.7.3.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.3</span></td>
</tr>
<tr id="S3.T1.8.4" class="ltx_tr">
<td id="S3.T1.8.4.2" class="ltx_td ltx_align_left"><span id="S3.T1.8.4.2.1" class="ltx_text" style="font-size:80%;">MobileViT S</span></td>
<td id="S3.T1.8.4.1" class="ltx_td ltx_align_center">
<span id="S3.T1.8.4.1.1" class="ltx_text" style="font-size:80%;">âœ“</span><math id="S3.T1.8.4.1.m1.1" class="ltx_Math" alttext="\mathrlap{{}^{*}}" display="inline"><semantics id="S3.T1.8.4.1.m1.1a"><msup id="S3.T1.8.4.1.m1.1.1" xref="S3.T1.8.4.1.m1.1.1.cmml"><mi id="S3.T1.8.4.1.m1.1.1a" xref="S3.T1.8.4.1.m1.1.1.cmml"></mi><mo mathsize="80%" id="S3.T1.8.4.1.m1.1.1.1" xref="S3.T1.8.4.1.m1.1.1.1.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.8.4.1.m1.1b"><apply id="S3.T1.8.4.1.m1.1.1.cmml" xref="S3.T1.8.4.1.m1.1.1"><times id="S3.T1.8.4.1.m1.1.1.1.cmml" xref="S3.T1.8.4.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.4.1.m1.1c">\mathrlap{{}^{*}}</annotation></semantics></math>
</td>
<td id="S3.T1.8.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.8.4.3.1" class="ltx_text" style="font-size:80%;">Bert Tiny</span></td>
<td id="S3.T1.8.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.8.4.4.1" class="ltx_text" style="font-size:80%;">Simple</span></td>
<td id="S3.T1.8.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.8.4.5.1" class="ltx_text" style="font-size:80%;">34.43 (1.94)</span></td>
<td id="S3.T1.8.4.6" class="ltx_td ltx_align_center"><span id="S3.T1.8.4.6.1" class="ltx_text" style="font-size:80%;">89.02 (0.27)</span></td>
<td id="S3.T1.8.4.7" class="ltx_td ltx_align_center"><span id="S3.T1.8.4.7.1" class="ltx_text" style="font-size:80%;">61.37 (0.96)</span></td>
<td id="S3.T1.8.4.8" class="ltx_td ltx_align_center"><span id="S3.T1.8.4.8.1" class="ltx_text" style="font-size:80%;">79.95 (0.23)</span></td>
<td id="S3.T1.8.4.9" class="ltx_td ltx_align_right"><span id="S3.T1.8.4.9.1" class="ltx_text" style="font-size:80%;">10.4</span></td>
<td id="S3.T1.8.4.10" class="ltx_td ltx_align_right"><span id="S3.T1.8.4.10.1" class="ltx_text" style="font-size:80%;">0.5</span></td>
</tr>
<tr id="S3.T1.9.5" class="ltx_tr">
<td id="S3.T1.9.5.2" class="ltx_td ltx_align_left"><span id="S3.T1.9.5.2.1" class="ltx_text" style="font-size:80%;">XCiT Nano</span></td>
<td id="S3.T1.9.5.1" class="ltx_td ltx_align_center">
<span id="S3.T1.9.5.1.1" class="ltx_text" style="font-size:80%;">âœ“</span><math id="S3.T1.9.5.1.m1.1" class="ltx_Math" alttext="\mathrlap{{}^{*}}" display="inline"><semantics id="S3.T1.9.5.1.m1.1a"><msup id="S3.T1.9.5.1.m1.1.1" xref="S3.T1.9.5.1.m1.1.1.cmml"><mi id="S3.T1.9.5.1.m1.1.1a" xref="S3.T1.9.5.1.m1.1.1.cmml"></mi><mo mathsize="80%" id="S3.T1.9.5.1.m1.1.1.1" xref="S3.T1.9.5.1.m1.1.1.1.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.9.5.1.m1.1b"><apply id="S3.T1.9.5.1.m1.1.1.cmml" xref="S3.T1.9.5.1.m1.1.1"><times id="S3.T1.9.5.1.m1.1.1.1.cmml" xref="S3.T1.9.5.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.5.1.m1.1c">\mathrlap{{}^{*}}</annotation></semantics></math>
</td>
<td id="S3.T1.9.5.3" class="ltx_td ltx_align_center"><span id="S3.T1.9.5.3.1" class="ltx_text" style="font-size:80%;">Bert Tiny</span></td>
<td id="S3.T1.9.5.4" class="ltx_td ltx_align_center"><span id="S3.T1.9.5.4.1" class="ltx_text" style="font-size:80%;">Simple</span></td>
<td id="S3.T1.9.5.5" class="ltx_td ltx_align_center">
<span id="S3.T1.9.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">39.50</span><span id="S3.T1.9.5.5.2" class="ltx_text" style="font-size:80%;"> (2.04)</span>
</td>
<td id="S3.T1.9.5.6" class="ltx_td ltx_align_center">
<span id="S3.T1.9.5.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.72</span><span id="S3.T1.9.5.6.2" class="ltx_text" style="font-size:80%;"> (0.65)</span>
</td>
<td id="S3.T1.9.5.7" class="ltx_td ltx_align_center">
<span id="S3.T1.9.5.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">64.61</span><span id="S3.T1.9.5.7.2" class="ltx_text" style="font-size:80%;"> (1.35)</span>
</td>
<td id="S3.T1.9.5.8" class="ltx_td ltx_align_center">
<span id="S3.T1.9.5.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">81.27</span><span id="S3.T1.9.5.8.2" class="ltx_text" style="font-size:80%;"> (0.88)</span>
</td>
<td id="S3.T1.9.5.9" class="ltx_td ltx_align_right"><span id="S3.T1.9.5.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">8.1</span></td>
<td id="S3.T1.9.5.10" class="ltx_td ltx_align_right"><span id="S3.T1.9.5.10.1" class="ltx_text" style="font-size:80%;">0.6</span></td>
</tr>
<tr id="S3.T1.9.10.5" class="ltx_tr">
<td id="S3.T1.9.10.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.9.10.5.1.1" class="ltx_text" style="font-size:80%;">Deit Tiny</span></td>
<td id="S3.T1.9.10.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.10.5.2.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S3.T1.9.10.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.10.5.3.1" class="ltx_text" style="font-size:80%;">Bert Tiny</span></td>
<td id="S3.T1.9.10.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.10.5.4.1" class="ltx_text" style="font-size:80%;">Simple</span></td>
<td id="S3.T1.9.10.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.10.5.5.1" class="ltx_text" style="font-size:80%;">32.53 (1.93)</span></td>
<td id="S3.T1.9.10.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.10.5.6.1" class="ltx_text" style="font-size:80%;">86.80 (0.43)</span></td>
<td id="S3.T1.9.10.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.10.5.7.1" class="ltx_text" style="font-size:80%;">59.67 (1.16)</span></td>
<td id="S3.T1.9.10.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.10.5.8.1" class="ltx_text" style="font-size:80%;">77.67 (0.66)</span></td>
<td id="S3.T1.9.10.5.9" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.9.10.5.9.1" class="ltx_text" style="font-size:80%;">11.0</span></td>
<td id="S3.T1.9.10.5.10" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.9.10.5.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.3</span></td>
</tr>
<tr id="S3.T1.9.11.6" class="ltx_tr">
<td id="S3.T1.9.11.6.1" class="ltx_td ltx_align_left"><span id="S3.T1.9.11.6.1.1" class="ltx_text" style="font-size:80%;">MobileViT S</span></td>
<td id="S3.T1.9.11.6.2" class="ltx_td ltx_align_center"><span id="S3.T1.9.11.6.2.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S3.T1.9.11.6.3" class="ltx_td ltx_align_center"><span id="S3.T1.9.11.6.3.1" class="ltx_text" style="font-size:80%;">Bert Tiny</span></td>
<td id="S3.T1.9.11.6.4" class="ltx_td ltx_align_center"><span id="S3.T1.9.11.6.4.1" class="ltx_text" style="font-size:80%;">Simple</span></td>
<td id="S3.T1.9.11.6.5" class="ltx_td ltx_align_center"><span id="S3.T1.9.11.6.5.1" class="ltx_text" style="font-size:80%;">27.18 (3.18)</span></td>
<td id="S3.T1.9.11.6.6" class="ltx_td ltx_align_center"><span id="S3.T1.9.11.6.6.1" class="ltx_text" style="font-size:80%;">85.51 (2.19)</span></td>
<td id="S3.T1.9.11.6.7" class="ltx_td ltx_align_center"><span id="S3.T1.9.11.6.7.1" class="ltx_text" style="font-size:80%;">56.34 (2.63)</span></td>
<td id="S3.T1.9.11.6.8" class="ltx_td ltx_align_center"><span id="S3.T1.9.11.6.8.1" class="ltx_text" style="font-size:80%;">75.70 (2.32)</span></td>
<td id="S3.T1.9.11.6.9" class="ltx_td ltx_align_right"><span id="S3.T1.9.11.6.9.1" class="ltx_text" style="font-size:80%;">10.4</span></td>
<td id="S3.T1.9.11.6.10" class="ltx_td ltx_align_right"><span id="S3.T1.9.11.6.10.1" class="ltx_text" style="font-size:80%;">0.5</span></td>
</tr>
<tr id="S3.T1.9.12.7" class="ltx_tr">
<td id="S3.T1.9.12.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T1.9.12.7.1.1" class="ltx_text" style="font-size:80%;">XCiT Nano</span></td>
<td id="S3.T1.9.12.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.9.12.7.2.1" class="ltx_text" style="font-size:80%;">âœ—</span></td>
<td id="S3.T1.9.12.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.9.12.7.3.1" class="ltx_text" style="font-size:80%;">Bert Tiny</span></td>
<td id="S3.T1.9.12.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.9.12.7.4.1" class="ltx_text" style="font-size:80%;">Simple</span></td>
<td id="S3.T1.9.12.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.9.12.7.5.1" class="ltx_text" style="font-size:80%;">21.45 (4.07)</span></td>
<td id="S3.T1.9.12.7.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.9.12.7.6.1" class="ltx_text" style="font-size:80%;">81.97 (4.66)</span></td>
<td id="S3.T1.9.12.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.9.12.7.7.1" class="ltx_text" style="font-size:80%;">51.71 (4.36)</span></td>
<td id="S3.T1.9.12.7.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.9.12.7.8.1" class="ltx_text" style="font-size:80%;">71.79 (4.56)</span></td>
<td id="S3.T1.9.12.7.9" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T1.9.12.7.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;">8.1</span></td>
<td id="S3.T1.9.12.7.10" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T1.9.12.7.10.1" class="ltx_text" style="font-size:80%;">0.6</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we have studied efficient transformer-based models in the framework of <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.
In particular, we have introduced the LiT-4-RSVQA architecture.
Our architecture is based on lightweight transformer encoder modules, a feature fusion module, and a classification module.
Specifically, <abbr title="bidirectional encoder representations from transformers" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">BERT</span></abbr><math id="S4.p1.1.m1.1" class="ltx_Math" alttext="{}_{\text{TINY}}" display="inline"><semantics id="S4.p1.1.m1.1a"><msub id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1a" xref="S4.p1.1.m1.1.1.cmml"></mi><mtext id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1a.cmml">TINY</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><ci id="S4.p1.1.m1.1.1.1a.cmml" xref="S4.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1">TINY</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">{}_{\text{TINY}}</annotation></semantics></math> is used as a lightweight text encoder, and <abbr title="data-efficient vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">Deit Tiny</span></abbr>, <abbr title="mobile vision transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MobileViT-S</span></abbr>, and <abbr title="cross-covariance image transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">XCiT Nano</span></abbr> are considered as lightweight image encoders.
Experimental results show that the investigated models can achieve high accuracy in the task of <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> for <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr> while having significantly fewer parameters and, therefore, lower computational requirements than larger models.
We would like to note that our architecture is not limited to the considered lightweight models.
As future works, we plan to: i) investigate different encoder models and on-board processing with <span title="field-programmable gate array" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">field-programmable gate arrays</span></span> and <span title="application-specific integrated circuit" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">application-specific integrated circuits</span></span>; and ii) develop a library to simplify the development of <abbr title="visual question answering" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VQA</span></abbr> models in <abbr title="remote sensing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RS</span></abbr>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgements</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work is supported by the European Research Council (ERC) through the ERC-2017-STG BigEarth Project under Grant 759764 and by the European Space Agency through the DA4DTE (Demonstrator precursor Digital Assistant interface for Digital Twin Earth) project and by the German Ministry for Economic Affairs and Climate Action through the AI-Cube Project under Grant 50EE2012B.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>REFERENCES</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bibx1.1.1" class="ltx_text" style="font-size:90%;">Sylvain Lobry, Diego Marcos, Jesse Murray and Devis Tuia</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx1.2.1" class="ltx_text" style="font-size:90%;">â€œRSVQA: Visual Question Answering for Remote Sensing Dataâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx1.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx1.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE TGRS</em><span id="bib.bibx1.5.3" class="ltx_text" style="font-size:90%;"> </span><span id="bib.bibx1.6.4" class="ltx_text ltx_font_bold" style="font-size:90%;">58</span><span id="bib.bibx1.7.5" class="ltx_text" style="font-size:90%;">, 2020, pp. 8555â€“8566</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx1.8.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.1109/tgrs.2020.2988782" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.1109/tgrs.2020.2988782</a>
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bibx2.1.1" class="ltx_text" style="font-size:90%;">Christel Chappuis et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx2.2.1" class="ltx_text" style="font-size:90%;">â€œLanguage Transformers for Remote Sensing Visual Question Answeringâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx2.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx2.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE IGARSS</em><span id="bib.bibx2.5.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 4855â€“4858</span>
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bibx3.1.1" class="ltx_text" style="font-size:90%;">Jacob Devlin, Ming Wei Chang, Kenton Lee and Kristina Toutanova</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx3.2.1" class="ltx_text" style="font-size:90%;">â€œBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx3.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx3.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span id="bib.bibx3.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 4171â€“4186</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx3.6.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.48550/arxiv.1810.04805" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.48550/arxiv.1810.04805</a>
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bibx4.1.1" class="ltx_text" style="font-size:90%;">Tim Siebert, Kai Norman Clasen, Mahdyar Ravanbakhsh and BegÃ¼m Demir</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx4.2.1" class="ltx_text" style="font-size:90%;">â€œMulti-Modal Fusion Transformer for Visual Question Answering in Remote Sensingâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx4.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx4.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SPIE Image and Signal Processing for Remote Sensing</em><span id="bib.bibx4.5.3" class="ltx_text" style="font-size:90%;">, 2022</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx4.6.1" class="ltx_text" style="font-size:90%;">URL: </span><a target="_blank" href="http://arxiv.org/abs/2210.04510" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/2210.04510</a>
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bibx5.1.1" class="ltx_text" style="font-size:90%;">Liunian Harold Li et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx5.2.1" class="ltx_text" style="font-size:90%;">â€œVisualBERT: A Simple and Performant Baseline for Vision and Languageâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx5.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx5.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 1908.03557</em><span id="bib.bibx5.5.3" class="ltx_text" style="font-size:90%;">, 2019</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx5.6.1" class="ltx_text" style="font-size:90%;">URL: </span><a target="_blank" href="http://arxiv.org/abs/1908.03557" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1908.03557</a>
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bibx6.1.1" class="ltx_text" style="font-size:90%;">JoÃ£o Daniel Silva, JoÃ£o MagalhÃ£es, Devis Tuia and Bruno Martins</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx6.2.1" class="ltx_text" style="font-size:90%;">â€œRemote sensing visual question answering with a self-attention multi-modal encoderâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx6.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx6.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery</em><span id="bib.bibx6.5.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 40â€“49</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx6.6.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.1145/3557918.3565874" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.1145/3557918.3565874</a>
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bibx7.1.1" class="ltx_text" style="font-size:90%;">Yash Khare et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx7.2.1" class="ltx_text" style="font-size:90%;">â€œMMBERT: Multimodal Bert pre-training for improved medical VQAâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx7.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx7.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE ISBI</em><span id="bib.bibx7.5.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 1033â€“1036</span>
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bibx8.1.1" class="ltx_text" style="font-size:90%;">Congcong Wen et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx8.2.1" class="ltx_text" style="font-size:90%;">â€œVision-Language Models in Remote Sensing: Current Progress and Future Trendsâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx8.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx8.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 2305.05726</em><span id="bib.bibx8.5.3" class="ltx_text" style="font-size:90%;">, 2023</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx8.6.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.48550/arxiv.2305.05726" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.48550/arxiv.2305.05726</a>
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bibx9.1.1" class="ltx_text" style="font-size:90%;">Hugo Touvron et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx9.2.1" class="ltx_text" style="font-size:90%;">â€œTraining data-efficient image transformers &amp; distillation through attentionâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx9.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx9.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 2012.12877</em><span id="bib.bibx9.5.3" class="ltx_text" style="font-size:90%;">, 2020</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx9.6.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.48550/arxiv.2012.12877" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.48550/arxiv.2012.12877</a>
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bibx10.1.1" class="ltx_text" style="font-size:90%;">Sachin Mehta and Mohammad Rastegari</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx10.2.1" class="ltx_text" style="font-size:90%;">â€œMobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformerâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx10.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx10.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bibx10.5.3" class="ltx_text" style="font-size:90%;">, 2022</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx10.6.1" class="ltx_text" style="font-size:90%;">URL: </span><a target="_blank" href="http://arxiv.org/abs/2110.02178" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/2110.02178</a>
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bibx11.1.1" class="ltx_text" style="font-size:90%;">Alaaeldin El-Nouby et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx11.2.1" class="ltx_text" style="font-size:90%;">â€œXCiT: Cross-Covariance Image Transformersâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx11.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx11.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bibx11.5.3" class="ltx_text" style="font-size:90%;"> </span><span id="bib.bibx11.6.4" class="ltx_text ltx_font_bold" style="font-size:90%;">24</span><span id="bib.bibx11.7.5" class="ltx_text" style="font-size:90%;">, 2021, pp. 20014â€“20027</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx11.8.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.48550/arxiv.2106.09681" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.48550/arxiv.2106.09681</a>
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bibx12.1.1" class="ltx_text" style="font-size:90%;">Ashish Vaswani et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx12.2.1" class="ltx_text" style="font-size:90%;">â€œAttention Is All You Needâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx12.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx12.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bibx12.5.3" class="ltx_text" style="font-size:90%;"> </span><span id="bib.bibx12.6.4" class="ltx_text ltx_font_bold" style="font-size:90%;">30</span><span id="bib.bibx12.7.5" class="ltx_text" style="font-size:90%;">, 2017</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx12.8.1" class="ltx_text" style="font-size:90%;">URL: </span><a target="_blank" href="http://arxiv.org/abs/1706.03762" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1706.03762</a>
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bibx13.1.1" class="ltx_text" style="font-size:90%;">Alexey Dosovitskiy et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx13.2.1" class="ltx_text" style="font-size:90%;">â€œAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx13.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx13.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 2010.11929</em><span id="bib.bibx13.5.3" class="ltx_text" style="font-size:90%;">, 2020</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx13.6.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.48550/arxiv.2010.11929" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.48550/arxiv.2010.11929</a>
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bibx14.1.1" class="ltx_text" style="font-size:90%;">Iulia Turc, Ming-Wei Chang, Kenton Lee and Kristina Toutanova</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx14.2.1" class="ltx_text" style="font-size:90%;">â€œWell-read Students Learn Better: On the Importance of Pre-training Compact Modelsâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx14.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx14.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 1908.08962</em><span id="bib.bibx14.5.3" class="ltx_text" style="font-size:90%;">, 2019</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx14.6.1" class="ltx_text" style="font-size:90%;">URL: </span><a target="_blank" href="http://arxiv.org/abs/1908.08962" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1908.08962</a>
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bibx15.1.1" class="ltx_text" style="font-size:90%;">Sylvain Lobry, Begum Demir and Devis Tuia</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx15.2.1" class="ltx_text" style="font-size:90%;">â€œRSVQA Meets BigEarthNet: A New, Large-Scale, Visual Question Answering Dataset for Remote Sensingâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx15.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx15.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE IGARSS</em><span id="bib.bibx15.5.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 1218â€“1221</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx15.6.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.1109/IGARSS47720.2021.9553307" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.1109/IGARSS47720.2021.9553307</a>
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bibx16.1.1" class="ltx_text" style="font-size:90%;">Gencer Sumbul et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx16.2.1" class="ltx_text" style="font-size:90%;">â€œBigEarthNet-MM: A Large Scale Multi-Modal Multi-Label Benchmark Archive for Remote Sensing Image Classification and Retrievalâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx16.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx16.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE GRSM</em><span id="bib.bibx16.5.3" class="ltx_text" style="font-size:90%;"> </span><span id="bib.bibx16.6.4" class="ltx_text ltx_font_bold" style="font-size:90%;">9.3</span><span id="bib.bibx16.7.5" class="ltx_text" style="font-size:90%;">, 2021, pp. 174â€“180</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx16.8.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.1109/MGRS.2021.3089174" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.1109/MGRS.2021.3089174</a>
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bibx17.1.1" class="ltx_text" style="font-size:90%;">Thomas Wolf et al.</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx17.2.1" class="ltx_text" style="font-size:90%;">â€œTransformers: State-of-the-Art Natural Language Processingâ€</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx17.3.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bibx17.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em><span id="bib.bibx17.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 38â€“45</span>
</span>
<span class="ltx_bibblock"><span id="bib.bibx17.6.1" class="ltx_text" style="font-size:90%;">DOI: </span><a target="_blank" href="https://dx.doi.org/10.18653/v1/2020.emnlp-demos.6" title="" class="ltx_ref ltx_href" style="font-size:90%;">10.18653/v1/2020.emnlp-demos.6</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.00757" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.00758" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.00758">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.00758" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.00759" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 02:57:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
