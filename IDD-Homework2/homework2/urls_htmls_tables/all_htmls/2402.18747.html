<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains</title>
<!--Generated on Tue Jun  4 04:11:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.18747v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S1" title="In Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S2" title="In Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Metric types.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Domain specificity.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S3" title="In Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>New Bio MQM Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S3.SS1" title="In 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset Creation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S3.SS1.SSS0.Px1" title="In 3.1 Dataset Creation ‣ 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Step 1: Reference re-translation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S3.SS1.SSS0.Px2" title="In 3.1 Dataset Creation ‣ 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Step 2: Reference quality.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S3.SS1.SSS0.Px3" title="In 3.1 Dataset Creation ‣ 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Step 3: MQM annotations.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4" title="In Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS1" title="In 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Are fine-tuned metrics robust across domains?</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS1.SSS0.Px1" title="In 4.1 Are fine-tuned metrics robust across domains? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Measuring domain robustness.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS1.SSS0.Px2" title="In 4.1 Are fine-tuned metrics robust across domains? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Observations.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS2" title="In 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>How does fine-tuning affect domain robustness?</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS2.SSS0.Px1" title="In 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Model description.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS2.SSS0.Px2" title="In 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Setup.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS2.SSS0.Px3" title="In 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Observation 1: Domain gap persists throughout the fine-tuning process.</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS2.SSS0.Px4" title="In Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Observation 2: In-domain data dramatically improves <span class="ltx_text ltx_font_smallcaps">Comet</span>.</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS3" title="In Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>How does the pre-trained model affect domain robustness?</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS3.SSS0.Px1" title="In 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Setup.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS3.SSS0.Px2" title="In 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Observations: XLM-Roberta.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS3.SSS0.Px3" title="In 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Observations: NLLB.</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S5" title="In Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A1" title="In Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Domain Overlap Between WMT and bio</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A2" title="In Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Corpus Statistics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A3" title="In Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Translator/Annotator Qualifications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A4" title="In Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>MQM Annotation Guidelines</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A4.SS0.SSS0.Px1" title="In Appendix D MQM Annotation Guidelines ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Overview:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A4.SS0.SSS0.Px2" title="In Appendix D MQM Annotation Guidelines ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Task:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A4.SS0.SSS0.Px3" title="In Appendix D MQM Annotation Guidelines ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Delivery format:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A4.SS0.SSS0.Px4" title="In Appendix D MQM Annotation Guidelines ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Error categories:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A4.SS0.SSS0.Px5" title="In Appendix D MQM Annotation Guidelines ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title">Severity (no weights, just severity):</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A5" title="In Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Supplementary Information on Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A5.SS1" title="In Appendix E Supplementary Information on Experiments ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Training Steps and Compute Time for Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A5.SS2" title="In Appendix E Supplementary Information on Experiments ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>List of Data for Fine-Tuning Pre-Trained Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A6" title="In Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Raw Scores for <span class="ltx_text ltx_ref_tag">Figure 2</span></span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\newmdenv</span>
<p class="ltx_p" id="p1.2">[
linecolor=black,
linewidth=1.2pt,
topline=false,
bottomline=false,
rightline=false,
innertopmargin=0mm,
innerbottommargin=-0.5mm,
innerleftmargin=1mm,
skipabove=1.1skipbelow=0.5]quotebox


















<span class="ltx_text" id="p1.2.1" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_tabular ltx_align_middle" id="id6.6.6">
<span class="ltx_tbody">
<span class="ltx_tr" id="id3.3.3.3">
<span class="ltx_td ltx_align_center" id="id1.1.1.1.1">Vilém Zouhar<sup class="ltx_sup" id="id1.1.1.1.1.1">1</sup></span>
<span class="ltx_td ltx_align_center" id="id2.2.2.2.2">Shuoyang Ding<sup class="ltx_sup" id="id2.2.2.2.2.1">2</sup></span>
<span class="ltx_td ltx_align_center" id="id3.3.3.3.3">Anna Currey<sup class="ltx_sup" id="id3.3.3.3.3.1">2</sup></span></span>
<span class="ltx_tr" id="id6.6.6.6">
<span class="ltx_td ltx_align_center" id="id4.4.4.4.1">Tatyana Badeka<sup class="ltx_sup" id="id4.4.4.4.1.1">2</sup></span>
<span class="ltx_td ltx_align_center" id="id5.5.5.5.2">Jenyuan Wang<sup class="ltx_sup" id="id5.5.5.5.2.1">2</sup></span>
<span class="ltx_td ltx_align_center" id="id6.6.6.6.3">Brian Thompson<sup class="ltx_sup" id="id6.6.6.6.3.1">2</sup></span></span>
</span>
</span>
<br class="ltx_break"/><sup class="ltx_sup" id="id9.9.id1">1</sup>ETH Zürich <sup class="ltx_sup" id="id10.10.id2">2</sup>AWS AI Labs 
<br class="ltx_break"/><a class="ltx_ref ltx_href" href="mailto:brianjt@amazon.com" title="">brianjt@amazon.com</a>
</span><span class="ltx_author_notes">  Work done during an internship at Amazon.  Corresponding author</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1"><span class="ltx_text" id="id11.id1.1" lang="en">We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain.
We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference.
We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to both metrics that rely on the surface form and pre-trained metrics that are not fine-tuned on MT quality judgments.</span></p>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Automatic metrics are vital for machine translation (MT) research: given the cost and effort required for manual evaluation, automatic metrics are useful for model development and reproducible comparison between research papers <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib13" title="">2019</a>)</cite>.
In recent years, the MT field has been moving away from string-matching metrics like <span class="ltx_text ltx_font_smallcaps" id="S1.p1.1.1">BLEU</span> <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib16" title="">2002</a>)</cite> towards fine-tuned metrics like <span class="ltx_text ltx_font_smallcaps" id="S1.p1.1.2">Comet</span> <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib18" title="">2020</a>)</cite>, which start with pre-trained models and then fine-tune them on human-generated quality judgments.
Fine-tuned metrics have been the best performers in recent WMT metrics shared task evaluations <cite class="ltx_cite ltx_citemacro_cite">Freitag et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib8" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib7" title="">2023</a>)</cite> and
are recommended by the shared task organizers, who go so far as to say, <span class="ltx_text ltx_font_italic" id="S1.p1.1.3">“Neural fine-tuned metrics are not only better, but also robust to different domains.”</span> (<cite class="ltx_cite ltx_citemacro_citep">Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib8" title="">2022</a></cite>).</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Given the growing popularity of fine-tuned metrics, it is important to better understand their behavior. Here, we examine the question of domain robustness of fine-tuned metrics.
Fine-tuned metrics contain extra parameters on top of the pre-trained model which are initialized randomly (or to zero) and then fine-tuned on human-generated MT quality annotations.
The primary source of those annotations is prior WMT metrics shared tasks, and
domains in WMT are often carried over from year to year (e.g. news). This raises the question: are fine-tuned metrics in fact robust across any domain (including domains not seen in training)? Or can their apparent strong performance
be attributed in part to
the artificially good domain match between training and test data?</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="591" id="S1.F1.g1" src="x1.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Automatic machine translation metric performance on the WMT and biomedical domains, averaged across metric types (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S3.F2" title="Figure 2 ‣ 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> for full results).</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To answer these questions, we first collect human multidimensional quality metrics (MQM) annotations in the biomedical (bio) domain. Vocabulary overlap and error analysis suggest that this new dataset is distinct from the domains used in WMT.
This data covers 11 language pairs and 21 translation systems, with 25k total judgments.
In addition to the MQM annotations, we also create new high-quality reference translations for all directions.
We release this data publicly, along with code for replication of our experiments.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://github.com/amazon-science/bio-mqm-dataset" title=""><span class="ltx_ref ltx_nolink">github.com/amazon-science/bio-mqm-dataset</span></a></span></span></span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Next, we examine how different types of metrics perform on our new bio test set relative to the WMT test set. We find that fine-tuned metrics have substantially lower correlation with human judgments in the bio domain, despite other types of metrics having higher correlation in the bio domain
(see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>), indicating they struggle with the training/inference domain mismatch.
Finally, we present analysis showing that this performance gap persists throughout different stages of the fine-tuning process and is not the result of a deficiency with the pre-trained model.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T1.4" style="width:346.9pt;height:274.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(66.8pt,-52.9pt) scale(1.62711229972933,1.62711229972933) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T1.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S1.T1.4.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.4.4.5.1.1.1">Architecture</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S1.T1.4.4.5.1.2"><span class="ltx_text ltx_font_bold" id="S1.T1.4.4.5.1.2.1">Metrics</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S1.T1.1.1.1.1" style="padding-bottom:5.0pt;">
<span class="ltx_inline-block ltx_align_center" id="S1.T1.1.1.1.1.1">
<span class="ltx_p" id="S1.T1.1.1.1.1.1.2">Surface-Form</span>
<span class="ltx_p" id="S1.T1.1.1.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="32" id="S1.T1.1.1.1.1.1.1.g1" src="x2.png" width="168"/></span>
</span>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.1.2" style="padding-bottom:5.0pt;">
<span class="ltx_inline-block ltx_align_left" id="S1.T1.1.1.1.2.1">
<span class="ltx_p" id="S1.T1.1.1.1.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.1.1.1.2.1.1.1">BLEU</span></span>
<span class="ltx_p" id="S1.T1.1.1.1.2.1.2"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.1.1.1.2.1.2.1">ChrF</span></span>
<span class="ltx_p" id="S1.T1.1.1.1.2.1.3"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.1.1.1.2.1.3.1">TER</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S1.T1.2.2.2.1" style="padding-bottom:5.0pt;">
<span class="ltx_inline-block ltx_align_center" id="S1.T1.2.2.2.1.1">
<span class="ltx_p" id="S1.T1.2.2.2.1.1.2">Pre-trained+Algorithm</span>
<span class="ltx_p" id="S1.T1.2.2.2.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="42" id="S1.T1.2.2.2.1.1.1.g1" src="x3.png" width="252"/></span>
</span>
</th>
<td class="ltx_td ltx_align_left" id="S1.T1.2.2.2.2" style="padding-bottom:5.0pt;">
<span class="ltx_inline-block ltx_align_left" id="S1.T1.2.2.2.2.1">
<span class="ltx_p" id="S1.T1.2.2.2.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.2.2.2.2.1.1.1">BERTScore</span></span>
<span class="ltx_p" id="S1.T1.2.2.2.2.1.2"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.2.2.2.2.1.2.1">Prism</span></span>
<span class="ltx_p" id="S1.T1.2.2.2.2.1.3"></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S1.T1.3.3.3.1" style="padding-bottom:5.0pt;">
<span class="ltx_inline-block ltx_align_center" id="S1.T1.3.3.3.1.1">
<span class="ltx_p" id="S1.T1.3.3.3.1.1.2">Pre-trained+Fine-tuned</span>
<span class="ltx_p" id="S1.T1.3.3.3.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="42" id="S1.T1.3.3.3.1.1.1.g1" src="x4.png" width="252"/></span>
</span>
</th>
<td class="ltx_td ltx_align_left" id="S1.T1.3.3.3.2" style="padding-bottom:5.0pt;">
<span class="ltx_inline-block ltx_align_left" id="S1.T1.3.3.3.2.1">
<span class="ltx_p" id="S1.T1.3.3.3.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.3.3.3.2.1.1.1">Comet</span></span>
<span class="ltx_p" id="S1.T1.3.3.3.2.1.2"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.3.3.3.2.1.2.1">UniTE</span></span>
<span class="ltx_p" id="S1.T1.3.3.3.2.1.3"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.3.3.3.2.1.3.1">Bleurt</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S1.T1.4.4.4.1">
<span class="ltx_inline-block ltx_align_center" id="S1.T1.4.4.4.1.1">
<span class="ltx_p" id="S1.T1.4.4.4.1.1.2">Pre-trained+Prompt</span>
<span class="ltx_p" id="S1.T1.4.4.4.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="42" id="S1.T1.4.4.4.1.1.1.g1" src="x5.png" width="252"/></span>
</span>
</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S1.T1.4.4.4.2">
<span class="ltx_inline-block ltx_align_left" id="S1.T1.4.4.4.2.1">
<span class="ltx_p" id="S1.T1.4.4.4.2.1.1"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.4.4.4.2.1.1.1">GEMBA</span></span>
<span class="ltx_p" id="S1.T1.4.4.4.2.1.2"><span class="ltx_text ltx_font_smallcaps" id="S1.T1.4.4.4.2.1.2.1">AutoMQM</span></span>
<span class="ltx_p" id="S1.T1.4.4.4.2.1.3"></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Metric types considered in this work. The <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="16" id="S1.T1.8.g1" src="x9.png" width="15"/> components have trainable parameters while <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="16" id="S1.T1.9.g2" src="x10.png" width="17"/> use handcrafted heuristics or algorithms and <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="16" id="S1.T1.10.g3" src="x11.png" width="14"/> decodes from a language model.
The <span class="ltx_text ltx_font_italic" id="S1.T1.12.1">ref</span> input is omitted in the case of reference-free metrics (i.e. quality estimation).
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Metric types.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Table 1</span></a> summarizes the different types of metrics that are commonly used to evaluate MT.
The earliest type of MT metrics are <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.1">Surface-Form</em> metrics, which are purely heuristic and use word- or character-based features.
We consider three common <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.2">Surface-Form</em> metrics: <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.3">BLEU</span> <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib16" title="">2002</a>)</cite>, <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.4">TER</span> <cite class="ltx_cite ltx_citemacro_citep">(Snover et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib21" title="">2006</a>)</cite> and <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.5">ChrF</span> <cite class="ltx_cite ltx_citemacro_citep">(Popović, <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib17" title="">2015</a>)</cite>.
Metrics like <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.6">Comet</span> <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib18" title="">2020</a>)</cite>,
<span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.7">Bleurt</span> <cite class="ltx_cite ltx_citemacro_citep">(Sellam et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib19" title="">2020</a>)</cite>,
and <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.8">UniTE</span> <cite class="ltx_cite ltx_citemacro_citep">(Wan et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib30" title="">2022</a>)</cite>
start with a pre-trained language model and fine-tune it on human-generated MT quality judgments. We denote these metrics <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.9">Pre-trained+Fine-tuned</em>.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The WMT metrics task calls these “trained” metrics.</span></span></span>
Another class of metrics also start with a pre-trained model but do not perform fine-tuning. Examples of such metrics include <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.10">Prism</span> <cite class="ltx_cite ltx_citemacro_citep">(Thompson and Post, <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib26" title="">2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib27" title="">b</a>)</cite>, which uses the perplexity of a neural paraphraser, and <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.11">BERTScore</span> <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib22" title="">2022</a>)</cite>, which is based on cosine similarity of word embeddings.
We denote such metrics <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.12">Pre-trained+Algorithm</em> metrics.
More recently, metrics like <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.13">GEMBA</span> <cite class="ltx_cite ltx_citemacro_citep">(Kocmi and Federmann, <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib11" title="">2023</a>)</cite> and <span class="ltx_text ltx_font_smallcaps" id="S2.SS0.SSS0.Px1.p1.1.14">AutoMQM</span> <cite class="ltx_cite ltx_citemacro_citep">(Fernandes et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib4" title="">2023</a>)</cite> have proposed prompting a large language model. We denote these as <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.15">Pre-trained+Prompt</em> metrics.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Domain specificity.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Domain specificity for MT metrics was first explored by <cite class="ltx_cite ltx_citemacro_citet">C. de Souza et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib2" title="">2014</a>)</cite> for <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.1">Surface-Form</em> metrics.
<cite class="ltx_cite ltx_citemacro_citet">Sharami et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib20" title="">2023</a>)</cite> brought attention to the issue of domain adaptation for quality estimation (QE), offering solutions based on curriculum learning and generating synthetic scores similar to <cite class="ltx_cite ltx_citemacro_citet">Heo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib10" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Baek et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib1" title="">2020</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Zouhar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib33" title="">2023</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib22" title="">2022</a>)</cite> examined general-purpose natural language generation metrics and documented their bias with respect to social fairness.
For word-level QE, <cite class="ltx_cite ltx_citemacro_citet">Sharami et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib20" title="">2023</a>)</cite> reported the lack of robustness of neural metrics.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>New Bio MQM Dataset</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We create and release new translations and MQM annotations for the system submissions from 21 participants to the WMT21 biomedical translation shared task <cite class="ltx_cite ltx_citemacro_citep">(Yeganova et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib32" title="">2021</a>)</cite>.
To explore how different the bio domain is from the WMT22 metric task domains, we computed the vocabulary overlap coefficient between each domain. Bio had the smallest average overlap with the WMT domains (0.436) compared to 0.507, 0.486, 0.507, and 0.582 for e-commerce, news, social, and conversation, respectively. See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A1" title="Appendix A Domain Overlap Between WMT and bio ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Appendix A</span></a> for full details and example sentences from each domain.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="S3.F2.g1" src="x12.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Gains in segment-level correlation (Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="S3.F2.2.m1.1"><semantics id="S3.F2.2.m1.1b"><mi id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><ci id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.F2.2.m1.1e">italic_τ</annotation></semantics></math>) when comparing <em class="ltx_emph ltx_font_italic" id="S3.F2.9.1">Surface-Form</em> metrics (average performance of <span class="ltx_text ltx_font_smallcaps" id="S3.F2.10.2">BLEU</span>, <span class="ltx_text ltx_font_smallcaps" id="S3.F2.11.3">ChrF</span>, and <span class="ltx_text ltx_font_smallcaps" id="S3.F2.12.4">TER</span>) to a given metric, on the WMT and bio test sets.
Gains for <em class="ltx_emph ltx_font_italic" id="S3.F2.13.5">Pre-trained+Fine-tuned</em> metrics are much smaller in the unseen bio domain than the WMT domain.
<em class="ltx_emph ltx_font_italic" id="S3.F2.14.6">Pre-trained+Algorithm</em> metrics, which do not train on prior WMT data, do not exhibit the same bias.
See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A6" title="Appendix F Raw Scores for Figure 2 ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Appendix F</span></a> for results in tabular form.
</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset Creation</h3>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T2.1.1.1.1"></td>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1">WMT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.4.1">Bio</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.2.2.1" rowspan="4"><span class="ltx_text" id="S3.T2.1.2.2.1.1">
<span class="ltx_inline-block ltx_align_left" id="S3.T2.1.2.2.1.1.1">
<span class="ltx_p" id="S3.T2.1.2.2.1.1.1.1">Error</span>
<span class="ltx_p" id="S3.T2.1.2.2.1.1.1.2">severity</span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.2.2.2">Critical</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.2.3">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.2.4">8%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.3">
<td class="ltx_td ltx_align_left" id="S3.T2.1.3.3.1">Major</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.3.2">26%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.3.3">44%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.4">
<td class="ltx_td ltx_align_left" id="S3.T2.1.4.4.1">Minor</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.4.2">43%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.4.3">31%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.5">
<td class="ltx_td ltx_align_left" id="S3.T2.1.5.5.1" style="padding-bottom:6.99997pt;">Neutral</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.5.2" style="padding-bottom:6.99997pt;">31%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.5.3" style="padding-bottom:6.99997pt;">16%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.6">
<td class="ltx_td ltx_align_left" id="S3.T2.1.6.6.1" rowspan="5"><span class="ltx_text" id="S3.T2.1.6.6.1.1">
<span class="ltx_inline-block ltx_align_left" id="S3.T2.1.6.6.1.1.1">
<span class="ltx_p" id="S3.T2.1.6.6.1.1.1.1">Error</span>
<span class="ltx_p" id="S3.T2.1.6.6.1.1.1.2">category</span>
</span></span></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.6.6.2">Fluency</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.6.3">47%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.6.4">66%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.7">
<td class="ltx_td ltx_align_left" id="S3.T2.1.7.7.1">Accuracy</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.7.2">44%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.7.3">18%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.8">
<td class="ltx_td ltx_align_left" id="S3.T2.1.8.8.1">Terminology</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.8.2">6%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.8.3">10%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.9">
<td class="ltx_td ltx_align_left" id="S3.T2.1.9.9.1">Locale</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.9.2">2%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.9.3">2%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.10">
<td class="ltx_td ltx_align_left" id="S3.T2.1.10.10.1" style="padding-bottom:6.99997pt;">Other</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.10.2" style="padding-bottom:6.99997pt;">1%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.10.3" style="padding-bottom:6.99997pt;">4%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.11.11">
<td class="ltx_td ltx_align_left" colspan="2" id="S3.T2.1.11.11.1">Error-free segments</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.11.2">45%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.11.3">72%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.12.12">
<td class="ltx_td ltx_align_left" colspan="2" id="S3.T2.1.12.12.1">Errors per erroneous segment</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.12.2">1.9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.12.3">2.1</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_bb" colspan="2" id="S3.T2.1.13.13.1">Abs. erroneous segment score</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.13.13.2">-4.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.13.13.3">-7.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Error distribution of our new bio dataset and the existing WMT22 MQM dataset. The MQM annotation scheme for WMT in most cases did not contain the <span class="ltx_text ltx_font_italic" id="S3.T2.3.1">Critical</span> category.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We created the bio MQM dataset in three steps.
Annotations and translations were performed by expert linguists with experience in the medical domain (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A3" title="Appendix C Translator/Annotator Qualifications ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Appendix C</span></a> for full details).</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Step 1: Reference re-translation.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">The original bio test set consists of bilingual abstracts from crawled academic papers, which might be written by non-native speakers <cite class="ltx_cite ltx_citemacro_citep">(Névéol et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib14" title="">2020</a>)</cite> or even MT <cite class="ltx_cite ltx_citemacro_cite">Thompson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib23" title="">2024</a>)</cite>.
Therefore, we create new professional reference translations.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Step 2: Reference quality.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">To ensure a high bar of quality for the reference translations, we ask a separate set of annotators to provide MQM annotations for the new references.
Any issues identified by this round of MQM annotation are then fixed by a new set of translators, resulting in the final reference translations that we release in this dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Step 3: MQM annotations.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">Finally, we conduct the main MQM annotation on the references and shared task system outputs.
In this step, a single annotator rates all translations of a given document (from all systems and the reference).<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This allows us to distribute annotation jobs to multiple annotators while still allowing the annotator to access document-level context and ensuring that the whole document is ranked consistently.</span></span></span>
Our MQM schema follows <cite class="ltx_cite ltx_citemacro_citet">Freitag et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib6" title="">2021</a>)</cite> except that we add a <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px3.p1.1.1">Critical</span> severity (assigned the same score as <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px3.p1.1.2">Major</span> for backward compatibility). Full annotator instructions are in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A4" title="Appendix D MQM Annotation Guidelines ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Appendix D</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p2.1">The resulting dataset contains roughly 25k segment-level annotations spanning 11 translation directions.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Pt<math alttext="\rightarrow" class="ltx_Math" display="inline" id="footnote4.m1.1"><semantics id="footnote4.m1.1b"><mo id="footnote4.m1.1.1" stretchy="false" xref="footnote4.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><ci id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="footnote4.m1.1e">→</annotation></semantics></math>En, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="footnote4.m2.1"><semantics id="footnote4.m2.1b"><mo id="footnote4.m2.1.1" stretchy="false" xref="footnote4.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="footnote4.m2.1c"><ci id="footnote4.m2.1.1.cmml" xref="footnote4.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m2.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="footnote4.m2.1e">↔</annotation></semantics></math>De, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="footnote4.m3.1"><semantics id="footnote4.m3.1b"><mo id="footnote4.m3.1.1" stretchy="false" xref="footnote4.m3.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="footnote4.m3.1c"><ci id="footnote4.m3.1.1.cmml" xref="footnote4.m3.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m3.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="footnote4.m3.1e">↔</annotation></semantics></math>Es, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="footnote4.m4.1"><semantics id="footnote4.m4.1b"><mo id="footnote4.m4.1.1" stretchy="false" xref="footnote4.m4.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="footnote4.m4.1c"><ci id="footnote4.m4.1.1.cmml" xref="footnote4.m4.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m4.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="footnote4.m4.1e">↔</annotation></semantics></math>Ru, En<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="footnote4.m5.1"><semantics id="footnote4.m5.1b"><mo id="footnote4.m5.1.1" stretchy="false" xref="footnote4.m5.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="footnote4.m5.1c"><ci id="footnote4.m5.1.1.cmml" xref="footnote4.m5.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m5.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="footnote4.m5.1e">↔</annotation></semantics></math>Fr, Zh<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="footnote4.m6.1"><semantics id="footnote4.m6.1b"><mo id="footnote4.m6.1.1" stretchy="false" xref="footnote4.m6.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="footnote4.m6.1c"><ci id="footnote4.m6.1.1.cmml" xref="footnote4.m6.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m6.1d">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="footnote4.m6.1e">↔</annotation></semantics></math>En</span></span></span>
In contrast, most publicly available MQM data to date covers only a few language pairs.
We use ~25% of the segments for each language pair as the train/dev set, leaving the rest as the test set (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A2" title="Appendix B Corpus Statistics ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Appendix B</span></a> for exact sizes in each pair).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p3">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p3.1">We compare error distributions on our new bio MQM dataset and the existing WMT MQM dataset in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S3.T2" title="Table 2 ‣ 3.1 Dataset Creation ‣ 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
Bio MQM contains more <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px3.p3.1.1">Critical</span>/<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px3.p3.1.2">Major</span> errors, and lower absolute scores on average. However, WMT MQM has more overall sentences where an error occurs.
Error category distribution also diverges, notably in <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px3.p3.1.3">Fluency</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px3.p3.1.4">Accuracy</span>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Analysis</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Are fine-tuned metrics robust across domains?</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Measuring domain robustness.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">The performance of a MT metric is typically measured by a certain <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px1.p1.1.1">meta-evaluation metric</em>, such as segment-level Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px1.p1.1.m1.1d">italic_τ</annotation></semantics></math> correlation with human judgments. Intuitively, one could simply measure domain robustness by comparing the performance of a certain metric on domain A and domain B.
This, however, is not straightforward with meta-evaluations for metrics, since performance measured by those meta-evaluations is also affected by factors such as the quantity and quality of the translations included in the dataset, which is often hard to control for.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">As a result, we resort to comparisons of <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px1.p2.1.1">relative</em> performance measured against a domain-invariant baseline. To establish such comparison, we make two assumptions:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">We assume <em class="ltx_emph ltx_font_italic" id="S4.I1.i1.p1.1.1">Surface-Form</em> metrics can serve as a domain-invariant baseline, as they are purely based on heuristics and do not involve parameters specifically tuned on a certain domain. We use average performance of <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i1.p1.1.2">BLEU</span>, <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i1.p1.1.3">ChrF</span>, and <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i1.p1.1.4">TER</span> as the baseline to minimize the impact of specific choice of heuristics.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">We assume segment-level Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="S4.I1.i2.p1.1.m1.1"><semantics id="S4.I1.i2.p1.1.m1.1a"><mi id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.m1.1d">italic_τ</annotation></semantics></math> correlation with human judgments has a linear relationship with the objective performance of a metric. Hence, relative performance can be measured by simple linear subtraction.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Observations.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">Compared to <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.1">Surface-Form</em> metrics, we find that <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.2">Pre-trained+Fine-tuned</em> metrics provide
a substantially smaller (sometimes even negative) improvement in human correlation in the bio domain than the WMT domain (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S3.F2" title="Figure 2 ‣ 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>).
On the other hand, <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px2.p1.1.3">Pre-trained+Algorithm</em> metrics, which have not been trained on WMT data,
do not exhibit the same gap. This gap suggests that fine-tuned metrics struggle with unseen domains.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.1">We also observe a very large performance gap for <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px2.p2.1.1">Pre-trained+Prompt</em> metrics.
Unfortunately, these metrics rely on closed-source LLMs without published training procedures, so we do not know what data the underlying LLMs were trained on.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>How does fine-tuning affect domain robustness?</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Model description.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">For this section, we focus on <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px1.p1.1.1">Comet</span> (reference-based) and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px1.p1.1.2">Comet-QE</span> (reference-free) as they are among the most commonly used MT metrics.
The <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px1.p1.1.3">Comet</span> model works by representing the source, the hypothesis and the reference as three fixed-width vectors using a language model, such as XLM-Roberta-large <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib3" title="">2019</a>)</cite>.
These vectors and their combinations serve as an input to a simple feed-forward regressor which is fine-tuned to minimize the MSE loss with human MQM scores.
A <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px1.p1.1.4">Comet</span> model is trained in two stages, first on direct assessment (DA) quality annotations and then on MQM annotations, both from WMT shared tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Setup.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We limit our experiments to the En-De, Zh-En and Ru-En language directions because of WMT MQM availability. We largely followed the training recipe in the <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px2.p1.1.1">Comet</span> Github repo<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_href" href="https://github.com/Unbabel/COMET/tree/master/configs" title=""><span class="ltx_ref ltx_nolink">github.com/Unbabel/COMET/tree/master/configs</span></a></span></span></span>. For details, please refer to our code.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">There is high inter-annotator variance in the WMT and bio MQM data.
Training on the raw MQM scores is very unstable and therefore per-annotator z-normalizing is necessary to replicate our setup.
Note that the publicly available WMT MQM data are not z-normalized.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Observation 1: Domain gap persists throughout the fine-tuning process.</h4>
<figure class="ltx_table" id="S4.SS2.SSS0.Px3.2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S4.SS2.SSS0.Px3.2.3" style="width:390.3pt;height:391.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(69.4pt,-69.5pt) scale(1.5519113660005,1.5519113660005) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.SS2.SSS0.Px3.2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.SS2.SSS0.Px3.2.3.1.1.1.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px3.2.3.1.1.1.1.1">Test:WMT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6" id="S4.SS2.SSS0.Px3.2.3.1.1.1.2" style="padding:0.5pt 3.4pt;">MQM epochs</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.SS2.SSS0.Px3.2.3.1.2.1.1" rowspan="6" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_parbox ltx_align_top" id="S4.SS2.SSS0.Px3.2.3.1.2.1.1.1" style="width:5.7pt;">
<span class="ltx_p" id="S4.SS2.SSS0.Px3.2.3.1.2.1.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.SSS0.Px3.2.3.1.2.1.1.1.1.1" style="width:8.9pt;height:61.6pt;vertical-align:-28.3pt;"><span class="ltx_transformed_inner" style="width:61.6pt;transform:translate(-26.38pt,2.92pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.SS2.SSS0.Px3.2.3.1.2.1.1.1.1.1.1">DA epochs</span>
</span></span></span>
</span>
</th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.2.1.2" style="padding:0.5pt 3.4pt;"></th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.2.1.3" style="padding:0.5pt 3.4pt;">0</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.2.1.4" style="padding:0.5pt 3.4pt;">1</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.2.1.5" style="padding:0.5pt 3.4pt;">2</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.2.1.6" style="padding:0.5pt 3.4pt;">4</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.2.1.7" style="padding:0.5pt 3.4pt;">8</td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.3.2.1" style="padding:0.5pt 3.4pt;">0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.3.2.2" style="background-color:#FFFFFF;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.3.2.2.1" style="background-color:#FFFFFF;">0.118</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.3.2.3" style="background-color:#F3F3F3;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.3.2.3.1" style="background-color:#F3F3F3;">0.285</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.3.2.4" style="background-color:#FAFAFA;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.3.2.4.1" style="background-color:#FAFAFA;">0.281</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.3.2.5" style="background-color:#FFFFFF;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.3.2.5.1" style="background-color:#FFFFFF;">0.279</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.3.2.6" style="background-color:#DADADA;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.3.2.6.1" style="background-color:#DADADA;">0.295</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.4.3.1" style="padding:0.5pt 3.4pt;">1</th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.4.3.2" style="background-color:#9A9A9A;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.4.3.2.1" style="background-color:#9A9A9A;">0.324</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.4.3.3" style="background-color:#848484;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.4.3.3.1" style="background-color:#848484;">0.333</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.4.3.4" style="background-color:#A6A6A6;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.4.3.4.1" style="background-color:#A6A6A6;">0.318</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.4.3.5" style="background-color:#AAAAAA;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.4.3.5.1" style="background-color:#AAAAAA;">0.317</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.4.3.6" style="background-color:#9A9A9A;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.4.3.6.1" style="background-color:#9A9A9A;">0.323</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.5.4.1" style="padding:0.5pt 3.4pt;">2</th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.5.4.2" style="background-color:#949494;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.5.4.2.1" style="background-color:#949494;">0.326</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.5.4.3" style="background-color:#7C7C7C;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.5.4.3.1" style="background-color:#7C7C7C;">0.337</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.5.4.4" style="background-color:#9B9B9B;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.5.4.4.1" style="background-color:#9B9B9B;">0.323</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.5.4.5" style="background-color:#9B9B9B;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.5.4.5.1" style="background-color:#9B9B9B;">0.323</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.5.4.6" style="background-color:#969696;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.5.4.6.1" style="background-color:#969696;">0.325</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.6.5.1" style="padding:0.5pt 3.4pt;">4</th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.6.5.2" style="background-color:#9E9E9E;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.6.5.2.1" style="background-color:#9E9E9E;">0.322</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.6.5.3" style="background-color:#808080;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.6.5.3.1" style="background-color:#808080;">0.335</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.6.5.4" style="background-color:#9C9C9C;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.6.5.4.1" style="background-color:#9C9C9C;">0.323</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.6.5.5" style="background-color:#9E9E9E;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.6.5.5.1" style="background-color:#9E9E9E;">0.322</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.6.5.6" style="background-color:#A1A1A1;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.6.5.6.1" style="background-color:#A1A1A1;">0.321</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.7.6.1" style="padding:0.5pt 3.4pt;">8</th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.7.6.2" style="background-color:#B7B7B7;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.7.6.2.1" style="background-color:#B7B7B7;">0.311</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.7.6.3" style="background-color:#808080;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.7.6.3.1" style="background-color:#808080;">0.335</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.7.6.4" style="background-color:#989898;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.7.6.4.1" style="background-color:#989898;">0.324</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.7.6.5" style="background-color:#9E9E9E;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.7.6.5.1" style="background-color:#9E9E9E;">0.322</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.7.6.6" style="background-color:#ACACAC;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.7.6.6.1" style="background-color:#ACACAC;">0.316</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.SS2.SSS0.Px3.2.3.1.8.7.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px3.2.3.1.8.7.1.1">Test:Bio</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6" id="S4.SS2.SSS0.Px3.2.3.1.8.7.2" style="padding:0.5pt 3.4pt;">MQM epochs</th>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.SS2.SSS0.Px3.2.3.1.9.8.1" rowspan="6" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_parbox ltx_align_top" id="S4.SS2.SSS0.Px3.2.3.1.9.8.1.1" style="width:5.7pt;">
<span class="ltx_p" id="S4.SS2.SSS0.Px3.2.3.1.9.8.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.SS2.SSS0.Px3.2.3.1.9.8.1.1.1.1" style="width:8.9pt;height:61.6pt;vertical-align:-28.3pt;"><span class="ltx_transformed_inner" style="width:61.6pt;transform:translate(-26.38pt,2.92pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.SS2.SSS0.Px3.2.3.1.9.8.1.1.1.1.1">DA epochs</span>
</span></span></span>
</span>
</th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.9.8.2" style="padding:0.5pt 3.4pt;"></th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.9.8.3" style="padding:0.5pt 3.4pt;">0</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.9.8.4" style="padding:0.5pt 3.4pt;">1</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.9.8.5" style="padding:0.5pt 3.4pt;">2</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.9.8.6" style="padding:0.5pt 3.4pt;">4</td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.9.8.7" style="padding:0.5pt 3.4pt;">8</td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.10.9.1" style="padding:0.5pt 3.4pt;">0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.10.9.2" style="background-color:#FFFFFF;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.10.9.2.1" style="background-color:#FFFFFF;">0.071</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.10.9.3" style="background-color:#F3F3F3;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.10.9.3.1" style="background-color:#F3F3F3;">0.234</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.10.9.4" style="background-color:#FFFFFF;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.10.9.4.1" style="background-color:#FFFFFF;">0.229</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.10.9.5" style="background-color:#E4E4E4;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.10.9.5.1" style="background-color:#E4E4E4;">0.240</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS2.SSS0.Px3.2.3.1.10.9.6" style="background-color:#CCCCCC;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.10.9.6.1" style="background-color:#CCCCCC;">0.250</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.11.10.1" style="padding:0.5pt 3.4pt;">1</th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.11.10.2" style="background-color:#7E7E7E;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.11.10.2.1" style="background-color:#7E7E7E;">0.282</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.11.10.3" style="background-color:#858585;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.11.10.3.1" style="background-color:#858585;">0.280</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.11.10.4" style="background-color:#808080;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.11.10.4.1" style="background-color:#808080;">0.282</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.11.10.5" style="background-color:#929292;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.11.10.5.1" style="background-color:#929292;">0.274</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.11.10.6" style="background-color:#9D9D9D;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.11.10.6.1" style="background-color:#9D9D9D;">0.270</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.12.11.1" style="padding:0.5pt 3.4pt;">2</th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.12.11.2" style="background-color:#9D9D9D;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.12.11.2.1" style="background-color:#9D9D9D;">0.270</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.12.11.3" style="background-color:#A9A9A9;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.12.11.3.1" style="background-color:#A9A9A9;">0.265</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.12.11.4" style="background-color:#959595;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.12.11.4.1" style="background-color:#959595;">0.273</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.12.11.5" style="background-color:#A0A0A0;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.12.11.5.1" style="background-color:#A0A0A0;">0.268</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.12.11.6" style="background-color:#A7A7A7;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.12.11.6.1" style="background-color:#A7A7A7;">0.266</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.13.12.1" style="padding:0.5pt 3.4pt;">4</th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.13.12.2" style="background-color:#C1C1C1;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.13.12.2.1" style="background-color:#C1C1C1;">0.255</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.13.12.3" style="background-color:#D5D5D5;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.13.12.3.1" style="background-color:#D5D5D5;">0.246</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.13.12.4" style="background-color:#B9B9B9;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.13.12.4.1" style="background-color:#B9B9B9;">0.258</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.13.12.5" style="background-color:#B7B7B7;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.13.12.5.1" style="background-color:#B7B7B7;">0.259</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.13.12.6" style="background-color:#C5C5C5;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.13.12.6.1" style="background-color:#C5C5C5;">0.253</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS2.SSS0.Px3.2.3.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.SS2.SSS0.Px3.2.3.1.14.13.1" style="padding:0.5pt 3.4pt;">8</th>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.14.13.2" style="background-color:#E5E5E5;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.14.13.2.1" style="background-color:#E5E5E5;">0.240</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.14.13.3" style="background-color:#E0E0E0;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.14.13.3.1" style="background-color:#E0E0E0;">0.242</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.14.13.4" style="background-color:#B2B2B2;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.14.13.4.1" style="background-color:#B2B2B2;">0.261</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS2.SSS0.Px3.2.3.1.14.13.5" style="background-color:#B4B4B4;padding:0.5pt 3.4pt;"><span class="ltx_text" id="S4.SS2.SSS0.Px3.2.3.1.14.13.5.1" style="background-color:#B4B4B4;">0.260</span></td>
<td class="ltx_td" id="S4.SS2.SSS0.Px3.2.3.1.14.13.6" style="padding:0.5pt 3.4pt;"></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Segment-level correlation (Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.2.2.m1.1"><semantics id="S4.SS2.SSS0.Px3.2.2.m1.1b"><mi id="S4.SS2.SSS0.Px3.2.2.m1.1.1" xref="S4.SS2.SSS0.Px3.2.2.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.2.2.m1.1c"><ci id="S4.SS2.SSS0.Px3.2.2.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.2.2.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.2.2.m1.1d">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.2.2.m1.1e">italic_τ</annotation></semantics></math>) between metrics and human judgments on the WMT (top) and bio (bottom) test sets, for <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px3.2.5.1">Comet</span> with varying epochs of WMT domain DA and MQM training.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.SS2.SSS0.Px3.2.6">We would like to understand which stage among the two training stages for <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px3.2.6.1">Comet</span> accounts for the domain gap.
To this end, we retrained <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px3.2.6.2">Comet</span> with varying epochs on DA/MQM data, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS2.SSS0.Px3" title="Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
In contrast to catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">Goodfellow et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib9" title="">2013</a>); Thompson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib24" title="">2019a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib25" title="">b</a>)</cite>, where a model starts with good general-domain performance and then overfits while being adapted to a new task or domain, we do not see a sharp dropoff in the bio domain performance when training on more WMT (DA and/or MQM) data. This indicates that the model is a weak bio metric at all stages, as opposed to first learning and then forgetting.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_paragraph ltx_figure_panel" id="S4.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Observation 2: In-domain data dramatically improves <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px4.1.1">Comet</span>.</h4>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="470" id="S4.F3.g1" src="x13.png" width="822"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average performance (8 seeds) of <span class="ltx_text ltx_font_smallcaps" id="S4.F3.2.1">Comet</span> fine-tuned on varying amounts of MQM bio data. </figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px4.p1.1">Generally, including bio MQM annotations in training improves <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px4.p1.1.1">Comet</span>’s performance in the bio test set, increasing correlation from 0.287 to 0.328 with 6k bio judgments. Indeed, just 1k judgements improves correlation to 0.313 (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S4.F3" title="Figure 3 ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>).
This rules out the possibility that bio is inherently problematic for <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px4.p1.1.2">Comet</span>’s architecture or fine-tuning strategy.</p>
</div>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>How does the pre-trained model affect domain robustness?</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.1">Comet</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.2">BERTScore</span> are both based on XLM-Roberta-large <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib3" title="">2019</a>)</cite>, allowing us to explore how the same changes to the pre-trained model affect each metric. To see whether improving the underlying pre-trained model improves <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.3">Pre-trained+Algorithm</em> metrics built on those pre-trained models, we fine-tune XLM-Roberta with data similar to the WMT and bio domain setup, respectively.
Similarly, we also investigate how <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.4">Prism</span>, another <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.5">Pre-trained+Algorithm</em> metric,
is affected with changes to the pre-trained model. We use <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.6">Prism</span> with the NLLB multilingual MT models <cite class="ltx_cite ltx_citemacro_citep">(NLLB Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib15" title="">2022</a>)</cite> as they are larger and more recent than the model released with <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.p1.1.7">Prism</span>.</p>
</div>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Setup.</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">Our fine-tuning data covers the four languages of interest, namely English, German, Russian, and Chinese (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A5.SS2" title="E.2 List of Data for Fine-Tuning Pre-Trained Model ‣ Appendix E Supplementary Information on Experiments ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">E.2</span></a> for a detailed data list).
Since NLLB is a translation model, we use only parallel data to fine-tune the model.
For the XLM-Roberta case, note that it was fine-tuned with two objectives: masked language model (MLM) and translation language model (TLM).
We use both parallel and monolingual data for MLM training and parallel data for TLM training.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="514" id="S4.F4.g1" src="x14.png" width="823"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Metric performance when pre-trained model is fine-tuned (FT) on bio or WMT domain data.
<span class="ltx_text" id="S4.F4.10.1" style="color:#339933;">Lower</span> perplexity <span class="ltx_text" id="S4.F4.11.2" style="color:#339933;">improves</span> <span class="ltx_text ltx_font_smallcaps" id="S4.F4.12.3">BERTScore</span> <math alttext="\bigcirc" class="ltx_Math" display="inline" id="S4.F4.3.m1.1"><semantics id="S4.F4.3.m1.1b"><mo id="S4.F4.3.m1.1.1" xref="S4.F4.3.m1.1.1.cmml">○</mo><annotation-xml encoding="MathML-Content" id="S4.F4.3.m1.1c"><ci id="S4.F4.3.m1.1.1.cmml" xref="S4.F4.3.m1.1.1">○</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.3.m1.1d">\bigcirc</annotation><annotation encoding="application/x-llamapun" id="S4.F4.3.m1.1e">○</annotation></semantics></math> but <span class="ltx_text" id="S4.F4.13.4" style="color:#E64D4D;">worsens</span> <span class="ltx_text ltx_font_smallcaps" id="S4.F4.14.5">Comet</span> <math alttext="\square" class="ltx_Math" display="inline" id="S4.F4.4.m2.1"><semantics id="S4.F4.4.m2.1b"><mi id="S4.F4.4.m2.1.1" mathvariant="normal" xref="S4.F4.4.m2.1.1.cmml">□</mi><annotation-xml encoding="MathML-Content" id="S4.F4.4.m2.1c"><ci id="S4.F4.4.m2.1.1.cmml" xref="S4.F4.4.m2.1.1">□</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.4.m2.1d">\square</annotation><annotation encoding="application/x-llamapun" id="S4.F4.4.m2.1e">□</annotation></semantics></math>. Perplexity is average of MLM and TLM objectives on the text portion of the MQM dataset for both domains.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="470" id="S4.F5.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Multiple NLLB MT models are used as the base model for <span class="ltx_text ltx_font_smallcaps" id="S4.F5.3.1">PrismSrc</span>. Fine-tuning the underlying MT model <span class="ltx_text" id="S4.F5.4.2" style="color:#339933;">improves the metric</span>. Compute constraints preclude finetuning NLLB-3.3B.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Observations: XLM-Roberta.</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">For both domains, improving the pre-trained model
improves
<span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.1">BERTScore</span>
but not
<span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.2">Comet</span> (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S4.F4" title="Figure 4 ‣ Setup. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>).
This indicates that the limiting factor for the poor performance of <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.3">Comet</span> on bio is the effect from its various fine-tuning stages (discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS2.SSS0.Px3" title="Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">4.2</span></a>), not an underlying weakness in the pre-trained model on bio.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Observations: NLLB.</h4>
<div class="ltx_para" id="S4.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px3.p1.1">Our findings are shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S4.F5" title="Figure 5 ‣ Setup. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>. In general, we found that improving the pre-trained models performance (as measured by <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px3.p1.1.1">BLEU</span> on a held out test set) also improved <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px3.p1.1.2">Prism</span>’s performance.</p>
</div>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper investigated the performance of machine translation metrics across divergent domains.
To this end, we introduced a new, extensive MQM-annotated dataset covering 11 language pairs in the bio domain. Our analysis showed that <em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">Pre-trained+Fine-tuned</em> metrics (i.e. those that use prior human quality annotations of MT output) exhibit a larger gap between in-domain and out-of-domain performance than <em class="ltx_emph ltx_font_italic" id="S5.p1.1.2">Pre-trained+Algorithm</em> metrics (like <span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.3">BERTScore</span>).
Further experiments showed that this gap can be attributed to the DA and MQM fine-tuning stage.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Despite the gap between in-domain and out-of-domain performance, <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.1">Comet</span> is still the best performing metric on the bio domain in absolute terms.
Thus, our findings suggest potential directions for future work including collecting more diverse human judgments for <em class="ltx_emph ltx_font_italic" id="S5.p2.1.2">Pre-trained+Fine-tuned</em> metrics and exploring ways to improve the generalization of such metrics during fine-tuning.</p>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our findings are dependent on two empirical assumptions we discussed in section <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#S4.SS1" title="4.1 Are fine-tuned metrics robust across domains? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">4.1</span></a>. To the best of our knowledge, those assumptions are necessary to achieve a fair comparison of metrics across domains, but conclusions may change if our assumptions are refuted in future studies.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">We draw conclusions based on a single unseen domain (biomedical). While additional domains would have been preferable, data collection was cost prohibitive.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Context has been shown to be beneficial in machine translation evaluation <cite class="ltx_cite ltx_citemacro_cite">Läubli et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib12" title="">2018</a>); Toral (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib28" title="">2020</a>)</cite> and some metrics used in this work have document-level versions
<cite class="ltx_cite ltx_citemacro_cite">Vernikos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib29" title="">2022</a>)</cite>.
However, in order to draw fair comparisons with existing metrics which do not yet have a document-level version, we only evaluated metrics at the sentence level.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">We focused on segment-level evaluation and did not attempt system-level comparisons because of
the limited number of system submissions to the WMT biomedical translation shared task.</p>
</div>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We would like to thank Georgiana Dinu, Marcello Federico, Prashant Mathur, Stefano Soatto, and other colleagues for their feedback at different stages of drafting.</p>
</div>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Ethical Considerations</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">Our human annotations were conducted through a vendor. Annotators were compensated in accordance to the industry standard – specifically, in the range of $27.50 to $37.50 on an hourly basis, depending on the experience of the annotator.</p>
</div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baek et al. (2020)</span>
<span class="ltx_bibblock">
Yujin Baek, Zae Myung Kim, Jihyung Moon, Hyunjoong Kim, and Eunjeong Park. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.113" title="">PATQUEST: Papago translation quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 991–998, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">C. de Souza et al. (2014)</span>
<span class="ltx_bibblock">
José G. C. de Souza, Marco Turchi, and Matteo Negri. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/C14-1040" title="">Machine translation quality estimation across domains</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</em>, pages 409–420, Dublin, Ireland. Dublin City University and Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2019)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1911.02116" title="">Unsupervised cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/1911.02116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandes et al. (2023)</span>
<span class="ltx_bibblock">
Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, and Orhan Firat. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2308.07286" title="">The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frei and Kramer (2023)</span>
<span class="ltx_bibblock">
Johann Frei and Frank Kramer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.2196/39077" title="">German medical named entity recognition model and data set creation using machine translation and word alignment: Algorithm development and validation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">JMIR Form Res</em>, 7:e39077.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2021)</span>
<span class="ltx_bibblock">
Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00437" title="">Experts, errors, and context: A large-scale study of human evaluation for machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Transactions of the Association for Computational Linguistics</em>, 9:1460–1474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2023)</span>
<span class="ltx_bibblock">
Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.51" title="">Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 578–628, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2022)</span>
<span class="ltx_bibblock">
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.2" title="">Results of WMT22 metrics shared task: Stop using BLEU – neural metrics are better and more robust</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 46–68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2013)</span>
<span class="ltx_bibblock">
Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. 2013.

</span>
<span class="ltx_bibblock">An empirical investigation of catastrophic forgetting in gradient-based neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1312.6211</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heo et al. (2021)</span>
<span class="ltx_bibblock">
Dam Heo, WonKee Lee, Baikjin Jung, and Jong-Hyeok Lee. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.96" title="">Quality estimation using dual encoders with transfer learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 920–927, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi and Federmann (2023)</span>
<span class="ltx_bibblock">
Tom Kocmi and Christian Federmann. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2302.14520" title="">Large language models are state-of-the-art evaluators of translation quality</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2302.14520</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Läubli et al. (2018)</span>
<span class="ltx_bibblock">
Samuel Läubli, Rico Sennrich, and Martin Volk. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1512" title="">Has machine translation achieved human parity? a case for document-level evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 4791–4796, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2019)</span>
<span class="ltx_bibblock">
Qingsong Ma, Johnny Wei, Ondřej Bojar, and Yvette Graham. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5302" title="">Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</em>, pages 62–90, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Névéol et al. (2020)</span>
<span class="ltx_bibblock">
Aurélie Névéol, Antonio Jimeno Yepes, and Mariana Neves. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.lrec-1.453" title="">MEDLINE as a parallel corpus: a survey to gain insight on French-, Spanish- and Portuguese-speaking authors’ abstract writing practice</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, pages 3676–3682, Marseille, France. European Language Resources Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NLLB Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2015)</span>
<span class="ltx_bibblock">
Maja Popović. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W15-3049" title="">chrF: character n-gram F-score for automatic MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.213" title="">COMET: A neural framework for MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 2685–2702, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. (2020)</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.704" title="">BLEURT: Learning robust metrics for text generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 7881–7892, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharami et al. (2023)</span>
<span class="ltx_bibblock">
Javad Pourmostafa Roshan Sharami, Dimitar Shterionov, Frédéric Blain, Eva Vanmassenhove, Mirella De Sisto, Chris Emmery, and Pieter Spronck. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eamt-1.2" title="">Tailoring domain adaptation for machine translation quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 9–20, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snover et al. (2006)</span>
<span class="ltx_bibblock">
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2006.amta-papers.25" title="">A study of translation edit rate with targeted human annotation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers</em>, pages 223–231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2022)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Junliang He, Xipeng Qiu, and Xuanjing Huang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.emnlp-main.245" title="">BERTScore is unfair: On social bias in language model-based metrics for text generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 3726–3739, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thompson et al. (2024)</span>
<span class="ltx_bibblock">
Brian Thompson, Mehak Preet Dhaliwal, Peter Frisch, Tobias Domhan, and Marcello Federico. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2401.05749" title="">A shocking amount of the web is machine translated: Insights from multi-way parallelism</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2401.05749</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thompson et al. (2019a)</span>
<span class="ltx_bibblock">
Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. 2019a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1209" title="">Overcoming catastrophic forgetting during domain adaptation of neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 2062–2068, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thompson et al. (2019b)</span>
<span class="ltx_bibblock">
Brian Thompson, Rebecca Knowles, Xuan Zhang, Huda Khayrallah, Kevin Duh, and Philipp Koehn. 2019b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1142" title="">HABLex: Human annotated bilingual lexicons for experiments in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1382–1387, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thompson and Post (2020a)</span>
<span class="ltx_bibblock">
Brian Thompson and Matt Post. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.8" title="">Automatic machine translation evaluation in many languages via zero-shot paraphrasing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 90–121, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thompson and Post (2020b)</span>
<span class="ltx_bibblock">
Brian Thompson and Matt Post. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.67" title="">Paraphrase generation as zero-shot multilingual translation: Disentangling semantic similarity from lexical and syntactic diversity</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 561–570, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toral (2020)</span>
<span class="ltx_bibblock">
Antonio Toral. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.20" title="">Reassessing claims of human parity and super-human performance in machine translation at WMT 2019</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, pages 185–194, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vernikos et al. (2022)</span>
<span class="ltx_bibblock">
Giorgos Vernikos, Brian Thompson, Prashant Mathur, and Marcello Federico. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.6" title="">Embarrassingly easy document-level MT metrics: How to convert any pretrained metric into a document-level metric</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 118–128, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al. (2022)</span>
<span class="ltx_bibblock">
Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.558" title="">UniTE: Unified translation evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 8117–8127, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Michael Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey A. Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Nancy Xin Ru Wang, Christopher Wilhelm, Boya Xie, Douglas M. Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.nlpcovid19-acl.1" title="">CORD-19: The COVID-19 open research dataset</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020</em>, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yeganova et al. (2021)</span>
<span class="ltx_bibblock">
Lana Yeganova, Dina Wiemann, Mariana Neves, Federica Vezzani, Amy Siu, Inigo Jauregi Unanue, Maite Oronoz, Nancy Mah, Aurélie Névéol, David Martinez, Rachel Bawden, Giorgio Maria Di Nunzio, Roland Roller, Philippe Thomas, Cristian Grozea, Olatz Perez-de Viñaspre, Maika Vicente Navarro, and Antonio Jimeno Yepes. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.70" title="">Findings of the WMT 2021 biomedical translation shared task: Summaries of animal experiments as new test set</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 664–683, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zouhar et al. (2023)</span>
<span class="ltx_bibblock">
Vilém Zouhar, Shehzaad Dhuliawala, Wangchunshu Zhou, Nico Daheim, Tom Kocmi, Yuchen Eleanor Jiang, and Mrinmaya Sachan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eacl-main.95" title="">Poor man’s quality estimation: Predicting reference-based MT metrics without the reference</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>, pages 1311–1325, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Domain Overlap Between WMT and bio</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">To evaluate the overlap between the WMT and bio domains, we calculate the vocabulary overlap coefficient <math alttext="(\frac{|A\cap B|}{\min(|A|,|B|)})" class="ltx_Math" display="inline" id="A1.p1.1.m1.6"><semantics id="A1.p1.1.m1.6a"><mrow id="A1.p1.1.m1.6.7.2" xref="A1.p1.1.m1.6.6.cmml"><mo id="A1.p1.1.m1.6.7.2.1" stretchy="false" xref="A1.p1.1.m1.6.6.cmml">(</mo><mfrac id="A1.p1.1.m1.6.6" xref="A1.p1.1.m1.6.6.cmml"><mrow id="A1.p1.1.m1.1.1.1.1" xref="A1.p1.1.m1.1.1.1.2.cmml"><mo id="A1.p1.1.m1.1.1.1.1.2" stretchy="false" xref="A1.p1.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="A1.p1.1.m1.1.1.1.1.1" xref="A1.p1.1.m1.1.1.1.1.1.cmml"><mi id="A1.p1.1.m1.1.1.1.1.1.2" xref="A1.p1.1.m1.1.1.1.1.1.2.cmml">A</mi><mo id="A1.p1.1.m1.1.1.1.1.1.1" xref="A1.p1.1.m1.1.1.1.1.1.1.cmml">∩</mo><mi id="A1.p1.1.m1.1.1.1.1.1.3" xref="A1.p1.1.m1.1.1.1.1.1.3.cmml">B</mi></mrow><mo id="A1.p1.1.m1.1.1.1.1.3" stretchy="false" xref="A1.p1.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="A1.p1.1.m1.6.6.6.5" xref="A1.p1.1.m1.6.6.6.6.cmml"><mi id="A1.p1.1.m1.4.4.4.3" xref="A1.p1.1.m1.4.4.4.3.cmml">min</mi><mo id="A1.p1.1.m1.6.6.6.5a" xref="A1.p1.1.m1.6.6.6.6.cmml">⁡</mo><mrow id="A1.p1.1.m1.6.6.6.5.2" xref="A1.p1.1.m1.6.6.6.6.cmml"><mo id="A1.p1.1.m1.6.6.6.5.2.3" stretchy="false" xref="A1.p1.1.m1.6.6.6.6.cmml">(</mo><mrow id="A1.p1.1.m1.5.5.5.4.1.1.2" xref="A1.p1.1.m1.5.5.5.4.1.1.1.cmml"><mo id="A1.p1.1.m1.5.5.5.4.1.1.2.1" stretchy="false" xref="A1.p1.1.m1.5.5.5.4.1.1.1.1.cmml">|</mo><mi id="A1.p1.1.m1.2.2.2.1" xref="A1.p1.1.m1.2.2.2.1.cmml">A</mi><mo id="A1.p1.1.m1.5.5.5.4.1.1.2.2" stretchy="false" xref="A1.p1.1.m1.5.5.5.4.1.1.1.1.cmml">|</mo></mrow><mo id="A1.p1.1.m1.6.6.6.5.2.4" xref="A1.p1.1.m1.6.6.6.6.cmml">,</mo><mrow id="A1.p1.1.m1.6.6.6.5.2.2.2" xref="A1.p1.1.m1.6.6.6.5.2.2.1.cmml"><mo id="A1.p1.1.m1.6.6.6.5.2.2.2.1" stretchy="false" xref="A1.p1.1.m1.6.6.6.5.2.2.1.1.cmml">|</mo><mi id="A1.p1.1.m1.3.3.3.2" xref="A1.p1.1.m1.3.3.3.2.cmml">B</mi><mo id="A1.p1.1.m1.6.6.6.5.2.2.2.2" stretchy="false" xref="A1.p1.1.m1.6.6.6.5.2.2.1.1.cmml">|</mo></mrow><mo id="A1.p1.1.m1.6.6.6.5.2.5" stretchy="false" xref="A1.p1.1.m1.6.6.6.6.cmml">)</mo></mrow></mrow></mfrac><mo id="A1.p1.1.m1.6.7.2.2" stretchy="false" xref="A1.p1.1.m1.6.6.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.6b"><apply id="A1.p1.1.m1.6.6.cmml" xref="A1.p1.1.m1.6.7.2"><divide id="A1.p1.1.m1.6.6.7.cmml" xref="A1.p1.1.m1.6.7.2"></divide><apply id="A1.p1.1.m1.1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.1.1"><abs id="A1.p1.1.m1.1.1.1.2.1.cmml" xref="A1.p1.1.m1.1.1.1.1.2"></abs><apply id="A1.p1.1.m1.1.1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1.1.1"><intersect id="A1.p1.1.m1.1.1.1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1.1.1.1"></intersect><ci id="A1.p1.1.m1.1.1.1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.1.1.1.2">𝐴</ci><ci id="A1.p1.1.m1.1.1.1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.1.1.1.3">𝐵</ci></apply></apply><apply id="A1.p1.1.m1.6.6.6.6.cmml" xref="A1.p1.1.m1.6.6.6.5"><min id="A1.p1.1.m1.4.4.4.3.cmml" xref="A1.p1.1.m1.4.4.4.3"></min><apply id="A1.p1.1.m1.5.5.5.4.1.1.1.cmml" xref="A1.p1.1.m1.5.5.5.4.1.1.2"><abs id="A1.p1.1.m1.5.5.5.4.1.1.1.1.cmml" xref="A1.p1.1.m1.5.5.5.4.1.1.2.1"></abs><ci id="A1.p1.1.m1.2.2.2.1.cmml" xref="A1.p1.1.m1.2.2.2.1">𝐴</ci></apply><apply id="A1.p1.1.m1.6.6.6.5.2.2.1.cmml" xref="A1.p1.1.m1.6.6.6.5.2.2.2"><abs id="A1.p1.1.m1.6.6.6.5.2.2.1.1.cmml" xref="A1.p1.1.m1.6.6.6.5.2.2.2.1"></abs><ci id="A1.p1.1.m1.3.3.3.2.cmml" xref="A1.p1.1.m1.3.3.3.2">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.6c">(\frac{|A\cap B|}{\min(|A|,|B|)})</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.6d">( divide start_ARG | italic_A ∩ italic_B | end_ARG start_ARG roman_min ( | italic_A | , | italic_B | ) end_ARG )</annotation></semantics></math> between our new bio MQM dataset and the domains used in the WMT22 metrics shared task. The per-domain overlap matrix is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A1.F6" title="Figure 6 ‣ Appendix A Domain Overlap Between WMT and bio ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.
Randomly selected sentences from each domain are provided for illustration in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A1.F7" title="Figure 7 ‣ Appendix A Domain Overlap Between WMT and bio ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.F6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.F6.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.F6.1.1.1.1" style="padding:0.5pt 3.4pt;"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.F6.1.1.1.2" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.1.1.2.1">e-commerce</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.F6.1.1.1.3" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.1.1.3.1">news</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.F6.1.1.1.4" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.1.1.4.1">social</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.F6.1.1.1.5" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.1.1.5.1">conversation</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.F6.1.1.1.6" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.1.1.6.1">biomedical</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.F6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.F6.1.2.1.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.2.1.1.1">e-commerce</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.F6.1.2.1.2" style="padding:0.5pt 3.4pt;">1.000</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.F6.1.2.1.3" style="padding:0.5pt 3.4pt;">0.349</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.F6.1.2.1.4" style="padding:0.5pt 3.4pt;">0.511</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.F6.1.2.1.5" style="padding:0.5pt 3.4pt;">0.662</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.F6.1.2.1.6" style="padding:0.5pt 3.4pt;">0.369</td>
</tr>
<tr class="ltx_tr" id="A1.F6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.F6.1.3.2.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.3.2.1.1">news</span></th>
<td class="ltx_td ltx_align_right" id="A1.F6.1.3.2.2" style="padding:0.5pt 3.4pt;">0.349</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.3.2.3" style="padding:0.5pt 3.4pt;">1.000</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.3.2.4" style="padding:0.5pt 3.4pt;">0.517</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.3.2.5" style="padding:0.5pt 3.4pt;">0.592</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.3.2.6" style="padding:0.5pt 3.4pt;">0.359</td>
</tr>
<tr class="ltx_tr" id="A1.F6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.F6.1.4.3.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.4.3.1.1">social</span></th>
<td class="ltx_td ltx_align_right" id="A1.F6.1.4.3.2" style="padding:0.5pt 3.4pt;">0.511</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.4.3.3" style="padding:0.5pt 3.4pt;">0.517</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.4.3.4" style="padding:0.5pt 3.4pt;">1.000</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.4.3.5" style="padding:0.5pt 3.4pt;">0.494</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.4.3.6" style="padding:0.5pt 3.4pt;">0.462</td>
</tr>
<tr class="ltx_tr" id="A1.F6.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.F6.1.5.4.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.5.4.1.1">conversation</span></th>
<td class="ltx_td ltx_align_right" id="A1.F6.1.5.4.2" style="padding:0.5pt 3.4pt;">0.662</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.5.4.3" style="padding:0.5pt 3.4pt;">0.592</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.5.4.4" style="padding:0.5pt 3.4pt;">0.494</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.5.4.5" style="padding:0.5pt 3.4pt;">1.000</td>
<td class="ltx_td ltx_align_right" id="A1.F6.1.5.4.6" style="padding:0.5pt 3.4pt;">0.554</td>
</tr>
<tr class="ltx_tr" id="A1.F6.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.F6.1.6.5.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A1.F6.1.6.5.1.1">biomedical</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.F6.1.6.5.2" style="padding:0.5pt 3.4pt;">0.369</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.F6.1.6.5.3" style="padding:0.5pt 3.4pt;">0.359</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.F6.1.6.5.4" style="padding:0.5pt 3.4pt;">0.462</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.F6.1.6.5.5" style="padding:0.5pt 3.4pt;">0.554</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.F6.1.6.5.6" style="padding:0.5pt 3.4pt;">1.000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Vocabulary overlap coefficient between the English source-side data for each domain in the WMT22 and our bio dataset. </figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.F7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.F7.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A1.F7.1.1.1.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.1.1.1.1">
<span class="ltx_p" id="A1.F7.1.1.1.1.1.1" style="width:72.3pt;"><span class="ltx_text ltx_font_bold" id="A1.F7.1.1.1.1.1.1.1">e-commerce</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A1.F7.1.1.1.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.1.1.2.1">
<span class="ltx_p" id="A1.F7.1.1.1.2.1.1" style="width:346.9pt;">This was one of the first albums I purchased of Keith’s "back in the day".</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.F7.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.F7.1.2.1.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.2.1.1.1">
<span class="ltx_p" id="A1.F7.1.2.1.1.1.1" style="width:72.3pt;"><span class="ltx_text ltx_font_bold" id="A1.F7.1.2.1.1.1.1.1">news</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.F7.1.2.1.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.2.1.2.1">
<span class="ltx_p" id="A1.F7.1.2.1.2.1.1" style="width:346.9pt;">Sean Combs has been variously known as Puff Daddy, P. Diddy or Diddy, but this year announced his preference for the names Love and Brother Love.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.F7.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.F7.1.3.2.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.3.2.1.1">
<span class="ltx_p" id="A1.F7.1.3.2.1.1.1" style="width:72.3pt;"><span class="ltx_text ltx_font_bold" id="A1.F7.1.3.2.1.1.1.1">social</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.F7.1.3.2.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.3.2.2.1">
<span class="ltx_p" id="A1.F7.1.3.2.2.1.1" style="width:346.9pt;">The comment about boiling being inefficient is probably correct bc even though the water heater is running continuously, that thing has SO MUCH insulation.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.F7.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.F7.1.4.3.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.4.3.1.1">
<span class="ltx_p" id="A1.F7.1.4.3.1.1.1" style="width:72.3pt;"><span class="ltx_text ltx_font_bold" id="A1.F7.1.4.3.1.1.1.1">conversation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.F7.1.4.3.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.4.3.2.1">
<span class="ltx_p" id="A1.F7.1.4.3.2.1.1" style="width:346.9pt;">Let me know if you were able to create your new password and sign in with it</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.F7.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A1.F7.1.5.4.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.5.4.1.1">
<span class="ltx_p" id="A1.F7.1.5.4.1.1.1" style="width:72.3pt;"><span class="ltx_text ltx_font_bold" id="A1.F7.1.5.4.1.1.1.1">biomedical</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A1.F7.1.5.4.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.F7.1.5.4.2.1">
<span class="ltx_p" id="A1.F7.1.5.4.2.1.1" style="width:346.9pt;">Though neither perfectly sensitive nor perfectly specific for trachoma, these signs have been essential tools for identifying populations that need interventions to eliminate trachoma as a public health problem.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Randomly selected English example sentences from each domain in the WMT22 metrics shared task as well as our new bio dataset.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Corpus Statistics</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A2.T4" title="Table 4 ‣ Appendix B Corpus Statistics ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Table 4</span></a> shows the size per language pair of our bio MQM dataset, as well as the WMT MQM dataset for comparison. The bio MQM dataset contains roughly 25k annotated segments, covering 11 language pairs. We split the data into test (roughly 75%) and development (roughly 25%) sets.</p>
</div>
<figure class="ltx_table" id="A2.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T4.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T4.3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T4.3.1.1.1" style="padding:0.5pt 3.4pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="2" id="A2.T4.3.1.1.2" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.3.1.1.2.1">WMT</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A2.T4.3.1.1.3" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.3.1.1.3.1">Bio</span></td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.2.2.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.3.2.2.1.1">Langs</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.2.2.2" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.3.2.2.2.1">Test</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.2.2.3" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.3.2.2.3.1">Train</span></th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.2.2.4" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.3.2.2.4.1">Test</span></td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.2.2.5" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.3.2.2.5.1">Dev</span></td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.2.2.6" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.3.2.2.6.1">Total</span></td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A2.T4.3.3.3.1" style="padding:0.5pt 3.4pt;">De-En</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A2.T4.3.3.3.2" style="padding:0.5pt 3.4pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A2.T4.3.3.3.3" style="padding:0.5pt 3.4pt;">-</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.3.3.3.4" style="padding:0.5pt 3.4pt;">2457</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.3.3.3.5" style="padding:0.5pt 3.4pt;">903</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.3.3.3.6" style="padding:0.5pt 3.4pt;">3360</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.4.4.1" style="padding:0.5pt 3.4pt;">En-De</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.4.4.2" style="padding:0.5pt 3.4pt;">18k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.4.4.3" style="padding:0.5pt 3.4pt;">28k</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.4.4.4" style="padding:0.5pt 3.4pt;">2695</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.4.4.5" style="padding:0.5pt 3.4pt;">917</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.4.4.6" style="padding:0.5pt 3.4pt;">3612</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.5.5.1" style="padding:0.5pt 3.4pt;">Es-En</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.5.5.2" style="padding:0.5pt 3.4pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.5.5.3" style="padding:0.5pt 3.4pt;">-</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.5.5.4" style="padding:0.5pt 3.4pt;">1013</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.5.5.5" style="padding:0.5pt 3.4pt;">309</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.5.5.6" style="padding:0.5pt 3.4pt;">1322</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.6.6.1" style="padding:0.5pt 3.4pt;">En-Es</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.6.6.2" style="padding:0.5pt 3.4pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.6.6.3" style="padding:0.5pt 3.4pt;">-</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.6.6.4" style="padding:0.5pt 3.4pt;">1112</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.6.6.5" style="padding:0.5pt 3.4pt;">330</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.6.6.6" style="padding:0.5pt 3.4pt;">1442</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.7.7.1" style="padding:0.5pt 3.4pt;">Ru-En</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.7.7.2" style="padding:0.5pt 3.4pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.7.7.3" style="padding:0.5pt 3.4pt;">-</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.7.7.4" style="padding:0.5pt 3.4pt;">1324</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.7.7.5" style="padding:0.5pt 3.4pt;">388</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.7.7.6" style="padding:0.5pt 3.4pt;">1712</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.8.8.1" style="padding:0.5pt 3.4pt;">En-Ru</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.8.8.2" style="padding:0.5pt 3.4pt;">19k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.8.8.3" style="padding:0.5pt 3.4pt;">16k</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.8.8.4" style="padding:0.5pt 3.4pt;">825</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.8.8.5" style="padding:0.5pt 3.4pt;">237</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.8.8.6" style="padding:0.5pt 3.4pt;">1062</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.9.9.1" style="padding:0.5pt 3.4pt;">Fr-En</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.9.9.2" style="padding:0.5pt 3.4pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.9.9.3" style="padding:0.5pt 3.4pt;">-</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.9.9.4" style="padding:0.5pt 3.4pt;">1108</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.9.9.5" style="padding:0.5pt 3.4pt;">352</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.9.9.6" style="padding:0.5pt 3.4pt;">1460</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.10.10.1" style="padding:0.5pt 3.4pt;">En-Fr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.10.10.2" style="padding:0.5pt 3.4pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.10.10.3" style="padding:0.5pt 3.4pt;">-</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.10.10.4" style="padding:0.5pt 3.4pt;">1228</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.10.10.5" style="padding:0.5pt 3.4pt;">308</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.10.10.6" style="padding:0.5pt 3.4pt;">1536</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.11.11.1" style="padding:0.5pt 3.4pt;">Zh-En</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.11.11.2" style="padding:0.5pt 3.4pt;">23k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.11.11.3" style="padding:0.5pt 3.4pt;">27k</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.11.11.4" style="padding:0.5pt 3.4pt;">2838</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.11.11.5" style="padding:0.5pt 3.4pt;">913</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.11.11.6" style="padding:0.5pt 3.4pt;">3751</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.12.12.1" style="padding:0.5pt 3.4pt;">En-Zh</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.12.12.2" style="padding:0.5pt 3.4pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.12.12.3" style="padding:0.5pt 3.4pt;">-</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.12.12.4" style="padding:0.5pt 3.4pt;">3900</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.12.12.5" style="padding:0.5pt 3.4pt;">1200</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.12.12.6" style="padding:0.5pt 3.4pt;">5100</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.13.13.1" style="padding:0.5pt 3.4pt;">Pt-En</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.13.13.2" style="padding:0.5pt 3.4pt;">-</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A2.T4.3.13.13.3" style="padding:0.5pt 3.4pt;">-</th>
<td class="ltx_td ltx_align_center" id="A2.T4.3.13.13.4" style="padding:0.5pt 3.4pt;">701</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.13.13.5" style="padding:0.5pt 3.4pt;">222</td>
<td class="ltx_td ltx_align_center" id="A2.T4.3.13.13.6" style="padding:0.5pt 3.4pt;">924</td>
</tr>
<tr class="ltx_tr" id="A2.T4.3.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A2.T4.3.14.14.1" style="padding:0.5pt 3.4pt;">All</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A2.T4.3.14.14.2" style="padding:0.5pt 3.4pt;">60k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A2.T4.3.14.14.3" style="padding:0.5pt 3.4pt;">71k</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T4.3.14.14.4" style="padding:0.5pt 3.4pt;">19k</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T4.3.14.14.5" style="padding:0.5pt 3.4pt;">6k</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A2.T4.3.14.14.6" style="padding:0.5pt 3.4pt;">25k</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Data split of the bio MQM data released in this work, and WMT22 MQM <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib8" title="">2022</a>)</cite> data. All test results are reported with the <span class="ltx_text ltx_font_italic" id="A2.T4.6.1">test</span> split which is approximately <math alttext="75\%" class="ltx_Math" display="inline" id="A2.T4.2.m1.1"><semantics id="A2.T4.2.m1.1b"><mrow id="A2.T4.2.m1.1.1" xref="A2.T4.2.m1.1.1.cmml"><mn id="A2.T4.2.m1.1.1.2" xref="A2.T4.2.m1.1.1.2.cmml">75</mn><mo id="A2.T4.2.m1.1.1.1" xref="A2.T4.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.T4.2.m1.1c"><apply id="A2.T4.2.m1.1.1.cmml" xref="A2.T4.2.m1.1.1"><csymbol cd="latexml" id="A2.T4.2.m1.1.1.1.cmml" xref="A2.T4.2.m1.1.1.1">percent</csymbol><cn id="A2.T4.2.m1.1.1.2.cmml" type="integer" xref="A2.T4.2.m1.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.2.m1.1d">75\%</annotation><annotation encoding="application/x-llamapun" id="A2.T4.2.m1.1e">75 %</annotation></semantics></math> of <span class="ltx_text ltx_font_italic" id="A2.T4.7.2">total</span>. Splits were created to respect document-level boundaries. For WMT, 2022 is used for testing and 2020 and 2021 for training.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Translator/Annotator Qualifications</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">There were 2-4 MQM annotators for each language pair, and a total of 46 annotators. All linguists had experience in translating/post-editing/reviewing content in the bio domain. This was the main requirement to be able to work on the project.
The other qualification criteria for this project were in line with the ISO standard 17100. In particular, the linguists met one or more of the following criteria:
(1) A recognized higher education degree in translation;
(2) Equivalent third-level degree in another subject plus a minimum of two years of documented professional translation experience;
(3) A minimum of five years of documented professional translation experience;
(4) Native speaker of the target language.
Although linguists were experts in the bio domain, not all of them were experts in MQM annotation. For this reason, the annotators completed an MQM quiz before onboarding them to ensure they understood the guidelines and requirements.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">For the translation and post-editing tasks, we used a two step process (initial post editor + reviewer). In each case the reviewer was a linguist with experience translating medical texts. There were no specific educational or vocational stipulations on that medical qualification, however they were asked to provide a medical-text-specific translation test for us to be onboarded for the project.
The initial post-editor in each case was a linguistic expert, but not specifically an expert in medical translations, which is why we followed up with reviewers to ensure contents were translated accurately.
Linguists had to demonstrate the following to onboard to the project:
(1) At least 3+ years of professional translation experience
(2) Proven proficiency in English writing skills
(3) In-depth understanding and exposure to the language
(4) Strong ability in translating, reviewing, adjusting, and providing adaptation for various writing styles of particular requests.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>MQM Annotation Guidelines</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">Below, we reproduce the MQM annotation guidelines that we provided to the annotators.</p>
</div>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Overview:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p1.1">You are asked to evaluate the translations using the guidelines below, and assign error categories and severities considering the context segments available.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Task:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px2.p1">
<ol class="ltx_enumerate" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.p1.1">Please identify all errors within each translated segment, <span class="ltx_text ltx_framed ltx_framed_underline" id="A4.I1.i1.p1.1.1">up to a maximum of five</span>.</p>
<ol class="ltx_enumerate" id="A4.I1.i1.I1">
<li class="ltx_item" id="A4.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="A4.I1.i1.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.I1.i1.p1.1">If there are more than five errors, identify only the five most severe.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="A4.I1.i1.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i1.I1.i2.p1.1">If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Unintelligible error that spans the entire segment</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="A4.I1.i1.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i1.I1.i3.p1.1">Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.p1.1">To identify an error, highlight the relevant span of text.</p>
<ol class="ltx_enumerate" id="A4.I1.i2.I1">
<li class="ltx_item" id="A4.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="A4.I1.i2.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i2.I1.i1.p1.1">Omission and Source error should be tagged in the source text.</p>
<ol class="ltx_enumerate" id="A4.I1.i2.I1.i1.I1">
<li class="ltx_item" id="A4.I1.i2.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">i.</span>
<div class="ltx_para" id="A4.I1.i2.I1.i1.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i2.I1.i1.I1.i1.p1.1">All other errors should be tagged in the target text.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="A4.I1.i2.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.I1.i2.p1.1">Unintelligible error should have an entire sentence tagged; if you think a smaller span is needed, then you should select another error category (Mistranslation, etc.).</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A4.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i3.p1.1">Select a category/sub-category and severity level from the available options.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A4.I1.i4.p1">
<p class="ltx_p" id="A4.I1.i4.p1.1">When identifying errors, please be as fine-grained as possible.</p>
<ol class="ltx_enumerate" id="A4.I1.i4.I1">
<li class="ltx_item" id="A4.I1.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="A4.I1.i4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i4.I1.i1.p1.1">If a sentence contains more than one error of the same category, each one should be logged separately. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="A4.I1.i4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i4.I1.i2.p1.1">If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe.</p>
<ol class="ltx_enumerate" id="A4.I1.i4.I1.i2.I1">
<li class="ltx_item" id="A4.I1.i4.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">i.</span>
<div class="ltx_para" id="A4.I1.i4.I1.i2.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i4.I1.i2.I1.i1.p1.1">If all have the same severity, choose the first matching category listed in the error typology (e.g. Accuracy, then Fluency, then Terminology, etc.).</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="A4.I1.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="A4.I1.i4.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i4.I1.i3.p1.1">For repetitive errors that appear systematically through the document: please annotate each instance with the appropriate weight.</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="A4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="A4.I1.i5.p1">
<p class="ltx_p" id="A4.I1.i5.p1.1">Please pay particular attention to the context when annotating. You will be shown several context segments before and after the segment for evaluation. If a translation is questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Delivery format:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px3.p1">
<ul class="ltx_itemize" id="A4.I2">
<li class="ltx_item" id="A4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i1.p1">
<p class="ltx_p" id="A4.I2.i1.p1.1">file format: a TSV with additional columns for error categories and severity + JSON</p>
<ul class="ltx_itemize" id="A4.I2.i1.I1">
<li class="ltx_item" id="A4.I2.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A4.I2.i1.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="A4.I2.i1.I1.i1.p1">
<p class="ltx_p" id="A4.I2.i1.I1.i1.p1.1">for multiple errors in one segment: additional row for each error + severity</p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A4.I2.i1.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="A4.I2.i1.I1.i2.p1">
<p class="ltx_p" id="A4.I2.i1.I1.i2.p1.1">text spans will be highlighted for the annotation process and exported as tag</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Error categories:</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A4.T5" title="Table 5 ‣ Error categories: ‣ Appendix D MQM Annotation Guidelines ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">5</span></a></p>
</div>
<figure class="ltx_table" id="A4.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A4.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T5.1.1.1">
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.1.1.1" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_top" id="A4.T5.1.1.1.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.1.1.3" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.1.1.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.1.1.4.1">
<span class="ltx_p" id="A4.T5.1.1.1.4.1.1" style="width:56.9pt;">Tag</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.2.2">
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.2.2.1" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_top" id="A4.T5.1.2.2.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.2.2.3" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.2.2.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.2.2.4.1">
<span class="ltx_p" id="A4.T5.1.2.2.4.1.1" style="width:56.9pt;">Location</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.3.3.1" rowspan="4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.3.3.1.1">
<span class="ltx_p" id="A4.T5.1.3.3.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="A4.T5.1.3.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A4.T5.1.3.3.1.1.1.1.1">Accuracy</span> – errors occurring when the target text does not accurately correspond to the propositional content of the source text, introduced by distorting, omitting, or adding to the message</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.3.3.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.3.3.2.1">
<span class="ltx_p" id="A4.T5.1.3.3.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.3.3.2.1.1.1">Mistranslation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.3.3.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.3.3.3.1">
<span class="ltx_p" id="A4.T5.1.3.3.3.1.1" style="width:170.7pt;">Target content that does not accurately represent the source content.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.3.3.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.3.3.4.1">
<span class="ltx_p" id="A4.T5.1.3.3.4.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.4.4.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.4.4.1.1">
<span class="ltx_p" id="A4.T5.1.4.4.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.4.4.1.1.1.1">Addition</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.4.4.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.4.4.2.1">
<span class="ltx_p" id="A4.T5.1.4.4.2.1.1" style="width:170.7pt;">Target content that includes content not present in the source.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.4.4.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.4.4.3.1">
<span class="ltx_p" id="A4.T5.1.4.4.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.5.5.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.5.5.1.1">
<span class="ltx_p" id="A4.T5.1.5.5.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.5.5.1.1.1.1">Omission</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.5.5.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.5.5.2.1">
<span class="ltx_p" id="A4.T5.1.5.5.2.1.1" style="width:170.7pt;">Errors where content is missing from the translation that is present in the source.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.5.5.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.5.5.3.1">
<span class="ltx_p" id="A4.T5.1.5.5.3.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A4.T5.1.5.5.3.1.1.1">Source</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.6.6.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.6.6.1.1">
<span class="ltx_p" id="A4.T5.1.6.6.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.6.6.1.1.1.1">Untranslated</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.6.6.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.6.6.2.1">
<span class="ltx_p" id="A4.T5.1.6.6.2.1.1" style="width:170.7pt;">Errors occurring when a text segment that was intended for translation is left untranslated in the target content.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.6.6.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.6.6.3.1">
<span class="ltx_p" id="A4.T5.1.6.6.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.7.7.1" rowspan="5" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.7.7.1.1">
<span class="ltx_p" id="A4.T5.1.7.7.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="A4.T5.1.7.7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A4.T5.1.7.7.1.1.1.1.1">Linguistic Conventions (former Fluency)</span> - errors related to the linguistic well-formedness of the text, including problems with, for instance, grammaticality and mechanical correctness.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.7.7.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.7.7.2.1">
<span class="ltx_p" id="A4.T5.1.7.7.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.7.7.2.1.1.1">Grammar</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.7.7.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.7.7.3.1">
<span class="ltx_p" id="A4.T5.1.7.7.3.1.1" style="width:170.7pt;">Error that occurs when a text string (sentence, phrase, other) in the translation violates the grammatical rules of the target language.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.7.7.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.7.7.4.1">
<span class="ltx_p" id="A4.T5.1.7.7.4.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.8.8.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.8.8.1.1">
<span class="ltx_p" id="A4.T5.1.8.8.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.8.8.1.1.1.1">Punctuation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.8.8.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.8.8.2.1">
<span class="ltx_p" id="A4.T5.1.8.8.2.1.1" style="width:170.7pt;">Punctuation incorrect for the locale or style.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.8.8.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.8.8.3.1">
<span class="ltx_p" id="A4.T5.1.8.8.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.9.9.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.9.9.1.1">
<span class="ltx_p" id="A4.T5.1.9.9.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.9.9.1.1.1.1">Spelling</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.9.9.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.9.9.2.1">
<span class="ltx_p" id="A4.T5.1.9.9.2.1.1" style="width:170.7pt;">Error occurring when the letters in a word in an alphabetic language are not arranged in the normally specified order.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.9.9.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.9.9.3.1">
<span class="ltx_p" id="A4.T5.1.9.9.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.10.10.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.10.10.1.1">
<span class="ltx_p" id="A4.T5.1.10.10.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.10.10.1.1.1.1">Character encoding</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.10.10.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.10.10.2.1">
<span class="ltx_p" id="A4.T5.1.10.10.2.1.1" style="width:170.7pt;">Error occurring when characters garbled due to incorrect application of an encoding.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.10.10.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.10.10.3.1">
<span class="ltx_p" id="A4.T5.1.10.10.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.11.11.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.11.11.1.1">
<span class="ltx_p" id="A4.T5.1.11.11.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.11.11.1.1.1.1">Register</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.11.11.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.11.11.2.1">
<span class="ltx_p" id="A4.T5.1.11.11.2.1.1" style="width:170.7pt;">Errors occurring when a text uses a level of formality higher or lower than required by the specifications or by common language conventions.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.11.11.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.11.11.3.1">
<span class="ltx_p" id="A4.T5.1.11.11.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.12.12.1" rowspan="2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.12.12.1.1">
<span class="ltx_p" id="A4.T5.1.12.12.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="A4.T5.1.12.12.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A4.T5.1.12.12.1.1.1.1.1">Terminology</span> - errors arising when a term does not conform to normative domain or organizational terminology standards or when a term in the target text is not the correct, normative equivalent of the corresponding term in the source text.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_t" id="A4.T5.1.12.12.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.12.12.3" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_top ltx_border_t" id="A4.T5.1.12.12.4" style="padding:0.5pt 3.4pt;"></td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.13.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.13.13.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.13.13.1.1">
<span class="ltx_p" id="A4.T5.1.13.13.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.13.13.1.1.1.1">Inconsistent use of terminology</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.13.13.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.13.13.2.1">
<span class="ltx_p" id="A4.T5.1.13.13.2.1.1" style="width:170.7pt;">Use of multiple terms for the same concept (technical terms, medical terms, etc.)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.13.13.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.13.13.3.1">
<span class="ltx_p" id="A4.T5.1.13.13.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.14.14">
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.14.14.1" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.14.14.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.14.14.2.1">
<span class="ltx_p" id="A4.T5.1.14.14.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.14.14.2.1.1.1">Wrong term</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A4.T5.1.14.14.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.14.14.3.1">
<span class="ltx_p" id="A4.T5.1.14.14.3.1.1" style="width:170.7pt;">Use of term that it is not the term a domain expert would use or because it gives rise to a conceptual mismatch.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.14.14.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.14.14.4.1">
<span class="ltx_p" id="A4.T5.1.14.14.4.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.15.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.15.15.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.15.15.1.1">
<span class="ltx_p" id="A4.T5.1.15.15.1.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.15.15.1.1.1.1">Style</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.15.15.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.15.15.2.1">
<span class="ltx_p" id="A4.T5.1.15.15.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.15.15.2.1.1.1">Non-fluent</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.15.15.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.15.15.3.1">
<span class="ltx_p" id="A4.T5.1.15.15.3.1.1" style="width:170.7pt;">Text does not sound fluent or natural as if it were translated by a non-native speaker or because the translation is following the source too closely.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.15.15.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.15.15.4.1">
<span class="ltx_p" id="A4.T5.1.15.15.4.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.16.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.16.16.1" rowspan="7" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.16.16.1.1">
<span class="ltx_p" id="A4.T5.1.16.16.1.1.1" style="width:113.8pt;"><span class="ltx_text" id="A4.T5.1.16.16.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A4.T5.1.16.16.1.1.1.1.1">Locale Conventions</span> - errors occurring when the translation product violates locale-specific content or formatting requirements for data elements.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.16.16.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.16.16.2.1">
<span class="ltx_p" id="A4.T5.1.16.16.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.16.16.2.1.1.1">Number format</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.16.16.3" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.16.16.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.16.16.4.1">
<span class="ltx_p" id="A4.T5.1.16.16.4.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.17.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.17.17.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.17.17.1.1">
<span class="ltx_p" id="A4.T5.1.17.17.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.17.17.1.1.1.1">Currency format</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.17.17.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.17.17.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.17.17.3.1">
<span class="ltx_p" id="A4.T5.1.17.17.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.18.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.18.18.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.18.18.1.1">
<span class="ltx_p" id="A4.T5.1.18.18.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.18.18.1.1.1.1">Measurement format</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.18.18.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.18.18.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.18.18.3.1">
<span class="ltx_p" id="A4.T5.1.18.18.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.19.19">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.19.19.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.19.19.1.1">
<span class="ltx_p" id="A4.T5.1.19.19.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.19.19.1.1.1.1">Time format</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.19.19.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.19.19.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.19.19.3.1">
<span class="ltx_p" id="A4.T5.1.19.19.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.20.20">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.20.20.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.20.20.1.1">
<span class="ltx_p" id="A4.T5.1.20.20.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.20.20.1.1.1.1">Date format</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.20.20.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.20.20.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.20.20.3.1">
<span class="ltx_p" id="A4.T5.1.20.20.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.21.21">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.21.21.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.21.21.1.1">
<span class="ltx_p" id="A4.T5.1.21.21.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.21.21.1.1.1.1">Address format</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.21.21.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.21.21.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.21.21.3.1">
<span class="ltx_p" id="A4.T5.1.21.21.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.22.22">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.22.22.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.22.22.1.1">
<span class="ltx_p" id="A4.T5.1.22.22.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.22.22.1.1.1.1">Telephone format</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_r" id="A4.T5.1.22.22.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A4.T5.1.22.22.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.22.22.3.1">
<span class="ltx_p" id="A4.T5.1.22.22.3.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.23.23">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.23.23.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.23.23.1.1">
<span class="ltx_p" id="A4.T5.1.23.23.1.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.23.23.1.1.1.1">Other</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_t" id="A4.T5.1.23.23.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.23.23.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.23.23.3.1">
<span class="ltx_p" id="A4.T5.1.23.23.3.1.1" style="width:170.7pt;">any error that does not fit the categories above</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.23.23.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.23.23.4.1">
<span class="ltx_p" id="A4.T5.1.23.23.4.1.1" style="width:56.9pt;">Target</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.24.24">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.24.24.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.24.24.1.1">
<span class="ltx_p" id="A4.T5.1.24.24.1.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.24.24.1.1.1.1">Source errors</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.24.24.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.24.24.2.1">
<span class="ltx_p" id="A4.T5.1.24.24.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.24.24.2.1.1.1">source error</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.24.24.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.24.24.3.1">
<span class="ltx_p" id="A4.T5.1.24.24.3.1.1" style="width:170.7pt;">The error that occurs in the source. All source errors (e.g. non-fluent source) should be annotated as source errors — no sub-categories need to be selected.
<span class="ltx_text ltx_font_bold" id="A4.T5.1.24.24.3.1.1.1">If the source error caused a target error: </span>
- if the source error and target errors belong to the same category, then only flag the source.
-If source and target errors belong to different categories - even if you know that the source error caused the translation error - do flag both.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.24.24.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.24.24.4.1">
<span class="ltx_p" id="A4.T5.1.24.24.4.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A4.T5.1.24.24.4.1.1.1">Source</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.25.25">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.25.25.1" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.25.25.1.1">
<span class="ltx_p" id="A4.T5.1.25.25.1.1.1" style="width:113.8pt;"><span class="ltx_text ltx_font_bold" id="A4.T5.1.25.25.1.1.1.1">Unintelligible</span></span>
</span>
</td>
<td class="ltx_td ltx_align_top ltx_border_t" id="A4.T5.1.25.25.2" style="padding:0.5pt 3.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T5.1.25.25.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.25.25.3.1">
<span class="ltx_p" id="A4.T5.1.25.25.3.1.1" style="width:170.7pt;">So many errors, or errors are so outrageous, that text becomes incomprehensible, and it is hard to pinpoint a specific error type.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T5.1.25.25.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T5.1.25.25.4.1">
<span class="ltx_p" id="A4.T5.1.25.25.4.1.1" style="width:56.9pt;">Target. Tag the entire sentence. If the span is smaller, then a different category should be applied, such as Mistranslation, Untranslated, etc.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>MQM error categories provided in annotator instructions.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Severity (no weights, just severity):</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px5.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A4.T6" title="Table 6 ‣ Severity (no weights, just severity): ‣ Appendix D MQM Annotation Guidelines ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">6</span></a></p>
</div>
<figure class="ltx_table" id="A4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A4.T6.1.1.1.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.1.1.1.1">severity</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="A4.T6.1.1.1.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.1.1.2.1">
<span class="ltx_p" id="A4.T6.1.1.1.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.1.1.2.1.1.1">Definition</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="A4.T6.1.1.1.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.1.1.3.1">
<span class="ltx_p" id="A4.T6.1.1.1.3.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.1.1.3.1.1.1">Source example</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="A4.T6.1.1.1.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.1.1.4.1">
<span class="ltx_p" id="A4.T6.1.1.1.4.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.1.1.4.1.1.1">Translation example</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T6.1.2.1.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.2.1.1.1">Neutral</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.2.1.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.2.1.2.1">
<span class="ltx_p" id="A4.T6.1.2.1.2.1.1" style="width:170.7pt;">Neutral issues are items that need to be noted for further attention or fixing but which should not count against the translation. This severity level can be perceived as a flag for attention that does not impose a penalty.
It should be used for “preferential errors” (i.e, items that are not wrong, per se, but where the reviewer or requester would like to see a different solution).</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.2.1.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.2.1.3.1">
<span class="ltx_p" id="A4.T6.1.2.1.3.1.1" style="width:85.4pt;">Source: Join us in celebrating 10 years of the company!</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.2.1.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.2.1.4.1">
<span class="ltx_p" id="A4.T6.1.2.1.4.1.1" style="width:85.4pt;">Target: Join us to celebrate 10 years of the company!</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T6.1.3.2.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.3.2.1.1">Minor</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.3.2.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.3.2.2.1">
<span class="ltx_p" id="A4.T6.1.3.2.2.1.1" style="width:170.7pt;">Minor issues are issues that do not impact usability or understandability of the content. If the typical reader/user is able to correct the error reliably and it does not impact the usability of the content, it should be classified as minor.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.3.2.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.3.2.3.1">
<span class="ltx_p" id="A4.T6.1.3.2.3.1.1" style="width:85.4pt;">S1: Accurately distinguish between legitimate and high<span class="ltx_text ltx_font_bold" id="A4.T6.1.3.2.3.1.1.1">-r</span>isk account registrations</span>
<span class="ltx_p" id="A4.T6.1.3.2.3.1.2">S2: See how organizations worldwide are using fraud detection.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.3.2.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.3.2.4.1">
<span class="ltx_p" id="A4.T6.1.3.2.4.1.1" style="width:85.4pt;">T1: Accurately distinguish between legitimate and high<span class="ltx_text ltx_font_bold" id="A4.T6.1.3.2.4.1.1.1">- r</span>isk account registrations</span>
<span class="ltx_p" id="A4.T6.1.3.2.4.1.2">T2: See how organization worldwide are using fraud detection.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T6.1.4.3.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.4.3.1.1">Major</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.4.3.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.4.3.2.1">
<span class="ltx_p" id="A4.T6.1.4.3.2.1.1" style="width:170.7pt;">errors that would impact usability or understandability of the content but which would not render it unusable. For example, a misspelled word that may require extra effort for the reader to understand the intended meaning but does not make it impossible to comprehend should be labeled as a major error. Additionally, if an error cannot be reliably corrected by the reader/user (e.g., the intended meaning is not clear) but it does not render the content unfit for purpose, it should be categorized as major.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.4.3.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.4.3.3.1">
<span class="ltx_p" id="A4.T6.1.4.3.3.1.1" style="width:85.4pt;">Source: Set the performance to 50 percent</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.4.3.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.4.3.4.1">
<span class="ltx_p" id="A4.T6.1.4.3.4.1.1" style="width:85.4pt;">Target: Set performance 50 percent</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T6.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T6.1.5.4.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A4.T6.1.5.4.1.1">Critical</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.5.4.2" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.5.4.2.1">
<span class="ltx_p" id="A4.T6.1.5.4.2.1.1" style="width:170.7pt;">errors that would render a text unusable, which is determined by considering the intended audience and specified purpose. For example, a particularly bad grammar error that changes the meaning of the text would be considered Critical. Critical errors could result in damage to people, equipment, or an organization’s reputation if not corrected before use. If the error causes the text to become unintelligible, it would be considered Critical.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.5.4.3" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.5.4.3.1">
<span class="ltx_p" id="A4.T6.1.5.4.3.1.1" style="width:85.4pt;">S1: Set the device on the highest temperature setting.</span>
<span class="ltx_p" id="A4.T6.1.5.4.3.1.2">S2: The next step would be to identify the point of leakage.</span>
<span class="ltx_p" id="A4.T6.1.5.4.3.1.3">S3: 1.3 degrees</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A4.T6.1.5.4.4" style="padding:0.5pt 3.4pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T6.1.5.4.4.1">
<span class="ltx_p" id="A4.T6.1.5.4.4.1.1" style="width:85.4pt;">T1: Set the device on the lowest temperature setting.</span>
<span class="ltx_p" id="A4.T6.1.5.4.4.1.2">T2: It would be to identify the next point of leakage.</span>
<span class="ltx_p" id="A4.T6.1.5.4.4.1.3">T3: 1,300 degrees</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Severity examples and explanations provided in MQM annotation instructions.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Supplementary Information on Experiments</h2>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Training Steps and Compute Time for Experiments</h3>
<div class="ltx_para" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.1">The overall training consists of the following steps (compute times using a single A10 GPU). The times are per epoch and some experiments require training for multiple epochs.</p>
</div>
<div class="ltx_para" id="A5.SS1.p2">
<ul class="ltx_itemize" id="A5.I1">
<li class="ltx_item" id="A5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A5.I1.i1.p1">
<p class="ltx_p" id="A5.I1.i1.p1.1">Language modeling <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A5.I1.i1.p1.1.m1.1"><semantics id="A5.I1.i1.p1.1.m1.1a"><mo id="A5.I1.i1.p1.1.m1.1.1" stretchy="false" xref="A5.I1.i1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.1.m1.1b"><ci id="A5.I1.i1.p1.1.m1.1.1.cmml" xref="A5.I1.i1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i1.p1.1.m1.1d">→</annotation></semantics></math> XLM-Roberta, 10hr/ep.</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A5.I1.i2.p1">
<p class="ltx_p" id="A5.I1.i2.p1.1">DA scores regression <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A5.I1.i2.p1.1.m1.1"><semantics id="A5.I1.i2.p1.1.m1.1a"><mo id="A5.I1.i2.p1.1.m1.1.1" stretchy="false" xref="A5.I1.i2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.1.m1.1b"><ci id="A5.I1.i2.p1.1.m1.1.1.cmml" xref="A5.I1.i2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i2.p1.1.m1.1d">→</annotation></semantics></math> <span class="ltx_text ltx_font_smallcaps" id="A5.I1.i2.p1.1.1">CometDA</span>, 10hr/ep.</p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A5.I1.i3.p1">
<p class="ltx_p" id="A5.I1.i3.p1.1">MQM scores regression <math alttext="\rightarrow" class="ltx_Math" display="inline" id="A5.I1.i3.p1.1.m1.1"><semantics id="A5.I1.i3.p1.1.m1.1a"><mo id="A5.I1.i3.p1.1.m1.1.1" stretchy="false" xref="A5.I1.i3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.1.m1.1b"><ci id="A5.I1.i3.p1.1.m1.1.1.cmml" xref="A5.I1.i3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i3.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i3.p1.1.m1.1d">→</annotation></semantics></math> <span class="ltx_text ltx_font_smallcaps" id="A5.I1.i3.p1.1.1">Comet</span>, 1hr/ep.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>List of Data for Fine-Tuning Pre-Trained Model</h3>
<div class="ltx_para" id="A5.SS2.p1">
<p class="ltx_p" id="A5.SS2.p1.1">For WMT domain, we used news-commentary v18.1 dataset<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_href" href="https://data.statmt.org/news-commentary/v18.1/" title=""><span class="ltx_ref ltx_nolink">data.statmt.org/news-commentary/v18.1/</span></a></span></span></span> for all languages.
For the bio domain, we list the data in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#A5.T7" title="Table 7 ‣ E.2 List of Data for Fine-Tuning Pre-Trained Model ‣ Appendix E Supplementary Information on Experiments ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure class="ltx_table" id="A5.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T7.1" style="width:330.3pt;height:174.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.3pt,26.1pt) scale(0.77,0.77) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T7.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T7.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T7.1.1.1.1.1" style="padding:0.5pt 3.4pt;">Data Type</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T7.1.1.1.1.2" style="padding:0.5pt 3.4pt;">Language(s)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T7.1.1.1.1.3" style="padding:0.5pt 3.4pt;">Dataset</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T7.1.1.1.1.4" style="padding:0.5pt 3.4pt;">Lines</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T7.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T7.1.1.2.1.1" rowspan="4" style="padding:0.5pt 3.4pt;"><span class="ltx_text" id="A5.T7.1.1.2.1.1.1">Parallel</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T7.1.1.2.1.2" style="padding:0.5pt 3.4pt;">en-de</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T7.1.1.2.1.3" style="padding:0.5pt 3.4pt;">
<table class="ltx_tabular ltx_align_middle" id="A5.T7.1.1.2.1.3.1">
<tr class="ltx_tr" id="A5.T7.1.1.2.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.2.1.3.1.1.1" style="padding:0.5pt 3.4pt;">UFAL Medical Corpus</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.2.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.2.1.3.1.2.1" style="padding:0.5pt 3.4pt;"><cite class="ltx_cite ltx_citemacro_cite">Yeganova et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib32" title="">2021</a>)</cite></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A5.T7.1.1.2.1.4" style="padding:0.5pt 3.4pt;">3M</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.3.2">
<td class="ltx_td ltx_align_left" id="A5.T7.1.1.3.2.1" style="padding:0.5pt 3.4pt;">en-de</td>
<td class="ltx_td ltx_align_left" id="A5.T7.1.1.3.2.2" rowspan="3" style="padding:0.5pt 3.4pt;"><span class="ltx_text" id="A5.T7.1.1.3.2.2.1">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.1.1.3.2.2.1.1">
<span class="ltx_tr" id="A5.T7.1.1.3.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.3.2.2.1.1.1.1" style="padding:0.5pt 3.4pt;">MEDLINE</span></span>
<span class="ltx_tr" id="A5.T7.1.1.3.2.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.3.2.2.1.1.2.1" style="padding:0.5pt 3.4pt;"><cite class="ltx_cite ltx_citemacro_cite">Yeganova et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib32" title="">2021</a>)</cite></span></span>
</span></span></td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.3.2.3" style="padding:0.5pt 3.4pt;">35k</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.4.3">
<td class="ltx_td ltx_align_left" id="A5.T7.1.1.4.3.1" style="padding:0.5pt 3.4pt;">en-ru</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.4.3.2" style="padding:0.5pt 3.4pt;">29k</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.5.4">
<td class="ltx_td ltx_align_left" id="A5.T7.1.1.5.4.1" style="padding:0.5pt 3.4pt;">en-zh</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.5.4.2" style="padding:0.5pt 3.4pt;">19k</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.T7.1.1.6.5.1" rowspan="4" style="padding:0.5pt 3.4pt;"><span class="ltx_text" id="A5.T7.1.1.6.5.1.1">Monoling.</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T7.1.1.6.5.2" style="padding:0.5pt 3.4pt;">En</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T7.1.1.6.5.3" style="padding:0.5pt 3.4pt;">CORD <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib31" title="">2020</a>)</cite>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A5.T7.1.1.6.5.4" style="padding:0.5pt 3.4pt;">1M</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.7.6">
<td class="ltx_td ltx_align_left" id="A5.T7.1.1.7.6.1" style="padding:0.5pt 3.4pt;">De</td>
<td class="ltx_td ltx_align_left" id="A5.T7.1.1.7.6.2" style="padding:0.5pt 3.4pt;">
<table class="ltx_tabular ltx_align_middle" id="A5.T7.1.1.7.6.2.1">
<tr class="ltx_tr" id="A5.T7.1.1.7.6.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.7.6.2.1.1.1" style="padding:0.5pt 3.4pt;">Animal Experiments<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_href" href="https://www.openagrar.de/receive/openagrar_mods_00046540?lang=en" title=""><span class="ltx_ref ltx_nolink">www.openagrar.de/receive/openagrar_mods_00046540?lang=en</span></a></span></span></span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.7.6.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.7.6.2.1.2.1" style="padding:0.5pt 3.4pt;">GERNERMED</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.7.6.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.7.6.2.1.3.1" style="padding:0.5pt 3.4pt;"><cite class="ltx_cite ltx_citemacro_cite">Frei and Kramer (<a class="ltx_ref" href="https://arxiv.org/html/2402.18747v2#bib.bib5" title="">2023</a>)</cite></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.7.6.3" style="padding:0.5pt 3.4pt;">250k</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.8.7">
<td class="ltx_td ltx_align_left" id="A5.T7.1.1.8.7.1" style="padding:0.5pt 3.4pt;">Ru</td>
<td class="ltx_td ltx_align_left" id="A5.T7.1.1.8.7.2" style="padding:0.5pt 3.4pt;">Medical QA</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T7.1.1.8.7.3" style="padding:0.5pt 3.4pt;">250k</td>
</tr>
<tr class="ltx_tr" id="A5.T7.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A5.T7.1.1.9.8.1" style="padding:0.5pt 3.4pt;">Zh</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A5.T7.1.1.9.8.2" style="padding:0.5pt 3.4pt;">Chinese Medical Dataset<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/shibing624/medical" title=""><span class="ltx_ref ltx_nolink">huggingface.co/datasets/shibing624/medical</span></a></span></span></span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb" id="A5.T7.1.1.9.8.3" style="padding:0.5pt 3.4pt;">2M</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Collection of bio domain data used in pre-trained model fine-tuning experiments.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Raw Scores for <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S3.F2" title="Figure 2 ‣ 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>
</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">The segment-level correlation (Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="A6.p1.1.m1.1"><semantics id="A6.p1.1.m1.1a"><mi id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><ci id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A6.p1.1.m1.1d">italic_τ</annotation></semantics></math>) scores used to compute improvements in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#S3.F2" title="Figure 2 ‣ 3 New Bio MQM Dataset ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> are provided in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.18747v2#A6.T8" title="Table 8 ‣ Appendix F Raw Scores for Figure 2 ‣ Ethical Considerations ‣ Acknowledgements ‣ Limitations ‣ 5 Conclusion and Future Work ‣ Observations: NLLB. ‣ 4.3 How does the pre-trained model affect domain robustness? ‣ Observation 2: In-domain data dramatically improves Comet. ‣ Observation 1: Domain gap persists throughout the fine-tuning process. ‣ 4.2 How does fine-tuning affect domain robustness? ‣ 4 Analysis ‣ Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains"><span class="ltx_text ltx_ref_tag">Table 8</span></a>. Note that there is no public <span class="ltx_text ltx_font_smallcaps" id="A6.p1.1.1">Comet</span> 22 MQM model.</p>
</div>
<figure class="ltx_table" id="A6.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A6.T8.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A6.T8.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A6.T8.3.1.1.1" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A6.T8.3.1.1.1.1">Type</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A6.T8.3.1.1.2" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A6.T8.3.1.1.2.1">Metric</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T8.3.1.1.3" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A6.T8.3.1.1.3.1">Test:WMT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T8.3.1.1.4" style="padding:0.5pt 3.4pt;"><span class="ltx_text ltx_font_bold" id="A6.T8.3.1.1.4.1">Test:Bio</span></td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A6.T8.3.2.2.1" rowspan="3" style="padding:0.5pt 3.4pt;"><span class="ltx_text" id="A6.T8.3.2.2.1.1">Surface-Form</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A6.T8.3.2.2.2" style="padding:0.5pt 3.4pt;">BLEU</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T8.3.2.2.3" style="padding:0.5pt 3.4pt;">0.134</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T8.3.2.2.4" style="padding:0.5pt 3.4pt;">0.213</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.3.3.1" style="padding:0.5pt 3.4pt;">ChrF</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.3.3.2" style="padding:0.5pt 3.4pt;">0.151</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.3.3.3" style="padding:0.5pt 3.4pt;">0.192</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.4.4.1" style="padding:0.5pt 3.4pt;">TER</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.4.4.2" style="padding:0.5pt 3.4pt;">0.140</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.4.4.3" style="padding:0.5pt 3.4pt;">0.100</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.5.5.1" rowspan="3" style="padding:0.5pt 3.4pt;"><span class="ltx_text" id="A6.T8.3.5.5.1.1">Pre-trained+Algorithm</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A6.T8.3.5.5.2" style="padding:0.5pt 3.4pt;">PRISMREF</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T8.3.5.5.3" style="padding:0.5pt 3.4pt;">0.216</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T8.3.5.5.4" style="padding:0.5pt 3.4pt;">0.242</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.6.6.1" style="padding:0.5pt 3.4pt;">PRISMSRC</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.6.6.2" style="padding:0.5pt 3.4pt;">0.121</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.6.6.3" style="padding:0.5pt 3.4pt;">0.267</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.7.7.1" style="padding:0.5pt 3.4pt;">BERTScore</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.7.7.2" style="padding:0.5pt 3.4pt;">0.216</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.7.7.3" style="padding:0.5pt 3.4pt;">0.227</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.8.8.1" rowspan="2" style="padding:0.5pt 3.4pt;"><span class="ltx_text" id="A6.T8.3.8.8.1.1">Pre-trained+Prompt</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A6.T8.3.8.8.2" style="padding:0.5pt 3.4pt;">GEMBADAV3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T8.3.8.8.3" style="padding:0.5pt 3.4pt;">0.280</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T8.3.8.8.4" style="padding:0.5pt 3.4pt;">0.159</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.9.9.1" style="padding:0.5pt 3.4pt;">GEMBADAV3.QE</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.9.9.2" style="padding:0.5pt 3.4pt;">0.222</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.9.9.3" style="padding:0.5pt 3.4pt;">0.173</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A6.T8.3.10.10.1" rowspan="7" style="padding:0.5pt 3.4pt;"><span class="ltx_text" id="A6.T8.3.10.10.1.1">Pre-trained+Fine-tuned</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A6.T8.3.10.10.2" style="padding:0.5pt 3.4pt;">COMETMQM.21</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T8.3.10.10.3" style="padding:0.5pt 3.4pt;">0.328</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T8.3.10.10.4" style="padding:0.5pt 3.4pt;">0.249</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.11.11.1" style="padding:0.5pt 3.4pt;">COMETQE.21</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.11.11.2" style="padding:0.5pt 3.4pt;">0.294</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.11.11.3" style="padding:0.5pt 3.4pt;">0.205</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.12.12.1" style="padding:0.5pt 3.4pt;">COMETDA.21</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.12.12.2" style="padding:0.5pt 3.4pt;">0.309</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.12.12.3" style="padding:0.5pt 3.4pt;">0.284</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.13.13.1" style="padding:0.5pt 3.4pt;">COMETINHO.21</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.13.13.2" style="padding:0.5pt 3.4pt;">0.255</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.13.13.3" style="padding:0.5pt 3.4pt;">0.182</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.14.14.1" style="padding:0.5pt 3.4pt;">COMETDA.22</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.14.14.2" style="padding:0.5pt 3.4pt;">0.304</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.14.14.3" style="padding:0.5pt 3.4pt;">0.269</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T8.3.15.15.1" style="padding:0.5pt 3.4pt;">UniTE</th>
<td class="ltx_td ltx_align_center" id="A6.T8.3.15.15.2" style="padding:0.5pt 3.4pt;">0.301</td>
<td class="ltx_td ltx_align_center" id="A6.T8.3.15.15.3" style="padding:0.5pt 3.4pt;">0.249</td>
</tr>
<tr class="ltx_tr" id="A6.T8.3.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A6.T8.3.16.16.1" style="padding:0.5pt 3.4pt;">BLEURT</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T8.3.16.16.2" style="padding:0.5pt 3.4pt;">0.214</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T8.3.16.16.3" style="padding:0.5pt 3.4pt;">0.100</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Segment-level correlation (Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="A6.T8.2.m1.1"><semantics id="A6.T8.2.m1.1b"><mi id="A6.T8.2.m1.1.1" xref="A6.T8.2.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A6.T8.2.m1.1c"><ci id="A6.T8.2.m1.1.1.cmml" xref="A6.T8.2.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T8.2.m1.1d">\tau</annotation><annotation encoding="application/x-llamapun" id="A6.T8.2.m1.1e">italic_τ</annotation></semantics></math>) between metrics and human judgments on the WMT and bio domain.
<em class="ltx_emph ltx_font_italic" id="A6.T8.7.1">Pre-trained+Fine-tuned</em> metrics have lower correlation on bio than on WMT,
while <em class="ltx_emph ltx_font_italic" id="A6.T8.8.2">Surface-Form</em> and <em class="ltx_emph ltx_font_italic" id="A6.T8.9.3">Pre-trained+Algorithm</em> tend to
have higher correlation. </figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun  4 04:11:46 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
