<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis</title>
<!--Generated on Thu Oct 10 21:58:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.08397v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S1" title="In VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S2" title="In VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S2.SS1" title="In 2 Related Work â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Brain Region Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S2.SS2" title="In 2 Related Work â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Learning Across Medical Imaging Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S2.SS3" title="In 2 Related Work â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Medical Vision-Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S2.SS4" title="In 2 Related Work â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Language Models as Agents</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S3" title="In VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S3.SS1" title="In 3 Method â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S3.SS2" title="In 3 Method â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Supervised Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S3.SS3" title="In 3 Method â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Implementation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S4" title="In VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Task and Data Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S4.SS1" title="In 4 Task and Data Design â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S4.SS2" title="In 4 Task and Data Design â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Prompt Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S4.SS3" title="In 4 Task and Data Design â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Images</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5" title="In VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.SS1" title="In 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Illustrative Use Cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.SS2" title="In 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Segmentation Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.SS3" title="In 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Pathology Characterization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S6" title="In VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS1" title="In Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Native-Space Convolutions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS2" title="In Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>List of Anatomical Structures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS3" title="In Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Lesion Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS4" title="In Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Segmentation Analysis Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS5" title="In Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Classification Analysis Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS6" title="In Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Zero-Shot Baseline Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS7" title="In Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.7 </span>Radiopaedia Data</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">VoxelPrompt: A Vision-Language Agent for
<br class="ltx_break"/> Grounded Medical Image Analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">
Andrew HoopesÂ <sup class="ltx_sup" id="id1.1.id1.1">1,2</sup>,Â 
Victor Ion ButoiÂ <sup class="ltx_sup" id="id1.1.id1.2">1</sup>,Â 
John V.Â GuttagÂ <sup class="ltx_sup" id="id1.1.id1.3">1</sup>,Â 
Adrian V.Â DalcaÂ <sup class="ltx_sup" id="id1.1.id1.4">1,2,3</sup>
<br class="ltx_break"/></span>
<span class="ltx_text ltx_font_bold" id="id2.2.id2" style="font-size:90%;">1.<span class="ltx_text ltx_font_medium" id="id2.2.id2.1">Â Massachusetts Institute of Technology,
</span>2.<span class="ltx_text ltx_font_medium" id="id2.2.id2.2">Â Massachusetts General Hospital,
</span>3.<span class="ltx_text ltx_font_medium" id="id2.2.id2.3">Â Harvard Medical School
</span></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1"><span class="ltx_text ltx_font_bold" id="id3.id1.1" style="font-size:90%;">We present VoxelPrompt, an agent-driven vision-language framework that tackles diverse radiological tasks through joint modelling of natural language, image volumes, and analytical metrics. VoxelPrompt is multi-modal and versatile, leveraging the flexibility of language interaction while providing quantitatively grounded image analysis. Given a variable number of 3D medical volumes, such as MRI and CT scans, VoxelPrompt employs a language agent that iteratively predicts executable instructions to solve a task specified by an input prompt. These instructions communicate with a vision network to encode image features and generate volumetric outputs (e.g., segmentations). VoxelPrompt interprets the results of intermediate instructions and plans further actions to compute discrete measures (e.g., tumor growth across a series of scans) and present relevant outputs to the user. We evaluate this framework in a sandbox of diverse neuroimaging tasks, and we show that the single VoxelPrompt model can delineate hundreds of anatomical and pathological features, measure many complex morphological properties, and perform open-language analysis of lesion characteristics. VoxelPrompt carries out these objectives with accuracy similar to that of fine-tuned, single-task models for segmentation and visual question-answering, while facilitating a much larger range of tasks. Therefore, by supporting accurate image processing with language interaction, VoxelPrompt provides comprehensive utility for numerous imaging tasks that traditionally require specialized models to address.


<br class="ltx_break"/>Keywords: Vision-Language Agent, Medical Image Analysis, Neuroimage Foundation Model</span></p>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_rule" style="width:433.6pt;height:0.5pt;background:black;display:inline-block;">Â </span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Automated methods for medical image analysis increasingly use learning-based techniques to address clinical and research aims. Typically, these methods are tailored to a narrow set of computational objectives and output modalities (e.g.Â domain-specific segmentations, classification of tumors, or natural language characterizations). This specialization limits the broad use of deep-learning advancements in radiology, by forcing clinicians and researchers to navigate a fragmented collection of inflexible tools â€“ often failing to find one that meets their need.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">For example, a practitioner might need to analyze the growth of a specific tumor in a patient exhibiting multiple lesions and abnormalities. While many tumor segmentation models exist, none enable the user to easily specify and differentiate the desired pathology, for instance, based on descriptors such as relative location, size, or signal intensity. Moreover, these methods rarely extend to cover the downstream computational steps required to extract meaningful analytical metrics. This ultimately leaves the burden of task-specific post-processing to the users.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we present a mechanism that supports a <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">unified</span> medical imaging model, capable of conducting interactive, end-to-end analysis across a spectrum of goals. We introduce VoxelPrompt, a system that, in response to a text prompt, processes volumetricÂ (3D) medical scans and outputs varied modalities, including language, volumes, and computed metrics. With this system, a single model can learn to localize precise anatomical and pathological regions of interest (ROIs), carry out complex computational measurements that relate multiple scans to one another, and perform open-language characterization of image featuresÂ (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="789" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Examples from the diverse set of tasks supported by the VoxelPrompt framework. For each example, we show the input prompt (gray) above the input images(s). VoxelPrompt annotates the images and generates language responses (shown in purple). These scans are processed entirely in 3D, but here we show only a single extracted slice.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">VoxelPrompt features an image encoder, image generator, and language model, all trained jointly. The language model functions as a planning agent, iteratively predicting instructions as executable code and interpreting its outcomes to achieve a target goal. The dynamically-evaluated instructions orchestrate the encoding and generation of spatial features (e.g., segmentations), incorporate natural language responses, and access a predefined library of functions to compute and provide outputs to the user.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We implement an instructable convolutional network that integrates language model embeddings to promote fine-grained, language-conditioned visual analysis. This vision network includes attention-based interaction of features across a volume sequence of <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">any length</span> to accommodate adaptive analysis in multi-acquisition and longitudinal scenarios. Unlike typical volumetric processing models, which are largely restricted to a fixed number of image channels and voxel spacing, VoxelPrompt supports a variable number of inputs and performs convolutions in the native acquisition resolution.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We evaluate the capability of the VoxelPrompt system to support end-to-end, multi-modal analysis using a handcrafted set of diverse brain-imaging tasks, covering a wide range of MRI and CT acquisitions, anatomies, and diseases. We demonstrate that the single VoxelPrompt model properly plans and executes the steps necessary to perform these tasks while <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">also</span> capturing the individual accuracy of fine-tuned, single-task baselines for brain feature segmentation and pathology-based visual question-answering. Additionally, we show that natural language offers an effective mechanism for flexible model interaction. VoxelPrompt aligns descriptive text with image features to target hundreds of anatomical and pathological ROIs, and it properly considers descriptive factors such as spatial and signal characteristics.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We focus this work in brain image analysis â€“ a large field with crucial impact in neurology. However, we design VoxelPrompt to be broadly applicable across medical domains, and we hope it provides a technical framework for future development of universal radiology utilities.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Brain Region Analysis</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In neuroimaging, interpreting and analyzing ROI characteristics is central to clinical decision-making as well as understanding brain structure, function, and development. To support these aims, widely-used processing pipelines typically employ algorithms that delineate relevant features and quantify their size, shape, connectivity, composition, and change over timeÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Modern processing methods train deep-learning architectures to segment diverse anatomy and pathology, such as subcortical and cortical structuresÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">12</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, cerebral vesselsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">14</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, white matter tractsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">16</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">17</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, tumorsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">19</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">20</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">21</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, intracranial hemorrhagesÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, ischemic strokeÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">25</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, and various other lesionsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">27</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. Specialized for individual applications, these tools generally output only fixed segmentation targets and require significant human involvement in their use for analyzing data and deriving downstream ROI measures. Alternative learning-based approaches enable the direct estimation of image characterizations â€“ such as tumor classificationÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>, brain ageÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">32</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">33</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, and dementia scoresÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">35</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">36</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">37</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> â€“ without the need for intermediate ROI processing.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Learning Across Medical Imaging Tasks</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Recent methods in medical image analysis consolidate multiple objectives within a <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">single</span> framework and build on multi-task learning techniquesÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">39</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">40</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>. These approaches aim to improve performance, generalization, and training efficiency by exploiting shared representations across diverse segmentation, classification, registration, and statistical modelling objectivesÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">42</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">43</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">44</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">45</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Adaptable multi-task models learn to address specific tasks at inference with user-provided support. For instance, interactive segmentation tools can adapt to specific biomedical targets, prompted by partial image annotationsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">47</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">48</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">49</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">50</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>. Similarly, few-shot and in-context learning methodsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">52</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>, which leverage a set of example input-output image pairs as guidance, can generalize to medical segmentationÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">54</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">55</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">56</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> and broad neuroimaging tasksÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> without model fine-tuning. Task-query mechanisms offer robust test-time versatility, but creating support data for each analysis can be challenging and impractical.
Overall, these multi-task models are developed as either pretraining or specialized methods that do not aim to address a complete analytical pipeline.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Medical Vision-Language Models</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Vision-language models (VLMs) learn joint representations of images and text to facilitate language-driven image analysis. They often combine language models with image encoders, aligning visual and text features with contrastive learning techniquesÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>. Using large-scale biomedical image-caption datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">60</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">62</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, generalized VLM pretraining can facilitate downstream fine-tuning (and even zero-shot or few-shot performance) for visual question answering tasks in medical imagingÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">66</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">68</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">69</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">61</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">70</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">71</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">72</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">73</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">74</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">75</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">76</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">78</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">63</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>. More specialized language-generating medical VLMs focus on individual aims, such as the estimation of clinical reports directly from imagesÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">80</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">81</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">82</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">83</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">84</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">85</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib86" title=""><span class="ltx_text" style="font-size:90%;">86</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">87</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">88</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">89</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>.
However, with the exception of few recent worksÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">64</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">91</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">92</span></a>]</cite>, most general-domain models are trained exclusively on two-dimensional image slices, often X-rays, making them inappropriate for standard 3D MR and CT imaging.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Alternatively, image-generating medical VLMs incorporate text embeddings into vision networks to develop promptable biomedical segmentersÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib93" title=""><span class="ltx_text" style="font-size:90%;">93</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib94" title=""><span class="ltx_text" style="font-size:90%;">94</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">95</span></a>]</cite> or generators of scans with user-specified characteristicsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">96</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib97" title=""><span class="ltx_text" style="font-size:90%;">97</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib98" title=""><span class="ltx_text" style="font-size:90%;">98</span></a>]</cite>. Augmented training schemes use text as additional supervision for improving pathology segmentationÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite> or localizationÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib99" title=""><span class="ltx_text" style="font-size:90%;">99</span></a>]</cite>, but these do not focus on using or generating language at inference time. Recent work introduces a question-answering VLM that generates both language, for image characterization, and conditional segmentations for a predefined set of image targetsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">92</span></a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Language Models as Agents</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Building on the code-prediction and chain-of-thought capabilities of large language modelsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib100" title=""><span class="ltx_text" style="font-size:90%;">100</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib101" title=""><span class="ltx_text" style="font-size:90%;">101</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib102" title=""><span class="ltx_text" style="font-size:90%;">102</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib103" title=""><span class="ltx_text" style="font-size:90%;">103</span></a>]</cite>, recent work beyond the medical domain trains text generating networks as intelligent agents that plan and execute actions to solve a prescribed computational task. Agents call on external APIs to address problems that require more than natural language prediction, such as mathematical computationÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib104" title=""><span class="ltx_text" style="font-size:90%;">104</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib105" title=""><span class="ltx_text" style="font-size:90%;">105</span></a>]</cite>, image analysisÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib106" title=""><span class="ltx_text" style="font-size:90%;">106</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib107" title=""><span class="ltx_text" style="font-size:90%;">107</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib108" title=""><span class="ltx_text" style="font-size:90%;">108</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib109" title=""><span class="ltx_text" style="font-size:90%;">109</span></a>]</cite>, web interactionÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib110" title=""><span class="ltx_text" style="font-size:90%;">110</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib111" title=""><span class="ltx_text" style="font-size:90%;">111</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib112" title=""><span class="ltx_text" style="font-size:90%;">112</span></a>]</cite>, scientific discoveryÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib113" title=""><span class="ltx_text" style="font-size:90%;">113</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib114" title=""><span class="ltx_text" style="font-size:90%;">114</span></a>]</cite>, and general-domain task solvingÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib115" title=""><span class="ltx_text" style="font-size:90%;">115</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib116" title=""><span class="ltx_text" style="font-size:90%;">116</span></a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Adaptive, or feedback-driven, agents manage complex problems with unpredictable dynamics by making planning decisions based on observed outcomes of intermediate actionsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib117" title=""><span class="ltx_text" style="font-size:90%;">117</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib118" title=""><span class="ltx_text" style="font-size:90%;">118</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib119" title=""><span class="ltx_text" style="font-size:90%;">119</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib120" title=""><span class="ltx_text" style="font-size:90%;">120</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib121" title=""><span class="ltx_text" style="font-size:90%;">121</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib122" title=""><span class="ltx_text" style="font-size:90%;">122</span></a>]</cite>. Instead of predicting an entire action sequence at once, these adaptive agents follow an iterative, step-wise process of planning, executing, and interpreting an actionâ€™s effect within an environment. In VoxelPrompt, we build on these ideas to implement an adaptive agent that interacts with a library of processing functions and instructs image volume encoding and generation through jointly-learned vision networks.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">A recent contemporaneous methodÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib123" title=""><span class="ltx_text" style="font-size:90%;">123</span></a>]</cite> trains a language model to address a discrete set of medical imaging goals by selecting and executing pretrained, task-specific tools. Unlike VoxelPrompt, this model does not execute downstream operations to extract key metrics and does not leverage the flexibility of language prompting for distinguishing nuanced ROIs with desired characteristics.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.9">We design a vision-language instruction system that processes a set of medical image volumesÂ <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">ğ’±</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ’±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">caligraphic_V</annotation></semantics></math> based on an input text promptÂ <math alttext="p" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_p</annotation></semantics></math> (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S3.F2" title="Figure 2 â€£ 3 Method â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">2</span></a>). To address the task defined byÂ <math alttext="p" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_p</annotation></semantics></math>, a language-model-based agentÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">italic_Î±</annotation></semantics></math> outputs instructionsÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">italic_Î·</annotation></semantics></math>, which include code executed in an environmentÂ <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" mathvariant="normal" xref="S3.p1.6.m6.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">roman_Î©</annotation></semantics></math>. This code calls functions from a predefined library to perform actions such as mathematical computation, user-interface interaction, volume interpretation, and segmentation. Various volume operations are enabled by a vision encoderÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><msub id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml"><mi id="S3.p1.7.m7.1.1.2" xref="S3.p1.7.m7.1.1.2.cmml">m</mi><mtext id="S3.p1.7.m7.1.1.3" xref="S3.p1.7.m7.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><apply id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.1.1.cmml" xref="S3.p1.7.m7.1.1">subscript</csymbol><ci id="S3.p1.7.m7.1.1.2.cmml" xref="S3.p1.7.m7.1.1.2">ğ‘š</ci><ci id="S3.p1.7.m7.1.1.3a.cmml" xref="S3.p1.7.m7.1.1.3"><mtext id="S3.p1.7.m7.1.1.3.cmml" mathsize="70%" xref="S3.p1.7.m7.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> and generatorÂ <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.p1.8.m8.1"><semantics id="S3.p1.8.m8.1a"><msub id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml"><mi id="S3.p1.8.m8.1.1.2" xref="S3.p1.8.m8.1.1.2.cmml">m</mi><mtext id="S3.p1.8.m8.1.1.3" xref="S3.p1.8.m8.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><apply id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.1.cmml" xref="S3.p1.8.m8.1.1">subscript</csymbol><ci id="S3.p1.8.m8.1.1.2.cmml" xref="S3.p1.8.m8.1.1.2">ğ‘š</ci><ci id="S3.p1.8.m8.1.1.3a.cmml" xref="S3.p1.8.m8.1.1.3"><mtext id="S3.p1.8.m8.1.1.3.cmml" mathsize="70%" xref="S3.p1.8.m8.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>, which are jointly trained withÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.p1.9.m9.1"><semantics id="S3.p1.9.m9.1a"><mi id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><ci id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m9.1d">italic_Î±</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.18.9.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.16.8" style="font-size:90%;">To solve a language-prompted task, the adaptive agent modelÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.F2.9.1.m1.1"><semantics id="S3.F2.9.1.m1.1b"><mi id="S3.F2.9.1.m1.1.1" xref="S3.F2.9.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.F2.9.1.m1.1c"><ci id="S3.F2.9.1.m1.1.1.cmml" xref="S3.F2.9.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.1.m1.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.F2.9.1.m1.1e">italic_Î±</annotation></semantics></math> outputs instructionsÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.F2.10.2.m2.1"><semantics id="S3.F2.10.2.m2.1b"><mi id="S3.F2.10.2.m2.1.1" xref="S3.F2.10.2.m2.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.F2.10.2.m2.1c"><ci id="S3.F2.10.2.m2.1.1.cmml" xref="S3.F2.10.2.m2.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.2.m2.1d">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.F2.10.2.m2.1e">italic_Î·</annotation></semantics></math> as code to run in a persistent execution environmentÂ <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.F2.11.3.m3.1"><semantics id="S3.F2.11.3.m3.1b"><mi id="S3.F2.11.3.m3.1.1" mathvariant="normal" xref="S3.F2.11.3.m3.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.F2.11.3.m3.1c"><ci id="S3.F2.11.3.m3.1.1.cmml" xref="S3.F2.11.3.m3.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.11.3.m3.1d">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.F2.11.3.m3.1e">roman_Î©</annotation></semantics></math>. Across multiple steps, the agent interprets execution outcomesÂ <math alttext="z" class="ltx_Math" display="inline" id="S3.F2.12.4.m4.1"><semantics id="S3.F2.12.4.m4.1b"><mi id="S3.F2.12.4.m4.1.1" xref="S3.F2.12.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.F2.12.4.m4.1c"><ci id="S3.F2.12.4.m4.1.1.cmml" xref="S3.F2.12.4.m4.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.12.4.m4.1d">z</annotation><annotation encoding="application/x-llamapun" id="S3.F2.12.4.m4.1e">italic_z</annotation></semantics></math> (blue) to guide subsequent instruction prediction. To perform vision operations, such as volume encoding or generation,Â <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.F2.13.5.m5.1"><semantics id="S3.F2.13.5.m5.1b"><mi id="S3.F2.13.5.m5.1.1" xref="S3.F2.13.5.m5.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.F2.13.5.m5.1c"><ci id="S3.F2.13.5.m5.1.1.cmml" xref="S3.F2.13.5.m5.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.13.5.m5.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.F2.13.5.m5.1e">italic_Î±</annotation></semantics></math> can instruct the execution of vision networksÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.F2.14.6.m6.1"><semantics id="S3.F2.14.6.m6.1b"><msub id="S3.F2.14.6.m6.1.1" xref="S3.F2.14.6.m6.1.1.cmml"><mi id="S3.F2.14.6.m6.1.1.2" xref="S3.F2.14.6.m6.1.1.2.cmml">m</mi><mtext id="S3.F2.14.6.m6.1.1.3" xref="S3.F2.14.6.m6.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.F2.14.6.m6.1c"><apply id="S3.F2.14.6.m6.1.1.cmml" xref="S3.F2.14.6.m6.1.1"><csymbol cd="ambiguous" id="S3.F2.14.6.m6.1.1.1.cmml" xref="S3.F2.14.6.m6.1.1">subscript</csymbol><ci id="S3.F2.14.6.m6.1.1.2.cmml" xref="S3.F2.14.6.m6.1.1.2">ğ‘š</ci><ci id="S3.F2.14.6.m6.1.1.3a.cmml" xref="S3.F2.14.6.m6.1.1.3"><mtext id="S3.F2.14.6.m6.1.1.3.cmml" mathsize="70%" xref="S3.F2.14.6.m6.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.14.6.m6.1d">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.14.6.m6.1e">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> andÂ <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.F2.15.7.m7.1"><semantics id="S3.F2.15.7.m7.1b"><msub id="S3.F2.15.7.m7.1.1" xref="S3.F2.15.7.m7.1.1.cmml"><mi id="S3.F2.15.7.m7.1.1.2" xref="S3.F2.15.7.m7.1.1.2.cmml">m</mi><mtext id="S3.F2.15.7.m7.1.1.3" xref="S3.F2.15.7.m7.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.F2.15.7.m7.1c"><apply id="S3.F2.15.7.m7.1.1.cmml" xref="S3.F2.15.7.m7.1.1"><csymbol cd="ambiguous" id="S3.F2.15.7.m7.1.1.1.cmml" xref="S3.F2.15.7.m7.1.1">subscript</csymbol><ci id="S3.F2.15.7.m7.1.1.2.cmml" xref="S3.F2.15.7.m7.1.1.2">ğ‘š</ci><ci id="S3.F2.15.7.m7.1.1.3a.cmml" xref="S3.F2.15.7.m7.1.1.3"><mtext id="S3.F2.15.7.m7.1.1.3.cmml" mathsize="70%" xref="S3.F2.15.7.m7.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.15.7.m7.1d">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.15.7.m7.1e">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>, which are manipulated by image-specific latent instruction embeddingsÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S3.F2.16.8.m8.1"><semantics id="S3.F2.16.8.m8.1b"><mi id="S3.F2.16.8.m8.1.1" xref="S3.F2.16.8.m8.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.F2.16.8.m8.1c"><ci id="S3.F2.16.8.m8.1.1.cmml" xref="S3.F2.16.8.m8.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.16.8.m8.1d">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.F2.16.8.m8.1e">italic_Ï•</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.9"><span class="ltx_text ltx_font_bold" id="S3.p2.9.1">Feedback-Driven Instruction.</span>
The agentÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_Î±</annotation></semantics></math> prepares instructions over several iterative steps, guided by outcomes of previously executed actions. In stepÂ <math alttext="i" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_i</annotation></semantics></math>, the agent computes an instruction embedding sequenceÂ <math alttext="\eta_{i}=\alpha(\mu_{i})" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mrow id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><msub id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml"><mi id="S3.p2.3.m3.1.1.3.2" xref="S3.p2.3.m3.1.1.3.2.cmml">Î·</mi><mi id="S3.p2.3.m3.1.1.3.3" xref="S3.p2.3.m3.1.1.3.3.cmml">i</mi></msub><mo id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">=</mo><mrow id="S3.p2.3.m3.1.1.1" xref="S3.p2.3.m3.1.1.1.cmml"><mi id="S3.p2.3.m3.1.1.1.3" xref="S3.p2.3.m3.1.1.1.3.cmml">Î±</mi><mo id="S3.p2.3.m3.1.1.1.2" xref="S3.p2.3.m3.1.1.1.2.cmml">â¢</mo><mrow id="S3.p2.3.m3.1.1.1.1.1" xref="S3.p2.3.m3.1.1.1.1.1.1.cmml"><mo id="S3.p2.3.m3.1.1.1.1.1.2" stretchy="false" xref="S3.p2.3.m3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.p2.3.m3.1.1.1.1.1.1" xref="S3.p2.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.p2.3.m3.1.1.1.1.1.1.2" xref="S3.p2.3.m3.1.1.1.1.1.1.2.cmml">Î¼</mi><mi id="S3.p2.3.m3.1.1.1.1.1.1.3" xref="S3.p2.3.m3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p2.3.m3.1.1.1.1.1.3" stretchy="false" xref="S3.p2.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><eq id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2"></eq><apply id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.3.1.cmml" xref="S3.p2.3.m3.1.1.3">subscript</csymbol><ci id="S3.p2.3.m3.1.1.3.2.cmml" xref="S3.p2.3.m3.1.1.3.2">ğœ‚</ci><ci id="S3.p2.3.m3.1.1.3.3.cmml" xref="S3.p2.3.m3.1.1.3.3">ğ‘–</ci></apply><apply id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1"><times id="S3.p2.3.m3.1.1.1.2.cmml" xref="S3.p2.3.m3.1.1.1.2"></times><ci id="S3.p2.3.m3.1.1.1.3.cmml" xref="S3.p2.3.m3.1.1.1.3">ğ›¼</ci><apply id="S3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.2">ğœ‡</ci><ci id="S3.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\eta_{i}=\alpha(\mu_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_Î· start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_Î± ( italic_Î¼ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>, using an input state representationÂ <math alttext="\mu_{i}\in\mathbb{R}^{\ell,d}" class="ltx_Math" display="inline" id="S3.p2.4.m4.2"><semantics id="S3.p2.4.m4.2a"><mrow id="S3.p2.4.m4.2.3" xref="S3.p2.4.m4.2.3.cmml"><msub id="S3.p2.4.m4.2.3.2" xref="S3.p2.4.m4.2.3.2.cmml"><mi id="S3.p2.4.m4.2.3.2.2" xref="S3.p2.4.m4.2.3.2.2.cmml">Î¼</mi><mi id="S3.p2.4.m4.2.3.2.3" xref="S3.p2.4.m4.2.3.2.3.cmml">i</mi></msub><mo id="S3.p2.4.m4.2.3.1" xref="S3.p2.4.m4.2.3.1.cmml">âˆˆ</mo><msup id="S3.p2.4.m4.2.3.3" xref="S3.p2.4.m4.2.3.3.cmml"><mi id="S3.p2.4.m4.2.3.3.2" xref="S3.p2.4.m4.2.3.3.2.cmml">â„</mi><mrow id="S3.p2.4.m4.2.2.2.4" xref="S3.p2.4.m4.2.2.2.3.cmml"><mi id="S3.p2.4.m4.1.1.1.1" mathvariant="normal" xref="S3.p2.4.m4.1.1.1.1.cmml">â„“</mi><mo id="S3.p2.4.m4.2.2.2.4.1" xref="S3.p2.4.m4.2.2.2.3.cmml">,</mo><mi id="S3.p2.4.m4.2.2.2.2" xref="S3.p2.4.m4.2.2.2.2.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.2b"><apply id="S3.p2.4.m4.2.3.cmml" xref="S3.p2.4.m4.2.3"><in id="S3.p2.4.m4.2.3.1.cmml" xref="S3.p2.4.m4.2.3.1"></in><apply id="S3.p2.4.m4.2.3.2.cmml" xref="S3.p2.4.m4.2.3.2"><csymbol cd="ambiguous" id="S3.p2.4.m4.2.3.2.1.cmml" xref="S3.p2.4.m4.2.3.2">subscript</csymbol><ci id="S3.p2.4.m4.2.3.2.2.cmml" xref="S3.p2.4.m4.2.3.2.2">ğœ‡</ci><ci id="S3.p2.4.m4.2.3.2.3.cmml" xref="S3.p2.4.m4.2.3.2.3">ğ‘–</ci></apply><apply id="S3.p2.4.m4.2.3.3.cmml" xref="S3.p2.4.m4.2.3.3"><csymbol cd="ambiguous" id="S3.p2.4.m4.2.3.3.1.cmml" xref="S3.p2.4.m4.2.3.3">superscript</csymbol><ci id="S3.p2.4.m4.2.3.3.2.cmml" xref="S3.p2.4.m4.2.3.3.2">â„</ci><list id="S3.p2.4.m4.2.2.2.3.cmml" xref="S3.p2.4.m4.2.2.2.4"><ci id="S3.p2.4.m4.1.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1.1">â„“</ci><ci id="S3.p2.4.m4.2.2.2.2.cmml" xref="S3.p2.4.m4.2.2.2.2">ğ‘‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.2c">\mu_{i}\in\mathbb{R}^{\ell,d}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.2d">italic_Î¼ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT roman_â„“ , italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> with sequence lengthÂ <math alttext="\ell" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mi id="S3.p2.5.m5.1.1" mathvariant="normal" xref="S3.p2.5.m5.1.1.cmml">â„“</mi><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><ci id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">â„“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">roman_â„“</annotation></semantics></math> and embedding dimensionÂ <math alttext="d" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><mi id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><ci id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">italic_d</annotation></semantics></math>. We decodeÂ <math alttext="\eta_{i}" class="ltx_Math" display="inline" id="S3.p2.7.m7.1"><semantics id="S3.p2.7.m7.1a"><msub id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">Î·</mi><mi id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1">subscript</csymbol><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">ğœ‚</ci><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">\eta_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m7.1d">italic_Î· start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> into interpretable textÂ <math alttext="\varphi_{i}" class="ltx_Math" display="inline" id="S3.p2.8.m8.1"><semantics id="S3.p2.8.m8.1a"><msub id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><mi id="S3.p2.8.m8.1.1.2" xref="S3.p2.8.m8.1.1.2.cmml">Ï†</mi><mi id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1">subscript</csymbol><ci id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1.2">ğœ‘</ci><ci id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">\varphi_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.8.m8.1d">italic_Ï† start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, which we execute as code inÂ <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.p2.9.m9.1"><semantics id="S3.p2.9.m9.1a"><mi id="S3.p2.9.m9.1.1" mathvariant="normal" xref="S3.p2.9.m9.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.p2.9.m9.1b"><ci id="S3.p2.9.m9.1.1.cmml" xref="S3.p2.9.m9.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.9.m9.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.p2.9.m9.1d">roman_Î©</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.12">To initialize the first step, we setÂ <math alttext="\mu_{1}" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">Î¼</mi><mn id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">ğœ‡</ci><cn id="S3.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\mu_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_Î¼ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> as the embedded representation of the promptÂ <math alttext="p" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_p</annotation></semantics></math> and acquisition metadata (such as scan date) for each volumeÂ <math alttext="v\in\mathcal{V}" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mrow id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">v</mi><mo id="S3.p3.3.m3.1.1.1" xref="S3.p3.3.m3.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml">ğ’±</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><in id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1.1"></in><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">ğ‘£</ci><ci id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3">ğ’±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">v\in\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_v âˆˆ caligraphic_V</annotation></semantics></math>. InÂ <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" mathvariant="normal" xref="S3.p3.4.m4.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">roman_Î©</annotation></semantics></math>, we predefine a variable corresponding to eachÂ <math alttext="v" class="ltx_Math" display="inline" id="S3.p3.5.m5.1"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">ğ‘£</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">v</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.1d">italic_v</annotation></semantics></math>. As the codeÂ <math alttext="\varphi_{i}" class="ltx_Math" display="inline" id="S3.p3.6.m6.1"><semantics id="S3.p3.6.m6.1a"><msub id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml"><mi id="S3.p3.6.m6.1.1.2" xref="S3.p3.6.m6.1.1.2.cmml">Ï†</mi><mi id="S3.p3.6.m6.1.1.3" xref="S3.p3.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><apply id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p3.6.m6.1.1.1.cmml" xref="S3.p3.6.m6.1.1">subscript</csymbol><ci id="S3.p3.6.m6.1.1.2.cmml" xref="S3.p3.6.m6.1.1.2">ğœ‘</ci><ci id="S3.p3.6.m6.1.1.3.cmml" xref="S3.p3.6.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">\varphi_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.6.m6.1d">italic_Ï† start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is executed, new variables are defined and retained inÂ <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.p3.7.m7.1"><semantics id="S3.p3.7.m7.1a"><mi id="S3.p3.7.m7.1.1" mathvariant="normal" xref="S3.p3.7.m7.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.1b"><ci id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.p3.7.m7.1d">roman_Î©</annotation></semantics></math>, which persists across steps. To guide the next instruction step,Â <math alttext="\varphi_{i}" class="ltx_Math" display="inline" id="S3.p3.8.m8.1"><semantics id="S3.p3.8.m8.1a"><msub id="S3.p3.8.m8.1.1" xref="S3.p3.8.m8.1.1.cmml"><mi id="S3.p3.8.m8.1.1.2" xref="S3.p3.8.m8.1.1.2.cmml">Ï†</mi><mi id="S3.p3.8.m8.1.1.3" xref="S3.p3.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.8.m8.1b"><apply id="S3.p3.8.m8.1.1.cmml" xref="S3.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p3.8.m8.1.1.1.cmml" xref="S3.p3.8.m8.1.1">subscript</csymbol><ci id="S3.p3.8.m8.1.1.2.cmml" xref="S3.p3.8.m8.1.1.2">ğœ‘</ci><ci id="S3.p3.8.m8.1.1.3.cmml" xref="S3.p3.8.m8.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.8.m8.1c">\varphi_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.8.m8.1d">italic_Ï† start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> can include <span class="ltx_text ltx_font_italic" id="S3.p3.12.1">read</span> operations on intermediate variables inÂ <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.p3.9.m9.1"><semantics id="S3.p3.9.m9.1a"><mi id="S3.p3.9.m9.1.1" mathvariant="normal" xref="S3.p3.9.m9.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.p3.9.m9.1b"><ci id="S3.p3.9.m9.1.1.cmml" xref="S3.p3.9.m9.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.9.m9.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.p3.9.m9.1d">roman_Î©</annotation></semantics></math>. During execution, a read operation extracts the value of a variable as interpretable information. We include the embedded representationÂ <math alttext="z_{i}" class="ltx_Math" display="inline" id="S3.p3.10.m10.1"><semantics id="S3.p3.10.m10.1a"><msub id="S3.p3.10.m10.1.1" xref="S3.p3.10.m10.1.1.cmml"><mi id="S3.p3.10.m10.1.1.2" xref="S3.p3.10.m10.1.1.2.cmml">z</mi><mi id="S3.p3.10.m10.1.1.3" xref="S3.p3.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.10.m10.1b"><apply id="S3.p3.10.m10.1.1.cmml" xref="S3.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p3.10.m10.1.1.1.cmml" xref="S3.p3.10.m10.1.1">subscript</csymbol><ci id="S3.p3.10.m10.1.1.2.cmml" xref="S3.p3.10.m10.1.1.2">ğ‘§</ci><ci id="S3.p3.10.m10.1.1.3.cmml" xref="S3.p3.10.m10.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.10.m10.1c">z_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.10.m10.1d">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> of the extracted outputs as feedback in the next stateÂ <math alttext="\mu_{i+1}=\mu_{i}\shortparallel\eta_{i}\shortparallel z_{i}" class="ltx_Math" display="inline" id="S3.p3.11.m11.1"><semantics id="S3.p3.11.m11.1a"><mrow id="S3.p3.11.m11.1.1" xref="S3.p3.11.m11.1.1.cmml"><msub id="S3.p3.11.m11.1.1.2" xref="S3.p3.11.m11.1.1.2.cmml"><mi id="S3.p3.11.m11.1.1.2.2" xref="S3.p3.11.m11.1.1.2.2.cmml">Î¼</mi><mrow id="S3.p3.11.m11.1.1.2.3" xref="S3.p3.11.m11.1.1.2.3.cmml"><mi id="S3.p3.11.m11.1.1.2.3.2" xref="S3.p3.11.m11.1.1.2.3.2.cmml">i</mi><mo id="S3.p3.11.m11.1.1.2.3.1" xref="S3.p3.11.m11.1.1.2.3.1.cmml">+</mo><mn id="S3.p3.11.m11.1.1.2.3.3" xref="S3.p3.11.m11.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.p3.11.m11.1.1.3" xref="S3.p3.11.m11.1.1.3.cmml">=</mo><msub id="S3.p3.11.m11.1.1.4" xref="S3.p3.11.m11.1.1.4.cmml"><mi id="S3.p3.11.m11.1.1.4.2" xref="S3.p3.11.m11.1.1.4.2.cmml">Î¼</mi><mi id="S3.p3.11.m11.1.1.4.3" xref="S3.p3.11.m11.1.1.4.3.cmml">i</mi></msub><mo id="S3.p3.11.m11.1.1.5" xref="S3.p3.11.m11.1.1.5.cmml">âˆ¥</mo><msub id="S3.p3.11.m11.1.1.6" xref="S3.p3.11.m11.1.1.6.cmml"><mi id="S3.p3.11.m11.1.1.6.2" xref="S3.p3.11.m11.1.1.6.2.cmml">Î·</mi><mi id="S3.p3.11.m11.1.1.6.3" xref="S3.p3.11.m11.1.1.6.3.cmml">i</mi></msub><mo id="S3.p3.11.m11.1.1.7" xref="S3.p3.11.m11.1.1.7.cmml">âˆ¥</mo><msub id="S3.p3.11.m11.1.1.8" xref="S3.p3.11.m11.1.1.8.cmml"><mi id="S3.p3.11.m11.1.1.8.2" xref="S3.p3.11.m11.1.1.8.2.cmml">z</mi><mi id="S3.p3.11.m11.1.1.8.3" xref="S3.p3.11.m11.1.1.8.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.11.m11.1b"><apply id="S3.p3.11.m11.1.1.cmml" xref="S3.p3.11.m11.1.1"><and id="S3.p3.11.m11.1.1a.cmml" xref="S3.p3.11.m11.1.1"></and><apply id="S3.p3.11.m11.1.1b.cmml" xref="S3.p3.11.m11.1.1"><eq id="S3.p3.11.m11.1.1.3.cmml" xref="S3.p3.11.m11.1.1.3"></eq><apply id="S3.p3.11.m11.1.1.2.cmml" xref="S3.p3.11.m11.1.1.2"><csymbol cd="ambiguous" id="S3.p3.11.m11.1.1.2.1.cmml" xref="S3.p3.11.m11.1.1.2">subscript</csymbol><ci id="S3.p3.11.m11.1.1.2.2.cmml" xref="S3.p3.11.m11.1.1.2.2">ğœ‡</ci><apply id="S3.p3.11.m11.1.1.2.3.cmml" xref="S3.p3.11.m11.1.1.2.3"><plus id="S3.p3.11.m11.1.1.2.3.1.cmml" xref="S3.p3.11.m11.1.1.2.3.1"></plus><ci id="S3.p3.11.m11.1.1.2.3.2.cmml" xref="S3.p3.11.m11.1.1.2.3.2">ğ‘–</ci><cn id="S3.p3.11.m11.1.1.2.3.3.cmml" type="integer" xref="S3.p3.11.m11.1.1.2.3.3">1</cn></apply></apply><apply id="S3.p3.11.m11.1.1.4.cmml" xref="S3.p3.11.m11.1.1.4"><csymbol cd="ambiguous" id="S3.p3.11.m11.1.1.4.1.cmml" xref="S3.p3.11.m11.1.1.4">subscript</csymbol><ci id="S3.p3.11.m11.1.1.4.2.cmml" xref="S3.p3.11.m11.1.1.4.2">ğœ‡</ci><ci id="S3.p3.11.m11.1.1.4.3.cmml" xref="S3.p3.11.m11.1.1.4.3">ğ‘–</ci></apply></apply><apply id="S3.p3.11.m11.1.1c.cmml" xref="S3.p3.11.m11.1.1"><csymbol cd="latexml" id="S3.p3.11.m11.1.1.5.cmml" xref="S3.p3.11.m11.1.1.5">parallel-to</csymbol><share href="https://arxiv.org/html/2410.08397v1#S3.p3.11.m11.1.1.4.cmml" id="S3.p3.11.m11.1.1d.cmml" xref="S3.p3.11.m11.1.1"></share><apply id="S3.p3.11.m11.1.1.6.cmml" xref="S3.p3.11.m11.1.1.6"><csymbol cd="ambiguous" id="S3.p3.11.m11.1.1.6.1.cmml" xref="S3.p3.11.m11.1.1.6">subscript</csymbol><ci id="S3.p3.11.m11.1.1.6.2.cmml" xref="S3.p3.11.m11.1.1.6.2">ğœ‚</ci><ci id="S3.p3.11.m11.1.1.6.3.cmml" xref="S3.p3.11.m11.1.1.6.3">ğ‘–</ci></apply></apply><apply id="S3.p3.11.m11.1.1e.cmml" xref="S3.p3.11.m11.1.1"><csymbol cd="latexml" id="S3.p3.11.m11.1.1.7.cmml" xref="S3.p3.11.m11.1.1.7">parallel-to</csymbol><share href="https://arxiv.org/html/2410.08397v1#S3.p3.11.m11.1.1.6.cmml" id="S3.p3.11.m11.1.1f.cmml" xref="S3.p3.11.m11.1.1"></share><apply id="S3.p3.11.m11.1.1.8.cmml" xref="S3.p3.11.m11.1.1.8"><csymbol cd="ambiguous" id="S3.p3.11.m11.1.1.8.1.cmml" xref="S3.p3.11.m11.1.1.8">subscript</csymbol><ci id="S3.p3.11.m11.1.1.8.2.cmml" xref="S3.p3.11.m11.1.1.8.2">ğ‘§</ci><ci id="S3.p3.11.m11.1.1.8.3.cmml" xref="S3.p3.11.m11.1.1.8.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.11.m11.1c">\mu_{i+1}=\mu_{i}\shortparallel\eta_{i}\shortparallel z_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.11.m11.1d">italic_Î¼ start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = italic_Î¼ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ¥ italic_Î· start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ¥ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, whereÂ <math alttext="\shortparallel" class="ltx_Math" display="inline" id="S3.p3.12.m12.1"><semantics id="S3.p3.12.m12.1a"><mo id="S3.p3.12.m12.1.1" xref="S3.p3.12.m12.1.1.cmml">âˆ¥</mo><annotation-xml encoding="MathML-Content" id="S3.p3.12.m12.1b"><csymbol cd="latexml" id="S3.p3.12.m12.1.1.cmml" xref="S3.p3.12.m12.1.1">parallel-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.12.m12.1c">\shortparallel</annotation><annotation encoding="application/x-llamapun" id="S3.p3.12.m12.1d">âˆ¥</annotation></semantics></math> represents sequence dimension concatenation.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">This process of instruction planning, execution, and feedback repeats until the task is completed and a stopping instruction is executed.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.7"><span class="ltx_text ltx_font_bold" id="S3.p5.7.1">Vision Network Instruction.</span>
Volume encoding and generation functions instruct the execution ofÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.p5.1.m1.1"><semantics id="S3.p5.1.m1.1a"><msub id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">m</mi><mtext id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">ğ‘š</ci><ci id="S3.p5.1.m1.1.1.3a.cmml" xref="S3.p5.1.m1.1.1.3"><mtext id="S3.p5.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.p5.1.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.p5.2.m2.1"><semantics id="S3.p5.2.m2.1a"><msub id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml"><mi id="S3.p5.2.m2.1.1.2" xref="S3.p5.2.m2.1.1.2.cmml">m</mi><mtext id="S3.p5.2.m2.1.1.3" xref="S3.p5.2.m2.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><apply id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p5.2.m2.1.1.1.cmml" xref="S3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.p5.2.m2.1.1.2.cmml" xref="S3.p5.2.m2.1.1.2">ğ‘š</ci><ci id="S3.p5.2.m2.1.1.3a.cmml" xref="S3.p5.2.m2.1.1.3"><mtext id="S3.p5.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.p5.2.m2.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.2.m2.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>, respectively. Relevant code inÂ <math alttext="\varphi" class="ltx_Math" display="inline" id="S3.p5.3.m3.1"><semantics id="S3.p5.3.m3.1a"><mi id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml">Ï†</mi><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><ci id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1">ğœ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">\varphi</annotation><annotation encoding="application/x-llamapun" id="S3.p5.3.m3.1d">italic_Ï†</annotation></semantics></math> specifies the variables passed toÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.p5.4.m4.1"><semantics id="S3.p5.4.m4.1a"><msub id="S3.p5.4.m4.1.1" xref="S3.p5.4.m4.1.1.cmml"><mi id="S3.p5.4.m4.1.1.2" xref="S3.p5.4.m4.1.1.2.cmml">m</mi><mtext id="S3.p5.4.m4.1.1.3" xref="S3.p5.4.m4.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p5.4.m4.1b"><apply id="S3.p5.4.m4.1.1.cmml" xref="S3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p5.4.m4.1.1.1.cmml" xref="S3.p5.4.m4.1.1">subscript</csymbol><ci id="S3.p5.4.m4.1.1.2.cmml" xref="S3.p5.4.m4.1.1.2">ğ‘š</ci><ci id="S3.p5.4.m4.1.1.3a.cmml" xref="S3.p5.4.m4.1.1.3"><mtext id="S3.p5.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.p5.4.m4.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m4.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.4.m4.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.p5.5.m5.1"><semantics id="S3.p5.5.m5.1a"><msub id="S3.p5.5.m5.1.1" xref="S3.p5.5.m5.1.1.cmml"><mi id="S3.p5.5.m5.1.1.2" xref="S3.p5.5.m5.1.1.2.cmml">m</mi><mtext id="S3.p5.5.m5.1.1.3" xref="S3.p5.5.m5.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p5.5.m5.1b"><apply id="S3.p5.5.m5.1.1.cmml" xref="S3.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p5.5.m5.1.1.1.cmml" xref="S3.p5.5.m5.1.1">subscript</csymbol><ci id="S3.p5.5.m5.1.1.2.cmml" xref="S3.p5.5.m5.1.1.2">ğ‘š</ci><ci id="S3.p5.5.m5.1.1.3a.cmml" xref="S3.p5.5.m5.1.1.3"><mtext id="S3.p5.5.m5.1.1.3.cmml" mathsize="70%" xref="S3.p5.5.m5.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.5.m5.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.p5.5.m5.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>, while a set of latent instruction featuresÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S3.p5.6.m6.1"><semantics id="S3.p5.6.m6.1a"><mi id="S3.p5.6.m6.1.1" xref="S3.p5.6.m6.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.p5.6.m6.1b"><ci id="S3.p5.6.m6.1.1.cmml" xref="S3.p5.6.m6.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.6.m6.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.p5.6.m6.1d">italic_Ï•</annotation></semantics></math>, captured inÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.p5.7.m7.1"><semantics id="S3.p5.7.m7.1a"><mi id="S3.p5.7.m7.1.1" xref="S3.p5.7.m7.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.p5.7.m7.1b"><ci id="S3.p5.7.m7.1.1.cmml" xref="S3.p5.7.m7.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.7.m7.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.p5.7.m7.1d">italic_Î·</annotation></semantics></math>, modulate the intermediate activations of these networks.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.11">An encoding operation distillsÂ <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S3.p6.1.m1.1"><semantics id="S3.p6.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">ğ’±</mi><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">ğ’±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.1d">caligraphic_V</annotation></semantics></math> into sets of featuresÂ <math alttext="\mathcal{E}=m_{\text{enc}}(\mathcal{V},\phi)" class="ltx_Math" display="inline" id="S3.p6.2.m2.2"><semantics id="S3.p6.2.m2.2a"><mrow id="S3.p6.2.m2.2.3" xref="S3.p6.2.m2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p6.2.m2.2.3.2" xref="S3.p6.2.m2.2.3.2.cmml">â„°</mi><mo id="S3.p6.2.m2.2.3.1" xref="S3.p6.2.m2.2.3.1.cmml">=</mo><mrow id="S3.p6.2.m2.2.3.3" xref="S3.p6.2.m2.2.3.3.cmml"><msub id="S3.p6.2.m2.2.3.3.2" xref="S3.p6.2.m2.2.3.3.2.cmml"><mi id="S3.p6.2.m2.2.3.3.2.2" xref="S3.p6.2.m2.2.3.3.2.2.cmml">m</mi><mtext id="S3.p6.2.m2.2.3.3.2.3" xref="S3.p6.2.m2.2.3.3.2.3a.cmml">enc</mtext></msub><mo id="S3.p6.2.m2.2.3.3.1" xref="S3.p6.2.m2.2.3.3.1.cmml">â¢</mo><mrow id="S3.p6.2.m2.2.3.3.3.2" xref="S3.p6.2.m2.2.3.3.3.1.cmml"><mo id="S3.p6.2.m2.2.3.3.3.2.1" stretchy="false" xref="S3.p6.2.m2.2.3.3.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">ğ’±</mi><mo id="S3.p6.2.m2.2.3.3.3.2.2" xref="S3.p6.2.m2.2.3.3.3.1.cmml">,</mo><mi id="S3.p6.2.m2.2.2" xref="S3.p6.2.m2.2.2.cmml">Ï•</mi><mo id="S3.p6.2.m2.2.3.3.3.2.3" stretchy="false" xref="S3.p6.2.m2.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.2b"><apply id="S3.p6.2.m2.2.3.cmml" xref="S3.p6.2.m2.2.3"><eq id="S3.p6.2.m2.2.3.1.cmml" xref="S3.p6.2.m2.2.3.1"></eq><ci id="S3.p6.2.m2.2.3.2.cmml" xref="S3.p6.2.m2.2.3.2">â„°</ci><apply id="S3.p6.2.m2.2.3.3.cmml" xref="S3.p6.2.m2.2.3.3"><times id="S3.p6.2.m2.2.3.3.1.cmml" xref="S3.p6.2.m2.2.3.3.1"></times><apply id="S3.p6.2.m2.2.3.3.2.cmml" xref="S3.p6.2.m2.2.3.3.2"><csymbol cd="ambiguous" id="S3.p6.2.m2.2.3.3.2.1.cmml" xref="S3.p6.2.m2.2.3.3.2">subscript</csymbol><ci id="S3.p6.2.m2.2.3.3.2.2.cmml" xref="S3.p6.2.m2.2.3.3.2.2">ğ‘š</ci><ci id="S3.p6.2.m2.2.3.3.2.3a.cmml" xref="S3.p6.2.m2.2.3.3.2.3"><mtext id="S3.p6.2.m2.2.3.3.2.3.cmml" mathsize="70%" xref="S3.p6.2.m2.2.3.3.2.3">enc</mtext></ci></apply><interval closure="open" id="S3.p6.2.m2.2.3.3.3.1.cmml" xref="S3.p6.2.m2.2.3.3.3.2"><ci id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">ğ’±</ci><ci id="S3.p6.2.m2.2.2.cmml" xref="S3.p6.2.m2.2.2">italic-Ï•</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.2c">\mathcal{E}=m_{\text{enc}}(\mathcal{V},\phi)</annotation><annotation encoding="application/x-llamapun" id="S3.p6.2.m2.2d">caligraphic_E = italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT ( caligraphic_V , italic_Ï• )</annotation></semantics></math>. The setÂ <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.p6.3.m3.1"><semantics id="S3.p6.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml">â„°</mi><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><ci id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1">â„°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.3.m3.1d">caligraphic_E</annotation></semantics></math> contains volume-specific encodingsÂ <math alttext="\mathcal{E}_{v}" class="ltx_Math" display="inline" id="S3.p6.4.m4.1"><semantics id="S3.p6.4.m4.1a"><msub id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p6.4.m4.1.1.2" xref="S3.p6.4.m4.1.1.2.cmml">â„°</mi><mi id="S3.p6.4.m4.1.1.3" xref="S3.p6.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.1b"><apply id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p6.4.m4.1.1.1.cmml" xref="S3.p6.4.m4.1.1">subscript</csymbol><ci id="S3.p6.4.m4.1.1.2.cmml" xref="S3.p6.4.m4.1.1.2">â„°</ci><ci id="S3.p6.4.m4.1.1.3.cmml" xref="S3.p6.4.m4.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.1c">\mathcal{E}_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.4.m4.1d">caligraphic_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>, which have a subset of feature embeddingsÂ <math alttext="\varepsilon_{v}^{\circ}" class="ltx_Math" display="inline" id="S3.p6.5.m5.1"><semantics id="S3.p6.5.m5.1a"><msubsup id="S3.p6.5.m5.1.1" xref="S3.p6.5.m5.1.1.cmml"><mi id="S3.p6.5.m5.1.1.2.2" xref="S3.p6.5.m5.1.1.2.2.cmml">Îµ</mi><mi id="S3.p6.5.m5.1.1.2.3" xref="S3.p6.5.m5.1.1.2.3.cmml">v</mi><mo id="S3.p6.5.m5.1.1.3" xref="S3.p6.5.m5.1.1.3.cmml">âˆ˜</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.p6.5.m5.1b"><apply id="S3.p6.5.m5.1.1.cmml" xref="S3.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p6.5.m5.1.1.1.cmml" xref="S3.p6.5.m5.1.1">superscript</csymbol><apply id="S3.p6.5.m5.1.1.2.cmml" xref="S3.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p6.5.m5.1.1.2.1.cmml" xref="S3.p6.5.m5.1.1">subscript</csymbol><ci id="S3.p6.5.m5.1.1.2.2.cmml" xref="S3.p6.5.m5.1.1.2.2">ğœ€</ci><ci id="S3.p6.5.m5.1.1.2.3.cmml" xref="S3.p6.5.m5.1.1.2.3">ğ‘£</ci></apply><compose id="S3.p6.5.m5.1.1.3.cmml" xref="S3.p6.5.m5.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.5.m5.1c">\varepsilon_{v}^{\circ}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.5.m5.1d">italic_Îµ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT</annotation></semantics></math> that are incorporable toÂ <math alttext="z" class="ltx_Math" display="inline" id="S3.p6.6.m6.1"><semantics id="S3.p6.6.m6.1a"><mi id="S3.p6.6.m6.1.1" xref="S3.p6.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p6.6.m6.1b"><ci id="S3.p6.6.m6.1.1.cmml" xref="S3.p6.6.m6.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.6.m6.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.p6.6.m6.1d">italic_z</annotation></semantics></math> following a <span class="ltx_text ltx_font_italic" id="S3.p6.11.1">read</span> function execution. Alternatively, a volume generation operation outputs a set of volumesÂ <math alttext="\mathcal{W}=m_{\text{gen}}(\mathcal{E},\phi)" class="ltx_Math" display="inline" id="S3.p6.7.m7.2"><semantics id="S3.p6.7.m7.2a"><mrow id="S3.p6.7.m7.2.3" xref="S3.p6.7.m7.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p6.7.m7.2.3.2" xref="S3.p6.7.m7.2.3.2.cmml">ğ’²</mi><mo id="S3.p6.7.m7.2.3.1" xref="S3.p6.7.m7.2.3.1.cmml">=</mo><mrow id="S3.p6.7.m7.2.3.3" xref="S3.p6.7.m7.2.3.3.cmml"><msub id="S3.p6.7.m7.2.3.3.2" xref="S3.p6.7.m7.2.3.3.2.cmml"><mi id="S3.p6.7.m7.2.3.3.2.2" xref="S3.p6.7.m7.2.3.3.2.2.cmml">m</mi><mtext id="S3.p6.7.m7.2.3.3.2.3" xref="S3.p6.7.m7.2.3.3.2.3a.cmml">gen</mtext></msub><mo id="S3.p6.7.m7.2.3.3.1" xref="S3.p6.7.m7.2.3.3.1.cmml">â¢</mo><mrow id="S3.p6.7.m7.2.3.3.3.2" xref="S3.p6.7.m7.2.3.3.3.1.cmml"><mo id="S3.p6.7.m7.2.3.3.3.2.1" stretchy="false" xref="S3.p6.7.m7.2.3.3.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.p6.7.m7.1.1" xref="S3.p6.7.m7.1.1.cmml">â„°</mi><mo id="S3.p6.7.m7.2.3.3.3.2.2" xref="S3.p6.7.m7.2.3.3.3.1.cmml">,</mo><mi id="S3.p6.7.m7.2.2" xref="S3.p6.7.m7.2.2.cmml">Ï•</mi><mo id="S3.p6.7.m7.2.3.3.3.2.3" stretchy="false" xref="S3.p6.7.m7.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.7.m7.2b"><apply id="S3.p6.7.m7.2.3.cmml" xref="S3.p6.7.m7.2.3"><eq id="S3.p6.7.m7.2.3.1.cmml" xref="S3.p6.7.m7.2.3.1"></eq><ci id="S3.p6.7.m7.2.3.2.cmml" xref="S3.p6.7.m7.2.3.2">ğ’²</ci><apply id="S3.p6.7.m7.2.3.3.cmml" xref="S3.p6.7.m7.2.3.3"><times id="S3.p6.7.m7.2.3.3.1.cmml" xref="S3.p6.7.m7.2.3.3.1"></times><apply id="S3.p6.7.m7.2.3.3.2.cmml" xref="S3.p6.7.m7.2.3.3.2"><csymbol cd="ambiguous" id="S3.p6.7.m7.2.3.3.2.1.cmml" xref="S3.p6.7.m7.2.3.3.2">subscript</csymbol><ci id="S3.p6.7.m7.2.3.3.2.2.cmml" xref="S3.p6.7.m7.2.3.3.2.2">ğ‘š</ci><ci id="S3.p6.7.m7.2.3.3.2.3a.cmml" xref="S3.p6.7.m7.2.3.3.2.3"><mtext id="S3.p6.7.m7.2.3.3.2.3.cmml" mathsize="70%" xref="S3.p6.7.m7.2.3.3.2.3">gen</mtext></ci></apply><interval closure="open" id="S3.p6.7.m7.2.3.3.3.1.cmml" xref="S3.p6.7.m7.2.3.3.3.2"><ci id="S3.p6.7.m7.1.1.cmml" xref="S3.p6.7.m7.1.1">â„°</ci><ci id="S3.p6.7.m7.2.2.cmml" xref="S3.p6.7.m7.2.2">italic-Ï•</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.7.m7.2c">\mathcal{W}=m_{\text{gen}}(\mathcal{E},\phi)</annotation><annotation encoding="application/x-llamapun" id="S3.p6.7.m7.2d">caligraphic_W = italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT ( caligraphic_E , italic_Ï• )</annotation></semantics></math>, such as segmentations. In this operation,Â <math alttext="\phi" class="ltx_Math" display="inline" id="S3.p6.8.m8.1"><semantics id="S3.p6.8.m8.1a"><mi id="S3.p6.8.m8.1.1" xref="S3.p6.8.m8.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.p6.8.m8.1b"><ci id="S3.p6.8.m8.1.1.cmml" xref="S3.p6.8.m8.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.8.m8.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.p6.8.m8.1d">italic_Ï•</annotation></semantics></math> might modulateÂ <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.p6.9.m9.1"><semantics id="S3.p6.9.m9.1a"><msub id="S3.p6.9.m9.1.1" xref="S3.p6.9.m9.1.1.cmml"><mi id="S3.p6.9.m9.1.1.2" xref="S3.p6.9.m9.1.1.2.cmml">m</mi><mtext id="S3.p6.9.m9.1.1.3" xref="S3.p6.9.m9.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p6.9.m9.1b"><apply id="S3.p6.9.m9.1.1.cmml" xref="S3.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p6.9.m9.1.1.1.cmml" xref="S3.p6.9.m9.1.1">subscript</csymbol><ci id="S3.p6.9.m9.1.1.2.cmml" xref="S3.p6.9.m9.1.1.2">ğ‘š</ci><ci id="S3.p6.9.m9.1.1.3a.cmml" xref="S3.p6.9.m9.1.1.3"><mtext id="S3.p6.9.m9.1.1.3.cmml" mathsize="70%" xref="S3.p6.9.m9.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.9.m9.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.9.m9.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math> to segment, for example, a particular anatomical target. VolumesÂ <math alttext="\mathcal{W}" class="ltx_Math" display="inline" id="S3.p6.10.m10.1"><semantics id="S3.p6.10.m10.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.10.m10.1.1" xref="S3.p6.10.m10.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="S3.p6.10.m10.1b"><ci id="S3.p6.10.m10.1.1.cmml" xref="S3.p6.10.m10.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.10.m10.1c">\mathcal{W}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.10.m10.1d">caligraphic_W</annotation></semantics></math> can be further processed or used by instructions inÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.p6.11.m11.1"><semantics id="S3.p6.11.m11.1a"><mi id="S3.p6.11.m11.1.1" xref="S3.p6.11.m11.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.p6.11.m11.1b"><ci id="S3.p6.11.m11.1.1.cmml" xref="S3.p6.11.m11.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.11.m11.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.p6.11.m11.1d">italic_Î·</annotation></semantics></math> to compute downstream, task-specific results.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.15"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.15.1">Language Agent.</span>
We implement the agentÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_Î±</annotation></semantics></math> using a transformer-based language model architecture. We convert text into the model embedding space by splitting character groups into tokens (from a vocabulary of sizeÂ <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_Î³</annotation></semantics></math>) and mapping them to a sequence ofÂ <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msup id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">â„</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">â„</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> features via an embedding matrix inÂ <math alttext="\mathbb{R}^{\gamma,d}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.2"><semantics id="S3.SS1.p1.4.m4.2a"><msup id="S3.SS1.p1.4.m4.2.3" xref="S3.SS1.p1.4.m4.2.3.cmml"><mi id="S3.SS1.p1.4.m4.2.3.2" xref="S3.SS1.p1.4.m4.2.3.2.cmml">â„</mi><mrow id="S3.SS1.p1.4.m4.2.2.2.4" xref="S3.SS1.p1.4.m4.2.2.2.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.cmml">Î³</mi><mo id="S3.SS1.p1.4.m4.2.2.2.4.1" xref="S3.SS1.p1.4.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.4.m4.2.2.2.2" xref="S3.SS1.p1.4.m4.2.2.2.2.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><apply id="S3.SS1.p1.4.m4.2.3.cmml" xref="S3.SS1.p1.4.m4.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3">superscript</csymbol><ci id="S3.SS1.p1.4.m4.2.3.2.cmml" xref="S3.SS1.p1.4.m4.2.3.2">â„</ci><list id="S3.SS1.p1.4.m4.2.2.2.3.cmml" xref="S3.SS1.p1.4.m4.2.2.2.4"><ci id="S3.SS1.p1.4.m4.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1">ğ›¾</ci><ci id="S3.SS1.p1.4.m4.2.2.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2.2.2">ğ‘‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">\mathbb{R}^{\gamma,d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.2d">blackboard_R start_POSTSUPERSCRIPT italic_Î³ , italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>. The modelÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_Î±</annotation></semantics></math> auto-regressively generates instruction embeddingsÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_Î·</annotation></semantics></math> based on the inputÂ <math alttext="\mu" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_Î¼</annotation></semantics></math>. We splitÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_Î·</annotation></semantics></math> along the sequence-dimension to extractÂ <math alttext="\eta^{\varphi}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><msup id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">Î·</mi><mi id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml">Ï†</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">superscript</csymbol><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">ğœ‚</ci><ci id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3">ğœ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">\eta^{\varphi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï† end_POSTSUPERSCRIPT</annotation></semantics></math> andÂ <math alttext="\eta^{\phi}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10.1"><semantics id="S3.SS1.p1.10.m10.1a"><msup id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">Î·</mi><mi id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml">Ï•</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">superscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2">ğœ‚</ci><ci id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">\eta^{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.10.m10.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï• end_POSTSUPERSCRIPT</annotation></semantics></math>. We passÂ <math alttext="\eta^{\phi}" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11.1"><semantics id="S3.SS1.p1.11.m11.1a"><msup id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">Î·</mi><mi id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3.cmml">Ï•</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">superscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">ğœ‚</ci><ci id="S3.SS1.p1.11.m11.1.1.3.cmml" xref="S3.SS1.p1.11.m11.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">\eta^{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m11.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï• end_POSTSUPERSCRIPT</annotation></semantics></math> through a fully-connected layer to compute the vision network modulatorsÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m12.1"><semantics id="S3.SS1.p1.12.m12.1a"><mi id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><ci id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.12.m12.1d">italic_Ï•</annotation></semantics></math>. We passÂ <math alttext="\eta^{\varphi}" class="ltx_Math" display="inline" id="S3.SS1.p1.13.m13.1"><semantics id="S3.SS1.p1.13.m13.1a"><msup id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml"><mi id="S3.SS1.p1.13.m13.1.1.2" xref="S3.SS1.p1.13.m13.1.1.2.cmml">Î·</mi><mi id="S3.SS1.p1.13.m13.1.1.3" xref="S3.SS1.p1.13.m13.1.1.3.cmml">Ï†</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><apply id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.13.m13.1.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">superscript</csymbol><ci id="S3.SS1.p1.13.m13.1.1.2.cmml" xref="S3.SS1.p1.13.m13.1.1.2">ğœ‚</ci><ci id="S3.SS1.p1.13.m13.1.1.3.cmml" xref="S3.SS1.p1.13.m13.1.1.3">ğœ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">\eta^{\varphi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.13.m13.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï† end_POSTSUPERSCRIPT</annotation></semantics></math> through a fully-connected layer to obtain text token probabilitiesÂ <math alttext="P(\varphi)" class="ltx_Math" display="inline" id="S3.SS1.p1.14.m14.1"><semantics id="S3.SS1.p1.14.m14.1a"><mrow id="S3.SS1.p1.14.m14.1.2" xref="S3.SS1.p1.14.m14.1.2.cmml"><mi id="S3.SS1.p1.14.m14.1.2.2" xref="S3.SS1.p1.14.m14.1.2.2.cmml">P</mi><mo id="S3.SS1.p1.14.m14.1.2.1" xref="S3.SS1.p1.14.m14.1.2.1.cmml">â¢</mo><mrow id="S3.SS1.p1.14.m14.1.2.3.2" xref="S3.SS1.p1.14.m14.1.2.cmml"><mo id="S3.SS1.p1.14.m14.1.2.3.2.1" stretchy="false" xref="S3.SS1.p1.14.m14.1.2.cmml">(</mo><mi id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml">Ï†</mi><mo id="S3.SS1.p1.14.m14.1.2.3.2.2" stretchy="false" xref="S3.SS1.p1.14.m14.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><apply id="S3.SS1.p1.14.m14.1.2.cmml" xref="S3.SS1.p1.14.m14.1.2"><times id="S3.SS1.p1.14.m14.1.2.1.cmml" xref="S3.SS1.p1.14.m14.1.2.1"></times><ci id="S3.SS1.p1.14.m14.1.2.2.cmml" xref="S3.SS1.p1.14.m14.1.2.2">ğ‘ƒ</ci><ci id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">ğœ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">P(\varphi)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.14.m14.1d">italic_P ( italic_Ï† )</annotation></semantics></math>, which we decode intoÂ <math alttext="\varphi" class="ltx_Math" display="inline" id="S3.SS1.p1.15.m15.1"><semantics id="S3.SS1.p1.15.m15.1a"><mi id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml">Ï†</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><ci id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1">ğœ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">\varphi</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.15.m15.1d">italic_Ï†</annotation></semantics></math> by choosing the maximum probability token at each sequence position.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.20">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S3.T1.1.1.1">Â Â Â Â Â  <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">ğ’±</mi><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">ğ’±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">caligraphic_V</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.2">Set of input volumes</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.2.2.1">Â Â Â Â Â  <math alttext="p" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1.1"><semantics id="S3.T1.2.2.1.m1.1a"><mi id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><ci id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.m1.1d">italic_p</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.2.2.2">Input text prompt</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.3.3.1">Â Â Â Â Â  <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1.1"><semantics id="S3.T1.3.3.1.m1.1a"><mi id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><ci id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.m1.1d">italic_Î±</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.3.2">Agent</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.4.4.1">Â Â Â Â Â  <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.T1.4.4.1.m1.1"><semantics id="S3.T1.4.4.1.m1.1a"><msub id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml"><mi id="S3.T1.4.4.1.m1.1.1.2" xref="S3.T1.4.4.1.m1.1.1.2.cmml">m</mi><mtext id="S3.T1.4.4.1.m1.1.1.3" xref="S3.T1.4.4.1.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b"><apply id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1">subscript</csymbol><ci id="S3.T1.4.4.1.m1.1.1.2.cmml" xref="S3.T1.4.4.1.m1.1.1.2">ğ‘š</ci><ci id="S3.T1.4.4.1.m1.1.1.3a.cmml" xref="S3.T1.4.4.1.m1.1.1.3"><mtext id="S3.T1.4.4.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.T1.4.4.1.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.1.m1.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.4.4.2">Volume encoder</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.5.5.1">Â Â Â Â Â  <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.T1.5.5.1.m1.1"><semantics id="S3.T1.5.5.1.m1.1a"><msub id="S3.T1.5.5.1.m1.1.1" xref="S3.T1.5.5.1.m1.1.1.cmml"><mi id="S3.T1.5.5.1.m1.1.1.2" xref="S3.T1.5.5.1.m1.1.1.2.cmml">m</mi><mtext id="S3.T1.5.5.1.m1.1.1.3" xref="S3.T1.5.5.1.m1.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.m1.1b"><apply id="S3.T1.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.5.5.1.m1.1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1">subscript</csymbol><ci id="S3.T1.5.5.1.m1.1.1.2.cmml" xref="S3.T1.5.5.1.m1.1.1.2">ğ‘š</ci><ci id="S3.T1.5.5.1.m1.1.1.3a.cmml" xref="S3.T1.5.5.1.m1.1.1.3"><mtext id="S3.T1.5.5.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.T1.5.5.1.m1.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.m1.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.1.m1.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.5.5.2">Volume generator</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.1">Â Â Â Â Â  <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.T1.6.6.1.m1.1"><semantics id="S3.T1.6.6.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.T1.6.6.1.m1.1.1" xref="S3.T1.6.6.1.m1.1.1.cmml">â„°</mi><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.1.m1.1b"><ci id="S3.T1.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.1.m1.1.1">â„°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.1.m1.1d">caligraphic_E</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.7.7.2">Set of volume encodings output by <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.T1.7.7.2.m1.1"><semantics id="S3.T1.7.7.2.m1.1a"><msub id="S3.T1.7.7.2.m1.1.1" xref="S3.T1.7.7.2.m1.1.1.cmml"><mi id="S3.T1.7.7.2.m1.1.1.2" xref="S3.T1.7.7.2.m1.1.1.2.cmml">m</mi><mtext id="S3.T1.7.7.2.m1.1.1.3" xref="S3.T1.7.7.2.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.2.m1.1b"><apply id="S3.T1.7.7.2.m1.1.1.cmml" xref="S3.T1.7.7.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.7.7.2.m1.1.1.1.cmml" xref="S3.T1.7.7.2.m1.1.1">subscript</csymbol><ci id="S3.T1.7.7.2.m1.1.1.2.cmml" xref="S3.T1.7.7.2.m1.1.1.2">ğ‘š</ci><ci id="S3.T1.7.7.2.m1.1.1.3a.cmml" xref="S3.T1.7.7.2.m1.1.1.3"><mtext id="S3.T1.7.7.2.m1.1.1.3.cmml" mathsize="70%" xref="S3.T1.7.7.2.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.2.m1.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.2.m1.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.8.8.1">Â Â Â Â Â  <math alttext="\mathcal{W}" class="ltx_Math" display="inline" id="S3.T1.8.8.1.m1.1"><semantics id="S3.T1.8.8.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.T1.8.8.1.m1.1.1" xref="S3.T1.8.8.1.m1.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.1.m1.1b"><ci id="S3.T1.8.8.1.m1.1.1.cmml" xref="S3.T1.8.8.1.m1.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.1.m1.1c">\mathcal{W}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.1.m1.1d">caligraphic_W</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.9.9.2">Set of volumes generated by <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.T1.9.9.2.m1.1"><semantics id="S3.T1.9.9.2.m1.1a"><msub id="S3.T1.9.9.2.m1.1.1" xref="S3.T1.9.9.2.m1.1.1.cmml"><mi id="S3.T1.9.9.2.m1.1.1.2" xref="S3.T1.9.9.2.m1.1.1.2.cmml">m</mi><mtext id="S3.T1.9.9.2.m1.1.1.3" xref="S3.T1.9.9.2.m1.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.2.m1.1b"><apply id="S3.T1.9.9.2.m1.1.1.cmml" xref="S3.T1.9.9.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.9.9.2.m1.1.1.1.cmml" xref="S3.T1.9.9.2.m1.1.1">subscript</csymbol><ci id="S3.T1.9.9.2.m1.1.1.2.cmml" xref="S3.T1.9.9.2.m1.1.1.2">ğ‘š</ci><ci id="S3.T1.9.9.2.m1.1.1.3a.cmml" xref="S3.T1.9.9.2.m1.1.1.3"><mtext id="S3.T1.9.9.2.m1.1.1.3.cmml" mathsize="70%" xref="S3.T1.9.9.2.m1.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.2.m1.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.2.m1.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.10.10.1">Â Â Â Â Â  <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.T1.10.10.1.m1.1"><semantics id="S3.T1.10.10.1.m1.1a"><mi id="S3.T1.10.10.1.m1.1.1" mathvariant="normal" xref="S3.T1.10.10.1.m1.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.1.m1.1b"><ci id="S3.T1.10.10.1.m1.1.1.cmml" xref="S3.T1.10.10.1.m1.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.1.m1.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.10.1.m1.1d">roman_Î©</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.10.10.2">Persistent execution environment</td>
</tr>
<tr class="ltx_tr" id="S3.T1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.11.11.1">Â Â Â Â Â  <math alttext="\mu" class="ltx_Math" display="inline" id="S3.T1.11.11.1.m1.1"><semantics id="S3.T1.11.11.1.m1.1a"><mi id="S3.T1.11.11.1.m1.1.1" xref="S3.T1.11.11.1.m1.1.1.cmml">Î¼</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.1.m1.1b"><ci id="S3.T1.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.1.m1.1.1">ğœ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.11.1.m1.1d">italic_Î¼</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.11.11.2">State representation embeddings</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.12.12.1">Â Â Â Â Â  <math alttext="\eta" class="ltx_Math" display="inline" id="S3.T1.12.12.1.m1.1"><semantics id="S3.T1.12.12.1.m1.1a"><mi id="S3.T1.12.12.1.m1.1.1" xref="S3.T1.12.12.1.m1.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.1.m1.1b"><ci id="S3.T1.12.12.1.m1.1.1.cmml" xref="S3.T1.12.12.1.m1.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.1.m1.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.T1.12.12.1.m1.1d">italic_Î·</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.13.13.2">Instruction embeddings output by <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.T1.13.13.2.m1.1"><semantics id="S3.T1.13.13.2.m1.1a"><mi id="S3.T1.13.13.2.m1.1.1" xref="S3.T1.13.13.2.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.T1.13.13.2.m1.1b"><ci id="S3.T1.13.13.2.m1.1.1.cmml" xref="S3.T1.13.13.2.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.13.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.T1.13.13.2.m1.1d">italic_Î±</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.14.14.1">Â Â Â Â Â  <math alttext="z" class="ltx_Math" display="inline" id="S3.T1.14.14.1.m1.1"><semantics id="S3.T1.14.14.1.m1.1a"><mi id="S3.T1.14.14.1.m1.1.1" xref="S3.T1.14.14.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.T1.14.14.1.m1.1b"><ci id="S3.T1.14.14.1.m1.1.1.cmml" xref="S3.T1.14.14.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.14.1.m1.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.T1.14.14.1.m1.1d">italic_z</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.14.14.2">Execution feedback embeddings</td>
</tr>
<tr class="ltx_tr" id="S3.T1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.15.15.1">Â Â Â Â Â  <math alttext="\varphi" class="ltx_Math" display="inline" id="S3.T1.15.15.1.m1.1"><semantics id="S3.T1.15.15.1.m1.1a"><mi id="S3.T1.15.15.1.m1.1.1" xref="S3.T1.15.15.1.m1.1.1.cmml">Ï†</mi><annotation-xml encoding="MathML-Content" id="S3.T1.15.15.1.m1.1b"><ci id="S3.T1.15.15.1.m1.1.1.cmml" xref="S3.T1.15.15.1.m1.1.1">ğœ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.15.1.m1.1c">\varphi</annotation><annotation encoding="application/x-llamapun" id="S3.T1.15.15.1.m1.1d">italic_Ï†</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.16.16.2">Executable code derived fromÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.T1.16.16.2.m1.1"><semantics id="S3.T1.16.16.2.m1.1a"><mi id="S3.T1.16.16.2.m1.1.1" xref="S3.T1.16.16.2.m1.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.T1.16.16.2.m1.1b"><ci id="S3.T1.16.16.2.m1.1.1.cmml" xref="S3.T1.16.16.2.m1.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.16.2.m1.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.T1.16.16.2.m1.1d">italic_Î·</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.18.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T1.17.17.1">Â Â Â Â Â  <math alttext="\phi" class="ltx_Math" display="inline" id="S3.T1.17.17.1.m1.1"><semantics id="S3.T1.17.17.1.m1.1a"><mi id="S3.T1.17.17.1.m1.1.1" xref="S3.T1.17.17.1.m1.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.T1.17.17.1.m1.1b"><ci id="S3.T1.17.17.1.m1.1.1.cmml" xref="S3.T1.17.17.1.m1.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.17.1.m1.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.T1.17.17.1.m1.1d">italic_Ï•</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="S3.T1.18.18.2">Manipulation features derived fromÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.T1.18.18.2.m1.1"><semantics id="S3.T1.18.18.2.m1.1a"><mi id="S3.T1.18.18.2.m1.1.1" xref="S3.T1.18.18.2.m1.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.T1.18.18.2.m1.1b"><ci id="S3.T1.18.18.2.m1.1.1.cmml" xref="S3.T1.18.18.2.m1.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.18.18.2.m1.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.T1.18.18.2.m1.1d">italic_Î·</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T1.19.19.1">Â Â Â Â Â  <math alttext="d" class="ltx_Math" display="inline" id="S3.T1.19.19.1.m1.1"><semantics id="S3.T1.19.19.1.m1.1a"><mi id="S3.T1.19.19.1.m1.1.1" xref="S3.T1.19.19.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.T1.19.19.1.m1.1b"><ci id="S3.T1.19.19.1.m1.1.1.cmml" xref="S3.T1.19.19.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.19.19.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.T1.19.19.1.m1.1d">italic_d</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.19.19.2">Embedding dimension size</td>
</tr>
<tr class="ltx_tr" id="S3.T1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T1.20.20.1">Â Â Â Â Â  <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.T1.20.20.1.m1.1"><semantics id="S3.T1.20.20.1.m1.1a"><mi id="S3.T1.20.20.1.m1.1.1" xref="S3.T1.20.20.1.m1.1.1.cmml">Î³</mi><annotation-xml encoding="MathML-Content" id="S3.T1.20.20.1.m1.1b"><ci id="S3.T1.20.20.1.m1.1.1.cmml" xref="S3.T1.20.20.1.m1.1.1">ğ›¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.20.20.1.m1.1c">\gamma</annotation><annotation encoding="application/x-llamapun" id="S3.T1.20.20.1.m1.1d">italic_Î³</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.20.20.2">Token vocabulary size</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.22.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.23.2" style="font-size:90%;">Summary of key variables defined in the method.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.7"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.7.1">Flexible Vision Networks.</span>
We implement <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">m</mi><mtext id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ‘š</ci><ci id="S3.SS1.p2.1.m1.1.1.3a.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><mtext id="S3.SS1.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p2.1.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">m</mi><mtext id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">ğ‘š</ci><ci id="S3.SS1.p2.2.m2.1.1.3a.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><mtext id="S3.SS1.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p2.2.m2.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math> as two multi-scale convolutional networks that incorporateÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_Ï•</annotation></semantics></math> and share information across a flexible number of inputs. We propagate each input volumeÂ <math alttext="v" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğ‘£</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">v</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_v</annotation></semantics></math> throughÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">m</mi><mtext id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">ğ‘š</ci><ci id="S3.SS1.p2.5.m5.1.1.3a.cmml" xref="S3.SS1.p2.5.m5.1.1.3"><mtext id="S3.SS1.p2.5.m5.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p2.5.m5.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> or each volume encodingÂ <math alttext="\mathcal{E}_{v}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">â„°</mi><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">â„°</ci><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathcal{E}_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">caligraphic_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> throughÂ <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">m</mi><mtext id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">ğ‘š</ci><ci id="S3.SS1.p2.7.m7.1.1.3a.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><mtext id="S3.SS1.p2.7.m7.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p2.7.m7.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>, computing individual <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.7.2">streams</span> of intermediate activations that interact with each other at intermittent layers.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.11">Specifically, for voxel featuresÂ <math alttext="a_{s}\in\mathbb{R}^{c}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><msub id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2.2" xref="S3.SS1.p3.1.m1.1.1.2.2.cmml">a</mi><mi id="S3.SS1.p3.1.m1.1.1.2.3" xref="S3.SS1.p3.1.m1.1.1.2.3.cmml">s</mi></msub><mo id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.3.2.cmml">â„</mi><mi id="S3.SS1.p3.1.m1.1.1.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.cmml">c</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><in id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></in><apply id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2.2">ğ‘</ci><ci id="S3.SS1.p3.1.m1.1.1.2.3.cmml" xref="S3.SS1.p3.1.m1.1.1.2.3">ğ‘ </ci></apply><apply id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2">â„</ci><ci id="S3.SS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">a_{s}\in\mathbb{R}^{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_a start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math> in volume streamÂ <math alttext="s" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_s</annotation></semantics></math>, computed by a convolutional layer withÂ <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">italic_c</annotation></semantics></math> output channels, we concatenateÂ <math alttext="a_{s}" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">ğ‘</ci><ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">a_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_a start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> with stream-specificÂ <math alttext="\phi_{s}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">Ï•</mi><mi id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">italic-Ï•</ci><ci id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\phi_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m5.1d">italic_Ï• start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> along the channel dimension. We then use a fully-connected layer to yieldÂ <math alttext="a^{\prime}_{s}\in\mathbb{R}^{c}" class="ltx_Math" display="inline" id="S3.SS1.p3.6.m6.1"><semantics id="S3.SS1.p3.6.m6.1a"><mrow id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><msubsup id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml"><mi id="S3.SS1.p3.6.m6.1.1.2.2.2" xref="S3.SS1.p3.6.m6.1.1.2.2.2.cmml">a</mi><mi id="S3.SS1.p3.6.m6.1.1.2.3" xref="S3.SS1.p3.6.m6.1.1.2.3.cmml">s</mi><mo id="S3.SS1.p3.6.m6.1.1.2.2.3" xref="S3.SS1.p3.6.m6.1.1.2.2.3.cmml">â€²</mo></msubsup><mo id="S3.SS1.p3.6.m6.1.1.1" xref="S3.SS1.p3.6.m6.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml"><mi id="S3.SS1.p3.6.m6.1.1.3.2" xref="S3.SS1.p3.6.m6.1.1.3.2.cmml">â„</mi><mi id="S3.SS1.p3.6.m6.1.1.3.3" xref="S3.SS1.p3.6.m6.1.1.3.3.cmml">c</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><in id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1.1"></in><apply id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.2.1.cmml" xref="S3.SS1.p3.6.m6.1.1.2">subscript</csymbol><apply id="S3.SS1.p3.6.m6.1.1.2.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.2.2.1.cmml" xref="S3.SS1.p3.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.2.2.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2.2.2">ğ‘</ci><ci id="S3.SS1.p3.6.m6.1.1.2.2.3.cmml" xref="S3.SS1.p3.6.m6.1.1.2.2.3">â€²</ci></apply><ci id="S3.SS1.p3.6.m6.1.1.2.3.cmml" xref="S3.SS1.p3.6.m6.1.1.2.3">ğ‘ </ci></apply><apply id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.3.1.cmml" xref="S3.SS1.p3.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.3.2.cmml" xref="S3.SS1.p3.6.m6.1.1.3.2">â„</ci><ci id="S3.SS1.p3.6.m6.1.1.3.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">a^{\prime}_{s}\in\mathbb{R}^{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.6.m6.1d">italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT</annotation></semantics></math>. At a given spatial location, we stack corresponding voxel representationsÂ <math alttext="a^{\prime}_{s}" class="ltx_Math" display="inline" id="S3.SS1.p3.7.m7.1"><semantics id="S3.SS1.p3.7.m7.1a"><msubsup id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml"><mi id="S3.SS1.p3.7.m7.1.1.2.2" xref="S3.SS1.p3.7.m7.1.1.2.2.cmml">a</mi><mi id="S3.SS1.p3.7.m7.1.1.3" xref="S3.SS1.p3.7.m7.1.1.3.cmml">s</mi><mo id="S3.SS1.p3.7.m7.1.1.2.3" xref="S3.SS1.p3.7.m7.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><apply id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.1.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">subscript</csymbol><apply id="S3.SS1.p3.7.m7.1.1.2.cmml" xref="S3.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.7.m7.1.1.2.1.cmml" xref="S3.SS1.p3.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.p3.7.m7.1.1.2.2.cmml" xref="S3.SS1.p3.7.m7.1.1.2.2">ğ‘</ci><ci id="S3.SS1.p3.7.m7.1.1.2.3.cmml" xref="S3.SS1.p3.7.m7.1.1.2.3">â€²</ci></apply><ci id="S3.SS1.p3.7.m7.1.1.3.cmml" xref="S3.SS1.p3.7.m7.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">a^{\prime}_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.7.m7.1d">italic_a start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> acrossÂ <math alttext="S" class="ltx_Math" display="inline" id="S3.SS1.p3.8.m8.1"><semantics id="S3.SS1.p3.8.m8.1a"><mi id="S3.SS1.p3.8.m8.1.1" xref="S3.SS1.p3.8.m8.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m8.1b"><ci id="S3.SS1.p3.8.m8.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.8.m8.1d">italic_S</annotation></semantics></math> streams to constructÂ <math alttext="A^{\prime}\in\mathbb{R}^{S,c}" class="ltx_Math" display="inline" id="S3.SS1.p3.9.m9.2"><semantics id="S3.SS1.p3.9.m9.2a"><mrow id="S3.SS1.p3.9.m9.2.3" xref="S3.SS1.p3.9.m9.2.3.cmml"><msup id="S3.SS1.p3.9.m9.2.3.2" xref="S3.SS1.p3.9.m9.2.3.2.cmml"><mi id="S3.SS1.p3.9.m9.2.3.2.2" xref="S3.SS1.p3.9.m9.2.3.2.2.cmml">A</mi><mo id="S3.SS1.p3.9.m9.2.3.2.3" xref="S3.SS1.p3.9.m9.2.3.2.3.cmml">â€²</mo></msup><mo id="S3.SS1.p3.9.m9.2.3.1" xref="S3.SS1.p3.9.m9.2.3.1.cmml">âˆˆ</mo><msup id="S3.SS1.p3.9.m9.2.3.3" xref="S3.SS1.p3.9.m9.2.3.3.cmml"><mi id="S3.SS1.p3.9.m9.2.3.3.2" xref="S3.SS1.p3.9.m9.2.3.3.2.cmml">â„</mi><mrow id="S3.SS1.p3.9.m9.2.2.2.4" xref="S3.SS1.p3.9.m9.2.2.2.3.cmml"><mi id="S3.SS1.p3.9.m9.1.1.1.1" xref="S3.SS1.p3.9.m9.1.1.1.1.cmml">S</mi><mo id="S3.SS1.p3.9.m9.2.2.2.4.1" xref="S3.SS1.p3.9.m9.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p3.9.m9.2.2.2.2" xref="S3.SS1.p3.9.m9.2.2.2.2.cmml">c</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.m9.2b"><apply id="S3.SS1.p3.9.m9.2.3.cmml" xref="S3.SS1.p3.9.m9.2.3"><in id="S3.SS1.p3.9.m9.2.3.1.cmml" xref="S3.SS1.p3.9.m9.2.3.1"></in><apply id="S3.SS1.p3.9.m9.2.3.2.cmml" xref="S3.SS1.p3.9.m9.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p3.9.m9.2.3.2.1.cmml" xref="S3.SS1.p3.9.m9.2.3.2">superscript</csymbol><ci id="S3.SS1.p3.9.m9.2.3.2.2.cmml" xref="S3.SS1.p3.9.m9.2.3.2.2">ğ´</ci><ci id="S3.SS1.p3.9.m9.2.3.2.3.cmml" xref="S3.SS1.p3.9.m9.2.3.2.3">â€²</ci></apply><apply id="S3.SS1.p3.9.m9.2.3.3.cmml" xref="S3.SS1.p3.9.m9.2.3.3"><csymbol cd="ambiguous" id="S3.SS1.p3.9.m9.2.3.3.1.cmml" xref="S3.SS1.p3.9.m9.2.3.3">superscript</csymbol><ci id="S3.SS1.p3.9.m9.2.3.3.2.cmml" xref="S3.SS1.p3.9.m9.2.3.3.2">â„</ci><list id="S3.SS1.p3.9.m9.2.2.2.3.cmml" xref="S3.SS1.p3.9.m9.2.2.2.4"><ci id="S3.SS1.p3.9.m9.1.1.1.1.cmml" xref="S3.SS1.p3.9.m9.1.1.1.1">ğ‘†</ci><ci id="S3.SS1.p3.9.m9.2.2.2.2.cmml" xref="S3.SS1.p3.9.m9.2.2.2.2">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.m9.2c">A^{\prime}\in\mathbb{R}^{S,c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.9.m9.2d">italic_A start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_S , italic_c end_POSTSUPERSCRIPT</annotation></semantics></math>. We interact streams inÂ <math alttext="A^{\prime}" class="ltx_Math" display="inline" id="S3.SS1.p3.10.m10.1"><semantics id="S3.SS1.p3.10.m10.1a"><msup id="S3.SS1.p3.10.m10.1.1" xref="S3.SS1.p3.10.m10.1.1.cmml"><mi id="S3.SS1.p3.10.m10.1.1.2" xref="S3.SS1.p3.10.m10.1.1.2.cmml">A</mi><mo id="S3.SS1.p3.10.m10.1.1.3" xref="S3.SS1.p3.10.m10.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.10.m10.1b"><apply id="S3.SS1.p3.10.m10.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.10.m10.1.1.1.cmml" xref="S3.SS1.p3.10.m10.1.1">superscript</csymbol><ci id="S3.SS1.p3.10.m10.1.1.2.cmml" xref="S3.SS1.p3.10.m10.1.1.2">ğ´</ci><ci id="S3.SS1.p3.10.m10.1.1.3.cmml" xref="S3.SS1.p3.10.m10.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.10.m10.1c">A^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.10.m10.1d">italic_A start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math> using an attention mechanism with dimensionÂ <math alttext="b" class="ltx_Math" display="inline" id="S3.SS1.p3.11.m11.1"><semantics id="S3.SS1.p3.11.m11.1a"><mi id="S3.SS1.p3.11.m11.1.1" xref="S3.SS1.p3.11.m11.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.11.m11.1b"><ci id="S3.SS1.p3.11.m11.1.1.cmml" xref="S3.SS1.p3.11.m11.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.11.m11.1c">b</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.11.m11.1d">italic_b</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="B=f\big{(}~{}\text{softmax}(~{}QK^{T}b^{-1/2}~{})~{}V~{}\big{)}+A," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">B</mi><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">f</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2" maxsize="120%" minsize="120%" rspace="0.330em" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3a.cmml">softmax</mtext><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2" rspace="0.330em" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">Q</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><msup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">K</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">T</mi></msup><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><msup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.cmml">b</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.cmml">âˆ’</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.2.cmml">1</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.1.cmml">/</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.3.cmml">2</mn></mrow></mrow></msup></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2a" lspace="0.330em" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">â¢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.4.cmml">V</mi></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3" lspace="0.330em" maxsize="120%" minsize="120%" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">+</mo><mi id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">A</mi></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">ğµ</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">ğ‘“</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3"><mtext id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">softmax</mtext></ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘„</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¾</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘‡</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.2">ğ‘</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2"><divide id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.1"></divide><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.2.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.2">1</cn><cn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.4.3.2.3">2</cn></apply></apply></apply></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.4">ğ‘‰</ci></apply></apply><ci id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">ğ´</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">B=f\big{(}~{}\text{softmax}(~{}QK^{T}b^{-1/2}~{})~{}V~{}\big{)}+A,</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_B = italic_f ( softmax ( italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_b start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT ) italic_V ) + italic_A ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p3.16">whereÂ <math alttext="Q,K,V\in\mathbb{R}^{S,b}" class="ltx_Math" display="inline" id="S3.SS1.p3.12.m1.5"><semantics id="S3.SS1.p3.12.m1.5a"><mrow id="S3.SS1.p3.12.m1.5.6" xref="S3.SS1.p3.12.m1.5.6.cmml"><mrow id="S3.SS1.p3.12.m1.5.6.2.2" xref="S3.SS1.p3.12.m1.5.6.2.1.cmml"><mi id="S3.SS1.p3.12.m1.3.3" xref="S3.SS1.p3.12.m1.3.3.cmml">Q</mi><mo id="S3.SS1.p3.12.m1.5.6.2.2.1" xref="S3.SS1.p3.12.m1.5.6.2.1.cmml">,</mo><mi id="S3.SS1.p3.12.m1.4.4" xref="S3.SS1.p3.12.m1.4.4.cmml">K</mi><mo id="S3.SS1.p3.12.m1.5.6.2.2.2" xref="S3.SS1.p3.12.m1.5.6.2.1.cmml">,</mo><mi id="S3.SS1.p3.12.m1.5.5" xref="S3.SS1.p3.12.m1.5.5.cmml">V</mi></mrow><mo id="S3.SS1.p3.12.m1.5.6.1" xref="S3.SS1.p3.12.m1.5.6.1.cmml">âˆˆ</mo><msup id="S3.SS1.p3.12.m1.5.6.3" xref="S3.SS1.p3.12.m1.5.6.3.cmml"><mi id="S3.SS1.p3.12.m1.5.6.3.2" xref="S3.SS1.p3.12.m1.5.6.3.2.cmml">â„</mi><mrow id="S3.SS1.p3.12.m1.2.2.2.4" xref="S3.SS1.p3.12.m1.2.2.2.3.cmml"><mi id="S3.SS1.p3.12.m1.1.1.1.1" xref="S3.SS1.p3.12.m1.1.1.1.1.cmml">S</mi><mo id="S3.SS1.p3.12.m1.2.2.2.4.1" xref="S3.SS1.p3.12.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p3.12.m1.2.2.2.2" xref="S3.SS1.p3.12.m1.2.2.2.2.cmml">b</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.12.m1.5b"><apply id="S3.SS1.p3.12.m1.5.6.cmml" xref="S3.SS1.p3.12.m1.5.6"><in id="S3.SS1.p3.12.m1.5.6.1.cmml" xref="S3.SS1.p3.12.m1.5.6.1"></in><list id="S3.SS1.p3.12.m1.5.6.2.1.cmml" xref="S3.SS1.p3.12.m1.5.6.2.2"><ci id="S3.SS1.p3.12.m1.3.3.cmml" xref="S3.SS1.p3.12.m1.3.3">ğ‘„</ci><ci id="S3.SS1.p3.12.m1.4.4.cmml" xref="S3.SS1.p3.12.m1.4.4">ğ¾</ci><ci id="S3.SS1.p3.12.m1.5.5.cmml" xref="S3.SS1.p3.12.m1.5.5">ğ‘‰</ci></list><apply id="S3.SS1.p3.12.m1.5.6.3.cmml" xref="S3.SS1.p3.12.m1.5.6.3"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m1.5.6.3.1.cmml" xref="S3.SS1.p3.12.m1.5.6.3">superscript</csymbol><ci id="S3.SS1.p3.12.m1.5.6.3.2.cmml" xref="S3.SS1.p3.12.m1.5.6.3.2">â„</ci><list id="S3.SS1.p3.12.m1.2.2.2.3.cmml" xref="S3.SS1.p3.12.m1.2.2.2.4"><ci id="S3.SS1.p3.12.m1.1.1.1.1.cmml" xref="S3.SS1.p3.12.m1.1.1.1.1">ğ‘†</ci><ci id="S3.SS1.p3.12.m1.2.2.2.2.cmml" xref="S3.SS1.p3.12.m1.2.2.2.2">ğ‘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.12.m1.5c">Q,K,V\in\mathbb{R}^{S,b}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.12.m1.5d">italic_Q , italic_K , italic_V âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_S , italic_b end_POSTSUPERSCRIPT</annotation></semantics></math> are linear transformations ofÂ <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.p3.13.m2.1"><semantics id="S3.SS1.p3.13.m2.1a"><mi id="S3.SS1.p3.13.m2.1.1" xref="S3.SS1.p3.13.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.13.m2.1b"><ci id="S3.SS1.p3.13.m2.1.1.cmml" xref="S3.SS1.p3.13.m2.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.13.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.13.m2.1d">italic_A</annotation></semantics></math>, and fully-connected layerÂ <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p3.14.m3.1"><semantics id="S3.SS1.p3.14.m3.1a"><mi id="S3.SS1.p3.14.m3.1.1" xref="S3.SS1.p3.14.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.14.m3.1b"><ci id="S3.SS1.p3.14.m3.1.1.cmml" xref="S3.SS1.p3.14.m3.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.14.m3.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.14.m3.1d">italic_f</annotation></semantics></math> projects the output toÂ <math alttext="\mathbb{R}^{S,c}" class="ltx_Math" display="inline" id="S3.SS1.p3.15.m4.2"><semantics id="S3.SS1.p3.15.m4.2a"><msup id="S3.SS1.p3.15.m4.2.3" xref="S3.SS1.p3.15.m4.2.3.cmml"><mi id="S3.SS1.p3.15.m4.2.3.2" xref="S3.SS1.p3.15.m4.2.3.2.cmml">â„</mi><mrow id="S3.SS1.p3.15.m4.2.2.2.4" xref="S3.SS1.p3.15.m4.2.2.2.3.cmml"><mi id="S3.SS1.p3.15.m4.1.1.1.1" xref="S3.SS1.p3.15.m4.1.1.1.1.cmml">S</mi><mo id="S3.SS1.p3.15.m4.2.2.2.4.1" xref="S3.SS1.p3.15.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p3.15.m4.2.2.2.2" xref="S3.SS1.p3.15.m4.2.2.2.2.cmml">c</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.15.m4.2b"><apply id="S3.SS1.p3.15.m4.2.3.cmml" xref="S3.SS1.p3.15.m4.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.15.m4.2.3.1.cmml" xref="S3.SS1.p3.15.m4.2.3">superscript</csymbol><ci id="S3.SS1.p3.15.m4.2.3.2.cmml" xref="S3.SS1.p3.15.m4.2.3.2">â„</ci><list id="S3.SS1.p3.15.m4.2.2.2.3.cmml" xref="S3.SS1.p3.15.m4.2.2.2.4"><ci id="S3.SS1.p3.15.m4.1.1.1.1.cmml" xref="S3.SS1.p3.15.m4.1.1.1.1">ğ‘†</ci><ci id="S3.SS1.p3.15.m4.2.2.2.2.cmml" xref="S3.SS1.p3.15.m4.2.2.2.2">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.15.m4.2c">\mathbb{R}^{S,c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.15.m4.2d">blackboard_R start_POSTSUPERSCRIPT italic_S , italic_c end_POSTSUPERSCRIPT</annotation></semantics></math>. We then separate <math alttext="B" class="ltx_Math" display="inline" id="S3.SS1.p3.16.m5.1"><semantics id="S3.SS1.p3.16.m5.1a"><mi id="S3.SS1.p3.16.m5.1.1" xref="S3.SS1.p3.16.m5.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.16.m5.1b"><ci id="S3.SS1.p3.16.m5.1.1.cmml" xref="S3.SS1.p3.16.m5.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.16.m5.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.16.m5.1d">italic_B</annotation></semantics></math> back into stream-specific voxel features.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.7"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.7.1">Interpretable Volume Encodings.</span>
For each volumeÂ <math alttext="v" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ğ‘£</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">v</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_v</annotation></semantics></math> passed throughÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">m</mi><mtext id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">ğ‘š</ci><ci id="S3.SS1.p4.2.m2.1.1.3a.cmml" xref="S3.SS1.p4.2.m2.1.1.3"><mtext id="S3.SS1.p4.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p4.2.m2.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math>, we reduce the spatial dimensions of the deepest layer output using a global max operator. We pass these pooled features through a fully-connected layer to computeÂ <math alttext="\varepsilon_{v}^{\circ}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><msubsup id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2.2.2" xref="S3.SS1.p4.3.m3.1.1.2.2.2.cmml">Îµ</mi><mi id="S3.SS1.p4.3.m3.1.1.2.2.3" xref="S3.SS1.p4.3.m3.1.1.2.2.3.cmml">v</mi><mo id="S3.SS1.p4.3.m3.1.1.2.3" xref="S3.SS1.p4.3.m3.1.1.2.3.cmml">âˆ˜</mo></msubsup><mo id="S3.SS1.p4.3.m3.1.1.1" xref="S3.SS1.p4.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml"><mi id="S3.SS1.p4.3.m3.1.1.3.2" xref="S3.SS1.p4.3.m3.1.1.3.2.cmml">â„</mi><mi id="S3.SS1.p4.3.m3.1.1.3.3" xref="S3.SS1.p4.3.m3.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><in id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1.1"></in><apply id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.2.1.cmml" xref="S3.SS1.p4.3.m3.1.1.2">superscript</csymbol><apply id="S3.SS1.p4.3.m3.1.1.2.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.2.2.1.cmml" xref="S3.SS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.2.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2.2.2">ğœ€</ci><ci id="S3.SS1.p4.3.m3.1.1.2.2.3.cmml" xref="S3.SS1.p4.3.m3.1.1.2.2.3">ğ‘£</ci></apply><compose id="S3.SS1.p4.3.m3.1.1.2.3.cmml" xref="S3.SS1.p4.3.m3.1.1.2.3"></compose></apply><apply id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.3.1.cmml" xref="S3.SS1.p4.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.3.2.cmml" xref="S3.SS1.p4.3.m3.1.1.3.2">â„</ci><ci id="S3.SS1.p4.3.m3.1.1.3.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\varepsilon_{v}^{\circ}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">italic_Îµ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>. When a <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.7.2">read</span> instruction is executed on a set of volume encodingsÂ <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.p4.4.m4.1"><semantics id="S3.SS1.p4.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">â„°</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><ci id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">â„°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.4.m4.1d">caligraphic_E</annotation></semantics></math> defined inÂ <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.SS1.p4.5.m5.1"><semantics id="S3.SS1.p4.5.m5.1a"><mi id="S3.SS1.p4.5.m5.1.1" mathvariant="normal" xref="S3.SS1.p4.5.m5.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><ci id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.5.m5.1d">roman_Î©</annotation></semantics></math>, we concatenate eachÂ <math alttext="\varepsilon_{v}^{\circ}\in\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.p4.6.m6.1"><semantics id="S3.SS1.p4.6.m6.1a"><mrow id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml"><msubsup id="S3.SS1.p4.6.m6.1.1.2" xref="S3.SS1.p4.6.m6.1.1.2.cmml"><mi id="S3.SS1.p4.6.m6.1.1.2.2.2" xref="S3.SS1.p4.6.m6.1.1.2.2.2.cmml">Îµ</mi><mi id="S3.SS1.p4.6.m6.1.1.2.2.3" xref="S3.SS1.p4.6.m6.1.1.2.2.3.cmml">v</mi><mo id="S3.SS1.p4.6.m6.1.1.2.3" xref="S3.SS1.p4.6.m6.1.1.2.3.cmml">âˆ˜</mo></msubsup><mo id="S3.SS1.p4.6.m6.1.1.1" xref="S3.SS1.p4.6.m6.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.6.m6.1.1.3" xref="S3.SS1.p4.6.m6.1.1.3.cmml">â„°</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><apply id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1"><in id="S3.SS1.p4.6.m6.1.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1.1"></in><apply id="S3.SS1.p4.6.m6.1.1.2.cmml" xref="S3.SS1.p4.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m6.1.1.2.1.cmml" xref="S3.SS1.p4.6.m6.1.1.2">superscript</csymbol><apply id="S3.SS1.p4.6.m6.1.1.2.2.cmml" xref="S3.SS1.p4.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.6.m6.1.1.2.2.1.cmml" xref="S3.SS1.p4.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.6.m6.1.1.2.2.2.cmml" xref="S3.SS1.p4.6.m6.1.1.2.2.2">ğœ€</ci><ci id="S3.SS1.p4.6.m6.1.1.2.2.3.cmml" xref="S3.SS1.p4.6.m6.1.1.2.2.3">ğ‘£</ci></apply><compose id="S3.SS1.p4.6.m6.1.1.2.3.cmml" xref="S3.SS1.p4.6.m6.1.1.2.3"></compose></apply><ci id="S3.SS1.p4.6.m6.1.1.3.cmml" xref="S3.SS1.p4.6.m6.1.1.3">â„°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">\varepsilon_{v}^{\circ}\in\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.6.m6.1d">italic_Îµ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ˜ end_POSTSUPERSCRIPT âˆˆ caligraphic_E</annotation></semantics></math> into the feedback embeddingsÂ <math alttext="z" class="ltx_Math" display="inline" id="S3.SS1.p4.7.m7.1"><semantics id="S3.SS1.p4.7.m7.1a"><mi id="S3.SS1.p4.7.m7.1.1" xref="S3.SS1.p4.7.m7.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.1b"><ci id="S3.SS1.p4.7.m7.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.7.m7.1d">italic_z</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Supervised Training</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.12">We optimize trainable model parametersÂ <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_Î¸</annotation></semantics></math> from scratch using a diverse and carefully curated set of tasksÂ <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">ğ’¯</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">caligraphic_T</annotation></semantics></math> (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S4.SS1" title="4.1 Tasks â€£ 4 Task and Data Design â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">4.1</span></a>), whereÂ <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_Î¸</annotation></semantics></math> encompasses the parameters of the language modelÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_Î±</annotation></semantics></math> and vision networksÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">m</mi><mtext id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">ğ‘š</ci><ci id="S3.SS2.p1.5.m5.1.1.3a.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><mtext id="S3.SS2.p1.5.m5.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p1.5.m5.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> andÂ <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">m</mi><mtext id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">ğ‘š</ci><ci id="S3.SS2.p1.6.m6.1.1.3a.cmml" xref="S3.SS2.p1.6.m6.1.1.3"><mtext id="S3.SS2.p1.6.m6.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p1.6.m6.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>. For each taskÂ <math alttext="\tau\in\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><mrow id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml">Ï„</mi><mo id="S3.SS2.p1.7.m7.1.1.1" xref="S3.SS2.p1.7.m7.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml">ğ’¯</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><in id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1.1"></in><ci id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2">ğœ</ci><ci id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3">ğ’¯</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">\tau\in\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.1d">italic_Ï„ âˆˆ caligraphic_T</annotation></semantics></math>, we predefine target (ground-truth) codeÂ <math alttext="\varphi^{*}" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1"><semantics id="S3.SS2.p1.8.m8.1a"><msup id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mi id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">Ï†</mi><mo id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">superscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">ğœ‘</ci><times id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">\varphi^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m8.1d">italic_Ï† start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math> that carries out the task objective. During a training step, we randomly sampleÂ <math alttext="\tau\sim\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m9.1"><semantics id="S3.SS2.p1.9.m9.1a"><mrow id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml"><mi id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml">Ï„</mi><mo id="S3.SS2.p1.9.m9.1.1.1" xref="S3.SS2.p1.9.m9.1.1.1.cmml">âˆ¼</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.9.m9.1.1.3" xref="S3.SS2.p1.9.m9.1.1.3.cmml">ğ’¯</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1"><csymbol cd="latexml" id="S3.SS2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1.1">similar-to</csymbol><ci id="S3.SS2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2">ğœ</ci><ci id="S3.SS2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.3">ğ’¯</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">\tau\sim\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.9.m9.1d">italic_Ï„ âˆ¼ caligraphic_T</annotation></semantics></math>, synthesize a task-specific promptÂ <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m10.1"><semantics id="S3.SS2.p1.10.m10.1a"><mi id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><ci id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.10.m10.1d">italic_p</annotation></semantics></math>, and sample volume inputsÂ <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S3.SS2.p1.11.m11.1"><semantics id="S3.SS2.p1.11.m11.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">ğ’±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><ci id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">ğ’±</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.11.m11.1d">caligraphic_V</annotation></semantics></math> with ground-truth outputsÂ <math alttext="\mathcal{W}^{*}" class="ltx_Math" display="inline" id="S3.SS2.p1.12.m12.1"><semantics id="S3.SS2.p1.12.m12.1a"><msup id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.12.m12.1.1.2" xref="S3.SS2.p1.12.m12.1.1.2.cmml">ğ’²</mi><mo id="S3.SS2.p1.12.m12.1.1.3" xref="S3.SS2.p1.12.m12.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><apply id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m12.1.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">superscript</csymbol><ci id="S3.SS2.p1.12.m12.1.1.2.cmml" xref="S3.SS2.p1.12.m12.1.1.2">ğ’²</ci><times id="S3.SS2.p1.12.m12.1.1.3.cmml" xref="S3.SS2.p1.12.m12.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">\mathcal{W}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.12.m12.1d">caligraphic_W start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math>, if required. Using these samples at each agent training step, we evaluate the loss:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{ce}\big{(}P(\varphi),\varphi^{*}\big{)}+\lambda~{}\sum_{j=1}^{|%
\mathcal{W}|}\mathcal{L}_{img}\big{(}\mathcal{W}_{j},\mathcal{W}_{j}^{*}\big{)}," class="ltx_Math" display="block" id="S3.E2.m1.3"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><msub id="S3.E2.m1.3.3.1.1.2.4" xref="S3.E2.m1.3.3.1.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.2.4.2" xref="S3.E2.m1.3.3.1.1.2.4.2.cmml">â„’</mi><mrow id="S3.E2.m1.3.3.1.1.2.4.3" xref="S3.E2.m1.3.3.1.1.2.4.3.cmml"><mi id="S3.E2.m1.3.3.1.1.2.4.3.2" xref="S3.E2.m1.3.3.1.1.2.4.3.2.cmml">c</mi><mo id="S3.E2.m1.3.3.1.1.2.4.3.1" xref="S3.E2.m1.3.3.1.1.2.4.3.1.cmml">â¢</mo><mi id="S3.E2.m1.3.3.1.1.2.4.3.3" xref="S3.E2.m1.3.3.1.1.2.4.3.3.cmml">e</mi></mrow></msub><mo id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml">â¢</mo><mrow id="S3.E2.m1.3.3.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml"><mo id="S3.E2.m1.3.3.1.1.2.2.2.3" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml">P</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">â¢</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">Ï†</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.2.2.2.4" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">,</mo><msup id="S3.E2.m1.3.3.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.2.2.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.2.2.cmml">Ï†</mi><mo id="S3.E2.m1.3.3.1.1.2.2.2.2.3" xref="S3.E2.m1.3.3.1.1.2.2.2.2.3.cmml">âˆ—</mo></msup><mo id="S3.E2.m1.3.3.1.1.2.2.2.5" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.5" xref="S3.E2.m1.3.3.1.1.5.cmml">+</mo><mrow id="S3.E2.m1.3.3.1.1.4" xref="S3.E2.m1.3.3.1.1.4.cmml"><mi id="S3.E2.m1.3.3.1.1.4.4" xref="S3.E2.m1.3.3.1.1.4.4.cmml">Î»</mi><mo id="S3.E2.m1.3.3.1.1.4.3" lspace="0.497em" xref="S3.E2.m1.3.3.1.1.4.3.cmml">â¢</mo><mrow id="S3.E2.m1.3.3.1.1.4.2" xref="S3.E2.m1.3.3.1.1.4.2.cmml"><munderover id="S3.E2.m1.3.3.1.1.4.2.3" xref="S3.E2.m1.3.3.1.1.4.2.3.cmml"><mo id="S3.E2.m1.3.3.1.1.4.2.3.2.2" movablelimits="false" xref="S3.E2.m1.3.3.1.1.4.2.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.3.3.1.1.4.2.3.2.3" xref="S3.E2.m1.3.3.1.1.4.2.3.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.4.2.3.2.3.2" xref="S3.E2.m1.3.3.1.1.4.2.3.2.3.2.cmml">j</mi><mo id="S3.E2.m1.3.3.1.1.4.2.3.2.3.1" xref="S3.E2.m1.3.3.1.1.4.2.3.2.3.1.cmml">=</mo><mn id="S3.E2.m1.3.3.1.1.4.2.3.2.3.3" xref="S3.E2.m1.3.3.1.1.4.2.3.2.3.3.cmml">1</mn></mrow><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.3.1" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">ğ’²</mi><mo id="S3.E2.m1.1.1.1.3.2" stretchy="false" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></munderover><mrow id="S3.E2.m1.3.3.1.1.4.2.2" xref="S3.E2.m1.3.3.1.1.4.2.2.cmml"><msub id="S3.E2.m1.3.3.1.1.4.2.2.4" xref="S3.E2.m1.3.3.1.1.4.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.4.2.2.4.2" xref="S3.E2.m1.3.3.1.1.4.2.2.4.2.cmml">â„’</mi><mrow id="S3.E2.m1.3.3.1.1.4.2.2.4.3" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.cmml"><mi id="S3.E2.m1.3.3.1.1.4.2.2.4.3.2" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.4.2.2.4.3.1" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.1.cmml">â¢</mo><mi id="S3.E2.m1.3.3.1.1.4.2.2.4.3.3" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.3.cmml">m</mi><mo id="S3.E2.m1.3.3.1.1.4.2.2.4.3.1a" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.1.cmml">â¢</mo><mi id="S3.E2.m1.3.3.1.1.4.2.2.4.3.4" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.4.cmml">g</mi></mrow></msub><mo id="S3.E2.m1.3.3.1.1.4.2.2.3" xref="S3.E2.m1.3.3.1.1.4.2.2.3.cmml">â¢</mo><mrow id="S3.E2.m1.3.3.1.1.4.2.2.2.2" xref="S3.E2.m1.3.3.1.1.4.2.2.2.3.cmml"><mo id="S3.E2.m1.3.3.1.1.4.2.2.2.2.3" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.4.2.2.2.3.cmml">(</mo><msub id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.2.cmml">ğ’²</mi><mi id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S3.E2.m1.3.3.1.1.4.2.2.2.2.4" xref="S3.E2.m1.3.3.1.1.4.2.2.2.3.cmml">,</mo><msubsup id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.2" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.2.cmml">ğ’²</mi><mi id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.3" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.3.cmml">j</mi><mo id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.3" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.3.cmml">âˆ—</mo></msubsup><mo id="S3.E2.m1.3.3.1.1.4.2.2.2.2.5" maxsize="120%" minsize="120%" xref="S3.E2.m1.3.3.1.1.4.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><plus id="S3.E2.m1.3.3.1.1.5.cmml" xref="S3.E2.m1.3.3.1.1.5"></plus><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><times id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3"></times><apply id="S3.E2.m1.3.3.1.1.2.4.cmml" xref="S3.E2.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.4.1.cmml" xref="S3.E2.m1.3.3.1.1.2.4">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.4.2.cmml" xref="S3.E2.m1.3.3.1.1.2.4.2">â„’</ci><apply id="S3.E2.m1.3.3.1.1.2.4.3.cmml" xref="S3.E2.m1.3.3.1.1.2.4.3"><times id="S3.E2.m1.3.3.1.1.2.4.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.4.3.1"></times><ci id="S3.E2.m1.3.3.1.1.2.4.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.4.3.2">ğ‘</ci><ci id="S3.E2.m1.3.3.1.1.2.4.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.4.3.3">ğ‘’</ci></apply></apply><interval closure="open" id="S3.E2.m1.3.3.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1"></times><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2">ğ‘ƒ</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ğœ‘</ci></apply><apply id="S3.E2.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E2.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2.2">ğœ‘</ci><times id="S3.E2.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2.2.3"></times></apply></interval></apply><apply id="S3.E2.m1.3.3.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.4"><times id="S3.E2.m1.3.3.1.1.4.3.cmml" xref="S3.E2.m1.3.3.1.1.4.3"></times><ci id="S3.E2.m1.3.3.1.1.4.4.cmml" xref="S3.E2.m1.3.3.1.1.4.4">ğœ†</ci><apply id="S3.E2.m1.3.3.1.1.4.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2"><apply id="S3.E2.m1.3.3.1.1.4.2.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.4.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.3">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.4.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.4.2.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.3">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.4.2.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.3.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.4.2.3.2.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.3.2.3"><eq id="S3.E2.m1.3.3.1.1.4.2.3.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.3.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.4.2.3.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.3.2.3.2">ğ‘—</ci><cn id="S3.E2.m1.3.3.1.1.4.2.3.2.3.3.cmml" type="integer" xref="S3.E2.m1.3.3.1.1.4.2.3.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.3"><abs id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></abs><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ğ’²</ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.4.2.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2"><times id="S3.E2.m1.3.3.1.1.4.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.3"></times><apply id="S3.E2.m1.3.3.1.1.4.2.2.4.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.4.2.2.4.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.4.2.2.4.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4.2">â„’</ci><apply id="S3.E2.m1.3.3.1.1.4.2.2.4.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3"><times id="S3.E2.m1.3.3.1.1.4.2.2.4.3.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.1"></times><ci id="S3.E2.m1.3.3.1.1.4.2.2.4.3.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.2">ğ‘–</ci><ci id="S3.E2.m1.3.3.1.1.4.2.2.4.3.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.3">ğ‘š</ci><ci id="S3.E2.m1.3.3.1.1.4.2.2.4.3.4.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.4.3.4">ğ‘”</ci></apply></apply><interval closure="open" id="S3.E2.m1.3.3.1.1.4.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2"><apply id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.2">ğ’²</ci><ci id="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.2">ğ’²</ci><ci id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.2.3">ğ‘—</ci></apply><times id="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.4.2.2.2.2.2.3"></times></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\mathcal{L}_{ce}\big{(}P(\varphi),\varphi^{*}\big{)}+\lambda~{}\sum_{j=1}^{|%
\mathcal{W}|}\mathcal{L}_{img}\big{(}\mathcal{W}_{j},\mathcal{W}_{j}^{*}\big{)},</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">caligraphic_L start_POSTSUBSCRIPT italic_c italic_e end_POSTSUBSCRIPT ( italic_P ( italic_Ï† ) , italic_Ï† start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) + italic_Î» âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_W | end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT ( caligraphic_W start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , caligraphic_W start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.19">whereÂ <math alttext="P(\varphi)" class="ltx_Math" display="inline" id="S3.SS2.p1.13.m1.1"><semantics id="S3.SS2.p1.13.m1.1a"><mrow id="S3.SS2.p1.13.m1.1.2" xref="S3.SS2.p1.13.m1.1.2.cmml"><mi id="S3.SS2.p1.13.m1.1.2.2" xref="S3.SS2.p1.13.m1.1.2.2.cmml">P</mi><mo id="S3.SS2.p1.13.m1.1.2.1" xref="S3.SS2.p1.13.m1.1.2.1.cmml">â¢</mo><mrow id="S3.SS2.p1.13.m1.1.2.3.2" xref="S3.SS2.p1.13.m1.1.2.cmml"><mo id="S3.SS2.p1.13.m1.1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.13.m1.1.2.cmml">(</mo><mi id="S3.SS2.p1.13.m1.1.1" xref="S3.SS2.p1.13.m1.1.1.cmml">Ï†</mi><mo id="S3.SS2.p1.13.m1.1.2.3.2.2" stretchy="false" xref="S3.SS2.p1.13.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m1.1b"><apply id="S3.SS2.p1.13.m1.1.2.cmml" xref="S3.SS2.p1.13.m1.1.2"><times id="S3.SS2.p1.13.m1.1.2.1.cmml" xref="S3.SS2.p1.13.m1.1.2.1"></times><ci id="S3.SS2.p1.13.m1.1.2.2.cmml" xref="S3.SS2.p1.13.m1.1.2.2">ğ‘ƒ</ci><ci id="S3.SS2.p1.13.m1.1.1.cmml" xref="S3.SS2.p1.13.m1.1.1">ğœ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m1.1c">P(\varphi)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.13.m1.1d">italic_P ( italic_Ï† )</annotation></semantics></math> is output by the language model and volumesÂ <math alttext="\mathcal{W}" class="ltx_Math" display="inline" id="S3.SS2.p1.14.m2.1"><semantics id="S3.SS2.p1.14.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.14.m2.1.1" xref="S3.SS2.p1.14.m2.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m2.1b"><ci id="S3.SS2.p1.14.m2.1.1.cmml" xref="S3.SS2.p1.14.m2.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.14.m2.1c">\mathcal{W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.14.m2.1d">caligraphic_W</annotation></semantics></math> are generated by the vision networks while executingÂ <math alttext="\varphi^{*}" class="ltx_Math" display="inline" id="S3.SS2.p1.15.m3.1"><semantics id="S3.SS2.p1.15.m3.1a"><msup id="S3.SS2.p1.15.m3.1.1" xref="S3.SS2.p1.15.m3.1.1.cmml"><mi id="S3.SS2.p1.15.m3.1.1.2" xref="S3.SS2.p1.15.m3.1.1.2.cmml">Ï†</mi><mo id="S3.SS2.p1.15.m3.1.1.3" xref="S3.SS2.p1.15.m3.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.15.m3.1b"><apply id="S3.SS2.p1.15.m3.1.1.cmml" xref="S3.SS2.p1.15.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.15.m3.1.1.1.cmml" xref="S3.SS2.p1.15.m3.1.1">superscript</csymbol><ci id="S3.SS2.p1.15.m3.1.1.2.cmml" xref="S3.SS2.p1.15.m3.1.1.2">ğœ‘</ci><times id="S3.SS2.p1.15.m3.1.1.3.cmml" xref="S3.SS2.p1.15.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.15.m3.1c">\varphi^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.15.m3.1d">italic_Ï† start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math>. The functionÂ <math alttext="\mathcal{L}_{ce}" class="ltx_Math" display="inline" id="S3.SS2.p1.16.m4.1"><semantics id="S3.SS2.p1.16.m4.1a"><msub id="S3.SS2.p1.16.m4.1.1" xref="S3.SS2.p1.16.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.16.m4.1.1.2" xref="S3.SS2.p1.16.m4.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.p1.16.m4.1.1.3" xref="S3.SS2.p1.16.m4.1.1.3.cmml"><mi id="S3.SS2.p1.16.m4.1.1.3.2" xref="S3.SS2.p1.16.m4.1.1.3.2.cmml">c</mi><mo id="S3.SS2.p1.16.m4.1.1.3.1" xref="S3.SS2.p1.16.m4.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.16.m4.1.1.3.3" xref="S3.SS2.p1.16.m4.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.16.m4.1b"><apply id="S3.SS2.p1.16.m4.1.1.cmml" xref="S3.SS2.p1.16.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.16.m4.1.1.1.cmml" xref="S3.SS2.p1.16.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.16.m4.1.1.2.cmml" xref="S3.SS2.p1.16.m4.1.1.2">â„’</ci><apply id="S3.SS2.p1.16.m4.1.1.3.cmml" xref="S3.SS2.p1.16.m4.1.1.3"><times id="S3.SS2.p1.16.m4.1.1.3.1.cmml" xref="S3.SS2.p1.16.m4.1.1.3.1"></times><ci id="S3.SS2.p1.16.m4.1.1.3.2.cmml" xref="S3.SS2.p1.16.m4.1.1.3.2">ğ‘</ci><ci id="S3.SS2.p1.16.m4.1.1.3.3.cmml" xref="S3.SS2.p1.16.m4.1.1.3.3">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.16.m4.1c">\mathcal{L}_{ce}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.16.m4.1d">caligraphic_L start_POSTSUBSCRIPT italic_c italic_e end_POSTSUBSCRIPT</annotation></semantics></math> measures cross-entropy between predicted vocabulary probabilities and tokenized target text, and <math alttext="\mathcal{L}_{img}" class="ltx_Math" display="inline" id="S3.SS2.p1.17.m5.1"><semantics id="S3.SS2.p1.17.m5.1a"><msub id="S3.SS2.p1.17.m5.1.1" xref="S3.SS2.p1.17.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.17.m5.1.1.2" xref="S3.SS2.p1.17.m5.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.p1.17.m5.1.1.3" xref="S3.SS2.p1.17.m5.1.1.3.cmml"><mi id="S3.SS2.p1.17.m5.1.1.3.2" xref="S3.SS2.p1.17.m5.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p1.17.m5.1.1.3.1" xref="S3.SS2.p1.17.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.17.m5.1.1.3.3" xref="S3.SS2.p1.17.m5.1.1.3.3.cmml">m</mi><mo id="S3.SS2.p1.17.m5.1.1.3.1a" xref="S3.SS2.p1.17.m5.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.17.m5.1.1.3.4" xref="S3.SS2.p1.17.m5.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.17.m5.1b"><apply id="S3.SS2.p1.17.m5.1.1.cmml" xref="S3.SS2.p1.17.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.17.m5.1.1.1.cmml" xref="S3.SS2.p1.17.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.17.m5.1.1.2.cmml" xref="S3.SS2.p1.17.m5.1.1.2">â„’</ci><apply id="S3.SS2.p1.17.m5.1.1.3.cmml" xref="S3.SS2.p1.17.m5.1.1.3"><times id="S3.SS2.p1.17.m5.1.1.3.1.cmml" xref="S3.SS2.p1.17.m5.1.1.3.1"></times><ci id="S3.SS2.p1.17.m5.1.1.3.2.cmml" xref="S3.SS2.p1.17.m5.1.1.3.2">ğ‘–</ci><ci id="S3.SS2.p1.17.m5.1.1.3.3.cmml" xref="S3.SS2.p1.17.m5.1.1.3.3">ğ‘š</ci><ci id="S3.SS2.p1.17.m5.1.1.3.4.cmml" xref="S3.SS2.p1.17.m5.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.17.m5.1c">\mathcal{L}_{img}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.17.m5.1d">caligraphic_L start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT</annotation></semantics></math> measures differences between a predicted and target image, weighted by a scalarÂ <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.p1.18.m6.1"><semantics id="S3.SS2.p1.18.m6.1a"><mi id="S3.SS2.p1.18.m6.1.1" xref="S3.SS2.p1.18.m6.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.18.m6.1b"><ci id="S3.SS2.p1.18.m6.1.1.cmml" xref="S3.SS2.p1.18.m6.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.18.m6.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.18.m6.1d">italic_Î»</annotation></semantics></math>. In our experiments, we focus on the most consequential type of intermediate volume for ROI processing â€“ spatial segmentation. Therefore, we set <math alttext="\mathcal{L}_{img}" class="ltx_Math" display="inline" id="S3.SS2.p1.19.m7.1"><semantics id="S3.SS2.p1.19.m7.1a"><msub id="S3.SS2.p1.19.m7.1.1" xref="S3.SS2.p1.19.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.19.m7.1.1.2" xref="S3.SS2.p1.19.m7.1.1.2.cmml">â„’</mi><mrow id="S3.SS2.p1.19.m7.1.1.3" xref="S3.SS2.p1.19.m7.1.1.3.cmml"><mi id="S3.SS2.p1.19.m7.1.1.3.2" xref="S3.SS2.p1.19.m7.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p1.19.m7.1.1.3.1" xref="S3.SS2.p1.19.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.19.m7.1.1.3.3" xref="S3.SS2.p1.19.m7.1.1.3.3.cmml">m</mi><mo id="S3.SS2.p1.19.m7.1.1.3.1a" xref="S3.SS2.p1.19.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.19.m7.1.1.3.4" xref="S3.SS2.p1.19.m7.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.19.m7.1b"><apply id="S3.SS2.p1.19.m7.1.1.cmml" xref="S3.SS2.p1.19.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.19.m7.1.1.1.cmml" xref="S3.SS2.p1.19.m7.1.1">subscript</csymbol><ci id="S3.SS2.p1.19.m7.1.1.2.cmml" xref="S3.SS2.p1.19.m7.1.1.2">â„’</ci><apply id="S3.SS2.p1.19.m7.1.1.3.cmml" xref="S3.SS2.p1.19.m7.1.1.3"><times id="S3.SS2.p1.19.m7.1.1.3.1.cmml" xref="S3.SS2.p1.19.m7.1.1.3.1"></times><ci id="S3.SS2.p1.19.m7.1.1.3.2.cmml" xref="S3.SS2.p1.19.m7.1.1.3.2">ğ‘–</ci><ci id="S3.SS2.p1.19.m7.1.1.3.3.cmml" xref="S3.SS2.p1.19.m7.1.1.3.3">ğ‘š</ci><ci id="S3.SS2.p1.19.m7.1.1.3.4.cmml" xref="S3.SS2.p1.19.m7.1.1.3.4">ğ‘”</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.19.m7.1c">\mathcal{L}_{img}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.19.m7.1d">caligraphic_L start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT</annotation></semantics></math> as the soft Dice lossÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib124" title=""><span class="ltx_text" style="font-size:90%;">124</span></a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Implementation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.3"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.3.1">Development Tools.</span>
We implement VoxelPrompt with PyTorchÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib125" title=""><span class="ltx_text" style="font-size:90%;">125</span></a>]</cite> and use Python as the programming language ofÂ <math alttext="\varphi" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">Ï†</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğœ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\varphi</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_Ï†</annotation></semantics></math> andÂ <math alttext="\Omega" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS3.p1.2.m2.1.1.cmml">Î©</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">Î©</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\Omega</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">roman_Î©</annotation></semantics></math>. To support the wide range of imaging operations required byÂ <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">ğ’¯</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">caligraphic_T</annotation></semantics></math>, we develop and use a PyTorch library of volumetric medical image utilities, called <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.3.2">Voxel</span>, which we release as an open-source package at <a class="ltx_ref ltx_url ltx_font_typewriter" href="github.com/dalcalab/voxel" title="">github.com/dalcalab/voxel</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.6"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.6.1">Agent Model.</span>
We implement the agent modelÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_Î±</annotation></semantics></math> as a decoder-only transformer stack, using a randomly initialized LLaMA architectureÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib126" title=""><span class="ltx_text" style="font-size:90%;">126</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib127" title=""><span class="ltx_text" style="font-size:90%;">127</span></a>]</cite> with <math alttext="16" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mn id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><cn id="S3.SS3.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS3.p2.2.m2.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">16</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">16</annotation></semantics></math>Â transformer layers, a hidden representation of sizeÂ <math alttext="d=512" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">d</mi><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><eq id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></eq><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">ğ‘‘</ci><cn id="S3.SS3.p2.3.m3.1.1.3.cmml" type="integer" xref="S3.SS3.p2.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">d=512</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_d = 512</annotation></semantics></math>, a linear representation of sizeÂ <math alttext="2,048" class="ltx_markedasmath" display="inline" id="S3.SS3.p2.4.m4.1.1.m1.2"><semantics id="S3.SS3.p2.4.m4.1.1.m1.2a"><mrow id="S3.SS3.p2.4.m4.1.1.m1.2.2.4"><mn id="S3.SS3.p2.4.m4.1.1.m1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.m1.2.2.3.cmml">2</mn><mo id="S3.SS3.p2.4.m4.1.1.m1.2.2.4.1" xref="S3.SS3.p2.4.m4.1.1.m1.2.2.3.cmml">,</mo><mn id="S3.SS3.p2.4.m4.1.1.m1.2.2.2.2" xref="S3.SS3.p2.4.m4.1.1.m1.2.2.3.cmml">048</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1.1.m1.2b"><cn id="S3.SS3.p2.4.m4.1.1.m1.2.2.3.cmml" type="integer" xref="S3.SS3.p2.4.m4.1.1.m1.1.1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1.1.m1.2c">2,048</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1.1.m1.2d">2 , 048</annotation></semantics></math>, and <math alttext="32" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><mn id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><cn id="S3.SS3.p2.5.m5.1.1.cmml" type="integer" xref="S3.SS3.p2.5.m5.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">32</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">32</annotation></semantics></math>Â attention heads. We use the pre-computed tokenizer released with LLaMA 2, which includes a vocabulary of sizeÂ <math alttext="\gamma=$32,000$" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.3"><semantics id="S3.SS3.p2.6.m6.3a"><mrow id="S3.SS3.p2.6.m6.3.4" xref="S3.SS3.p2.6.m6.3.4.cmml"><mi id="S3.SS3.p2.6.m6.3.4.2" xref="S3.SS3.p2.6.m6.3.4.2.cmml">Î³</mi><mo id="S3.SS3.p2.6.m6.3.4.1" xref="S3.SS3.p2.6.m6.3.4.1.cmml">=</mo><mrow id="S3.SS3.p2.6.m6.3.3.4" xref="S3.SS3.p2.6.m6.3.4.cmml"><mn id="S3.SS3.p2.6.m6.2.2.1.1" xref="S3.SS3.p2.6.m6.3.3.3.cmml">32</mn><mo id="S3.SS3.p2.6.m6.3.3.4.1" xref="S3.SS3.p2.6.m6.3.3.3.cmml">,</mo><mn id="S3.SS3.p2.6.m6.3.3.2.2" xref="S3.SS3.p2.6.m6.3.3.3.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.3b"><apply id="S3.SS3.p2.6.m6.3.4.cmml" xref="S3.SS3.p2.6.m6.3.4"><eq id="S3.SS3.p2.6.m6.3.4.1.cmml" xref="S3.SS3.p2.6.m6.3.4.1"></eq><ci id="S3.SS3.p2.6.m6.3.4.2.cmml" xref="S3.SS3.p2.6.m6.3.4.2">ğ›¾</ci><cn id="S3.SS3.p2.6.m6.3.3.3.cmml" type="integer" xref="S3.SS3.p2.6.m6.2.2.1.1">32000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.3c">\gamma=$32,000$</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.3d">italic_Î³ = 32 , 000</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.9">To splitÂ <math alttext="\eta^{\phi}" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><msup id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">Î·</mi><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">Ï•</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">ğœ‚</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\eta^{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï• end_POSTSUPERSCRIPT</annotation></semantics></math> andÂ <math alttext="\eta^{\varphi}" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><msup id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">Î·</mi><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">Ï†</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">ğœ‚</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">ğœ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\eta^{\varphi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï† end_POSTSUPERSCRIPT</annotation></semantics></math> in practice, we first transformÂ <math alttext="\eta" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><mi id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\eta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">italic_Î·</annotation></semantics></math> embeddings into a sequence of max-probability tokens. We extractÂ <math alttext="\eta^{\phi}" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m4.1"><semantics id="S3.SS3.p3.4.m4.1a"><msup id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml">Î·</mi><mi id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml">Ï•</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2">ğœ‚</ci><ci id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">\eta^{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m4.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï• end_POSTSUPERSCRIPT</annotation></semantics></math> from all sequence positions that immediately follow special tokenÂ <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.9.1">&lt;MOD&gt;</span>, and we extractÂ <math alttext="\eta^{\varphi}" class="ltx_Math" display="inline" id="S3.SS3.p3.5.m5.1"><semantics id="S3.SS3.p3.5.m5.1a"><msup id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml"><mi id="S3.SS3.p3.5.m5.1.1.2" xref="S3.SS3.p3.5.m5.1.1.2.cmml">Î·</mi><mi id="S3.SS3.p3.5.m5.1.1.3" xref="S3.SS3.p3.5.m5.1.1.3.cmml">Ï†</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><apply id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.1.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">superscript</csymbol><ci id="S3.SS3.p3.5.m5.1.1.2.cmml" xref="S3.SS3.p3.5.m5.1.1.2">ğœ‚</ci><ci id="S3.SS3.p3.5.m5.1.1.3.cmml" xref="S3.SS3.p3.5.m5.1.1.3">ğœ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">\eta^{\varphi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.5.m5.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï† end_POSTSUPERSCRIPT</annotation></semantics></math> from all remaining positions. The agentÂ <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS3.p3.6.m6.1"><semantics id="S3.SS3.p3.6.m6.1a"><mi id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><ci id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.6.m6.1d">italic_Î±</annotation></semantics></math> predictsÂ <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.9.2">&lt;MOD&gt;</span> and subsequentÂ <math alttext="\eta^{\phi}" class="ltx_Math" display="inline" id="S3.SS3.p3.7.m7.1"><semantics id="S3.SS3.p3.7.m7.1a"><msup id="S3.SS3.p3.7.m7.1.1" xref="S3.SS3.p3.7.m7.1.1.cmml"><mi id="S3.SS3.p3.7.m7.1.1.2" xref="S3.SS3.p3.7.m7.1.1.2.cmml">Î·</mi><mi id="S3.SS3.p3.7.m7.1.1.3" xref="S3.SS3.p3.7.m7.1.1.3.cmml">Ï•</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m7.1b"><apply id="S3.SS3.p3.7.m7.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.7.m7.1.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1">superscript</csymbol><ci id="S3.SS3.p3.7.m7.1.1.2.cmml" xref="S3.SS3.p3.7.m7.1.1.2">ğœ‚</ci><ci id="S3.SS3.p3.7.m7.1.1.3.cmml" xref="S3.SS3.p3.7.m7.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m7.1c">\eta^{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.7.m7.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï• end_POSTSUPERSCRIPT</annotation></semantics></math> features after every volume encoding and generation function argument. We projectÂ <math alttext="\eta^{\phi}" class="ltx_Math" display="inline" id="S3.SS3.p3.8.m8.1"><semantics id="S3.SS3.p3.8.m8.1a"><msup id="S3.SS3.p3.8.m8.1.1" xref="S3.SS3.p3.8.m8.1.1.cmml"><mi id="S3.SS3.p3.8.m8.1.1.2" xref="S3.SS3.p3.8.m8.1.1.2.cmml">Î·</mi><mi id="S3.SS3.p3.8.m8.1.1.3" xref="S3.SS3.p3.8.m8.1.1.3.cmml">Ï•</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m8.1b"><apply id="S3.SS3.p3.8.m8.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.8.m8.1.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1">superscript</csymbol><ci id="S3.SS3.p3.8.m8.1.1.2.cmml" xref="S3.SS3.p3.8.m8.1.1.2">ğœ‚</ci><ci id="S3.SS3.p3.8.m8.1.1.3.cmml" xref="S3.SS3.p3.8.m8.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m8.1c">\eta^{\phi}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.8.m8.1d">italic_Î· start_POSTSUPERSCRIPT italic_Ï• end_POSTSUPERSCRIPT</annotation></semantics></math> embeddings toÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS3.p3.9.m9.1"><semantics id="S3.SS3.p3.9.m9.1a"><mi id="S3.SS3.p3.9.m9.1.1" xref="S3.SS3.p3.9.m9.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.m9.1b"><ci id="S3.SS3.p3.9.m9.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.m9.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.9.m9.1d">italic_Ï•</annotation></semantics></math> using a fully-connected layer with 32Â output channels and SiLU activation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.7"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.7.1">Volume Encoder and Generator Models.</span>
We implement <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><msub id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">m</mi><mtext id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">ğ‘š</ci><ci id="S3.SS3.p4.1.m1.1.1.3a.cmml" xref="S3.SS3.p4.1.m1.1.1.3"><mtext id="S3.SS3.p4.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p4.1.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> andÂ <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.1"><semantics id="S3.SS3.p4.2.m2.1a"><msub id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">m</mi><mtext id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">ğ‘š</ci><ci id="S3.SS3.p4.2.m2.1.1.3a.cmml" xref="S3.SS3.p4.2.m2.1.1.3"><mtext id="S3.SS3.p4.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p4.2.m2.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math> as the respective down-sampling and up-sampling arms of a six-level UNet-like modelÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib128" title=""><span class="ltx_text" style="font-size:90%;">128</span></a>]</cite>. Each level consists of a 3D convolutional layer followed by a latent featureÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS3.p4.3.m3.1"><semantics id="S3.SS3.p4.3.m3.1a"><mi id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.m3.1d">italic_Ï•</annotation></semantics></math> mixing layer and stream interaction layer with <math alttext="b=32" class="ltx_Math" display="inline" id="S3.SS3.p4.4.m4.1"><semantics id="S3.SS3.p4.4.m4.1a"><mrow id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml"><mi id="S3.SS3.p4.4.m4.1.1.2" xref="S3.SS3.p4.4.m4.1.1.2.cmml">b</mi><mo id="S3.SS3.p4.4.m4.1.1.1" xref="S3.SS3.p4.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS3.p4.4.m4.1.1.3" xref="S3.SS3.p4.4.m4.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><apply id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1"><eq id="S3.SS3.p4.4.m4.1.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1.1"></eq><ci id="S3.SS3.p4.4.m4.1.1.2.cmml" xref="S3.SS3.p4.4.m4.1.1.2">ğ‘</ci><cn id="S3.SS3.p4.4.m4.1.1.3.cmml" type="integer" xref="S3.SS3.p4.4.m4.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">b=32</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.4.m4.1d">italic_b = 32</annotation></semantics></math>, both defined in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S3.SS1" title="3.1 Architecture â€£ 3 Method â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">3.1</span></a>. All layers use SiLU activations. The spatial outputs at every level are channel-normalized with a group size of four, then max-pooledÂ (<math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.SS3.p4.5.m5.1"><semantics id="S3.SS3.p4.5.m5.1a"><msub id="S3.SS3.p4.5.m5.1.1" xref="S3.SS3.p4.5.m5.1.1.cmml"><mi id="S3.SS3.p4.5.m5.1.1.2" xref="S3.SS3.p4.5.m5.1.1.2.cmml">m</mi><mtext id="S3.SS3.p4.5.m5.1.1.3" xref="S3.SS3.p4.5.m5.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.1b"><apply id="S3.SS3.p4.5.m5.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m5.1.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p4.5.m5.1.1.2.cmml" xref="S3.SS3.p4.5.m5.1.1.2">ğ‘š</ci><ci id="S3.SS3.p4.5.m5.1.1.3a.cmml" xref="S3.SS3.p4.5.m5.1.1.3"><mtext id="S3.SS3.p4.5.m5.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p4.5.m5.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.5.m5.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math>) or trilinearly upsampledÂ (<math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.SS3.p4.6.m6.1"><semantics id="S3.SS3.p4.6.m6.1a"><msub id="S3.SS3.p4.6.m6.1.1" xref="S3.SS3.p4.6.m6.1.1.cmml"><mi id="S3.SS3.p4.6.m6.1.1.2" xref="S3.SS3.p4.6.m6.1.1.2.cmml">m</mi><mtext id="S3.SS3.p4.6.m6.1.1.3" xref="S3.SS3.p4.6.m6.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m6.1b"><apply id="S3.SS3.p4.6.m6.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.6.m6.1.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p4.6.m6.1.1.2.cmml" xref="S3.SS3.p4.6.m6.1.1.2">ğ‘š</ci><ci id="S3.SS3.p4.6.m6.1.1.3a.cmml" xref="S3.SS3.p4.6.m6.1.1.3"><mtext id="S3.SS3.p4.6.m6.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p4.6.m6.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m6.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.6.m6.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>) by a factor of two. Convolution kernels have sizeÂ <math alttext="3^{3}" class="ltx_Math" display="inline" id="S3.SS3.p4.7.m7.1"><semantics id="S3.SS3.p4.7.m7.1a"><msup id="S3.SS3.p4.7.m7.1.1" xref="S3.SS3.p4.7.m7.1.1.cmml"><mn id="S3.SS3.p4.7.m7.1.1.2" xref="S3.SS3.p4.7.m7.1.1.2.cmml">3</mn><mn id="S3.SS3.p4.7.m7.1.1.3" xref="S3.SS3.p4.7.m7.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m7.1b"><apply id="S3.SS3.p4.7.m7.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.1.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1">superscript</csymbol><cn id="S3.SS3.p4.7.m7.1.1.2.cmml" type="integer" xref="S3.SS3.p4.7.m7.1.1.2">3</cn><cn id="S3.SS3.p4.7.m7.1.1.3.cmml" type="integer" xref="S3.SS3.p4.7.m7.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m7.1c">3^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.7.m7.1d">3 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, with 32 output channels at the top resolution level and 96 output channels at all other levels.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.3">For all input volume streams, we populateÂ <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.1"><semantics id="S3.SS3.p5.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">â„°</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">â„°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.1.m1.1d">caligraphic_E</annotation></semantics></math> with spatial features output at each level inÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S3.SS3.p5.2.m2.1"><semantics id="S3.SS3.p5.2.m2.1a"><msub id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml"><mi id="S3.SS3.p5.2.m2.1.1.2" xref="S3.SS3.p5.2.m2.1.1.2.cmml">m</mi><mtext id="S3.SS3.p5.2.m2.1.1.3" xref="S3.SS3.p5.2.m2.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><apply id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.1.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p5.2.m2.1.1.2.cmml" xref="S3.SS3.p5.2.m2.1.1.2">ğ‘š</ci><ci id="S3.SS3.p5.2.m2.1.1.3a.cmml" xref="S3.SS3.p5.2.m2.1.1.3"><mtext id="S3.SS3.p5.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p5.2.m2.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.2.m2.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math>. We use these latent features as skip-connections to corresponding level inputs in the generatorÂ <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S3.SS3.p5.3.m3.1"><semantics id="S3.SS3.p5.3.m3.1a"><msub id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml"><mi id="S3.SS3.p5.3.m3.1.1.2" xref="S3.SS3.p5.3.m3.1.1.2.cmml">m</mi><mtext id="S3.SS3.p5.3.m3.1.1.3" xref="S3.SS3.p5.3.m3.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><apply id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p5.3.m3.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.2">ğ‘š</ci><ci id="S3.SS3.p5.3.m3.1.1.3a.cmml" xref="S3.SS3.p5.3.m3.1.1.3"><mtext id="S3.SS3.p5.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p5.3.m3.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.3.m3.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>, which predicts volumes through a convolutional layer with one output channel. To generate segmentations, we use sigmoid activation. We process images in their original acquisition resolution and shape by implementing a novel, native-space convolutional operation (AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS1" title="A.1 Native-Space Convolutions â€£ Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">A.1</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.3"><span class="ltx_text ltx_font_bold" id="S3.SS3.p6.3.1">Optimization.</span>
We train VoxelPrompt using the Adam optimizerÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib129" title=""><span class="ltx_text" style="font-size:90%;">129</span></a>]</cite> with an initial learning rate ofÂ <math alttext="10^{-4}" class="ltx_Math" display="inline" id="S3.SS3.p6.1.m1.1"><semantics id="S3.SS3.p6.1.m1.1a"><msup id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mn id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">10</mn><mrow id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml"><mo id="S3.SS3.p6.1.m1.1.1.3a" xref="S3.SS3.p6.1.m1.1.1.3.cmml">âˆ’</mo><mn id="S3.SS3.p6.1.m1.1.1.3.2" xref="S3.SS3.p6.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">superscript</csymbol><cn id="S3.SS3.p6.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p6.1.m1.1.1.2">10</cn><apply id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3"><minus id="S3.SS3.p6.1.m1.1.1.3.1.cmml" xref="S3.SS3.p6.1.m1.1.1.3"></minus><cn id="S3.SS3.p6.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS3.p6.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p6.1.m1.1d">10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, a batch size of one, and 10 gradient accumulation steps on an NVIDIA A100 GPU. We halve the learning rate afterÂ <math alttext="10^{5}" class="ltx_Math" display="inline" id="S3.SS3.p6.2.m2.1"><semantics id="S3.SS3.p6.2.m2.1a"><msup id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml"><mn id="S3.SS3.p6.2.m2.1.1.2" xref="S3.SS3.p6.2.m2.1.1.2.cmml">10</mn><mn id="S3.SS3.p6.2.m2.1.1.3" xref="S3.SS3.p6.2.m2.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><apply id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.2.m2.1.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">superscript</csymbol><cn id="S3.SS3.p6.2.m2.1.1.2.cmml" type="integer" xref="S3.SS3.p6.2.m2.1.1.2">10</cn><cn id="S3.SS3.p6.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.p6.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">10^{5}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p6.2.m2.1d">10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT</annotation></semantics></math> steps with no improvement in validation accuracy, stopping training after four sets of learning rate updates. We set the volume loss weightÂ <math alttext="\lambda=0.1" class="ltx_Math" display="inline" id="S3.SS3.p6.3.m3.1"><semantics id="S3.SS3.p6.3.m3.1a"><mrow id="S3.SS3.p6.3.m3.1.1" xref="S3.SS3.p6.3.m3.1.1.cmml"><mi id="S3.SS3.p6.3.m3.1.1.2" xref="S3.SS3.p6.3.m3.1.1.2.cmml">Î»</mi><mo id="S3.SS3.p6.3.m3.1.1.1" xref="S3.SS3.p6.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS3.p6.3.m3.1.1.3" xref="S3.SS3.p6.3.m3.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m3.1b"><apply id="S3.SS3.p6.3.m3.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1"><eq id="S3.SS3.p6.3.m3.1.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1.1"></eq><ci id="S3.SS3.p6.3.m3.1.1.2.cmml" xref="S3.SS3.p6.3.m3.1.1.2">ğœ†</ci><cn id="S3.SS3.p6.3.m3.1.1.3.cmml" type="float" xref="S3.SS3.p6.3.m3.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m3.1c">\lambda=0.1</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p6.3.m3.1d">italic_Î» = 0.1</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Task and Data Design</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.2">We handcraft a datasetÂ <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">ğ’¯</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">caligraphic_T</annotation></semantics></math> of brain imaging tasks that involve a wide range of image acquisitions, segmentation protocols, and annotation types. While broadly diverse, this task set is still preliminary and not meant to encompass the entire spectrum of possible objectives in brain image analysis. We designÂ <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">ğ’¯</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">caligraphic_T</annotation></semantics></math> to encompass a set of heterogeneous tasks that evaluate the promise of VoxelPromptâ€™s joint generation of analytical instructions, spatial delineations, and natural language descriptions.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Tasks</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">We include inÂ <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">ğ’¯</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">caligraphic_T</annotation></semantics></math> a set of clinically-orientated objectives, which are broadly categorized as either ROI processing or pathology description tasks. For each task, we provide ground-truth codeÂ <math alttext="\varphi^{*}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><msup id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">Ï†</mi><mo id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">ğœ‘</ci><times id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\varphi^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_Ï† start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math>, used in training and evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Quantitative ROI Processing.</span>
Quantitative processing tasks involve image feature segmentation, optionally followed by downstream steps to compute ROI measures. We include a standard segmentation task for all structures and pathology classes in our dataset.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Downstream processing tasks use predicted segmentations, sometimes in conjunction with the input image volumes. For example, some tasks involve removing, extracting, or cropping the field of viewÂ (FOV) around a segmented region. Others use segmentations to compute ROI-specific statistics of the image signal intensities, including mean intensity or signal-to-noise ratioÂ (SNR). Morphological tasks analyze ROI shape and involve the computation of total volume, bounding box dimensions, or the individual height, width, and depth of a segmented structure. We also include tasks that compute and compare such metrics across multiple segmentations. For example, longitudinal tasks measure change in ROI properties across a series of scan sessions, and multi-region tasks compare metrics from different ROIs in single session.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">For each ROI processing task, ground-truth codeÂ <math alttext="\varphi^{*}" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><msup id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">Ï†</mi><mo id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">ğœ‘</ci><times id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\varphi^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">italic_Ï† start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math> includes a sequence of functions to (1) encode the input volumes, (2) read the encoded volume features, and (3)Â generate a segmentation map for each ROI relevant to the task. Depending on the task, further analysis may be required, and we include functions that (4) operate on these segmentations (and possibly the input volumes) to compute intermediate metrics. Lastly, we append functions that (5) read the values of resulting metrics and (6)Â format them into an output message. FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S3.F2" title="Figure 2 â€£ 3 Method â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the instruction forward-pass for a task that measures longitudinal change in tumor size.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Open-Language Classification.</span>
Another set of tasks include the generation of language-based image characterizations. We formulate these tasks as classification-style question-answering problems, in which VoxelPrompt outputs natural language corresponding to a text response from a finite set of possible answers.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">For example, one task involves classifying lesion image signal intensity as either hyperintense,Â hypointense, or isointense relative to surrounding healthy tissue. Another set of tasks involves identifying anatomical lesion location. Possible supratentorial locations include the frontal, parietal, occipital, temporal, insular lobes, the lateral ventricles, and the posteriorÂ (PCA), middleÂ (MCA), and anteriorÂ (ACA) cerebral arterial vascular territories. Infratentorial lesion locations include the brainstem, cerebellum, fourth ventricle, and cerebellopontine angle. We also include tasks that require integrating information across multiple input images. For example, these involve detecting whether a lesion restricts diffusion, given both an DWI and ADC map, or whether a lesion signal enhances after intravenous contrast administration, given both pre- and post-contrast scans.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">We handcraft a target language response for all possible answers to these classification-style tasks. For each task, we construct the ground-truth instruction codeÂ <math alttext="\varphi^{*}" class="ltx_Math" display="inline" id="S4.SS1.p7.1.m1.1"><semantics id="S4.SS1.p7.1.m1.1a"><msup id="S4.SS1.p7.1.m1.1.1" xref="S4.SS1.p7.1.m1.1.1.cmml"><mi id="S4.SS1.p7.1.m1.1.1.2" xref="S4.SS1.p7.1.m1.1.1.2.cmml">Ï†</mi><mo id="S4.SS1.p7.1.m1.1.1.3" xref="S4.SS1.p7.1.m1.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.1.m1.1b"><apply id="S4.SS1.p7.1.m1.1.1.cmml" xref="S4.SS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p7.1.m1.1.1.1.cmml" xref="S4.SS1.p7.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p7.1.m1.1.1.2.cmml" xref="S4.SS1.p7.1.m1.1.1.2">ğœ‘</ci><times id="S4.SS1.p7.1.m1.1.1.3.cmml" xref="S4.SS1.p7.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.1.m1.1c">\varphi^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p7.1.m1.1d">italic_Ï† start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math> with functions to (1) encode the input volumes, (2) read the encoded volume features, and (3) output a text response with the correct natural language answer.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Prompt Synthesis</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.9">We develop a combinatorial strategy to synthesize a diverse set of input prompts to train VoxelPrompt. For a taskÂ <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_Ï„</annotation></semantics></math>, we establish a set of prompt templatesÂ <math alttext="\mathcal{P}_{\tau}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><msub id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">ğ’«</mi><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">Ï„</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">ğ’«</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\mathcal{P}_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">caligraphic_P start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT</annotation></semantics></math>, which serve as a basis for synthesizing text. They include placeholders to accommodate multiple words, terminologies, and phrases with similar meanings. We compile a list of interchangeable textÂ <math alttext="\mathcal{C}_{k}" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><msub id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">ğ’</mi><mi id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">ğ’</ci><ci id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\mathcal{C}_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">caligraphic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> for each placeholderÂ <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m4.1"><semantics id="S4.SS2.p1.4.m4.1a"><mi id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><ci id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.m4.1d">italic_k</annotation></semantics></math>. To generate a promptÂ <math alttext="p" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m5.1"><semantics id="S4.SS2.p1.5.m5.1a"><mi id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><ci id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m5.1d">italic_p</annotation></semantics></math> for taskÂ <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS2.p1.6.m6.1"><semantics id="S4.SS2.p1.6.m6.1a"><mi id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><ci id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.6.m6.1d">italic_Ï„</annotation></semantics></math> during training, we randomly sample a template from <math alttext="\mathcal{P}_{\tau}" class="ltx_Math" display="inline" id="S4.SS2.p1.7.m7.1"><semantics id="S4.SS2.p1.7.m7.1a"><msub id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.7.m7.1.1.2" xref="S4.SS2.p1.7.m7.1.1.2.cmml">ğ’«</mi><mi id="S4.SS2.p1.7.m7.1.1.3" xref="S4.SS2.p1.7.m7.1.1.3.cmml">Ï„</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><apply id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.p1.7.m7.1.1.2">ğ’«</ci><ci id="S4.SS2.p1.7.m7.1.1.3.cmml" xref="S4.SS2.p1.7.m7.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">\mathcal{P}_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.7.m7.1d">caligraphic_P start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT</annotation></semantics></math>, then fill each placeholderÂ <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.8.m8.1"><semantics id="S4.SS2.p1.8.m8.1a"><mi id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><ci id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.8.m8.1d">italic_k</annotation></semantics></math> with text sampled fromÂ <math alttext="\mathcal{C}_{k}" class="ltx_Math" display="inline" id="S4.SS2.p1.9.m9.1"><semantics id="S4.SS2.p1.9.m9.1a"><msub id="S4.SS2.p1.9.m9.1.1" xref="S4.SS2.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.9.m9.1.1.2" xref="S4.SS2.p1.9.m9.1.1.2.cmml">ğ’</mi><mi id="S4.SS2.p1.9.m9.1.1.3" xref="S4.SS2.p1.9.m9.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.1b"><apply id="S4.SS2.p1.9.m9.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.9.m9.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1">subscript</csymbol><ci id="S4.SS2.p1.9.m9.1.1.2.cmml" xref="S4.SS2.p1.9.m9.1.1.2">ğ’</ci><ci id="S4.SS2.p1.9.m9.1.1.3.cmml" xref="S4.SS2.p1.9.m9.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.1c">\mathcal{C}_{k}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.9.m9.1d">caligraphic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. This process is recursive, as a single placeholder may include other placeholders. The distribution of prompts encompasses a range of clinical and imaging terminologies, as well as linguistic variations across tenses, syntactic structures, and lexical choices.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Images</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.2">We assemble and annotate a collection of 6,925 3D brain MRI and CT scans from 15 public datasets, and we generate segmentations corresponding to 185 different bilateral anatomical targets and 14 pathology classes. The MRI collection includes images acquired with T1-weightedÂ (T1w), T2-weightedÂ (T2w), FLAIR, proton-densityÂ (PD), gradient echoÂ (GRE), and diffusion-weightedÂ (DWI) sequences, with various scan resolutions. The subjects are divided into training, validation, and test sets, with <math alttext="4,852" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.2"><semantics id="S4.SS3.p1.1.m1.2a"><mrow id="S4.SS3.p1.1.m1.2.2.4"><mn id="S4.SS3.p1.1.m1.1.1.1.1" xref="S4.SS3.p1.1.m1.2.2.3.cmml">4</mn><mo id="S4.SS3.p1.1.m1.2.2.4.1" xref="S4.SS3.p1.1.m1.2.2.3.cmml">,</mo><mn id="S4.SS3.p1.1.m1.2.2.2.2" xref="S4.SS3.p1.1.m1.2.2.3.cmml">852</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.2b"><cn id="S4.SS3.p1.1.m1.2.2.3.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.1.1">4852</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.2c">4,852</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.2d">4 , 852</annotation></semantics></math>, 213, and <math alttext="1,860" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.2"><semantics id="S4.SS3.p1.2.m2.2a"><mrow id="S4.SS3.p1.2.m2.2.2.4"><mn id="S4.SS3.p1.2.m2.1.1.1.1" xref="S4.SS3.p1.2.m2.2.2.3.cmml">1</mn><mo id="S4.SS3.p1.2.m2.2.2.4.1" xref="S4.SS3.p1.2.m2.2.2.3.cmml">,</mo><mn id="S4.SS3.p1.2.m2.2.2.2.2" xref="S4.SS3.p1.2.m2.2.2.3.cmml">860</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.2b"><cn id="S4.SS3.p1.2.m2.2.2.3.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.1.1">1860</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.2c">1,860</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.2d">1 , 860</annotation></semantics></math> 3D images, respectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Anatomical Segmentations.</span>
We generate segmentations for whole-brain anatomical structures on images from the FSMÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, OASISÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib130" title=""><span class="ltx_text" style="font-size:90%;">130</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib131" title=""><span class="ltx_text" style="font-size:90%;">131</span></a>]</cite>, Mind Brain BodyÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib132" title=""><span class="ltx_text" style="font-size:90%;">132</span></a>]</cite>, IBCÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib133" title=""><span class="ltx_text" style="font-size:90%;">133</span></a>]</cite>, CERMEPÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib134" title=""><span class="ltx_text" style="font-size:90%;">134</span></a>]</cite>, and Forrest GumpÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib135" title=""><span class="ltx_text" style="font-size:90%;">135</span></a>]</cite> cohorts, using established automated methodsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib136" title=""><span class="ltx_text" style="font-size:90%;">136</span></a>]</cite>.
We select high-quality acquisitions and thoroughly inspect and correct errors in the label maps.
Additionally, we make use of multiple image atlases with precomputed segmentationsÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib137" title=""><span class="ltx_text" style="font-size:90%;">137</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib138" title=""><span class="ltx_text" style="font-size:90%;">138</span></a>]</cite>, and we manually segment additional structures across a small group of 16 images. AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS2" title="A.2 List of Anatomical Structures â€£ Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">A.2</span></a> provides a complete list of anatomical labels.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Pathology Segmentations.</span>
We collect scans with pathology from the BraTSÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, ISLESÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib139" title=""><span class="ltx_text" style="font-size:90%;">139</span></a>]</cite>, ATLASÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib140" title=""><span class="ltx_text" style="font-size:90%;">140</span></a>]</cite>, and WMHÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> datasets. These provide segmentations for glioma, edema, contrast-enhancing tissue, infarcts, and white matter hyperintensities (leukoaraiosis).</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Additionally, we compile data from the <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.1">Radiopaedia</span> online resourceÂ (AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS7" title="A.7 Radiopaedia Data â€£ Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">A.7</span></a>), assembling images of conditions that include infarcts, arachnoid cysts, epidermoid cysts, cavum veli interpositi, glioma, choroid plexus papilloma, meningioma, central neurocytoma, and intracranial hemorrhages. We manually delineate the lesions and their various sub-components, including edema, enhancing tissue, and heterogeneous intralesion features.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">During training, we apply a synthesis procedure to conditionally generate lesions with a variety of location and signal characteristics in healthy brain parenchyma. We detail this mechanism in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS3" title="A.3 Lesion Synthesis â€£ Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">A.3</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p6.1.1">Pathology Characterization.</span>
We manually annotate properties of lesions, including anatomical location within the brain, image intensity properties, and relative position or size compared to other features present in the scan. When appropriate, we also mark whether a lesion restricts the diffusion of water or whether it enhances (increases in signal) after a contrast agent is administered to the subject.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p7.1.1">Preprocessing.</span>
We normalize voxel intensities within the rangeÂ <math alttext="[0,1]" class="ltx_Math" display="inline" id="S4.SS3.p7.1.m1.2"><semantics id="S4.SS3.p7.1.m1.2a"><mrow id="S4.SS3.p7.1.m1.2.3.2" xref="S4.SS3.p7.1.m1.2.3.1.cmml"><mo id="S4.SS3.p7.1.m1.2.3.2.1" stretchy="false" xref="S4.SS3.p7.1.m1.2.3.1.cmml">[</mo><mn id="S4.SS3.p7.1.m1.1.1" xref="S4.SS3.p7.1.m1.1.1.cmml">0</mn><mo id="S4.SS3.p7.1.m1.2.3.2.2" xref="S4.SS3.p7.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS3.p7.1.m1.2.2" xref="S4.SS3.p7.1.m1.2.2.cmml">1</mn><mo id="S4.SS3.p7.1.m1.2.3.2.3" stretchy="false" xref="S4.SS3.p7.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p7.1.m1.2b"><interval closure="closed" id="S4.SS3.p7.1.m1.2.3.1.cmml" xref="S4.SS3.p7.1.m1.2.3.2"><cn id="S4.SS3.p7.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p7.1.m1.1.1">0</cn><cn id="S4.SS3.p7.1.m1.2.2.cmml" type="integer" xref="S4.SS3.p7.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p7.1.m1.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p7.1.m1.2d">[ 0 , 1 ]</annotation></semantics></math>, spatially conform the volume array to a right-anterior-superiorÂ (RAS) orientation, and crop images to a 20 mm margin around the cranial cavity. We co-register all images acquired from each subjectÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib141" title=""><span class="ltx_text" style="font-size:90%;">141</span></a>]</cite>. In training, we augment images by applying random affine transformations, spatial intensity distortions, exponential scaling, lateral anatomical flipping, cropping, anatomical masking, and voxel resizing.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We analyze the capability of VoxelPrompt to carry out both language-driven and computationally grounded image analysis across a range of tasks. We train VoxelPrompt on our handcrafted set of neuroimaging tasks, and we evaluate on a set of held-out imaging data. We first showcase a range of representative VoxelPrompt use cases through illustrative examples. Then, we quantitatively compare VoxelPrompt outputs with those of specialized model benchmarks for a subset of segmentation and language-based question-answering tasks. Our goal is to evaluate whether a single VoxelPrompt model can capture the individual accuracy of this collection of models, rather than attempt to surpass benchmark performance.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">During evaluation, we use input prompts equivalent or similar to those synthesized for training. Unless otherwise stated, we pass all images for a given subject as input to VoxelPrompt and benchmark models.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1119" id="S5.F3.g1" src="x3.png" width="832"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S5.F3.3.2" style="font-size:90%;">Natural language input prompts (shown above each image) enable targeted analysis of nuanced, context-specific image regions, especially for multi-lesion cases. Borders of segmentations predicted by VoxelPrompt are shown in purple.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Illustrative Use Cases</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Multi-Task Capability.</span>
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">1</span></a>, we illustrate several results on held-out data that demonstrate the diversity of tasks VoxelPrompt can encompass. A single VoxelPrompt model can learn to localize a range of brain anatomy and
pathology regions as well as mask or crop tissue groups for improved visual analysis. Within targeted ROIs, the model can extract statistical intensity metrics, such as average Hounsfield units or apparent diffusion coefficient (ADC) values, and derive measurements of structural morphology, such as total volume and extent dimensions.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">VoxelPrompt can learn to compute and compare metrics across multiple ROIs, for instance, it can measure hippocampal asymmetry, normalize subcortical volumes by intracranial size, and quantify acute and chronic components of subdural hemorrhage. When provided with multiple scans, VoxelPrompt can measure longitudinal changes, such as tumor progression, ventricular growth in encephalitis, or signal decay in stroke progression. VoxelPrompt is able to characterize lesion locations and tissue properties, such as diffusion restriction (indicating areas of acute ischemia) and post-contrast enhancement (indicating blood-brain barrier disruption), by integrating information from multiple acquisitions simultaneously.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Flexible Analysis via Language.</span>
FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.F3" title="Figure 3 â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">3</span></a> shows that VoxelPrompt supports flexible targeting of context-specific components in multifocal disease scenarios. Given language descriptors, the model can learn to isolate or differentiate lesions based on factors such as signal intensity, size, relative position, or anatomical context (e.g.,Â including hemisphere, arterial territory, or cerebral lobe).</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="441" id="S5.F4.g1" src="x4.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.4.2" style="font-size:90%;">VoxelPrompt (black) yields segmentations with accuracy that matches or exceeds that of specialized single-task (purple) and state-of-the-art (blue) brain segmentation benchmarks. We plot the Dice score between estimated segmentations and the ground truth for a set of label targets, with the average differences between VoxelPrompt and the single-task benchmarks above each label plot. Significant differences (p &lt; 0.05, paired <span class="ltx_text ltx_font_italic" id="S5.F4.4.2.1">t</span>-test) are marked with an asterisk. For visual simplicity in the SynthSeg comparison, we group results for cortical regions by their associated cortical lobe.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Segmentation Accuracy</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Image segmentation is a core component in learned ROI processing tasks. In this section, we compare the segmentation accuracy of VoxelPrompt with that of specialized models trained on specific labels as well as an existing state-of-the-art method. To generate segmentations, we explicitly prompt VoxelPrompt to â€œsegment the ROIâ€, replacing â€œROIâ€ with the target region name or description. For bilateral brain structures, we prompt VoxelPrompt to segment the â€œleft and right ROIâ€. We assess accuracy using the Dice coefficientÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib142" title=""><span class="ltx_text" style="font-size:90%;">142</span></a>]</cite> to measure overlap between ground-truth and predicted segmentations. When computing this overlap, we group bilateral structures together as a single label. We evaluate statistical differences in Dice between VoxelPrompt and each benchmark model through a paired <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">t</span>-test.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.4"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.4.1">Specialized Benchmarks.</span> To test if VoxelPrompt can match the combined performance of multiple specialized models, we optimize an individual, label-specific segmentation network for a subset of distinct ROIs in our dataset. This UNet-like benchmark architecture is equivalent to combiningÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><msub id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">m</mi><mtext id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">ğ‘š</ci><ci id="S5.SS2.p2.1.m1.1.1.3a.cmml" xref="S5.SS2.p2.1.m1.1.1.3"><mtext id="S5.SS2.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S5.SS2.p2.1.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math> andÂ <math alttext="m_{\text{gen}}" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><msub id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mi id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">m</mi><mtext id="S5.SS2.p2.2.m2.1.1.3" xref="S5.SS2.p2.2.m2.1.1.3a.cmml">gen</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">ğ‘š</ci><ci id="S5.SS2.p2.2.m2.1.1.3a.cmml" xref="S5.SS2.p2.2.m2.1.1.3"><mtext id="S5.SS2.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S5.SS2.p2.2.m2.1.1.3">gen</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">m_{\text{gen}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">italic_m start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT</annotation></semantics></math>, with eachÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">italic_Ï•</annotation></semantics></math> mixing layer replaced by a fully-connected layer (withoutÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">italic_Ï•</annotation></semantics></math> inputs) to ensure comparable capacity to VoxelPrompt. We optimize each benchmark model with the soft Dice loss and we use all training images containing segmentations for a specific reference label. Since optimizing a specialized baseline for each ROI in our training dataset is computationally prohibitive, we select a subset of 10 anatomical and 7 pathology targets spanning diverse shapes and locations. In total, the resulting evaluation subset encompasses 638 held-out subjects, detailed in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS4" title="A.4 Segmentation Analysis Data â€£ Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">A.4</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">State-of-the-Art Baseline.</span> We also evaluate SynthSegÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>, a state-of-the-art method for multi-class brain segmentation that generalizes across the diverse acquisition contrasts exhibited in our image dataset. We use the more recent SynthSeg v2, which also parcellates the cortex into discrete subregions. For evaluation, we organize a structural MRI test set of 108 images, which contain ground-truth segmentations for 45 individual anatomical structures that are segmented by SynthSeg. Since SynthSeg supports only one input image in the forward pass, we evaluate segmentation performance individually, for each image in a scan session.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.8"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.8.1">Results.</span> FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.F4" title="Figure 4 â€£ 5.1 Illustrative Use Cases â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">4</span></a> (top) shows that VoxelPrompt matches (<math alttext="p&gt;0.05" class="ltx_Math" display="inline" id="S5.SS2.p4.1.m1.1"><semantics id="S5.SS2.p4.1.m1.1a"><mrow id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml"><mi id="S5.SS2.p4.1.m1.1.1.2" xref="S5.SS2.p4.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS2.p4.1.m1.1.1.1" xref="S5.SS2.p4.1.m1.1.1.1.cmml">&gt;</mo><mn id="S5.SS2.p4.1.m1.1.1.3" xref="S5.SS2.p4.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><apply id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1"><gt id="S5.SS2.p4.1.m1.1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1.1"></gt><ci id="S5.SS2.p4.1.m1.1.1.2.cmml" xref="S5.SS2.p4.1.m1.1.1.2">ğ‘</ci><cn id="S5.SS2.p4.1.m1.1.1.3.cmml" type="float" xref="S5.SS2.p4.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">p&gt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.1.m1.1d">italic_p &gt; 0.05</annotation></semantics></math>) or exceedsÂ (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S5.SS2.p4.2.m2.1"><semantics id="S5.SS2.p4.2.m2.1a"><mrow id="S5.SS2.p4.2.m2.1.1" xref="S5.SS2.p4.2.m2.1.1.cmml"><mi id="S5.SS2.p4.2.m2.1.1.2" xref="S5.SS2.p4.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS2.p4.2.m2.1.1.1" xref="S5.SS2.p4.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.p4.2.m2.1.1.3" xref="S5.SS2.p4.2.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.2.m2.1b"><apply id="S5.SS2.p4.2.m2.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1"><lt id="S5.SS2.p4.2.m2.1.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1.1"></lt><ci id="S5.SS2.p4.2.m2.1.1.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2">ğ‘</ci><cn id="S5.SS2.p4.2.m2.1.1.3.cmml" type="float" xref="S5.SS2.p4.2.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.2.m2.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.2.m2.1d">italic_p &lt; 0.05</annotation></semantics></math>) the performance ofÂ <math alttext="13/17" class="ltx_Math" display="inline" id="S5.SS2.p4.3.m3.1"><semantics id="S5.SS2.p4.3.m3.1a"><mrow id="S5.SS2.p4.3.m3.1.1" xref="S5.SS2.p4.3.m3.1.1.cmml"><mn id="S5.SS2.p4.3.m3.1.1.2" xref="S5.SS2.p4.3.m3.1.1.2.cmml">13</mn><mo id="S5.SS2.p4.3.m3.1.1.1" xref="S5.SS2.p4.3.m3.1.1.1.cmml">/</mo><mn id="S5.SS2.p4.3.m3.1.1.3" xref="S5.SS2.p4.3.m3.1.1.3.cmml">17</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.3.m3.1b"><apply id="S5.SS2.p4.3.m3.1.1.cmml" xref="S5.SS2.p4.3.m3.1.1"><divide id="S5.SS2.p4.3.m3.1.1.1.cmml" xref="S5.SS2.p4.3.m3.1.1.1"></divide><cn id="S5.SS2.p4.3.m3.1.1.2.cmml" type="integer" xref="S5.SS2.p4.3.m3.1.1.2">13</cn><cn id="S5.SS2.p4.3.m3.1.1.3.cmml" type="integer" xref="S5.SS2.p4.3.m3.1.1.3">17</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.3.m3.1c">13/17</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.3.m3.1d">13 / 17</annotation></semantics></math> single-task benchmarks. The mean Dice score difference (VoxelPrompt relative to benchmarks) is <math alttext="+4.3\pm 5.7\%" class="ltx_Math" display="inline" id="S5.SS2.p4.4.m4.1"><semantics id="S5.SS2.p4.4.m4.1a"><mrow id="S5.SS2.p4.4.m4.1.1" xref="S5.SS2.p4.4.m4.1.1.cmml"><mrow id="S5.SS2.p4.4.m4.1.1.2" xref="S5.SS2.p4.4.m4.1.1.2.cmml"><mo id="S5.SS2.p4.4.m4.1.1.2a" xref="S5.SS2.p4.4.m4.1.1.2.cmml">+</mo><mn id="S5.SS2.p4.4.m4.1.1.2.2" xref="S5.SS2.p4.4.m4.1.1.2.2.cmml">4.3</mn></mrow><mo id="S5.SS2.p4.4.m4.1.1.1" xref="S5.SS2.p4.4.m4.1.1.1.cmml">Â±</mo><mrow id="S5.SS2.p4.4.m4.1.1.3" xref="S5.SS2.p4.4.m4.1.1.3.cmml"><mn id="S5.SS2.p4.4.m4.1.1.3.2" xref="S5.SS2.p4.4.m4.1.1.3.2.cmml">5.7</mn><mo id="S5.SS2.p4.4.m4.1.1.3.1" xref="S5.SS2.p4.4.m4.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.4.m4.1b"><apply id="S5.SS2.p4.4.m4.1.1.cmml" xref="S5.SS2.p4.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.p4.4.m4.1.1.1.cmml" xref="S5.SS2.p4.4.m4.1.1.1">plus-or-minus</csymbol><apply id="S5.SS2.p4.4.m4.1.1.2.cmml" xref="S5.SS2.p4.4.m4.1.1.2"><plus id="S5.SS2.p4.4.m4.1.1.2.1.cmml" xref="S5.SS2.p4.4.m4.1.1.2"></plus><cn id="S5.SS2.p4.4.m4.1.1.2.2.cmml" type="float" xref="S5.SS2.p4.4.m4.1.1.2.2">4.3</cn></apply><apply id="S5.SS2.p4.4.m4.1.1.3.cmml" xref="S5.SS2.p4.4.m4.1.1.3"><csymbol cd="latexml" id="S5.SS2.p4.4.m4.1.1.3.1.cmml" xref="S5.SS2.p4.4.m4.1.1.3.1">percent</csymbol><cn id="S5.SS2.p4.4.m4.1.1.3.2.cmml" type="float" xref="S5.SS2.p4.4.m4.1.1.3.2">5.7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.4.m4.1c">+4.3\pm 5.7\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.4.m4.1d">+ 4.3 Â± 5.7 %</annotation></semantics></math> across pathology targets and <math alttext="-0.1\pm 0.3\%" class="ltx_Math" display="inline" id="S5.SS2.p4.5.m5.1"><semantics id="S5.SS2.p4.5.m5.1a"><mrow id="S5.SS2.p4.5.m5.1.1" xref="S5.SS2.p4.5.m5.1.1.cmml"><mrow id="S5.SS2.p4.5.m5.1.1.2" xref="S5.SS2.p4.5.m5.1.1.2.cmml"><mo id="S5.SS2.p4.5.m5.1.1.2a" xref="S5.SS2.p4.5.m5.1.1.2.cmml">âˆ’</mo><mn id="S5.SS2.p4.5.m5.1.1.2.2" xref="S5.SS2.p4.5.m5.1.1.2.2.cmml">0.1</mn></mrow><mo id="S5.SS2.p4.5.m5.1.1.1" xref="S5.SS2.p4.5.m5.1.1.1.cmml">Â±</mo><mrow id="S5.SS2.p4.5.m5.1.1.3" xref="S5.SS2.p4.5.m5.1.1.3.cmml"><mn id="S5.SS2.p4.5.m5.1.1.3.2" xref="S5.SS2.p4.5.m5.1.1.3.2.cmml">0.3</mn><mo id="S5.SS2.p4.5.m5.1.1.3.1" xref="S5.SS2.p4.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.5.m5.1b"><apply id="S5.SS2.p4.5.m5.1.1.cmml" xref="S5.SS2.p4.5.m5.1.1"><csymbol cd="latexml" id="S5.SS2.p4.5.m5.1.1.1.cmml" xref="S5.SS2.p4.5.m5.1.1.1">plus-or-minus</csymbol><apply id="S5.SS2.p4.5.m5.1.1.2.cmml" xref="S5.SS2.p4.5.m5.1.1.2"><minus id="S5.SS2.p4.5.m5.1.1.2.1.cmml" xref="S5.SS2.p4.5.m5.1.1.2"></minus><cn id="S5.SS2.p4.5.m5.1.1.2.2.cmml" type="float" xref="S5.SS2.p4.5.m5.1.1.2.2">0.1</cn></apply><apply id="S5.SS2.p4.5.m5.1.1.3.cmml" xref="S5.SS2.p4.5.m5.1.1.3"><csymbol cd="latexml" id="S5.SS2.p4.5.m5.1.1.3.1.cmml" xref="S5.SS2.p4.5.m5.1.1.3.1">percent</csymbol><cn id="S5.SS2.p4.5.m5.1.1.3.2.cmml" type="float" xref="S5.SS2.p4.5.m5.1.1.3.2">0.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.5.m5.1c">-0.1\pm 0.3\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.5.m5.1d">- 0.1 Â± 0.3 %</annotation></semantics></math> across anatomical structures. For the four ROIs in which VoxelPrompt significantly under-performs these benchmarks, the mean difference in Dice is only <math alttext="-0.4\pm 0.2\%" class="ltx_Math" display="inline" id="S5.SS2.p4.6.m6.1"><semantics id="S5.SS2.p4.6.m6.1a"><mrow id="S5.SS2.p4.6.m6.1.1" xref="S5.SS2.p4.6.m6.1.1.cmml"><mrow id="S5.SS2.p4.6.m6.1.1.2" xref="S5.SS2.p4.6.m6.1.1.2.cmml"><mo id="S5.SS2.p4.6.m6.1.1.2a" xref="S5.SS2.p4.6.m6.1.1.2.cmml">âˆ’</mo><mn id="S5.SS2.p4.6.m6.1.1.2.2" xref="S5.SS2.p4.6.m6.1.1.2.2.cmml">0.4</mn></mrow><mo id="S5.SS2.p4.6.m6.1.1.1" xref="S5.SS2.p4.6.m6.1.1.1.cmml">Â±</mo><mrow id="S5.SS2.p4.6.m6.1.1.3" xref="S5.SS2.p4.6.m6.1.1.3.cmml"><mn id="S5.SS2.p4.6.m6.1.1.3.2" xref="S5.SS2.p4.6.m6.1.1.3.2.cmml">0.2</mn><mo id="S5.SS2.p4.6.m6.1.1.3.1" xref="S5.SS2.p4.6.m6.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.6.m6.1b"><apply id="S5.SS2.p4.6.m6.1.1.cmml" xref="S5.SS2.p4.6.m6.1.1"><csymbol cd="latexml" id="S5.SS2.p4.6.m6.1.1.1.cmml" xref="S5.SS2.p4.6.m6.1.1.1">plus-or-minus</csymbol><apply id="S5.SS2.p4.6.m6.1.1.2.cmml" xref="S5.SS2.p4.6.m6.1.1.2"><minus id="S5.SS2.p4.6.m6.1.1.2.1.cmml" xref="S5.SS2.p4.6.m6.1.1.2"></minus><cn id="S5.SS2.p4.6.m6.1.1.2.2.cmml" type="float" xref="S5.SS2.p4.6.m6.1.1.2.2">0.4</cn></apply><apply id="S5.SS2.p4.6.m6.1.1.3.cmml" xref="S5.SS2.p4.6.m6.1.1.3"><csymbol cd="latexml" id="S5.SS2.p4.6.m6.1.1.3.1.cmml" xref="S5.SS2.p4.6.m6.1.1.3.1">percent</csymbol><cn id="S5.SS2.p4.6.m6.1.1.3.2.cmml" type="float" xref="S5.SS2.p4.6.m6.1.1.3.2">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.6.m6.1c">-0.4\pm 0.2\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.6.m6.1d">- 0.4 Â± 0.2 %</annotation></semantics></math>. FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.F4" title="Figure 4 â€£ 5.1 Illustrative Use Cases â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">4</span></a> (bottom) shows that VoxelPrompt significantlyÂ (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S5.SS2.p4.7.m7.1"><semantics id="S5.SS2.p4.7.m7.1a"><mrow id="S5.SS2.p4.7.m7.1.1" xref="S5.SS2.p4.7.m7.1.1.cmml"><mi id="S5.SS2.p4.7.m7.1.1.2" xref="S5.SS2.p4.7.m7.1.1.2.cmml">p</mi><mo id="S5.SS2.p4.7.m7.1.1.1" xref="S5.SS2.p4.7.m7.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.p4.7.m7.1.1.3" xref="S5.SS2.p4.7.m7.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.7.m7.1b"><apply id="S5.SS2.p4.7.m7.1.1.cmml" xref="S5.SS2.p4.7.m7.1.1"><lt id="S5.SS2.p4.7.m7.1.1.1.cmml" xref="S5.SS2.p4.7.m7.1.1.1"></lt><ci id="S5.SS2.p4.7.m7.1.1.2.cmml" xref="S5.SS2.p4.7.m7.1.1.2">ğ‘</ci><cn id="S5.SS2.p4.7.m7.1.1.3.cmml" type="float" xref="S5.SS2.p4.7.m7.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.7.m7.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.7.m7.1d">italic_p &lt; 0.05</annotation></semantics></math>) outperforms SynthSeg on 23/45 ROIs and exhibits a mean Dice improvement ofÂ <math alttext="+1.1\pm 2.3\%" class="ltx_Math" display="inline" id="S5.SS2.p4.8.m8.1"><semantics id="S5.SS2.p4.8.m8.1a"><mrow id="S5.SS2.p4.8.m8.1.1" xref="S5.SS2.p4.8.m8.1.1.cmml"><mrow id="S5.SS2.p4.8.m8.1.1.2" xref="S5.SS2.p4.8.m8.1.1.2.cmml"><mo id="S5.SS2.p4.8.m8.1.1.2a" xref="S5.SS2.p4.8.m8.1.1.2.cmml">+</mo><mn id="S5.SS2.p4.8.m8.1.1.2.2" xref="S5.SS2.p4.8.m8.1.1.2.2.cmml">1.1</mn></mrow><mo id="S5.SS2.p4.8.m8.1.1.1" xref="S5.SS2.p4.8.m8.1.1.1.cmml">Â±</mo><mrow id="S5.SS2.p4.8.m8.1.1.3" xref="S5.SS2.p4.8.m8.1.1.3.cmml"><mn id="S5.SS2.p4.8.m8.1.1.3.2" xref="S5.SS2.p4.8.m8.1.1.3.2.cmml">2.3</mn><mo id="S5.SS2.p4.8.m8.1.1.3.1" xref="S5.SS2.p4.8.m8.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.8.m8.1b"><apply id="S5.SS2.p4.8.m8.1.1.cmml" xref="S5.SS2.p4.8.m8.1.1"><csymbol cd="latexml" id="S5.SS2.p4.8.m8.1.1.1.cmml" xref="S5.SS2.p4.8.m8.1.1.1">plus-or-minus</csymbol><apply id="S5.SS2.p4.8.m8.1.1.2.cmml" xref="S5.SS2.p4.8.m8.1.1.2"><plus id="S5.SS2.p4.8.m8.1.1.2.1.cmml" xref="S5.SS2.p4.8.m8.1.1.2"></plus><cn id="S5.SS2.p4.8.m8.1.1.2.2.cmml" type="float" xref="S5.SS2.p4.8.m8.1.1.2.2">1.1</cn></apply><apply id="S5.SS2.p4.8.m8.1.1.3.cmml" xref="S5.SS2.p4.8.m8.1.1.3"><csymbol cd="latexml" id="S5.SS2.p4.8.m8.1.1.3.1.cmml" xref="S5.SS2.p4.8.m8.1.1.3.1">percent</csymbol><cn id="S5.SS2.p4.8.m8.1.1.3.2.cmml" type="float" xref="S5.SS2.p4.8.m8.1.1.3.2">2.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.8.m8.1c">+1.1\pm 2.3\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.8.m8.1d">+ 1.1 Â± 2.3 %</annotation></semantics></math> over all 45 structures. Overall, this demonstrates that a single VoxelPrompt model can perform on par with individual, specialized segmentation tools across a wide range of targets.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="141" id="S5.F5.g1" src="x5.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.3.2" style="font-size:90%;">Accuracy of pathology characterization using natural language for five separate classification subtasks. Average subtask accuracy is shown on the left. VoxelPrompt (black) parallels the performance of individually-trained, single-task classifiers (purple) and a fine-tuned RadFM model (blue) â€“ a state-of-the-art method for 3D visual question-answering.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Pathology Characterization</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We evaluate the ability of VoxelPrompt to characterize image features with predicted language description. Specifically, we focus on five pathology-based visual question-answering tasks (also used during training). These involve classifying lesion (1) signal intensity, (2) broad cerebral location, (3) stroke-affected vascular territory, (4) diffusion restriction, and (5) post-contrast enhancement.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">For each of these tasks, we curate a subset of held-out subjects with relevant features while ensuring equal representation of possible classification categories in each subset. In total, the evaluation set consists of 102 cases, with per-task breakdowns detailed in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS5" title="A.5 Classification Analysis Data â€£ Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">A.5</span></a>. During model evaluation, we consider a prediction as correct if the output natural language response exactly matches the expected characterization. Using a paired <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">t</span>-test, we compare the VoxelPrompt per-task classification accuracy to that of multiple models, defined below.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.5"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.5.1">Specialized Benchmarks.</span>
We compare VoxelPrompt to a set of classifier benchmarks, each trained for one of the five pathology characterization tasks inÂ <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">ğ’¯</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><ci id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">caligraphic_T</annotation></semantics></math>. As opposed to using language, the single-task benchmark models directly predict label probabilities for a fixed set of task-specific characterizations. We implement these models using the architecture ofÂ <math alttext="m_{\text{enc}}" class="ltx_Math" display="inline" id="S5.SS3.p3.2.m2.1"><semantics id="S5.SS3.p3.2.m2.1a"><msub id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml"><mi id="S5.SS3.p3.2.m2.1.1.2" xref="S5.SS3.p3.2.m2.1.1.2.cmml">m</mi><mtext id="S5.SS3.p3.2.m2.1.1.3" xref="S5.SS3.p3.2.m2.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><apply id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.2.m2.1.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p3.2.m2.1.1.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2">ğ‘š</ci><ci id="S5.SS3.p3.2.m2.1.1.3a.cmml" xref="S5.SS3.p3.2.m2.1.1.3"><mtext id="S5.SS3.p3.2.m2.1.1.3.cmml" mathsize="70%" xref="S5.SS3.p3.2.m2.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">m_{\text{enc}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.2.m2.1d">italic_m start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT</annotation></semantics></math>, withÂ <math alttext="\phi" class="ltx_Math" display="inline" id="S5.SS3.p3.3.m3.1"><semantics id="S5.SS3.p3.3.m3.1a"><mi id="S5.SS3.p3.3.m3.1.1" xref="S5.SS3.p3.3.m3.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.1b"><ci id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.3.m3.1d">italic_Ï•</annotation></semantics></math> mixing layers replaced as in the benchmarks of the previous experiment. We reduce the spatial dimensions of the deepest encoder layer output using a global max operator, then we compute the max over all input volume streams. To compute classification probabilities forÂ <math alttext="n" class="ltx_Math" display="inline" id="S5.SS3.p3.4.m4.1"><semantics id="S5.SS3.p3.4.m4.1a"><mi id="S5.SS3.p3.4.m4.1.1" xref="S5.SS3.p3.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.4.m4.1b"><ci id="S5.SS3.p3.4.m4.1.1.cmml" xref="S5.SS3.p3.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.4.m4.1d">italic_n</annotation></semantics></math> possible descriptions, we pass the stream-pooled features to a fully-connected layer withÂ <math alttext="n" class="ltx_Math" display="inline" id="S5.SS3.p3.5.m5.1"><semantics id="S5.SS3.p3.5.m5.1a"><mi id="S5.SS3.p3.5.m5.1.1" xref="S5.SS3.p3.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.5.m5.1b"><ci id="S5.SS3.p3.5.m5.1.1.cmml" xref="S5.SS3.p3.5.m5.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.5.m5.1d">italic_n</annotation></semantics></math> output channels and softmax activation. During benchmark optimization, we use the categorical cross-entropy loss on these predicted probabilities.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">State-of-the-Art Baseline.</span>
We also compare VoxelPrompt to RadFMÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite>, the only publicly released, state-of-the-art architecture for medical visual question answering that can process multiple 3D images simultaneously. Overall, we find that the pretrained RadFM as well as universal 2D vision-language models, such as ChatGPTÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib143" title=""><span class="ltx_text" style="font-size:90%;">143</span></a>]</cite>, cannot generalize to the neuroimaging tasks used in this experiment (AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#Sx2.SS6" title="A.6 Zero-Shot Baseline Evaluation â€£ Appendix â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">A.6</span></a>). Therefore, we <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.2">fine-tune</span> RadFM on our subset of pathology characterization tasks, using training code released with the pretrained model weights. To fit the optimization within 80 GB of GPU memory, we keep only the first eight hidden transformer layers of the language model. We do not modify any other model components. As required by the vision transformer, we resize all input volume spatial dimensions to the nearest multiple ofÂ <math alttext="32\times 32\times 4" class="ltx_Math" display="inline" id="S5.SS3.p4.1.m1.1"><semantics id="S5.SS3.p4.1.m1.1a"><mrow id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml"><mn id="S5.SS3.p4.1.m1.1.1.2" xref="S5.SS3.p4.1.m1.1.1.2.cmml">32</mn><mo id="S5.SS3.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS3.p4.1.m1.1.1.1.cmml">Ã—</mo><mn id="S5.SS3.p4.1.m1.1.1.3" xref="S5.SS3.p4.1.m1.1.1.3.cmml">32</mn><mo id="S5.SS3.p4.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S5.SS3.p4.1.m1.1.1.1.cmml">Ã—</mo><mn id="S5.SS3.p4.1.m1.1.1.4" xref="S5.SS3.p4.1.m1.1.1.4.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><apply id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"><times id="S5.SS3.p4.1.m1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1.1"></times><cn id="S5.SS3.p4.1.m1.1.1.2.cmml" type="integer" xref="S5.SS3.p4.1.m1.1.1.2">32</cn><cn id="S5.SS3.p4.1.m1.1.1.3.cmml" type="integer" xref="S5.SS3.p4.1.m1.1.1.3">32</cn><cn id="S5.SS3.p4.1.m1.1.1.4.cmml" type="integer" xref="S5.SS3.p4.1.m1.1.1.4">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">32\times 32\times 4</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.1.m1.1d">32 Ã— 32 Ã— 4</annotation></semantics></math>. During fine-tuning, we use only the expected language response (without code) as target text.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.4"><span class="ltx_text ltx_font_bold" id="S5.SS3.p5.4.1">Results.</span>
FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.F5" title="Figure 5 â€£ 5.2 Segmentation Accuracy â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">5</span></a> shows that VoxelPrompt achieves an average classification accuracy ofÂ <math alttext="89.0\pm 3.6\%" class="ltx_Math" display="inline" id="S5.SS3.p5.1.m1.1"><semantics id="S5.SS3.p5.1.m1.1a"><mrow id="S5.SS3.p5.1.m1.1.1" xref="S5.SS3.p5.1.m1.1.1.cmml"><mn id="S5.SS3.p5.1.m1.1.1.2" xref="S5.SS3.p5.1.m1.1.1.2.cmml">89.0</mn><mo id="S5.SS3.p5.1.m1.1.1.1" xref="S5.SS3.p5.1.m1.1.1.1.cmml">Â±</mo><mrow id="S5.SS3.p5.1.m1.1.1.3" xref="S5.SS3.p5.1.m1.1.1.3.cmml"><mn id="S5.SS3.p5.1.m1.1.1.3.2" xref="S5.SS3.p5.1.m1.1.1.3.2.cmml">3.6</mn><mo id="S5.SS3.p5.1.m1.1.1.3.1" xref="S5.SS3.p5.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.1.m1.1b"><apply id="S5.SS3.p5.1.m1.1.1.cmml" xref="S5.SS3.p5.1.m1.1.1"><csymbol cd="latexml" id="S5.SS3.p5.1.m1.1.1.1.cmml" xref="S5.SS3.p5.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S5.SS3.p5.1.m1.1.1.2.cmml" type="float" xref="S5.SS3.p5.1.m1.1.1.2">89.0</cn><apply id="S5.SS3.p5.1.m1.1.1.3.cmml" xref="S5.SS3.p5.1.m1.1.1.3"><csymbol cd="latexml" id="S5.SS3.p5.1.m1.1.1.3.1.cmml" xref="S5.SS3.p5.1.m1.1.1.3.1">percent</csymbol><cn id="S5.SS3.p5.1.m1.1.1.3.2.cmml" type="float" xref="S5.SS3.p5.1.m1.1.1.3.2">3.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.1.m1.1c">89.0\pm 3.6\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.1.m1.1d">89.0 Â± 3.6 %</annotation></semantics></math> over all tasks, matching the performanceÂ (<math alttext="p&gt;0.05" class="ltx_Math" display="inline" id="S5.SS3.p5.2.m2.1"><semantics id="S5.SS3.p5.2.m2.1a"><mrow id="S5.SS3.p5.2.m2.1.1" xref="S5.SS3.p5.2.m2.1.1.cmml"><mi id="S5.SS3.p5.2.m2.1.1.2" xref="S5.SS3.p5.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS3.p5.2.m2.1.1.1" xref="S5.SS3.p5.2.m2.1.1.1.cmml">&gt;</mo><mn id="S5.SS3.p5.2.m2.1.1.3" xref="S5.SS3.p5.2.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.2.m2.1b"><apply id="S5.SS3.p5.2.m2.1.1.cmml" xref="S5.SS3.p5.2.m2.1.1"><gt id="S5.SS3.p5.2.m2.1.1.1.cmml" xref="S5.SS3.p5.2.m2.1.1.1"></gt><ci id="S5.SS3.p5.2.m2.1.1.2.cmml" xref="S5.SS3.p5.2.m2.1.1.2">ğ‘</ci><cn id="S5.SS3.p5.2.m2.1.1.3.cmml" type="float" xref="S5.SS3.p5.2.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.2.m2.1c">p&gt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.2.m2.1d">italic_p &gt; 0.05</annotation></semantics></math>) of the single-task benchmarksÂ (<math alttext="89.3\pm 4.2" class="ltx_Math" display="inline" id="S5.SS3.p5.3.m3.1"><semantics id="S5.SS3.p5.3.m3.1a"><mrow id="S5.SS3.p5.3.m3.1.1" xref="S5.SS3.p5.3.m3.1.1.cmml"><mn id="S5.SS3.p5.3.m3.1.1.2" xref="S5.SS3.p5.3.m3.1.1.2.cmml">89.3</mn><mo id="S5.SS3.p5.3.m3.1.1.1" xref="S5.SS3.p5.3.m3.1.1.1.cmml">Â±</mo><mn id="S5.SS3.p5.3.m3.1.1.3" xref="S5.SS3.p5.3.m3.1.1.3.cmml">4.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.3.m3.1b"><apply id="S5.SS3.p5.3.m3.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p5.3.m3.1.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1.1">plus-or-minus</csymbol><cn id="S5.SS3.p5.3.m3.1.1.2.cmml" type="float" xref="S5.SS3.p5.3.m3.1.1.2">89.3</cn><cn id="S5.SS3.p5.3.m3.1.1.3.cmml" type="float" xref="S5.SS3.p5.3.m3.1.1.3">4.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.3.m3.1c">89.3\pm 4.2</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.3.m3.1d">89.3 Â± 4.2</annotation></semantics></math>%) as well as the fine-tuned RadFM modelÂ (<math alttext="87.1\pm 7.9\%" class="ltx_Math" display="inline" id="S5.SS3.p5.4.m4.1"><semantics id="S5.SS3.p5.4.m4.1a"><mrow id="S5.SS3.p5.4.m4.1.1" xref="S5.SS3.p5.4.m4.1.1.cmml"><mn id="S5.SS3.p5.4.m4.1.1.2" xref="S5.SS3.p5.4.m4.1.1.2.cmml">87.1</mn><mo id="S5.SS3.p5.4.m4.1.1.1" xref="S5.SS3.p5.4.m4.1.1.1.cmml">Â±</mo><mrow id="S5.SS3.p5.4.m4.1.1.3" xref="S5.SS3.p5.4.m4.1.1.3.cmml"><mn id="S5.SS3.p5.4.m4.1.1.3.2" xref="S5.SS3.p5.4.m4.1.1.3.2.cmml">7.9</mn><mo id="S5.SS3.p5.4.m4.1.1.3.1" xref="S5.SS3.p5.4.m4.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.4.m4.1b"><apply id="S5.SS3.p5.4.m4.1.1.cmml" xref="S5.SS3.p5.4.m4.1.1"><csymbol cd="latexml" id="S5.SS3.p5.4.m4.1.1.1.cmml" xref="S5.SS3.p5.4.m4.1.1.1">plus-or-minus</csymbol><cn id="S5.SS3.p5.4.m4.1.1.2.cmml" type="float" xref="S5.SS3.p5.4.m4.1.1.2">87.1</cn><apply id="S5.SS3.p5.4.m4.1.1.3.cmml" xref="S5.SS3.p5.4.m4.1.1.3"><csymbol cd="latexml" id="S5.SS3.p5.4.m4.1.1.3.1.cmml" xref="S5.SS3.p5.4.m4.1.1.3.1">percent</csymbol><cn id="S5.SS3.p5.4.m4.1.1.3.2.cmml" type="float" xref="S5.SS3.p5.4.m4.1.1.3.2">7.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.4.m4.1c">87.1\pm 7.9\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.4.m4.1d">87.1 Â± 7.9 %</annotation></semantics></math>). These results demonstrate that VoxelPrompt can achieve language-based image characterization with comparable performance to specialized classification and medical vision-language architectures.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduced VoxelPrompt, a vision-language agent that can perform diverse radiological tasks (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">1</span></a>). This single, unified framework conducts medical image analyses that typically require a multitude of specialized models and extensive user oversight.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Grounded Analysis.</span> To solve a desired task, the VoxelPrompt agent orchestrates the use of various computational tools, including instructable vision networks that can accurately segment hundreds of image featuresÂ (ExperimentÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.SS2" title="5.2 Segmentation Accuracy â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">5.2</span></a>). This approach provides results that are grounded by explicit image processing, leading to better reliability than existing vision-language models. For instance, rather than modelling metrics directly as estimated text, VoxelPrompt computes relevant outputs through a traceable sequence of operations, providing a level of transparency that is essential for clinical applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Flexible Language Prompting.</span> In VoxelPrompt, natural language prompting facilitates intuitive interaction and context-specific queries. Unlike interactive techniques relying on only bounding boxes, clicks, or in-context example pairs, language provides a compact mechanism to define detailed ROI targets with a vocabulary familiar to practitionersÂ (FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.F3" title="Figure 3 â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">We achieve this flexibility through latent communication between the jointly-trained agent and vision networks, as opposed to selecting from a collection of specialized, pretrained segmentation models. Such an approach would face combinatorial challenges for context-specific analysis of ROIs distinguished by nuanced criteria, such as relative position or signal intensity. We demonstrate that VoxelPrompt matches the performance of individual segmentation benchmarks while enabling a flexible interaction mechanism for a landscape of goals.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p5">
<p class="ltx_p" id="S6.p5.1"><span class="ltx_text ltx_font_bold" id="S6.p5.1.1">Data Limitations.</span> VoxelPrompt demonstrates the ability to effectively tackle a diverse set of ROI processing and language-based image characterization tasks. However, since we optimize the model from scratch, its utility is restricted to the scenarios covered by the training domain.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">To extend the scope of VoxelPrompt, we plan to construct a comprehensive training dataset that spans a substantially larger range of imaging tasks and anatomical fields. We can incorporate real hospital data, along with associated text reports, to improve the ability of VoxelPrompt to generate key clinical documentation and align learned representations with complex pathologies seen in practice. Additionally, leveraging pretrained language models with broad medical, programming, and problem solving knowledge could establish a robust backbone to support generalization to unfamiliar tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p7">
<p class="ltx_p" id="S6.p7.1"><span class="ltx_text ltx_font_bold" id="S6.p7.1.1">System Integration.</span> As a code-predicting agent framework, VoxelPrompt can integrate with an array of external systems and APIs. This includes accessing medical databases to integrate patient records for additional context, pulling population-level statistics to support referential analysis, or implementing on-the-scanner interaction for real-time acquisition guidance. As an integrated interface, VoxelPrompt can grow to assist practitioners in efficiently navigating and interpreting imaging studies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p8">
<p class="ltx_p" id="S6.p8.1">By combining natural language interaction, instructable vision models, and agent-based planning, we believe VoxelPrompt provides an advanced, multi-task system that promises to support the spectrum of real-world clinical and research imaging aims.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work is supported in part by the National Institute of Biomedical Imaging and BioengineeringÂ (R01Â EB033773, T32Â EB001680), the Harvard-MIT Neuroimaging Training Program, the National Science Foundation Graduate Research Fellowships Program, and Quanta Computer Inc.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.4.4.1" style="font-size:90%;">Ashburner and Friston [2005]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.6.1" style="font-size:90%;">
John Ashburner and KarlÂ J Friston.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">Unified segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.8.1" style="font-size:90%;">neuroimage</em><span class="ltx_text" id="bib.bib1.9.2" style="font-size:90%;">, 26(3):839â€“851, 2005.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.4.4.1" style="font-size:90%;">Cox [1996]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.6.1" style="font-size:90%;">
RobertÂ W Cox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">Afni: Software for analysis and visualization of functional magnetic resonance neuroimages.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.8.1" style="font-size:90%;">Computers and Biomedical research</em><span class="ltx_text" id="bib.bib2.9.2" style="font-size:90%;">, 29(3):162â€“173, 1996.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.5.5.1" style="font-size:90%;">Craddock etÂ al. [2013]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">
Cameron Craddock, Sharad Sikka, Brian Cheung, Ranjeet Khanuja, SatrajitÂ S Ghosh, Chaogan Yan, Qingyang Li, Daniel Lurie, Joshua Vogelstein, Randal Burns, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.8.1" style="font-size:90%;">Towards automated analysis of connectomes: The configurable pipeline for the analysis of connectomes (c-pac).
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.9.1" style="font-size:90%;">Front Neuroinform</em><span class="ltx_text" id="bib.bib3.10.2" style="font-size:90%;">, 42(10.3389), 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.4.4.1" style="font-size:90%;">Fischl [2012]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.6.1" style="font-size:90%;">
Bruce Fischl.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">Freesurfer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.8.1" style="font-size:90%;">Neuroimage</em><span class="ltx_text" id="bib.bib4.9.2" style="font-size:90%;">, 62(2):774â€“781, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Jenkinson etÂ al. [2012]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Mark Jenkinson, ChristianÂ F Beckmann, TimothyÂ EJ Behrens, MarkÂ W Woolrich, and StephenÂ M Smith.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Fsl.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.9.1" style="font-size:90%;">Neuroimage</em><span class="ltx_text" id="bib.bib5.10.2" style="font-size:90%;">, 62(2):782â€“790, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.4.4.1" style="font-size:90%;">Shattuck and Leahy [2002]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.6.1" style="font-size:90%;">
DavidÂ W Shattuck and RichardÂ M Leahy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">Brainsuite: An automated cortical surface identification tool.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.8.1" style="font-size:90%;">Medical image analysis</em><span class="ltx_text" id="bib.bib6.9.2" style="font-size:90%;">, 6(2):129â€“142, 2002.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Tournier etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
J-Donald Tournier, Robert Smith, David Raffelt, Rami Tabbara, Thijs Dhollander, Maximilian Pietsch, Daan Christiaens, Ben Jeurissen, Chun-Hung Yeh, and Alan Connelly.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Mrtrix3: A fast, flexible and open software framework for medical image processing and visualisation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.9.1" style="font-size:90%;">Neuroimage</em><span class="ltx_text" id="bib.bib7.10.2" style="font-size:90%;">, 202:116137, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Billot etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Benjamin Billot, DouglasÂ N Greve, Oula Puonti, Axel Thielscher, Koen VanÂ Leemput, Bruce Fischl, AdrianÂ V Dalca, JuanÂ Eugenio Iglesias, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Synthseg: Segmentation of brain mri scans of any contrast and resolution without retraining.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.9.1" style="font-size:90%;">Medical image analysis</em><span class="ltx_text" id="bib.bib8.10.2" style="font-size:90%;">, 86:102789, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Faber etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Jennifer Faber, David KÃ¼gler, Emad Bahrami, Lea-Sophie Heinz, Dagmar Timmann, ThomasÂ M Ernst, Katerina Deike-Hofmann, Thomas Klockgether, Bart vanÂ de Warrenburg, Judith van Gaalen, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">Cerebnet: A fast and reliable deep-learning pipeline for detailed cerebellum sub-segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.9.1" style="font-size:90%;">Neuroimage</em><span class="ltx_text" id="bib.bib9.10.2" style="font-size:90%;">, 264:119703, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Greve etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
DouglasÂ N Greve, Benjamin Billot, Devani Cordero, Andrew Hoopes, Malte Hoffmann, AdrianÂ V Dalca, Bruce Fischl, JuanÂ Eugenio Iglesias, and JeanÂ C Augustinack.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">A deep learning toolbox for automatic segmentation of subcortical limbic structures from mri images.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.9.1" style="font-size:90%;">Neuroimage</em><span class="ltx_text" id="bib.bib10.10.2" style="font-size:90%;">, 244:118610, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Henschel etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Leonie Henschel, Sailesh Conjeti, Santiago Estrada, Kersten Diers, Bruce Fischl, and Martin Reuter.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Fastsurfer - a fast and accurate deep learning based neuroimaging pipeline.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.9.1" style="font-size:90%;">NeuroImage</em><span class="ltx_text" id="bib.bib11.10.2" style="font-size:90%;">, 219:117012, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.5.5.1" style="font-size:90%;">Liu etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">
Manhua Liu, Fan Li, Hao Yan, Kundong Wang, Yixin Ma, LiÂ Shen, Mingqing Xu, Alzheimerâ€™s DiseaseÂ Neuroimaging Initiative, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.8.1" style="font-size:90%;">A multi-model deep convolutional neural network for automatic hippocampus segmentation and classification in alzheimerâ€™s disease.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.9.1" style="font-size:90%;">Neuroimage</em><span class="ltx_text" id="bib.bib12.10.2" style="font-size:90%;">, 208:116459, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Tregidgo etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
HenryÂ FJ Tregidgo, Sonja Soskic, MarkÂ D Olchanyi, Juri Althonayan, Benjamin Billot, Chiara Maffei, Polina Golland, Anastasia Yendiki, DanielÂ C Alexander, Martina Bocchetta, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Domain-agnostic segmentation of thalamic nuclei from joint structural and diffusion mri.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">International Conference on Medical Image Computing and Computer-Assisted Intervention</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, pages 247â€“257. Springer, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Hilbert etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
Adam Hilbert, VinceÂ I Madai, ElaÂ M Akay, OrhunÂ U Aydin, Jonas Behland, Jan Sobesky, Ivana Galinovic, AhmedÂ A Khalil, AbdelÂ A Taha, Jens Wuerfel, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.8.1" style="font-size:90%;">Brave-net: fully automated arterial brain vessel segmentation in patients with cerebrovascular disease.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.9.1" style="font-size:90%;">Frontiers in artificial intelligence</em><span class="ltx_text" id="bib.bib14.10.2" style="font-size:90%;">, 3:552258, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Livne etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Michelle Livne, Jana Rieger, OrhunÂ Utku Aydin, AbdelÂ Aziz Taha, ElaÂ Marie Akay, Tabea Kossen, Jan Sobesky, JohnÂ D Kelleher, Kristian Hildebrand, Dietmar Frey, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">A u-net deep learning framework for high performance vessel segmentation in patients with cerebrovascular disease.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.9.1" style="font-size:90%;">Frontiers in neuroscience</em><span class="ltx_text" id="bib.bib15.10.2" style="font-size:90%;">, 13:97, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Lu etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
QiÂ Lu, Wan Liu, Zhizheng Zhuo, Yuxing Li, Yunyun Duan, Pinnan Yu, Liying Qu, Chuyang Ye, and Yaou Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">A transfer learning approach to few-shot segmentation of novel white matter tracts.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.9.1" style="font-size:90%;">Medical Image Analysis</em><span class="ltx_text" id="bib.bib16.10.2" style="font-size:90%;">, 79:102454, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Wasserthal etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Jakob Wasserthal, Peter Neher, and KlausÂ H Maier-Hein.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Tractseg-fast and accurate white matter tract segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">NeuroImage</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 183:239â€“253, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.5.5.1" style="font-size:90%;">Zhang etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">
Fan Zhang, SuheylaÂ Cetin Karayumak, Nico Hoffmann, Yogesh Rathi, AlexandraÂ J Golby, and LaurenÂ J Oâ€™Donnell.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">Deep white matter analysis (deepwma): fast and consistent tractography segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.9.1" style="font-size:90%;">Medical Image Analysis</em><span class="ltx_text" id="bib.bib18.10.2" style="font-size:90%;">, 65:101761, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Menze etÂ al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
BjoernÂ H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">The multimodal brain tumor image segmentation benchmark (brats).
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.9.1" style="font-size:90%;">IEEE transactions on medical imaging</em><span class="ltx_text" id="bib.bib19.10.2" style="font-size:90%;">, 34(10):1993â€“2024, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Ranjbarzadeh etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Ramin Ranjbarzadeh, Abbas BagherianÂ Kasgari, Saeid JafarzadehÂ Ghoushchi, Shokofeh Anari, Maryam Naseri, and Malika Bendechache.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Brain tumor segmentation based on deep learning and an attention mechanism using mri multi-modalities brain images.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.9.1" style="font-size:90%;">Scientific Reports</em><span class="ltx_text" id="bib.bib20.10.2" style="font-size:90%;">, 11(1):1â€“17, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Wang etÂ al. [2021a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Wenxuan Wang, Chen Chen, Meng Ding, Jiangyun Li, Hong Yu, and Sen Zha.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">Transbts: Multimodal brain tumor segmentation using transformer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.9.1" style="font-size:90%;">ArXiv</em><span class="ltx_text" id="bib.bib21.10.2" style="font-size:90%;">, abs/2103.04430, 2021a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Zhang etÂ al. [2022a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Yao Zhang, Nanjun He, Jiawei Yang, Yuexiang Li, Dong Wei, Yawen Huang, Yang Zhang, Zhiqiang He, and Yefeng Zheng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">mmformer: Multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">ArXiv</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, abs/2206.02425, 2022a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Hssayeni etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
MurtadhaÂ D Hssayeni, MuayadÂ S Croock, AymenÂ D Salman, HassanÂ Falah Al-Khafaji, ZakariaÂ A Yahya, and Behnaz Ghoraani.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Intracranial hemorrhage segmentation using a deep convolutional model.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.9.1" style="font-size:90%;">Data</em><span class="ltx_text" id="bib.bib23.10.2" style="font-size:90%;">, 5(1):14, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Chen etÂ al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Liang Chen, Paul Bentley, and Daniel Rueckert.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Fully automatic acute ischemic lesion segmentation in dwi using convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.9.1" style="font-size:90%;">NeuroImage : Clinical</em><span class="ltx_text" id="bib.bib24.10.2" style="font-size:90%;">, 15:633 â€“ 643, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Liu etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Chin-Fu Liu, Johnny T.Â C. Hsu, Xin Xu, Sandhya Ramachandran, Victor Wang, MichaelÂ I. Miller, ArgyeÂ Elizabeth Hillis, AndreiaÂ Vasconcellos Faria, Max Steven J. Gregory W. Stephen M. James C. Werner Do Wintermark Warach Albers Davis Grotta HackeÂ Kang K, Max Wintermark, StevenÂ J. Warach, GregoryÂ W. Albers, StephenÂ M. Davis, JamesÂ Charles Grotta, Werner Hacke, Dong-Wha Kang, Chelsea Kidwell, WalterÂ J. Koroshetz, KennedyÂ R. Lees, MichaelÂ H. Lev, DavidÂ S. Liebeskind, A.Â Gregory Sorensen, VincentÂ N. Thijs, GÃ¶tz Thomalla, JoannaÂ Marguerite Wardlaw, and Marie Luby.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Deep learning-based detection and segmentation of diffusion abnormalities in acute ischemic stroke.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.9.1" style="font-size:90%;">Communications Medicine</em><span class="ltx_text" id="bib.bib25.10.2" style="font-size:90%;">, 1, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Nazari-Farsani etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Sanaz Nazari-Farsani, Yannan Yu, RuiÂ Duarte Armindo, MaartenÂ G. Lansberg, DavidÂ S. Liebeskind, GregoryÂ W. Albers, SÃ¸ren Christensen, CraigÂ S. Levin, and Greg Zaharchuk.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Predicting final ischemic stroke lesions from initial diffusion-weighted images using a deep neural network.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.9.1" style="font-size:90%;">NeuroImage : Clinical</em><span class="ltx_text" id="bib.bib26.10.2" style="font-size:90%;">, 37, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Guerrero etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Ricardo Guerrero, Chen Qin, Ozan Oktay, Christopher Bowles, Liang Chen, Richard Joules, Robin Wolz, MÂ delÂ C ValdÃ©s-HernÃ¡ndez, DavidÂ Alexander Dickie, Joanna Wardlaw, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">White matter hyperintensity and stroke lesion segmentation and differentiation using convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.9.1" style="font-size:90%;">NeuroImage: Clinical</em><span class="ltx_text" id="bib.bib27.10.2" style="font-size:90%;">, 17:918â€“934, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.5.5.1" style="font-size:90%;">Kuijf etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">
HugoÂ J Kuijf, JÂ Matthijs Biesbroek, Jeroen DeÂ Bresser, Rutger Heinen, Simon Andermatt, Mariana Bento, Matt Berseth, Mikhail Belyaev, MÂ Jorge Cardoso, Adria Casamitjana, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.8.1" style="font-size:90%;">Standardized assessment of automatic segmentation of white matter hyperintensities and results of the wmh segmentation challenge.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.9.1" style="font-size:90%;">IEEE transactions on medical imaging</em><span class="ltx_text" id="bib.bib28.10.2" style="font-size:90%;">, 38(11):2556â€“2568, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Pinaya etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
WalterÂ HL Pinaya, Petru-Daniel Tudosiu, Robert Gray, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, and MÂ Jorge Cardoso.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Unsupervised brain imaging 3d anomaly detection and segmentation with transformers.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.9.1" style="font-size:90%;">Medical Image Analysis</em><span class="ltx_text" id="bib.bib29.10.2" style="font-size:90%;">, 79:102475, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Baid etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Ujjwal Baid, Satyam Ghodasara, Suyash Mohan, Michel Bilello, Evan Calabrese, Errol Colak, Keyvan Farahani, Jayashree Kalpathy-Cramer, FelipeÂ C Kitamura, Sarthak Pati, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.9.1" style="font-size:90%;">arXiv preprint arXiv:2107.02314</em><span class="ltx_text" id="bib.bib30.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Chatterjee etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Soumick Chatterjee, FarazÂ Ahmed Nizamani, Andreas NÃ¼rnberger, and Oliver Speck.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Classification of brain tumours in mr images using deep spatiospatial models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.9.1" style="font-size:90%;">Scientific Reports</em><span class="ltx_text" id="bib.bib31.10.2" style="font-size:90%;">, 12(1):1505, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Cole etÂ al. [2017]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
JamesÂ H Cole, RudraÂ PK Poudel, Dimosthenis Tsagkrasoulis, MatthanÂ WA Caan, Claire Steves, TimÂ D Spector, and Giovanni Montana.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.9.1" style="font-size:90%;">NeuroImage</em><span class="ltx_text" id="bib.bib32.10.2" style="font-size:90%;">, 163:115â€“124, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Lee etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Jeyeon Lee, BrianÂ J Burkett, Hoon-Ki Min, MatthewÂ L Senjem, EmilyÂ S Lundt, Hugo Botha, Jonathan Graff-Radford, LelandÂ R Barnard, JeffreyÂ L Gunter, ChristopherÂ G Schwarz, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Deep learning-based brain age prediction in normal aging and dementia.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.9.1" style="font-size:90%;">Nature Aging</em><span class="ltx_text" id="bib.bib33.10.2" style="font-size:90%;">, 2(5):412â€“424, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Peng etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Han Peng, Weikang Gong, ChristianÂ F Beckmann, Andrea Vedaldi, and StephenÂ M Smith.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Accurate brain age prediction with lightweight deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.9.1" style="font-size:90%;">Medical image analysis</em><span class="ltx_text" id="bib.bib34.10.2" style="font-size:90%;">, 68:101871, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Basaia etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Silvia Basaia, Federica Agosta, Luca Wagner, Elisa Canu, Giuseppe Magnani, Roberto Santangelo, Massimo Filippi, Alzheimerâ€™s DiseaseÂ Neuroimaging Initiative, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Automated classification of alzheimerâ€™s disease and mild cognitive impairment using a single mri and deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.9.1" style="font-size:90%;">NeuroImage: Clinical</em><span class="ltx_text" id="bib.bib35.10.2" style="font-size:90%;">, 21:101645, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Wen etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Junhao Wen, Elina Thibeau-Sutre, Mauricio Diaz-Melo, Jorge Samper-GonzÃ¡lez, Alexandre Routier, Simona Bottani, Didier Dormont, Stanley Durrleman, Ninon Burgos, Olivier Colliot, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Convolutional neural networks for classification of alzheimerâ€™s disease: Overview and reproducible evaluation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.9.1" style="font-size:90%;">Medical image analysis</em><span class="ltx_text" id="bib.bib36.10.2" style="font-size:90%;">, 63:101694, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Li etÂ al. [2019a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Hongming Li, Mohamad Habes, DavidÂ A Wolk, Yong Fan, Alzheimerâ€™s DiseaseÂ Neuroimaging Initiative, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">A deep learning model for early prediction of alzheimerâ€™s disease dementia based on hippocampal magnetic resonance imaging data.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.9.1" style="font-size:90%;">Alzheimerâ€™s &amp; Dementia</em><span class="ltx_text" id="bib.bib37.10.2" style="font-size:90%;">, 15(8):1059â€“1070, 2019a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Lee etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Garam Lee, Kwangsik Nho, Byungkon Kang, Kyung-Ah Sohn, and Dokyoon Kim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Predicting alzheimerâ€™s disease progression using multi-modal deep learning approach.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.9.1" style="font-size:90%;">Scientific reports</em><span class="ltx_text" id="bib.bib38.10.2" style="font-size:90%;">, 9(1):1952, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.4.4.1" style="font-size:90%;">Caruana [1997]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.6.1" style="font-size:90%;">
Rich Caruana.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">Multitask learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.8.1" style="font-size:90%;">Machine learning</em><span class="ltx_text" id="bib.bib39.9.2" style="font-size:90%;">, 28(1):41â€“75, 1997.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.4.4.1" style="font-size:90%;">Sener and Koltun [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.6.1" style="font-size:90%;">
Ozan Sener and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">Multi-task learning as multi-objective optimization.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.8.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib40.9.2" style="font-size:90%;">, 31, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Zhao etÂ al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Yan Zhao, Xiuying Wang, Tongtong Che, Guoqing Bao, and Shuyu Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Multi-task deep learning for medical image computing and analysis: A review.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.9.1" style="font-size:90%;">Computers in Biology and Medicine</em><span class="ltx_text" id="bib.bib41.10.2" style="font-size:90%;">, 153:106496, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Amyar etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Amine Amyar, Romain Modzelewski, Hua Li, and SuÂ Ruan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Multi-task deep learning based ct imaging analysis for covid-19 pneumonia: Classification and segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.9.1" style="font-size:90%;">Computers in biology and medicine</em><span class="ltx_text" id="bib.bib42.10.2" style="font-size:90%;">, 126:104037, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Elmahdy etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
MohamedÂ S Elmahdy, Laurens Beljaards, Sahar Yousefi, Hessam Sokooti, Fons Verbeek, UulkeÂ A Van DerÂ Heide, and Marius Staring.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Joint registration and segmentation via multi-task learning for adaptive radiotherapy of prostate cancer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.9.1" style="font-size:90%;">IEEE Access</em><span class="ltx_text" id="bib.bib43.10.2" style="font-size:90%;">, 9:95551â€“95568, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Graham etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
Simon Graham, QuocÂ Dang Vu, Mostafa Jahanifar, Shan EÂ Ahmed Raza, Fayyaz Minhas, David Snead, and Nasir Rajpoot.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.8.1" style="font-size:90%;">One model is all you need: multi-task learning enables simultaneous histology image segmentation and classification.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.9.1" style="font-size:90%;">Medical Image Analysis</em><span class="ltx_text" id="bib.bib44.10.2" style="font-size:90%;">, 83:102685, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Li etÂ al. [2019b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
BoÂ Li, WiroÂ J Niessen, Stefan Klein, Marius deÂ Groot, MÂ Arfan Ikram, MeikeÂ W Vernooij, and EstherÂ E Bron.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">A hybrid deep learning framework for integrated segmentation and registration: evaluation on longitudinal white matter tract changes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib45.10.2" style="font-size:90%;">Medical Image Computing and Computer Assisted Interventionâ€“MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13â€“17, 2019, Proceedings, Part III 22</em><span class="ltx_text" id="bib.bib45.11.3" style="font-size:90%;">, pages 645â€“653. Springer, 2019b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.5.5.1" style="font-size:90%;">Tellez etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">
David Tellez, Diederik HÃ¶ppener, Cornelis Verhoef, Dirk GrÃ¼nhagen, Pieter Nierop, Michal Drozdzal, Jeroen Laak, and Francesco Ciompi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.8.1" style="font-size:90%;">Extending unsupervised neural image compression with supervised multitask learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib46.10.2" style="font-size:90%;">Medical Imaging with Deep Learning</em><span class="ltx_text" id="bib.bib46.11.3" style="font-size:90%;">, pages 770â€“783. PMLR, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Cheng etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Sam-med2d.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.16184</em><span class="ltx_text" id="bib.bib47.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Luo etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Xiangde Luo, Guotai Wang, Tao Song, Jingyang Zhang, Michael Aertsen, Jan Deprest, Sebastien Ourselin, Tom Vercauteren, and Shaoting Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">Mideepseg: Minimally interactive segmentation of unseen objects from medical images using deep learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.9.1" style="font-size:90%;">Medical image analysis</em><span class="ltx_text" id="bib.bib48.10.2" style="font-size:90%;">, 72:102102, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">Ma etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and BoÂ Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.8.1" style="font-size:90%;">Segment anything in medical images.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.9.1" style="font-size:90%;">Nature Communications</em><span class="ltx_text" id="bib.bib49.10.2" style="font-size:90%;">, 15(1):654, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">Sakinis etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
Tomas Sakinis, Fausto Milletari, Holger Roth, Panagiotis Korfiatis, Petro Kostandy, Kenneth Philbrick, Zeynettin Akkus, Ziyue Xu, Daguang Xu, and BradleyÂ J Erickson.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">Interactive segmentation of medical images through fully convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.9.1" style="font-size:90%;">arXiv preprint arXiv:1903.08205</em><span class="ltx_text" id="bib.bib50.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">Wong etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
HalleeÂ E Wong, Marianne Rakic, John Guttag, and AdrianÂ V Dalca.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">Scribbleprompt: Fast and flexible interactive segmentation for any medical image.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.07381</em><span class="ltx_text" id="bib.bib51.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Min etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">Metaicl: Learning to learn in context.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.9.1" style="font-size:90%;">arXiv preprint arXiv:2110.15943</em><span class="ltx_text" id="bib.bib52.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Xie etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
SangÂ Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">An explanation of in-context learning as implicit bayesian inference.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.9.1" style="font-size:90%;">arXiv preprint arXiv:2111.02080</em><span class="ltx_text" id="bib.bib53.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">Butoi etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
VictorÂ Ion Butoi, Jose JavierÂ Gonzalez Ortiz, Tianyu Ma, MertÂ R Sabuncu, John Guttag, and AdrianÂ V Dalca.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">Universeg: Universal medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.06131</em><span class="ltx_text" id="bib.bib54.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Ouyang etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Cheng Ouyang, Carlo Biffi, Chen Chen, Turkay Kart, Huaqi Qiu, and Daniel Rueckert.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">Self-supervised learning for few-shot medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.9.1" style="font-size:90%;">IEEE Transactions on Medical Imaging</em><span class="ltx_text" id="bib.bib55.10.2" style="font-size:90%;">, 41(7):1837â€“1848, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="font-size:90%;">Rakic etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
Marianne Rakic, HalleeÂ E Wong, Jose JavierÂ Gonzalez Ortiz, Beth Cimini, John Guttag, and AdrianÂ V Dalca.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.8.1" style="font-size:90%;">Tyche: Stochastic in-context learning for medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.9.1" style="font-size:90%;">arXiv preprint arXiv:2401.13650</em><span class="ltx_text" id="bib.bib56.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">Roy etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
AbhijitÂ Guha Roy, Shayan Siddiqui, Sebastian PÃ¶lsterl, Nassir Navab, and Christian Wachinger.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">â€˜squeeze &amp; exciteâ€™guided few-shot segmentation of volumetric images.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.9.1" style="font-size:90%;">Medical image analysis</em><span class="ltx_text" id="bib.bib57.10.2" style="font-size:90%;">, 59:101587, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.4.4.1" style="font-size:90%;">Czolbe and Dalca [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.6.1" style="font-size:90%;">
Steffen Czolbe and AdrianÂ V Dalca.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">Neuralizer: General neuroimage analysis without re-training.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib58.9.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib58.10.3" style="font-size:90%;">, pages 6217â€“6230, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.5.5.1" style="font-size:90%;">Radford etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.7.1" style="font-size:90%;">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib59.10.2" style="font-size:90%;">International conference on machine learning</em><span class="ltx_text" id="bib.bib59.11.3" style="font-size:90%;">, pages 8748â€“8763. PMLR, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.5.5.1" style="font-size:90%;">Johnson etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">
AlistairÂ EW Johnson, TomÂ J Pollard, SethÂ J Berkowitz, NathanielÂ R Greenbaum, MatthewÂ P Lungren, Chih-ying Deng, RogerÂ G Mark, and Steven Horng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.8.1" style="font-size:90%;">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.9.1" style="font-size:90%;">Scientific data</em><span class="ltx_text" id="bib.bib60.10.2" style="font-size:90%;">, 6(1):317, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">Lin etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, YaÂ Zhang, Yanfeng Wang, and Weidi Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">Pmc-clip: Contrastive language-image pre-training using biomedical documents.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib61.10.2" style="font-size:90%;">International Conference on Medical Image Computing and Computer-Assisted Intervention</em><span class="ltx_text" id="bib.bib61.11.3" style="font-size:90%;">, pages 525â€“536. Springer, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.5.5.1" style="font-size:90%;">Pelka etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">
Obioma Pelka, Sven Koitka, Johannes RÃ¼ckert, Felix Nensa, and ChristophÂ M Friedrich.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.8.1" style="font-size:90%;">Radiology objects in context (roco): a multimodal image dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib62.10.2" style="font-size:90%;">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3</em><span class="ltx_text" id="bib.bib62.11.3" style="font-size:90%;">, pages 180â€“189. Springer, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">Zhang etÂ al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, MuÂ Wei, Naveen Valluri, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.9.1" style="font-size:90%;">arXiv preprint arXiv:2303.00915</em><span class="ltx_text" id="bib.bib63.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">Chen etÂ al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
Qiuhui Chen, Xinyue Hu, Zirui Wang, and YiÂ Hong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.8.1" style="font-size:90%;">Medblip: Bootstrapping language-image pre-training from 3d medical images and texts.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.10799</em><span class="ltx_text" id="bib.bib64.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">Chen etÂ al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
Yinda Chen, Che Liu, Wei Huang, Sibo Cheng, Rossella Arcucci, and Zhiwei Xiong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">Generative text-guided 3d vision-language pretraining for unified medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.04811</em><span class="ltx_text" id="bib.bib65.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">Chen etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
Zhihong Chen, Guanbin Li, and Xiang Wan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">Align, reason and learn: Enhancing medical vision-and-language pre-training with knowledge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib66.10.2" style="font-size:90%;">Proceedings of the 30th ACM International Conference on Multimedia</em><span class="ltx_text" id="bib.bib66.11.3" style="font-size:90%;">, pages 5152â€“5161, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.5.5.1" style="font-size:90%;">Eslami etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">
Sedigheh Eslami, Christoph Meinel, and Gerard DeÂ Melo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">Pubmedclip: How much does clip benefit visual question answering in the medical domain?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib67.10.2" style="font-size:90%;">Findings of the Association for Computational Linguistics: EACL 2023</em><span class="ltx_text" id="bib.bib67.11.3" style="font-size:90%;">, pages 1181â€“1193, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="font-size:90%;">Khare etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="font-size:90%;">
Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, UÂ Deva Priyakumar, and CVÂ Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.8.1" style="font-size:90%;">Mmbert: Multimodal bert pretraining for improved medical vqa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib68.10.2" style="font-size:90%;">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</em><span class="ltx_text" id="bib.bib68.11.3" style="font-size:90%;">, pages 1033â€“1036. IEEE, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="font-size:90%;">Li etÂ al. [2024a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="font-size:90%;">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.8.1" style="font-size:90%;">Llava-med: Training a large language-and-vision assistant for biomedicine in one day.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib69.10.2" style="font-size:90%;">, 36, 2024a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.5.5.1" style="font-size:90%;">Liu etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="font-size:90%;">
BoÂ Liu, Li-Ming Zhan, LiÂ Xu, and Xiao-Ming Wu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.8.1" style="font-size:90%;">Medical visual question answering via conditional reasoning and contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.9.1" style="font-size:90%;">IEEE transactions on medical imaging</em><span class="ltx_text" id="bib.bib70.10.2" style="font-size:90%;">, 42(5):1532â€“1545, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.5.5.1" style="font-size:90%;">Moon etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.7.1" style="font-size:90%;">
JongÂ Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim, and Edward Choi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.8.1" style="font-size:90%;">Multi-modal understanding and generation for medical images and text via vision-language pre-training.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.9.1" style="font-size:90%;">IEEE Journal of Biomedical and Health Informatics</em><span class="ltx_text" id="bib.bib71.10.2" style="font-size:90%;">, 26(12):6070â€“6080, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.5.5.1" style="font-size:90%;">Moor etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="font-size:90%;">
Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, EduardoÂ Pontes Reis, and Pranav Rajpurkar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.8.1" style="font-size:90%;">Med-flamingo: a multimodal medical few-shot learner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib72.10.2" style="font-size:90%;">Machine Learning for Health (ML4H)</em><span class="ltx_text" id="bib.bib72.11.3" style="font-size:90%;">, pages 353â€“367. PMLR, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="font-size:90%;">MÃ¼ller etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="font-size:90%;">
Philip MÃ¼ller, Georgios Kaissis, Congyu Zou, and Daniel Rueckert.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.8.1" style="font-size:90%;">Joint learning of localized representations from medical images and reports.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib73.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib73.11.3" style="font-size:90%;">, pages 685â€“701. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="font-size:90%;">Wang etÂ al. [2021b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="font-size:90%;">
Xiaosong Wang, Ziyue Xu, Leo Tam, Dong Yang, and Daguang Xu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.8.1" style="font-size:90%;">Self-supervised image-text pre-training with mixed data in chest x-rays.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.9.1" style="font-size:90%;">arXiv preprint arXiv:2103.16022</em><span class="ltx_text" id="bib.bib74.10.2" style="font-size:90%;">, 2021b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.5.5.1" style="font-size:90%;">Wang etÂ al. [2022a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.7.1" style="font-size:90%;">
Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.8.1" style="font-size:90%;">Medclip: Contrastive learning from unpaired medical images and text.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.9.1" style="font-size:90%;">arXiv preprint arXiv:2210.10163</em><span class="ltx_text" id="bib.bib75.10.2" style="font-size:90%;">, 2022a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="font-size:90%;">Wu etÂ al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="font-size:90%;">
Chaoyi Wu, Xiaoman Zhang, YaÂ Zhang, Yanfeng Wang, and Weidi Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.8.1" style="font-size:90%;">Medklip: Medical knowledge enhanced language-image pre-training in radiology.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.9.1" style="font-size:90%;">arXiv preprint arXiv:2301.02228</em><span class="ltx_text" id="bib.bib76.10.2" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="font-size:90%;">Wu etÂ al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="font-size:90%;">
Chaoyi Wu, Xiaoman Zhang, YaÂ Zhang, Yanfeng Wang, and Weidi Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.8.1" style="font-size:90%;">Towards generalist foundation model for radiology.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.02463</em><span class="ltx_text" id="bib.bib77.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.5.5.1" style="font-size:90%;">Zhang etÂ al. [2022b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.7.1" style="font-size:90%;">
Yuhao Zhang, Hang Jiang, Yasuhide Miura, ChristopherÂ D Manning, and CurtisÂ P Langlotz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.8.1" style="font-size:90%;">Contrastive learning of medical visual representations from paired images and text.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib78.10.2" style="font-size:90%;">Machine Learning for Healthcare Conference</em><span class="ltx_text" id="bib.bib78.11.3" style="font-size:90%;">, pages 2â€“25. PMLR, 2022b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="font-size:90%;">Zhang etÂ al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="font-size:90%;">
Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, YaÂ Zhang, Yanfeng Wang, and Weidi Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.8.1" style="font-size:90%;">Pmc-vqa: Visual instruction tuning for medical visual question answering.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.10415</em><span class="ltx_text" id="bib.bib79.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.5.5.1" style="font-size:90%;">Bannur etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.7.1" style="font-size:90%;">
Shruthi Bannur, Kenza Bouzid, DanielÂ C Castro, Anton Schwaighofer, Sam Bond-Taylor, Maximilian Ilse, Fernando PÃ©rez-GarcÃ­a, Valentina Salvatelli, Harshita Sharma, Felix Meissen, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.8.1" style="font-size:90%;">Maira-2: Grounded radiology report generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.9.1" style="font-size:90%;">arXiv preprint arXiv:2406.04449</em><span class="ltx_text" id="bib.bib80.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib81.5.5.1" style="font-size:90%;">Huang etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.7.1" style="font-size:90%;">
Zhongzhen Huang, Xiaofan Zhang, and Shaoting Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.8.1" style="font-size:90%;">Kiut: Knowledge-injected u-transformer for radiology report generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib81.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib81.11.3" style="font-size:90%;">, pages 19809â€“19818, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib82.5.5.1" style="font-size:90%;">Hyland etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.7.1" style="font-size:90%;">
StephanieÂ L Hyland, Shruthi Bannur, Kenza Bouzid, DanielÂ C Castro, Mercy Ranjit, Anton Schwaighofer, Fernando PÃ©rez-GarcÃ­a, Valentina Salvatelli, Shaury Srivastav, Anja Thieme, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.8.1" style="font-size:90%;">Maira-1: A specialised large multimodal model for radiology report generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.13668</em><span class="ltx_text" id="bib.bib82.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib83.5.5.1" style="font-size:90%;">Lei etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.7.1" style="font-size:90%;">
Jiayu Lei, Xiaoman Zhang, Chaoyi Wu, Lisong Dai, YaÂ Zhang, Yanyong Zhang, Yanfeng Wang, Weidi Xie, and Yuehua Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.8.1" style="font-size:90%;">Autorg-brain: Grounded report generation for brain mri.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.9.1" style="font-size:90%;">arXiv preprint arXiv:2407.16684</em><span class="ltx_text" id="bib.bib83.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib84.5.5.1" style="font-size:90%;">Li etÂ al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.7.1" style="font-size:90%;">
Mingjie Li, Bingqian Lin, Zicong Chen, Haokun Lin, Xiaodan Liang, and Xiaojun Chang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.8.1" style="font-size:90%;">Dynamic graph enhanced contrastive learning for chest x-ray report generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib84.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib84.11.3" style="font-size:90%;">, pages 3334â€“3343, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib85.5.5.1" style="font-size:90%;">Liu etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.7.1" style="font-size:90%;">
Guanxiong Liu, Tzu-MingÂ Harry Hsu, Matthew McDermott, Willie Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.8.1" style="font-size:90%;">Clinically accurate chest x-ray report generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib85.10.2" style="font-size:90%;">Machine Learning for Healthcare Conference</em><span class="ltx_text" id="bib.bib85.11.3" style="font-size:90%;">, pages 249â€“269. PMLR, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib86.5.5.1" style="font-size:90%;">Mohsan etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.7.1" style="font-size:90%;">
MashoodÂ Mohammad Mohsan, MuhammadÂ Usman Akram, Ghulam Rasool, NorahÂ Saleh Alghamdi, Muhammad AbdullahÂ Aamer Baqai, and Muhammad Abbas.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.8.1" style="font-size:90%;">Vision transformer and language model based radiology report generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.9.1" style="font-size:90%;">IEEE Access</em><span class="ltx_text" id="bib.bib86.10.2" style="font-size:90%;">, 11:1814â€“1824, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib87.5.5.1" style="font-size:90%;">Pellegrini etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.7.1" style="font-size:90%;">
Chantal Pellegrini, Ege Ã–zsoy, Benjamin Busam, Nassir Navab, and Matthias Keicher.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.8.1" style="font-size:90%;">Radialog: A large vision-language model for radiology report generation and conversational assistance.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.18681</em><span class="ltx_text" id="bib.bib87.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib88.5.5.1" style="font-size:90%;">Wang etÂ al. [2022b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.7.1" style="font-size:90%;">
Jun Wang, Abhir Bhalerao, and Yulan He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.8.1" style="font-size:90%;">Cross-modal prototype driven network for radiology report generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib88.10.2" style="font-size:90%;">European Conference on Computer Vision</em><span class="ltx_text" id="bib.bib88.11.3" style="font-size:90%;">, pages 563â€“579. Springer, 2022b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib89.5.5.1" style="font-size:90%;">Wang etÂ al. [2023a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.7.1" style="font-size:90%;">
Zhanyu Wang, Lingqiao Liu, Lei Wang, and Luping Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.8.1" style="font-size:90%;">R2gengpt: Radiology report generation with frozen llms.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.9.1" style="font-size:90%;">Meta-Radiology</em><span class="ltx_text" id="bib.bib89.10.2" style="font-size:90%;">, 1(3):100033, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib90.5.5.1" style="font-size:90%;">Wang etÂ al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.7.1" style="font-size:90%;">
Zhanyu Wang, Lingqiao Liu, Lei Wang, and Luping Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.8.1" style="font-size:90%;">Metransformer: Radiology report generation by transformer with multiple learnable expert tokens.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib90.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib90.11.3" style="font-size:90%;">, pages 11558â€“11567, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib91.5.5.1" style="font-size:90%;">Liu etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.7.1" style="font-size:90%;">
Che Liu, Cheng Ouyang, Yinda Chen, CesarÂ CÃ©sar QuilodrÃ¡n-Casas, Lei Ma, Jie Fu, Yike Guo, Anand Shah, Wenjia Bai, and Rossella Arcucci.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.8.1" style="font-size:90%;">T3d: Towards 3d medical image understanding through vision-language pre-training.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.01529</em><span class="ltx_text" id="bib.bib91.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib92.5.5.1" style="font-size:90%;">Zhou etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.7.1" style="font-size:90%;">
Hong-Yu Zhou, Subathra Adithan, JuliÃ¡nÂ NicolÃ¡s Acosta, EricÂ J Topol, and Pranav Rajpurkar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.8.1" style="font-size:90%;">A generalist learner for multifaceted medical image interpretation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.9.1" style="font-size:90%;">arXiv preprint arXiv:2405.07988</em><span class="ltx_text" id="bib.bib92.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib93.5.5.1" style="font-size:90%;">Li etÂ al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.7.1" style="font-size:90%;">
Zihan Li, Yunxiang Li, Qingde Li, Puyang Wang, Dazhou Guo, LeÂ Lu, Dakai Jin, You Zhang, and Qingqi Hong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.8.1" style="font-size:90%;">Lvit: language meets vision transformer in medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.9.1" style="font-size:90%;">IEEE transactions on medical imaging</em><span class="ltx_text" id="bib.bib93.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib94.5.5.1" style="font-size:90%;">Zhao etÂ al. [2023b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.7.1" style="font-size:90%;">
Ziheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, YaÂ Zhang, Yanfeng Wang, and Weidi Xie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.8.1" style="font-size:90%;">One model to rule them all: Towards universal segmentation for medical images with text prompts.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.17183</em><span class="ltx_text" id="bib.bib94.10.2" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib95.5.5.1" style="font-size:90%;">Zhao etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.7.1" style="font-size:90%;">
Theodore Zhao, YuÂ Gu, Jianwei Yang, Naoto Usuyama, HoÂ Hin Lee, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Brian Piening, Carlo Bifulco, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.8.1" style="font-size:90%;">Biomedparse: a biomedical foundation model for image parsing of everything everywhere all at once.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.9.1" style="font-size:90%;">arXiv preprint arXiv:2405.12971</em><span class="ltx_text" id="bib.bib95.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib96.5.5.1" style="font-size:90%;">Bluethgen etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.7.1" style="font-size:90%;">
Christian Bluethgen, Pierre Chambon, Jean-Benoit Delbrouck, Rogier vanÂ der Sluijs, MaÅ‚gorzata PoÅ‚acin, JuanÂ Manuel ZambranoÂ Chaves, TanishqÂ Mathew Abraham, Shivanshu Purohit, CurtisÂ P Langlotz, and AkshayÂ S Chaudhari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.8.1" style="font-size:90%;">A visionâ€“language foundation model for the generation of realistic chest x-ray images.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.9.1" style="font-size:90%;">Nature Biomedical Engineering</em><span class="ltx_text" id="bib.bib96.10.2" style="font-size:90%;">, pages 1â€“13, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib97.5.5.1" style="font-size:90%;">Chambon etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.7.1" style="font-size:90%;">
Pierre Chambon, Christian Bluethgen, Jean-Benoit Delbrouck, Rogier VanÂ der Sluijs, MaÅ‚gorzata PoÅ‚acin, Juan ManuelÂ Zambrano Chaves, TanishqÂ Mathew Abraham, Shivanshu Purohit, CurtisÂ P Langlotz, and Akshay Chaudhari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.8.1" style="font-size:90%;">Roentgen: vision-language foundation model for chest x-ray generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.9.1" style="font-size:90%;">arXiv preprint arXiv:2211.12737</em><span class="ltx_text" id="bib.bib97.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib98.5.5.1" style="font-size:90%;">Kim etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.7.1" style="font-size:90%;">
Kyuri Kim, Yoonho Na, Sung-Joon Ye, Jimin Lee, SungÂ Soo Ahn, JiÂ Eun Park, and Hwiyoung Kim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.8.1" style="font-size:90%;">Controllable text-to-image synthesis for multi-modality mr images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib98.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em><span class="ltx_text" id="bib.bib98.11.3" style="font-size:90%;">, pages 7936â€“7945, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib99.5.5.1" style="font-size:90%;">Bhalodia etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.7.1" style="font-size:90%;">
Riddhish Bhalodia, Ali Hatamizadeh, Leo Tam, Ziyue Xu, Xiaosong Wang, Evrim Turkbey, and Daguang Xu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.8.1" style="font-size:90%;">Improving pneumonia localization via cross-attention on medical images and reports.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib99.10.2" style="font-size:90%;">Medical Image Computing and Computer Assisted Interventionâ€“MICCAI 2021: 24th International Conference, Strasbourg, France, September 27â€“October 1, 2021, Proceedings, Part II 24</em><span class="ltx_text" id="bib.bib99.11.3" style="font-size:90%;">, pages 571â€“581. Springer, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib100.5.5.1" style="font-size:90%;">Chen etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib100.7.1" style="font-size:90%;">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde DeÂ Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib100.8.1" style="font-size:90%;">Evaluating large language models trained on code.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.9.1" style="font-size:90%;">arXiv preprint arXiv:2107.03374</em><span class="ltx_text" id="bib.bib100.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib101.5.5.1" style="font-size:90%;">Li etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib101.7.1" style="font-size:90%;">
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin DalÂ Lago, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib101.8.1" style="font-size:90%;">Competition-level code generation with alphacode.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.9.1" style="font-size:90%;">Science</em><span class="ltx_text" id="bib.bib101.10.2" style="font-size:90%;">, 378(6624):1092â€“1097, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib102.5.5.1" style="font-size:90%;">Patil etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib102.7.1" style="font-size:90%;">
ShishirÂ G Patil, Tianjun Zhang, Xin Wang, and JosephÂ E Gonzalez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib102.8.1" style="font-size:90%;">Gorilla: Large language model connected with massive apis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.15334</em><span class="ltx_text" id="bib.bib102.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib103.5.5.1" style="font-size:90%;">Wei etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib103.7.1" style="font-size:90%;">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, EdÂ Chi, QuocÂ V Le, Denny Zhou, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib103.8.1" style="font-size:90%;">Chain-of-thought prompting elicits reasoning in large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib103.10.2" style="font-size:90%;">, 35:24824â€“24837, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib104.5.5.1" style="font-size:90%;">Ruan etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.7.1" style="font-size:90%;">
Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.8.1" style="font-size:90%;">Tptu: Task planning and tool usage of large language model-based ai agents.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib104.10.2" style="font-size:90%;">NeurIPS 2023 Foundation Models for Decision Making Workshop</em><span class="ltx_text" id="bib.bib104.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib105.5.5.1" style="font-size:90%;">Gou etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib105.7.1" style="font-size:90%;">
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib105.8.1" style="font-size:90%;">Tora: A tool-integrated reasoning agent for mathematical problem solving.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.9.1" style="font-size:90%;">arXiv preprint arXiv:2309.17452</em><span class="ltx_text" id="bib.bib105.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib106.4.4.1" style="font-size:90%;">Gupta and Kembhavi [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.6.1" style="font-size:90%;">
Tanmay Gupta and Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.7.1" style="font-size:90%;">Visual programming: Compositional visual reasoning without training.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.8.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib106.9.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib106.10.3" style="font-size:90%;">, pages 14953â€“14962, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib107.5.5.1" style="font-size:90%;">Subramanian etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib107.7.1" style="font-size:90%;">
Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, and Dan Klein.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib107.8.1" style="font-size:90%;">Modular visual question answering via code generation.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.05392</em><span class="ltx_text" id="bib.bib107.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib108.5.5.1" style="font-size:90%;">SurÃ­s etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib108.7.1" style="font-size:90%;">
DÃ­dac SurÃ­s, Sachit Menon, and Carl Vondrick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib108.8.1" style="font-size:90%;">Vipergpt: Visual inference via python execution for reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib108.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib108.10.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib108.11.3" style="font-size:90%;">, pages 11888â€“11898, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib109.5.5.1" style="font-size:90%;">Yang etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib109.7.1" style="font-size:90%;">
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, CeÂ Liu, Michael Zeng, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib109.8.1" style="font-size:90%;">Mm-react: Prompting chatgpt for multimodal reasoning and action.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.9.1" style="font-size:90%;">arXiv preprint arXiv:2303.11381</em><span class="ltx_text" id="bib.bib109.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib110.5.5.1" style="font-size:90%;">Song etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib110.7.1" style="font-size:90%;">
Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, KeÂ Wang, Rong Yao, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib110.8.1" style="font-size:90%;">Restgpt: Connecting large language models with real-world restful apis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.06624</em><span class="ltx_text" id="bib.bib110.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib111.5.5.1" style="font-size:90%;">Yao etÂ al. [2022a]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib111.7.1" style="font-size:90%;">
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib111.8.1" style="font-size:90%;">Webshop: Towards scalable real-world web interaction with grounded language agents.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib111.10.2" style="font-size:90%;">, 35:20744â€“20757, 2022a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib112.5.5.1" style="font-size:90%;">Deng etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib112.7.1" style="font-size:90%;">
Xiang Deng, YuÂ Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and YuÂ Su.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib112.8.1" style="font-size:90%;">Mind2web: Towards a generalist agent for the web.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib112.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib113.5.5.1" style="font-size:90%;">Bran etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib113.7.1" style="font-size:90%;">
AndresÂ M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, AndrewÂ D White, and Philippe Schwaller.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib113.8.1" style="font-size:90%;">Chemcrow: Augmenting large-language models with chemistry tools.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.05376</em><span class="ltx_text" id="bib.bib113.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib114.5.5.1" style="font-size:90%;">Boiko etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib114.7.1" style="font-size:90%;">
DaniilÂ A Boiko, Robert MacKnight, and Gabe Gomes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib114.8.1" style="font-size:90%;">Emergent autonomous scientific research capabilities of large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.05332</em><span class="ltx_text" id="bib.bib114.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib115.5.5.1" style="font-size:90%;">Liang etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib115.7.1" style="font-size:90%;">
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, YuÂ Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib115.8.1" style="font-size:90%;">Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.9.1" style="font-size:90%;">Intelligent Computing</em><span class="ltx_text" id="bib.bib115.10.2" style="font-size:90%;">, 3:0063, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib116.5.5.1" style="font-size:90%;">Qin etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib116.7.1" style="font-size:90%;">
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib116.8.1" style="font-size:90%;">Toolllm: Facilitating large language models to master 16000+ real-world apis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.9.1" style="font-size:90%;">arXiv preprint arXiv:2307.16789</em><span class="ltx_text" id="bib.bib116.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib117.5.5.1" style="font-size:90%;">Huang etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib117.7.1" style="font-size:90%;">
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib117.8.1" style="font-size:90%;">Inner monologue: Embodied reasoning through planning with language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.9.1" style="font-size:90%;">arXiv preprint arXiv:2207.05608</em><span class="ltx_text" id="bib.bib117.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib118.5.5.1" style="font-size:90%;">Rana etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib118.7.1" style="font-size:90%;">
Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib118.8.1" style="font-size:90%;">Sayplan: Grounding large language models using 3d scene graphs for scalable task planning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.9.1" style="font-size:90%;">arXiv preprint arXiv:2307.06135</em><span class="ltx_text" id="bib.bib118.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib119.5.5.1" style="font-size:90%;">Wang etÂ al. [2023c]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib119.7.1" style="font-size:90%;">
Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib119.8.1" style="font-size:90%;">Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.9.1" style="font-size:90%;">arXiv preprint arXiv:2302.01560</em><span class="ltx_text" id="bib.bib119.10.2" style="font-size:90%;">, 2023c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib120.5.5.1" style="font-size:90%;">Wang etÂ al. [2023d]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib120.7.1" style="font-size:90%;">
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib120.8.1" style="font-size:90%;">Voyager: An open-ended embodied agent with large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.16291</em><span class="ltx_text" id="bib.bib120.10.2" style="font-size:90%;">, 2023d.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib121.5.5.1" style="font-size:90%;">Yao etÂ al. [2022b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib121.7.1" style="font-size:90%;">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib121.8.1" style="font-size:90%;">React: Synergizing reasoning and acting in language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib121.9.1" style="font-size:90%;">arXiv preprint arXiv:2210.03629</em><span class="ltx_text" id="bib.bib121.10.2" style="font-size:90%;">, 2022b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib122.5.5.1" style="font-size:90%;">Zhu etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib122.7.1" style="font-size:90%;">
Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib122.8.1" style="font-size:90%;">Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib122.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.17144</em><span class="ltx_text" id="bib.bib122.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib123.5.5.1" style="font-size:90%;">Li etÂ al. [2024b]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib123.7.1" style="font-size:90%;">
Binxu Li, Tiankai Yan, Yuanting Pan, Zhe Xu, Jie Luo, Ruiyang Ji, Shilong Liu, Haoyu Dong, Zihao Lin, and Yixin Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib123.8.1" style="font-size:90%;">Mmedagent: Learning to use medical tools with multi-modal agent.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.9.1" style="font-size:90%;">arXiv preprint arXiv:2407.02483</em><span class="ltx_text" id="bib.bib123.10.2" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib124.5.5.1" style="font-size:90%;">Milletari etÂ al. [2016]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib124.7.1" style="font-size:90%;">
Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib124.8.1" style="font-size:90%;">V-net: Fully convolutional neural networks for volumetric medical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib124.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib124.10.2" style="font-size:90%;">2016 fourth international conference on 3D vision (3DV)</em><span class="ltx_text" id="bib.bib124.11.3" style="font-size:90%;">, pages 565â€“571. Ieee, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib125.5.5.1" style="font-size:90%;">Paszke etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib125.7.1" style="font-size:90%;">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib125.8.1" style="font-size:90%;">Pytorch: An imperative style, high-performance deep learning library.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib125.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib125.10.2" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib126.5.5.1" style="font-size:90%;">Touvron etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib126.7.1" style="font-size:90%;">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib126.8.1" style="font-size:90%;">Llama 2: Open foundation and fine-tuned chat models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.9.1" style="font-size:90%;">arXiv preprint arXiv:2307.09288</em><span class="ltx_text" id="bib.bib126.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib127.5.5.1" style="font-size:90%;">Wolf etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib127.7.1" style="font-size:90%;">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib127.8.1" style="font-size:90%;">Huggingfaceâ€™s transformers: State-of-the-art natural language processing.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib127.9.1" style="font-size:90%;">arXiv preprint arXiv:1910.03771</em><span class="ltx_text" id="bib.bib127.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib128.5.5.1" style="font-size:90%;">Ronneberger etÂ al. [2015]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib128.7.1" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib128.8.1" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib128.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib128.10.2" style="font-size:90%;">Medical image computing and computer-assisted interventionâ€“MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em><span class="ltx_text" id="bib.bib128.11.3" style="font-size:90%;">, pages 234â€“241. Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib129.4.4.1" style="font-size:90%;">Kingma and Ba [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib129.6.1" style="font-size:90%;">
DiederikÂ P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib129.7.1" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.8.1" style="font-size:90%;">arXiv preprint arXiv:1412.6980</em><span class="ltx_text" id="bib.bib129.9.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib130.5.5.1" style="font-size:90%;">Marcus etÂ al. [2007]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib130.7.1" style="font-size:90%;">
DanielÂ S Marcus, TracyÂ H Wang, Jamie Parker, JohnÂ G Csernansky, JohnÂ C Morris, and RandyÂ L Buckner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib130.8.1" style="font-size:90%;">Open access series of imaging studies (oasis): cross-sectional mri data in young, middle aged, nondemented, and demented older adults.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.9.1" style="font-size:90%;">Journal of cognitive neuroscience</em><span class="ltx_text" id="bib.bib130.10.2" style="font-size:90%;">, 19(9):1498â€“1507, 2007.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib131.5.5.1" style="font-size:90%;">LaMontagne etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib131.7.1" style="font-size:90%;">
PamelaÂ J LaMontagne, TammieÂ LS Benzinger, JohnÂ C Morris, Sarah Keefe, Russ Hornbeck, Chengjie Xiong, Elizabeth Grant, Jason Hassenstab, Krista Moulder, AndreiÂ G Vlassenko, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib131.8.1" style="font-size:90%;">Oasis-3: longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and alzheimer disease.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.9.1" style="font-size:90%;">MedRxiv</em><span class="ltx_text" id="bib.bib131.10.2" style="font-size:90%;">, pages 2019â€“12, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib132.5.5.1" style="font-size:90%;">Babayan etÂ al. [2019]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib132.7.1" style="font-size:90%;">
Anahit Babayan, Miray Erbey, Deniz Kumral, JanisÂ D Reinelt, AndreaÂ MF Reiter, Josefin RÃ¶bbig, HÂ Lina Schaare, Marie Uhlig, Alfred Anwander, Pierre-Louis Bazin, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib132.8.1" style="font-size:90%;">A mind-brain-body dataset of mri, eeg, cognition, emotion, and peripheral physiology in young and old adults.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.9.1" style="font-size:90%;">Scientific data</em><span class="ltx_text" id="bib.bib132.10.2" style="font-size:90%;">, 6(1):1â€“21, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib133.5.5.1" style="font-size:90%;">Pinho etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib133.7.1" style="font-size:90%;">
AnaÂ LuÃ­sa Pinho, Alexis Amadon, Torsten Ruest, Murielle Fabre, Elvis Dohmatob, Isabelle Denghien, Chantal Ginisty, SÃ©verine Becuwe-Desmidt, SÃ©verine Roger, Laurence Laurier, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib133.8.1" style="font-size:90%;">Individual brain charting, a high-resolution fmri dataset for cognitive mapping.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.9.1" style="font-size:90%;">Scientific data</em><span class="ltx_text" id="bib.bib133.10.2" style="font-size:90%;">, 5(1):1â€“15, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib134.5.5.1" style="font-size:90%;">MÃ©rida etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib134.7.1" style="font-size:90%;">
InÃ©s MÃ©rida, Julien Jung, Sandrine Bouvard, Didier LeÂ Bars, Sophie Lancelot, Franck Lavenne, Caroline Bouillot, JÃ©rÃ´me RedoutÃ©, Alexander Hammers, and Nicolas Costes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib134.8.1" style="font-size:90%;">Cermep-idb-mrxfdg: A database of 37 normal adult human brain [18f] fdg pet, t1 and flair mri, and ct images available for research.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib134.9.1" style="font-size:90%;">EJNMMI research</em><span class="ltx_text" id="bib.bib134.10.2" style="font-size:90%;">, 11(1):1â€“10, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib135.5.5.1" style="font-size:90%;">Hanke etÂ al. [2014]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib135.7.1" style="font-size:90%;">
Michael Hanke, FlorianÂ J Baumgartner, Pierre Ibe, FalkoÂ R Kaule, Stefan Pollmann, Oliver Speck, Wolf Zinke, and JÃ¶rg Stadler.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib135.8.1" style="font-size:90%;">A high-resolution 7-tesla fmri dataset from complex natural stimulation with an audio movie.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.9.1" style="font-size:90%;">Scientific data</em><span class="ltx_text" id="bib.bib135.10.2" style="font-size:90%;">, 1(1):1â€“18, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib136.5.5.1" style="font-size:90%;">Hoopes etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib136.7.1" style="font-size:90%;">
Andrew Hoopes, JocelynÂ S Mora, AdrianÂ V Dalca, Bruce Fischl, and Malte Hoffmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib136.8.1" style="font-size:90%;">Synthstrip: Skull-stripping for any brain image.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib136.9.1" style="font-size:90%;">NeuroImage</em><span class="ltx_text" id="bib.bib136.10.2" style="font-size:90%;">, 260:119474, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib137.5.5.1" style="font-size:90%;">Adil etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib137.7.1" style="font-size:90%;">
SyedÂ M Adil, Evan Calabrese, LefkoÂ T Charalambous, JamesÂ J Cook, Shervin Rahimpour, AhmetÂ F Atik, GaryÂ P Cofer, BethÂ A Parente, GÂ Allan Johnson, ShivanandÂ P Lad, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib137.8.1" style="font-size:90%;">A high-resolution interactive atlas of the human brainstem using magnetic resonance imaging.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib137.9.1" style="font-size:90%;">Neuroimage</em><span class="ltx_text" id="bib.bib137.10.2" style="font-size:90%;">, 237:118135, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib138.5.5.1" style="font-size:90%;">Pauli etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib138.7.1" style="font-size:90%;">
WolfgangÂ M Pauli, AmandaÂ N Nili, and JÂ Michael Tyszka.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib138.8.1" style="font-size:90%;">A high-resolution probabilistic in vivo atlas of human subcortical brain nuclei.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.9.1" style="font-size:90%;">Scientific data</em><span class="ltx_text" id="bib.bib138.10.2" style="font-size:90%;">, 5(1):1â€“13, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib139.5.5.1" style="font-size:90%;">HernandezÂ Petzsche etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib139.7.1" style="font-size:90%;">
MoritzÂ R HernandezÂ Petzsche, Ezequiel deÂ la Rosa, Uta Hanning, Roland Wiest, Waldo Valenzuela, Mauricio Reyes, Maria Meyer, Sook-Lei Liew, Florian Kofler, Ivan Ezhov, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib139.8.1" style="font-size:90%;">Isles 2022: A multi-center magnetic resonance imaging stroke lesion segmentation dataset.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.9.1" style="font-size:90%;">Scientific data</em><span class="ltx_text" id="bib.bib139.10.2" style="font-size:90%;">, 9(1):762, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib140.5.5.1" style="font-size:90%;">Liew etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib140.7.1" style="font-size:90%;">
Sook-Lei Liew, BethanyÂ P Lo, MirandaÂ R Donnelly, Artemis Zavaliangos-Petropulu, JessicaÂ N Jeong, Giuseppe Barisano, Alexandre Hutton, JuliaÂ P Simon, JuliaÂ M Juliano, Anisha Suri, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib140.8.1" style="font-size:90%;">A large, curated, open-source stroke neuroimaging dataset to improve lesion segmentation algorithms.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.9.1" style="font-size:90%;">Scientific data</em><span class="ltx_text" id="bib.bib140.10.2" style="font-size:90%;">, 9(1):320, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib141.5.5.1" style="font-size:90%;">Hoffmann etÂ al. [2024]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib141.7.1" style="font-size:90%;">
Malte Hoffmann, Andrew Hoopes, DouglasÂ N. Greve, Bruce Fischl, and AdrianÂ V. Dalca.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib141.8.1" style="font-size:90%;">Anatomy-aware and acquisition-agnostic joint registration with SynthMorph.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.9.1" style="font-size:90%;">Imaging Neuroscience</em><span class="ltx_text" id="bib.bib141.10.2" style="font-size:90%;">, 2:1â€“33, 06 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib141.11.1" style="font-size:90%;">ISSN 2837-6056.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib141.12.1" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1162/imag_a_00197</span><span class="ltx_text" id="bib.bib141.13.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib142.4.4.1" style="font-size:90%;">Dice [1945]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib142.6.1" style="font-size:90%;">
LeeÂ R Dice.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib142.7.1" style="font-size:90%;">Measures of the amount of ecologic association between species.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.8.1" style="font-size:90%;">Ecology</em><span class="ltx_text" id="bib.bib142.9.2" style="font-size:90%;">, 26(3):297â€“302, 1945.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib143.5.5.1" style="font-size:90%;">Achiam etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib143.7.1" style="font-size:90%;">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib143.8.1" style="font-size:90%;">Gpt-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib143.9.1" style="font-size:90%;">arXiv preprint arXiv:2303.08774</em><span class="ltx_text" id="bib.bib143.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Appendix</h2>
<section class="ltx_subsection" id="Sx2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Native-Space Convolutions</h3>
<div class="ltx_para" id="Sx2.SS1.p1">
<p class="ltx_p" id="Sx2.SS1.p1.4">Volumetric image formats typically include a world coordinate transform that defines the in-plane voxel spacingÂ (<math alttext="s^{inp}" class="ltx_Math" display="inline" id="Sx2.SS1.p1.1.m1.1"><semantics id="Sx2.SS1.p1.1.m1.1a"><msup id="Sx2.SS1.p1.1.m1.1.1" xref="Sx2.SS1.p1.1.m1.1.1.cmml"><mi id="Sx2.SS1.p1.1.m1.1.1.2" xref="Sx2.SS1.p1.1.m1.1.1.2.cmml">s</mi><mrow id="Sx2.SS1.p1.1.m1.1.1.3" xref="Sx2.SS1.p1.1.m1.1.1.3.cmml"><mi id="Sx2.SS1.p1.1.m1.1.1.3.2" xref="Sx2.SS1.p1.1.m1.1.1.3.2.cmml">i</mi><mo id="Sx2.SS1.p1.1.m1.1.1.3.1" xref="Sx2.SS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="Sx2.SS1.p1.1.m1.1.1.3.3" xref="Sx2.SS1.p1.1.m1.1.1.3.3.cmml">n</mi><mo id="Sx2.SS1.p1.1.m1.1.1.3.1a" xref="Sx2.SS1.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="Sx2.SS1.p1.1.m1.1.1.3.4" xref="Sx2.SS1.p1.1.m1.1.1.3.4.cmml">p</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="Sx2.SS1.p1.1.m1.1b"><apply id="Sx2.SS1.p1.1.m1.1.1.cmml" xref="Sx2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx2.SS1.p1.1.m1.1.1.1.cmml" xref="Sx2.SS1.p1.1.m1.1.1">superscript</csymbol><ci id="Sx2.SS1.p1.1.m1.1.1.2.cmml" xref="Sx2.SS1.p1.1.m1.1.1.2">ğ‘ </ci><apply id="Sx2.SS1.p1.1.m1.1.1.3.cmml" xref="Sx2.SS1.p1.1.m1.1.1.3"><times id="Sx2.SS1.p1.1.m1.1.1.3.1.cmml" xref="Sx2.SS1.p1.1.m1.1.1.3.1"></times><ci id="Sx2.SS1.p1.1.m1.1.1.3.2.cmml" xref="Sx2.SS1.p1.1.m1.1.1.3.2">ğ‘–</ci><ci id="Sx2.SS1.p1.1.m1.1.1.3.3.cmml" xref="Sx2.SS1.p1.1.m1.1.1.3.3">ğ‘›</ci><ci id="Sx2.SS1.p1.1.m1.1.1.3.4.cmml" xref="Sx2.SS1.p1.1.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS1.p1.1.m1.1c">s^{inp}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS1.p1.1.m1.1d">italic_s start_POSTSUPERSCRIPT italic_i italic_n italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>) and the spacing between slicesÂ (<math alttext="s^{sep}" class="ltx_Math" display="inline" id="Sx2.SS1.p1.2.m2.1"><semantics id="Sx2.SS1.p1.2.m2.1a"><msup id="Sx2.SS1.p1.2.m2.1.1" xref="Sx2.SS1.p1.2.m2.1.1.cmml"><mi id="Sx2.SS1.p1.2.m2.1.1.2" xref="Sx2.SS1.p1.2.m2.1.1.2.cmml">s</mi><mrow id="Sx2.SS1.p1.2.m2.1.1.3" xref="Sx2.SS1.p1.2.m2.1.1.3.cmml"><mi id="Sx2.SS1.p1.2.m2.1.1.3.2" xref="Sx2.SS1.p1.2.m2.1.1.3.2.cmml">s</mi><mo id="Sx2.SS1.p1.2.m2.1.1.3.1" xref="Sx2.SS1.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="Sx2.SS1.p1.2.m2.1.1.3.3" xref="Sx2.SS1.p1.2.m2.1.1.3.3.cmml">e</mi><mo id="Sx2.SS1.p1.2.m2.1.1.3.1a" xref="Sx2.SS1.p1.2.m2.1.1.3.1.cmml">â¢</mo><mi id="Sx2.SS1.p1.2.m2.1.1.3.4" xref="Sx2.SS1.p1.2.m2.1.1.3.4.cmml">p</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="Sx2.SS1.p1.2.m2.1b"><apply id="Sx2.SS1.p1.2.m2.1.1.cmml" xref="Sx2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Sx2.SS1.p1.2.m2.1.1.1.cmml" xref="Sx2.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="Sx2.SS1.p1.2.m2.1.1.2.cmml" xref="Sx2.SS1.p1.2.m2.1.1.2">ğ‘ </ci><apply id="Sx2.SS1.p1.2.m2.1.1.3.cmml" xref="Sx2.SS1.p1.2.m2.1.1.3"><times id="Sx2.SS1.p1.2.m2.1.1.3.1.cmml" xref="Sx2.SS1.p1.2.m2.1.1.3.1"></times><ci id="Sx2.SS1.p1.2.m2.1.1.3.2.cmml" xref="Sx2.SS1.p1.2.m2.1.1.3.2">ğ‘ </ci><ci id="Sx2.SS1.p1.2.m2.1.1.3.3.cmml" xref="Sx2.SS1.p1.2.m2.1.1.3.3">ğ‘’</ci><ci id="Sx2.SS1.p1.2.m2.1.1.3.4.cmml" xref="Sx2.SS1.p1.2.m2.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS1.p1.2.m2.1c">s^{sep}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS1.p1.2.m2.1d">italic_s start_POSTSUPERSCRIPT italic_s italic_e italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>). We define the relative slice-to-in-plane spacing asÂ <math alttext="\omega=s^{sep}/s^{inp}" class="ltx_Math" display="inline" id="Sx2.SS1.p1.3.m3.1"><semantics id="Sx2.SS1.p1.3.m3.1a"><mrow id="Sx2.SS1.p1.3.m3.1.1" xref="Sx2.SS1.p1.3.m3.1.1.cmml"><mi id="Sx2.SS1.p1.3.m3.1.1.2" xref="Sx2.SS1.p1.3.m3.1.1.2.cmml">Ï‰</mi><mo id="Sx2.SS1.p1.3.m3.1.1.1" xref="Sx2.SS1.p1.3.m3.1.1.1.cmml">=</mo><mrow id="Sx2.SS1.p1.3.m3.1.1.3" xref="Sx2.SS1.p1.3.m3.1.1.3.cmml"><msup id="Sx2.SS1.p1.3.m3.1.1.3.2" xref="Sx2.SS1.p1.3.m3.1.1.3.2.cmml"><mi id="Sx2.SS1.p1.3.m3.1.1.3.2.2" xref="Sx2.SS1.p1.3.m3.1.1.3.2.2.cmml">s</mi><mrow id="Sx2.SS1.p1.3.m3.1.1.3.2.3" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.cmml"><mi id="Sx2.SS1.p1.3.m3.1.1.3.2.3.2" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.2.cmml">s</mi><mo id="Sx2.SS1.p1.3.m3.1.1.3.2.3.1" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.1.cmml">â¢</mo><mi id="Sx2.SS1.p1.3.m3.1.1.3.2.3.3" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.3.cmml">e</mi><mo id="Sx2.SS1.p1.3.m3.1.1.3.2.3.1a" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.1.cmml">â¢</mo><mi id="Sx2.SS1.p1.3.m3.1.1.3.2.3.4" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.4.cmml">p</mi></mrow></msup><mo id="Sx2.SS1.p1.3.m3.1.1.3.1" xref="Sx2.SS1.p1.3.m3.1.1.3.1.cmml">/</mo><msup id="Sx2.SS1.p1.3.m3.1.1.3.3" xref="Sx2.SS1.p1.3.m3.1.1.3.3.cmml"><mi id="Sx2.SS1.p1.3.m3.1.1.3.3.2" xref="Sx2.SS1.p1.3.m3.1.1.3.3.2.cmml">s</mi><mrow id="Sx2.SS1.p1.3.m3.1.1.3.3.3" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.cmml"><mi id="Sx2.SS1.p1.3.m3.1.1.3.3.3.2" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.2.cmml">i</mi><mo id="Sx2.SS1.p1.3.m3.1.1.3.3.3.1" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.1.cmml">â¢</mo><mi id="Sx2.SS1.p1.3.m3.1.1.3.3.3.3" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.3.cmml">n</mi><mo id="Sx2.SS1.p1.3.m3.1.1.3.3.3.1a" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.1.cmml">â¢</mo><mi id="Sx2.SS1.p1.3.m3.1.1.3.3.3.4" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.4.cmml">p</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS1.p1.3.m3.1b"><apply id="Sx2.SS1.p1.3.m3.1.1.cmml" xref="Sx2.SS1.p1.3.m3.1.1"><eq id="Sx2.SS1.p1.3.m3.1.1.1.cmml" xref="Sx2.SS1.p1.3.m3.1.1.1"></eq><ci id="Sx2.SS1.p1.3.m3.1.1.2.cmml" xref="Sx2.SS1.p1.3.m3.1.1.2">ğœ”</ci><apply id="Sx2.SS1.p1.3.m3.1.1.3.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3"><divide id="Sx2.SS1.p1.3.m3.1.1.3.1.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.1"></divide><apply id="Sx2.SS1.p1.3.m3.1.1.3.2.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.2"><csymbol cd="ambiguous" id="Sx2.SS1.p1.3.m3.1.1.3.2.1.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.2">superscript</csymbol><ci id="Sx2.SS1.p1.3.m3.1.1.3.2.2.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.2.2">ğ‘ </ci><apply id="Sx2.SS1.p1.3.m3.1.1.3.2.3.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3"><times id="Sx2.SS1.p1.3.m3.1.1.3.2.3.1.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.1"></times><ci id="Sx2.SS1.p1.3.m3.1.1.3.2.3.2.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.2">ğ‘ </ci><ci id="Sx2.SS1.p1.3.m3.1.1.3.2.3.3.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.3">ğ‘’</ci><ci id="Sx2.SS1.p1.3.m3.1.1.3.2.3.4.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.2.3.4">ğ‘</ci></apply></apply><apply id="Sx2.SS1.p1.3.m3.1.1.3.3.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="Sx2.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.3">superscript</csymbol><ci id="Sx2.SS1.p1.3.m3.1.1.3.3.2.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.3.2">ğ‘ </ci><apply id="Sx2.SS1.p1.3.m3.1.1.3.3.3.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3"><times id="Sx2.SS1.p1.3.m3.1.1.3.3.3.1.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.1"></times><ci id="Sx2.SS1.p1.3.m3.1.1.3.3.3.2.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.2">ğ‘–</ci><ci id="Sx2.SS1.p1.3.m3.1.1.3.3.3.3.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.3">ğ‘›</ci><ci id="Sx2.SS1.p1.3.m3.1.1.3.3.3.4.cmml" xref="Sx2.SS1.p1.3.m3.1.1.3.3.3.4">ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS1.p1.3.m3.1c">\omega=s^{sep}/s^{inp}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS1.p1.3.m3.1d">italic_Ï‰ = italic_s start_POSTSUPERSCRIPT italic_s italic_e italic_p end_POSTSUPERSCRIPT / italic_s start_POSTSUPERSCRIPT italic_i italic_n italic_p end_POSTSUPERSCRIPT</annotation></semantics></math>. Standard image processing methods resample inputs to a common, often isotropic resolution (i.e.,Â <math alttext="\omega=1" class="ltx_Math" display="inline" id="Sx2.SS1.p1.4.m4.1"><semantics id="Sx2.SS1.p1.4.m4.1a"><mrow id="Sx2.SS1.p1.4.m4.1.1" xref="Sx2.SS1.p1.4.m4.1.1.cmml"><mi id="Sx2.SS1.p1.4.m4.1.1.2" xref="Sx2.SS1.p1.4.m4.1.1.2.cmml">Ï‰</mi><mo id="Sx2.SS1.p1.4.m4.1.1.1" xref="Sx2.SS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="Sx2.SS1.p1.4.m4.1.1.3" xref="Sx2.SS1.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS1.p1.4.m4.1b"><apply id="Sx2.SS1.p1.4.m4.1.1.cmml" xref="Sx2.SS1.p1.4.m4.1.1"><eq id="Sx2.SS1.p1.4.m4.1.1.1.cmml" xref="Sx2.SS1.p1.4.m4.1.1.1"></eq><ci id="Sx2.SS1.p1.4.m4.1.1.2.cmml" xref="Sx2.SS1.p1.4.m4.1.1.2">ğœ”</ci><cn id="Sx2.SS1.p1.4.m4.1.1.3.cmml" type="integer" xref="Sx2.SS1.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS1.p1.4.m4.1c">\omega=1</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS1.p1.4.m4.1d">italic_Ï‰ = 1</annotation></semantics></math>). However, this can remove signal of high-resolution scans or unnecessarily increase data density of thick-slice acquisitions.</p>
</div>
<div class="ltx_para" id="Sx2.SS1.p2">
<p class="ltx_p" id="Sx2.SS1.p2.1">To address this, we implement resolution-agnostic convolutional layers that process volumes in their native voxel spacing. Throughout the multi-scale vision network, we track and recompute voxel spacings. In the downsampling operation following resolution level <math alttext="n" class="ltx_Math" display="inline" id="Sx2.SS1.p2.1.m1.1"><semantics id="Sx2.SS1.p2.1.m1.1a"><mi id="Sx2.SS1.p2.1.m1.1.1" xref="Sx2.SS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="Sx2.SS1.p2.1.m1.1b"><ci id="Sx2.SS1.p2.1.m1.1.1.cmml" xref="Sx2.SS1.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS1.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS1.p2.1.m1.1d">italic_n</annotation></semantics></math>, images are resampled to the following spacing:</p>
<table class="ltx_equation ltx_eqn_table" id="Sx2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s_{n+1}^{inp}=2~{}s_{n}^{inp},~{}~{}s_{n+1}^{sep}=\begin{cases}s^{sep}&amp;\text{%
if }\omega_{n}&gt;2,\\
s_{n+1}^{inp}&amp;\text{otherwise.}\end{cases}" class="ltx_Math" display="block" id="Sx2.Ex1.m1.6"><semantics id="Sx2.Ex1.m1.6a"><mrow id="Sx2.Ex1.m1.6.6.2" xref="Sx2.Ex1.m1.6.6.3.cmml"><mrow id="Sx2.Ex1.m1.5.5.1.1" xref="Sx2.Ex1.m1.5.5.1.1.cmml"><msubsup id="Sx2.Ex1.m1.5.5.1.1.2" xref="Sx2.Ex1.m1.5.5.1.1.2.cmml"><mi id="Sx2.Ex1.m1.5.5.1.1.2.2.2" xref="Sx2.Ex1.m1.5.5.1.1.2.2.2.cmml">s</mi><mrow id="Sx2.Ex1.m1.5.5.1.1.2.2.3" xref="Sx2.Ex1.m1.5.5.1.1.2.2.3.cmml"><mi id="Sx2.Ex1.m1.5.5.1.1.2.2.3.2" xref="Sx2.Ex1.m1.5.5.1.1.2.2.3.2.cmml">n</mi><mo id="Sx2.Ex1.m1.5.5.1.1.2.2.3.1" xref="Sx2.Ex1.m1.5.5.1.1.2.2.3.1.cmml">+</mo><mn id="Sx2.Ex1.m1.5.5.1.1.2.2.3.3" xref="Sx2.Ex1.m1.5.5.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="Sx2.Ex1.m1.5.5.1.1.2.3" xref="Sx2.Ex1.m1.5.5.1.1.2.3.cmml"><mi id="Sx2.Ex1.m1.5.5.1.1.2.3.2" xref="Sx2.Ex1.m1.5.5.1.1.2.3.2.cmml">i</mi><mo id="Sx2.Ex1.m1.5.5.1.1.2.3.1" xref="Sx2.Ex1.m1.5.5.1.1.2.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.5.5.1.1.2.3.3" xref="Sx2.Ex1.m1.5.5.1.1.2.3.3.cmml">n</mi><mo id="Sx2.Ex1.m1.5.5.1.1.2.3.1a" xref="Sx2.Ex1.m1.5.5.1.1.2.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.5.5.1.1.2.3.4" xref="Sx2.Ex1.m1.5.5.1.1.2.3.4.cmml">p</mi></mrow></msubsup><mo id="Sx2.Ex1.m1.5.5.1.1.1" xref="Sx2.Ex1.m1.5.5.1.1.1.cmml">=</mo><mrow id="Sx2.Ex1.m1.5.5.1.1.3" xref="Sx2.Ex1.m1.5.5.1.1.3.cmml"><mn id="Sx2.Ex1.m1.5.5.1.1.3.2" xref="Sx2.Ex1.m1.5.5.1.1.3.2.cmml">2</mn><mo id="Sx2.Ex1.m1.5.5.1.1.3.1" lspace="0.330em" xref="Sx2.Ex1.m1.5.5.1.1.3.1.cmml">â¢</mo><msubsup id="Sx2.Ex1.m1.5.5.1.1.3.3" xref="Sx2.Ex1.m1.5.5.1.1.3.3.cmml"><mi id="Sx2.Ex1.m1.5.5.1.1.3.3.2.2" xref="Sx2.Ex1.m1.5.5.1.1.3.3.2.2.cmml">s</mi><mi id="Sx2.Ex1.m1.5.5.1.1.3.3.2.3" xref="Sx2.Ex1.m1.5.5.1.1.3.3.2.3.cmml">n</mi><mrow id="Sx2.Ex1.m1.5.5.1.1.3.3.3" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.cmml"><mi id="Sx2.Ex1.m1.5.5.1.1.3.3.3.2" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.2.cmml">i</mi><mo id="Sx2.Ex1.m1.5.5.1.1.3.3.3.1" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.5.5.1.1.3.3.3.3" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.3.cmml">n</mi><mo id="Sx2.Ex1.m1.5.5.1.1.3.3.3.1a" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.5.5.1.1.3.3.3.4" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.4.cmml">p</mi></mrow></msubsup></mrow></mrow><mo id="Sx2.Ex1.m1.6.6.2.3" rspace="0.827em" xref="Sx2.Ex1.m1.6.6.3a.cmml">,</mo><mrow id="Sx2.Ex1.m1.6.6.2.2" xref="Sx2.Ex1.m1.6.6.2.2.cmml"><msubsup id="Sx2.Ex1.m1.6.6.2.2.2" xref="Sx2.Ex1.m1.6.6.2.2.2.cmml"><mi id="Sx2.Ex1.m1.6.6.2.2.2.2.2" xref="Sx2.Ex1.m1.6.6.2.2.2.2.2.cmml">s</mi><mrow id="Sx2.Ex1.m1.6.6.2.2.2.2.3" xref="Sx2.Ex1.m1.6.6.2.2.2.2.3.cmml"><mi id="Sx2.Ex1.m1.6.6.2.2.2.2.3.2" xref="Sx2.Ex1.m1.6.6.2.2.2.2.3.2.cmml">n</mi><mo id="Sx2.Ex1.m1.6.6.2.2.2.2.3.1" xref="Sx2.Ex1.m1.6.6.2.2.2.2.3.1.cmml">+</mo><mn id="Sx2.Ex1.m1.6.6.2.2.2.2.3.3" xref="Sx2.Ex1.m1.6.6.2.2.2.2.3.3.cmml">1</mn></mrow><mrow id="Sx2.Ex1.m1.6.6.2.2.2.3" xref="Sx2.Ex1.m1.6.6.2.2.2.3.cmml"><mi id="Sx2.Ex1.m1.6.6.2.2.2.3.2" xref="Sx2.Ex1.m1.6.6.2.2.2.3.2.cmml">s</mi><mo id="Sx2.Ex1.m1.6.6.2.2.2.3.1" xref="Sx2.Ex1.m1.6.6.2.2.2.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.6.6.2.2.2.3.3" xref="Sx2.Ex1.m1.6.6.2.2.2.3.3.cmml">e</mi><mo id="Sx2.Ex1.m1.6.6.2.2.2.3.1a" xref="Sx2.Ex1.m1.6.6.2.2.2.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.6.6.2.2.2.3.4" xref="Sx2.Ex1.m1.6.6.2.2.2.3.4.cmml">p</mi></mrow></msubsup><mo id="Sx2.Ex1.m1.6.6.2.2.1" xref="Sx2.Ex1.m1.6.6.2.2.1.cmml">=</mo><mrow id="Sx2.Ex1.m1.4.4" xref="Sx2.Ex1.m1.6.6.2.2.3.1.cmml"><mo id="Sx2.Ex1.m1.4.4.5" xref="Sx2.Ex1.m1.6.6.2.2.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="Sx2.Ex1.m1.4.4.4" rowspacing="0pt" xref="Sx2.Ex1.m1.6.6.2.2.3.1.cmml"><mtr id="Sx2.Ex1.m1.4.4.4a" xref="Sx2.Ex1.m1.6.6.2.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="Sx2.Ex1.m1.4.4.4b" xref="Sx2.Ex1.m1.6.6.2.2.3.1.cmml"><msup id="Sx2.Ex1.m1.1.1.1.1.1.1" xref="Sx2.Ex1.m1.1.1.1.1.1.1.cmml"><mi id="Sx2.Ex1.m1.1.1.1.1.1.1.2" xref="Sx2.Ex1.m1.1.1.1.1.1.1.2.cmml">s</mi><mrow id="Sx2.Ex1.m1.1.1.1.1.1.1.3" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.cmml"><mi id="Sx2.Ex1.m1.1.1.1.1.1.1.3.2" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.2.cmml">s</mi><mo id="Sx2.Ex1.m1.1.1.1.1.1.1.3.1" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.1.1.1.1.1.1.3.3" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.3.cmml">e</mi><mo id="Sx2.Ex1.m1.1.1.1.1.1.1.3.1a" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.1.1.1.1.1.1.3.4" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.4.cmml">p</mi></mrow></msup></mtd><mtd class="ltx_align_left" columnalign="left" id="Sx2.Ex1.m1.4.4.4c" xref="Sx2.Ex1.m1.6.6.2.2.3.1.cmml"><mrow id="Sx2.Ex1.m1.2.2.2.2.2.1.1" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.cmml"><mtext id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.2" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.2a.cmml">ifÂ </mtext><mo id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.1" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.1.cmml">â¢</mo><msub id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.cmml"><mi id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.2" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.2.cmml">Ï‰</mi><mi id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.3" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.3.cmml">n</mi></msub></mrow><mo id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.1" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.1.cmml">&gt;</mo><mn id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.3" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.3.cmml">2</mn></mrow><mo id="Sx2.Ex1.m1.2.2.2.2.2.1.1.2" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="Sx2.Ex1.m1.4.4.4d" xref="Sx2.Ex1.m1.6.6.2.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="Sx2.Ex1.m1.4.4.4e" xref="Sx2.Ex1.m1.6.6.2.2.3.1.cmml"><msubsup id="Sx2.Ex1.m1.3.3.3.3.1.1" xref="Sx2.Ex1.m1.3.3.3.3.1.1.cmml"><mi id="Sx2.Ex1.m1.3.3.3.3.1.1.2.2" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.2.cmml">s</mi><mrow id="Sx2.Ex1.m1.3.3.3.3.1.1.2.3" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.cmml"><mi id="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.2" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.2.cmml">n</mi><mo id="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.1" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.1.cmml">+</mo><mn id="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.3" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.3.cmml">1</mn></mrow><mrow id="Sx2.Ex1.m1.3.3.3.3.1.1.3" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.cmml"><mi id="Sx2.Ex1.m1.3.3.3.3.1.1.3.2" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.2.cmml">i</mi><mo id="Sx2.Ex1.m1.3.3.3.3.1.1.3.1" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.3.3.3.3.1.1.3.3" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.3.cmml">n</mi><mo id="Sx2.Ex1.m1.3.3.3.3.1.1.3.1a" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.1.cmml">â¢</mo><mi id="Sx2.Ex1.m1.3.3.3.3.1.1.3.4" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.4.cmml">p</mi></mrow></msubsup></mtd><mtd class="ltx_align_left" columnalign="left" id="Sx2.Ex1.m1.4.4.4f" xref="Sx2.Ex1.m1.6.6.2.2.3.1.cmml"><mtext id="Sx2.Ex1.m1.4.4.4.4.2.1" xref="Sx2.Ex1.m1.4.4.4.4.2.1a.cmml">otherwise.</mtext></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.Ex1.m1.6b"><apply id="Sx2.Ex1.m1.6.6.3.cmml" xref="Sx2.Ex1.m1.6.6.2"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.6.6.3a.cmml" xref="Sx2.Ex1.m1.6.6.2.3">formulae-sequence</csymbol><apply id="Sx2.Ex1.m1.5.5.1.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1"><eq id="Sx2.Ex1.m1.5.5.1.1.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.1"></eq><apply id="Sx2.Ex1.m1.5.5.1.1.2.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.5.5.1.1.2.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2">superscript</csymbol><apply id="Sx2.Ex1.m1.5.5.1.1.2.2.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.5.5.1.1.2.2.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2">subscript</csymbol><ci id="Sx2.Ex1.m1.5.5.1.1.2.2.2.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.2.2">ğ‘ </ci><apply id="Sx2.Ex1.m1.5.5.1.1.2.2.3.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.2.3"><plus id="Sx2.Ex1.m1.5.5.1.1.2.2.3.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.2.3.1"></plus><ci id="Sx2.Ex1.m1.5.5.1.1.2.2.3.2.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.2.3.2">ğ‘›</ci><cn id="Sx2.Ex1.m1.5.5.1.1.2.2.3.3.cmml" type="integer" xref="Sx2.Ex1.m1.5.5.1.1.2.2.3.3">1</cn></apply></apply><apply id="Sx2.Ex1.m1.5.5.1.1.2.3.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.3"><times id="Sx2.Ex1.m1.5.5.1.1.2.3.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.3.1"></times><ci id="Sx2.Ex1.m1.5.5.1.1.2.3.2.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.3.2">ğ‘–</ci><ci id="Sx2.Ex1.m1.5.5.1.1.2.3.3.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.3.3">ğ‘›</ci><ci id="Sx2.Ex1.m1.5.5.1.1.2.3.4.cmml" xref="Sx2.Ex1.m1.5.5.1.1.2.3.4">ğ‘</ci></apply></apply><apply id="Sx2.Ex1.m1.5.5.1.1.3.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3"><times id="Sx2.Ex1.m1.5.5.1.1.3.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.1"></times><cn id="Sx2.Ex1.m1.5.5.1.1.3.2.cmml" type="integer" xref="Sx2.Ex1.m1.5.5.1.1.3.2">2</cn><apply id="Sx2.Ex1.m1.5.5.1.1.3.3.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.5.5.1.1.3.3.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3">superscript</csymbol><apply id="Sx2.Ex1.m1.5.5.1.1.3.3.2.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.5.5.1.1.3.3.2.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3">subscript</csymbol><ci id="Sx2.Ex1.m1.5.5.1.1.3.3.2.2.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3.2.2">ğ‘ </ci><ci id="Sx2.Ex1.m1.5.5.1.1.3.3.2.3.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3.2.3">ğ‘›</ci></apply><apply id="Sx2.Ex1.m1.5.5.1.1.3.3.3.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3"><times id="Sx2.Ex1.m1.5.5.1.1.3.3.3.1.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.1"></times><ci id="Sx2.Ex1.m1.5.5.1.1.3.3.3.2.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.2">ğ‘–</ci><ci id="Sx2.Ex1.m1.5.5.1.1.3.3.3.3.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.3">ğ‘›</ci><ci id="Sx2.Ex1.m1.5.5.1.1.3.3.3.4.cmml" xref="Sx2.Ex1.m1.5.5.1.1.3.3.3.4">ğ‘</ci></apply></apply></apply></apply><apply id="Sx2.Ex1.m1.6.6.2.2.cmml" xref="Sx2.Ex1.m1.6.6.2.2"><eq id="Sx2.Ex1.m1.6.6.2.2.1.cmml" xref="Sx2.Ex1.m1.6.6.2.2.1"></eq><apply id="Sx2.Ex1.m1.6.6.2.2.2.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.6.6.2.2.2.1.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2">superscript</csymbol><apply id="Sx2.Ex1.m1.6.6.2.2.2.2.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.6.6.2.2.2.2.1.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2">subscript</csymbol><ci id="Sx2.Ex1.m1.6.6.2.2.2.2.2.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.2.2">ğ‘ </ci><apply id="Sx2.Ex1.m1.6.6.2.2.2.2.3.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.2.3"><plus id="Sx2.Ex1.m1.6.6.2.2.2.2.3.1.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.2.3.1"></plus><ci id="Sx2.Ex1.m1.6.6.2.2.2.2.3.2.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.2.3.2">ğ‘›</ci><cn id="Sx2.Ex1.m1.6.6.2.2.2.2.3.3.cmml" type="integer" xref="Sx2.Ex1.m1.6.6.2.2.2.2.3.3">1</cn></apply></apply><apply id="Sx2.Ex1.m1.6.6.2.2.2.3.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.3"><times id="Sx2.Ex1.m1.6.6.2.2.2.3.1.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.3.1"></times><ci id="Sx2.Ex1.m1.6.6.2.2.2.3.2.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.3.2">ğ‘ </ci><ci id="Sx2.Ex1.m1.6.6.2.2.2.3.3.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.3.3">ğ‘’</ci><ci id="Sx2.Ex1.m1.6.6.2.2.2.3.4.cmml" xref="Sx2.Ex1.m1.6.6.2.2.2.3.4">ğ‘</ci></apply></apply><apply id="Sx2.Ex1.m1.6.6.2.2.3.1.cmml" xref="Sx2.Ex1.m1.4.4"><csymbol cd="latexml" id="Sx2.Ex1.m1.6.6.2.2.3.1.1.cmml" xref="Sx2.Ex1.m1.4.4.5">cases</csymbol><apply id="Sx2.Ex1.m1.1.1.1.1.1.1.cmml" xref="Sx2.Ex1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="Sx2.Ex1.m1.1.1.1.1.1.1">superscript</csymbol><ci id="Sx2.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="Sx2.Ex1.m1.1.1.1.1.1.1.2">ğ‘ </ci><apply id="Sx2.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3"><times id="Sx2.Ex1.m1.1.1.1.1.1.1.3.1.cmml" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.1"></times><ci id="Sx2.Ex1.m1.1.1.1.1.1.1.3.2.cmml" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.2">ğ‘ </ci><ci id="Sx2.Ex1.m1.1.1.1.1.1.1.3.3.cmml" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.3">ğ‘’</ci><ci id="Sx2.Ex1.m1.1.1.1.1.1.1.3.4.cmml" xref="Sx2.Ex1.m1.1.1.1.1.1.1.3.4">ğ‘</ci></apply></apply><apply id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1"><gt id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.1.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.1"></gt><apply id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2"><times id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.1.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.1"></times><ci id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.2a.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.2"><mtext id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.2.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.2">ifÂ </mtext></ci><apply id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.1.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3">subscript</csymbol><ci id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.2.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.2">ğœ”</ci><ci id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.3.cmml" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.2.3.3">ğ‘›</ci></apply></apply><cn id="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.3.cmml" type="integer" xref="Sx2.Ex1.m1.2.2.2.2.2.1.1.1.3">2</cn></apply><apply id="Sx2.Ex1.m1.3.3.3.3.1.1.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.3.3.3.3.1.1.1.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1">superscript</csymbol><apply id="Sx2.Ex1.m1.3.3.3.3.1.1.2.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1"><csymbol cd="ambiguous" id="Sx2.Ex1.m1.3.3.3.3.1.1.2.1.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1">subscript</csymbol><ci id="Sx2.Ex1.m1.3.3.3.3.1.1.2.2.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.2">ğ‘ </ci><apply id="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.3"><plus id="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.1.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.1"></plus><ci id="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.2.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.2">ğ‘›</ci><cn id="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.3.cmml" type="integer" xref="Sx2.Ex1.m1.3.3.3.3.1.1.2.3.3">1</cn></apply></apply><apply id="Sx2.Ex1.m1.3.3.3.3.1.1.3.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3"><times id="Sx2.Ex1.m1.3.3.3.3.1.1.3.1.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.1"></times><ci id="Sx2.Ex1.m1.3.3.3.3.1.1.3.2.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.2">ğ‘–</ci><ci id="Sx2.Ex1.m1.3.3.3.3.1.1.3.3.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.3">ğ‘›</ci><ci id="Sx2.Ex1.m1.3.3.3.3.1.1.3.4.cmml" xref="Sx2.Ex1.m1.3.3.3.3.1.1.3.4">ğ‘</ci></apply></apply><ci id="Sx2.Ex1.m1.4.4.4.4.2.1a.cmml" xref="Sx2.Ex1.m1.4.4.4.4.2.1"><mtext id="Sx2.Ex1.m1.4.4.4.4.2.1.cmml" xref="Sx2.Ex1.m1.4.4.4.4.2.1">otherwise.</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.Ex1.m1.6c">s_{n+1}^{inp}=2~{}s_{n}^{inp},~{}~{}s_{n+1}^{sep}=\begin{cases}s^{sep}&amp;\text{%
if }\omega_{n}&gt;2,\\
s_{n+1}^{inp}&amp;\text{otherwise.}\end{cases}</annotation><annotation encoding="application/x-llamapun" id="Sx2.Ex1.m1.6d">italic_s start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i italic_n italic_p end_POSTSUPERSCRIPT = 2 italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i italic_n italic_p end_POSTSUPERSCRIPT , italic_s start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s italic_e italic_p end_POSTSUPERSCRIPT = { start_ROW start_CELL italic_s start_POSTSUPERSCRIPT italic_s italic_e italic_p end_POSTSUPERSCRIPT end_CELL start_CELL if italic_Ï‰ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT &gt; 2 , end_CELL end_ROW start_ROW start_CELL italic_s start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i italic_n italic_p end_POSTSUPERSCRIPT end_CELL start_CELL otherwise. end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Sx2.SS1.p2.2">During upsampling, this process is reversed. To prevent disproportionate convolution of spatial information for volumes with <math alttext="\omega_{n}&gt;2" class="ltx_Math" display="inline" id="Sx2.SS1.p2.2.m1.1"><semantics id="Sx2.SS1.p2.2.m1.1a"><mrow id="Sx2.SS1.p2.2.m1.1.1" xref="Sx2.SS1.p2.2.m1.1.1.cmml"><msub id="Sx2.SS1.p2.2.m1.1.1.2" xref="Sx2.SS1.p2.2.m1.1.1.2.cmml"><mi id="Sx2.SS1.p2.2.m1.1.1.2.2" xref="Sx2.SS1.p2.2.m1.1.1.2.2.cmml">Ï‰</mi><mi id="Sx2.SS1.p2.2.m1.1.1.2.3" xref="Sx2.SS1.p2.2.m1.1.1.2.3.cmml">n</mi></msub><mo id="Sx2.SS1.p2.2.m1.1.1.1" xref="Sx2.SS1.p2.2.m1.1.1.1.cmml">&gt;</mo><mn id="Sx2.SS1.p2.2.m1.1.1.3" xref="Sx2.SS1.p2.2.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS1.p2.2.m1.1b"><apply id="Sx2.SS1.p2.2.m1.1.1.cmml" xref="Sx2.SS1.p2.2.m1.1.1"><gt id="Sx2.SS1.p2.2.m1.1.1.1.cmml" xref="Sx2.SS1.p2.2.m1.1.1.1"></gt><apply id="Sx2.SS1.p2.2.m1.1.1.2.cmml" xref="Sx2.SS1.p2.2.m1.1.1.2"><csymbol cd="ambiguous" id="Sx2.SS1.p2.2.m1.1.1.2.1.cmml" xref="Sx2.SS1.p2.2.m1.1.1.2">subscript</csymbol><ci id="Sx2.SS1.p2.2.m1.1.1.2.2.cmml" xref="Sx2.SS1.p2.2.m1.1.1.2.2">ğœ”</ci><ci id="Sx2.SS1.p2.2.m1.1.1.2.3.cmml" xref="Sx2.SS1.p2.2.m1.1.1.2.3">ğ‘›</ci></apply><cn id="Sx2.SS1.p2.2.m1.1.1.3.cmml" type="integer" xref="Sx2.SS1.p2.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS1.p2.2.m1.1c">\omega_{n}&gt;2</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS1.p2.2.m1.1d">italic_Ï‰ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT &gt; 2</annotation></semantics></math>, we instead apply a 2D convolution across each volume slice, using a central 3D kernel slice extracted from the image through-plane dimension.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>List of Anatomical Structures</h3>
<div class="ltx_para" id="Sx2.SS2.p1">
<p class="ltx_p" id="Sx2.SS2.p1.1">We use segmentations of various anatomical classes, listed below. Bilateral brain structures are defined by two distinct hemisphere-specific labels.</p>
</div>
<div class="ltx_para" id="Sx2.SS2.p2">
<p class="ltx_p" id="Sx2.SS2.p2.1">Global tissue classes include the brain, dura, skull cavity, cerebrum, cerebral white matter, cerebral cortex, brainstem, cerebellum, ventricular system, and cerebral spinal fluid (CSF). Brain sub-structure labels include the amygdala, nucleus accumbens, hippocampus, thalamus, caudate, putamen, dorsal striatum, globus pallidus (externus and internus), basal ganglia, hypothalamus, fornix (body, crus, and column), mammillary body, septal nucleus, subthalamic nucleus, habenula, ventral pallidum, extended amygdala, red nucleus, anterior and posterior commissures, pars compacta, pars reticulata, parabrachial pigmented nucleus, ventral tegmental area, fimbria, septum pellucidum, tectum, pineal gland, superior and inferior colliculus, cerebral peduncle, medullary pyramid, medial lemniscus, superior cerebellar peduncle, middle cerebellar peduncle, inferior cerebellar peduncle, cerebellar gray matter, and cerebellar white matter. Ventricular sub-structure labels include the lateral ventricle, inferior lateral ventricle, posterior lateral ventricle, anterior lateral ventricle, atrium, third ventricle, fourth ventricle, interventricular foramen, and cerebral aqueduct.</p>
</div>
<div class="ltx_para" id="Sx2.SS2.p3">
<p class="ltx_p" id="Sx2.SS2.p3.1">Cortical sub-region labels include the frontal lobe, parietal lobe, temporal lobe, occipital lobe, cingulate cortex, insular cortex, anterior cingulate cortex, caudal anterior cingulate cortex, rostral anterior cingulate cortex, posterior cingulate cortex, isthmus cingulate cortex, frontal pole, middle frontal gyrus, caudal middle frontal gyrus, rostral middle frontal gyrus, superior frontal gyrus, inferior frontal gyrus, pars opercularis, pars orbitalis, pars triangularis, lateral orbitofrontal cortex, medial orbitofrontal cortex, precentral gyrus, paracentral lobule, inferior parietal lobule, superior parietal lobule, supramarginal gyrus, precuneus, postcentral gyrus, entorhinal cortex, fusiform gyrus, parahippocampal gyrus, temporal pole, inferior temporal gyrus, middle temporal gyrus, superior temporal gyrus, transverse temporal gyrus, cuneus, lingual gyrus, and pericalcarine cortex.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Lesion Synthesis</h3>
<div class="ltx_para" id="Sx2.SS3.p1">
<p class="ltx_p" id="Sx2.SS3.p1.1">We broaden the diversity of pathological image features in training by synthesizing brain abnormalities with variable traits. We implement a model-based domain randomization technique to augment images from healthy patients.</p>
</div>
<div class="ltx_para" id="Sx2.SS3.p2">
<p class="ltx_p" id="Sx2.SS3.p2.1">To create a random lesion during training, we first choose a target anatomical location and define a lesion boundary radius. A spherical boundary of the chosen radius is then sampled within a segmentation corresponding to the target anatomy. Inside this boundary, we generate multiple ellipsoids with random positions, rotations, and sizes, which are combined and further modified using random dilation, erosion, and non-linear deformation to produce the final lesion mask.</p>
</div>
<div class="ltx_para" id="Sx2.SS3.p3">
<p class="ltx_p" id="Sx2.SS3.p3.1">The lesion mask is in-painted into scans of a sampled subject. For each scan, we decide whether the lesion will appear hyperintense, hypointense, or isointense, and assign a random fill intensity relative to the surrounding tissue. To simulate heterogeneous intensities, a secondary lesion geometry is superimposed on the original mask.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Segmentation Analysis Data</h3>
<div class="ltx_para" id="Sx2.SS4.p1">
<p class="ltx_p" id="Sx2.SS4.p1.1">In the table below, we summarize the number of unique images and subjects used to evaluate each label in the segmentation analysis (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.SS2" title="5.2 Segmentation Accuracy â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">5.2</span></a>). All evaluations that involve an <span class="ltx_text ltx_font_italic" id="Sx2.SS4.p1.1.1">anatomical</span> label use the same set of evaluation images, so we group them together below.</p>
</div>
<figure class="ltx_table" id="Sx2.SS4.2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx2.SS4.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx2.SS4.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="Sx2.SS4.2.2.3.1.1">Segmentation Target</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Sx2.SS4.2.2.3.1.2">Images</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Sx2.SS4.2.2.3.1.3">Subjects</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx2.SS4.2.2.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Sx2.SS4.2.2.4.1.1">infarct</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx2.SS4.2.2.4.1.2">206</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx2.SS4.2.2.4.1.3">206</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx2.SS4.1.1.1.2">glioma</th>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.1.1.1.1"><math alttext="1,376" class="ltx_Math" display="inline" id="Sx2.SS4.1.1.1.1.m1.2"><semantics id="Sx2.SS4.1.1.1.1.m1.2a"><mrow id="Sx2.SS4.1.1.1.1.m1.2.2.4"><mn id="Sx2.SS4.1.1.1.1.m1.1.1.1.1" xref="Sx2.SS4.1.1.1.1.m1.2.2.3.cmml">1</mn><mo id="Sx2.SS4.1.1.1.1.m1.2.2.4.1" xref="Sx2.SS4.1.1.1.1.m1.2.2.3.cmml">,</mo><mn id="Sx2.SS4.1.1.1.1.m1.2.2.2.2" xref="Sx2.SS4.1.1.1.1.m1.2.2.3.cmml">376</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS4.1.1.1.1.m1.2b"><cn id="Sx2.SS4.1.1.1.1.m1.2.2.3.cmml" type="integer" xref="Sx2.SS4.1.1.1.1.m1.1.1.1.1">1376</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS4.1.1.1.1.m1.2c">1,376</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS4.1.1.1.1.m1.2d">1 , 376</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.1.1.1.3">344</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS4.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx2.SS4.2.2.2.2">edema</th>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.2.1"><math alttext="1,386" class="ltx_Math" display="inline" id="Sx2.SS4.2.2.2.1.m1.2"><semantics id="Sx2.SS4.2.2.2.1.m1.2a"><mrow id="Sx2.SS4.2.2.2.1.m1.2.2.4"><mn id="Sx2.SS4.2.2.2.1.m1.1.1.1.1" xref="Sx2.SS4.2.2.2.1.m1.2.2.3.cmml">1</mn><mo id="Sx2.SS4.2.2.2.1.m1.2.2.4.1" xref="Sx2.SS4.2.2.2.1.m1.2.2.3.cmml">,</mo><mn id="Sx2.SS4.2.2.2.1.m1.2.2.2.2" xref="Sx2.SS4.2.2.2.1.m1.2.2.3.cmml">386</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS4.2.2.2.1.m1.2b"><cn id="Sx2.SS4.2.2.2.1.m1.2.2.3.cmml" type="integer" xref="Sx2.SS4.2.2.2.1.m1.1.1.1.1">1386</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS4.2.2.2.1.m1.2c">1,386</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS4.2.2.2.1.m1.2d">1 , 386</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.2.3">347</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS4.2.2.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx2.SS4.2.2.5.2.1">cyst</th>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.5.2.2">24</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.5.2.3">11</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS4.2.2.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx2.SS4.2.2.6.3.1">papilloma</th>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.6.3.2">16</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.6.3.3">6</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS4.2.2.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx2.SS4.2.2.7.4.1">meningioma</th>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.7.4.2">21</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.7.4.3">8</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS4.2.2.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Sx2.SS4.2.2.8.5.1">white matter hyperintensities</th>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.8.5.2">40</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS4.2.2.8.5.3">20</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS4.2.2.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Sx2.SS4.2.2.9.6.1">anatomical structure</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx2.SS4.2.2.9.6.2">108</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx2.SS4.2.2.9.6.3">40</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="Sx2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Classification Analysis Data</h3>
<div class="ltx_para" id="Sx2.SS5.p1">
<p class="ltx_p" id="Sx2.SS5.p1.1">In the table below, we summarize the number of unique images and subjects used to evaluate each task in the pathology characterization analysis (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.SS3" title="5.3 Pathology Characterization â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<figure class="ltx_table" id="Sx2.SS5.tab1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx2.SS5.tab1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx2.SS5.tab1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Sx2.SS5.tab1.1.1.1.1">Classification Task</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Sx2.SS5.tab1.1.1.1.2">Images</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="Sx2.SS5.tab1.1.1.1.3">Subjects</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx2.SS5.tab1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx2.SS5.tab1.1.2.1.1">characterize lesion signal intensity</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx2.SS5.tab1.1.2.1.2">26</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx2.SS5.tab1.1.2.1.3">26</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS5.tab1.1.3.2">
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.3.2.1">identify lesion cerebral location</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.3.2.2">112</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.3.2.3">30</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS5.tab1.1.4.3">
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.4.3.1">identify infarct vascular territory</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.4.3.2">16</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.4.3.3">12</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS5.tab1.1.5.4">
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.5.4.1">detect diffusion restriction</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.5.4.2">28</td>
<td class="ltx_td ltx_align_left" id="Sx2.SS5.tab1.1.5.4.3">14</td>
</tr>
<tr class="ltx_tr" id="Sx2.SS5.tab1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx2.SS5.tab1.1.6.5.1">detect post-contrast enhancement</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx2.SS5.tab1.1.6.5.2">40</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx2.SS5.tab1.1.6.5.3">20</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="Sx2.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Zero-Shot Baseline Evaluation</h3>
<div class="ltx_para ltx_noindent" id="Sx2.SS6.p1">
<p class="ltx_p" id="Sx2.SS6.p1.3">We assess the zero-shot performance of RadFMÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">77</span></a>]</cite> and ChatGPT-4oÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib143" title=""><span class="ltx_text" style="font-size:90%;">143</span></a>]</cite> on the subset of pathology characterization tasks used in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#S5.SS3" title="5.3 Pathology Characterization â€£ 5 Experiments â€£ VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis"><span class="ltx_text ltx_ref_tag">5.3</span></a>. For each test sample, we provide these models with a detailed task description and all available images. To ensure a fair comparison, we include additional context in the input prompt, along with a set of possible answers for each question. The input prompts used for each task category are detailed below:</p>
<ul class="ltx_itemize" id="Sx2.I1">
<li class="ltx_item" id="Sx2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€“</span>
<div class="ltx_para" id="Sx2.I1.i1.p1">
<p class="ltx_p" id="Sx2.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i1.p1.1.1">Given the following brain scan(s), classify the lesion signal intensity as either hyperintense or hypointense.</span></p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€“</span>
<div class="ltx_para" id="Sx2.I1.i2.p1">
<p class="ltx_p" id="Sx2.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i2.p1.1.1">Given the following brain scan(s), classify the lesion location as either the temporal lobe, frontal lobe, parietal lobe, brainstem, or cerebellum.</span></p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€“</span>
<div class="ltx_para" id="Sx2.I1.i3.p1">
<p class="ltx_p" id="Sx2.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i3.p1.1.1">Given the following brain scan(s), classify the infarct location as either the ACA, MCA, or PCA territory.</span></p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€“</span>
<div class="ltx_para" id="Sx2.I1.i4.p1">
<p class="ltx_p" id="Sx2.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i4.p1.1.1">Given the following DWI and ADC brain scans, classify the lesion as diffusion restricting or non-restricting.</span></p>
</div>
</li>
<li class="ltx_item" id="Sx2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€“</span>
<div class="ltx_para" id="Sx2.I1.i5.p1">
<p class="ltx_p" id="Sx2.I1.i5.p1.1"><span class="ltx_text ltx_font_italic" id="Sx2.I1.i5.p1.1.1">Given the following pre- and post-contrast brain scans, indicate if the lesion is contrast enhancing.</span></p>
</div>
</li>
</ul>
<p class="ltx_p" id="Sx2.SS6.p1.2"><span class="ltx_text ltx_font_bold" id="Sx2.SS6.p1.2.1">RadFM Zero-Shot Evaluation.</span>
To match RadFM input constraints, we resample all image shapes to multiples ofÂ <math alttext="32\times 32\times 4" class="ltx_Math" display="inline" id="Sx2.SS6.p1.1.m1.1"><semantics id="Sx2.SS6.p1.1.m1.1a"><mrow id="Sx2.SS6.p1.1.m1.1.1" xref="Sx2.SS6.p1.1.m1.1.1.cmml"><mn id="Sx2.SS6.p1.1.m1.1.1.2" xref="Sx2.SS6.p1.1.m1.1.1.2.cmml">32</mn><mo id="Sx2.SS6.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Sx2.SS6.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="Sx2.SS6.p1.1.m1.1.1.3" xref="Sx2.SS6.p1.1.m1.1.1.3.cmml">32</mn><mo id="Sx2.SS6.p1.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="Sx2.SS6.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="Sx2.SS6.p1.1.m1.1.1.4" xref="Sx2.SS6.p1.1.m1.1.1.4.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS6.p1.1.m1.1b"><apply id="Sx2.SS6.p1.1.m1.1.1.cmml" xref="Sx2.SS6.p1.1.m1.1.1"><times id="Sx2.SS6.p1.1.m1.1.1.1.cmml" xref="Sx2.SS6.p1.1.m1.1.1.1"></times><cn id="Sx2.SS6.p1.1.m1.1.1.2.cmml" type="integer" xref="Sx2.SS6.p1.1.m1.1.1.2">32</cn><cn id="Sx2.SS6.p1.1.m1.1.1.3.cmml" type="integer" xref="Sx2.SS6.p1.1.m1.1.1.3">32</cn><cn id="Sx2.SS6.p1.1.m1.1.1.4.cmml" type="integer" xref="Sx2.SS6.p1.1.m1.1.1.4">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS6.p1.1.m1.1c">32\times 32\times 4</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS6.p1.1.m1.1d">32 Ã— 32 Ã— 4</annotation></semantics></math>. For each test case, we run evaluations across all possible voxel orientations of the volume data and run additional tests with the volume depth resampled toÂ <math alttext="64" class="ltx_Math" display="inline" id="Sx2.SS6.p1.2.m2.1"><semantics id="Sx2.SS6.p1.2.m2.1a"><mn id="Sx2.SS6.p1.2.m2.1.1" xref="Sx2.SS6.p1.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="Sx2.SS6.p1.2.m2.1b"><cn id="Sx2.SS6.p1.2.m2.1.1.cmml" type="integer" xref="Sx2.SS6.p1.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS6.p1.2.m2.1c">64</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS6.p1.2.m2.1d">64</annotation></semantics></math> voxels â€“ the maximum depth used in RadFM pretraining.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx2.SS6.p2">
<p class="ltx_p" id="Sx2.SS6.p2.1"><span class="ltx_text ltx_font_bold" id="Sx2.SS6.p2.1.1">ChatGPT Zero-Shot Evaluation.</span>
ChatGPT-4o is primarily designed for 2D image inputs rather than volumetric data. Thus, we convert each 3D volume into multi-plane cross-sectional slices centered on the relevant region of interest. Note that this slicing method biases ChatGPT predictions by effectively pinpointing the lesion location within the full volume.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx2.SS6.p3">
<p class="ltx_p" id="Sx2.SS6.p3.1"><span class="ltx_text ltx_font_bold" id="Sx2.SS6.p3.1.1">Results.</span>
Across all characterization tasks, RadFM fails to correctly classify <span class="ltx_text ltx_font_italic" id="Sx2.SS6.p3.1.2">any</span> test sample, and ChatGPT achieves an average accuracy of onlyÂ <math alttext="58.6\pm 14.1\%" class="ltx_Math" display="inline" id="Sx2.SS6.p3.1.m1.1"><semantics id="Sx2.SS6.p3.1.m1.1a"><mrow id="Sx2.SS6.p3.1.m1.1.1" xref="Sx2.SS6.p3.1.m1.1.1.cmml"><mn id="Sx2.SS6.p3.1.m1.1.1.2" xref="Sx2.SS6.p3.1.m1.1.1.2.cmml">58.6</mn><mo id="Sx2.SS6.p3.1.m1.1.1.1" xref="Sx2.SS6.p3.1.m1.1.1.1.cmml">Â±</mo><mrow id="Sx2.SS6.p3.1.m1.1.1.3" xref="Sx2.SS6.p3.1.m1.1.1.3.cmml"><mn id="Sx2.SS6.p3.1.m1.1.1.3.2" xref="Sx2.SS6.p3.1.m1.1.1.3.2.cmml">14.1</mn><mo id="Sx2.SS6.p3.1.m1.1.1.3.1" xref="Sx2.SS6.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS6.p3.1.m1.1b"><apply id="Sx2.SS6.p3.1.m1.1.1.cmml" xref="Sx2.SS6.p3.1.m1.1.1"><csymbol cd="latexml" id="Sx2.SS6.p3.1.m1.1.1.1.cmml" xref="Sx2.SS6.p3.1.m1.1.1.1">plus-or-minus</csymbol><cn id="Sx2.SS6.p3.1.m1.1.1.2.cmml" type="float" xref="Sx2.SS6.p3.1.m1.1.1.2">58.6</cn><apply id="Sx2.SS6.p3.1.m1.1.1.3.cmml" xref="Sx2.SS6.p3.1.m1.1.1.3"><csymbol cd="latexml" id="Sx2.SS6.p3.1.m1.1.1.3.1.cmml" xref="Sx2.SS6.p3.1.m1.1.1.3.1">percent</csymbol><cn id="Sx2.SS6.p3.1.m1.1.1.3.2.cmml" type="float" xref="Sx2.SS6.p3.1.m1.1.1.3.2">14.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS6.p3.1.m1.1c">58.6\pm 14.1\%</annotation><annotation encoding="application/x-llamapun" id="Sx2.SS6.p3.1.m1.1d">58.6 Â± 14.1 %</annotation></semantics></math>, suggesting an inability to generalize to complex medical imaging tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Radiopaedia Data</h3>
<div class="ltx_para" id="Sx2.SS7.p1">
<p class="ltx_p" id="Sx2.SS7.p1.1">We download the following patient cases from Radiopaedia, a radiology reference website at <span class="ltx_text ltx_font_italic" id="Sx2.SS7.p1.1.1">https://radiopaedia.org</span>. Each case includes text-based notes and brain scans in the form of 2D image slices. We reconstruct volumetric data by stacking these slices and estimating an affine matrix to map voxel coordinates in world space. We compute this mapping by registering the image to an average brain template with SynthMorphÂ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.08397v1#bib.bib141" title=""><span class="ltx_text" style="font-size:90%;">141</span></a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="Sx2.SS7.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Case References</h4>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.1.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.1.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.1.p1.1.1" style="font-size:80%;">Abdelmonem H, Suprasellar arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.1.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-50712</span><span class="ltx_text" id="Sx2.SS7.SSSx1.1.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.2.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.2.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.2.p1.1.1" style="font-size:80%;">Abdrabou A, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.2.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-67618</span><span class="ltx_text" id="Sx2.SS7.SSSx1.2.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.3.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.3.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.3.p1.1.1" style="font-size:80%;">Abdrabou A, Fungal cerebral abscesses. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.3.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-34829</span><span class="ltx_text" id="Sx2.SS7.SSSx1.3.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.4.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.4.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.4.p1.1.1" style="font-size:80%;">Abdrabou A, Intracranial epidermoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.4.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-41892</span><span class="ltx_text" id="Sx2.SS7.SSSx1.4.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.5.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.5.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.5.p1.1.1" style="font-size:80%;">Abdrabou A, Meningioma - cerebellopontine angle. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.5.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-36505</span><span class="ltx_text" id="Sx2.SS7.SSSx1.5.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.6.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.6.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.6.p1.1.1" style="font-size:80%;">Abdrabou A, Pancoast tumor with cystic cerebral metastasis. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.6.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-27303</span><span class="ltx_text" id="Sx2.SS7.SSSx1.6.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.7.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.7.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.7.p1.1.1" style="font-size:80%;">Al Jaâ€™afreh H, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.7.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-174815</span><span class="ltx_text" id="Sx2.SS7.SSSx1.7.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.8.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.8.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.8.p1.1.1" style="font-size:80%;">Al Khateeb A, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.8.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-43429</span><span class="ltx_text" id="Sx2.SS7.SSSx1.8.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.9.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.9.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.9.p1.1.1" style="font-size:80%;">Al Khateeb A, Suprasellar arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.9.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-45162</span><span class="ltx_text" id="Sx2.SS7.SSSx1.9.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.10.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.10.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.10.p1.1.1" style="font-size:80%;">Al Salam H, Arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.10.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-10869</span><span class="ltx_text" id="Sx2.SS7.SSSx1.10.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.11.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.11.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.11.p1.1.1" style="font-size:80%;">Al Salam H, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.11.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-9589</span><span class="ltx_text" id="Sx2.SS7.SSSx1.11.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.12.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.12.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.12.p1.1.1" style="font-size:80%;">Al Salam H, Choroid plexus papilloma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.12.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-8466</span><span class="ltx_text" id="Sx2.SS7.SSSx1.12.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.13.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.13.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.13.p1.1.1" style="font-size:80%;">Alghamdi K, Intracranial epidermoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.13.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-78159</span><span class="ltx_text" id="Sx2.SS7.SSSx1.13.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.14.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.14.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.14.p1.1.1" style="font-size:80%;">Almoghazy S, Intracranial epidermoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.14.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-86783</span><span class="ltx_text" id="Sx2.SS7.SSSx1.14.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.15.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.15.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.15.p1.1.1" style="font-size:80%;">Amer M, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.15.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-184648</span><span class="ltx_text" id="Sx2.SS7.SSSx1.15.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.16.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.16.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.16.p1.1.1" style="font-size:80%;">Asadov D, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.16.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-8837</span><span class="ltx_text" id="Sx2.SS7.SSSx1.16.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.17.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.17.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.17.p1.1.1" style="font-size:80%;">Babu V, Arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.17.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-46791</span><span class="ltx_text" id="Sx2.SS7.SSSx1.17.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.18.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.18.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.18.p1.1.1" style="font-size:80%;">Balachandran G, Posterior fossa meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.18.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-5597</span><span class="ltx_text" id="Sx2.SS7.SSSx1.18.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.19.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.19.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.19.p1.1.1" style="font-size:80%;">Bayat M, Choroid plexus papilloma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.19.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-76732</span><span class="ltx_text" id="Sx2.SS7.SSSx1.19.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.20.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.20.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.20.p1.1.1" style="font-size:80%;">Ben-Shabat E, Stroke progression. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.20.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-93012</span><span class="ltx_text" id="Sx2.SS7.SSSx1.20.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.21.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.21.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.21.p1.1.1" style="font-size:80%;">Bickle I, Intraventricular meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.21.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-45529</span><span class="ltx_text" id="Sx2.SS7.SSSx1.21.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.22.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.22.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.22.p1.1.1" style="font-size:80%;">Bickle I, Multicentric glioblastoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.22.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-88861</span><span class="ltx_text" id="Sx2.SS7.SSSx1.22.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.23.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.23.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.23.p1.1.1" style="font-size:80%;">Bickle I, Petrous ridge meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.23.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-50777</span><span class="ltx_text" id="Sx2.SS7.SSSx1.23.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.24.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.24.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.24.p1.1.1" style="font-size:80%;">Bickle I, Subacute subdural hematoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.24.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-32123</span><span class="ltx_text" id="Sx2.SS7.SSSx1.24.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.25.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.25.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.25.p1.1.1" style="font-size:80%;">Bickle I, Suprasellar arachnoid cyst - pediatric. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.25.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-82691</span><span class="ltx_text" id="Sx2.SS7.SSSx1.25.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.26.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.26.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.26.p1.1.1" style="font-size:80%;">Cuete D, Mega cisterna magna. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.26.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-26363</span><span class="ltx_text" id="Sx2.SS7.SSSx1.26.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.27.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.27.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.27.p1.1.1" style="font-size:80%;">Cuete D, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.27.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-22919</span><span class="ltx_text" id="Sx2.SS7.SSSx1.27.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.28.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.28.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.28.p1.1.1" style="font-size:80%;">Desai P, Frontal meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.28.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-19463</span><span class="ltx_text" id="Sx2.SS7.SSSx1.28.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.29.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.29.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.29.p1.1.1" style="font-size:80%;">Di Muzio B, Brain metastasis (large cystic mass). </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.29.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-47497</span><span class="ltx_text" id="Sx2.SS7.SSSx1.29.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.30.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.30.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.30.p1.1.1" style="font-size:80%;">Di Muzio B, Falcine meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.30.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-40869</span><span class="ltx_text" id="Sx2.SS7.SSSx1.30.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.31.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.31.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.31.p1.1.1" style="font-size:80%;">Ebouda F, Left occipital late subacute epidural hematoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.31.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-28001</span><span class="ltx_text" id="Sx2.SS7.SSSx1.31.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.32.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.32.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.32.p1.1.1" style="font-size:80%;">Elfeky M, Arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.32.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-42196</span><span class="ltx_text" id="Sx2.SS7.SSSx1.32.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.33.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.33.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.33.p1.1.1" style="font-size:80%;">Elfeky M, Cerebral convexity epidermoid. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.33.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-43105</span><span class="ltx_text" id="Sx2.SS7.SSSx1.33.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.34.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.34.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.34.p1.1.1" style="font-size:80%;">Farhadi M, Arachnoid cyst - bilateral middle cranial fossa. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.34.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-183101</span><span class="ltx_text" id="Sx2.SS7.SSSx1.34.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.35.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.35.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.35.p1.1.1" style="font-size:80%;">Farzam F, Epidermoid cyst - middle cranial fossa. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.35.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-69937</span><span class="ltx_text" id="Sx2.SS7.SSSx1.35.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.36.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.36.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.36.p1.1.1" style="font-size:80%;">Gaillard F, Arachnoid cyst - cerebellopontine angle. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.36.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-8219</span><span class="ltx_text" id="Sx2.SS7.SSSx1.36.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.37.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.37.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.37.p1.1.1" style="font-size:80%;">Gaillard F, Arachnoid cyst - posterior fossa. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.37.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-4544</span><span class="ltx_text" id="Sx2.SS7.SSSx1.37.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.38.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.38.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.38.p1.1.1" style="font-size:80%;">Gaillard F, Arachnoid cyst - suprasellar. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.38.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-8914</span><span class="ltx_text" id="Sx2.SS7.SSSx1.38.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.39.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.39.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.39.p1.1.1" style="font-size:80%;">Gaillard F, Cavum velum interpositum cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.39.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-15981</span><span class="ltx_text" id="Sx2.SS7.SSSx1.39.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.40.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.40.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.40.p1.1.1" style="font-size:80%;">Gaillard F, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.40.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-21041</span><span class="ltx_text" id="Sx2.SS7.SSSx1.40.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.41.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.41.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.41.p1.1.1" style="font-size:80%;">Gaillard F, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.41.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-33049</span><span class="ltx_text" id="Sx2.SS7.SSSx1.41.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.42.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.42.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.42.p1.1.1" style="font-size:80%;">Gaillard F, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.42.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-4002</span><span class="ltx_text" id="Sx2.SS7.SSSx1.42.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.43.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.43.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.43.p1.1.1" style="font-size:80%;">Gaillard F, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.43.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-65317</span><span class="ltx_text" id="Sx2.SS7.SSSx1.43.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.44.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.44.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.44.p1.1.1" style="font-size:80%;">Gaillard F, Choroid plexus papilloma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.44.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-33040</span><span class="ltx_text" id="Sx2.SS7.SSSx1.44.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.45.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.45.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.45.p1.1.1" style="font-size:80%;">Gaillard F, Encephalomalacia. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.45.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-22285</span><span class="ltx_text" id="Sx2.SS7.SSSx1.45.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.46.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.46.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.46.p1.1.1" style="font-size:80%;">Gaillard F, Epidermoid cyst (cerebellopontine angle). </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.46.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-33762</span><span class="ltx_text" id="Sx2.SS7.SSSx1.46.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.47.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.47.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.47.p1.1.1" style="font-size:80%;">Gaillard F, Epidermoid cyst - suprasellar. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.47.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-16839</span><span class="ltx_text" id="Sx2.SS7.SSSx1.47.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.48.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.48.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.48.p1.1.1" style="font-size:80%;">Gaillard F, Epidermoid cyst: frontal and cerebellopontine angle. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.48.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-58135</span><span class="ltx_text" id="Sx2.SS7.SSSx1.48.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.49.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.49.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.49.p1.1.1" style="font-size:80%;">Gaillard F, Glioblastoma, IDH- wildtype (pseudoprogression). </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.49.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-42209</span><span class="ltx_text" id="Sx2.SS7.SSSx1.49.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.50.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.50.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.50.p1.1.1" style="font-size:80%;">Gaillard F, Olfactory groove meningioma - huge. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.50.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-31075</span><span class="ltx_text" id="Sx2.SS7.SSSx1.50.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.51.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.51.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.51.p1.1.1" style="font-size:80%;">Gaillard F, Pineal epidermoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.51.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-99497</span><span class="ltx_text" id="Sx2.SS7.SSSx1.51.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.52.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.52.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.52.p1.1.1" style="font-size:80%;">Gaillard F, Suprasellar arachnoid cyst - massive. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.52.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-62759</span><span class="ltx_text" id="Sx2.SS7.SSSx1.52.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.53.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.53.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.53.p1.1.1" style="font-size:80%;">Gaillard F, Suprasellar arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.53.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-75082</span><span class="ltx_text" id="Sx2.SS7.SSSx1.53.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.54.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.54.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.54.p1.1.1" style="font-size:80%;">Geetha Virupakshappa A, Arachnoid cyst - Galassi type III. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.54.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-149904</span><span class="ltx_text" id="Sx2.SS7.SSSx1.54.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.55.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.55.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.55.p1.1.1" style="font-size:80%;">Gewolb D, Arachnoid cyst - intracystic hemorrhage. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.55.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-161492</span><span class="ltx_text" id="Sx2.SS7.SSSx1.55.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.56.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.56.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.56.p1.1.1" style="font-size:80%;">Goel A, Hemorrhagic cerebral metastases. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.56.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-26687</span><span class="ltx_text" id="Sx2.SS7.SSSx1.56.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.57.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.57.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.57.p1.1.1" style="font-size:80%;">Hamidi H, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.57.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-50810</span><span class="ltx_text" id="Sx2.SS7.SSSx1.57.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.58.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.58.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.58.p1.1.1" style="font-size:80%;">Haouimi A, Arachnoid cyst - middle cranial fossa. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.58.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-85061</span><span class="ltx_text" id="Sx2.SS7.SSSx1.58.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.59.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.59.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.59.p1.1.1" style="font-size:80%;">Haouimi A, Arachnoid cyst - sellar and suprasellar. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.59.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-96728</span><span class="ltx_text" id="Sx2.SS7.SSSx1.59.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.60.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.60.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.60.p1.1.1" style="font-size:80%;">Haouimi A, Meningioma - frontal convexity. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.60.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-79485</span><span class="ltx_text" id="Sx2.SS7.SSSx1.60.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.61.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.61.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.61.p1.1.1" style="font-size:80%;">Haouimi A, Subependymal heterotopia - cyst of the velum interpositum. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.61.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-74488</span><span class="ltx_text" id="Sx2.SS7.SSSx1.61.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.62.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.62.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.62.p1.1.1" style="font-size:80%;">Haouimi A, Suprasellar arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.62.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-99076</span><span class="ltx_text" id="Sx2.SS7.SSSx1.62.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.63.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.63.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.63.p1.1.1" style="font-size:80%;">Hartung M, Acute on chronic subdural hematoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.63.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-78346</span><span class="ltx_text" id="Sx2.SS7.SSSx1.63.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.64.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.64.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.64.p1.1.1" style="font-size:80%;">Hospital R, Epidermoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.64.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-13721</span><span class="ltx_text" id="Sx2.SS7.SSSx1.64.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.65.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.65.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.65.p1.1.1" style="font-size:80%;">Hosseinabadi F, Suprasellar arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.65.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-74656</span><span class="ltx_text" id="Sx2.SS7.SSSx1.65.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.66.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.66.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.66.p1.1.1" style="font-size:80%;">Husen H, Choroid plexus papilloma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.66.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-89256</span><span class="ltx_text" id="Sx2.SS7.SSSx1.66.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.67.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.67.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.67.p1.1.1" style="font-size:80%;">Kabra U, Cerebral abscess. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.67.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-157965</span><span class="ltx_text" id="Sx2.SS7.SSSx1.67.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.68.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.68.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.68.p1.1.1" style="font-size:80%;">Kabra U, Choroid plexus papilloma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.68.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-84612</span><span class="ltx_text" id="Sx2.SS7.SSSx1.68.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.69.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.69.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.69.p1.1.1" style="font-size:80%;">Knipe H, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.69.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-56937</span><span class="ltx_text" id="Sx2.SS7.SSSx1.69.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.70.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.70.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.70.p1.1.1" style="font-size:80%;">Mahmoud Q, Brain metastases from breast cancer. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.70.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-164146</span><span class="ltx_text" id="Sx2.SS7.SSSx1.70.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.71.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.71.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.71.p1.1.1" style="font-size:80%;">Mahmoud Q, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.71.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-183767</span><span class="ltx_text" id="Sx2.SS7.SSSx1.71.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.72.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.72.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.72.p1.1.1" style="font-size:80%;">Marghany B, Epidermoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.72.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-67246</span><span class="ltx_text" id="Sx2.SS7.SSSx1.72.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.73.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.73.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.73.p1.1.1" style="font-size:80%;">Mishra H, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.73.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-90656</span><span class="ltx_text" id="Sx2.SS7.SSSx1.73.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.74.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.74.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.74.p1.1.1" style="font-size:80%;">Mudgal P, Bilateral sub-acute subdural hematomas. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.74.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-27476</span><span class="ltx_text" id="Sx2.SS7.SSSx1.74.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.75.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.75.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.75.p1.1.1" style="font-size:80%;">Mudgal P, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.75.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-30220</span><span class="ltx_text" id="Sx2.SS7.SSSx1.75.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.76.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.76.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.76.p1.1.1" style="font-size:80%;">Mudgal P, Intracranial epidermoid cyst - cerebellopontine angle. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.76.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-26565</span><span class="ltx_text" id="Sx2.SS7.SSSx1.76.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.77.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.77.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.77.p1.1.1" style="font-size:80%;">Neto A, Arachnoid cyst - middle cranial fossa. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.77.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-71674</span><span class="ltx_text" id="Sx2.SS7.SSSx1.77.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.78.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.78.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.78.p1.1.1" style="font-size:80%;">Neuropathology R, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.78.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-37664</span><span class="ltx_text" id="Sx2.SS7.SSSx1.78.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.79.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.79.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.79.p1.1.1" style="font-size:80%;">Neuropathology R, Choroid plexus papilloma and </span>
<br class="ltx_break"/><span class="ltx_text" id="Sx2.SS7.SSSx1.79.p1.1.2" style="font-size:80%;">acoustic scwhannoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.79.p1.1.3" style="font-size:80%;">https://doi.org/10.53347/rID-28131</span><span class="ltx_text" id="Sx2.SS7.SSSx1.79.p1.1.4" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.80.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.80.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.80.p1.1.1" style="font-size:80%;">Neuropathology R, Choroid plexus papilloma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.80.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-38522</span><span class="ltx_text" id="Sx2.SS7.SSSx1.80.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.81.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.81.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.81.p1.1.1" style="font-size:80%;">Niknejad M, Epidermoid cyst - cerebellopontine angle. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.81.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-61773</span><span class="ltx_text" id="Sx2.SS7.SSSx1.81.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.82.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.82.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.82.p1.1.1" style="font-size:80%;">Osman M, Chordoid meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.82.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-23037</span><span class="ltx_text" id="Sx2.SS7.SSSx1.82.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.83.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.83.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.83.p1.1.1" style="font-size:80%;">Perera A, Choroid plexus papilloma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.83.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-92022</span><span class="ltx_text" id="Sx2.SS7.SSSx1.83.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.84.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.84.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.84.p1.1.1" style="font-size:80%;">Pirzad F, Arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.84.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-9619</span><span class="ltx_text" id="Sx2.SS7.SSSx1.84.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.85.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.85.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.85.p1.1.1" style="font-size:80%;">Pirzad F, Cerebellar hemorrhage. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.85.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-9620</span><span class="ltx_text" id="Sx2.SS7.SSSx1.85.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.86.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.86.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.86.p1.1.1" style="font-size:80%;">Qureshi P, Meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.86.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-69335</span><span class="ltx_text" id="Sx2.SS7.SSSx1.86.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.87.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.87.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.87.p1.1.1" style="font-size:80%;">Ranchod A, Intracranial epidermoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.87.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-94043</span><span class="ltx_text" id="Sx2.SS7.SSSx1.87.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.88.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.88.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.88.p1.1.1" style="font-size:80%;">Ranchod A, Planum sphenoidale meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.88.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-97514</span><span class="ltx_text" id="Sx2.SS7.SSSx1.88.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.89.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.89.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.89.p1.1.1" style="font-size:80%;">Rasuli B, Epidermoid cyst - cerebellopontine angle. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.89.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-83470</span><span class="ltx_text" id="Sx2.SS7.SSSx1.89.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.90.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.90.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.90.p1.1.1" style="font-size:80%;">Rasuli B, Intracranial epidermoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.90.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-170531</span><span class="ltx_text" id="Sx2.SS7.SSSx1.90.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.91.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.91.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.91.p1.1.1" style="font-size:80%;">Saber M, Cavum velum interpositum cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.91.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-155458</span><span class="ltx_text" id="Sx2.SS7.SSSx1.91.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.92.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.92.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.92.p1.1.1" style="font-size:80%;">Saber M, Early and late subacute intracerebral hemorrhage. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.92.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-148791</span><span class="ltx_text" id="Sx2.SS7.SSSx1.92.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.93.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.93.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.93.p1.1.1" style="font-size:80%;">Schubert R, Cavum velum interpositum cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.93.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-13825</span><span class="ltx_text" id="Sx2.SS7.SSSx1.93.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.94.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.94.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.94.p1.1.1" style="font-size:80%;">Sorrentino S, Arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.94.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-19826</span><span class="ltx_text" id="Sx2.SS7.SSSx1.94.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.95.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.95.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.95.p1.1.1" style="font-size:80%;">Sorrentino S, Convexity meningioma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.95.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-16004</span><span class="ltx_text" id="Sx2.SS7.SSSx1.95.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.96.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.96.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.96.p1.1.1" style="font-size:80%;">Stanislavsky A, Central neurocytoma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.96.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-13188</span><span class="ltx_text" id="Sx2.SS7.SSSx1.96.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.97.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.97.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.97.p1.1.1" style="font-size:80%;">Tarasov N, Intracranial epidermoid cyst - posterior fossa. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.97.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-39109</span><span class="ltx_text" id="Sx2.SS7.SSSx1.97.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.98.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.98.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.98.p1.1.1" style="font-size:80%;">Thapa S, Arachnoid cyst - Galassi type III. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.98.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-40204</span><span class="ltx_text" id="Sx2.SS7.SSSx1.98.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.99.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.99.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.99.p1.1.1" style="font-size:80%;">Thibodeau R, Choroid plexus papilloma. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.99.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-166392</span><span class="ltx_text" id="Sx2.SS7.SSSx1.99.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.100.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.100.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.100.p1.1.1" style="font-size:80%;">Trajcevska E, Suprasellar arachnoid cyst. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.100.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-22877</span><span class="ltx_text" id="Sx2.SS7.SSSx1.100.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_para ltx_minipage ltx_align_middle" id="Sx2.SS7.SSSx1.101.p1" style="width:433.6pt;">
<p class="ltx_p" id="Sx2.SS7.SSSx1.101.p1.1"><span class="ltx_text" id="Sx2.SS7.SSSx1.101.p1.1.1" style="font-size:80%;">Voss Y, Hyperacute intracerebral hemorrhage. </span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="Sx2.SS7.SSSx1.101.p1.1.2" style="font-size:80%;">https://doi.org/10.53347/rID-49757</span><span class="ltx_text" id="Sx2.SS7.SSSx1.101.p1.1.3" style="font-size:80%;"></span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 10 21:58:31 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
