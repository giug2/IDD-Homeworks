<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>When Does Perceptual Alignment Benefit Vision Representations?</title>
<!--Generated on Mon Oct 14 17:10:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.10817v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S1" title="In When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S2" title="In When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S2.SS0.SSS0.Px1" title="In 2 Related Works ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title">Vision backbones as learned feature extractors.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S2.SS0.SSS0.Px2" title="In 2 Related Works ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title">View selection for self-supervised learning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S2.SS0.SSS0.Px3" title="In 2 Related Works ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title">Learning with human alignment.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3" title="In When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Learning from perceptual judgments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.SS1" title="In 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Human Similarity Annotations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.SS2" title="In 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Image-level objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.SS3" title="In 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Patch-level objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.SS4" title="In 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Implementation details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.SS4.SSS0.Px1" title="In 3.4 Implementation details ‣ 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title">Vision Model Backbones.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.SS4.SSS0.Px2" title="In 3.4 Implementation details ‣ 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title">Finetuning with human preference labels.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4" title="In When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS1" title="In 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dense Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS2" title="In 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Retrieval-augmented generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS3" title="In 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Counting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS4" title="In 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Instance retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS5" title="In 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>What type of human similarity annotation is most beneficial?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S5" title="In When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S5.SS0.SSS0.Px1" title="In 5 Discussion ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title">Limitations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S5.SS0.SSS0.Px2" title="In 5 Discussion ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title">Societal impacts.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1" title="In When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS1" title="In Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Classification with the VTAB Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS2" title="In Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Additional dataset ablations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS3" title="In Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Additional RAG results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS4" title="In Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Perceptual alignment for CNNs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2" title="In When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Qualitative examples</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2.SS1" title="In Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Additional visualizations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2.SS2" title="In Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Dataset examples</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3" title="In When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS1" title="In Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Training human-aligned models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS1.SSS1" title="In C.1 Training human-aligned models ‣ Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1.1 </span>Dense prediction heads</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS2" title="In Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Retrieval-augmented generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS2.SSS1" title="In C.2 Retrieval-augmented generation ‣ Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.1 </span>Object counting</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS3" title="In Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Instance retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS4" title="In Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.4 </span>Dataset ablations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS5" title="In Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.5 </span>VTAB classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS6" title="In Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.6 </span>Additional compute details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS7" title="In Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.7 </span>Licenses for existing assets</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\newfloatcommand</span>
<p class="ltx_p" id="p1.2">capbtabboxtable[][<span class="ltx_ERROR undefined" id="p1.2.1">\FBwidth</span>]








<span class="ltx_text" id="p1.2.2" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document">When Does Perceptual Alignment 
<br class="ltx_break"/>Benefit Vision Representations?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shobhita Sundaram<sup class="ltx_sup" id="id15.15.id1"><span class="ltx_text ltx_font_italic" id="id15.15.id1.1">1</span></sup>
     
Stephanie Fu<sup class="ltx_sup" id="id16.16.id2"><span class="ltx_text ltx_font_italic" id="id16.16.id2.1">2∗</span></sup>
     
<span class="ltx_text ltx_font_bold" id="id3.3.1">Lukas Muttenthaler<sup class="ltx_sup" id="id3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id3.3.1.1.1">3,4</span></sup></span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id4.4.2">Netanel Y. Tamir<sup class="ltx_sup" id="id4.4.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id4.4.2.1.1">5</span></sup></span>
    
<span class="ltx_text ltx_font_bold" id="id5.5.3">Lucy Chai<sup class="ltx_sup" id="id5.5.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id5.5.3.1.1">1</span></sup></span>
    
<span class="ltx_text ltx_font_bold" id="id6.6.4">Simon Kornblith<sup class="ltx_sup" id="id6.6.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.4.1.1">6</span></sup></span>
    
<span class="ltx_text ltx_font_bold" id="id7.7.5">Trevor Darrell<sup class="ltx_sup" id="id7.7.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.5.1.1">2</span></sup></span>
    
<span class="ltx_text ltx_font_bold" id="id8.8.6">Phillip Isola<sup class="ltx_sup" id="id8.8.6.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id8.8.6.1.1">1</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id17.17.id3"><span class="ltx_text ltx_font_italic" id="id17.17.id3.1">1</span></sup>MIT    <sup class="ltx_sup" id="id18.18.id4"><span class="ltx_text ltx_font_italic" id="id18.18.id4.1">2</span></sup>U.C. Berkeley    <sup class="ltx_sup" id="id19.19.id5"><span class="ltx_text ltx_font_italic" id="id19.19.id5.1">3</span></sup>TU Berlin    <sup class="ltx_sup" id="id20.20.id6"><span class="ltx_text ltx_font_italic" id="id20.20.id6.1">4</span></sup>BIFOLD    <sup class="ltx_sup" id="id21.21.id7"><span class="ltx_text ltx_font_italic" id="id21.21.id7.1">5</span></sup>Weizmann Institute of Science    <sup class="ltx_sup" id="id22.22.id8"><span class="ltx_text ltx_font_italic" id="id22.22.id8.1">6</span></sup>Anthropic
</span><span class="ltx_author_notes">Equal contribution.Work partly done while a Student Researcher at Google DeepMind.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id23.id1"><span class="ltx_text" id="id23.id1.1" lang="en">Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception.
While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability across diverse computer vision tasks. We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard vision benchmarks. We find that aligning models to perceptual judgments yields representations that <em class="ltx_emph ltx_font_italic" id="id23.id1.1.1">improve</em> upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized <em class="ltx_emph ltx_font_italic" id="id23.id1.1.2">out-of-distribution</em> domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations.</span></p>
</div>
<figure class="ltx_figure" id="S0.F1" lang="en"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="398" id="S0.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S0.F1.4.2" style="font-size:90%;">Does human perceptual alignment improve vision representations?<span class="ltx_text ltx_font_medium" id="S0.F1.4.2.1"> Vision models have been shown to learn useful image representations through large-scale pretraining (e.g., CLIP, DINO). We find that additionally aligning these models to human perceptual judgments yields representations that <em class="ltx_emph ltx_font_italic" id="S0.F1.4.2.1.1">improve</em> upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. Our blog post and code are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="percep-align.github.io" title="">percep-align.github.io</a>.</span></span></figcaption>
</figure>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Our sense of similarity is crucial to how we perceive and act in the world. Consider the range of factors that might influence your judgment of visual similarity: layout, color, perspective, semantics, and more. These characteristics shape our local inferences about object relationships, enabling us to build a comprehensive understanding of the visual world.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Given the importance of similarity judgments in our visual perception, it follows that aligning vision models to these judgments could help them develop more human-like visual capabilities.
In the language domain, we have seen the power of aligning LLMs to human feedback <cite class="ltx_cite ltx_citemacro_citep">[RLHF; <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib9" title="">9</a>]</cite>, resulting in safer and more useful models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib3" title="">3</a>]</cite>.
Similar trends are emerging in vision, where there is growing interest in alignment with human perceptual judgments to improve vision representations <cite class="ltx_cite ltx_citemacro_citep">[e.g., <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib43" title="">43</a>]</cite>. Human feedback has also been used to improve the aesthetic quality of diffusion-generated images <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib30" title="">30</a>]</cite>, increase retrieval capabilities <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite>, and improve downstream task performance of image/text models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib42" title="">42</a>]</cite>. While acknowledging this wide space of human preference data used for alignment in vision and language, we focus our study on image similarity judgments, which give us a more stable and shared measure of human visual perception  <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib43" title="">43</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Although there is consensus that alignment to perceptual judgements can enable specific goals (e.g., similarity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib42" title="">42</a>]</cite>), their utility in creating <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">general-purpose</em> representations is less clear.
Does human alignment improve model performance by leveraging human-provided labels as an inductive bias, or compromise the model’s original representational power by diverting it towards a separate task? Recent work suggests that the conclusions are nuanced: naively incorporating human perceptual knowledge into model finetuning can distort representations, requiring strong regularization to maintain downstream performance while increasing the representations’ interpretability and alignment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib42" title="">42</a>]</cite>. Furthermore, objective function and training data appear to matter more than architecture or model size for aligning to perceptual judgments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib41" title="">41</a>]</cite>. The nature of human preference labels also plays a crucial role, as different types of labels produce distinct learning signals. For instance, fine-tuning vision backbones with mid-level perceptual similarity judgments yields representations well-suited for image retrieval tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite>, while low-level similarity labels are more effective for image reconstruction losses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">It is clear from this body of work that careful perceptual alignment helps with specific tasks, however the effect of these adjustments on the models’ representation spaces is less well understood. That is, are models aligned to perceptual judgments only better at specific tasks – such as predicting image similarity – or do they, like humans, actually have better general-purpose representations?
Here, we evaluate the usefulness of human-aligned representations not just in predicting perceptual judgments but also on standard vision benchmarks requiring diverse notions of visual understanding.
We finetune several state-of-the-art models – including CLIP, DINO, DINOv2, and SynCLR – on NIGHTS, a dataset of human similarity judgments over synthetic image triplets <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite>, and evaluate the finetuned models on standard tasks.
Our experiments suggest that these human-aligned representations demonstrate strong gains compared to the original model backbones, even on tasks requiring skills beyond those needed during training or perceptual alignment.
We also identify limitations of human perceptual alignment, and find that finetuning can sacrifice performance on some natural data tasks in which models had strong prior performance.

In summary, our contributions are the following:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We investigate the effects of aligning pretrained vision models to human perceptual judgments on various downstream visual recognition tasks.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We find that propagating global image-level human similarity annotations to ViT patch tokens benefits downstream dense prediction tasks such as depth prediction and segmentation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We show that human-aligned representations are also beneficial in retrieval-based tasks requiring global image understanding, including retrieval-augmented generation for recent vision-language models, counting-based retrieval, and instance-based retrieval.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We ablate the effect of various human-annotated visual similarity datasets, and find that mid-level image similarity, as opposed to low-level pixel-level variations or high-level semantic associations, offers the largest improvements in generalization capabilities.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Vision backbones as learned feature extractors.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Using the intermediate activations of deep networks has been a long-standing strategy, originally used as a way of harnessing data-driven priors from models trained on fewer large datasets and repurposing them for downstream tasks where such expensive supervision is less readily available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib50" title="">50</a>]</cite>. For instance, ImageNet pretrained models have proven useful for tasks such as texture synthesis <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib19" title="">19</a>]</cite>, style-transfer <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib20" title="">20</a>]</cite>, and super-resolution <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib27" title="">27</a>]</cite>. But given the challenges of collecting large-scale labelled datasets using human annotators, other approaches such as self-supervised learning <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib44" title="">44</a>]</cite> and contrastive learning on image/alt text pairs <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib26" title="">26</a>]</cite> have since superseded supervised learning as the standard approach for training vision models. Such models have been shown to yield rich multipurpose representations that can generalize to a variety of tasks involving visual perception, scene understanding, and image generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib57" title="">57</a>]</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">View selection for self-supervised learning.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">The self-supervised, contrastive-learning objective aims to maximize the feature similarity between similar views of the data and minimize the similarity between different (negative) views. While originally the negative views are selected randomly from a pool of images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib22" title="">22</a>]</cite>, this often results in requiring large batch sizes to learn useful representations. The process of selecting these positive and negative learning examples remains an active research area. Recent approaches have suggested that alternative strategies including hard-negative mining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib47" title="">47</a>]</cite>, nearest neighbors for positive pairs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib14" title="">14</a>]</cite>, and supervised labels when available <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib28" title="">28</a>]</cite> can provide useful learning signals. Other avenues include training the contrastive framework on synthetic data from generative models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib55" title="">55</a>]</cite>, which provides an infinite data source of image variations rather than the traditional data augmentations typically used to generate alternative views.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Learning with human alignment.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Learning directly from human feedback can provide models with more targeted supervision using fewer examples <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib9" title="">9</a>]</cite>, and, thus, has been beneficial for fine-tuning large models towards specific human preferences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib62" title="">62</a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet">Ding et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib11" title="">11</a>]</cite> show that image similarity metrics, trained on human feedback, can be subsequently used for evaluating the diversity of a text-to-image model. Several datasets aim to annotate human visual preferences in image similarity, including low-level <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>]</cite> and high-level <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib23" title="">23</a>]</cite> image variations. In particular, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite> use these datasets to learn a more human-aligned perceptual metric that improves the image retrieval abilities of vision models, while <cite class="ltx_cite ltx_citemacro_citet">Muttenthaler et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib42" title="">42</a>]</cite> used THINGS for learning a linear transform on top of vision representations to increase downstream task performance and improve alignment with human similarity judgments. These annotated human preference datasets serve as useful signals for contrastive objectives, providing direct supervision for positive and negative pairings that are aligned with human decisions; here, we investigate how fine-tuning self-supervised models using these human annotations impacts model performance on various downstream tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Learning from perceptual judgments</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We propose to use the method described below as a "second pretraining stage", which aligns the feature representations from large vision models with human perceptual judgments before applying them to downstream tasks. We note that prior work on this dataset aimed to develop a model for measuring image similarity based on human judgments. Here, we investigate if pretraining on this dataset leads to a better general-purpose representation, as measured by performance on different downstream tasks.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Human Similarity Annotations</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We use the NIGHTS dataset to produce human-aligned variations of several large vision models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite>. The NIGHTS dataset consists of 20k synthetically generated image triplets, annotated with two alternative forced-choice human similarity judgments. These triplets are collected so that each has 6-10 unanimous human ratings, thus eliminating ambiguous cases where humans are likely to disagree.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">NIGHTS consists of image triplets varying in <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">mid-level</span> information. Images in a triplet roughly share the same semantic content; however, they vary in pose, layout, shape, color, and the number of objects (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2.F15" title="Figure 15 ‣ B.2 Dataset examples ‣ Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">15</span></a> in the Appendix for examples). Thus, the perceptual judgments indicate the shared visual appearance properties, as opposed to requiring higher-level semantic knowledge about the image content.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Image-level objective</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.12">Given a pre-trained backbone <math alttext="f_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑓</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">f_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, we fine-tune its parameters <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_θ</annotation></semantics></math> on a dataset of triplets <math alttext="\mathcal{D}=\{(x,\tilde{x_{0}},\tilde{x_{1}}),y\}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.5"><semantics id="S3.SS2.p1.3.m3.5a"><mrow id="S3.SS2.p1.3.m3.5.5" xref="S3.SS2.p1.3.m3.5.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.3.m3.5.5.3" xref="S3.SS2.p1.3.m3.5.5.3.cmml">𝒟</mi><mo id="S3.SS2.p1.3.m3.5.5.2" xref="S3.SS2.p1.3.m3.5.5.2.cmml">=</mo><mrow id="S3.SS2.p1.3.m3.5.5.1.1" xref="S3.SS2.p1.3.m3.5.5.1.2.cmml"><mo id="S3.SS2.p1.3.m3.5.5.1.1.2" stretchy="false" xref="S3.SS2.p1.3.m3.5.5.1.2.cmml">{</mo><mrow id="S3.SS2.p1.3.m3.5.5.1.1.1.2" xref="S3.SS2.p1.3.m3.5.5.1.1.1.1.cmml"><mo id="S3.SS2.p1.3.m3.5.5.1.1.1.2.1" stretchy="false" xref="S3.SS2.p1.3.m3.5.5.1.1.1.1.cmml">(</mo><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">x</mi><mo id="S3.SS2.p1.3.m3.5.5.1.1.1.2.2" xref="S3.SS2.p1.3.m3.5.5.1.1.1.1.cmml">,</mo><mover accent="true" id="S3.SS2.p1.3.m3.2.2" xref="S3.SS2.p1.3.m3.2.2.cmml"><msub id="S3.SS2.p1.3.m3.2.2.2" xref="S3.SS2.p1.3.m3.2.2.2.cmml"><mi id="S3.SS2.p1.3.m3.2.2.2.2" xref="S3.SS2.p1.3.m3.2.2.2.2.cmml">x</mi><mn id="S3.SS2.p1.3.m3.2.2.2.3" xref="S3.SS2.p1.3.m3.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS2.p1.3.m3.2.2.1" xref="S3.SS2.p1.3.m3.2.2.1.cmml">~</mo></mover><mo id="S3.SS2.p1.3.m3.5.5.1.1.1.2.3" xref="S3.SS2.p1.3.m3.5.5.1.1.1.1.cmml">,</mo><mover accent="true" id="S3.SS2.p1.3.m3.3.3" xref="S3.SS2.p1.3.m3.3.3.cmml"><msub id="S3.SS2.p1.3.m3.3.3.2" xref="S3.SS2.p1.3.m3.3.3.2.cmml"><mi id="S3.SS2.p1.3.m3.3.3.2.2" xref="S3.SS2.p1.3.m3.3.3.2.2.cmml">x</mi><mn id="S3.SS2.p1.3.m3.3.3.2.3" xref="S3.SS2.p1.3.m3.3.3.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.3.m3.3.3.1" xref="S3.SS2.p1.3.m3.3.3.1.cmml">~</mo></mover><mo id="S3.SS2.p1.3.m3.5.5.1.1.1.2.4" stretchy="false" xref="S3.SS2.p1.3.m3.5.5.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS2.p1.3.m3.5.5.1.1.3" xref="S3.SS2.p1.3.m3.5.5.1.2.cmml">,</mo><mi id="S3.SS2.p1.3.m3.4.4" xref="S3.SS2.p1.3.m3.4.4.cmml">y</mi><mo id="S3.SS2.p1.3.m3.5.5.1.1.4" stretchy="false" xref="S3.SS2.p1.3.m3.5.5.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.5b"><apply id="S3.SS2.p1.3.m3.5.5.cmml" xref="S3.SS2.p1.3.m3.5.5"><eq id="S3.SS2.p1.3.m3.5.5.2.cmml" xref="S3.SS2.p1.3.m3.5.5.2"></eq><ci id="S3.SS2.p1.3.m3.5.5.3.cmml" xref="S3.SS2.p1.3.m3.5.5.3">𝒟</ci><set id="S3.SS2.p1.3.m3.5.5.1.2.cmml" xref="S3.SS2.p1.3.m3.5.5.1.1"><vector id="S3.SS2.p1.3.m3.5.5.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.5.5.1.1.1.2"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑥</ci><apply id="S3.SS2.p1.3.m3.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2"><ci id="S3.SS2.p1.3.m3.2.2.1.cmml" xref="S3.SS2.p1.3.m3.2.2.1">~</ci><apply id="S3.SS2.p1.3.m3.2.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.2.2.2.1.cmml" xref="S3.SS2.p1.3.m3.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.2.2.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2.2.2">𝑥</ci><cn id="S3.SS2.p1.3.m3.2.2.2.3.cmml" type="integer" xref="S3.SS2.p1.3.m3.2.2.2.3">0</cn></apply></apply><apply id="S3.SS2.p1.3.m3.3.3.cmml" xref="S3.SS2.p1.3.m3.3.3"><ci id="S3.SS2.p1.3.m3.3.3.1.cmml" xref="S3.SS2.p1.3.m3.3.3.1">~</ci><apply id="S3.SS2.p1.3.m3.3.3.2.cmml" xref="S3.SS2.p1.3.m3.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.3.3.2.1.cmml" xref="S3.SS2.p1.3.m3.3.3.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.3.3.2.2.cmml" xref="S3.SS2.p1.3.m3.3.3.2.2">𝑥</ci><cn id="S3.SS2.p1.3.m3.3.3.2.3.cmml" type="integer" xref="S3.SS2.p1.3.m3.3.3.2.3">1</cn></apply></apply></vector><ci id="S3.SS2.p1.3.m3.4.4.cmml" xref="S3.SS2.p1.3.m3.4.4">𝑦</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.5c">\mathcal{D}=\{(x,\tilde{x_{0}},\tilde{x_{1}}),y\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.5d">caligraphic_D = { ( italic_x , over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG , over~ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ) , italic_y }</annotation></semantics></math>, where <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_x</annotation></semantics></math> denotes a reference image, and <math alttext="\tilde{x_{0}}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mover accent="true" id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><msub id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2.2" xref="S3.SS2.p1.5.m5.1.1.2.2.cmml">x</mi><mn id="S3.SS2.p1.5.m5.1.1.2.3" xref="S3.SS2.p1.5.m5.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS2.p1.5.m5.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><ci id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1">~</ci><apply id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.2.1.cmml" xref="S3.SS2.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2.2">𝑥</ci><cn id="S3.SS2.p1.5.m5.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.5.m5.1.1.2.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">\tilde{x_{0}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> and <math alttext="\tilde{x_{1}}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mover accent="true" id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><msub id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2.2" xref="S3.SS2.p1.6.m6.1.1.2.2.cmml">x</mi><mn id="S3.SS2.p1.6.m6.1.1.2.3" xref="S3.SS2.p1.6.m6.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.6.m6.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><ci id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1">~</ci><apply id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2.2">𝑥</ci><cn id="S3.SS2.p1.6.m6.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.6.m6.1.1.2.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">\tilde{x_{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">over~ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> denote two variation images. The judgement <math alttext="y\in\{0,1\}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.2"><semantics id="S3.SS2.p1.7.m7.2a"><mrow id="S3.SS2.p1.7.m7.2.3" xref="S3.SS2.p1.7.m7.2.3.cmml"><mi id="S3.SS2.p1.7.m7.2.3.2" xref="S3.SS2.p1.7.m7.2.3.2.cmml">y</mi><mo id="S3.SS2.p1.7.m7.2.3.1" xref="S3.SS2.p1.7.m7.2.3.1.cmml">∈</mo><mrow id="S3.SS2.p1.7.m7.2.3.3.2" xref="S3.SS2.p1.7.m7.2.3.3.1.cmml"><mo id="S3.SS2.p1.7.m7.2.3.3.2.1" stretchy="false" xref="S3.SS2.p1.7.m7.2.3.3.1.cmml">{</mo><mn id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">0</mn><mo id="S3.SS2.p1.7.m7.2.3.3.2.2" xref="S3.SS2.p1.7.m7.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p1.7.m7.2.2" xref="S3.SS2.p1.7.m7.2.2.cmml">1</mn><mo id="S3.SS2.p1.7.m7.2.3.3.2.3" stretchy="false" xref="S3.SS2.p1.7.m7.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.2b"><apply id="S3.SS2.p1.7.m7.2.3.cmml" xref="S3.SS2.p1.7.m7.2.3"><in id="S3.SS2.p1.7.m7.2.3.1.cmml" xref="S3.SS2.p1.7.m7.2.3.1"></in><ci id="S3.SS2.p1.7.m7.2.3.2.cmml" xref="S3.SS2.p1.7.m7.2.3.2">𝑦</ci><set id="S3.SS2.p1.7.m7.2.3.3.1.cmml" xref="S3.SS2.p1.7.m7.2.3.3.2"><cn id="S3.SS2.p1.7.m7.1.1.cmml" type="integer" xref="S3.SS2.p1.7.m7.1.1">0</cn><cn id="S3.SS2.p1.7.m7.2.2.cmml" type="integer" xref="S3.SS2.p1.7.m7.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.2c">y\in\{0,1\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.2d">italic_y ∈ { 0 , 1 }</annotation></semantics></math> indicates which of <math alttext="\tilde{x_{0}}" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1"><semantics id="S3.SS2.p1.8.m8.1a"><mover accent="true" id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><msub id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml"><mi id="S3.SS2.p1.8.m8.1.1.2.2" xref="S3.SS2.p1.8.m8.1.1.2.2.cmml">x</mi><mn id="S3.SS2.p1.8.m8.1.1.2.3" xref="S3.SS2.p1.8.m8.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS2.p1.8.m8.1.1.1" xref="S3.SS2.p1.8.m8.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><ci id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1.1">~</ci><apply id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.2.1.cmml" xref="S3.SS2.p1.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.2.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2.2">𝑥</ci><cn id="S3.SS2.p1.8.m8.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.8.m8.1.1.2.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">\tilde{x_{0}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m8.1d">over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> and <math alttext="\tilde{x_{1}}" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m9.1"><semantics id="S3.SS2.p1.9.m9.1a"><mover accent="true" id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml"><msub id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml"><mi id="S3.SS2.p1.9.m9.1.1.2.2" xref="S3.SS2.p1.9.m9.1.1.2.2.cmml">x</mi><mn id="S3.SS2.p1.9.m9.1.1.2.3" xref="S3.SS2.p1.9.m9.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.9.m9.1.1.1" xref="S3.SS2.p1.9.m9.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1"><ci id="S3.SS2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1.1">~</ci><apply id="S3.SS2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m9.1.1.2.1.cmml" xref="S3.SS2.p1.9.m9.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.9.m9.1.1.2.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2.2">𝑥</ci><cn id="S3.SS2.p1.9.m9.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.9.m9.1.1.2.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">\tilde{x_{1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.9.m9.1d">over~ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> is more similar to <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m10.1"><semantics id="S3.SS2.p1.10.m10.1a"><mi id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><ci id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.10.m10.1d">italic_x</annotation></semantics></math>. We measure distance (dissimilarity) between two images <math alttext="(x,\tilde{x_{0}})" class="ltx_Math" display="inline" id="S3.SS2.p1.11.m11.2"><semantics id="S3.SS2.p1.11.m11.2a"><mrow id="S3.SS2.p1.11.m11.2.3.2" xref="S3.SS2.p1.11.m11.2.3.1.cmml"><mo id="S3.SS2.p1.11.m11.2.3.2.1" stretchy="false" xref="S3.SS2.p1.11.m11.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">x</mi><mo id="S3.SS2.p1.11.m11.2.3.2.2" xref="S3.SS2.p1.11.m11.2.3.1.cmml">,</mo><mover accent="true" id="S3.SS2.p1.11.m11.2.2" xref="S3.SS2.p1.11.m11.2.2.cmml"><msub id="S3.SS2.p1.11.m11.2.2.2" xref="S3.SS2.p1.11.m11.2.2.2.cmml"><mi id="S3.SS2.p1.11.m11.2.2.2.2" xref="S3.SS2.p1.11.m11.2.2.2.2.cmml">x</mi><mn id="S3.SS2.p1.11.m11.2.2.2.3" xref="S3.SS2.p1.11.m11.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS2.p1.11.m11.2.2.1" xref="S3.SS2.p1.11.m11.2.2.1.cmml">~</mo></mover><mo id="S3.SS2.p1.11.m11.2.3.2.3" stretchy="false" xref="S3.SS2.p1.11.m11.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.2b"><interval closure="open" id="S3.SS2.p1.11.m11.2.3.1.cmml" xref="S3.SS2.p1.11.m11.2.3.2"><ci id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">𝑥</ci><apply id="S3.SS2.p1.11.m11.2.2.cmml" xref="S3.SS2.p1.11.m11.2.2"><ci id="S3.SS2.p1.11.m11.2.2.1.cmml" xref="S3.SS2.p1.11.m11.2.2.1">~</ci><apply id="S3.SS2.p1.11.m11.2.2.2.cmml" xref="S3.SS2.p1.11.m11.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m11.2.2.2.1.cmml" xref="S3.SS2.p1.11.m11.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.11.m11.2.2.2.2.cmml" xref="S3.SS2.p1.11.m11.2.2.2.2">𝑥</ci><cn id="S3.SS2.p1.11.m11.2.2.2.3.cmml" type="integer" xref="S3.SS2.p1.11.m11.2.2.2.3">0</cn></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.2c">(x,\tilde{x_{0}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.11.m11.2d">( italic_x , over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG )</annotation></semantics></math> using the cosine distance between their respective image features <math alttext="(f_{\theta}(x),f_{\theta}(\tilde{x_{0}}))" class="ltx_Math" display="inline" id="S3.SS2.p1.12.m12.4"><semantics id="S3.SS2.p1.12.m12.4a"><mrow id="S3.SS2.p1.12.m12.4.4.2" xref="S3.SS2.p1.12.m12.4.4.3.cmml"><mo id="S3.SS2.p1.12.m12.4.4.2.3" stretchy="false" xref="S3.SS2.p1.12.m12.4.4.3.cmml">(</mo><mrow id="S3.SS2.p1.12.m12.3.3.1.1" xref="S3.SS2.p1.12.m12.3.3.1.1.cmml"><msub id="S3.SS2.p1.12.m12.3.3.1.1.2" xref="S3.SS2.p1.12.m12.3.3.1.1.2.cmml"><mi id="S3.SS2.p1.12.m12.3.3.1.1.2.2" xref="S3.SS2.p1.12.m12.3.3.1.1.2.2.cmml">f</mi><mi id="S3.SS2.p1.12.m12.3.3.1.1.2.3" xref="S3.SS2.p1.12.m12.3.3.1.1.2.3.cmml">θ</mi></msub><mo id="S3.SS2.p1.12.m12.3.3.1.1.1" xref="S3.SS2.p1.12.m12.3.3.1.1.1.cmml">⁢</mo><mrow id="S3.SS2.p1.12.m12.3.3.1.1.3.2" xref="S3.SS2.p1.12.m12.3.3.1.1.cmml"><mo id="S3.SS2.p1.12.m12.3.3.1.1.3.2.1" stretchy="false" xref="S3.SS2.p1.12.m12.3.3.1.1.cmml">(</mo><mi id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml">x</mi><mo id="S3.SS2.p1.12.m12.3.3.1.1.3.2.2" stretchy="false" xref="S3.SS2.p1.12.m12.3.3.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.12.m12.4.4.2.4" xref="S3.SS2.p1.12.m12.4.4.3.cmml">,</mo><mrow id="S3.SS2.p1.12.m12.4.4.2.2" xref="S3.SS2.p1.12.m12.4.4.2.2.cmml"><msub id="S3.SS2.p1.12.m12.4.4.2.2.2" xref="S3.SS2.p1.12.m12.4.4.2.2.2.cmml"><mi id="S3.SS2.p1.12.m12.4.4.2.2.2.2" xref="S3.SS2.p1.12.m12.4.4.2.2.2.2.cmml">f</mi><mi id="S3.SS2.p1.12.m12.4.4.2.2.2.3" xref="S3.SS2.p1.12.m12.4.4.2.2.2.3.cmml">θ</mi></msub><mo id="S3.SS2.p1.12.m12.4.4.2.2.1" xref="S3.SS2.p1.12.m12.4.4.2.2.1.cmml">⁢</mo><mrow id="S3.SS2.p1.12.m12.4.4.2.2.3.2" xref="S3.SS2.p1.12.m12.2.2.cmml"><mo id="S3.SS2.p1.12.m12.4.4.2.2.3.2.1" stretchy="false" xref="S3.SS2.p1.12.m12.2.2.cmml">(</mo><mover accent="true" id="S3.SS2.p1.12.m12.2.2" xref="S3.SS2.p1.12.m12.2.2.cmml"><msub id="S3.SS2.p1.12.m12.2.2.2" xref="S3.SS2.p1.12.m12.2.2.2.cmml"><mi id="S3.SS2.p1.12.m12.2.2.2.2" xref="S3.SS2.p1.12.m12.2.2.2.2.cmml">x</mi><mn id="S3.SS2.p1.12.m12.2.2.2.3" xref="S3.SS2.p1.12.m12.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS2.p1.12.m12.2.2.1" xref="S3.SS2.p1.12.m12.2.2.1.cmml">~</mo></mover><mo id="S3.SS2.p1.12.m12.4.4.2.2.3.2.2" stretchy="false" xref="S3.SS2.p1.12.m12.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.12.m12.4.4.2.5" stretchy="false" xref="S3.SS2.p1.12.m12.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.4b"><interval closure="open" id="S3.SS2.p1.12.m12.4.4.3.cmml" xref="S3.SS2.p1.12.m12.4.4.2"><apply id="S3.SS2.p1.12.m12.3.3.1.1.cmml" xref="S3.SS2.p1.12.m12.3.3.1.1"><times id="S3.SS2.p1.12.m12.3.3.1.1.1.cmml" xref="S3.SS2.p1.12.m12.3.3.1.1.1"></times><apply id="S3.SS2.p1.12.m12.3.3.1.1.2.cmml" xref="S3.SS2.p1.12.m12.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m12.3.3.1.1.2.1.cmml" xref="S3.SS2.p1.12.m12.3.3.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.12.m12.3.3.1.1.2.2.cmml" xref="S3.SS2.p1.12.m12.3.3.1.1.2.2">𝑓</ci><ci id="S3.SS2.p1.12.m12.3.3.1.1.2.3.cmml" xref="S3.SS2.p1.12.m12.3.3.1.1.2.3">𝜃</ci></apply><ci id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">𝑥</ci></apply><apply id="S3.SS2.p1.12.m12.4.4.2.2.cmml" xref="S3.SS2.p1.12.m12.4.4.2.2"><times id="S3.SS2.p1.12.m12.4.4.2.2.1.cmml" xref="S3.SS2.p1.12.m12.4.4.2.2.1"></times><apply id="S3.SS2.p1.12.m12.4.4.2.2.2.cmml" xref="S3.SS2.p1.12.m12.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m12.4.4.2.2.2.1.cmml" xref="S3.SS2.p1.12.m12.4.4.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.12.m12.4.4.2.2.2.2.cmml" xref="S3.SS2.p1.12.m12.4.4.2.2.2.2">𝑓</ci><ci id="S3.SS2.p1.12.m12.4.4.2.2.2.3.cmml" xref="S3.SS2.p1.12.m12.4.4.2.2.2.3">𝜃</ci></apply><apply id="S3.SS2.p1.12.m12.2.2.cmml" xref="S3.SS2.p1.12.m12.4.4.2.2.3.2"><ci id="S3.SS2.p1.12.m12.2.2.1.cmml" xref="S3.SS2.p1.12.m12.2.2.1">~</ci><apply id="S3.SS2.p1.12.m12.2.2.2.cmml" xref="S3.SS2.p1.12.m12.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m12.2.2.2.1.cmml" xref="S3.SS2.p1.12.m12.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.12.m12.2.2.2.2.cmml" xref="S3.SS2.p1.12.m12.2.2.2.2">𝑥</ci><cn id="S3.SS2.p1.12.m12.2.2.2.3.cmml" type="integer" xref="S3.SS2.p1.12.m12.2.2.2.3">0</cn></apply></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.4c">(f_{\theta}(x),f_{\theta}(\tilde{x_{0}}))</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.12.m12.4d">( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) , italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ) )</annotation></semantics></math>, which is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(x,\tilde{x_{0}})=1-\frac{f_{\theta}(x)\cdot f_{\theta}(\tilde{x_{0}})}{|f_{%
\theta}(x)||f_{\theta}(\tilde{x_{0}})|}." class="ltx_Math" display="block" id="S3.E1.m1.9"><semantics id="S3.E1.m1.9a"><mrow id="S3.E1.m1.9.9.1" xref="S3.E1.m1.9.9.1.1.cmml"><mrow id="S3.E1.m1.9.9.1.1" xref="S3.E1.m1.9.9.1.1.cmml"><mrow id="S3.E1.m1.9.9.1.1.2" xref="S3.E1.m1.9.9.1.1.2.cmml"><mi id="S3.E1.m1.9.9.1.1.2.2" xref="S3.E1.m1.9.9.1.1.2.2.cmml">d</mi><mo id="S3.E1.m1.9.9.1.1.2.1" xref="S3.E1.m1.9.9.1.1.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.9.9.1.1.2.3.2" xref="S3.E1.m1.9.9.1.1.2.3.1.cmml"><mo id="S3.E1.m1.9.9.1.1.2.3.2.1" stretchy="false" xref="S3.E1.m1.9.9.1.1.2.3.1.cmml">(</mo><mi id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml">x</mi><mo id="S3.E1.m1.9.9.1.1.2.3.2.2" xref="S3.E1.m1.9.9.1.1.2.3.1.cmml">,</mo><mover accent="true" id="S3.E1.m1.8.8" xref="S3.E1.m1.8.8.cmml"><msub id="S3.E1.m1.8.8.2" xref="S3.E1.m1.8.8.2.cmml"><mi id="S3.E1.m1.8.8.2.2" xref="S3.E1.m1.8.8.2.2.cmml">x</mi><mn id="S3.E1.m1.8.8.2.3" xref="S3.E1.m1.8.8.2.3.cmml">0</mn></msub><mo id="S3.E1.m1.8.8.1" xref="S3.E1.m1.8.8.1.cmml">~</mo></mover><mo id="S3.E1.m1.9.9.1.1.2.3.2.3" stretchy="false" xref="S3.E1.m1.9.9.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.9.9.1.1.1" xref="S3.E1.m1.9.9.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.9.9.1.1.3" xref="S3.E1.m1.9.9.1.1.3.cmml"><mn id="S3.E1.m1.9.9.1.1.3.2" xref="S3.E1.m1.9.9.1.1.3.2.cmml">1</mn><mo id="S3.E1.m1.9.9.1.1.3.1" xref="S3.E1.m1.9.9.1.1.3.1.cmml">−</mo><mfrac id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml"><mrow id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.2.cmml"><msub id="S3.E1.m1.2.2.2.4.2.2" xref="S3.E1.m1.2.2.2.4.2.2.cmml"><mi id="S3.E1.m1.2.2.2.4.2.2.2" xref="S3.E1.m1.2.2.2.4.2.2.2.cmml">f</mi><mi id="S3.E1.m1.2.2.2.4.2.2.3" xref="S3.E1.m1.2.2.2.4.2.2.3.cmml">θ</mi></msub><mo id="S3.E1.m1.2.2.2.4.2.1" xref="S3.E1.m1.2.2.2.4.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.2.4.2.3.2" xref="S3.E1.m1.2.2.2.4.2.cmml"><mo id="S3.E1.m1.2.2.2.4.2.3.2.1" stretchy="false" xref="S3.E1.m1.2.2.2.4.2.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">x</mi><mo id="S3.E1.m1.2.2.2.4.2.3.2.2" rspace="0.055em" stretchy="false" xref="S3.E1.m1.2.2.2.4.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2.4.1" rspace="0.222em" xref="S3.E1.m1.2.2.2.4.1.cmml">⋅</mo><msub id="S3.E1.m1.2.2.2.4.3" xref="S3.E1.m1.2.2.2.4.3.cmml"><mi id="S3.E1.m1.2.2.2.4.3.2" xref="S3.E1.m1.2.2.2.4.3.2.cmml">f</mi><mi id="S3.E1.m1.2.2.2.4.3.3" xref="S3.E1.m1.2.2.2.4.3.3.cmml">θ</mi></msub></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.2.5.2" xref="S3.E1.m1.2.2.2.2.cmml"><mo id="S3.E1.m1.2.2.2.5.2.1" stretchy="false" xref="S3.E1.m1.2.2.2.2.cmml">(</mo><mover accent="true" id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml"><msub id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml">x</mi><mn id="S3.E1.m1.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.3.cmml">0</mn></msub><mo id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.cmml">~</mo></mover><mo id="S3.E1.m1.2.2.2.5.2.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.6.6.6" xref="S3.E1.m1.6.6.6.cmml"><mrow id="S3.E1.m1.5.5.5.3.1" xref="S3.E1.m1.5.5.5.3.2.cmml"><mo id="S3.E1.m1.5.5.5.3.1.2" stretchy="false" xref="S3.E1.m1.5.5.5.3.2.1.cmml">|</mo><mrow id="S3.E1.m1.5.5.5.3.1.1" xref="S3.E1.m1.5.5.5.3.1.1.cmml"><msub id="S3.E1.m1.5.5.5.3.1.1.2" xref="S3.E1.m1.5.5.5.3.1.1.2.cmml"><mi id="S3.E1.m1.5.5.5.3.1.1.2.2" xref="S3.E1.m1.5.5.5.3.1.1.2.2.cmml">f</mi><mi id="S3.E1.m1.5.5.5.3.1.1.2.3" xref="S3.E1.m1.5.5.5.3.1.1.2.3.cmml">θ</mi></msub><mo id="S3.E1.m1.5.5.5.3.1.1.1" xref="S3.E1.m1.5.5.5.3.1.1.1.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.5.3.1.1.3.2" xref="S3.E1.m1.5.5.5.3.1.1.cmml"><mo id="S3.E1.m1.5.5.5.3.1.1.3.2.1" stretchy="false" xref="S3.E1.m1.5.5.5.3.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">x</mi><mo id="S3.E1.m1.5.5.5.3.1.1.3.2.2" stretchy="false" xref="S3.E1.m1.5.5.5.3.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.5.3.1.3" stretchy="false" xref="S3.E1.m1.5.5.5.3.2.1.cmml">|</mo></mrow><mo id="S3.E1.m1.6.6.6.5" xref="S3.E1.m1.6.6.6.5.cmml">⁢</mo><mrow id="S3.E1.m1.6.6.6.4.1" xref="S3.E1.m1.6.6.6.4.2.cmml"><mo id="S3.E1.m1.6.6.6.4.1.2" stretchy="false" xref="S3.E1.m1.6.6.6.4.2.1.cmml">|</mo><mrow id="S3.E1.m1.6.6.6.4.1.1" xref="S3.E1.m1.6.6.6.4.1.1.cmml"><msub id="S3.E1.m1.6.6.6.4.1.1.2" xref="S3.E1.m1.6.6.6.4.1.1.2.cmml"><mi id="S3.E1.m1.6.6.6.4.1.1.2.2" xref="S3.E1.m1.6.6.6.4.1.1.2.2.cmml">f</mi><mi id="S3.E1.m1.6.6.6.4.1.1.2.3" xref="S3.E1.m1.6.6.6.4.1.1.2.3.cmml">θ</mi></msub><mo id="S3.E1.m1.6.6.6.4.1.1.1" xref="S3.E1.m1.6.6.6.4.1.1.1.cmml">⁢</mo><mrow id="S3.E1.m1.6.6.6.4.1.1.3.2" xref="S3.E1.m1.4.4.4.2.cmml"><mo id="S3.E1.m1.6.6.6.4.1.1.3.2.1" stretchy="false" xref="S3.E1.m1.4.4.4.2.cmml">(</mo><mover accent="true" id="S3.E1.m1.4.4.4.2" xref="S3.E1.m1.4.4.4.2.cmml"><msub id="S3.E1.m1.4.4.4.2.2" xref="S3.E1.m1.4.4.4.2.2.cmml"><mi id="S3.E1.m1.4.4.4.2.2.2" xref="S3.E1.m1.4.4.4.2.2.2.cmml">x</mi><mn id="S3.E1.m1.4.4.4.2.2.3" xref="S3.E1.m1.4.4.4.2.2.3.cmml">0</mn></msub><mo id="S3.E1.m1.4.4.4.2.1" xref="S3.E1.m1.4.4.4.2.1.cmml">~</mo></mover><mo id="S3.E1.m1.6.6.6.4.1.1.3.2.2" stretchy="false" xref="S3.E1.m1.4.4.4.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.6.4.1.3" stretchy="false" xref="S3.E1.m1.6.6.6.4.2.1.cmml">|</mo></mrow></mrow></mfrac></mrow></mrow><mo id="S3.E1.m1.9.9.1.2" lspace="0em" xref="S3.E1.m1.9.9.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.9b"><apply id="S3.E1.m1.9.9.1.1.cmml" xref="S3.E1.m1.9.9.1"><eq id="S3.E1.m1.9.9.1.1.1.cmml" xref="S3.E1.m1.9.9.1.1.1"></eq><apply id="S3.E1.m1.9.9.1.1.2.cmml" xref="S3.E1.m1.9.9.1.1.2"><times id="S3.E1.m1.9.9.1.1.2.1.cmml" xref="S3.E1.m1.9.9.1.1.2.1"></times><ci id="S3.E1.m1.9.9.1.1.2.2.cmml" xref="S3.E1.m1.9.9.1.1.2.2">𝑑</ci><interval closure="open" id="S3.E1.m1.9.9.1.1.2.3.1.cmml" xref="S3.E1.m1.9.9.1.1.2.3.2"><ci id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7">𝑥</ci><apply id="S3.E1.m1.8.8.cmml" xref="S3.E1.m1.8.8"><ci id="S3.E1.m1.8.8.1.cmml" xref="S3.E1.m1.8.8.1">~</ci><apply id="S3.E1.m1.8.8.2.cmml" xref="S3.E1.m1.8.8.2"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.2.1.cmml" xref="S3.E1.m1.8.8.2">subscript</csymbol><ci id="S3.E1.m1.8.8.2.2.cmml" xref="S3.E1.m1.8.8.2.2">𝑥</ci><cn id="S3.E1.m1.8.8.2.3.cmml" type="integer" xref="S3.E1.m1.8.8.2.3">0</cn></apply></apply></interval></apply><apply id="S3.E1.m1.9.9.1.1.3.cmml" xref="S3.E1.m1.9.9.1.1.3"><minus id="S3.E1.m1.9.9.1.1.3.1.cmml" xref="S3.E1.m1.9.9.1.1.3.1"></minus><cn id="S3.E1.m1.9.9.1.1.3.2.cmml" type="integer" xref="S3.E1.m1.9.9.1.1.3.2">1</cn><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><divide id="S3.E1.m1.6.6.7.cmml" xref="S3.E1.m1.6.6"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><apply id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4"><ci id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4.1">⋅</ci><apply id="S3.E1.m1.2.2.2.4.2.cmml" xref="S3.E1.m1.2.2.2.4.2"><times id="S3.E1.m1.2.2.2.4.2.1.cmml" xref="S3.E1.m1.2.2.2.4.2.1"></times><apply id="S3.E1.m1.2.2.2.4.2.2.cmml" xref="S3.E1.m1.2.2.2.4.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.2.2.1.cmml" xref="S3.E1.m1.2.2.2.4.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.4.2.2.2.cmml" xref="S3.E1.m1.2.2.2.4.2.2.2">𝑓</ci><ci id="S3.E1.m1.2.2.2.4.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4.2.2.3">𝜃</ci></apply><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑥</ci></apply><apply id="S3.E1.m1.2.2.2.4.3.cmml" xref="S3.E1.m1.2.2.2.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.3.1.cmml" xref="S3.E1.m1.2.2.2.4.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.4.3.2.cmml" xref="S3.E1.m1.2.2.2.4.3.2">𝑓</ci><ci id="S3.E1.m1.2.2.2.4.3.3.cmml" xref="S3.E1.m1.2.2.2.4.3.3">𝜃</ci></apply></apply><apply id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.5.2"><ci id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1">~</ci><apply id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2">𝑥</ci><cn id="S3.E1.m1.2.2.2.2.2.3.cmml" type="integer" xref="S3.E1.m1.2.2.2.2.2.3">0</cn></apply></apply></apply><apply id="S3.E1.m1.6.6.6.cmml" xref="S3.E1.m1.6.6.6"><times id="S3.E1.m1.6.6.6.5.cmml" xref="S3.E1.m1.6.6.6.5"></times><apply id="S3.E1.m1.5.5.5.3.2.cmml" xref="S3.E1.m1.5.5.5.3.1"><abs id="S3.E1.m1.5.5.5.3.2.1.cmml" xref="S3.E1.m1.5.5.5.3.1.2"></abs><apply id="S3.E1.m1.5.5.5.3.1.1.cmml" xref="S3.E1.m1.5.5.5.3.1.1"><times id="S3.E1.m1.5.5.5.3.1.1.1.cmml" xref="S3.E1.m1.5.5.5.3.1.1.1"></times><apply id="S3.E1.m1.5.5.5.3.1.1.2.cmml" xref="S3.E1.m1.5.5.5.3.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.5.3.1.1.2.1.cmml" xref="S3.E1.m1.5.5.5.3.1.1.2">subscript</csymbol><ci id="S3.E1.m1.5.5.5.3.1.1.2.2.cmml" xref="S3.E1.m1.5.5.5.3.1.1.2.2">𝑓</ci><ci id="S3.E1.m1.5.5.5.3.1.1.2.3.cmml" xref="S3.E1.m1.5.5.5.3.1.1.2.3">𝜃</ci></apply><ci id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1">𝑥</ci></apply></apply><apply id="S3.E1.m1.6.6.6.4.2.cmml" xref="S3.E1.m1.6.6.6.4.1"><abs id="S3.E1.m1.6.6.6.4.2.1.cmml" xref="S3.E1.m1.6.6.6.4.1.2"></abs><apply id="S3.E1.m1.6.6.6.4.1.1.cmml" xref="S3.E1.m1.6.6.6.4.1.1"><times id="S3.E1.m1.6.6.6.4.1.1.1.cmml" xref="S3.E1.m1.6.6.6.4.1.1.1"></times><apply id="S3.E1.m1.6.6.6.4.1.1.2.cmml" xref="S3.E1.m1.6.6.6.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.6.4.1.1.2.1.cmml" xref="S3.E1.m1.6.6.6.4.1.1.2">subscript</csymbol><ci id="S3.E1.m1.6.6.6.4.1.1.2.2.cmml" xref="S3.E1.m1.6.6.6.4.1.1.2.2">𝑓</ci><ci id="S3.E1.m1.6.6.6.4.1.1.2.3.cmml" xref="S3.E1.m1.6.6.6.4.1.1.2.3">𝜃</ci></apply><apply id="S3.E1.m1.4.4.4.2.cmml" xref="S3.E1.m1.6.6.6.4.1.1.3.2"><ci id="S3.E1.m1.4.4.4.2.1.cmml" xref="S3.E1.m1.4.4.4.2.1">~</ci><apply id="S3.E1.m1.4.4.4.2.2.cmml" xref="S3.E1.m1.4.4.4.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.2.2.1.cmml" xref="S3.E1.m1.4.4.4.2.2">subscript</csymbol><ci id="S3.E1.m1.4.4.4.2.2.2.cmml" xref="S3.E1.m1.4.4.4.2.2.2">𝑥</ci><cn id="S3.E1.m1.4.4.4.2.2.3.cmml" type="integer" xref="S3.E1.m1.4.4.4.2.2.3">0</cn></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.9c">d(x,\tilde{x_{0}})=1-\frac{f_{\theta}(x)\cdot f_{\theta}(\tilde{x_{0}})}{|f_{%
\theta}(x)||f_{\theta}(\tilde{x_{0}})|}.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.9d">italic_d ( italic_x , over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ) = 1 - divide start_ARG italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) ⋅ italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ) end_ARG start_ARG | italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) | | italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ) | end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.17">We use an alignment loss to encourage the model to match human preferences:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{alignment}}(\theta)=\max(0,m-\Delta d\cdot\bar{y})," class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml"><msub id="S3.E2.m1.4.4.1.1.3.2" xref="S3.E2.m1.4.4.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.3.2.2" xref="S3.E2.m1.4.4.1.1.3.2.2.cmml">ℒ</mi><mtext id="S3.E2.m1.4.4.1.1.3.2.3" xref="S3.E2.m1.4.4.1.1.3.2.3a.cmml">alignment</mtext></msub><mo id="S3.E2.m1.4.4.1.1.3.1" xref="S3.E2.m1.4.4.1.1.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.4.4.1.1.3.3.2" xref="S3.E2.m1.4.4.1.1.3.cmml"><mo id="S3.E2.m1.4.4.1.1.3.3.2.1" stretchy="false" xref="S3.E2.m1.4.4.1.1.3.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">θ</mi><mo id="S3.E2.m1.4.4.1.1.3.3.2.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.4.4.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">max</mi><mo id="S3.E2.m1.4.4.1.1.1.1a" xref="S3.E2.m1.4.4.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.2.cmml"><mo id="S3.E2.m1.4.4.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.2.cmml">(</mo><mn id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">0</mn><mo id="S3.E2.m1.4.4.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.2.cmml">,</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml">m</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml"><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.2" mathvariant="normal" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.2.cmml">Δ</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.3.cmml">d</mi></mrow><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml">⋅</mo><mover accent="true" id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.2.cmml">y</mi><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.1.cmml">¯</mo></mover></mrow></mrow><mo id="S3.E2.m1.4.4.1.1.1.1.1.4" stretchy="false" xref="S3.E2.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2"></eq><apply id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"><times id="S3.E2.m1.4.4.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3.1"></times><apply id="S3.E2.m1.4.4.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.2.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2">ℒ</ci><ci id="S3.E2.m1.4.4.1.1.3.2.3a.cmml" xref="S3.E2.m1.4.4.1.1.3.2.3"><mtext id="S3.E2.m1.4.4.1.1.3.2.3.cmml" mathsize="70%" xref="S3.E2.m1.4.4.1.1.3.2.3">alignment</mtext></ci></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝜃</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1"><max id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"></max><cn id="S3.E2.m1.3.3.cmml" type="integer" xref="S3.E2.m1.3.3">0</cn><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1"><minus id="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1"></minus><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2">𝑚</ci><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3"><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.1">⋅</ci><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2"><times id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.1"></times><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.2">Δ</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.3">𝑑</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.3"><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.1">¯</ci><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.2">𝑦</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\mathcal{L}_{\text{alignment}}(\theta)=\max(0,m-\Delta d\cdot\bar{y}),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">caligraphic_L start_POSTSUBSCRIPT alignment end_POSTSUBSCRIPT ( italic_θ ) = roman_max ( 0 , italic_m - roman_Δ italic_d ⋅ over¯ start_ARG italic_y end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.16">where <math alttext="\Delta d=d(x,\tilde{x_{0}})-d(x,\tilde{x_{1}})" class="ltx_Math" display="inline" id="S3.SS2.p1.13.m1.4"><semantics id="S3.SS2.p1.13.m1.4a"><mrow id="S3.SS2.p1.13.m1.4.5" xref="S3.SS2.p1.13.m1.4.5.cmml"><mrow id="S3.SS2.p1.13.m1.4.5.2" xref="S3.SS2.p1.13.m1.4.5.2.cmml"><mi id="S3.SS2.p1.13.m1.4.5.2.2" mathvariant="normal" xref="S3.SS2.p1.13.m1.4.5.2.2.cmml">Δ</mi><mo id="S3.SS2.p1.13.m1.4.5.2.1" xref="S3.SS2.p1.13.m1.4.5.2.1.cmml">⁢</mo><mi id="S3.SS2.p1.13.m1.4.5.2.3" xref="S3.SS2.p1.13.m1.4.5.2.3.cmml">d</mi></mrow><mo id="S3.SS2.p1.13.m1.4.5.1" xref="S3.SS2.p1.13.m1.4.5.1.cmml">=</mo><mrow id="S3.SS2.p1.13.m1.4.5.3" xref="S3.SS2.p1.13.m1.4.5.3.cmml"><mrow id="S3.SS2.p1.13.m1.4.5.3.2" xref="S3.SS2.p1.13.m1.4.5.3.2.cmml"><mi id="S3.SS2.p1.13.m1.4.5.3.2.2" xref="S3.SS2.p1.13.m1.4.5.3.2.2.cmml">d</mi><mo id="S3.SS2.p1.13.m1.4.5.3.2.1" xref="S3.SS2.p1.13.m1.4.5.3.2.1.cmml">⁢</mo><mrow id="S3.SS2.p1.13.m1.4.5.3.2.3.2" xref="S3.SS2.p1.13.m1.4.5.3.2.3.1.cmml"><mo id="S3.SS2.p1.13.m1.4.5.3.2.3.2.1" stretchy="false" xref="S3.SS2.p1.13.m1.4.5.3.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.13.m1.1.1" xref="S3.SS2.p1.13.m1.1.1.cmml">x</mi><mo id="S3.SS2.p1.13.m1.4.5.3.2.3.2.2" xref="S3.SS2.p1.13.m1.4.5.3.2.3.1.cmml">,</mo><mover accent="true" id="S3.SS2.p1.13.m1.2.2" xref="S3.SS2.p1.13.m1.2.2.cmml"><msub id="S3.SS2.p1.13.m1.2.2.2" xref="S3.SS2.p1.13.m1.2.2.2.cmml"><mi id="S3.SS2.p1.13.m1.2.2.2.2" xref="S3.SS2.p1.13.m1.2.2.2.2.cmml">x</mi><mn id="S3.SS2.p1.13.m1.2.2.2.3" xref="S3.SS2.p1.13.m1.2.2.2.3.cmml">0</mn></msub><mo id="S3.SS2.p1.13.m1.2.2.1" xref="S3.SS2.p1.13.m1.2.2.1.cmml">~</mo></mover><mo id="S3.SS2.p1.13.m1.4.5.3.2.3.2.3" stretchy="false" xref="S3.SS2.p1.13.m1.4.5.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.13.m1.4.5.3.1" xref="S3.SS2.p1.13.m1.4.5.3.1.cmml">−</mo><mrow id="S3.SS2.p1.13.m1.4.5.3.3" xref="S3.SS2.p1.13.m1.4.5.3.3.cmml"><mi id="S3.SS2.p1.13.m1.4.5.3.3.2" xref="S3.SS2.p1.13.m1.4.5.3.3.2.cmml">d</mi><mo id="S3.SS2.p1.13.m1.4.5.3.3.1" xref="S3.SS2.p1.13.m1.4.5.3.3.1.cmml">⁢</mo><mrow id="S3.SS2.p1.13.m1.4.5.3.3.3.2" xref="S3.SS2.p1.13.m1.4.5.3.3.3.1.cmml"><mo id="S3.SS2.p1.13.m1.4.5.3.3.3.2.1" stretchy="false" xref="S3.SS2.p1.13.m1.4.5.3.3.3.1.cmml">(</mo><mi id="S3.SS2.p1.13.m1.3.3" xref="S3.SS2.p1.13.m1.3.3.cmml">x</mi><mo id="S3.SS2.p1.13.m1.4.5.3.3.3.2.2" xref="S3.SS2.p1.13.m1.4.5.3.3.3.1.cmml">,</mo><mover accent="true" id="S3.SS2.p1.13.m1.4.4" xref="S3.SS2.p1.13.m1.4.4.cmml"><msub id="S3.SS2.p1.13.m1.4.4.2" xref="S3.SS2.p1.13.m1.4.4.2.cmml"><mi id="S3.SS2.p1.13.m1.4.4.2.2" xref="S3.SS2.p1.13.m1.4.4.2.2.cmml">x</mi><mn id="S3.SS2.p1.13.m1.4.4.2.3" xref="S3.SS2.p1.13.m1.4.4.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.13.m1.4.4.1" xref="S3.SS2.p1.13.m1.4.4.1.cmml">~</mo></mover><mo id="S3.SS2.p1.13.m1.4.5.3.3.3.2.3" stretchy="false" xref="S3.SS2.p1.13.m1.4.5.3.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m1.4b"><apply id="S3.SS2.p1.13.m1.4.5.cmml" xref="S3.SS2.p1.13.m1.4.5"><eq id="S3.SS2.p1.13.m1.4.5.1.cmml" xref="S3.SS2.p1.13.m1.4.5.1"></eq><apply id="S3.SS2.p1.13.m1.4.5.2.cmml" xref="S3.SS2.p1.13.m1.4.5.2"><times id="S3.SS2.p1.13.m1.4.5.2.1.cmml" xref="S3.SS2.p1.13.m1.4.5.2.1"></times><ci id="S3.SS2.p1.13.m1.4.5.2.2.cmml" xref="S3.SS2.p1.13.m1.4.5.2.2">Δ</ci><ci id="S3.SS2.p1.13.m1.4.5.2.3.cmml" xref="S3.SS2.p1.13.m1.4.5.2.3">𝑑</ci></apply><apply id="S3.SS2.p1.13.m1.4.5.3.cmml" xref="S3.SS2.p1.13.m1.4.5.3"><minus id="S3.SS2.p1.13.m1.4.5.3.1.cmml" xref="S3.SS2.p1.13.m1.4.5.3.1"></minus><apply id="S3.SS2.p1.13.m1.4.5.3.2.cmml" xref="S3.SS2.p1.13.m1.4.5.3.2"><times id="S3.SS2.p1.13.m1.4.5.3.2.1.cmml" xref="S3.SS2.p1.13.m1.4.5.3.2.1"></times><ci id="S3.SS2.p1.13.m1.4.5.3.2.2.cmml" xref="S3.SS2.p1.13.m1.4.5.3.2.2">𝑑</ci><interval closure="open" id="S3.SS2.p1.13.m1.4.5.3.2.3.1.cmml" xref="S3.SS2.p1.13.m1.4.5.3.2.3.2"><ci id="S3.SS2.p1.13.m1.1.1.cmml" xref="S3.SS2.p1.13.m1.1.1">𝑥</ci><apply id="S3.SS2.p1.13.m1.2.2.cmml" xref="S3.SS2.p1.13.m1.2.2"><ci id="S3.SS2.p1.13.m1.2.2.1.cmml" xref="S3.SS2.p1.13.m1.2.2.1">~</ci><apply id="S3.SS2.p1.13.m1.2.2.2.cmml" xref="S3.SS2.p1.13.m1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.13.m1.2.2.2.1.cmml" xref="S3.SS2.p1.13.m1.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.13.m1.2.2.2.2.cmml" xref="S3.SS2.p1.13.m1.2.2.2.2">𝑥</ci><cn id="S3.SS2.p1.13.m1.2.2.2.3.cmml" type="integer" xref="S3.SS2.p1.13.m1.2.2.2.3">0</cn></apply></apply></interval></apply><apply id="S3.SS2.p1.13.m1.4.5.3.3.cmml" xref="S3.SS2.p1.13.m1.4.5.3.3"><times id="S3.SS2.p1.13.m1.4.5.3.3.1.cmml" xref="S3.SS2.p1.13.m1.4.5.3.3.1"></times><ci id="S3.SS2.p1.13.m1.4.5.3.3.2.cmml" xref="S3.SS2.p1.13.m1.4.5.3.3.2">𝑑</ci><interval closure="open" id="S3.SS2.p1.13.m1.4.5.3.3.3.1.cmml" xref="S3.SS2.p1.13.m1.4.5.3.3.3.2"><ci id="S3.SS2.p1.13.m1.3.3.cmml" xref="S3.SS2.p1.13.m1.3.3">𝑥</ci><apply id="S3.SS2.p1.13.m1.4.4.cmml" xref="S3.SS2.p1.13.m1.4.4"><ci id="S3.SS2.p1.13.m1.4.4.1.cmml" xref="S3.SS2.p1.13.m1.4.4.1">~</ci><apply id="S3.SS2.p1.13.m1.4.4.2.cmml" xref="S3.SS2.p1.13.m1.4.4.2"><csymbol cd="ambiguous" id="S3.SS2.p1.13.m1.4.4.2.1.cmml" xref="S3.SS2.p1.13.m1.4.4.2">subscript</csymbol><ci id="S3.SS2.p1.13.m1.4.4.2.2.cmml" xref="S3.SS2.p1.13.m1.4.4.2.2">𝑥</ci><cn id="S3.SS2.p1.13.m1.4.4.2.3.cmml" type="integer" xref="S3.SS2.p1.13.m1.4.4.2.3">1</cn></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m1.4c">\Delta d=d(x,\tilde{x_{0}})-d(x,\tilde{x_{1}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.13.m1.4d">roman_Δ italic_d = italic_d ( italic_x , over~ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ) - italic_d ( italic_x , over~ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG )</annotation></semantics></math>, <math alttext="\bar{y}" class="ltx_Math" display="inline" id="S3.SS2.p1.14.m2.1"><semantics id="S3.SS2.p1.14.m2.1a"><mover accent="true" id="S3.SS2.p1.14.m2.1.1" xref="S3.SS2.p1.14.m2.1.1.cmml"><mi id="S3.SS2.p1.14.m2.1.1.2" xref="S3.SS2.p1.14.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.p1.14.m2.1.1.1" xref="S3.SS2.p1.14.m2.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m2.1b"><apply id="S3.SS2.p1.14.m2.1.1.cmml" xref="S3.SS2.p1.14.m2.1.1"><ci id="S3.SS2.p1.14.m2.1.1.1.cmml" xref="S3.SS2.p1.14.m2.1.1.1">¯</ci><ci id="S3.SS2.p1.14.m2.1.1.2.cmml" xref="S3.SS2.p1.14.m2.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.14.m2.1c">\bar{y}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.14.m2.1d">over¯ start_ARG italic_y end_ARG</annotation></semantics></math> maps <math alttext="y\in\{0,1\}\rightarrow\{-1,1\}" class="ltx_Math" display="inline" id="S3.SS2.p1.15.m3.4"><semantics id="S3.SS2.p1.15.m3.4a"><mrow id="S3.SS2.p1.15.m3.4.4" xref="S3.SS2.p1.15.m3.4.4.cmml"><mi id="S3.SS2.p1.15.m3.4.4.3" xref="S3.SS2.p1.15.m3.4.4.3.cmml">y</mi><mo id="S3.SS2.p1.15.m3.4.4.4" xref="S3.SS2.p1.15.m3.4.4.4.cmml">∈</mo><mrow id="S3.SS2.p1.15.m3.4.4.5.2" xref="S3.SS2.p1.15.m3.4.4.5.1.cmml"><mo id="S3.SS2.p1.15.m3.4.4.5.2.1" stretchy="false" xref="S3.SS2.p1.15.m3.4.4.5.1.cmml">{</mo><mn id="S3.SS2.p1.15.m3.1.1" xref="S3.SS2.p1.15.m3.1.1.cmml">0</mn><mo id="S3.SS2.p1.15.m3.4.4.5.2.2" xref="S3.SS2.p1.15.m3.4.4.5.1.cmml">,</mo><mn id="S3.SS2.p1.15.m3.2.2" xref="S3.SS2.p1.15.m3.2.2.cmml">1</mn><mo id="S3.SS2.p1.15.m3.4.4.5.2.3" stretchy="false" xref="S3.SS2.p1.15.m3.4.4.5.1.cmml">}</mo></mrow><mo id="S3.SS2.p1.15.m3.4.4.6" stretchy="false" xref="S3.SS2.p1.15.m3.4.4.6.cmml">→</mo><mrow id="S3.SS2.p1.15.m3.4.4.1.1" xref="S3.SS2.p1.15.m3.4.4.1.2.cmml"><mo id="S3.SS2.p1.15.m3.4.4.1.1.2" stretchy="false" xref="S3.SS2.p1.15.m3.4.4.1.2.cmml">{</mo><mrow id="S3.SS2.p1.15.m3.4.4.1.1.1" xref="S3.SS2.p1.15.m3.4.4.1.1.1.cmml"><mo id="S3.SS2.p1.15.m3.4.4.1.1.1a" xref="S3.SS2.p1.15.m3.4.4.1.1.1.cmml">−</mo><mn id="S3.SS2.p1.15.m3.4.4.1.1.1.2" xref="S3.SS2.p1.15.m3.4.4.1.1.1.2.cmml">1</mn></mrow><mo id="S3.SS2.p1.15.m3.4.4.1.1.3" xref="S3.SS2.p1.15.m3.4.4.1.2.cmml">,</mo><mn id="S3.SS2.p1.15.m3.3.3" xref="S3.SS2.p1.15.m3.3.3.cmml">1</mn><mo id="S3.SS2.p1.15.m3.4.4.1.1.4" stretchy="false" xref="S3.SS2.p1.15.m3.4.4.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.15.m3.4b"><apply id="S3.SS2.p1.15.m3.4.4.cmml" xref="S3.SS2.p1.15.m3.4.4"><and id="S3.SS2.p1.15.m3.4.4a.cmml" xref="S3.SS2.p1.15.m3.4.4"></and><apply id="S3.SS2.p1.15.m3.4.4b.cmml" xref="S3.SS2.p1.15.m3.4.4"><in id="S3.SS2.p1.15.m3.4.4.4.cmml" xref="S3.SS2.p1.15.m3.4.4.4"></in><ci id="S3.SS2.p1.15.m3.4.4.3.cmml" xref="S3.SS2.p1.15.m3.4.4.3">𝑦</ci><set id="S3.SS2.p1.15.m3.4.4.5.1.cmml" xref="S3.SS2.p1.15.m3.4.4.5.2"><cn id="S3.SS2.p1.15.m3.1.1.cmml" type="integer" xref="S3.SS2.p1.15.m3.1.1">0</cn><cn id="S3.SS2.p1.15.m3.2.2.cmml" type="integer" xref="S3.SS2.p1.15.m3.2.2">1</cn></set></apply><apply id="S3.SS2.p1.15.m3.4.4c.cmml" xref="S3.SS2.p1.15.m3.4.4"><ci id="S3.SS2.p1.15.m3.4.4.6.cmml" xref="S3.SS2.p1.15.m3.4.4.6">→</ci><share href="https://arxiv.org/html/2410.10817v1#S3.SS2.p1.15.m3.4.4.5.cmml" id="S3.SS2.p1.15.m3.4.4d.cmml" xref="S3.SS2.p1.15.m3.4.4"></share><set id="S3.SS2.p1.15.m3.4.4.1.2.cmml" xref="S3.SS2.p1.15.m3.4.4.1.1"><apply id="S3.SS2.p1.15.m3.4.4.1.1.1.cmml" xref="S3.SS2.p1.15.m3.4.4.1.1.1"><minus id="S3.SS2.p1.15.m3.4.4.1.1.1.1.cmml" xref="S3.SS2.p1.15.m3.4.4.1.1.1"></minus><cn id="S3.SS2.p1.15.m3.4.4.1.1.1.2.cmml" type="integer" xref="S3.SS2.p1.15.m3.4.4.1.1.1.2">1</cn></apply><cn id="S3.SS2.p1.15.m3.3.3.cmml" type="integer" xref="S3.SS2.p1.15.m3.3.3">1</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.15.m3.4c">y\in\{0,1\}\rightarrow\{-1,1\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.15.m3.4d">italic_y ∈ { 0 , 1 } → { - 1 , 1 }</annotation></semantics></math>, and <math alttext="m" class="ltx_Math" display="inline" id="S3.SS2.p1.16.m4.1"><semantics id="S3.SS2.p1.16.m4.1a"><mi id="S3.SS2.p1.16.m4.1.1" xref="S3.SS2.p1.16.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.16.m4.1b"><ci id="S3.SS2.p1.16.m4.1.1.cmml" xref="S3.SS2.p1.16.m4.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.16.m4.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.16.m4.1d">italic_m</annotation></semantics></math> is the margin, which we set to 0.05 following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite>. Note that this loss is equivalent to the triplet loss <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib5" title="">5</a>]</cite>, and thus minimizes the cosine distance between the representations of the more similar pair and maximizes the distance between the representations of the other pair.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Patch-level objective</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="190" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.4.2" style="font-size:90%;">Diagram of our feature extraction method when training with a patch-level objective.<span class="ltx_text ltx_font_medium" id="S3.F2.4.2.1"> Left: We extract the CLS and patch embeddings from DINO and DINOv2, perform a spatial average-pool on the patch embeddings, and concatenate [CLS, patch] vectors. Right: We train these concatenated features with a hinge loss, identical to the image-level objective.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The NIGHTS dataset contains global annotations of similarity at the image level, but the holistic label is the result of several local attributes, such as perspective, layout, foreground appearance etc. In addition to the global <math alttext="\mathrm{CLS}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">CLS</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">CLS</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\mathrm{CLS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">roman_CLS</annotation></semantics></math> token from the Vision Transformer model backbones, each model also contains a set of spatial patch embeddings. Propagating this global human annotation to individual patch tokens allows for spatial representations that are aligned with human similarity preferences. Thus, we formulate a patch alignment objective to optimize these local patch features jointly with the global image label.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The local objective only differs from the global objective in how the features are extracted. Instead of computing <math alttext="\mathcal{L}\left(\mathrm{CLS_{A}},\mathrm{CLS_{B}}\right)" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.2"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.2.2.4" xref="S3.SS3.p2.1.m1.2.2.4.cmml">ℒ</mi><mo id="S3.SS3.p2.1.m1.2.2.3" xref="S3.SS3.p2.1.m1.2.2.3.cmml">⁢</mo><mrow id="S3.SS3.p2.1.m1.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml"><mo id="S3.SS3.p2.1.m1.2.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">(</mo><msub id="S3.SS3.p2.1.m1.1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml">CLS</mi><mi id="S3.SS3.p2.1.m1.1.1.1.1.1.3" mathvariant="normal" xref="S3.SS3.p2.1.m1.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.SS3.p2.1.m1.2.2.2.2.4" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.SS3.p2.1.m1.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS3.p2.1.m1.2.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2.cmml">CLS</mi><mi id="S3.SS3.p2.1.m1.2.2.2.2.2.3" mathvariant="normal" xref="S3.SS3.p2.1.m1.2.2.2.2.2.3.cmml">B</mi></msub><mo id="S3.SS3.p2.1.m1.2.2.2.2.5" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><apply id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2"><times id="S3.SS3.p2.1.m1.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.3"></times><ci id="S3.SS3.p2.1.m1.2.2.4.cmml" xref="S3.SS3.p2.1.m1.2.2.4">ℒ</ci><interval closure="open" id="S3.SS3.p2.1.m1.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2"><apply id="S3.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2">CLS</ci><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.3">A</ci></apply><apply id="S3.SS3.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.2">CLS</ci><ci id="S3.SS3.p2.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2.3">B</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">\mathcal{L}\left(\mathrm{CLS_{A}},\mathrm{CLS_{B}}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.2d">caligraphic_L ( roman_CLS start_POSTSUBSCRIPT roman_A end_POSTSUBSCRIPT , roman_CLS start_POSTSUBSCRIPT roman_B end_POSTSUBSCRIPT )</annotation></semantics></math>, we compute</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}\left(\mathrm{cat[CLS_{A},pool(PATCH_{A})],cat[CLS_{B},pool(PATCH_{%
B})]}\right)." class="ltx_Math" display="block" id="S3.Ex1.m1.1"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.4.cmml">ℒ</mi><mo id="S3.Ex1.m1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml"><mo id="S3.Ex1.m1.1.1.1.1.2.2.3" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.4.cmml">cat</mi><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.3.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.3.cmml">[</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">CLS</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3" mathvariant="normal" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.4" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml">pool</mi><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.2" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml">(</mo><msub id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.2.cmml">PATCH</mi><mi id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.3" mathvariant="normal" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.3.cmml">A</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.5" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.3.cmml">]</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.2.2.4" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml">,</mo><mrow id="S3.Ex1.m1.1.1.1.1.2.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.2.2.4" xref="S3.Ex1.m1.1.1.1.1.2.2.2.4.cmml">cat</mi><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.3" xref="S3.Ex1.m1.1.1.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml"><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml">[</mo><msub id="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1" xref="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.2.cmml">CLS</mi><mi id="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.3" mathvariant="normal" xref="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.3.cmml">B</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.4" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml">,</mo><mrow id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.3" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.3.cmml">pool</mi><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.2" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.2.cmml">PATCH</mi><mi id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.3" mathvariant="normal" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.3.cmml">B</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.3" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.5" stretchy="false" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml">]</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.1.2.2.5" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.1.1.1.2" lspace="0em" xref="S3.Ex1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3"></times><ci id="S3.Ex1.m1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.4">ℒ</ci><interval closure="open" id="S3.Ex1.m1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2"><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1"><times id="S3.Ex1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.3"></times><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.4">cat</ci><interval closure="closed" id="S3.Ex1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2"><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2">CLS</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3">A</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2"><times id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.2"></times><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.3">pool</ci><apply id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.2">PATCH</ci><ci id="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1.1.2.2.2.1.1.1.3">A</ci></apply></apply></interval></apply><apply id="S3.Ex1.m1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2"><times id="S3.Ex1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.3"></times><ci id="S3.Ex1.m1.1.1.1.1.2.2.2.4.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.4">cat</ci><interval closure="closed" id="S3.Ex1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2"><apply id="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.2">CLS</ci><ci id="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.1.1.1.3">B</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2"><times id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.2"></times><ci id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.3">pool</ci><apply id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.2">PATCH</ci><ci id="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2.2.2.2.2.1.1.1.3">B</ci></apply></apply></interval></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\mathcal{L}\left(\mathrm{cat[CLS_{A},pool(PATCH_{A})],cat[CLS_{B},pool(PATCH_{%
B})]}\right).</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.1d">caligraphic_L ( roman_cat [ roman_CLS start_POSTSUBSCRIPT roman_A end_POSTSUBSCRIPT , roman_pool ( roman_PATCH start_POSTSUBSCRIPT roman_A end_POSTSUBSCRIPT ) ] , roman_cat [ roman_CLS start_POSTSUBSCRIPT roman_B end_POSTSUBSCRIPT , roman_pool ( roman_PATCH start_POSTSUBSCRIPT roman_B end_POSTSUBSCRIPT ) ] ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p2.11"><math alttext="\mathrm{CLS}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m1.1"><semantics id="S3.SS3.p2.2.m1.1a"><mi id="S3.SS3.p2.2.m1.1.1" xref="S3.SS3.p2.2.m1.1.1.cmml">CLS</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m1.1b"><ci id="S3.SS3.p2.2.m1.1.1.cmml" xref="S3.SS3.p2.2.m1.1.1">CLS</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m1.1c">\mathrm{CLS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m1.1d">roman_CLS</annotation></semantics></math> is of dimension <math alttext="(1,d)" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m2.2"><semantics id="S3.SS3.p2.3.m2.2a"><mrow id="S3.SS3.p2.3.m2.2.3.2" xref="S3.SS3.p2.3.m2.2.3.1.cmml"><mo id="S3.SS3.p2.3.m2.2.3.2.1" stretchy="false" xref="S3.SS3.p2.3.m2.2.3.1.cmml">(</mo><mn id="S3.SS3.p2.3.m2.1.1" xref="S3.SS3.p2.3.m2.1.1.cmml">1</mn><mo id="S3.SS3.p2.3.m2.2.3.2.2" xref="S3.SS3.p2.3.m2.2.3.1.cmml">,</mo><mi id="S3.SS3.p2.3.m2.2.2" xref="S3.SS3.p2.3.m2.2.2.cmml">d</mi><mo id="S3.SS3.p2.3.m2.2.3.2.3" stretchy="false" xref="S3.SS3.p2.3.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m2.2b"><interval closure="open" id="S3.SS3.p2.3.m2.2.3.1.cmml" xref="S3.SS3.p2.3.m2.2.3.2"><cn id="S3.SS3.p2.3.m2.1.1.cmml" type="integer" xref="S3.SS3.p2.3.m2.1.1">1</cn><ci id="S3.SS3.p2.3.m2.2.2.cmml" xref="S3.SS3.p2.3.m2.2.2">𝑑</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m2.2c">(1,d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m2.2d">( 1 , italic_d )</annotation></semantics></math> and <math alttext="\mathrm{PATCH}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m3.1"><semantics id="S3.SS3.p2.4.m3.1a"><mi id="S3.SS3.p2.4.m3.1.1" xref="S3.SS3.p2.4.m3.1.1.cmml">PATCH</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m3.1b"><ci id="S3.SS3.p2.4.m3.1.1.cmml" xref="S3.SS3.p2.4.m3.1.1">PATCH</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m3.1c">\mathrm{PATCH}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m3.1d">roman_PATCH</annotation></semantics></math> is <math alttext="(s,s,d)" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m4.3"><semantics id="S3.SS3.p2.5.m4.3a"><mrow id="S3.SS3.p2.5.m4.3.4.2" xref="S3.SS3.p2.5.m4.3.4.1.cmml"><mo id="S3.SS3.p2.5.m4.3.4.2.1" stretchy="false" xref="S3.SS3.p2.5.m4.3.4.1.cmml">(</mo><mi id="S3.SS3.p2.5.m4.1.1" xref="S3.SS3.p2.5.m4.1.1.cmml">s</mi><mo id="S3.SS3.p2.5.m4.3.4.2.2" xref="S3.SS3.p2.5.m4.3.4.1.cmml">,</mo><mi id="S3.SS3.p2.5.m4.2.2" xref="S3.SS3.p2.5.m4.2.2.cmml">s</mi><mo id="S3.SS3.p2.5.m4.3.4.2.3" xref="S3.SS3.p2.5.m4.3.4.1.cmml">,</mo><mi id="S3.SS3.p2.5.m4.3.3" xref="S3.SS3.p2.5.m4.3.3.cmml">d</mi><mo id="S3.SS3.p2.5.m4.3.4.2.4" stretchy="false" xref="S3.SS3.p2.5.m4.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m4.3b"><vector id="S3.SS3.p2.5.m4.3.4.1.cmml" xref="S3.SS3.p2.5.m4.3.4.2"><ci id="S3.SS3.p2.5.m4.1.1.cmml" xref="S3.SS3.p2.5.m4.1.1">𝑠</ci><ci id="S3.SS3.p2.5.m4.2.2.cmml" xref="S3.SS3.p2.5.m4.2.2">𝑠</ci><ci id="S3.SS3.p2.5.m4.3.3.cmml" xref="S3.SS3.p2.5.m4.3.3">𝑑</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m4.3c">(s,s,d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m4.3d">( italic_s , italic_s , italic_d )</annotation></semantics></math> where <math alttext="s" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m5.1"><semantics id="S3.SS3.p2.6.m5.1a"><mi id="S3.SS3.p2.6.m5.1.1" xref="S3.SS3.p2.6.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m5.1b"><ci id="S3.SS3.p2.6.m5.1.1.cmml" xref="S3.SS3.p2.6.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m5.1d">italic_s</annotation></semantics></math> is the number of patches along each spatial dimension. We spatially average the patch tokens to get dimension <math alttext="(1,d)" class="ltx_Math" display="inline" id="S3.SS3.p2.7.m6.2"><semantics id="S3.SS3.p2.7.m6.2a"><mrow id="S3.SS3.p2.7.m6.2.3.2" xref="S3.SS3.p2.7.m6.2.3.1.cmml"><mo id="S3.SS3.p2.7.m6.2.3.2.1" stretchy="false" xref="S3.SS3.p2.7.m6.2.3.1.cmml">(</mo><mn id="S3.SS3.p2.7.m6.1.1" xref="S3.SS3.p2.7.m6.1.1.cmml">1</mn><mo id="S3.SS3.p2.7.m6.2.3.2.2" xref="S3.SS3.p2.7.m6.2.3.1.cmml">,</mo><mi id="S3.SS3.p2.7.m6.2.2" xref="S3.SS3.p2.7.m6.2.2.cmml">d</mi><mo id="S3.SS3.p2.7.m6.2.3.2.3" stretchy="false" xref="S3.SS3.p2.7.m6.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m6.2b"><interval closure="open" id="S3.SS3.p2.7.m6.2.3.1.cmml" xref="S3.SS3.p2.7.m6.2.3.2"><cn id="S3.SS3.p2.7.m6.1.1.cmml" type="integer" xref="S3.SS3.p2.7.m6.1.1">1</cn><ci id="S3.SS3.p2.7.m6.2.2.cmml" xref="S3.SS3.p2.7.m6.2.2">𝑑</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m6.2c">(1,d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.7.m6.2d">( 1 , italic_d )</annotation></semantics></math>. We then concatenate the <math alttext="\mathrm{CLS}" class="ltx_Math" display="inline" id="S3.SS3.p2.8.m7.1"><semantics id="S3.SS3.p2.8.m7.1a"><mi id="S3.SS3.p2.8.m7.1.1" xref="S3.SS3.p2.8.m7.1.1.cmml">CLS</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m7.1b"><ci id="S3.SS3.p2.8.m7.1.1.cmml" xref="S3.SS3.p2.8.m7.1.1">CLS</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m7.1c">\mathrm{CLS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.8.m7.1d">roman_CLS</annotation></semantics></math> and pooled patch tokens to get dimension <math alttext="(1,2d)" class="ltx_Math" display="inline" id="S3.SS3.p2.9.m8.2"><semantics id="S3.SS3.p2.9.m8.2a"><mrow id="S3.SS3.p2.9.m8.2.2.1" xref="S3.SS3.p2.9.m8.2.2.2.cmml"><mo id="S3.SS3.p2.9.m8.2.2.1.2" stretchy="false" xref="S3.SS3.p2.9.m8.2.2.2.cmml">(</mo><mn id="S3.SS3.p2.9.m8.1.1" xref="S3.SS3.p2.9.m8.1.1.cmml">1</mn><mo id="S3.SS3.p2.9.m8.2.2.1.3" xref="S3.SS3.p2.9.m8.2.2.2.cmml">,</mo><mrow id="S3.SS3.p2.9.m8.2.2.1.1" xref="S3.SS3.p2.9.m8.2.2.1.1.cmml"><mn id="S3.SS3.p2.9.m8.2.2.1.1.2" xref="S3.SS3.p2.9.m8.2.2.1.1.2.cmml">2</mn><mo id="S3.SS3.p2.9.m8.2.2.1.1.1" xref="S3.SS3.p2.9.m8.2.2.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p2.9.m8.2.2.1.1.3" xref="S3.SS3.p2.9.m8.2.2.1.1.3.cmml">d</mi></mrow><mo id="S3.SS3.p2.9.m8.2.2.1.4" stretchy="false" xref="S3.SS3.p2.9.m8.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m8.2b"><interval closure="open" id="S3.SS3.p2.9.m8.2.2.2.cmml" xref="S3.SS3.p2.9.m8.2.2.1"><cn id="S3.SS3.p2.9.m8.1.1.cmml" type="integer" xref="S3.SS3.p2.9.m8.1.1">1</cn><apply id="S3.SS3.p2.9.m8.2.2.1.1.cmml" xref="S3.SS3.p2.9.m8.2.2.1.1"><times id="S3.SS3.p2.9.m8.2.2.1.1.1.cmml" xref="S3.SS3.p2.9.m8.2.2.1.1.1"></times><cn id="S3.SS3.p2.9.m8.2.2.1.1.2.cmml" type="integer" xref="S3.SS3.p2.9.m8.2.2.1.1.2">2</cn><ci id="S3.SS3.p2.9.m8.2.2.1.1.3.cmml" xref="S3.SS3.p2.9.m8.2.2.1.1.3">𝑑</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m8.2c">(1,2d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.9.m8.2d">( 1 , 2 italic_d )</annotation></semantics></math>. We fine-tune the same alignment loss (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.E2" title="In 3.2 Image-level objective ‣ 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">2</span></a>) on concatenated <math alttext="\mathrm{CLS}" class="ltx_Math" display="inline" id="S3.SS3.p2.10.m9.1"><semantics id="S3.SS3.p2.10.m9.1a"><mi id="S3.SS3.p2.10.m9.1.1" xref="S3.SS3.p2.10.m9.1.1.cmml">CLS</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m9.1b"><ci id="S3.SS3.p2.10.m9.1.1.cmml" xref="S3.SS3.p2.10.m9.1.1">CLS</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m9.1c">\mathrm{CLS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.10.m9.1d">roman_CLS</annotation></semantics></math> and averaged-pooled patch tokens, and train heads for semantic segmentation and depth estimation on the resulting patch embeddings. Note that only experiments reported in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS1" title="4.1 Dense Prediction ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4.1</span></a> use this objective, as they require local features. For all other evaluations, we exclusively use <math alttext="\mathrm{CLS}" class="ltx_Math" display="inline" id="S3.SS3.p2.11.m10.1"><semantics id="S3.SS3.p2.11.m10.1a"><mi id="S3.SS3.p2.11.m10.1.1" xref="S3.SS3.p2.11.m10.1.1.cmml">CLS</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m10.1b"><ci id="S3.SS3.p2.11.m10.1.1.cmml" xref="S3.SS3.p2.11.m10.1.1">CLS</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m10.1c">\mathrm{CLS}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.11.m10.1d">roman_CLS</annotation></semantics></math> tokens (see Eq. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.E2" title="In 3.2 Image-level objective ‣ 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">2</span></a>) if not mentioned otherwise.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Implementation details</h3>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Vision Model Backbones.</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">We fine-tune several state-of-the-art Vision Transformer <cite class="ltx_cite ltx_citemacro_citep">[VIT; <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib13" title="">13</a>]</cite> backbones including DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib4" title="">4</a>]</cite>, DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib44" title="">44</a>]</cite>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib46" title="">46</a>]</cite>, OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib8" title="">8</a>]</cite>, and SynCLR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib55" title="">55</a>]</cite>. For DINO, DINOv2, MAE, and SynCLR, we use the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS0.Px1.p1.1.1">CLS</span> token of the final layer. We extract the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS0.Px1.p1.1.2">CLS</span> token before layer norm for MAE and after it for the other backbones. For CLIP and OpenCLIP, we use the representations of the image encoder. We also experiment with concatenated features from DINO, CLIP, and OpenCLIP, used to train the DreamSim Ensemble in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite> (referred to as "ensemble" in our results). For each model, we use its base size, ViT-B.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p2.1">We implicitly ablate the usage of synthetic triplets in NIGHTS by including SynCLR in our experiments; as that backbone was contrastively trained on generated images, additional performance changes can be attributed to human perceptual alignment.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Finetuning with human preference labels.</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">We finetune each backbone using Low-Rank Adaptation (LoRA), which was found to achieve better alignment performance and efficiency than full fine-tuning in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite>. For more training and technical details see Sections <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS1" title="C.1 Training human-aligned models ‣ Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">C.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS6" title="C.6 Additional compute details ‣ Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">C.6</span></a> in the Appendix.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we evaluate perceptually-aligned backbones against base models on common vision tasks. We study global representations through instance retrieval, object-counting, and retrieval-augmented generation experiments. Additionally, we find that local patch-level representations can be improved by tuning on image-level perceptual judgments, and show performance increases on semantic segmentation and depth estimation.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dense Prediction</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Semantic segmentation.</span> Following the procedure detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.SS3" title="3.3 Patch-level objective ‣ 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">3.3</span></a> and Fig.  <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.F2" title="Figure 2 ‣ 3.3 Patch-level objective ‣ 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">2</span></a>, we LoRA-tune new backbones with perceptually-aligned CLS and patch tokens. To evaluate segmentation performance, we freeze these backbones and train a single linear layer transforming patch tokens to a segmentation map. We evaluate DINO and DINOv2 on standard segmentation benchmarks in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.T1" title="Table 1 ‣ 4.1 Dense Prediction ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">1</span></a> and show that human-aligned models boost performance in 16 out of 20 cases. Across all datasets and metrics, human-aligned DINO (denoted as DINO-HA) outperforms the base model and often achieves the highest mIoU and Pixel Accuracy (P.A.) overall. DINOv2-HA also outperforms its nonaligned counterpart on COCO and DAVIS2017.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We flag datasets already seen in the DINOv2 retrieval pretraining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib44" title="">44</a>]</cite>, such as Pascal VOC, ADE20k, and Cityscapes. If a dataset is already in-distribution for a backbone, fine-tuning on different data may be more likely to change the feature space such that that dataset is more out-of-distribution. Thus this is a potential confounding factor.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.13.13" style="width:411.9pt;height:91.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.9pt,8.6pt) scale(0.84102154436579,0.84102154436579) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.13.13.13">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.3.3.3.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.3.3.3.3.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.1.1.1.1.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1.1" style="background-color:#EDEDED;">Pascal VOC<sup class="ltx_sup" id="S4.T1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T1.1.1.1.1.1.1.1.1">∗</span></sup></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.2.2.1">ADE20k<sup class="ltx_sup" id="S4.T1.2.2.2.2.2.1.1"><span class="ltx_text ltx_font_medium" id="S4.T1.2.2.2.2.2.1.1.1">∗</span></sup></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.3.3.3.3.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.3.3.1" style="background-color:#EDEDED;">Cityscapes<sup class="ltx_sup" id="S4.T1.3.3.3.3.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.T1.3.3.3.3.3.1.1.1">∗</span></sup></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.3.3.3.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.3.5.1">COCO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.3.3.3.3.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.3.6.1" style="background-color:#EDEDED;">DAVIS2017</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.13.13.13.14.1">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.13.13.13.14.1.1"></th>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.14.1.2.1" style="background-color:#EDEDED;">mIoU</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.14.1.3.1" style="background-color:#EDEDED;">P.A.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.4">mIoU</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.5">P.A.</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.14.1.6.1" style="background-color:#EDEDED;">mIoU</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.14.1.7.1" style="background-color:#EDEDED;">P.A.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.8">mIoU</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.9">P.A.</td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.10" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.14.1.10.1" style="background-color:#EDEDED;">mIoU</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.13.13.13.14.1.11" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.14.1.11.1" style="background-color:#EDEDED;">P.A.</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.13.13.13.15.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.13.13.13.15.2.1">DINO</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.15.2.2.1" style="background-color:#EDEDED;">0.729</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.15.2.3.1" style="background-color:#EDEDED;">0.800</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.4">0.342</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.5">0.548</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.15.2.6.1" style="background-color:#EDEDED;">0.562</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.15.2.7.1" style="background-color:#EDEDED;">0.749</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.8">0.810</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.9">0.901</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.10" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.15.2.10.1" style="background-color:#EDEDED;">0.809</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.13.13.13.15.2.11" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.15.2.11.1" style="background-color:#EDEDED;">0.886</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.11.11.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.11.11.11.11.9">DINO-HA</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.4.4.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.4.1.1" style="background-color:#EDEDED;">0.745<sup class="ltx_sup" id="S4.T1.4.4.4.4.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.4.4.4.4.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.5.5.5.2" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.5.5.5.5.2.1" style="background-color:#EDEDED;">0.840<sup class="ltx_sup" id="S4.T1.5.5.5.5.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.5.5.5.5.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.11.11.11.11.10"><span class="ltx_text ltx_font_bold" id="S4.T1.11.11.11.11.10.1">0.391</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.11.11.11.11.11"><span class="ltx_text ltx_font_bold" id="S4.T1.11.11.11.11.11.1">0.602</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.6.6.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.6.6.6.3.1" style="background-color:#EDEDED;">0.573<sup class="ltx_sup" id="S4.T1.6.6.6.6.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.6.6.6.6.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.7.7.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.7.7.7.7.4.1" style="background-color:#EDEDED;">0.769<sup class="ltx_sup" id="S4.T1.7.7.7.7.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.7.7.7.7.4.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.8.8.5"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.8.8.5.1">0.816<sup class="ltx_sup" id="S4.T1.8.8.8.8.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.8.8.8.8.5.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.9.9.9.6"><span class="ltx_text ltx_font_bold" id="S4.T1.9.9.9.9.6.1">0.914<sup class="ltx_sup" id="S4.T1.9.9.9.9.6.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.9.9.9.9.6.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.10.10.7" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.10.10.7.1" style="background-color:#EDEDED;">0.818<sup class="ltx_sup" id="S4.T1.10.10.10.10.7.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.10.10.10.10.7.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.11.11.11.11.8" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.11.11.11.11.8.1" style="background-color:#EDEDED;">0.913<sup class="ltx_sup" id="S4.T1.11.11.11.11.8.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.11.11.11.11.8.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.12.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.12.12.12.12.2">DINOv2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.12.12.12.12.3.1" style="background-color:#EDEDED;">0.686</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.12.12.12.12.4.1" style="background-color:#EDEDED;">0.803</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T1.12.12.12.12.1.1">0.441<sup class="ltx_sup" id="S4.T1.12.12.12.12.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.12.12.12.12.1.1.1.1">†</span></sup></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.5">0.645</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.12.12.12.12.6.1" style="background-color:#EDEDED;">0.535</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.12.12.12.12.7.1" style="background-color:#EDEDED;">0.737</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.8">0.759</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.9">0.877</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.10" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.12.12.12.12.10.1" style="background-color:#EDEDED;">0.794</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.12.12.12.12.11" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.12.12.12.12.11.1" style="background-color:#EDEDED;">0.906</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.13.13.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.13.13.13.13.2">DINOv2-HA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.13.3.1" style="background-color:#EDEDED;">0.635</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.4" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.13.4.1" style="background-color:#EDEDED;">0.771</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.5">0.418</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.1"><span class="ltx_text ltx_font_bold" id="S4.T1.13.13.13.13.1.1">0.700<sup class="ltx_sup" id="S4.T1.13.13.13.13.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.13.13.13.13.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T1.13.13.13.13.6.1" style="background-color:#EDEDED;">0.528</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.7" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.13.13.13.13.7.1" style="background-color:#EDEDED;">0.739</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.8"><span class="ltx_text ltx_font_bold" id="S4.T1.13.13.13.13.8.1">0.761</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.9"><span class="ltx_text ltx_font_bold" id="S4.T1.13.13.13.13.9.1">0.891</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.10" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.13.13.13.13.10.1" style="background-color:#EDEDED;">0.810</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.13.13.13.13.11" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T1.13.13.13.13.11.1" style="background-color:#EDEDED;">0.908</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.17.2.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T1.15.1" style="font-size:90%;">Base and human-aligned model performance on semantic segmentation.<span class="ltx_text ltx_font_medium" id="S4.T1.15.1.1"> Aligned models largely outperform baselines, with DINO-HA achieving the highest performance across models for 4 out of 5 datasets. Note that Pascal VOC, ADE20k, and Cityscapes were included in DINOv2’s retrieval pretraining. <math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T1.15.1.1.m1.1"><semantics id="S4.T1.15.1.1.m1.1b"><mo id="S4.T1.15.1.1.m1.1.1" xref="S4.T1.15.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.1.1.m1.1c"><ci id="S4.T1.15.1.1.m1.1.1.cmml" xref="S4.T1.15.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.1.1.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.1.1.m1.1e">†</annotation></semantics></math> indicates best score in the column.</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Depth estimation.</span> We follow the evaluation protocol of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib37" title="">37</a>]</cite> and train a single linear layer on frozen patch tokens to output a depth map with values mapped into 256 uniformly-distributed bins. This head is trained with a scale-invariant log loss introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib15" title="">15</a>]</cite> and a scale-invariant gradient-matching term as described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib36" title="">36</a>]</cite>. In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.T2" title="Table 2 ‣ 4.1 Dense Prediction ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">2</span></a>, we report performance on monocular depth estimation and show that human-aligned models outperform base models in 27 out of 36 cases. Consistent with segmentation performance, human-aligned DINO outperforms the base model on all metrics across all datasets, and is often the highest-performing model overall (denoted by <math alttext="\dagger" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">†</annotation></semantics></math>). We also evaluate out-of-distribution generalization by training a depth head on NYUv2 and evaluating on the 4D Light Field dataset; combined with the performance boost even on datasets that DINOv2 was trained on (NYUv2 and SUN-RGBD), these results demonstrate that human-aligned models have strong generalization capabilities prior to any training for downstream tasks.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.49.49" style="width:390.3pt;height:268.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-41.4pt,28.3pt) scale(0.825083282499095,0.825083282499095) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.49.49.49">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6" id="S4.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1">NYUv2<sup class="ltx_sup" id="S4.T2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.1.1.1.1.1.1.1.1">∗</span></sup></span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.10.10.10.10">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.10.10.10.10.10"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.2.2.2.2.1" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.2.2.2.2.1.1" style="background-color:#EDEDED;">RMSE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.1.1.m1.1"><semantics id="S4.T2.2.2.2.2.1.1.m1.1a"><mo id="S4.T2.2.2.2.2.1.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.2.2.2.2.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.1.1.m1.1b"><ci id="S4.T2.2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.2.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.1.1.m1.1d">↓</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.3.3.3.3.2">AbsRel (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.2.m1.1"><semantics id="S4.T2.3.3.3.3.2.m1.1a"><mo id="S4.T2.3.3.3.3.2.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.3.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.2.m1.1b"><ci id="S4.T2.3.3.3.3.2.m1.1.1.cmml" xref="S4.T2.3.3.3.3.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.2.m1.1d">↓</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.4.4.4.4.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.4.4.4.4.3.1" style="background-color:#EDEDED;">log10 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.4.3.1.m1.1"><semantics id="S4.T2.4.4.4.4.3.1.m1.1a"><mo id="S4.T2.4.4.4.4.3.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.4.4.4.4.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.3.1.m1.1b"><ci id="S4.T2.4.4.4.4.3.1.m1.1.1.cmml" xref="S4.T2.4.4.4.4.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.4.3.1.m1.1d">↓</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.6.6.6.6.5">
<math alttext="\delta&gt;1.25" class="ltx_Math" display="inline" id="S4.T2.5.5.5.5.4.m1.1"><semantics id="S4.T2.5.5.5.5.4.m1.1a"><mrow id="S4.T2.5.5.5.5.4.m1.1.1" xref="S4.T2.5.5.5.5.4.m1.1.1.cmml"><mi id="S4.T2.5.5.5.5.4.m1.1.1.2" xref="S4.T2.5.5.5.5.4.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.5.5.5.5.4.m1.1.1.1" xref="S4.T2.5.5.5.5.4.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T2.5.5.5.5.4.m1.1.1.3" xref="S4.T2.5.5.5.5.4.m1.1.1.3.cmml">1.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.4.m1.1b"><apply id="S4.T2.5.5.5.5.4.m1.1.1.cmml" xref="S4.T2.5.5.5.5.4.m1.1.1"><gt id="S4.T2.5.5.5.5.4.m1.1.1.1.cmml" xref="S4.T2.5.5.5.5.4.m1.1.1.1"></gt><ci id="S4.T2.5.5.5.5.4.m1.1.1.2.cmml" xref="S4.T2.5.5.5.5.4.m1.1.1.2">𝛿</ci><cn id="S4.T2.5.5.5.5.4.m1.1.1.3.cmml" type="float" xref="S4.T2.5.5.5.5.4.m1.1.1.3">1.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.4.m1.1c">\delta&gt;1.25</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.5.4.m1.1d">italic_δ &gt; 1.25</annotation></semantics></math> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.6.6.6.6.5.m2.1"><semantics id="S4.T2.6.6.6.6.5.m2.1a"><mo id="S4.T2.6.6.6.6.5.m2.1.1" stretchy="false" xref="S4.T2.6.6.6.6.5.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.6.5.m2.1b"><ci id="S4.T2.6.6.6.6.5.m2.1.1.cmml" xref="S4.T2.6.6.6.6.5.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.6.5.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.6.5.m2.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.8.8.8.8.7" style="background-color:#EDEDED;">
<math alttext="\delta&gt;1.25^{2}" class="ltx_Math" display="inline" id="S4.T2.7.7.7.7.6.m1.1" style="background-color:#EDEDED;"><semantics id="S4.T2.7.7.7.7.6.m1.1a"><mrow id="S4.T2.7.7.7.7.6.m1.1.1" xref="S4.T2.7.7.7.7.6.m1.1.1.cmml"><mi id="S4.T2.7.7.7.7.6.m1.1.1.2" mathbackground="#EDEDED" xref="S4.T2.7.7.7.7.6.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.7.7.7.7.6.m1.1.1.1" mathbackground="#EDEDED" xref="S4.T2.7.7.7.7.6.m1.1.1.1.cmml">&gt;</mo><msup id="S4.T2.7.7.7.7.6.m1.1.1.3" xref="S4.T2.7.7.7.7.6.m1.1.1.3.cmml"><mn id="S4.T2.7.7.7.7.6.m1.1.1.3.2" mathbackground="#EDEDED" xref="S4.T2.7.7.7.7.6.m1.1.1.3.2.cmml">1.25</mn><mn id="S4.T2.7.7.7.7.6.m1.1.1.3.3" mathbackground="#EDEDED" xref="S4.T2.7.7.7.7.6.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.7.6.m1.1b"><apply id="S4.T2.7.7.7.7.6.m1.1.1.cmml" xref="S4.T2.7.7.7.7.6.m1.1.1"><gt id="S4.T2.7.7.7.7.6.m1.1.1.1.cmml" xref="S4.T2.7.7.7.7.6.m1.1.1.1"></gt><ci id="S4.T2.7.7.7.7.6.m1.1.1.2.cmml" xref="S4.T2.7.7.7.7.6.m1.1.1.2">𝛿</ci><apply id="S4.T2.7.7.7.7.6.m1.1.1.3.cmml" xref="S4.T2.7.7.7.7.6.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.7.7.7.7.6.m1.1.1.3.1.cmml" xref="S4.T2.7.7.7.7.6.m1.1.1.3">superscript</csymbol><cn id="S4.T2.7.7.7.7.6.m1.1.1.3.2.cmml" type="float" xref="S4.T2.7.7.7.7.6.m1.1.1.3.2">1.25</cn><cn id="S4.T2.7.7.7.7.6.m1.1.1.3.3.cmml" type="integer" xref="S4.T2.7.7.7.7.6.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.7.6.m1.1c">\delta&gt;1.25^{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.7.7.6.m1.1d">italic_δ &gt; 1.25 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T2.8.8.8.8.7.1" style="background-color:#EDEDED;"> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.8.8.8.8.7.1.m1.1"><semantics id="S4.T2.8.8.8.8.7.1.m1.1a"><mo id="S4.T2.8.8.8.8.7.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.8.8.8.8.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.8.7.1.m1.1b"><ci id="S4.T2.8.8.8.8.7.1.m1.1.1.cmml" xref="S4.T2.8.8.8.8.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.8.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.8.8.7.1.m1.1d">↑</annotation></semantics></math>)</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.10.10.10.10.9">
<math alttext="\delta&gt;1.25^{3}" class="ltx_Math" display="inline" id="S4.T2.9.9.9.9.8.m1.1"><semantics id="S4.T2.9.9.9.9.8.m1.1a"><mrow id="S4.T2.9.9.9.9.8.m1.1.1" xref="S4.T2.9.9.9.9.8.m1.1.1.cmml"><mi id="S4.T2.9.9.9.9.8.m1.1.1.2" xref="S4.T2.9.9.9.9.8.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.9.9.9.9.8.m1.1.1.1" xref="S4.T2.9.9.9.9.8.m1.1.1.1.cmml">&gt;</mo><msup id="S4.T2.9.9.9.9.8.m1.1.1.3" xref="S4.T2.9.9.9.9.8.m1.1.1.3.cmml"><mn id="S4.T2.9.9.9.9.8.m1.1.1.3.2" xref="S4.T2.9.9.9.9.8.m1.1.1.3.2.cmml">1.25</mn><mn id="S4.T2.9.9.9.9.8.m1.1.1.3.3" xref="S4.T2.9.9.9.9.8.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.9.8.m1.1b"><apply id="S4.T2.9.9.9.9.8.m1.1.1.cmml" xref="S4.T2.9.9.9.9.8.m1.1.1"><gt id="S4.T2.9.9.9.9.8.m1.1.1.1.cmml" xref="S4.T2.9.9.9.9.8.m1.1.1.1"></gt><ci id="S4.T2.9.9.9.9.8.m1.1.1.2.cmml" xref="S4.T2.9.9.9.9.8.m1.1.1.2">𝛿</ci><apply id="S4.T2.9.9.9.9.8.m1.1.1.3.cmml" xref="S4.T2.9.9.9.9.8.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.9.9.9.9.8.m1.1.1.3.1.cmml" xref="S4.T2.9.9.9.9.8.m1.1.1.3">superscript</csymbol><cn id="S4.T2.9.9.9.9.8.m1.1.1.3.2.cmml" type="float" xref="S4.T2.9.9.9.9.8.m1.1.1.3.2">1.25</cn><cn id="S4.T2.9.9.9.9.8.m1.1.1.3.3.cmml" type="integer" xref="S4.T2.9.9.9.9.8.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.9.8.m1.1c">\delta&gt;1.25^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.9.9.8.m1.1d">italic_δ &gt; 1.25 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.10.10.10.10.9.m2.1"><semantics id="S4.T2.10.10.10.10.9.m2.1a"><mo id="S4.T2.10.10.10.10.9.m2.1.1" stretchy="false" xref="S4.T2.10.10.10.10.9.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.10.9.m2.1b"><ci id="S4.T2.10.10.10.10.9.m2.1.1.cmml" xref="S4.T2.10.10.10.10.9.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.10.9.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.10.10.9.m2.1d">↑</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.49.49.49.50.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.49.49.49.50.1.1">DINO</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.50.1.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.50.1.2.1" style="background-color:#EDEDED;">1.034</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.50.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.50.1.3.1">3.517</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.50.1.4" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.50.1.4.1" style="background-color:#EDEDED;">0.173</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.50.1.5">0.415</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.50.1.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.50.1.6.1" style="background-color:#EDEDED;">0.746</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.50.1.7">0.895</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.12.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.12.12.12.12.3">DINO-HA</th>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.12.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.12.12.12.4.1" style="background-color:#EDEDED;">1.032</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.12.5">3.759</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.11.11.11.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.11.11.11.1.1" style="background-color:#EDEDED;">0.169<sup class="ltx_sup" id="S4.T2.11.11.11.11.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.11.11.11.11.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.12.2"><span class="ltx_text ltx_font_bold" id="S4.T2.12.12.12.12.2.1">0.445<sup class="ltx_sup" id="S4.T2.12.12.12.12.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.12.12.12.12.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.12.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.12.12.12.12.6.1" style="background-color:#EDEDED;">0.761</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.12.7"><span class="ltx_text ltx_font_bold" id="S4.T2.12.12.12.12.7.1">0.900</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.16.16.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.16.16.16.16.5">DINOv2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.13.13.13.13.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.13.13.13.13.1.1" style="background-color:#EDEDED;">1.003<sup class="ltx_sup" id="S4.T2.13.13.13.13.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.13.13.13.13.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.16.16.16.16.6">3.188</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.16.16.16.16.7" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.16.16.16.16.7.1" style="background-color:#EDEDED;">0.178</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.14.14.14.14.2"><span class="ltx_text ltx_font_bold" id="S4.T2.14.14.14.14.2.1">0.445<sup class="ltx_sup" id="S4.T2.14.14.14.14.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.14.14.14.14.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.15.15.15.15.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.15.15.15.15.3.1" style="background-color:#EDEDED;">0.785<sup class="ltx_sup" id="S4.T2.15.15.15.15.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.15.15.15.15.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.16.16.16.16.4"><span class="ltx_text ltx_font_bold" id="S4.T2.16.16.16.16.4.1">0.907<sup class="ltx_sup" id="S4.T2.16.16.16.16.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.16.16.16.16.4.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.17.17.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.17.17.17.17.2">DINOv2-HA</th>
<td class="ltx_td ltx_align_center" id="S4.T2.17.17.17.17.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.17.17.17.17.3.1" style="background-color:#EDEDED;">1.062</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.17.17.17.17.1"><span class="ltx_text ltx_font_bold" id="S4.T2.17.17.17.17.1.1">3.167<sup class="ltx_sup" id="S4.T2.17.17.17.17.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.17.17.17.17.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.17.17.17.17.4" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.17.17.17.17.4.1" style="background-color:#EDEDED;">0.185</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.17.17.17.17.5">0.419</td>
<td class="ltx_td ltx_align_center" id="S4.T2.17.17.17.17.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.17.17.17.17.6.1" style="background-color:#EDEDED;">0.749</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.17.17.17.17.7">0.891</td>
</tr>
<tr class="ltx_tr" id="S4.T2.18.18.18.18">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.18.18.18.18.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="6" id="S4.T2.18.18.18.18.1"><span class="ltx_text ltx_font_bold" id="S4.T2.18.18.18.18.1.1">NYUv2 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.18.18.18.18.1.1.m1.1"><semantics id="S4.T2.18.18.18.18.1.1.m1.1a"><mo id="S4.T2.18.18.18.18.1.1.m1.1.1" stretchy="false" xref="S4.T2.18.18.18.18.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.18.18.1.1.m1.1b"><ci id="S4.T2.18.18.18.18.1.1.m1.1.1.cmml" xref="S4.T2.18.18.18.18.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.18.18.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.18.18.18.18.1.1.m1.1d">→</annotation></semantics></math> 4D LF</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.27.27.27.27">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.27.27.27.27.10"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.19.19.19.19.1" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.19.19.19.19.1.1" style="background-color:#EDEDED;">RMSE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.19.19.19.19.1.1.m1.1"><semantics id="S4.T2.19.19.19.19.1.1.m1.1a"><mo id="S4.T2.19.19.19.19.1.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.19.19.19.19.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.19.19.19.19.1.1.m1.1b"><ci id="S4.T2.19.19.19.19.1.1.m1.1.1.cmml" xref="S4.T2.19.19.19.19.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.19.19.19.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.19.19.19.19.1.1.m1.1d">↓</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.20.20.20.20.2">AbsRel (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.20.20.20.20.2.m1.1"><semantics id="S4.T2.20.20.20.20.2.m1.1a"><mo id="S4.T2.20.20.20.20.2.m1.1.1" stretchy="false" xref="S4.T2.20.20.20.20.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.20.20.20.20.2.m1.1b"><ci id="S4.T2.20.20.20.20.2.m1.1.1.cmml" xref="S4.T2.20.20.20.20.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.20.20.20.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.20.20.20.20.2.m1.1d">↓</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.21.21.21.21.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.21.21.21.21.3.1" style="background-color:#EDEDED;">log10 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.21.21.21.21.3.1.m1.1"><semantics id="S4.T2.21.21.21.21.3.1.m1.1a"><mo id="S4.T2.21.21.21.21.3.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.21.21.21.21.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.21.21.21.21.3.1.m1.1b"><ci id="S4.T2.21.21.21.21.3.1.m1.1.1.cmml" xref="S4.T2.21.21.21.21.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.21.21.21.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.21.21.21.21.3.1.m1.1d">↓</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.23.23.23.23.5">
<math alttext="\delta&gt;1.25" class="ltx_Math" display="inline" id="S4.T2.22.22.22.22.4.m1.1"><semantics id="S4.T2.22.22.22.22.4.m1.1a"><mrow id="S4.T2.22.22.22.22.4.m1.1.1" xref="S4.T2.22.22.22.22.4.m1.1.1.cmml"><mi id="S4.T2.22.22.22.22.4.m1.1.1.2" xref="S4.T2.22.22.22.22.4.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.22.22.22.22.4.m1.1.1.1" xref="S4.T2.22.22.22.22.4.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T2.22.22.22.22.4.m1.1.1.3" xref="S4.T2.22.22.22.22.4.m1.1.1.3.cmml">1.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.22.22.22.22.4.m1.1b"><apply id="S4.T2.22.22.22.22.4.m1.1.1.cmml" xref="S4.T2.22.22.22.22.4.m1.1.1"><gt id="S4.T2.22.22.22.22.4.m1.1.1.1.cmml" xref="S4.T2.22.22.22.22.4.m1.1.1.1"></gt><ci id="S4.T2.22.22.22.22.4.m1.1.1.2.cmml" xref="S4.T2.22.22.22.22.4.m1.1.1.2">𝛿</ci><cn id="S4.T2.22.22.22.22.4.m1.1.1.3.cmml" type="float" xref="S4.T2.22.22.22.22.4.m1.1.1.3">1.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.22.22.22.4.m1.1c">\delta&gt;1.25</annotation><annotation encoding="application/x-llamapun" id="S4.T2.22.22.22.22.4.m1.1d">italic_δ &gt; 1.25</annotation></semantics></math> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.23.23.23.23.5.m2.1"><semantics id="S4.T2.23.23.23.23.5.m2.1a"><mo id="S4.T2.23.23.23.23.5.m2.1.1" stretchy="false" xref="S4.T2.23.23.23.23.5.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.23.23.23.23.5.m2.1b"><ci id="S4.T2.23.23.23.23.5.m2.1.1.cmml" xref="S4.T2.23.23.23.23.5.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.23.23.23.5.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.23.23.23.23.5.m2.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.25.25.25.25.7" style="background-color:#EDEDED;">
<math alttext="\delta&gt;1.25^{2}" class="ltx_Math" display="inline" id="S4.T2.24.24.24.24.6.m1.1" style="background-color:#EDEDED;"><semantics id="S4.T2.24.24.24.24.6.m1.1a"><mrow id="S4.T2.24.24.24.24.6.m1.1.1" xref="S4.T2.24.24.24.24.6.m1.1.1.cmml"><mi id="S4.T2.24.24.24.24.6.m1.1.1.2" mathbackground="#EDEDED" xref="S4.T2.24.24.24.24.6.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.24.24.24.24.6.m1.1.1.1" mathbackground="#EDEDED" xref="S4.T2.24.24.24.24.6.m1.1.1.1.cmml">&gt;</mo><msup id="S4.T2.24.24.24.24.6.m1.1.1.3" xref="S4.T2.24.24.24.24.6.m1.1.1.3.cmml"><mn id="S4.T2.24.24.24.24.6.m1.1.1.3.2" mathbackground="#EDEDED" xref="S4.T2.24.24.24.24.6.m1.1.1.3.2.cmml">1.25</mn><mn id="S4.T2.24.24.24.24.6.m1.1.1.3.3" mathbackground="#EDEDED" xref="S4.T2.24.24.24.24.6.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.24.24.24.24.6.m1.1b"><apply id="S4.T2.24.24.24.24.6.m1.1.1.cmml" xref="S4.T2.24.24.24.24.6.m1.1.1"><gt id="S4.T2.24.24.24.24.6.m1.1.1.1.cmml" xref="S4.T2.24.24.24.24.6.m1.1.1.1"></gt><ci id="S4.T2.24.24.24.24.6.m1.1.1.2.cmml" xref="S4.T2.24.24.24.24.6.m1.1.1.2">𝛿</ci><apply id="S4.T2.24.24.24.24.6.m1.1.1.3.cmml" xref="S4.T2.24.24.24.24.6.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.24.24.24.24.6.m1.1.1.3.1.cmml" xref="S4.T2.24.24.24.24.6.m1.1.1.3">superscript</csymbol><cn id="S4.T2.24.24.24.24.6.m1.1.1.3.2.cmml" type="float" xref="S4.T2.24.24.24.24.6.m1.1.1.3.2">1.25</cn><cn id="S4.T2.24.24.24.24.6.m1.1.1.3.3.cmml" type="integer" xref="S4.T2.24.24.24.24.6.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.24.24.24.6.m1.1c">\delta&gt;1.25^{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.24.24.24.24.6.m1.1d">italic_δ &gt; 1.25 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T2.25.25.25.25.7.1" style="background-color:#EDEDED;"> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.25.25.25.25.7.1.m1.1"><semantics id="S4.T2.25.25.25.25.7.1.m1.1a"><mo id="S4.T2.25.25.25.25.7.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.25.25.25.25.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.25.25.25.25.7.1.m1.1b"><ci id="S4.T2.25.25.25.25.7.1.m1.1.1.cmml" xref="S4.T2.25.25.25.25.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.25.25.25.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.25.25.25.25.7.1.m1.1d">↑</annotation></semantics></math>)</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.27.27.27.27.9">
<math alttext="\delta&gt;1.25^{3}" class="ltx_Math" display="inline" id="S4.T2.26.26.26.26.8.m1.1"><semantics id="S4.T2.26.26.26.26.8.m1.1a"><mrow id="S4.T2.26.26.26.26.8.m1.1.1" xref="S4.T2.26.26.26.26.8.m1.1.1.cmml"><mi id="S4.T2.26.26.26.26.8.m1.1.1.2" xref="S4.T2.26.26.26.26.8.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.26.26.26.26.8.m1.1.1.1" xref="S4.T2.26.26.26.26.8.m1.1.1.1.cmml">&gt;</mo><msup id="S4.T2.26.26.26.26.8.m1.1.1.3" xref="S4.T2.26.26.26.26.8.m1.1.1.3.cmml"><mn id="S4.T2.26.26.26.26.8.m1.1.1.3.2" xref="S4.T2.26.26.26.26.8.m1.1.1.3.2.cmml">1.25</mn><mn id="S4.T2.26.26.26.26.8.m1.1.1.3.3" xref="S4.T2.26.26.26.26.8.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.26.26.26.26.8.m1.1b"><apply id="S4.T2.26.26.26.26.8.m1.1.1.cmml" xref="S4.T2.26.26.26.26.8.m1.1.1"><gt id="S4.T2.26.26.26.26.8.m1.1.1.1.cmml" xref="S4.T2.26.26.26.26.8.m1.1.1.1"></gt><ci id="S4.T2.26.26.26.26.8.m1.1.1.2.cmml" xref="S4.T2.26.26.26.26.8.m1.1.1.2">𝛿</ci><apply id="S4.T2.26.26.26.26.8.m1.1.1.3.cmml" xref="S4.T2.26.26.26.26.8.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.26.26.26.26.8.m1.1.1.3.1.cmml" xref="S4.T2.26.26.26.26.8.m1.1.1.3">superscript</csymbol><cn id="S4.T2.26.26.26.26.8.m1.1.1.3.2.cmml" type="float" xref="S4.T2.26.26.26.26.8.m1.1.1.3.2">1.25</cn><cn id="S4.T2.26.26.26.26.8.m1.1.1.3.3.cmml" type="integer" xref="S4.T2.26.26.26.26.8.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.26.26.26.8.m1.1c">\delta&gt;1.25^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.26.26.26.26.8.m1.1d">italic_δ &gt; 1.25 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.27.27.27.27.9.m2.1"><semantics id="S4.T2.27.27.27.27.9.m2.1a"><mo id="S4.T2.27.27.27.27.9.m2.1.1" stretchy="false" xref="S4.T2.27.27.27.27.9.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.27.27.27.27.9.m2.1b"><ci id="S4.T2.27.27.27.27.9.m2.1.1.cmml" xref="S4.T2.27.27.27.27.9.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.27.27.27.9.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.27.27.27.27.9.m2.1d">↑</annotation></semantics></math>)</th>
</tr>
<tr class="ltx_tr" id="S4.T2.49.49.49.51.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.49.49.49.51.2.1">DINO</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.51.2.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.51.2.2.1" style="background-color:#EDEDED;">3.817</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.51.2.3">0.543</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.51.2.4" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.51.2.4.1" style="background-color:#EDEDED;">0.380</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.51.2.5">0.160</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.51.2.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.51.2.6.1" style="background-color:#EDEDED;">0.335</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.51.2.7">0.454</td>
</tr>
<tr class="ltx_tr" id="S4.T2.49.49.49.52.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.49.49.49.52.3.1">DINO-HA</th>
<td class="ltx_td ltx_align_center" id="S4.T2.49.49.49.52.3.2" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.52.3.2.1" style="background-color:#EDEDED;">3.757</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.49.49.49.52.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.52.3.3.1">0.538</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.49.49.49.52.3.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.52.3.4.1" style="background-color:#EDEDED;">0.362</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.49.49.49.52.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.52.3.5.1">0.187</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.49.49.49.52.3.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.52.3.6.1" style="background-color:#EDEDED;">0.346</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.49.49.49.52.3.7"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.52.3.7.1">0.463</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.30.30.30.30">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.30.30.30.30.4">DINOv2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.28.28.28.28.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.28.28.28.28.1.1" style="background-color:#EDEDED;">3.460<sup class="ltx_sup" id="S4.T2.28.28.28.28.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.28.28.28.28.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.30.30.30.30.5">0.497</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.29.29.29.29.2" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.29.29.29.29.2.1" style="background-color:#EDEDED;">0.337<sup class="ltx_sup" id="S4.T2.29.29.29.29.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.29.29.29.29.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.30.30.30.30.6">0.194</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.30.30.30.30.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.30.30.30.30.7.1" style="background-color:#EDEDED;">0.337</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.30.30.30.30.3"><span class="ltx_text ltx_font_bold" id="S4.T2.30.30.30.30.3.1">0.464<sup class="ltx_sup" id="S4.T2.30.30.30.30.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.30.30.30.30.3.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.33.33.33.33">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.33.33.33.33.4">DINOv2-HA</th>
<td class="ltx_td ltx_align_center" id="S4.T2.33.33.33.33.5" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.33.33.33.33.5.1" style="background-color:#EDEDED;">3.720</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.31.31.31.31.1"><span class="ltx_text ltx_font_bold" id="S4.T2.31.31.31.31.1.1">0.487<sup class="ltx_sup" id="S4.T2.31.31.31.31.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.31.31.31.31.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.33.33.33.33.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.33.33.33.33.6.1" style="background-color:#EDEDED;">0.356</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.32.32.32.32.2"><span class="ltx_text ltx_font_bold" id="S4.T2.32.32.32.32.2.1">0.203<sup class="ltx_sup" id="S4.T2.32.32.32.32.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.32.32.32.32.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.33.33.33.33.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.33.33.33.33.3.1" style="background-color:#EDEDED;">0.356<sup class="ltx_sup" id="S4.T2.33.33.33.33.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.33.33.33.33.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.33.33.33.33.7">0.455</td>
</tr>
<tr class="ltx_tr" id="S4.T2.34.34.34.34">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.34.34.34.34.2"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="6" id="S4.T2.34.34.34.34.1"><span class="ltx_text ltx_font_bold" id="S4.T2.34.34.34.34.1.1">SUN-RGBD<sup class="ltx_sup" id="S4.T2.34.34.34.34.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.34.34.34.34.1.1.1.1">∗</span></sup></span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.43.43.43.43">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.43.43.43.43.10"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.35.35.35.35.1" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.35.35.35.35.1.1" style="background-color:#EDEDED;">RMSE (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.35.35.35.35.1.1.m1.1"><semantics id="S4.T2.35.35.35.35.1.1.m1.1a"><mo id="S4.T2.35.35.35.35.1.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.35.35.35.35.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.35.35.35.35.1.1.m1.1b"><ci id="S4.T2.35.35.35.35.1.1.m1.1.1.cmml" xref="S4.T2.35.35.35.35.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.35.35.35.35.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.35.35.35.35.1.1.m1.1d">↓</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.36.36.36.36.2">AbsRel (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.36.36.36.36.2.m1.1"><semantics id="S4.T2.36.36.36.36.2.m1.1a"><mo id="S4.T2.36.36.36.36.2.m1.1.1" stretchy="false" xref="S4.T2.36.36.36.36.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.36.36.36.36.2.m1.1b"><ci id="S4.T2.36.36.36.36.2.m1.1.1.cmml" xref="S4.T2.36.36.36.36.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.36.36.36.36.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.36.36.36.36.2.m1.1d">↓</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.37.37.37.37.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.37.37.37.37.3.1" style="background-color:#EDEDED;">log10 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.37.37.37.37.3.1.m1.1"><semantics id="S4.T2.37.37.37.37.3.1.m1.1a"><mo id="S4.T2.37.37.37.37.3.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.37.37.37.37.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.37.37.37.37.3.1.m1.1b"><ci id="S4.T2.37.37.37.37.3.1.m1.1.1.cmml" xref="S4.T2.37.37.37.37.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.37.37.37.37.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.37.37.37.37.3.1.m1.1d">↓</annotation></semantics></math>)</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.39.39.39.39.5">
<math alttext="\delta&gt;1.25" class="ltx_Math" display="inline" id="S4.T2.38.38.38.38.4.m1.1"><semantics id="S4.T2.38.38.38.38.4.m1.1a"><mrow id="S4.T2.38.38.38.38.4.m1.1.1" xref="S4.T2.38.38.38.38.4.m1.1.1.cmml"><mi id="S4.T2.38.38.38.38.4.m1.1.1.2" xref="S4.T2.38.38.38.38.4.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.38.38.38.38.4.m1.1.1.1" xref="S4.T2.38.38.38.38.4.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T2.38.38.38.38.4.m1.1.1.3" xref="S4.T2.38.38.38.38.4.m1.1.1.3.cmml">1.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.38.38.38.38.4.m1.1b"><apply id="S4.T2.38.38.38.38.4.m1.1.1.cmml" xref="S4.T2.38.38.38.38.4.m1.1.1"><gt id="S4.T2.38.38.38.38.4.m1.1.1.1.cmml" xref="S4.T2.38.38.38.38.4.m1.1.1.1"></gt><ci id="S4.T2.38.38.38.38.4.m1.1.1.2.cmml" xref="S4.T2.38.38.38.38.4.m1.1.1.2">𝛿</ci><cn id="S4.T2.38.38.38.38.4.m1.1.1.3.cmml" type="float" xref="S4.T2.38.38.38.38.4.m1.1.1.3">1.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.38.38.38.38.4.m1.1c">\delta&gt;1.25</annotation><annotation encoding="application/x-llamapun" id="S4.T2.38.38.38.38.4.m1.1d">italic_δ &gt; 1.25</annotation></semantics></math> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.39.39.39.39.5.m2.1"><semantics id="S4.T2.39.39.39.39.5.m2.1a"><mo id="S4.T2.39.39.39.39.5.m2.1.1" stretchy="false" xref="S4.T2.39.39.39.39.5.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.39.39.39.39.5.m2.1b"><ci id="S4.T2.39.39.39.39.5.m2.1.1.cmml" xref="S4.T2.39.39.39.39.5.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.39.39.39.39.5.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.39.39.39.39.5.m2.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.41.41.41.41.7" style="background-color:#EDEDED;">
<math alttext="\delta&gt;1.25^{2}" class="ltx_Math" display="inline" id="S4.T2.40.40.40.40.6.m1.1" style="background-color:#EDEDED;"><semantics id="S4.T2.40.40.40.40.6.m1.1a"><mrow id="S4.T2.40.40.40.40.6.m1.1.1" xref="S4.T2.40.40.40.40.6.m1.1.1.cmml"><mi id="S4.T2.40.40.40.40.6.m1.1.1.2" mathbackground="#EDEDED" xref="S4.T2.40.40.40.40.6.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.40.40.40.40.6.m1.1.1.1" mathbackground="#EDEDED" xref="S4.T2.40.40.40.40.6.m1.1.1.1.cmml">&gt;</mo><msup id="S4.T2.40.40.40.40.6.m1.1.1.3" xref="S4.T2.40.40.40.40.6.m1.1.1.3.cmml"><mn id="S4.T2.40.40.40.40.6.m1.1.1.3.2" mathbackground="#EDEDED" xref="S4.T2.40.40.40.40.6.m1.1.1.3.2.cmml">1.25</mn><mn id="S4.T2.40.40.40.40.6.m1.1.1.3.3" mathbackground="#EDEDED" xref="S4.T2.40.40.40.40.6.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.40.40.40.40.6.m1.1b"><apply id="S4.T2.40.40.40.40.6.m1.1.1.cmml" xref="S4.T2.40.40.40.40.6.m1.1.1"><gt id="S4.T2.40.40.40.40.6.m1.1.1.1.cmml" xref="S4.T2.40.40.40.40.6.m1.1.1.1"></gt><ci id="S4.T2.40.40.40.40.6.m1.1.1.2.cmml" xref="S4.T2.40.40.40.40.6.m1.1.1.2">𝛿</ci><apply id="S4.T2.40.40.40.40.6.m1.1.1.3.cmml" xref="S4.T2.40.40.40.40.6.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.40.40.40.40.6.m1.1.1.3.1.cmml" xref="S4.T2.40.40.40.40.6.m1.1.1.3">superscript</csymbol><cn id="S4.T2.40.40.40.40.6.m1.1.1.3.2.cmml" type="float" xref="S4.T2.40.40.40.40.6.m1.1.1.3.2">1.25</cn><cn id="S4.T2.40.40.40.40.6.m1.1.1.3.3.cmml" type="integer" xref="S4.T2.40.40.40.40.6.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.40.40.40.40.6.m1.1c">\delta&gt;1.25^{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.40.40.40.40.6.m1.1d">italic_δ &gt; 1.25 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.T2.41.41.41.41.7.1" style="background-color:#EDEDED;"> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.41.41.41.41.7.1.m1.1"><semantics id="S4.T2.41.41.41.41.7.1.m1.1a"><mo id="S4.T2.41.41.41.41.7.1.m1.1.1" mathbackground="#EDEDED" stretchy="false" xref="S4.T2.41.41.41.41.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.41.41.41.41.7.1.m1.1b"><ci id="S4.T2.41.41.41.41.7.1.m1.1.1.cmml" xref="S4.T2.41.41.41.41.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.41.41.41.41.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.41.41.41.41.7.1.m1.1d">↑</annotation></semantics></math>)</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S4.T2.43.43.43.43.9">
<math alttext="\delta&gt;1.25^{3}" class="ltx_Math" display="inline" id="S4.T2.42.42.42.42.8.m1.1"><semantics id="S4.T2.42.42.42.42.8.m1.1a"><mrow id="S4.T2.42.42.42.42.8.m1.1.1" xref="S4.T2.42.42.42.42.8.m1.1.1.cmml"><mi id="S4.T2.42.42.42.42.8.m1.1.1.2" xref="S4.T2.42.42.42.42.8.m1.1.1.2.cmml">δ</mi><mo id="S4.T2.42.42.42.42.8.m1.1.1.1" xref="S4.T2.42.42.42.42.8.m1.1.1.1.cmml">&gt;</mo><msup id="S4.T2.42.42.42.42.8.m1.1.1.3" xref="S4.T2.42.42.42.42.8.m1.1.1.3.cmml"><mn id="S4.T2.42.42.42.42.8.m1.1.1.3.2" xref="S4.T2.42.42.42.42.8.m1.1.1.3.2.cmml">1.25</mn><mn id="S4.T2.42.42.42.42.8.m1.1.1.3.3" xref="S4.T2.42.42.42.42.8.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.42.42.42.42.8.m1.1b"><apply id="S4.T2.42.42.42.42.8.m1.1.1.cmml" xref="S4.T2.42.42.42.42.8.m1.1.1"><gt id="S4.T2.42.42.42.42.8.m1.1.1.1.cmml" xref="S4.T2.42.42.42.42.8.m1.1.1.1"></gt><ci id="S4.T2.42.42.42.42.8.m1.1.1.2.cmml" xref="S4.T2.42.42.42.42.8.m1.1.1.2">𝛿</ci><apply id="S4.T2.42.42.42.42.8.m1.1.1.3.cmml" xref="S4.T2.42.42.42.42.8.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.42.42.42.42.8.m1.1.1.3.1.cmml" xref="S4.T2.42.42.42.42.8.m1.1.1.3">superscript</csymbol><cn id="S4.T2.42.42.42.42.8.m1.1.1.3.2.cmml" type="float" xref="S4.T2.42.42.42.42.8.m1.1.1.3.2">1.25</cn><cn id="S4.T2.42.42.42.42.8.m1.1.1.3.3.cmml" type="integer" xref="S4.T2.42.42.42.42.8.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.42.42.42.42.8.m1.1c">\delta&gt;1.25^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.42.42.42.42.8.m1.1d">italic_δ &gt; 1.25 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.43.43.43.43.9.m2.1"><semantics id="S4.T2.43.43.43.43.9.m2.1a"><mo id="S4.T2.43.43.43.43.9.m2.1.1" stretchy="false" xref="S4.T2.43.43.43.43.9.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.43.43.43.43.9.m2.1b"><ci id="S4.T2.43.43.43.43.9.m2.1.1.cmml" xref="S4.T2.43.43.43.43.9.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.43.43.43.43.9.m2.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.43.43.43.43.9.m2.1d">↑</annotation></semantics></math>)</th>
</tr>
<tr class="ltx_tr" id="S4.T2.49.49.49.53.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.49.49.49.53.4.1">DINO</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.53.4.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.53.4.2.1" style="background-color:#EDEDED;">4.900</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.53.4.3">2.350</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.53.4.4" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.53.4.4.1" style="background-color:#EDEDED;">0.533</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.53.4.5">0.205</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.53.4.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.53.4.6.1" style="background-color:#EDEDED;">0.385</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.53.4.7">0.524</td>
</tr>
<tr class="ltx_tr" id="S4.T2.48.48.48.48">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.48.48.48.48.6">DINO-HA</th>
<td class="ltx_td ltx_align_center" id="S4.T2.44.44.44.44.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.44.44.44.44.1.1" style="background-color:#EDEDED;">4.788<sup class="ltx_sup" id="S4.T2.44.44.44.44.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.44.44.44.44.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.45.45.45.45.2"><span class="ltx_text ltx_font_bold" id="S4.T2.45.45.45.45.2.1">2.300<sup class="ltx_sup" id="S4.T2.45.45.45.45.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.45.45.45.45.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.46.46.46.46.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.46.46.46.46.3.1" style="background-color:#EDEDED;">0.526<sup class="ltx_sup" id="S4.T2.46.46.46.46.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.46.46.46.46.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.48.7"><span class="ltx_text ltx_font_bold" id="S4.T2.48.48.48.48.7.1">0.230</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.47.47.47.47.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.47.47.47.47.4.1" style="background-color:#EDEDED;">0.418<sup class="ltx_sup" id="S4.T2.47.47.47.47.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.47.47.47.47.4.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.48.48.48.48.5"><span class="ltx_text ltx_font_bold" id="S4.T2.48.48.48.48.5.1">0.531<sup class="ltx_sup" id="S4.T2.48.48.48.48.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.48.48.48.48.5.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.49.49.49.54.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.49.49.49.54.5.1">DINOv2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.54.5.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.54.5.2.1" style="background-color:#EDEDED;">5.200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.54.5.3">3.023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.54.5.4" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.54.5.4.1" style="background-color:#EDEDED;">0.615</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.54.5.5">0.173</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.54.5.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.T2.49.49.49.54.5.6.1" style="background-color:#EDEDED;">0.309</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.49.49.49.54.5.7">0.444</td>
</tr>
<tr class="ltx_tr" id="S4.T2.49.49.49.49">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.49.49.49.49.2">DINOv2-HA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.49.49.49.49.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.49.3.1" style="background-color:#EDEDED;">5.082</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.49.49.49.49.4"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.49.4.1">2.904</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.49.49.49.49.5" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.49.5.1" style="background-color:#EDEDED;">0.599</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.49.49.49.49.1"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.49.1.1">0.237<sup class="ltx_sup" id="S4.T2.49.49.49.49.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.49.49.49.49.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.49.49.49.49.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.49.6.1" style="background-color:#EDEDED;">0.409</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.49.49.49.49.7"><span class="ltx_text ltx_font_bold" id="S4.T2.49.49.49.49.7.1">0.487</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.55.3.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T2.53.2" style="font-size:90%;">Human-aligned DINO and DINOv2 performance on monocular depth estimation benchmarks.<span class="ltx_text ltx_font_medium" id="S4.T2.53.2.2"> Note that NYUv2 and SUN-RGBD were included in DINOv2’s retrieval pretraining set, yet human-aligned DINOv2 still outperforms the base model on SUN-RGBD. Along with the results on an unseen test data domain (train on NYUv2 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.52.1.1.m1.1"><semantics id="S4.T2.52.1.1.m1.1b"><mo id="S4.T2.52.1.1.m1.1.1" stretchy="false" xref="S4.T2.52.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.52.1.1.m1.1c"><ci id="S4.T2.52.1.1.m1.1.1.cmml" xref="S4.T2.52.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.52.1.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.52.1.1.m1.1e">→</annotation></semantics></math> test on 4D Light Field), these results demonstrate strong generalization performance of models aligned to human perceptual judgments. <math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T2.53.2.2.m2.1"><semantics id="S4.T2.53.2.2.m2.1b"><mo id="S4.T2.53.2.2.m2.1.1" xref="S4.T2.53.2.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T2.53.2.2.m2.1c"><ci id="S4.T2.53.2.2.m2.1.1.cmml" xref="S4.T2.53.2.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.53.2.2.m2.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T2.53.2.2.m2.1e">†</annotation></semantics></math> indicates best score in the column.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Retrieval-augmented generation</h3>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="318" id="S4.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F3.4.2" style="font-size:90%;">Left: Diagram of evaluation setup for retrieval-augmented generation.<span class="ltx_text ltx_font_medium" id="S4.F3.4.2.1"> We retrieve the top-3 nearest image-prompt examples for each datasets and prompt OpenFlamingo with them before inputting the query image. </span>Right: Classification accuracy on VTAB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib35" title="">35</a>]</cite> from wide-varying domains.<span class="ltx_text ltx_font_medium" id="S4.F3.4.2.2"> Error bars indicate 95% confidence interval over 5 random seeds.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">First introduced for text generation with non-parametric memory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib34" title="">34</a>]</cite>, retrieval-augmented generation (RAG) has become a popular method for selecting relevant few-shot examples when prompting large vision-language models (VLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib33" title="">33</a>]</cite>. RAG evaluations go beyond conventional retrieval benchmarks on top-k recall, offering a more informative indicator of downstream large-model performance and utility in the multimodal domain. We evaluate OpenFlamingo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib2" title="">2</a>]</cite>’s few-shot classification accuracy by using a vision backbone to retrieve a query image’s 3 nearest neighbors and prepending the query image with those examples along with their class labels. Following OpenFlamingo’s image classification evaluation framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib2" title="">2</a>]</cite>, we extract the model’s logits per object class and determine the model’s decision by selecting the class with the largest log probability. See the Appendix for full details on the RAG experimental setup.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.F3" title="Figure 3 ‣ 4.2 Retrieval-augmented generation ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">3</span></a>, classification accuracy on a wide variety of data domains improves with prompts retrieved by human-aligned models, compared to the original model backbones. Even in out-of-distribution domains such as medical imagery and 2D renders of game scenes, human-aligned models retrieve in-context examples that boost OpenFlamingo classification accuracy. These results suggest that human-aligned models can select more informative examples for in-context learning, thereby boosting the few-shot generalization abilities of a downstream multimodal VLM.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Counting</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.2">A well-documented limitation of large vision backbones is their performance on compositional tasks: in particular, on object counting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib45" title="">45</a>]</cite>. We investigate how aligning to perceptual judgments affects performance on counting tasks via the FSC147, CARPK, and Clevr-Count (adapted from the original Clevr dataset by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib60" title="">60</a>]</cite>) benchmarks by computer k-Nearest Neighbors accuracy on frozen vision representations. We report results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.T3" title="Table 3 ‣ 4.3 Counting ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">3</span></a> and retrieval visualizations for few (<math alttext="n=3" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">n</mi><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><eq id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></eq><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝑛</ci><cn id="S4.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">n=3</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_n = 3</annotation></semantics></math>) and many (<math alttext="n=8,10" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.2"><semantics id="S4.SS3.p1.2.m2.2a"><mrow id="S4.SS3.p1.2.m2.2.3" xref="S4.SS3.p1.2.m2.2.3.cmml"><mi id="S4.SS3.p1.2.m2.2.3.2" xref="S4.SS3.p1.2.m2.2.3.2.cmml">n</mi><mo id="S4.SS3.p1.2.m2.2.3.1" xref="S4.SS3.p1.2.m2.2.3.1.cmml">=</mo><mrow id="S4.SS3.p1.2.m2.2.3.3.2" xref="S4.SS3.p1.2.m2.2.3.3.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">8</mn><mo id="S4.SS3.p1.2.m2.2.3.3.2.1" xref="S4.SS3.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="S4.SS3.p1.2.m2.2.2" xref="S4.SS3.p1.2.m2.2.2.cmml">10</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.2b"><apply id="S4.SS3.p1.2.m2.2.3.cmml" xref="S4.SS3.p1.2.m2.2.3"><eq id="S4.SS3.p1.2.m2.2.3.1.cmml" xref="S4.SS3.p1.2.m2.2.3.1"></eq><ci id="S4.SS3.p1.2.m2.2.3.2.cmml" xref="S4.SS3.p1.2.m2.2.3.2">𝑛</ci><list id="S4.SS3.p1.2.m2.2.3.3.1.cmml" xref="S4.SS3.p1.2.m2.2.3.3.2"><cn id="S4.SS3.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1">8</cn><cn id="S4.SS3.p1.2.m2.2.2.cmml" type="integer" xref="S4.SS3.p1.2.m2.2.2">10</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.2c">n=8,10</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.2d">italic_n = 8 , 10</annotation></semantics></math>) objects in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.F4" title="Figure 4 ‣ 4.3 Counting ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4</span></a> and find that, across 6 different models, the human-aligned versions outperform their counterparts in 35 out of 36 cases. See the Appendix for full details on the counting experimental setup.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="215" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F4.4.2" style="font-size:90%;">Visualizations of nearest-neighbor examples retrieved by CLIP, DINO, and Ensemble models as well as their human-aligned versions.<span class="ltx_text ltx_font_medium" id="S4.F4.4.2.1"> Overall, we see retrieved images with more accurate object counts in CLIP-HA, DINO-HA, and Ensemble-HA across multiple nearest neighbors.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Given that the perceptual similarity dataset we use for finetuning contains image-level similarity judgments, the consistent improvements that we observe on counting tasks, which requires local object awareness, is somewhat surprising. We hypothesize that the sensitivity of our human-aligned models to object counts may be a byproduct of NIGHTS examples themselves, many of which include image triplets with varying numbers of objects (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2.F15" title="Figure 15 ‣ B.2 Dataset examples ‣ Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">15</span></a> in the Appendix). In terms of human perception, we note that humans consider object count when evaluating image similarity as soon as they develop counting profiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib40" title="">40</a>]</cite>. Thus, it is possible that given the prevalence of triplets with object-count variations in NIGHTS, the human annotations naturally capture this counting aware effect in the global image-level labels and propagate this information to the human-aligned models.</p>
</div>
<figure class="ltx_figure" id="S4.SS3.8">
<div class="ltx_block" id="S4.SS3.8.9">
<span class="ltx_ERROR undefined" id="S4.SS3.8.9.1">\RawFloats</span>
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S4.F5" style="width:138.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="1205" id="S4.SS3.1.1.g1" src="x5.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.4.2" style="font-size:90%;">Performance improvements on Clevr-Count visualized by backbone for RMSE (top) and MAE (bottom), averaged across all datasets. Lower is better.</span></figcaption>
</figure>
<figure class="ltx_figure ltx_minipage ltx_align_center ltx_align_middle" id="S4.SS3.8.8" style="width:273.2pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S4.SS3.8.8.7" style="width:433.6pt;height:337.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(55.0pt,-42.8pt) scale(1.34005483065337,1.34005483065337) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.SS3.8.8.7.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.8.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.SS3.8.8.7.7.8.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.SS3.8.8.7.7.8.1.2" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.8.1.2.1" style="background-color:#EDEDED;">FSC147</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.SS3.8.8.7.7.8.1.3"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.8.1.3.1">CARPK</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.SS3.8.8.7.7.8.1.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.8.1.4.1" style="background-color:#EDEDED;">Clever-Count</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.9.1">
<th class="ltx_td ltx_th ltx_th_row" id="S4.SS3.8.8.7.7.9.1.1"></th>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.9.1.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.9.1.2.1" style="background-color:#EDEDED;">MAE</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.9.1.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.9.1.3.1" style="background-color:#EDEDED;">RMSE</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.9.1.4">MAE</td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.9.1.5">RMSE</td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.9.1.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.9.1.6.1" style="background-color:#EDEDED;">MAE</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.9.1.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.9.1.7.1" style="background-color:#EDEDED;">RMSE</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.10.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.SS3.8.8.7.7.10.2.1">DINO</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.10.2.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.10.2.2.1" style="background-color:#EDEDED;">44.1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.10.2.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.10.2.3.1" style="background-color:#EDEDED;">118.7</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.10.2.4">51.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.10.2.5">56.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.10.2.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.10.2.6.1" style="background-color:#EDEDED;">1.25</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.10.2.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.10.2.7.1" style="background-color:#EDEDED;">1.70</span></th>
</tr>
<tr class="ltx_tr" id="S4.SS3.3.3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS3.3.3.2.2.2.3">DINO-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS3.2.2.1.1.1.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.2.2.1.1.1.1.1" style="background-color:#EDEDED;">41.3<sup class="ltx_sup" id="S4.SS3.2.2.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.2.2.1.1.1.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.3.3.2.2.2.4" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.3.3.2.2.2.4.1" style="background-color:#EDEDED;">119.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.3.3.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.SS3.3.3.2.2.2.5.1">48.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.3.3.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.SS3.3.3.2.2.2.2.1">54.5<sup class="ltx_sup" id="S4.SS3.3.3.2.2.2.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.3.3.2.2.2.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.3.3.2.2.2.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.3.3.2.2.2.6.1" style="background-color:#EDEDED;">1.08</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.3.3.2.2.2.7" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.3.3.2.2.2.7.1" style="background-color:#EDEDED;">1.50</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.11.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.SS3.8.8.7.7.11.3.1">DINOv2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.11.3.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.11.3.2.1" style="background-color:#EDEDED;">57.5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.11.3.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.11.3.3.1" style="background-color:#EDEDED;">128.3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.11.3.4">52.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.11.3.5">59.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.11.3.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.11.3.6.1" style="background-color:#EDEDED;">1.18</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.11.3.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.11.3.7.1" style="background-color:#EDEDED;">1.60</span></th>
</tr>
<tr class="ltx_tr" id="S4.SS3.4.4.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS3.4.4.3.3.3.2">DINOv2-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS3.4.4.3.3.3.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.4.4.3.3.3.3.1" style="background-color:#EDEDED;">44.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.4.4.3.3.3.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.4.4.3.3.3.1.1" style="background-color:#EDEDED;">113.6<sup class="ltx_sup" id="S4.SS3.4.4.3.3.3.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.4.4.3.3.3.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.4.4.3.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.SS3.4.4.3.3.3.4.1">52.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.4.4.3.3.3.5"><span class="ltx_text ltx_font_bold" id="S4.SS3.4.4.3.3.3.5.1">58.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.4.4.3.3.3.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.4.4.3.3.3.6.1" style="background-color:#EDEDED;">0.84</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.4.4.3.3.3.7" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.4.4.3.3.3.7.1" style="background-color:#EDEDED;">1.20</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.12.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.SS3.8.8.7.7.12.4.1">CLIP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.12.4.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.12.4.2.1" style="background-color:#EDEDED;">59.1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.12.4.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.12.4.3.1" style="background-color:#EDEDED;">158.4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.12.4.4">52.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.12.4.5">60.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.12.4.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.12.4.6.1" style="background-color:#EDEDED;">1.09</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.12.4.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.12.4.7.1" style="background-color:#EDEDED;">1.52</span></th>
</tr>
<tr class="ltx_tr" id="S4.SS3.5.5.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS3.5.5.4.4.4.2">CLIP-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS3.5.5.4.4.4.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.5.5.4.4.4.3.1" style="background-color:#EDEDED;">53.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.5.5.4.4.4.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.5.5.4.4.4.4.1" style="background-color:#EDEDED;">156.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.5.5.4.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.SS3.5.5.4.4.4.5.1">52.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.5.5.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.SS3.5.5.4.4.4.6.1">58.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.5.5.4.4.4.7" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.5.5.4.4.4.7.1" style="background-color:#EDEDED;">0.81</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.5.5.4.4.4.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.5.5.4.4.4.1.1" style="background-color:#EDEDED;">1.15<sup class="ltx_sup" id="S4.SS3.5.5.4.4.4.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.5.5.4.4.4.1.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.13.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.SS3.8.8.7.7.13.5.1">OpenCLIP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.13.5.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.13.5.2.1" style="background-color:#EDEDED;">54.4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.13.5.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.13.5.3.1" style="background-color:#EDEDED;">153.5</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.13.5.4">54.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.13.5.5">60.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.13.5.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.13.5.6.1" style="background-color:#EDEDED;">0.97</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.13.5.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.13.5.7.1" style="background-color:#EDEDED;">1.36</span></th>
</tr>
<tr class="ltx_tr" id="S4.SS3.7.7.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS3.7.7.6.6.6.3">OpenCLIP-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS3.7.7.6.6.6.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.7.7.6.6.6.4.1" style="background-color:#EDEDED;">50.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.7.7.6.6.6.5" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.7.7.6.6.6.5.1" style="background-color:#EDEDED;">139.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.7.7.6.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.SS3.7.7.6.6.6.6.1">49.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.7.7.6.6.6.7"><span class="ltx_text ltx_font_bold" id="S4.SS3.7.7.6.6.6.7.1">55.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.6.6.5.5.5.1" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.6.6.5.5.5.1.1" style="background-color:#EDEDED;">0.80<sup class="ltx_sup" id="S4.SS3.6.6.5.5.5.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.6.6.5.5.5.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.7.7.6.6.6.2" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.7.7.6.6.6.2.1" style="background-color:#EDEDED;">1.15<sup class="ltx_sup" id="S4.SS3.7.7.6.6.6.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.7.7.6.6.6.2.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.14.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.SS3.8.8.7.7.14.6.1">SynCLR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.14.6.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.14.6.2.1" style="background-color:#EDEDED;">50.6</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.14.6.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.14.6.3.1" style="background-color:#EDEDED;">139.6</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.14.6.4">54.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.14.6.5">60.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.14.6.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.14.6.6.1" style="background-color:#EDEDED;">1.05</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.14.6.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.14.6.7.1" style="background-color:#EDEDED;">1.45</span></th>
</tr>
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.15.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS3.8.8.7.7.15.7.1">SynCLR-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.15.7.2" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.15.7.2.1" style="background-color:#EDEDED;">46.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.15.7.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.15.7.3.1" style="background-color:#EDEDED;">128.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.15.7.4"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.15.7.4.1">51.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.15.7.5"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.15.7.5.1">58.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.15.7.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.15.7.6.1" style="background-color:#EDEDED;">0.98</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS3.8.8.7.7.15.7.7" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.15.7.7.1" style="background-color:#EDEDED;">1.37</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.16.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.SS3.8.8.7.7.16.8.1">Ensemble</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.16.8.2" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.16.8.2.1" style="background-color:#EDEDED;">48.4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.16.8.3" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.16.8.3.1" style="background-color:#EDEDED;">132.4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.16.8.4">49.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.16.8.5">60.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.16.8.6" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.16.8.6.1" style="background-color:#EDEDED;">1.10</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.SS3.8.8.7.7.16.8.7" style="background-color:#EDEDED;"><span class="ltx_text" id="S4.SS3.8.8.7.7.16.8.7.1" style="background-color:#EDEDED;">1.51</span></th>
</tr>
<tr class="ltx_tr" id="S4.SS3.8.8.7.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.SS3.8.8.7.7.7.2">Ensemble-HA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS3.8.8.7.7.7.3" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.7.3.1" style="background-color:#EDEDED;">45.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS3.8.8.7.7.7.4" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.7.4.1" style="background-color:#EDEDED;">130.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS3.8.8.7.7.7.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.7.1.1">48.4<sup class="ltx_sup" id="S4.SS3.8.8.7.7.7.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.8.8.7.7.7.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS3.8.8.7.7.7.5"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.7.5.1">55.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS3.8.8.7.7.7.6" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.7.6.1" style="background-color:#EDEDED;">0.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS3.8.8.7.7.7.7" style="background-color:#EDEDED;"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.8.7.7.7.7.1" style="background-color:#EDEDED;">1.19</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.4.2.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.2.1" style="font-size:90%;">Error comparisons for base and human-aligned models on standard counting benchmarks. Though FSC147 and CARPK have examples with extreme object counts (tens and hundreds) unseen in the NIGHTS data, human-aligned models still achieve higher performance in each pair. <math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T3.2.1.m1.1"><semantics id="S4.T3.2.1.m1.1b"><mo id="S4.T3.2.1.m1.1.1" xref="S4.T3.2.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.1.m1.1c"><ci id="S4.T3.2.1.m1.1.1.cmml" xref="S4.T3.2.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.1.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.1.m1.1e">†</annotation></semantics></math> indicates best score in the column, lower is better.</span></figcaption>
</figure>
</div>
</div>
</figure>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Instance retrieval</h3>
<figure class="ltx_figure ltx_align_floatright" id="S4.SS4.3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S4.SS4.3.3" style="width:433.6pt;height:556.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(118.6pt,-152.3pt) scale(2.20858811788403,2.20858811788403) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.SS4.3.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.SS4.3.3.3.4.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.SS4.3.3.3.4.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.SS4.3.3.3.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.4.1.2.1">DeepFashion2</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.5.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.SS4.3.3.3.5.2.1"></th>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.5.2.2">Top-1</td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.5.2.3">Top-3</td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.5.2.4">Top-5</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.SS4.3.3.3.6.3.1">DINO</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.6.3.2">8.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.6.3.3">12.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.6.3.4">14.44</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS4.3.3.3.7.4.1">DINO-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.7.4.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.7.4.2.1">11.69</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.7.4.3"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.7.4.3.1">17.55</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.7.4.4"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.7.4.4.1">20.84</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.SS4.3.3.3.8.5.1">DINOv2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.8.5.2">5.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.8.5.3">8.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.8.5.4">9.98</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS4.3.3.3.9.6.1">DINOv2-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.9.6.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.9.6.2.1">9.03</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.9.6.3"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.9.6.3.1">13.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.9.6.4"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.9.6.4.1">16.30</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.SS4.3.3.3.10.7.1">CLIP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.10.7.2">4.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.10.7.3">6.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.10.7.4">8.23</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS4.3.3.3.11.8.1">CLIP-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.11.8.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.11.8.2.1">8.62</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.11.8.3"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.11.8.3.1">13.02</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.11.8.4"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.11.8.4.1">15.60</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.SS4.3.3.3.12.9.1">OpenCLIP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.12.9.2">14.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.12.9.3">22.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.12.9.4">27.36</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS4.3.3.3.13.10.1">OpenCLIP-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.13.10.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.13.10.2.1">16.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.13.10.3"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.13.10.3.1">24.58</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.13.10.4"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.13.10.4.1">28.64</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.14.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.SS4.3.3.3.14.11.1">SynCLR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.14.11.2">4.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.14.11.3">7.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.14.11.4">9.02</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.15.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.SS4.3.3.3.15.12.1">SynCLR-HA</th>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.15.12.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.15.12.2.1">8.35</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.15.12.3"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.15.12.3.1">12.31</span></td>
<td class="ltx_td ltx_align_center" id="S4.SS4.3.3.3.15.12.4"><span class="ltx_text ltx_font_bold" id="S4.SS4.3.3.3.15.12.4.1">14.86</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.16.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.SS4.3.3.3.16.13.1">Ensemble</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.16.13.2">13.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.16.13.3">20.01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.SS4.3.3.3.16.13.4">23.54</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.3.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.SS4.3.3.3.3.4">Ensemble-HA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS4.1.1.1.1.1"><math alttext="\mathbf{23.39}^{\dagger}" class="ltx_Math" display="inline" id="S4.SS4.1.1.1.1.1.m1.1"><semantics id="S4.SS4.1.1.1.1.1.m1.1a"><msup id="S4.SS4.1.1.1.1.1.m1.1.1" xref="S4.SS4.1.1.1.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="S4.SS4.1.1.1.1.1.m1.1.1.2" mathvariant="bold" xref="S4.SS4.1.1.1.1.1.m1.1.1.2.cmml">23.39</mn><mo id="S4.SS4.1.1.1.1.1.m1.1.1.3" xref="S4.SS4.1.1.1.1.1.m1.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.1.1.1.1.1.m1.1b"><apply id="S4.SS4.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS4.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.SS4.1.1.1.1.1.m1.1.1">superscript</csymbol><cn id="S4.SS4.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S4.SS4.1.1.1.1.1.m1.1.1.2">23.39</cn><ci id="S4.SS4.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.SS4.1.1.1.1.1.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.1.1.1.1.1.m1.1c">\mathbf{23.39}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.1.1.1.1.1.m1.1d">bold_23.39 start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS4.2.2.2.2.2"><math alttext="\mathbf{32.47}^{\dagger}" class="ltx_Math" display="inline" id="S4.SS4.2.2.2.2.2.m1.1"><semantics id="S4.SS4.2.2.2.2.2.m1.1a"><msup id="S4.SS4.2.2.2.2.2.m1.1.1" xref="S4.SS4.2.2.2.2.2.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="S4.SS4.2.2.2.2.2.m1.1.1.2" mathvariant="bold" xref="S4.SS4.2.2.2.2.2.m1.1.1.2.cmml">32.47</mn><mo id="S4.SS4.2.2.2.2.2.m1.1.1.3" xref="S4.SS4.2.2.2.2.2.m1.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.2.2.2.2.2.m1.1b"><apply id="S4.SS4.2.2.2.2.2.m1.1.1.cmml" xref="S4.SS4.2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.2.2.2.2.2.m1.1.1.1.cmml" xref="S4.SS4.2.2.2.2.2.m1.1.1">superscript</csymbol><cn id="S4.SS4.2.2.2.2.2.m1.1.1.2.cmml" type="float" xref="S4.SS4.2.2.2.2.2.m1.1.1.2">32.47</cn><ci id="S4.SS4.2.2.2.2.2.m1.1.1.3.cmml" xref="S4.SS4.2.2.2.2.2.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.2.2.2.2.2.m1.1c">\mathbf{32.47}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.2.2.2.2.2.m1.1d">bold_32.47 start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.SS4.3.3.3.3.3"><math alttext="\mathbf{37.20}^{\dagger}" class="ltx_Math" display="inline" id="S4.SS4.3.3.3.3.3.m1.1"><semantics id="S4.SS4.3.3.3.3.3.m1.1a"><msup id="S4.SS4.3.3.3.3.3.m1.1.1" xref="S4.SS4.3.3.3.3.3.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" id="S4.SS4.3.3.3.3.3.m1.1.1.2" mathvariant="bold" xref="S4.SS4.3.3.3.3.3.m1.1.1.2.cmml">37.20</mn><mo id="S4.SS4.3.3.3.3.3.m1.1.1.3" xref="S4.SS4.3.3.3.3.3.m1.1.1.3.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.3.3.3.3.3.m1.1b"><apply id="S4.SS4.3.3.3.3.3.m1.1.1.cmml" xref="S4.SS4.3.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.3.3.3.3.3.m1.1.1.1.cmml" xref="S4.SS4.3.3.3.3.3.m1.1.1">superscript</csymbol><cn id="S4.SS4.3.3.3.3.3.m1.1.1.2.cmml" type="float" xref="S4.SS4.3.3.3.3.3.m1.1.1.2">37.20</cn><ci id="S4.SS4.3.3.3.3.3.m1.1.1.3.cmml" xref="S4.SS4.3.3.3.3.3.m1.1.1.3">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.3.3.3.3.3.m1.1c">\mathbf{37.20}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.3.3.3.3.3.m1.1d">bold_37.20 start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.4.2.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.2.1" style="font-size:90%;">Top-1, -3, and -5 recall scores for instance retrieval on DeepFashion 2. <math alttext="\dagger" class="ltx_Math" display="inline" id="S4.T4.2.1.m1.1"><semantics id="S4.T4.2.1.m1.1b"><mo id="S4.T4.2.1.m1.1.1" xref="S4.T4.2.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.1.m1.1c"><ci id="S4.T4.2.1.m1.1.1.cmml" xref="S4.T4.2.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.1.m1.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.1.m1.1e">†</annotation></semantics></math> indicates best score in the column, higher is better.</span></figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In this section, we evaluate how aligning models to perceptual judgments affects their performance on the instance retrieval task. This task aims to retrieve images from a gallery containing a shared subject or object with the query image. At test time, retrieval is performed by computing the cosine similarity between the extracted features of the query and gallery images. Succeeding at this task requires that a representation be robust to recognizing instance identities under different lighting, backgrounds, poses, and other such shifts.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">We evaluate all base models and their human-aligned counterparts on the Consumer-to-Shop benchmark of the DeepFashion2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib21" title="">21</a>]</cite>. The benchmark consists of 10990 consumer "in the wild images" as the query set, and 21438 gallery images with matching clothing items to consumer images. Following the evaluation protocol from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib21" title="">21</a>]</cite>, we report Top-1, 3, and 5 accuracy in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.T4" title="Table 4 ‣ 4.4 Instance retrieval ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4</span></a>. Human aligned models outperform base models by a significant margin across all metrics and backbones (visualized in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.F6" title="Figure 6 ‣ 4.4 Instance retrieval ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">6</span></a>. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.F7" title="Figure 7 ‣ 4.4 Instance retrieval ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">7</span></a> we provide qualitative retrieval results. These results agree with prior work showing that training on NIGHTS improves performance in retrieving similar images to queries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.SS4.5">
<div class="ltx_block" id="S4.SS4.5.3">
<span class="ltx_ERROR undefined" id="S4.SS4.5.3.1">\RawFloats</span>
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S4.F6" style="width:177.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_square" height="709" id="S4.SS4.4.1.g1" src="x6.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.6.3.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.4.2" style="font-size:90%;">Performance improvements on the DeepFashion2 instance retrieval, task visualized by backbone and averaged across all <math alttext="k" class="ltx_Math" display="inline" id="S4.F6.3.1.m1.1"><semantics id="S4.F6.3.1.m1.1b"><mi id="S4.F6.3.1.m1.1.1" xref="S4.F6.3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F6.3.1.m1.1c"><ci id="S4.F6.3.1.m1.1.1.cmml" xref="S4.F6.3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.3.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F6.3.1.m1.1e">italic_k</annotation></semantics></math> for top-<math alttext="k" class="ltx_Math" display="inline" id="S4.F6.4.2.m2.1"><semantics id="S4.F6.4.2.m2.1b"><mi id="S4.F6.4.2.m2.1.1" xref="S4.F6.4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.F6.4.2.m2.1c"><ci id="S4.F6.4.2.m2.1.1.cmml" xref="S4.F6.4.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.4.2.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.F6.4.2.m2.1e">italic_k</annotation></semantics></math> recall. Higher is better.</span></figcaption>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S4.F7" style="width:238.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="604" id="S4.SS4.5.2.g1" src="x7.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.4.2" style="font-size:90%;">Examples of top-3 retrievals for a given query image on DeepFashion2. Overall, the human-aligned models return matching clothing items more frequently.</span></figcaption>
</figure>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>What type of human similarity annotation is most beneficial?</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">As evidenced by the current widespread use of large vision models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib46" title="">46</a>]</cite>, trained on massive datasets, model performance largely correlates with data scale.
This raises the question: are our performance gains purely due to training the base models on additional data, or as a result of the perceptual qualities embedded in NIGHTS? To investigate this, we ablate the training dataset - rather than tune on 13,900 NIGHTS triplets, we train on three other image triplet datasets of the same size:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">BAPPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>]</cite>: Originally the training set for the LPIPS perceptual similarity metric <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>]</cite>, BAPPS consists of image patch triplets with various low-level distortions applied (e.g. color jitter, gaussian blur, JPEG compression artifacts).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">THINGS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib24" title="">24</a>]</cite>: This dataset contains image triplets with each image encoding a different concept (e.g. a triplet of images categorized as {airplane, elephant, football}), labeled by humans tasked to determine which concept is the odd-one-out.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib48" title="">48</a>]</cite>: To ablate whether perceptual judgments at any level are needed, we construct an image triplet dataset by randomly selecting two images from one category and one image from another, labeling the first image pair as more similar to each other.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S4.SS5.p1.2">For all three datasets, we apply the same training settings as with the original LoRA-tuning on NIGHTS. See Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.F8" title="Figure 8 ‣ 4.5 What type of human similarity annotation is most beneficial? ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">8</span></a> for dataset ablations on object counting and instance retrieval.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Tuning on NIGHTS indeed provides the largest improvements across these tasks, with THINGS worsening performance overall and BAPPS/ImageNet having minimal effect. These trends may appear because BAPPS’ photometric distortions are too low-level to impart any perceptual signal onto the backbones, THINGS encodes a higher-level conceptual similarity irrelevant to these mid-level vision tasks, and pre-trained vision models already perform quite well at discriminating ImageNet categories. Indeed, previous work <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite> has found that similarity judgments by perceptual metrics trained on BAPPS correlate better to low-level metrics such as color than to semantic attributes.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS2" title="A.2 Additional dataset ablations ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">A.2</span></a> of the Appendix we find that the performance boost from NIGHTS over other datasets is also consistent across semantic segmentation and depth estimation. Conversely, tuning on NIGHTS fails to improve over other datasets on classification datasets; we further discuss this finding in Sections <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S5" title="5 Discussion ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS1" title="A.1 Classification with the VTAB Benchmark ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="193" id="S4.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.3.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.4.2" style="font-size:90%;">Evaluations comparing dataset utility on counting tasks (lower RMSE is better) and DeepFashion2 instance retrieval (higher recall is better). Across each task, tuning on NIGHTS yields the largest improvements while THINGS worsens performance and BAPPS/ImageNet makes minimal changes.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Recently, the vision research community has converged on the idea that using human perception to improve machine perception can bolster the transfer of vision representations to downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib43" title="">43</a>]</cite>. However, it remained unclear which tasks benefit most from alignment with human perceptual judgments; different tasks may demand representations that encode different levels of granularity and semantics. Here, we fine-tune modern backbones, pretrained on different tasks, on human perceptual judgments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite> and subsequently investigate which downstream tasks benefit. We develop a better understanding of how perceptual alignment affects performance on important tasks – e.g., segmentation, depth estimation, RAG – which may in turn inform future model training and data curation decisions across different applications.
By evaluating competency at downstream tasks, we quantify what these representations capture and make decodable, enabling a better understanding of the human-aligned feature space. While we probe representations in terms of competency, understanding them in terms of their mechanism is a rich direction for future work.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We find widespread benefits from perceptual alignment across both image- and patch-level tasks. At the global level, performance consistently improves for retrieval-augmented generation, counting-based and instance-based retrieval. Moreover, propagating the supervision from image-level similarity judgments to ViT patch tokens improves performance on dense prediction tasks (semantic segmentation and depth prediction). Since model performance is closely linked to data scale, we also ablate the choice of perceptual dataset. While fine-tuning on NIGHTS — a dataset of mid-level perceptual judgments — leads to improvements across various tasks, fine-tuning on triplets from ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib10" title="">10</a>]</cite>, BAPPS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib61" title="">61</a>]</cite>, and THINGS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib23" title="">23</a>]</cite> preserves or deteriorates transfer performance.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Why does fine-tuning on NIGHTS in particular lead to improvements? We hypothesize that the variations found in BAPPS and THINGS are solely high- or low-level, whereas the mid-level distortions in NIGHTS cover salient features that humans use when making inferences about what they see; these characteristics include style, pose, color, and count (see Fig.<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2.F15" title="Figure 15 ‣ B.2 Dataset examples ‣ Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">15</span></a>), and largely correlate with the characteristics a model must successfully extract for many computer vision tasks. Previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite> found that models fine-tuned on NIGHTS seem to attend to both low-level and semantic attributes. Aligning a feature space to these concepts may be useful for visual tasks requiring both visual and semantic knowledge, such as retrieval, counting, segmentation, etc. This hypothesis may also explain why tuning on NIGHTS hurts performance on fine-grained tasks, in which perceptually similar images may belong to different categories.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Limitations.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Perceptual alignment does not appear to improve performance for standard image classification tasks such as natural image datasets in the VTAB benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib60" title="">60</a>]</cite> (see Tables <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.T5" title="Table 5 ‣ A.1 Classification with the VTAB Benchmark ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">5</span></a>a-b in the Appendix). This is surprising in light of recent findings that demonstrate downstream task improvements in image classification tasks for human-aligned representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib42" title="">42</a>]</cite>. Although it is hard to pinpoint the exact cause, we hypothesize two reasons: First, perceptual judgments at different levels of abstraction may be helpful for different downstream tasks. While the mid-level perceptual judgments in NIGHTS boost performance for retrieval-based and dense prediction tasks, they may not impart a useful inductive bias for standard image classification tasks; high-level semantic associations could simply be better suited for these kinds of tasks. Alternatively, the visual features that humans use to judge similarity may not be appropriately captured in classification accuracy metrics. Some VTAB datasets have numerical ground truth in which the distance from the correct answer is meaningful; thus, it may be ill-suited for classification, and better suited to continuous evaluations, which we report in sections of the paper.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p2.1">Additionally, a key insight from our dataset ablations is that not all human preferences improve performance. Finetuning on perceptual datasets with solely high-level (THINGS) or low-level (BAPPS) variations hurts performance for many downstream tasks (Section  <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS5" title="4.5 What type of human similarity annotation is most beneficial? ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4.5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS2" title="A.2 Additional dataset ablations ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">A.2</span></a>). Similarly, <cite class="ltx_cite ltx_citemacro_citet">Muttenthaler et al. [<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib43" title="">43</a>]</cite> recently discovered that finetuning vision models on THINGS can hurt downstream task transfer. Due to our evaluation of the synthetically-pretrained SynCLR <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib55" title="">55</a>]</cite>, we attribute the drop in performance to perceptual alignment. Furthermore, by ablating the number of fine-tuning steps in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS2" title="A.2 Additional dataset ablations ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">A.2</span></a>, we show that overfitting to perceptual judgments may also harm downstream performance.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Societal impacts.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Beyond our findings, there are other possibilities for harm outside the scope of the type of perceptual annotations we have studied:
Human preferences may reflect unwanted biases. A long-standing problem in both language and vision is that biases (e.g. gender, racial) reflected in Internet language/images are inherited by large models, and reflected in their embeddings. One can imagine similar phenomena to happen in visual preferences <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib29" title="">29</a>]</cite>.
Humans may disagree on a preference label, or even disagree with themselves if asked at different points in time. This may lead to noisy data if not filtered carefully, thus harming representations <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib43" title="">43</a>]</cite>.
Without sufficient demographic diversity in the annotator group, emerging biases may be reflected in the model. For example, some RLHF-trained language models have been shown to develop a bias towards the opinions of high-income, liberal individuals over their non-RLHF counterparts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib49" title="">49</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx1" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank Tali Dekel and Richard Zhang for helpful discussions. This work was supported by the Sagol Weizmann-MIT Bridge Program and by a Packard Fellowship and a Sloan Research Fellowship to P.I. and an NSF GRFP Fellowship to S.S.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. [2022]</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awadalla et al. [2023]</span>
<span class="ltx_bibblock">
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Openflamingo: An open-source framework for training large autoregressive vision-language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2022]</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2204.05862</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caron et al. [2021]</span>
<span class="ltx_bibblock">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.

</span>
<span class="ltx_bibblock">Emerging properties in self-supervised vision transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  9650–9660, October 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chechik et al. [2010]</span>
<span class="ltx_bibblock">
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio.

</span>
<span class="ltx_bibblock">Large scale online learning of image similarity through ranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Journal of Machine Learning Research</em>, 11(3), 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020a]</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual representations.

</span>
<span class="ltx_bibblock">In Hal Daumé III and Aarti Singh (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 37th International Conference on Machine Learning</em>, volume 119 of <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2">Proceedings of Machine Learning Research</em>, pp.  1597–1607. PMLR, 13–18 Jul 2020a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020b]</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Big self-supervised models are strong semi-supervised learners.

</span>
<span class="ltx_bibblock">In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, volume 33, pp.  22243–22255. Curran Associates, Inc., 2020b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherti et al. [2023]</span>
<span class="ltx_bibblock">
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.

</span>
<span class="ltx_bibblock">Reproducible scaling laws for contrastive language-image learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  2818–2829, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al. [2017]</span>
<span class="ltx_bibblock">
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2009]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">ImageNet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  248–255, 2009.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/CVPR.2009.5206848</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. [2023]</span>
<span class="ltx_bibblock">
Li Ding, Jenny Zhang, Jeff Clune, Lee Spector, and Joel Lehman.

</span>
<span class="ltx_bibblock">Quality diversity through human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2310.12103</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Donahue et al. [2014]</span>
<span class="ltx_bibblock">
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Decaf: A deep convolutional activation feature for generic visual recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International conference on machine learning</em>, pp.  647–655. PMLR, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. [2021]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwibedi et al. [2021]</span>
<span class="ltx_bibblock">
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  9588–9597, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eigen et al. [2014]</span>
<span class="ltx_bibblock">
David Eigen, Christian Puhrsch, and Rob Fergus.

</span>
<span class="ltx_bibblock">Depth map prediction from a single image using a multi-scale deep network, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. [2023]</span>
<span class="ltx_bibblock">
Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee.

</span>
<span class="ltx_bibblock">Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FEL et al. [2022]</span>
<span class="ltx_bibblock">
Thomas FEL, Ivan F Rodriguez Rodriguez, Drew Linsley, and Thomas Serre.

</span>
<span class="ltx_bibblock">Harmonizing the object recognition strategies of deep neural networks with humans.

</span>
<span class="ltx_bibblock">In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</em>, volume 35, pp.  9432–9446. Curran Associates, Inc., 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. [2023]</span>
<span class="ltx_bibblock">
Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola.

</span>
<span class="ltx_bibblock">Dreamsim: Learning new dimensions of human visual similarity using synthetic data.

</span>
<span class="ltx_bibblock">In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Advances in Neural Information Processing Systems</em>, volume 36, pp.  50742–50768. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gatys et al. [2015a]</span>
<span class="ltx_bibblock">
Leon Gatys, Alexander S Ecker, and Matthias Bethge.

</span>
<span class="ltx_bibblock">Texture synthesis using convolutional neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in neural information processing systems</em>, 28, 2015a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gatys et al. [2015b]</span>
<span class="ltx_bibblock">
Leon A Gatys, Alexander S Ecker, and Matthias Bethge.

</span>
<span class="ltx_bibblock">A neural algorithm of artistic style.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1508.06576</em>, 2015b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. [2019]</span>
<span class="ltx_bibblock">
Yuying Ge, Ruimao Zhang, Lingyun Wu, Xiaogang Wang, Xiaoou Tang, and Ping Luo.

</span>
<span class="ltx_bibblock">Deepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  5332–5340, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020]</span>
<span class="ltx_bibblock">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.

</span>
<span class="ltx_bibblock">Momentum Contrast for Unsupervised Visual Representation Learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hebart et al. [2020]</span>
<span class="ltx_bibblock">
Martin N. Hebart, Charles Y. Zheng, Francisco Pereira, and Chris I. Baker.

</span>
<span class="ltx_bibblock">Revealing the multidimensional mental representations of natural objects underlying human similarity judgements.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Nature Human Behaviour</em>, 4(11):1173–1185, October 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41562-020-00951-3</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hebart et al. [2023]</span>
<span class="ltx_bibblock">
Martin N Hebart, Oliver Contier, Lina Teichmann, Adam H Rockter, Charles Y Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I Baker.

</span>
<span class="ltx_bibblock">Things-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">eLife</em>, 12:e82580, feb 2023.

</span>
<span class="ltx_bibblock">ISSN 2050-084X.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.7554/eLife.82580</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jahanian et al. [2021]</span>
<span class="ltx_bibblock">
Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.

</span>
<span class="ltx_bibblock">Generative models as a data source for multiview representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2106.05258</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. [2021]</span>
<span class="ltx_bibblock">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with noisy text supervision.

</span>
<span class="ltx_bibblock">In Marina Meila and Tong Zhang (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 38th International Conference on Machine Learning</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">Proceedings of Machine Learning Research</em>, pp.  4904–4916. PMLR, 18–24 Jul 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. [2016]</span>
<span class="ltx_bibblock">
Justin Johnson, Alexandre Alahi, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Perceptual losses for real-time style transfer and super-resolution.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</em>, pp.  694–711. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosla et al. [2020]</span>
<span class="ltx_bibblock">
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan.

</span>
<span class="ltx_bibblock">Supervised contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Advances in neural information processing systems</em>, 33:18661–18673, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirstain et al. [2023a]</span>
<span class="ltx_bibblock">
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.

</span>
<span class="ltx_bibblock">Pick-a-pic: An open dataset of user preferences for text-to-image generation.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in Neural Information Processing Systems</em>, volume 36, pp.  36652–36663. Curran Associates, Inc., 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/73aacd8b3b05b4b503d58310b523553c-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2023/file/73aacd8b3b05b4b503d58310b523553c-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirstain et al. [2023b]</span>
<span class="ltx_bibblock">
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.

</span>
<span class="ltx_bibblock">Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kobayashi et al. [2022]</span>
<span class="ltx_bibblock">
Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann.

</span>
<span class="ltx_bibblock">Decomposing nerf for editing via feature field distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>, 35:23311–23330, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et al. [2023]</span>
<span class="ltx_bibblock">
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh.

</span>
<span class="ltx_bibblock">Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et al. [2024]</span>
<span class="ltx_bibblock">
Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh.

</span>
<span class="ltx_bibblock">What matters when building vision-language models?, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. [2021]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
Aoxue Li, Tiange Luo, Zhiwu Lu, Tao Xiang, and Liwei Wang.

</span>
<span class="ltx_bibblock">Large-scale few-shot learning: Knowledge transfer with class hierarchy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</em>, pp.  7212–7220, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li &amp; Snavely [2018]</span>
<span class="ltx_bibblock">
Zhengqi Li and Noah Snavely.

</span>
<span class="ltx_bibblock">Megadepth: Learning single-view depth prediction from internet photos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Computer Vision and Pattern Recognition (CVPR)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022]</span>
<span class="ltx_bibblock">
Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.

</span>
<span class="ltx_bibblock">Binsformer: Revisiting adaptive bins for monocular depth estimation, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2023]</span>
<span class="ltx_bibblock">
Yiqing Liang, Eliot Laidlaw, Alexander Meyerowitz, Srinath Sridhar, and James Tompkin.

</span>
<span class="ltx_bibblock">Semantic attention flow fields for monocular dynamic scene decomposition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  21797–21806, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2024]</span>
<span class="ltx_bibblock">
Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Diffusion hyperfeatures: Searching through time and space for semantic correspondence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mix [1999]</span>
<span class="ltx_bibblock">
Kelly S. Mix.

</span>
<span class="ltx_bibblock">Similarity and numerical equivalence: Appearances count.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Cognitive Development</em>, 14(2):269–297, 1999.

</span>
<span class="ltx_bibblock">ISSN 0885-2014.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/S0885-2014(99)00005-2</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muttenthaler et al. [2023a]</span>
<span class="ltx_bibblock">
Lukas Muttenthaler, Jonas Dippel, Lorenz Linhardt, Robert A Vandermeulen, and Simon Kornblith.

</span>
<span class="ltx_bibblock">Human alignment of neural network representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">11th International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, Mai 01-05, 2023</em>. OpenReview.net, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muttenthaler et al. [2023b]</span>
<span class="ltx_bibblock">
Lukas Muttenthaler, Lorenz Linhardt, Jonas Dippel, Robert A Vandermeulen, Katherine Hermann, Andrew Lampinen, and Simon Kornblith.

</span>
<span class="ltx_bibblock">Improving neural network representations using human similarity judgments.

</span>
<span class="ltx_bibblock">In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Advances in Neural Information Processing Systems</em>, volume 36, pp.  50978–51007. Curran Associates, Inc., 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muttenthaler et al. [2024]</span>
<span class="ltx_bibblock">
Lukas Muttenthaler, Klaus Greff, Frieda Born, Bernhard Spitzer, Simon Kornblith, Michael C Mozer, Klaus-Robert Müller, Thomas Unterthiner, and Andrew K Lampinen.

</span>
<span class="ltx_bibblock">Aligning machine and human visual representations across abstraction levels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2409.06509</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab et al. [2023]</span>
<span class="ltx_bibblock">
Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paiss et al. [2023]</span>
<span class="ltx_bibblock">
Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel.

</span>
<span class="ltx_bibblock">Teaching clip to count to ten.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pp.  3147–3157, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In Marina Meila and Tong Zhang (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 38th International Conference on Machine Learning</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib46.2.2">Proceedings of Machine Learning Research</em>, pp.  8748–8763. PMLR, 18–24 Jul 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson et al. [2020]</span>
<span class="ltx_bibblock">
Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka.

</span>
<span class="ltx_bibblock">Contrastive learning with hard negative samples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2010.04592</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky et al. [2015]</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li.

</span>
<span class="ltx_bibblock">ImageNet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Int. J. Comput. Vis.</em>, 115(3):211–252, 2015.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/s11263-015-0816-y</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santurkar et al. [2023]</span>
<span class="ltx_bibblock">
Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto.

</span>
<span class="ltx_bibblock">Whose opinions do language models reflect?

</span>
<span class="ltx_bibblock">In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib49.2.2">Proceedings of Machine Learning Research</em>, pp.  29971–30004. PMLR, 23–29 Jul 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v202/santurkar23a.html" title="">https://proceedings.mlr.press/v202/santurkar23a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharif Razavian et al. [2014]</span>
<span class="ltx_bibblock">
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.

</span>
<span class="ltx_bibblock">Cnn features off-the-shelf: an astounding baseline for recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</em>, pp.  806–813, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. [2023]</span>
<span class="ltx_bibblock">
Prafull Sharma, Julien Philip, Michaël Gharbi, Bill Freeman, Fredo Durand, and Valentin Deschaintre.

</span>
<span class="ltx_bibblock">Materialistic: Selecting similar materials in images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">ACM Transactions on Graphics (TOG)</em>, 42(4):1–14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. [2023]</span>
<span class="ltx_bibblock">
William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, and Phillip Isola.

</span>
<span class="ltx_bibblock">Distilled feature fields enable few-shot language-guided manipulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2308.07931</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohn et al. [2023]</span>
<span class="ltx_bibblock">
Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al.

</span>
<span class="ltx_bibblock">Styledrop: Text-to-image generation in any style.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2306.00983</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sucholutsky et al. [2023]</span>
<span class="ltx_bibblock">
Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C Love, Erin Grant, Jascha Achterberg, Joshua B Tenenbaum, et al.

</span>
<span class="ltx_bibblock">Getting aligned on representational alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2310.13018</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. [2023]</span>
<span class="ltx_bibblock">
Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola.

</span>
<span class="ltx_bibblock">Learning vision from models rivals learning vision from data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ArXiv</em>, abs/2312.17742, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong et al. [2024]</span>
<span class="ltx_bibblock">
Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie.

</span>
<span class="ltx_bibblock">Eyes wide shut? exploring the visual shortcomings of multimodal llms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp.  9568–9578, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tumanyan et al. [2024]</span>
<span class="ltx_bibblock">
Narek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel.

</span>
<span class="ltx_bibblock">Dino-tracker: Taming dino for self-supervised point tracking in a single video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2403.14548</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023]</span>
<span class="ltx_bibblock">
Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li.

</span>
<span class="ltx_bibblock">Human preference score: Better aligning text-to-image models with human preference.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023</em>, pp.  2096–2105. IEEE, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICCV51070.2023.00200</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zbontar et al. [2021]</span>
<span class="ltx_bibblock">
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny.

</span>
<span class="ltx_bibblock">Barlow twins: Self-supervised learning via redundancy reduction.

</span>
<span class="ltx_bibblock">In Marina Meila and Tong Zhang (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 38th International Conference on Machine Learning</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib59.2.2">Proceedings of Machine Learning Research</em>, pp.  12310–12320. PMLR, 18–24 Jul 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. [2019]</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, André Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby.

</span>
<span class="ltx_bibblock">A large-scale study of representation learning with the visual task adaptation benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv: Computer Vision and Pattern Recognition</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2018]</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as a perceptual metric.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023]</span>
<span class="ltx_bibblock">
Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al.

</span>
<span class="ltx_bibblock">Hive: Harnessing human feedback for instructional visual editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2303.09618</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1" lang="en">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>
<div class="ltx_para" id="Ax1.p1">
<p class="ltx_p" id="Ax1.p1.1">In the Appendix, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1" title="Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">A</span></a> contains additional experiments and ablations, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2" title="Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">B</span></a> contains qualitative results and examples from different perceptual datasets, and Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3" title="Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">C</span></a> contains methodological/implementation details.</p>
</div>
</section>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experiments</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Classification with the VTAB Benchmark</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We evaluate on the Visual Task Adaptation Benchmark (VTAB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib60" title="">60</a>]</cite>, a comprehensive set of nineteen classification datasets that is commonly used to study the transferability of representations. VTAB covers a broad set of domains beyond classic natural image datasets, including medical images, scene understanding, and fine-grained classification. Several of the datasets are drawn from domains that are not included in the NIGHTS dataset.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">In Tables <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.T5" title="Table 5 ‣ A.1 Classification with the VTAB Benchmark ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">5</span></a>a-b, we show classification results on all VTAB datasets for each backbone, and its human-aligned version (denoted with -HA). The VTAB datasets are divided into three categories: Natural, Specialized, and Structured. Natural datasets include standard vision benchmarks; specialized datasets include benchmarks captured through specialized imaging equipment, such as satellites and microscopes; structured datasets include benchmarks that focus on structure and layout, with both naturally-captured and simulated images.</p>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1">We find that human-aligned models largely lead to worse performance on standard natural tasks. Results on specialized tasks are mixed; for most datasets, human alignment helps or hurts by a marginal amount. Results on structured datasets (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.T6" title="Table 6 ‣ A.1 Classification with the VTAB Benchmark ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">6</span></a>) are also mixed, however on particular datasets such as sNORB, dSprites-Orientation, and dSprites-Location, alignment improves performance across backbones. We remark, however, that these structured datasets may have limited evaluation utility. The Clevr-Count task, for instance, has a numerical ground truth (i.e. the number of objects in an image) in which the distance from the correct answer is meaningful; thus it may be ill-suited to classification, and better suited to continuous evaluations such as RMSE or MAE, which we report in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS3" title="4.3 Counting ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="A1.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_middle ltx_transformed_outer" id="A1.SS1.7.7.7" style="width:286.2pt;height:395.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(58.3pt,-53.1pt) scale(1.3677403953289,1.3677403953289) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.SS1.7.7.7.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.8.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="A1.SS1.7.7.7.7.8.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="7" id="A1.SS1.7.7.7.7.8.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.8.1.2.1">Natural</span></th>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.9.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A1.SS1.7.7.7.7.9.2.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.9.2.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.7.7.7.7.9.2.2">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.7.7.7.7.9.2.2.1" style="width:6.9pt;height:48.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:48.1pt;transform:translate(-20.56pt,-20.56pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.7.7.7.7.9.2.2.1.1">Caltech101</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.7.7.7.7.9.2.3">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.7.7.7.7.9.2.3.1" style="width:6.8pt;height:49.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:49.4pt;transform:translate(-21.31pt,-21.31pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.7.7.7.7.9.2.3.1.1">CIFAR-100</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.7.7.7.7.9.2.4">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.7.7.7.7.9.2.4.1" style="width:6.8pt;height:22.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:22.5pt;transform:translate(-7.83pt,-7.83pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.7.7.7.7.9.2.4.1.1">DTD</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.7.7.7.7.9.2.5">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.7.7.7.7.9.2.5.1" style="width:6.9pt;height:48.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:48.3pt;transform:translate(-20.67pt,-20.67pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.7.7.7.7.9.2.5.1.1">Flowers102</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.7.7.7.7.9.2.6">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.7.7.7.7.9.2.6.1" style="width:6.8pt;height:18.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:18.8pt;transform:translate(-5.99pt,-5.99pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.7.7.7.7.9.2.6.1.1">Pets</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.7.7.7.7.9.2.7">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.7.7.7.7.9.2.7.1" style="width:6.8pt;height:31.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.7pt;transform:translate(-12.42pt,-12.42pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.7.7.7.7.9.2.7.1.1">Sun397</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.7.7.7.7.9.2.8">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.7.7.7.7.9.2.8.1" style="width:6.8pt;height:28.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:28.1pt;transform:translate(-10.61pt,-10.61pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.7.7.7.7.9.2.8.1.1">SVHN</p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.7.7.7.7.10.1.1">DINO</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.10.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.10.1.2.1">95.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.10.1.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.10.1.3.1">83.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.10.1.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.10.1.4.1">78.7</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.10.1.5"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.10.1.5.1">96.7</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.10.1.6"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.10.1.6.1">93.1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.10.1.7"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.10.1.7.1">74.8</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.10.1.8">70.8</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.7.7.7.7.11.2.1">DINO-HA</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.11.2.2">93.6</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.11.2.3">80.8</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.11.2.4">72.6</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.11.2.5">93.2</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.11.2.6">91.0</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.11.2.7">69.7</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.11.2.8"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.11.2.8.1">73.2</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.3.3.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.3.3.3.3.3.4">DINOv2</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.3.3.3.3.3.5">95.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.1.1.1.1.1.1.1">88.9<sup class="ltx_sup" id="A1.SS1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.1.1.1.1.1.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.3.3.3.3.3.6"><span class="ltx_text ltx_font_bold" id="A1.SS1.3.3.3.3.3.6.1">82.8</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.2.2.2.2.2.2.1">99.7<sup class="ltx_sup" id="A1.SS1.2.2.2.2.2.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.2.2.2.2.2.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.3.3.3.3.3.3.1">95.9<sup class="ltx_sup" id="A1.SS1.3.3.3.3.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.3.3.3.3.3.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.3.3.3.3.3.7"><span class="ltx_text ltx_font_bold" id="A1.SS1.3.3.3.3.3.7.1">81.9</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.3.3.3.3.3.8">61.2</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.7.7.7.7.12.3.1">DINOv2-HA</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.12.3.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.12.3.2.1">96.2</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.12.3.3">87.7</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.12.3.4">80.2</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.12.3.5">97.9</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.12.3.6">89.7</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.12.3.7">77.0</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.12.3.8"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.12.3.8.1">63.3</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.7.7.7.7.13.4.1">CLIP</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.13.4.2">95.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.13.4.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.13.4.3.1">76.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.13.4.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.13.4.4.1">76.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.13.4.5"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.13.4.5.1">95.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.13.4.6"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.13.4.6.1">87.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.13.4.7"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.13.4.7.1">80.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.13.4.8">65.4</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.7.7.7.7.14.5.1">CLIP-HA</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.14.5.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.14.5.2.1">95.7</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.14.5.3">76.4</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.14.5.4">76.0</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.14.5.5">93.4</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.14.5.6">85.5</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.14.5.7">78.1</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.14.5.8"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.14.5.8.1">69.2</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.7.7.7.7.15.6.1">OpenCLIP</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.15.6.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.15.6.2.1">97.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.15.6.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.15.6.3.1">82.4</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.15.6.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.15.6.4.1">81.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.15.6.5"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.15.6.5.1">97.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.15.6.6"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.15.6.6.1">90.4</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.15.6.7"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.15.6.7.1">82.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.15.6.8"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.15.6.8.1">73.1</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.16.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.7.7.7.7.16.7.1">OpenCLIP-HA</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.16.7.2">96.0</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.16.7.3">75.8</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.16.7.4">77.3</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.16.7.5">93.5</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.16.7.6">83.7</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.16.7.7">76.6</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.16.7.8">70.9</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.17.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.7.7.7.7.17.8.1">SynCLR</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.17.8.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.17.8.2.1">95.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.17.8.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.17.8.3.1">74.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.17.8.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.17.8.4.1">80.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.17.8.5"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.17.8.5.1">98.8</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.17.8.6"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.17.8.6.1">92.9</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.17.8.7"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.17.8.7.1">81.4</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.7.7.7.7.17.8.8">56.2</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.18.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.7.7.7.7.18.9.1">SynCLR-HA</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.18.9.2">95.0</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.18.9.3">69.7</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.18.9.4">77.8</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.18.9.5">95.9</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.18.9.6">90.5</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.18.9.7">75.7</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.7.7.7.7.18.9.8"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.18.9.8.1">70.9</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.6.6.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.6.6.6.6.6.4">Ensemble</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.4.4.4.4.4.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.4.4.4.4.4.1.1">97.4<sup class="ltx_sup" id="A1.SS1.4.4.4.4.4.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.4.4.4.4.4.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.6.6.6.6.6.5"><span class="ltx_text ltx_font_bold" id="A1.SS1.6.6.6.6.6.5.1">86.8</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.5.5.5.5.5.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.5.5.5.5.5.2.1">84.3<sup class="ltx_sup" id="A1.SS1.5.5.5.5.5.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.5.5.5.5.5.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.6.6.6.6.6.6"><span class="ltx_text ltx_font_bold" id="A1.SS1.6.6.6.6.6.6.1">98.8</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.6.6.6.6.6.7"><span class="ltx_text ltx_font_bold" id="A1.SS1.6.6.6.6.6.7.1">94.4</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.6.6.6.6.6.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.6.6.6.6.6.3.1">83.9<sup class="ltx_sup" id="A1.SS1.6.6.6.6.6.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.6.6.6.6.6.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.6.6.6.6.6.8">80.1</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.7.7.7.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.SS1.7.7.7.7.7.2">Ensemble-HA</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.7.7.7.7.7.3">97.3</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.7.7.7.7.7.4">85.0</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.7.7.7.7.7.5">81.4</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.7.7.7.7.7.6">96.4</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.7.7.7.7.7.7">91.9</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.7.7.7.7.7.8">82.5</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.7.7.7.7.7.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.7.7.7.7.7.1.1">82.2<sup class="ltx_sup" id="A1.SS1.7.7.7.7.7.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.7.7.7.7.7.1.1.1.1">†</span></sup></span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A1.SS1.12.12" style="width:127.9pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.SS1.12.12.5" style="width:433.6pt;height:914.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(147.3pt,-310.8pt) scale(3.1180680647017,3.1180680647017) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.SS1.12.12.5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4" id="A1.SS1.12.12.5.5.6.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.6.1.1.1">Specialized</span></th>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.12.12.5.5.7.2.1">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.12.12.5.5.7.2.1.1" style="width:8.9pt;height:43.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:43.3pt;transform:translate(-17.22pt,-16.25pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.12.12.5.5.7.2.1.1.1">Camelyon</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.12.12.5.5.7.2.2">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.12.12.5.5.7.2.2.1" style="width:6.8pt;height:40.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:40.7pt;transform:translate(-16.94pt,-16.94pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.12.12.5.5.7.2.2.1.1">EuroSAT</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.12.12.5.5.7.2.3">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.12.12.5.5.7.2.3.1" style="width:6.8pt;height:36.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:36.9pt;transform:translate(-15.04pt,-15.04pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.12.12.5.5.7.2.3.1.1">Resisc45</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.SS1.12.12.5.5.7.2.4">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.SS1.12.12.5.5.7.2.4.1" style="width:8.9pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:54.0pt;transform:translate(-22.57pt,-21.6pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.SS1.12.12.5.5.7.2.4.1.1">Retinopathy</p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.8.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.8.1.1">85.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.8.1.2">97.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.8.1.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.8.1.3.1">93.8</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.8.1.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.8.1.4.1">77.9</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.9.2">
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.9.2.1">83.7</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.9.2.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.9.2.2.1">97.3</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.9.2.3">93.2</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.9.2.4">76.9</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.8.8.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.8.8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.8.8.1.1.1.1.1">86.7<sup class="ltx_sup" id="A1.SS1.8.8.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.8.8.1.1.1.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.8.8.1.1.1.2">96.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.8.8.1.1.1.3">92.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.8.8.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.8.8.1.1.1.4.1">78.6</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.10.3">
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.10.3.1">84.6</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.10.3.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.10.3.2.1">97.0</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.10.3.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.10.3.3.1">93.5</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.10.3.4">77.9</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.11.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.11.4.1">83.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.11.4.2">95.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.11.4.3">92.9</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.11.4.4">75.8</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.12.5">
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.12.5.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.12.5.1.1">84.0</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.12.5.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.12.5.2.1">95.9</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.12.5.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.12.5.3.1">93.0</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.12.5.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.12.5.4.1">76.1</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.13.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.13.6.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.13.6.1.1">82.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.13.6.2">96.4</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.13.6.3"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.13.6.3.1">83.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.12.12.5.5.13.6.4">96.1</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.14.7">
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.14.7.1">82.9</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.14.7.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.14.7.2.1">96.5</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.14.7.3">92.6</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.14.7.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.14.7.4.1">76.2</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.9.9.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.9.9.2.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.9.9.2.2.2.2.1">86.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.9.9.2.2.2.3">96.6</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.9.9.2.2.2.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.9.9.2.2.2.4.1">94.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.9.9.2.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.9.9.2.2.2.1.1">78.8<sup class="ltx_sup" id="A1.SS1.9.9.2.2.2.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.9.9.2.2.2.1.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.15.8">
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.15.8.1">85.1</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.15.8.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.15.8.2.1">96.8</span></td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.15.8.3">93.4</td>
<td class="ltx_td ltx_align_left" id="A1.SS1.12.12.5.5.15.8.4">78.0</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.11.11.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.11.11.4.4.4.3">85.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.10.10.3.3.3.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.10.10.3.3.3.1.1">98.0<sup class="ltx_sup" id="A1.SS1.10.10.3.3.3.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.10.10.3.3.3.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.11.11.4.4.4.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.11.11.4.4.4.2.1">96.1<sup class="ltx_sup" id="A1.SS1.11.11.4.4.4.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.11.11.4.4.4.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.11.11.4.4.4.4"><span class="ltx_text ltx_font_bold" id="A1.SS1.11.11.4.4.4.4.1">78.7</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.12.12.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.12.12.5.5.5.2">83.5</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.12.12.5.5.5.3">97.9</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.12.12.5.5.5.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.12.12.5.5.5.1.1">96.1<sup class="ltx_sup" id="A1.SS1.12.12.5.5.5.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.SS1.12.12.5.5.5.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.12.12.5.5.5.4">78.1</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T5.15.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text ltx_font_bold" id="A1.T5.16.2" style="font-size:90%;">Performance on VTAB natural subset (left) and specialized subset (right).</span></figcaption>
</figure>
<figure class="ltx_table" id="A1.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T6.9.9" style="width:260.2pt;height:225.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-42.9pt,37.1pt) scale(0.752021021722061,0.752021021722061) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T6.9.9.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T6.9.9.9.10.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="A1.T6.9.9.9.10.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="8" id="A1.T6.9.9.9.10.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.10.1.2.1">Structured</span></th>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A1.T6.9.9.9.11.2.1"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.11.2.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T6.9.9.9.11.2.2">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.9.9.9.11.2.2.1" style="width:6.9pt;height:53.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:53.9pt;transform:translate(-23.49pt,-23.49pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.T6.9.9.9.11.2.2.1.1">Clevr-Count</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T6.9.9.9.11.2.3">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.9.9.9.11.2.3.1" style="width:6.9pt;height:45.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.2pt;transform:translate(-19.14pt,-19.14pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.T6.9.9.9.11.2.3.1.1">Clevr-Dist</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T6.9.9.9.11.2.4">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.9.9.9.11.2.4.1" style="width:6.9pt;height:33.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:33.6pt;transform:translate(-13.33pt,-13.33pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.T6.9.9.9.11.2.4.1.1">DMLab</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T6.9.9.9.11.2.5">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.9.9.9.11.2.5.1" style="width:8.9pt;height:39.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:39.9pt;transform:translate(-15.5pt,-14.53pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.T6.9.9.9.11.2.5.1.1">dSpr-Loc</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T6.9.9.9.11.2.6">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.9.9.9.11.2.6.1" style="width:8.9pt;height:42.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.3pt;transform:translate(-16.69pt,-15.72pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.T6.9.9.9.11.2.6.1.1">dSpr-Orti</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T6.9.9.9.11.2.7">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.9.9.9.11.2.7.1" style="width:6.8pt;height:51pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.0pt;transform:translate(-22.1pt,-22.1pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.T6.9.9.9.11.2.7.1.1">KITTI-Dist</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T6.9.9.9.11.2.8">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.9.9.9.11.2.8.1" style="width:6.8pt;height:60.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:60.1pt;transform:translate(-26.61pt,-26.61pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.T6.9.9.9.11.2.8.1.1">sNORB-Azim</p>
</span></div>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="A1.T6.9.9.9.11.2.9">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T6.9.9.9.11.2.9.1" style="width:6.9pt;height:56.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:56.3pt;transform:translate(-24.68pt,-24.68pt) rotate(-90deg) ;">
<p class="ltx_p" id="A1.T6.9.9.9.11.2.9.1.1">sNORB-Elev</p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.9.9.9.12.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T6.9.9.9.12.1.1">DINO</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.12.1.2">82.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.12.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.12.1.3.1">58.1</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.12.1.4"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.12.1.4.1">53.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.12.1.5">63.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.12.1.6">57.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.12.1.7"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.12.1.7.1">72.6</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.12.1.8">61.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.12.1.9"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.12.1.9.1">56.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.13.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.9.9.9.13.2.1">DINO-HA</th>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.13.2.2">79.2</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.13.2.3">57.2</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.13.2.4">51.1</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.13.2.5"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.13.2.5.1">81.5</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.13.2.6"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.13.2.6.1">66.6</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.13.2.7">66.2</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.13.2.8"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.13.2.8.1">65.8</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.13.2.9">55.3</td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.14.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T6.9.9.9.14.3.1">DINOv2</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.14.3.2">87.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.14.3.3">54.6</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.14.3.4"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.14.3.4.1">57.7</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.14.3.5">59.7</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.14.3.6">68.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.14.3.7">62.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.14.3.8">55.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.14.3.9">52.8</td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.15.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.9.9.9.15.4.1">DINOv2-HA</th>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.15.4.2">83.6</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.15.4.3"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.15.4.3.1">59.0</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.15.4.4">53.7</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.15.4.5"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.15.4.5.1">79.2</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.15.4.6"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.15.4.6.1">74.1</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.15.4.7"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.15.4.7.1">69.1</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.15.4.8"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.15.4.8.1">64.7</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.15.4.9"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.15.4.9.1">55.9</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.16.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T6.9.9.9.16.5.1">CLIP</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.16.5.2">70.6</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.16.5.3">55.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.16.5.4">50.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.16.5.5">67.6</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.16.5.6">55.6</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.16.5.7"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.16.5.7.1">65.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.16.5.8">37.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.16.5.9">47.0</td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.17.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.9.9.9.17.6.1">CLIP-HA</th>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.17.6.2"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.17.6.2.1">72.9</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.17.6.3"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.17.6.3.1">57.8</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.17.6.4"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.17.6.4.1">51.4</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.17.6.5"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.17.6.5.1">80.2</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.17.6.6"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.17.6.6.1">60.3</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.17.6.7">64.6</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.17.6.8"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.17.6.8.1">42.6</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.17.6.9"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.17.6.9.1">47.2</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.18.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T6.9.9.9.18.7.1">OpenCLIP</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.18.7.2"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.18.7.2.1">78.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.18.7.3">53.8</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.18.7.4"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.18.7.4.1">52.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.18.7.5">63.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.18.7.6">55.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.18.7.7"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.18.7.7.1">67.9</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.18.7.8">38.0</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.9.9.9.18.7.9"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.18.7.9.1">50.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.19.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.9.9.9.19.8.1">OpenCLIP-HA</th>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.19.8.2">76.7</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.19.8.3"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.19.8.3.1">57.6</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.19.8.4">51.5</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.19.8.5"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.19.8.5.1">84.9</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.19.8.6"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.19.8.6.1">64.1</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.19.8.7">63.3</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.19.8.8"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.19.8.8.1">46.7</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.19.8.9">50.1</td>
</tr>
<tr class="ltx_tr" id="A1.T6.3.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T6.3.3.3.3.4">SynCLR</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.1.1">90.9<sup class="ltx_sup" id="A1.T6.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.1.1.1.1.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T6.2.2.2.2.2.1">63.8<sup class="ltx_sup" id="A1.T6.2.2.2.2.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.2.2.2.2.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.3.3.3.3.5"><span class="ltx_text ltx_font_bold" id="A1.T6.3.3.3.3.5.1">57.3</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.3.3.3.3.6">68.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.3.3.3.3.7">55.2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T6.3.3.3.3.3.1">76.9<sup class="ltx_sup" id="A1.T6.3.3.3.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.3.3.3.3.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.3.3.3.3.8">52.1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.3.3.3.3.9"><span class="ltx_text ltx_font_bold" id="A1.T6.3.3.3.3.9.1">60.5</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.20.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.9.9.9.20.9.1">SynCLR-HA</th>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.20.9.2">81.7</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.20.9.3">61.5</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.20.9.4">52.9</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.20.9.5"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.20.9.5.1">86.1</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.20.9.6"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.20.9.6.1">64.3</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.20.9.7">63.6</td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.20.9.8"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.20.9.8.1">66.6</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.9.9.9.20.9.9">58.5</td>
</tr>
<tr class="ltx_tr" id="A1.T6.5.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T6.5.5.5.5.3">Ensemble</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.5.5.5.5.4"><span class="ltx_text ltx_font_bold" id="A1.T6.5.5.5.5.4.1">89.2</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.5.5.5.5.5">63.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.4.4.4.4.1"><span class="ltx_text ltx_font_bold" id="A1.T6.4.4.4.4.1.1">58.4<sup class="ltx_sup" id="A1.T6.4.4.4.4.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.4.4.4.4.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.5.5.5.5.6">72.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.5.5.5.5.7">67.3</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.5.5.5.5.8"><span class="ltx_text ltx_font_bold" id="A1.T6.5.5.5.5.8.1">76.5</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.5.5.5.5.9">76.9</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.5.5.5.5.2"><span class="ltx_text ltx_font_bold" id="A1.T6.5.5.5.5.2.1">65.5<sup class="ltx_sup" id="A1.T6.5.5.5.5.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.5.5.5.5.2.1.1.1">†</span></sup></span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.9.9.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T6.9.9.9.9.5">Ensemble-HA</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.9.9.9.9.6">85.5</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.6.6.6.6.1"><span class="ltx_text ltx_font_bold" id="A1.T6.6.6.6.6.1.1">63.8<sup class="ltx_sup" id="A1.T6.6.6.6.6.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.6.6.6.6.1.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.9.9.9.9.7">57.1</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.7.7.7.7.2"><span class="ltx_text ltx_font_bold" id="A1.T6.7.7.7.7.2.1">91.8<sup class="ltx_sup" id="A1.T6.7.7.7.7.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.7.7.7.7.2.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.8.8.8.8.3"><span class="ltx_text ltx_font_bold" id="A1.T6.8.8.8.8.3.1">76.4<sup class="ltx_sup" id="A1.T6.8.8.8.8.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.8.8.8.8.3.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.9.9.9.9.8">70.0</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.9.9.9.9.4"><span class="ltx_text ltx_font_bold" id="A1.T6.9.9.9.9.4.1">81.0<sup class="ltx_sup" id="A1.T6.9.9.9.9.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A1.T6.9.9.9.9.4.1.1.1">†</span></sup></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.9.9.9.9.9">62.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T6.12.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text ltx_font_bold" id="A1.T6.13.2" style="font-size:90%;">Performance on VTAB structured subset.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Additional dataset ablations</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS5" title="4.5 What type of human similarity annotation is most beneficial? ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4.5</span></a> we show that tuning on NIGHTS leads to larger performance improvements on object counting and instance retrieval than training on other triplet datasets. Here, we show that this finding is consistent across dense prediction tasks as well. We evaluate models trained on NIGHTS, BAPPS, THINGS, and ImageNet – as described in section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS5" title="4.5 What type of human similarity annotation is most beneficial? ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4.5</span></a> – on semantic segmentation and depth estimation. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.F9" title="Figure 9 ‣ A.2 Additional dataset ablations ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">9</span></a>- <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.F10" title="Figure 10 ‣ A.2 Additional dataset ablations ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">10</span></a>, training on NIGHTS outperforms training on all other datasets.</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">We additionally run this ablation on a subset of VTAB (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.F11" title="Figure 11 ‣ A.2 Additional dataset ablations ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">11</span></a>a). On these classification datasets, base models perform best; the exception is sNORB (pose prediction) for which NIGHTS is best. Amongst perceptual datasets, NIGHTS is sometimes outperformed by BAPPS/ImageNet. This result is consistent with our findings in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.SS1" title="A.1 Classification with the VTAB Benchmark ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">A.1</span></a>, that tuning on NIGHTS often fails to improve classification performance.</p>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1">Finally, we ablate the <span class="ltx_text ltx_font_italic" id="A1.SS2.p3.1.1">strength</span> of alignment – i.e. the training loss when fine-tuning on different perceptual datasets. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.F11" title="Figure 11 ‣ A.2 Additional dataset ablations ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">11</span></a>b we show the downstream performance on DeepFashion2 when fine-tuning for increasing numbers of steps on different datasets. Tuning on NIGHTS outperforms other datasets over the full training trajectory. Moreover, while performance rises significantly with a small amount of alignment to NIGHTS, it trends down after &gt;1000 steps. This indicates that a small amount of alignment is helpful, however overfitting may harm performance.</p>
</div>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="128" id="A1.F9.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F9.3.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text ltx_font_bold" id="A1.F9.4.2" style="font-size:90%;">Dataset ablations on semantic segmentation. <span class="ltx_text ltx_font_medium" id="A1.F9.4.2.1"> Following the same procedure as in section 4.5 and the segmentation training in section 4.1 of the main paper, we ablate low/mid/high-level similarity. We find that tuning on mid-level similarity with NIGHTS provides the most improvement, with many other cases degrading the base DINO backbone.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="174" id="A1.F10.g1" src="x10.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F10.3.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text ltx_font_bold" id="A1.F10.4.2" style="font-size:90%;">Dataset ablations on depth estimation. <span class="ltx_text ltx_font_medium" id="A1.F10.4.2.1"> Following the ablation and depth training setups described in the main paper, we evaluate low/mid/high-level similarity. We find that tuning on mid-level similarity with NIGHTS results in the lowest error (RMSE), and that the other cases often result in worse performance than base DINO.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="192" id="A1.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F11.3.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text ltx_font_bold" id="A1.F11.4.2" style="font-size:90%;">(left) Dataset ablations on a subset of VTAB. <span class="ltx_text ltx_font_medium" id="A1.F11.4.2.1"> Following the same procedure as in section 4.5 and the segmentation training in section 4.1 of the main paper, we ablate low/mid/high-level similarity. We find that all perceptual datasets degrade the DINO backbone, with the exception of sNORB, for which NIGHTS is best. We also note that training on BAPPS or ImageNet often better preserves base model performance than NIGHTS. </span>(right) Training-step ablations on DeepFashion2. <span class="ltx_text ltx_font_medium" id="A1.F11.4.2.2"> We ablate low/mid/high-level similarity and test models finetuned for different numbers of steps on instance retrieval. We find that a small amount of finetuning on NIGHTS is best, after which performances declines (though remains above the base model). <span class="ltx_text ltx_font_italic" id="A1.F11.4.2.2.1">Note that the legend applies to both the left and right figure.</span></span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Additional RAG results</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">We replicate our RAG experiment on OpenFlamingo in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS2" title="4.2 Retrieval-augmented generation ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4.2</span></a> on IDEFICS2, a recently released 8B multimodal model achieving state-of-the-art results across several benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib56" title="">56</a>]</cite>. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.F12" title="Figure 12 ‣ A.3 Additional RAG results ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">12</span></a>, across the same four classification datasets, performance consistently improved when using NIGHTS-tuned models in the RAG pipeline. These results validate our RAG results on OpenFlamingo, suggesting that performance gains from perceptual alignment are not specific to a particular VLM.</p>
</div>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="A1.F12.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F12.3.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text ltx_font_bold" id="A1.F12.4.2" style="font-size:90%;">Retrieval-augmented generation results on image classification for four different VTAB datasets.<span class="ltx_text ltx_font_medium" id="A1.F12.4.2.1"> We evaluate the IDEFICS2 model, an 8B-parameter multimodal model released more recently than OpenFlamingo and capable of in-context learning. We follow the same experimental setup in retrieving examples as detailed in <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S4.SS2" title="4.2 Retrieval-augmented generation ‣ 4 Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Perceptual alignment for CNNs</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">Our main experiments assess the effects of perceptual alignment for an array of ViT models, as these are state-of-the-art pretrained vision representations. Of further interest, however, is whether the same effects are observed for pre-trained CNNs as well.</p>
</div>
<div class="ltx_para" id="A1.SS4.p2">
<p class="ltx_p" id="A1.SS4.p2.1">We assess the effects of perceptual alignment with NIGHTS on two popular CNNs: ResNet50 and ConvNeXt-B, using the same loss described in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#S3.SS2" title="3.2 Image-level objective ‣ 3 Learning from perceptual judgments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">3.2</span></a>. We evaluate on counting and instance retrieval tasks; our results are shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A1.T7" title="Table 7 ‣ A.4 Perceptual alignment for CNNs ‣ Appendix A Experiments ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">7</span></a>a-b. Similarly to ViTs, human-aligned CNNs outperform their pretrained counterparts across both tasks. Note that we report results for both ConvNeXt and ResNet, however acknowledge that the accuracies for ResNet are likely too small to draw conclusions from.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle ltx_transformed_outer" id="A1.T7.2" style="width:169.1pt;height:292.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,1.1pt) scale(0.992371431755214,0.992371431755214) ;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T7.2.1" style="width:433.6pt;height:293.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(137.1pt,-92.8pt) scale(2.7190904458116,2.7190904458116) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T7.2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.2.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.T7.2.1.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T7.2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T7.2.1.1.1.1.2.1">Clevr-Count</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.T7.2.1.1.2.2.1.1">Model</span></th>
<td class="ltx_td ltx_align_center" id="A1.T7.2.1.1.2.2.2">RMSE</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.1.1.2.2.3">MAE</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T7.2.1.1.3.3.1">ConvNeXt</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.1.1.3.3.2">2.045</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.1.1.3.3.3">1.522</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.2.1.1.4.4.1">ConvNeXt-HA</th>
<td class="ltx_td ltx_align_center" id="A1.T7.2.1.1.4.4.2"><span class="ltx_text ltx_font_bold" id="A1.T7.2.1.1.4.4.2.1">1.631</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.1.1.4.4.3"><span class="ltx_text ltx_font_bold" id="A1.T7.2.1.1.4.4.3.1">1.193</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T7.2.1.1.5.5.1">ResNet</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.1.1.5.5.2">3.140</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.1.1.5.5.3">2.551</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T7.2.1.1.6.6.1">ResNet-HA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.2.1.1.6.6.2"><span class="ltx_text ltx_font_bold" id="A1.T7.2.1.1.6.6.2.1">1.729</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.2.1.1.6.6.3"><span class="ltx_text ltx_font_bold" id="A1.T7.2.1.1.6.6.3.1">1.282</span></td>
</tr>
</tbody>
</table>
</span></div>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle ltx_transformed_outer" id="A1.T7.3" style="width:203.8pt;height:239.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.9pt) scale(0.992371431755214,0.992371431755214) ;">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T7.3.1" style="width:433.6pt;height:240.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(119.3pt,-66.1pt) scale(2.22432340155609,2.22432340155609) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T7.3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.3.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.T7.3.1.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T7.3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.1.1.1.2.1">DeepFashion2</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.3.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.1.2.2.1.1">Model</span></th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.1.1.2.2.2">Top-1</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.1.1.2.2.3">Top-3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.1.1.2.2.4">Top-5</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T7.3.1.1.3.3.1">ConvNeXt</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.1.1.3.3.2">2.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.1.1.3.3.3">3.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.1.1.3.3.4">4.59</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.3.1.1.4.4.1">ConvNeXt-HA</th>
<td class="ltx_td ltx_align_center" id="A1.T7.3.1.1.4.4.2"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.1.4.4.2.1">2.81</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.1.1.4.4.3"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.1.4.4.3.1">4.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.1.1.4.4.4"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.1.4.4.4.1">5.98</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T7.3.1.1.5.5.1">ResNet</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.1.1.5.5.2">0.018</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.1.5.5.3.1">0.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.1.1.5.5.4">0.14</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T7.3.1.1.6.6.1">ResNet-HA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.1.1.6.6.2">0.018</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.1.1.6.6.3">0.074</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.1.1.6.6.4"><span class="ltx_text ltx_font_bold" id="A1.T7.3.1.1.6.6.4.1">0.17</span></td>
</tr>
</tbody>
</table>
</span></div>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T7.5.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="A1.T7.6.2" style="font-size:90%;">Human-aligned vs. pretrained CNNs, evaluated on counting and instance-retrieval. In most cases, human-aligned CNNs perform better.</span></figcaption>
</figure>
<div class="ltx_para" id="A1.SS4.p3">
<p class="ltx_p" id="A1.SS4.p3.1">Due to a lack of open-source LoRA implementations for CNNs, we instead train MLPs on top of the respective final layers. Previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite> indicated that training MLPs with NIGHTS can still affect representations, but may be less effective than full fine-tuning. Nonetheless, we still see downstream improvements with this method.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Qualitative examples</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Additional visualizations</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">See Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2.F13" title="Figure 13 ‣ B.1 Additional visualizations ‣ Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">13</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2.F14" title="Figure 14 ‣ B.1 Additional visualizations ‣ Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">14</span></a> for additional examples of instance retrieval on DeepFashion2 and object counting on Clevr-count.</p>
</div>
<figure class="ltx_figure" id="A2.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="422" id="A2.F13.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F13.3.1.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="A2.F13.4.2" style="font-size:90%;">Additional visualizations for top-3 retrieved examples for a given query image in DeepFashion2. </span></figcaption>
</figure>
<figure class="ltx_figure" id="A2.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="198" id="A2.F14.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F14.3.1.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text" id="A2.F14.4.2" style="font-size:90%;">Additional Clevr-count examples comparing base and human-model nearest-neighbor image retrievals. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Dataset examples</h3>
<figure class="ltx_figure" id="A2.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="363" id="A2.F15.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F15.3.1.1" style="font-size:90%;">Figure 15</span>: </span><span class="ltx_text" id="A2.F15.4.2" style="font-size:90%;">Examples of triplets from the NIGHTS, BAPPS, THINGS, and ImageNet datasets, with the bordered images labeled as more similar to the reference (middle image in each triplet). </span></figcaption>
</figure>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">See Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A2.F15" title="Figure 15 ‣ B.2 Dataset examples ‣ Appendix B Qualitative examples ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">15</span></a> for examples of the triplet datasets we train on in this paper. In each triplet, the reference image (middle) and outlined image are labeled as the similar pair. Perceptually-aligned models in this paper are tuned on NIGHTS triplets (top row), whose image variations encompass a variety of mid-level perceptual attributes including object count, identity, layout, subject pose, and color. In contrast, the datasets studied in our ablation encompass differing image attributes: BAPPS focuses on image patches with low-level distortions such as CNN artifacts and color jitter, THINGS encodes similarity in concept space, and our constructed ImageNet triplets outline class boundaries.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Implementation Details</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Training human-aligned models</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.5">The human-aligned models studied in this paper were fine-tuned with LoRA parameters: <math alttext="r=16" class="ltx_Math" display="inline" id="A3.SS1.p1.1.m1.1"><semantics id="A3.SS1.p1.1.m1.1a"><mrow id="A3.SS1.p1.1.m1.1.1" xref="A3.SS1.p1.1.m1.1.1.cmml"><mi id="A3.SS1.p1.1.m1.1.1.2" xref="A3.SS1.p1.1.m1.1.1.2.cmml">r</mi><mo id="A3.SS1.p1.1.m1.1.1.1" xref="A3.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="A3.SS1.p1.1.m1.1.1.3" xref="A3.SS1.p1.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.1.m1.1b"><apply id="A3.SS1.p1.1.m1.1.1.cmml" xref="A3.SS1.p1.1.m1.1.1"><eq id="A3.SS1.p1.1.m1.1.1.1.cmml" xref="A3.SS1.p1.1.m1.1.1.1"></eq><ci id="A3.SS1.p1.1.m1.1.1.2.cmml" xref="A3.SS1.p1.1.m1.1.1.2">𝑟</ci><cn id="A3.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="A3.SS1.p1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.1.m1.1c">r=16</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.1.m1.1d">italic_r = 16</annotation></semantics></math>, <math alttext="\alpha=0.5" class="ltx_Math" display="inline" id="A3.SS1.p1.2.m2.1"><semantics id="A3.SS1.p1.2.m2.1a"><mrow id="A3.SS1.p1.2.m2.1.1" xref="A3.SS1.p1.2.m2.1.1.cmml"><mi id="A3.SS1.p1.2.m2.1.1.2" xref="A3.SS1.p1.2.m2.1.1.2.cmml">α</mi><mo id="A3.SS1.p1.2.m2.1.1.1" xref="A3.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="A3.SS1.p1.2.m2.1.1.3" xref="A3.SS1.p1.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.2.m2.1b"><apply id="A3.SS1.p1.2.m2.1.1.cmml" xref="A3.SS1.p1.2.m2.1.1"><eq id="A3.SS1.p1.2.m2.1.1.1.cmml" xref="A3.SS1.p1.2.m2.1.1.1"></eq><ci id="A3.SS1.p1.2.m2.1.1.2.cmml" xref="A3.SS1.p1.2.m2.1.1.2">𝛼</ci><cn id="A3.SS1.p1.2.m2.1.1.3.cmml" type="float" xref="A3.SS1.p1.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.2.m2.1c">\alpha=0.5</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.2.m2.1d">italic_α = 0.5</annotation></semantics></math>, <math alttext="p=0.0" class="ltx_Math" display="inline" id="A3.SS1.p1.3.m3.1"><semantics id="A3.SS1.p1.3.m3.1a"><mrow id="A3.SS1.p1.3.m3.1.1" xref="A3.SS1.p1.3.m3.1.1.cmml"><mi id="A3.SS1.p1.3.m3.1.1.2" xref="A3.SS1.p1.3.m3.1.1.2.cmml">p</mi><mo id="A3.SS1.p1.3.m3.1.1.1" xref="A3.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="A3.SS1.p1.3.m3.1.1.3" xref="A3.SS1.p1.3.m3.1.1.3.cmml">0.0</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.3.m3.1b"><apply id="A3.SS1.p1.3.m3.1.1.cmml" xref="A3.SS1.p1.3.m3.1.1"><eq id="A3.SS1.p1.3.m3.1.1.1.cmml" xref="A3.SS1.p1.3.m3.1.1.1"></eq><ci id="A3.SS1.p1.3.m3.1.1.2.cmml" xref="A3.SS1.p1.3.m3.1.1.2">𝑝</ci><cn id="A3.SS1.p1.3.m3.1.1.3.cmml" type="float" xref="A3.SS1.p1.3.m3.1.1.3">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.3.m3.1c">p=0.0</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.3.m3.1d">italic_p = 0.0</annotation></semantics></math>. All contrastive training (for image- and patch-level objectives) are trained with an Adam optimizer (<math alttext="lr=0.0003" class="ltx_Math" display="inline" id="A3.SS1.p1.4.m4.1"><semantics id="A3.SS1.p1.4.m4.1a"><mrow id="A3.SS1.p1.4.m4.1.1" xref="A3.SS1.p1.4.m4.1.1.cmml"><mrow id="A3.SS1.p1.4.m4.1.1.2" xref="A3.SS1.p1.4.m4.1.1.2.cmml"><mi id="A3.SS1.p1.4.m4.1.1.2.2" xref="A3.SS1.p1.4.m4.1.1.2.2.cmml">l</mi><mo id="A3.SS1.p1.4.m4.1.1.2.1" xref="A3.SS1.p1.4.m4.1.1.2.1.cmml">⁢</mo><mi id="A3.SS1.p1.4.m4.1.1.2.3" xref="A3.SS1.p1.4.m4.1.1.2.3.cmml">r</mi></mrow><mo id="A3.SS1.p1.4.m4.1.1.1" xref="A3.SS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="A3.SS1.p1.4.m4.1.1.3" xref="A3.SS1.p1.4.m4.1.1.3.cmml">0.0003</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.4.m4.1b"><apply id="A3.SS1.p1.4.m4.1.1.cmml" xref="A3.SS1.p1.4.m4.1.1"><eq id="A3.SS1.p1.4.m4.1.1.1.cmml" xref="A3.SS1.p1.4.m4.1.1.1"></eq><apply id="A3.SS1.p1.4.m4.1.1.2.cmml" xref="A3.SS1.p1.4.m4.1.1.2"><times id="A3.SS1.p1.4.m4.1.1.2.1.cmml" xref="A3.SS1.p1.4.m4.1.1.2.1"></times><ci id="A3.SS1.p1.4.m4.1.1.2.2.cmml" xref="A3.SS1.p1.4.m4.1.1.2.2">𝑙</ci><ci id="A3.SS1.p1.4.m4.1.1.2.3.cmml" xref="A3.SS1.p1.4.m4.1.1.2.3">𝑟</ci></apply><cn id="A3.SS1.p1.4.m4.1.1.3.cmml" type="float" xref="A3.SS1.p1.4.m4.1.1.3">0.0003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.4.m4.1c">lr=0.0003</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.4.m4.1d">italic_l italic_r = 0.0003</annotation></semantics></math>), batch size of 16, and a hinge loss margin of <math alttext="m=0.05" class="ltx_Math" display="inline" id="A3.SS1.p1.5.m5.1"><semantics id="A3.SS1.p1.5.m5.1a"><mrow id="A3.SS1.p1.5.m5.1.1" xref="A3.SS1.p1.5.m5.1.1.cmml"><mi id="A3.SS1.p1.5.m5.1.1.2" xref="A3.SS1.p1.5.m5.1.1.2.cmml">m</mi><mo id="A3.SS1.p1.5.m5.1.1.1" xref="A3.SS1.p1.5.m5.1.1.1.cmml">=</mo><mn id="A3.SS1.p1.5.m5.1.1.3" xref="A3.SS1.p1.5.m5.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p1.5.m5.1b"><apply id="A3.SS1.p1.5.m5.1.1.cmml" xref="A3.SS1.p1.5.m5.1.1"><eq id="A3.SS1.p1.5.m5.1.1.1.cmml" xref="A3.SS1.p1.5.m5.1.1.1"></eq><ci id="A3.SS1.p1.5.m5.1.1.2.cmml" xref="A3.SS1.p1.5.m5.1.1.2">𝑚</ci><cn id="A3.SS1.p1.5.m5.1.1.3.cmml" type="float" xref="A3.SS1.p1.5.m5.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p1.5.m5.1c">m=0.05</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p1.5.m5.1d">italic_m = 0.05</annotation></semantics></math> as detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib18" title="">18</a>]</cite>. We train all models for 8 epochs and evaluate on the checkpoint with the lowest validation loss. Train/val/test splits on NIGHTS, BAPPS, and THINGS were used as-provided in the dataset.</p>
</div>
<section class="ltx_subsubsection" id="A3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.1.1 </span>Dense prediction heads</h4>
<div class="ltx_para" id="A3.SS1.SSS1.p1">
<p class="ltx_p" id="A3.SS1.SSS1.p1.2">Each semantic segmentation head is a single linear layer mapping frozen DINO/DINOv2 patch tokens to a segmentation map. We train with the Adam optimizer (<math alttext="\alpha=0.0003" class="ltx_Math" display="inline" id="A3.SS1.SSS1.p1.1.m1.1"><semantics id="A3.SS1.SSS1.p1.1.m1.1a"><mrow id="A3.SS1.SSS1.p1.1.m1.1.1" xref="A3.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="A3.SS1.SSS1.p1.1.m1.1.1.2" xref="A3.SS1.SSS1.p1.1.m1.1.1.2.cmml">α</mi><mo id="A3.SS1.SSS1.p1.1.m1.1.1.1" xref="A3.SS1.SSS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="A3.SS1.SSS1.p1.1.m1.1.1.3" xref="A3.SS1.SSS1.p1.1.m1.1.1.3.cmml">0.0003</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.SSS1.p1.1.m1.1b"><apply id="A3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="A3.SS1.SSS1.p1.1.m1.1.1"><eq id="A3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="A3.SS1.SSS1.p1.1.m1.1.1.1"></eq><ci id="A3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="A3.SS1.SSS1.p1.1.m1.1.1.2">𝛼</ci><cn id="A3.SS1.SSS1.p1.1.m1.1.1.3.cmml" type="float" xref="A3.SS1.SSS1.p1.1.m1.1.1.3">0.0003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.SSS1.p1.1.m1.1c">\alpha=0.0003</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.SSS1.p1.1.m1.1d">italic_α = 0.0003</annotation></semantics></math>) for 10 epochs with a batch size of 16 and 4 workers. The semantic segmentation head is optimized with a Jaccard Index loss (equivalently, mean Intersection-over-Union) defined as <math alttext="J(A,B)=\frac{|A\cap B|}{|A\cup B|}" class="ltx_Math" display="inline" id="A3.SS1.SSS1.p1.2.m2.4"><semantics id="A3.SS1.SSS1.p1.2.m2.4a"><mrow id="A3.SS1.SSS1.p1.2.m2.4.5" xref="A3.SS1.SSS1.p1.2.m2.4.5.cmml"><mrow id="A3.SS1.SSS1.p1.2.m2.4.5.2" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.cmml"><mi id="A3.SS1.SSS1.p1.2.m2.4.5.2.2" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.2.cmml">J</mi><mo id="A3.SS1.SSS1.p1.2.m2.4.5.2.1" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.1.cmml">⁢</mo><mrow id="A3.SS1.SSS1.p1.2.m2.4.5.2.3.2" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.3.1.cmml"><mo id="A3.SS1.SSS1.p1.2.m2.4.5.2.3.2.1" stretchy="false" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.3.1.cmml">(</mo><mi id="A3.SS1.SSS1.p1.2.m2.3.3" xref="A3.SS1.SSS1.p1.2.m2.3.3.cmml">A</mi><mo id="A3.SS1.SSS1.p1.2.m2.4.5.2.3.2.2" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.3.1.cmml">,</mo><mi id="A3.SS1.SSS1.p1.2.m2.4.4" xref="A3.SS1.SSS1.p1.2.m2.4.4.cmml">B</mi><mo id="A3.SS1.SSS1.p1.2.m2.4.5.2.3.2.3" stretchy="false" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="A3.SS1.SSS1.p1.2.m2.4.5.1" xref="A3.SS1.SSS1.p1.2.m2.4.5.1.cmml">=</mo><mfrac id="A3.SS1.SSS1.p1.2.m2.2.2" xref="A3.SS1.SSS1.p1.2.m2.2.2.cmml"><mrow id="A3.SS1.SSS1.p1.2.m2.1.1.1.1" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.2.cmml"><mo id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.2" stretchy="false" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.2.1.cmml">|</mo><mrow id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.cmml"><mi id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.2" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.2.cmml">A</mi><mo id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.1" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.1.cmml">∩</mo><mi id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.3" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.3.cmml">B</mi></mrow><mo id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.3" stretchy="false" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.2.1.cmml">|</mo></mrow><mrow id="A3.SS1.SSS1.p1.2.m2.2.2.2.1" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.2.cmml"><mo id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.2" stretchy="false" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.2.1.cmml">|</mo><mrow id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.cmml"><mi id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.2" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.2.cmml">A</mi><mo id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.1" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.1.cmml">∪</mo><mi id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.3" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.3.cmml">B</mi></mrow><mo id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.3" stretchy="false" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.SSS1.p1.2.m2.4b"><apply id="A3.SS1.SSS1.p1.2.m2.4.5.cmml" xref="A3.SS1.SSS1.p1.2.m2.4.5"><eq id="A3.SS1.SSS1.p1.2.m2.4.5.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.4.5.1"></eq><apply id="A3.SS1.SSS1.p1.2.m2.4.5.2.cmml" xref="A3.SS1.SSS1.p1.2.m2.4.5.2"><times id="A3.SS1.SSS1.p1.2.m2.4.5.2.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.1"></times><ci id="A3.SS1.SSS1.p1.2.m2.4.5.2.2.cmml" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.2">𝐽</ci><interval closure="open" id="A3.SS1.SSS1.p1.2.m2.4.5.2.3.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.4.5.2.3.2"><ci id="A3.SS1.SSS1.p1.2.m2.3.3.cmml" xref="A3.SS1.SSS1.p1.2.m2.3.3">𝐴</ci><ci id="A3.SS1.SSS1.p1.2.m2.4.4.cmml" xref="A3.SS1.SSS1.p1.2.m2.4.4">𝐵</ci></interval></apply><apply id="A3.SS1.SSS1.p1.2.m2.2.2.cmml" xref="A3.SS1.SSS1.p1.2.m2.2.2"><divide id="A3.SS1.SSS1.p1.2.m2.2.2.3.cmml" xref="A3.SS1.SSS1.p1.2.m2.2.2"></divide><apply id="A3.SS1.SSS1.p1.2.m2.1.1.1.2.cmml" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1"><abs id="A3.SS1.SSS1.p1.2.m2.1.1.1.2.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.2"></abs><apply id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1"><intersect id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.1"></intersect><ci id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.2.cmml" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.2">𝐴</ci><ci id="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.3.cmml" xref="A3.SS1.SSS1.p1.2.m2.1.1.1.1.1.3">𝐵</ci></apply></apply><apply id="A3.SS1.SSS1.p1.2.m2.2.2.2.2.cmml" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1"><abs id="A3.SS1.SSS1.p1.2.m2.2.2.2.2.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.2"></abs><apply id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1"><union id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.1.cmml" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.1"></union><ci id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.2.cmml" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.2">𝐴</ci><ci id="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.3.cmml" xref="A3.SS1.SSS1.p1.2.m2.2.2.2.1.1.3">𝐵</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.SSS1.p1.2.m2.4c">J(A,B)=\frac{|A\cap B|}{|A\cup B|}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.SSS1.p1.2.m2.4d">italic_J ( italic_A , italic_B ) = divide start_ARG | italic_A ∩ italic_B | end_ARG start_ARG | italic_A ∪ italic_B | end_ARG</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A3.SS1.SSS1.p2">
<p class="ltx_p" id="A3.SS1.SSS1.p2.3">Dense prediction heads are defined as a single linear layer mapping frozen DINO/DINOv2 patch tokens to a spatial map, whose values are then categorized into 256 bins. We train with the Adam optimizer (<math alttext="\alpha=0.0003" class="ltx_Math" display="inline" id="A3.SS1.SSS1.p2.1.m1.1"><semantics id="A3.SS1.SSS1.p2.1.m1.1a"><mrow id="A3.SS1.SSS1.p2.1.m1.1.1" xref="A3.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="A3.SS1.SSS1.p2.1.m1.1.1.2" xref="A3.SS1.SSS1.p2.1.m1.1.1.2.cmml">α</mi><mo id="A3.SS1.SSS1.p2.1.m1.1.1.1" xref="A3.SS1.SSS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="A3.SS1.SSS1.p2.1.m1.1.1.3" xref="A3.SS1.SSS1.p2.1.m1.1.1.3.cmml">0.0003</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.SSS1.p2.1.m1.1b"><apply id="A3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="A3.SS1.SSS1.p2.1.m1.1.1"><eq id="A3.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="A3.SS1.SSS1.p2.1.m1.1.1.1"></eq><ci id="A3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="A3.SS1.SSS1.p2.1.m1.1.1.2">𝛼</ci><cn id="A3.SS1.SSS1.p2.1.m1.1.1.3.cmml" type="float" xref="A3.SS1.SSS1.p2.1.m1.1.1.3">0.0003</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.SSS1.p2.1.m1.1c">\alpha=0.0003</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.SSS1.p2.1.m1.1d">italic_α = 0.0003</annotation></semantics></math>) for 10 epochs with a batch size of 128 and 12 workers. The scale-invariant log loss for prediction <math alttext="A" class="ltx_Math" display="inline" id="A3.SS1.SSS1.p2.2.m2.1"><semantics id="A3.SS1.SSS1.p2.2.m2.1a"><mi id="A3.SS1.SSS1.p2.2.m2.1.1" xref="A3.SS1.SSS1.p2.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A3.SS1.SSS1.p2.2.m2.1b"><ci id="A3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="A3.SS1.SSS1.p2.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.SSS1.p2.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.SSS1.p2.2.m2.1d">italic_A</annotation></semantics></math> and target <math alttext="B" class="ltx_Math" display="inline" id="A3.SS1.SSS1.p2.3.m3.1"><semantics id="A3.SS1.SSS1.p2.3.m3.1a"><mi id="A3.SS1.SSS1.p2.3.m3.1.1" xref="A3.SS1.SSS1.p2.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="A3.SS1.SSS1.p2.3.m3.1b"><ci id="A3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="A3.SS1.SSS1.p2.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.SSS1.p2.3.m3.1c">B</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.SSS1.p2.3.m3.1d">italic_B</annotation></semantics></math> is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="A3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(A,B)=\frac{1}{N}\sum\limits_{i}d_{i}^{2}+0.15\cdot\frac{1}{N^{2}}(%
\sum\limits_{i}d_{i})^{2}" class="ltx_Math" display="block" id="A3.Ex2.m1.3"><semantics id="A3.Ex2.m1.3a"><mrow id="A3.Ex2.m1.3.3" xref="A3.Ex2.m1.3.3.cmml"><mrow id="A3.Ex2.m1.3.3.3" xref="A3.Ex2.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.Ex2.m1.3.3.3.2" xref="A3.Ex2.m1.3.3.3.2.cmml">ℒ</mi><mo id="A3.Ex2.m1.3.3.3.1" xref="A3.Ex2.m1.3.3.3.1.cmml">⁢</mo><mrow id="A3.Ex2.m1.3.3.3.3.2" xref="A3.Ex2.m1.3.3.3.3.1.cmml"><mo id="A3.Ex2.m1.3.3.3.3.2.1" stretchy="false" xref="A3.Ex2.m1.3.3.3.3.1.cmml">(</mo><mi id="A3.Ex2.m1.1.1" xref="A3.Ex2.m1.1.1.cmml">A</mi><mo id="A3.Ex2.m1.3.3.3.3.2.2" xref="A3.Ex2.m1.3.3.3.3.1.cmml">,</mo><mi id="A3.Ex2.m1.2.2" xref="A3.Ex2.m1.2.2.cmml">B</mi><mo id="A3.Ex2.m1.3.3.3.3.2.3" stretchy="false" xref="A3.Ex2.m1.3.3.3.3.1.cmml">)</mo></mrow></mrow><mo id="A3.Ex2.m1.3.3.2" xref="A3.Ex2.m1.3.3.2.cmml">=</mo><mrow id="A3.Ex2.m1.3.3.1" xref="A3.Ex2.m1.3.3.1.cmml"><mrow id="A3.Ex2.m1.3.3.1.3" xref="A3.Ex2.m1.3.3.1.3.cmml"><mfrac id="A3.Ex2.m1.3.3.1.3.2" xref="A3.Ex2.m1.3.3.1.3.2.cmml"><mn id="A3.Ex2.m1.3.3.1.3.2.2" xref="A3.Ex2.m1.3.3.1.3.2.2.cmml">1</mn><mi id="A3.Ex2.m1.3.3.1.3.2.3" xref="A3.Ex2.m1.3.3.1.3.2.3.cmml">N</mi></mfrac><mo id="A3.Ex2.m1.3.3.1.3.1" xref="A3.Ex2.m1.3.3.1.3.1.cmml">⁢</mo><mrow id="A3.Ex2.m1.3.3.1.3.3" xref="A3.Ex2.m1.3.3.1.3.3.cmml"><munder id="A3.Ex2.m1.3.3.1.3.3.1" xref="A3.Ex2.m1.3.3.1.3.3.1.cmml"><mo id="A3.Ex2.m1.3.3.1.3.3.1.2" movablelimits="false" xref="A3.Ex2.m1.3.3.1.3.3.1.2.cmml">∑</mo><mi id="A3.Ex2.m1.3.3.1.3.3.1.3" xref="A3.Ex2.m1.3.3.1.3.3.1.3.cmml">i</mi></munder><msubsup id="A3.Ex2.m1.3.3.1.3.3.2" xref="A3.Ex2.m1.3.3.1.3.3.2.cmml"><mi id="A3.Ex2.m1.3.3.1.3.3.2.2.2" xref="A3.Ex2.m1.3.3.1.3.3.2.2.2.cmml">d</mi><mi id="A3.Ex2.m1.3.3.1.3.3.2.2.3" xref="A3.Ex2.m1.3.3.1.3.3.2.2.3.cmml">i</mi><mn id="A3.Ex2.m1.3.3.1.3.3.2.3" xref="A3.Ex2.m1.3.3.1.3.3.2.3.cmml">2</mn></msubsup></mrow></mrow><mo id="A3.Ex2.m1.3.3.1.2" xref="A3.Ex2.m1.3.3.1.2.cmml">+</mo><mrow id="A3.Ex2.m1.3.3.1.1" xref="A3.Ex2.m1.3.3.1.1.cmml"><mrow id="A3.Ex2.m1.3.3.1.1.3" xref="A3.Ex2.m1.3.3.1.1.3.cmml"><mn id="A3.Ex2.m1.3.3.1.1.3.2" xref="A3.Ex2.m1.3.3.1.1.3.2.cmml">0.15</mn><mo id="A3.Ex2.m1.3.3.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="A3.Ex2.m1.3.3.1.1.3.1.cmml">⋅</mo><mfrac id="A3.Ex2.m1.3.3.1.1.3.3" xref="A3.Ex2.m1.3.3.1.1.3.3.cmml"><mn id="A3.Ex2.m1.3.3.1.1.3.3.2" xref="A3.Ex2.m1.3.3.1.1.3.3.2.cmml">1</mn><msup id="A3.Ex2.m1.3.3.1.1.3.3.3" xref="A3.Ex2.m1.3.3.1.1.3.3.3.cmml"><mi id="A3.Ex2.m1.3.3.1.1.3.3.3.2" xref="A3.Ex2.m1.3.3.1.1.3.3.3.2.cmml">N</mi><mn id="A3.Ex2.m1.3.3.1.1.3.3.3.3" xref="A3.Ex2.m1.3.3.1.1.3.3.3.3.cmml">2</mn></msup></mfrac></mrow><mo id="A3.Ex2.m1.3.3.1.1.2" xref="A3.Ex2.m1.3.3.1.1.2.cmml">⁢</mo><msup id="A3.Ex2.m1.3.3.1.1.1" xref="A3.Ex2.m1.3.3.1.1.1.cmml"><mrow id="A3.Ex2.m1.3.3.1.1.1.1.1" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.cmml"><mo id="A3.Ex2.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="A3.Ex2.m1.3.3.1.1.1.1.1.1" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.cmml"><munder id="A3.Ex2.m1.3.3.1.1.1.1.1.1.1" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.2" lspace="0em" movablelimits="false" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml">∑</mo><mi id="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.3" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.3.cmml">i</mi></munder><msub id="A3.Ex2.m1.3.3.1.1.1.1.1.1.2" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.2" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.2.cmml">d</mi><mi id="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.3" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.3.cmml">i</mi></msub></mrow><mo id="A3.Ex2.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="A3.Ex2.m1.3.3.1.1.1.3" xref="A3.Ex2.m1.3.3.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.Ex2.m1.3b"><apply id="A3.Ex2.m1.3.3.cmml" xref="A3.Ex2.m1.3.3"><eq id="A3.Ex2.m1.3.3.2.cmml" xref="A3.Ex2.m1.3.3.2"></eq><apply id="A3.Ex2.m1.3.3.3.cmml" xref="A3.Ex2.m1.3.3.3"><times id="A3.Ex2.m1.3.3.3.1.cmml" xref="A3.Ex2.m1.3.3.3.1"></times><ci id="A3.Ex2.m1.3.3.3.2.cmml" xref="A3.Ex2.m1.3.3.3.2">ℒ</ci><interval closure="open" id="A3.Ex2.m1.3.3.3.3.1.cmml" xref="A3.Ex2.m1.3.3.3.3.2"><ci id="A3.Ex2.m1.1.1.cmml" xref="A3.Ex2.m1.1.1">𝐴</ci><ci id="A3.Ex2.m1.2.2.cmml" xref="A3.Ex2.m1.2.2">𝐵</ci></interval></apply><apply id="A3.Ex2.m1.3.3.1.cmml" xref="A3.Ex2.m1.3.3.1"><plus id="A3.Ex2.m1.3.3.1.2.cmml" xref="A3.Ex2.m1.3.3.1.2"></plus><apply id="A3.Ex2.m1.3.3.1.3.cmml" xref="A3.Ex2.m1.3.3.1.3"><times id="A3.Ex2.m1.3.3.1.3.1.cmml" xref="A3.Ex2.m1.3.3.1.3.1"></times><apply id="A3.Ex2.m1.3.3.1.3.2.cmml" xref="A3.Ex2.m1.3.3.1.3.2"><divide id="A3.Ex2.m1.3.3.1.3.2.1.cmml" xref="A3.Ex2.m1.3.3.1.3.2"></divide><cn id="A3.Ex2.m1.3.3.1.3.2.2.cmml" type="integer" xref="A3.Ex2.m1.3.3.1.3.2.2">1</cn><ci id="A3.Ex2.m1.3.3.1.3.2.3.cmml" xref="A3.Ex2.m1.3.3.1.3.2.3">𝑁</ci></apply><apply id="A3.Ex2.m1.3.3.1.3.3.cmml" xref="A3.Ex2.m1.3.3.1.3.3"><apply id="A3.Ex2.m1.3.3.1.3.3.1.cmml" xref="A3.Ex2.m1.3.3.1.3.3.1"><csymbol cd="ambiguous" id="A3.Ex2.m1.3.3.1.3.3.1.1.cmml" xref="A3.Ex2.m1.3.3.1.3.3.1">subscript</csymbol><sum id="A3.Ex2.m1.3.3.1.3.3.1.2.cmml" xref="A3.Ex2.m1.3.3.1.3.3.1.2"></sum><ci id="A3.Ex2.m1.3.3.1.3.3.1.3.cmml" xref="A3.Ex2.m1.3.3.1.3.3.1.3">𝑖</ci></apply><apply id="A3.Ex2.m1.3.3.1.3.3.2.cmml" xref="A3.Ex2.m1.3.3.1.3.3.2"><csymbol cd="ambiguous" id="A3.Ex2.m1.3.3.1.3.3.2.1.cmml" xref="A3.Ex2.m1.3.3.1.3.3.2">superscript</csymbol><apply id="A3.Ex2.m1.3.3.1.3.3.2.2.cmml" xref="A3.Ex2.m1.3.3.1.3.3.2"><csymbol cd="ambiguous" id="A3.Ex2.m1.3.3.1.3.3.2.2.1.cmml" xref="A3.Ex2.m1.3.3.1.3.3.2">subscript</csymbol><ci id="A3.Ex2.m1.3.3.1.3.3.2.2.2.cmml" xref="A3.Ex2.m1.3.3.1.3.3.2.2.2">𝑑</ci><ci id="A3.Ex2.m1.3.3.1.3.3.2.2.3.cmml" xref="A3.Ex2.m1.3.3.1.3.3.2.2.3">𝑖</ci></apply><cn id="A3.Ex2.m1.3.3.1.3.3.2.3.cmml" type="integer" xref="A3.Ex2.m1.3.3.1.3.3.2.3">2</cn></apply></apply></apply><apply id="A3.Ex2.m1.3.3.1.1.cmml" xref="A3.Ex2.m1.3.3.1.1"><times id="A3.Ex2.m1.3.3.1.1.2.cmml" xref="A3.Ex2.m1.3.3.1.1.2"></times><apply id="A3.Ex2.m1.3.3.1.1.3.cmml" xref="A3.Ex2.m1.3.3.1.1.3"><ci id="A3.Ex2.m1.3.3.1.1.3.1.cmml" xref="A3.Ex2.m1.3.3.1.1.3.1">⋅</ci><cn id="A3.Ex2.m1.3.3.1.1.3.2.cmml" type="float" xref="A3.Ex2.m1.3.3.1.1.3.2">0.15</cn><apply id="A3.Ex2.m1.3.3.1.1.3.3.cmml" xref="A3.Ex2.m1.3.3.1.1.3.3"><divide id="A3.Ex2.m1.3.3.1.1.3.3.1.cmml" xref="A3.Ex2.m1.3.3.1.1.3.3"></divide><cn id="A3.Ex2.m1.3.3.1.1.3.3.2.cmml" type="integer" xref="A3.Ex2.m1.3.3.1.1.3.3.2">1</cn><apply id="A3.Ex2.m1.3.3.1.1.3.3.3.cmml" xref="A3.Ex2.m1.3.3.1.1.3.3.3"><csymbol cd="ambiguous" id="A3.Ex2.m1.3.3.1.1.3.3.3.1.cmml" xref="A3.Ex2.m1.3.3.1.1.3.3.3">superscript</csymbol><ci id="A3.Ex2.m1.3.3.1.1.3.3.3.2.cmml" xref="A3.Ex2.m1.3.3.1.1.3.3.3.2">𝑁</ci><cn id="A3.Ex2.m1.3.3.1.1.3.3.3.3.cmml" type="integer" xref="A3.Ex2.m1.3.3.1.1.3.3.3.3">2</cn></apply></apply></apply><apply id="A3.Ex2.m1.3.3.1.1.1.cmml" xref="A3.Ex2.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="A3.Ex2.m1.3.3.1.1.1.2.cmml" xref="A3.Ex2.m1.3.3.1.1.1">superscript</csymbol><apply id="A3.Ex2.m1.3.3.1.1.1.1.1.1.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1"><apply id="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><sum id="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.2"></sum><ci id="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><ci id="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.2">𝑑</ci><ci id="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="A3.Ex2.m1.3.3.1.1.1.1.1.1.2.3">𝑖</ci></apply></apply><cn id="A3.Ex2.m1.3.3.1.1.1.3.cmml" type="integer" xref="A3.Ex2.m1.3.3.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.Ex2.m1.3c">\mathcal{L}(A,B)=\frac{1}{N}\sum\limits_{i}d_{i}^{2}+0.15\cdot\frac{1}{N^{2}}(%
\sum\limits_{i}d_{i})^{2}</annotation><annotation encoding="application/x-llamapun" id="A3.Ex2.m1.3d">caligraphic_L ( italic_A , italic_B ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 0.15 ⋅ divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A3.SS1.SSS1.p2.6">where <math alttext="d_{i}=\log(A+\epsilon)-\log(B+\epsilon)" class="ltx_Math" display="inline" id="A3.SS1.SSS1.p2.4.m1.4"><semantics id="A3.SS1.SSS1.p2.4.m1.4a"><mrow id="A3.SS1.SSS1.p2.4.m1.4.4" xref="A3.SS1.SSS1.p2.4.m1.4.4.cmml"><msub id="A3.SS1.SSS1.p2.4.m1.4.4.4" xref="A3.SS1.SSS1.p2.4.m1.4.4.4.cmml"><mi id="A3.SS1.SSS1.p2.4.m1.4.4.4.2" xref="A3.SS1.SSS1.p2.4.m1.4.4.4.2.cmml">d</mi><mi id="A3.SS1.SSS1.p2.4.m1.4.4.4.3" xref="A3.SS1.SSS1.p2.4.m1.4.4.4.3.cmml">i</mi></msub><mo id="A3.SS1.SSS1.p2.4.m1.4.4.3" xref="A3.SS1.SSS1.p2.4.m1.4.4.3.cmml">=</mo><mrow id="A3.SS1.SSS1.p2.4.m1.4.4.2" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.cmml"><mrow id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.2.cmml"><mi id="A3.SS1.SSS1.p2.4.m1.1.1" xref="A3.SS1.SSS1.p2.4.m1.1.1.cmml">log</mi><mo id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1a" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.2.cmml">⁡</mo><mrow id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.2.cmml"><mo id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.2" stretchy="false" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.2.cmml">(</mo><mrow id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.cmml"><mi id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.2" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.2.cmml">A</mi><mo id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.1" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.1.cmml">+</mo><mi id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.3" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.3.cmml">ϵ</mi></mrow><mo id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.3" stretchy="false" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="A3.SS1.SSS1.p2.4.m1.4.4.2.3" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.3.cmml">−</mo><mrow id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.2.cmml"><mi id="A3.SS1.SSS1.p2.4.m1.2.2" xref="A3.SS1.SSS1.p2.4.m1.2.2.cmml">log</mi><mo id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1a" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.2.cmml">⁡</mo><mrow id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.2.cmml"><mo id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.2" stretchy="false" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.2.cmml">(</mo><mrow id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.cmml"><mi id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.2" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.2.cmml">B</mi><mo id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.1" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.1.cmml">+</mo><mi id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.3" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.3.cmml">ϵ</mi></mrow><mo id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.3" stretchy="false" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.SSS1.p2.4.m1.4b"><apply id="A3.SS1.SSS1.p2.4.m1.4.4.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4"><eq id="A3.SS1.SSS1.p2.4.m1.4.4.3.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.3"></eq><apply id="A3.SS1.SSS1.p2.4.m1.4.4.4.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.4"><csymbol cd="ambiguous" id="A3.SS1.SSS1.p2.4.m1.4.4.4.1.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.4">subscript</csymbol><ci id="A3.SS1.SSS1.p2.4.m1.4.4.4.2.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.4.2">𝑑</ci><ci id="A3.SS1.SSS1.p2.4.m1.4.4.4.3.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.4.3">𝑖</ci></apply><apply id="A3.SS1.SSS1.p2.4.m1.4.4.2.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.2"><minus id="A3.SS1.SSS1.p2.4.m1.4.4.2.3.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.3"></minus><apply id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.2.cmml" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1"><log id="A3.SS1.SSS1.p2.4.m1.1.1.cmml" xref="A3.SS1.SSS1.p2.4.m1.1.1"></log><apply id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.cmml" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1"><plus id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.1.cmml" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.1"></plus><ci id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.2.cmml" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.2">𝐴</ci><ci id="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.3.cmml" xref="A3.SS1.SSS1.p2.4.m1.3.3.1.1.1.1.1.3">italic-ϵ</ci></apply></apply><apply id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.2.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1"><log id="A3.SS1.SSS1.p2.4.m1.2.2.cmml" xref="A3.SS1.SSS1.p2.4.m1.2.2"></log><apply id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1"><plus id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.1.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.1"></plus><ci id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.2.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.2">𝐵</ci><ci id="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.3.cmml" xref="A3.SS1.SSS1.p2.4.m1.4.4.2.2.1.1.1.3">italic-ϵ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.SSS1.p2.4.m1.4c">d_{i}=\log(A+\epsilon)-\log(B+\epsilon)</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.SSS1.p2.4.m1.4d">italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_log ( italic_A + italic_ϵ ) - roman_log ( italic_B + italic_ϵ )</annotation></semantics></math>, <math alttext="\epsilon=0.001" class="ltx_Math" display="inline" id="A3.SS1.SSS1.p2.5.m2.1"><semantics id="A3.SS1.SSS1.p2.5.m2.1a"><mrow id="A3.SS1.SSS1.p2.5.m2.1.1" xref="A3.SS1.SSS1.p2.5.m2.1.1.cmml"><mi id="A3.SS1.SSS1.p2.5.m2.1.1.2" xref="A3.SS1.SSS1.p2.5.m2.1.1.2.cmml">ϵ</mi><mo id="A3.SS1.SSS1.p2.5.m2.1.1.1" xref="A3.SS1.SSS1.p2.5.m2.1.1.1.cmml">=</mo><mn id="A3.SS1.SSS1.p2.5.m2.1.1.3" xref="A3.SS1.SSS1.p2.5.m2.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.SSS1.p2.5.m2.1b"><apply id="A3.SS1.SSS1.p2.5.m2.1.1.cmml" xref="A3.SS1.SSS1.p2.5.m2.1.1"><eq id="A3.SS1.SSS1.p2.5.m2.1.1.1.cmml" xref="A3.SS1.SSS1.p2.5.m2.1.1.1"></eq><ci id="A3.SS1.SSS1.p2.5.m2.1.1.2.cmml" xref="A3.SS1.SSS1.p2.5.m2.1.1.2">italic-ϵ</ci><cn id="A3.SS1.SSS1.p2.5.m2.1.1.3.cmml" type="float" xref="A3.SS1.SSS1.p2.5.m2.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.SSS1.p2.5.m2.1c">\epsilon=0.001</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.SSS1.p2.5.m2.1d">italic_ϵ = 0.001</annotation></semantics></math> to avoid gradient issues, and <math alttext="N" class="ltx_Math" display="inline" id="A3.SS1.SSS1.p2.6.m3.1"><semantics id="A3.SS1.SSS1.p2.6.m3.1a"><mi id="A3.SS1.SSS1.p2.6.m3.1.1" xref="A3.SS1.SSS1.p2.6.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A3.SS1.SSS1.p2.6.m3.1b"><ci id="A3.SS1.SSS1.p2.6.m3.1.1.cmml" xref="A3.SS1.SSS1.p2.6.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.SSS1.p2.6.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.SSS1.p2.6.m3.1d">italic_N</annotation></semantics></math> is the number of valid pixels.</p>
</div>
<div class="ltx_para" id="A3.SS1.SSS1.p3">
<p class="ltx_p" id="A3.SS1.SSS1.p3.1">All training and evaluation for dense prediction tasks is done on a single NVIDIA Titan RTX GPU.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Retrieval-augmented generation</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">We follow the evaluation protocol in the codebase provided by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib2" title="">2</a>]</cite>, constructing a prompt containing three image/text examples, a query image, and a candidate class. We then average the model’s output logits, and repeat for each candidate class in a dataset to form a softmaxed array of log probabilities. OpenFlamingo’s classification prediction is determined as the class with the highest corresponding log probability, while classifications from IDEFICS2 are taken purely from a multiple-choice text response. Error bars were calculated as 95% confidence intervals over five trials with 1,000 randomly-sampled queries each.
All evaluation for RAG tasks is done on a single NVIDIA Titan RTX GPU.</p>
</div>
<section class="ltx_subsubsection" id="A3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.1 </span>Object counting</h4>
<div class="ltx_para" id="A3.SS2.SSS1.p1">
<p class="ltx_p" id="A3.SS2.SSS1.p1.2">Our k-Nearest Neighbor evaluations reported in the Counting section are run over <math alttext="k\in\{1,3,5,10\}" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p1.1.m1.4"><semantics id="A3.SS2.SSS1.p1.1.m1.4a"><mrow id="A3.SS2.SSS1.p1.1.m1.4.5" xref="A3.SS2.SSS1.p1.1.m1.4.5.cmml"><mi id="A3.SS2.SSS1.p1.1.m1.4.5.2" xref="A3.SS2.SSS1.p1.1.m1.4.5.2.cmml">k</mi><mo id="A3.SS2.SSS1.p1.1.m1.4.5.1" xref="A3.SS2.SSS1.p1.1.m1.4.5.1.cmml">∈</mo><mrow id="A3.SS2.SSS1.p1.1.m1.4.5.3.2" xref="A3.SS2.SSS1.p1.1.m1.4.5.3.1.cmml"><mo id="A3.SS2.SSS1.p1.1.m1.4.5.3.2.1" stretchy="false" xref="A3.SS2.SSS1.p1.1.m1.4.5.3.1.cmml">{</mo><mn id="A3.SS2.SSS1.p1.1.m1.1.1" xref="A3.SS2.SSS1.p1.1.m1.1.1.cmml">1</mn><mo id="A3.SS2.SSS1.p1.1.m1.4.5.3.2.2" xref="A3.SS2.SSS1.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="A3.SS2.SSS1.p1.1.m1.2.2" xref="A3.SS2.SSS1.p1.1.m1.2.2.cmml">3</mn><mo id="A3.SS2.SSS1.p1.1.m1.4.5.3.2.3" xref="A3.SS2.SSS1.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="A3.SS2.SSS1.p1.1.m1.3.3" xref="A3.SS2.SSS1.p1.1.m1.3.3.cmml">5</mn><mo id="A3.SS2.SSS1.p1.1.m1.4.5.3.2.4" xref="A3.SS2.SSS1.p1.1.m1.4.5.3.1.cmml">,</mo><mn id="A3.SS2.SSS1.p1.1.m1.4.4" xref="A3.SS2.SSS1.p1.1.m1.4.4.cmml">10</mn><mo id="A3.SS2.SSS1.p1.1.m1.4.5.3.2.5" stretchy="false" xref="A3.SS2.SSS1.p1.1.m1.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS2.SSS1.p1.1.m1.4b"><apply id="A3.SS2.SSS1.p1.1.m1.4.5.cmml" xref="A3.SS2.SSS1.p1.1.m1.4.5"><in id="A3.SS2.SSS1.p1.1.m1.4.5.1.cmml" xref="A3.SS2.SSS1.p1.1.m1.4.5.1"></in><ci id="A3.SS2.SSS1.p1.1.m1.4.5.2.cmml" xref="A3.SS2.SSS1.p1.1.m1.4.5.2">𝑘</ci><set id="A3.SS2.SSS1.p1.1.m1.4.5.3.1.cmml" xref="A3.SS2.SSS1.p1.1.m1.4.5.3.2"><cn id="A3.SS2.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="A3.SS2.SSS1.p1.1.m1.1.1">1</cn><cn id="A3.SS2.SSS1.p1.1.m1.2.2.cmml" type="integer" xref="A3.SS2.SSS1.p1.1.m1.2.2">3</cn><cn id="A3.SS2.SSS1.p1.1.m1.3.3.cmml" type="integer" xref="A3.SS2.SSS1.p1.1.m1.3.3">5</cn><cn id="A3.SS2.SSS1.p1.1.m1.4.4.cmml" type="integer" xref="A3.SS2.SSS1.p1.1.m1.4.4">10</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.SSS1.p1.1.m1.4c">k\in\{1,3,5,10\}</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.SSS1.p1.1.m1.4d">italic_k ∈ { 1 , 3 , 5 , 10 }</annotation></semantics></math> for the training set, and the argmax <math alttext="k" class="ltx_Math" display="inline" id="A3.SS2.SSS1.p1.2.m2.1"><semantics id="A3.SS2.SSS1.p1.2.m2.1a"><mi id="A3.SS2.SSS1.p1.2.m2.1.1" xref="A3.SS2.SSS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.SS2.SSS1.p1.2.m2.1b"><ci id="A3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="A3.SS2.SSS1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.SSS1.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.SSS1.p1.2.m2.1d">italic_k</annotation></semantics></math> with highest classification accuracy is used to report the final performance on the test set. We use the train and test sets as provided for the FSC147 and CARPK datasets, and construct a random 80:20 data split for the Clevr-Count dataset.
All evaluation for this section is done on a single NVIDIA Titan RTX GPU.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Instance retrieval</h3>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.2">We evaluate backbones on instance retrieval by extracting features (CLIP/OpenCLIP: embedding, DINO/DINOv2/SynCLR: CLS token) from the gallery and query sets. For every query image, we compute the cosine similarity with every image in the gallery. We then calculate top-<math alttext="k" class="ltx_Math" display="inline" id="A3.SS3.p1.1.m1.1"><semantics id="A3.SS3.p1.1.m1.1a"><mi id="A3.SS3.p1.1.m1.1.1" xref="A3.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.SS3.p1.1.m1.1b"><ci id="A3.SS3.p1.1.m1.1.1.cmml" xref="A3.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.SS3.p1.1.m1.1d">italic_k</annotation></semantics></math> accuracy as the number of queries for which a correct gallery pair image was in the top <math alttext="k" class="ltx_Math" display="inline" id="A3.SS3.p1.2.m2.1"><semantics id="A3.SS3.p1.2.m2.1a"><mi id="A3.SS3.p1.2.m2.1.1" xref="A3.SS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.SS3.p1.2.m2.1b"><ci id="A3.SS3.p1.2.m2.1.1.cmml" xref="A3.SS3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.SS3.p1.2.m2.1d">italic_k</annotation></semantics></math> retrievals.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.4 </span>Dataset ablations</h3>
<div class="ltx_para" id="A3.SS4.p1">
<p class="ltx_p" id="A3.SS4.p1.1">We train models for the dataset ablation section with identical hyperparameters as the human-aligned models (details in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#A3.SS1" title="C.1 Training human-aligned models ‣ Appendix C Implementation Details ‣ When Does Perceptual Alignment Benefit Vision Representations?"><span class="ltx_text ltx_ref_tag">C.1</span></a> and on the same amount of data as the NIGHTS training set (13,900 triplets). We randomly select these triplets without replacement from the BAPPS and THINGS training sets, and construct the ImageNet triplets by randomly sampling two images from one class and a third image from a different class.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.5 </span>VTAB classification</h3>
<div class="ltx_para" id="A3.SS5.p1">
<p class="ltx_p" id="A3.SS5.p1.2">To evaluate models on VTAB, we follow standard procedure and train a linear classifier on top of the frozen representations using multinomial logistic regression, with the sci-kit learn implementation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.10817v1#bib.bib42" title="">42</a>]</cite>. For each classifier we perform a hyperparameter search over the regularization strength using 10-fold cross validation over the training set, and report performance on the validation set. We search over the values <math alttext="c=\{1e0,1e1,1e2,1e3,1e4,1e5,1e6\}" class="ltx_Math" display="inline" id="A3.SS5.p1.1.m1.7"><semantics id="A3.SS5.p1.1.m1.7a"><mrow id="A3.SS5.p1.1.m1.7.7" xref="A3.SS5.p1.1.m1.7.7.cmml"><mi id="A3.SS5.p1.1.m1.7.7.9" xref="A3.SS5.p1.1.m1.7.7.9.cmml">c</mi><mo id="A3.SS5.p1.1.m1.7.7.8" xref="A3.SS5.p1.1.m1.7.7.8.cmml">=</mo><mrow id="A3.SS5.p1.1.m1.7.7.7.7" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml"><mo id="A3.SS5.p1.1.m1.7.7.7.7.8" stretchy="false" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml">{</mo><mrow id="A3.SS5.p1.1.m1.1.1.1.1.1" xref="A3.SS5.p1.1.m1.1.1.1.1.1.cmml"><mn id="A3.SS5.p1.1.m1.1.1.1.1.1.2" xref="A3.SS5.p1.1.m1.1.1.1.1.1.2.cmml">1</mn><mo id="A3.SS5.p1.1.m1.1.1.1.1.1.1" xref="A3.SS5.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A3.SS5.p1.1.m1.1.1.1.1.1.3" xref="A3.SS5.p1.1.m1.1.1.1.1.1.3.cmml">e</mi><mo id="A3.SS5.p1.1.m1.1.1.1.1.1.1a" xref="A3.SS5.p1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mn id="A3.SS5.p1.1.m1.1.1.1.1.1.4" xref="A3.SS5.p1.1.m1.1.1.1.1.1.4.cmml">0</mn></mrow><mo id="A3.SS5.p1.1.m1.7.7.7.7.9" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml">,</mo><mrow id="A3.SS5.p1.1.m1.2.2.2.2.2" xref="A3.SS5.p1.1.m1.2.2.2.2.2.cmml"><mn id="A3.SS5.p1.1.m1.2.2.2.2.2.2" xref="A3.SS5.p1.1.m1.2.2.2.2.2.2.cmml">1</mn><mo id="A3.SS5.p1.1.m1.2.2.2.2.2.1" xref="A3.SS5.p1.1.m1.2.2.2.2.2.1.cmml">⁢</mo><mi id="A3.SS5.p1.1.m1.2.2.2.2.2.3" xref="A3.SS5.p1.1.m1.2.2.2.2.2.3.cmml">e</mi><mo id="A3.SS5.p1.1.m1.2.2.2.2.2.1a" xref="A3.SS5.p1.1.m1.2.2.2.2.2.1.cmml">⁢</mo><mn id="A3.SS5.p1.1.m1.2.2.2.2.2.4" xref="A3.SS5.p1.1.m1.2.2.2.2.2.4.cmml">1</mn></mrow><mo id="A3.SS5.p1.1.m1.7.7.7.7.10" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml">,</mo><mrow id="A3.SS5.p1.1.m1.3.3.3.3.3" xref="A3.SS5.p1.1.m1.3.3.3.3.3.cmml"><mn id="A3.SS5.p1.1.m1.3.3.3.3.3.2" xref="A3.SS5.p1.1.m1.3.3.3.3.3.2.cmml">1</mn><mo id="A3.SS5.p1.1.m1.3.3.3.3.3.1" xref="A3.SS5.p1.1.m1.3.3.3.3.3.1.cmml">⁢</mo><mi id="A3.SS5.p1.1.m1.3.3.3.3.3.3" xref="A3.SS5.p1.1.m1.3.3.3.3.3.3.cmml">e</mi><mo id="A3.SS5.p1.1.m1.3.3.3.3.3.1a" xref="A3.SS5.p1.1.m1.3.3.3.3.3.1.cmml">⁢</mo><mn id="A3.SS5.p1.1.m1.3.3.3.3.3.4" xref="A3.SS5.p1.1.m1.3.3.3.3.3.4.cmml">2</mn></mrow><mo id="A3.SS5.p1.1.m1.7.7.7.7.11" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml">,</mo><mrow id="A3.SS5.p1.1.m1.4.4.4.4.4" xref="A3.SS5.p1.1.m1.4.4.4.4.4.cmml"><mn id="A3.SS5.p1.1.m1.4.4.4.4.4.2" xref="A3.SS5.p1.1.m1.4.4.4.4.4.2.cmml">1</mn><mo id="A3.SS5.p1.1.m1.4.4.4.4.4.1" xref="A3.SS5.p1.1.m1.4.4.4.4.4.1.cmml">⁢</mo><mi id="A3.SS5.p1.1.m1.4.4.4.4.4.3" xref="A3.SS5.p1.1.m1.4.4.4.4.4.3.cmml">e</mi><mo id="A3.SS5.p1.1.m1.4.4.4.4.4.1a" xref="A3.SS5.p1.1.m1.4.4.4.4.4.1.cmml">⁢</mo><mn id="A3.SS5.p1.1.m1.4.4.4.4.4.4" xref="A3.SS5.p1.1.m1.4.4.4.4.4.4.cmml">3</mn></mrow><mo id="A3.SS5.p1.1.m1.7.7.7.7.12" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml">,</mo><mrow id="A3.SS5.p1.1.m1.5.5.5.5.5" xref="A3.SS5.p1.1.m1.5.5.5.5.5.cmml"><mn id="A3.SS5.p1.1.m1.5.5.5.5.5.2" xref="A3.SS5.p1.1.m1.5.5.5.5.5.2.cmml">1</mn><mo id="A3.SS5.p1.1.m1.5.5.5.5.5.1" xref="A3.SS5.p1.1.m1.5.5.5.5.5.1.cmml">⁢</mo><mi id="A3.SS5.p1.1.m1.5.5.5.5.5.3" xref="A3.SS5.p1.1.m1.5.5.5.5.5.3.cmml">e</mi><mo id="A3.SS5.p1.1.m1.5.5.5.5.5.1a" xref="A3.SS5.p1.1.m1.5.5.5.5.5.1.cmml">⁢</mo><mn id="A3.SS5.p1.1.m1.5.5.5.5.5.4" xref="A3.SS5.p1.1.m1.5.5.5.5.5.4.cmml">4</mn></mrow><mo id="A3.SS5.p1.1.m1.7.7.7.7.13" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml">,</mo><mrow id="A3.SS5.p1.1.m1.6.6.6.6.6" xref="A3.SS5.p1.1.m1.6.6.6.6.6.cmml"><mn id="A3.SS5.p1.1.m1.6.6.6.6.6.2" xref="A3.SS5.p1.1.m1.6.6.6.6.6.2.cmml">1</mn><mo id="A3.SS5.p1.1.m1.6.6.6.6.6.1" xref="A3.SS5.p1.1.m1.6.6.6.6.6.1.cmml">⁢</mo><mi id="A3.SS5.p1.1.m1.6.6.6.6.6.3" xref="A3.SS5.p1.1.m1.6.6.6.6.6.3.cmml">e</mi><mo id="A3.SS5.p1.1.m1.6.6.6.6.6.1a" xref="A3.SS5.p1.1.m1.6.6.6.6.6.1.cmml">⁢</mo><mn id="A3.SS5.p1.1.m1.6.6.6.6.6.4" xref="A3.SS5.p1.1.m1.6.6.6.6.6.4.cmml">5</mn></mrow><mo id="A3.SS5.p1.1.m1.7.7.7.7.14" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml">,</mo><mrow id="A3.SS5.p1.1.m1.7.7.7.7.7" xref="A3.SS5.p1.1.m1.7.7.7.7.7.cmml"><mn id="A3.SS5.p1.1.m1.7.7.7.7.7.2" xref="A3.SS5.p1.1.m1.7.7.7.7.7.2.cmml">1</mn><mo id="A3.SS5.p1.1.m1.7.7.7.7.7.1" xref="A3.SS5.p1.1.m1.7.7.7.7.7.1.cmml">⁢</mo><mi id="A3.SS5.p1.1.m1.7.7.7.7.7.3" xref="A3.SS5.p1.1.m1.7.7.7.7.7.3.cmml">e</mi><mo id="A3.SS5.p1.1.m1.7.7.7.7.7.1a" xref="A3.SS5.p1.1.m1.7.7.7.7.7.1.cmml">⁢</mo><mn id="A3.SS5.p1.1.m1.7.7.7.7.7.4" xref="A3.SS5.p1.1.m1.7.7.7.7.7.4.cmml">6</mn></mrow><mo id="A3.SS5.p1.1.m1.7.7.7.7.15" stretchy="false" xref="A3.SS5.p1.1.m1.7.7.7.8.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS5.p1.1.m1.7b"><apply id="A3.SS5.p1.1.m1.7.7.cmml" xref="A3.SS5.p1.1.m1.7.7"><eq id="A3.SS5.p1.1.m1.7.7.8.cmml" xref="A3.SS5.p1.1.m1.7.7.8"></eq><ci id="A3.SS5.p1.1.m1.7.7.9.cmml" xref="A3.SS5.p1.1.m1.7.7.9">𝑐</ci><set id="A3.SS5.p1.1.m1.7.7.7.8.cmml" xref="A3.SS5.p1.1.m1.7.7.7.7"><apply id="A3.SS5.p1.1.m1.1.1.1.1.1.cmml" xref="A3.SS5.p1.1.m1.1.1.1.1.1"><times id="A3.SS5.p1.1.m1.1.1.1.1.1.1.cmml" xref="A3.SS5.p1.1.m1.1.1.1.1.1.1"></times><cn id="A3.SS5.p1.1.m1.1.1.1.1.1.2.cmml" type="integer" xref="A3.SS5.p1.1.m1.1.1.1.1.1.2">1</cn><ci id="A3.SS5.p1.1.m1.1.1.1.1.1.3.cmml" xref="A3.SS5.p1.1.m1.1.1.1.1.1.3">𝑒</ci><cn id="A3.SS5.p1.1.m1.1.1.1.1.1.4.cmml" type="integer" xref="A3.SS5.p1.1.m1.1.1.1.1.1.4">0</cn></apply><apply id="A3.SS5.p1.1.m1.2.2.2.2.2.cmml" xref="A3.SS5.p1.1.m1.2.2.2.2.2"><times id="A3.SS5.p1.1.m1.2.2.2.2.2.1.cmml" xref="A3.SS5.p1.1.m1.2.2.2.2.2.1"></times><cn id="A3.SS5.p1.1.m1.2.2.2.2.2.2.cmml" type="integer" xref="A3.SS5.p1.1.m1.2.2.2.2.2.2">1</cn><ci id="A3.SS5.p1.1.m1.2.2.2.2.2.3.cmml" xref="A3.SS5.p1.1.m1.2.2.2.2.2.3">𝑒</ci><cn id="A3.SS5.p1.1.m1.2.2.2.2.2.4.cmml" type="integer" xref="A3.SS5.p1.1.m1.2.2.2.2.2.4">1</cn></apply><apply id="A3.SS5.p1.1.m1.3.3.3.3.3.cmml" xref="A3.SS5.p1.1.m1.3.3.3.3.3"><times id="A3.SS5.p1.1.m1.3.3.3.3.3.1.cmml" xref="A3.SS5.p1.1.m1.3.3.3.3.3.1"></times><cn id="A3.SS5.p1.1.m1.3.3.3.3.3.2.cmml" type="integer" xref="A3.SS5.p1.1.m1.3.3.3.3.3.2">1</cn><ci id="A3.SS5.p1.1.m1.3.3.3.3.3.3.cmml" xref="A3.SS5.p1.1.m1.3.3.3.3.3.3">𝑒</ci><cn id="A3.SS5.p1.1.m1.3.3.3.3.3.4.cmml" type="integer" xref="A3.SS5.p1.1.m1.3.3.3.3.3.4">2</cn></apply><apply id="A3.SS5.p1.1.m1.4.4.4.4.4.cmml" xref="A3.SS5.p1.1.m1.4.4.4.4.4"><times id="A3.SS5.p1.1.m1.4.4.4.4.4.1.cmml" xref="A3.SS5.p1.1.m1.4.4.4.4.4.1"></times><cn id="A3.SS5.p1.1.m1.4.4.4.4.4.2.cmml" type="integer" xref="A3.SS5.p1.1.m1.4.4.4.4.4.2">1</cn><ci id="A3.SS5.p1.1.m1.4.4.4.4.4.3.cmml" xref="A3.SS5.p1.1.m1.4.4.4.4.4.3">𝑒</ci><cn id="A3.SS5.p1.1.m1.4.4.4.4.4.4.cmml" type="integer" xref="A3.SS5.p1.1.m1.4.4.4.4.4.4">3</cn></apply><apply id="A3.SS5.p1.1.m1.5.5.5.5.5.cmml" xref="A3.SS5.p1.1.m1.5.5.5.5.5"><times id="A3.SS5.p1.1.m1.5.5.5.5.5.1.cmml" xref="A3.SS5.p1.1.m1.5.5.5.5.5.1"></times><cn id="A3.SS5.p1.1.m1.5.5.5.5.5.2.cmml" type="integer" xref="A3.SS5.p1.1.m1.5.5.5.5.5.2">1</cn><ci id="A3.SS5.p1.1.m1.5.5.5.5.5.3.cmml" xref="A3.SS5.p1.1.m1.5.5.5.5.5.3">𝑒</ci><cn id="A3.SS5.p1.1.m1.5.5.5.5.5.4.cmml" type="integer" xref="A3.SS5.p1.1.m1.5.5.5.5.5.4">4</cn></apply><apply id="A3.SS5.p1.1.m1.6.6.6.6.6.cmml" xref="A3.SS5.p1.1.m1.6.6.6.6.6"><times id="A3.SS5.p1.1.m1.6.6.6.6.6.1.cmml" xref="A3.SS5.p1.1.m1.6.6.6.6.6.1"></times><cn id="A3.SS5.p1.1.m1.6.6.6.6.6.2.cmml" type="integer" xref="A3.SS5.p1.1.m1.6.6.6.6.6.2">1</cn><ci id="A3.SS5.p1.1.m1.6.6.6.6.6.3.cmml" xref="A3.SS5.p1.1.m1.6.6.6.6.6.3">𝑒</ci><cn id="A3.SS5.p1.1.m1.6.6.6.6.6.4.cmml" type="integer" xref="A3.SS5.p1.1.m1.6.6.6.6.6.4">5</cn></apply><apply id="A3.SS5.p1.1.m1.7.7.7.7.7.cmml" xref="A3.SS5.p1.1.m1.7.7.7.7.7"><times id="A3.SS5.p1.1.m1.7.7.7.7.7.1.cmml" xref="A3.SS5.p1.1.m1.7.7.7.7.7.1"></times><cn id="A3.SS5.p1.1.m1.7.7.7.7.7.2.cmml" type="integer" xref="A3.SS5.p1.1.m1.7.7.7.7.7.2">1</cn><ci id="A3.SS5.p1.1.m1.7.7.7.7.7.3.cmml" xref="A3.SS5.p1.1.m1.7.7.7.7.7.3">𝑒</ci><cn id="A3.SS5.p1.1.m1.7.7.7.7.7.4.cmml" type="integer" xref="A3.SS5.p1.1.m1.7.7.7.7.7.4">6</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS5.p1.1.m1.7c">c=\{1e0,1e1,1e2,1e3,1e4,1e5,1e6\}</annotation><annotation encoding="application/x-llamapun" id="A3.SS5.p1.1.m1.7d">italic_c = { 1 italic_e 0 , 1 italic_e 1 , 1 italic_e 2 , 1 italic_e 3 , 1 italic_e 4 , 1 italic_e 5 , 1 italic_e 6 }</annotation></semantics></math> where <math alttext="c" class="ltx_Math" display="inline" id="A3.SS5.p1.2.m2.1"><semantics id="A3.SS5.p1.2.m2.1a"><mi id="A3.SS5.p1.2.m2.1.1" xref="A3.SS5.p1.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A3.SS5.p1.2.m2.1b"><ci id="A3.SS5.p1.2.m2.1.1.cmml" xref="A3.SS5.p1.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS5.p1.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="A3.SS5.p1.2.m2.1d">italic_c</annotation></semantics></math> is the inverse regularization strength.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.6 </span>Additional compute details</h3>
<div class="ltx_para" id="A3.SS6.p1">
<p class="ltx_p" id="A3.SS6.p1.1">See subsections above for experiment-specific compute details. This full research project required additional compute for experiments and results that are not included in this paper; these computations were also done on single NVIDIA Titan RTX, GeForce 2080, GeForce 3090, and V100 GPUs.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.7 </span>Licenses for existing assets</h3>
<div class="ltx_para" id="A3.SS7.p1">
<p class="ltx_p" id="A3.SS7.p1.1">The datasets and models we use are released under the following licenses: 
<br class="ltx_break"/></p>
<table class="ltx_tabular ltx_centering ltx_minipage ltx_align_middle" id="A3.SS7.p1.2" style="width:151.8pt;">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.SS7.p1.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.SS7.p1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.SS7.p1.2.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.SS7.p1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.SS7.p1.2.1.1.2.1">License</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.SS7.p1.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.SS7.p1.2.2.1.1">NIGHTS</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.SS7.p1.2.2.1.2">MIT</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.3.2">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.3.2.1">VTAB</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.3.2.2">Apache 2.0</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.4.3">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.4.3.1">BAPPS</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.4.3.2">BSD 2-Clause</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.5.4">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.5.4.1">THINGS</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.5.4.2">CC0 1.0 Universal</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.6.5">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.6.5.1">ImageNet</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.6.5.2">CC BY-NC</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.7.6">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.7.6.1">FSC147</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.7.6.2">Apache 2.0</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.8.7">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.8.7.1">CARPK</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.8.7.2">CC0 1.0 Universal</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.9.8">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.9.8.1">Pascal VOC</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.9.8.2">CC BY 2.0</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.10.9">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.10.9.1">ADE20k</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.10.9.2">BSD 3-Clause</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.11.10">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.11.10.1">Cityscapes</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.11.10.2">CC BY-NC</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.12.11">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.12.11.1">COCO</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.12.11.2">CC BY 4.0</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.13.12">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.13.12.1">DAVIS2017</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.13.12.2">CC BY 4.0</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.14.13">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.14.13.1">NYUv2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.14.13.2">MIT</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.15.14">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.15.14.1">4D Light Field</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.15.14.2">CC BY-NC-SA 4.0</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.16.15">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.2.16.15.1">SUN-RGBD</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.2.16.15.2">MIT</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.2.17.16">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.SS7.p1.2.17.16.1">DeepFashion</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.SS7.p1.2.17.16.2">CC BY-NC</td>
</tr>
</tbody>
</table>
<table class="ltx_tabular ltx_centering ltx_minipage ltx_align_middle" id="A3.SS7.p1.3" style="width:173.4pt;">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.SS7.p1.3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.SS7.p1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.SS7.p1.3.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.SS7.p1.3.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.SS7.p1.3.1.1.2.1">License</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.SS7.p1.3.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.SS7.p1.3.2.1.1">DINO</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A3.SS7.p1.3.2.1.2">Apache 2.0</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.3.3.2">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.3.3.2.1">CLIP</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.3.3.2.2">MIT</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.3.4.3">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.3.4.3.1">OpenCLIP</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.3.4.3.2">MIT</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.3.5.4">
<td class="ltx_td ltx_align_center" id="A3.SS7.p1.3.5.4.1">DreamSim</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A3.SS7.p1.3.5.4.2">MIT</td>
</tr>
<tr class="ltx_tr" id="A3.SS7.p1.3.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.SS7.p1.3.6.5.1">SynCLR</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A3.SS7.p1.3.6.5.2">Apache 2.0</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct 14 17:10:14 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
