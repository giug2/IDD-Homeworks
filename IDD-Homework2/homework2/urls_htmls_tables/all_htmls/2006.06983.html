<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2006.06983] Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data</title><meta property="og:description" content="Federated learning (FL) is an emerging, privacy-preserving machine learning paradigm, drawing tremendous attention in both academia and industry.
A unique characteristic of FL is heterogeneity, which resides in the var…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2006.06983">

<!--Generated on Wed Mar  6 09:31:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated learning; heterogeneity; measurement study">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chengxu Yang, Qipeng Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_affiliation_institution">Key Lab of High Confidence Software Technologies (Peking University), MoE</span><span id="id3.2.id2" class="ltx_text ltx_affiliation_country">Beijing, China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yangchengxu@pku.edu.cn">yangchengxu@pku.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wangqipeng@stu.pku.edu.cn">wangqipeng@stu.pku.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mengwei Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">State Key Laboratory of Networking and Switching Technology (BUPT)</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_institution">Key Lab of High Confidence Software Technologies (Peking University), MoE</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">Beijing, China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:mwx@bupt.edu.cn">mwx@bupt.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenpeng Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Key Lab of High Confidence Software Technologies (Peking University), MoE</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country">Beijing, China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:czp@pku.edu.cn">czp@pku.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kaigui Bian
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">Department of Computer Science and Technology, Peking University</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_country">Beijing, China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:bkg@pku.edu.cn">bkg@pku.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunxin Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_affiliation_institution">Microsoft Research</span><span id="id12.2.id2" class="ltx_text ltx_affiliation_country">Beijing, China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yunxin.liu@microsoft.com">yunxin.liu@microsoft.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuanzhe Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Key Lab of High Confidence Software Technologies (Peking University), MoE</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_country">Beijing, China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xzl@pku.edu.cn">xzl@pku.edu.cn</a>
</span></span></span>
</div>
<div class="ltx_dates">(2021)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id15.id1" class="ltx_p">Federated learning (FL) is an emerging, privacy-preserving machine learning paradigm, drawing tremendous attention in both academia and industry.
A unique characteristic of FL is <span id="id15.id1.1" class="ltx_text ltx_font_italic">heterogeneity</span>, which resides in the various hardware specifications and dynamic states across the participating devices.
Theoretically, heterogeneity can exert a huge influence on the FL training process, e.g., causing a device unavailable for training or unable to upload its model updates.
Unfortunately, these impacts have never been systematically studied and quantified in existing FL literature.</p>
<p id="id1.1" class="ltx_p">In this paper, we carry out the first empirical study to characterize the impacts of heterogeneity in FL.
We collect large-scale data from 136k smartphones that can faithfully reflect heterogeneity in real-world settings.
We also build a <span id="id1.1.1" class="ltx_text ltx_font_italic">heterogeneity-aware</span> FL platform that complies with the standard FL protocol but with heterogeneity in consideration.
Based on the data and the platform, we conduct extensive experiments to compare the performance of state-of-the-art FL algorithms under <span id="id1.1.2" class="ltx_text ltx_font_italic">heterogeneity-aware</span> and <span id="id1.1.3" class="ltx_text ltx_font_italic">heterogeneity-unaware</span> settings.
Results show that heterogeneity causes non-trivial performance degradation in FL, including up to 9.2% accuracy drop, 2.32<math id="id1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation></semantics></math> lengthened training time, and undermined fairness.
Furthermore, we analyze potential impact factors and find that <span id="id1.1.4" class="ltx_text ltx_font_italic">device failure</span> and <span id="id1.1.5" class="ltx_text ltx_font_italic">participant bias</span> are two potential factors for performance degradation.
Our study provides insightful implications for FL practitioners.
On the one hand, our findings suggest that FL algorithm designers consider necessary heterogeneity during the evaluation.
On the other hand, our findings urge system providers to design specific mechanisms to mitigate the impacts of heterogeneity.</p>
</div>
<div class="ltx_keywords">Federated learning; heterogeneity; measurement study
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2021</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>iw3c2w3</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the Web Conference 2021; April 19–23, 2021; Ljubljana, Slovenia</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the Web Conference 2021 (WWW ’21), April 19–23, 2021,
Ljubljana, Slovenia</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3442381.3449851</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-8312-7/21/04</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Ubiquitous and mobile computing</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Artificial intelligence</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Security and privacy Privacy protections</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the past few years, we have witnessed the increase of machine learning (ML) applications deployed on mobile devices <cite class="ltx_cite ltx_citemacro_citep">(Xu
et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2019a</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>; Xu
et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2018b</a>, <a href="#bib.bib51" title="" class="ltx_ref">a</a>)</cite>. These applications usually need to collect personal user data to train ML models. However, due to the increasing concerns of user privacy, e.g., the recent released GDPR <cite class="ltx_cite ltx_citemacro_citep">(Wikipedia, <a href="#bib.bib48" title="" class="ltx_ref">2020c</a>)</cite> and CCPA <cite class="ltx_cite ltx_citemacro_citep">(Wikipedia, <a href="#bib.bib46" title="" class="ltx_ref">2020a</a>)</cite>,
personal data cannot be arbitrarily collected and used without permission granted <cite class="ltx_cite ltx_citemacro_citep">(Meurisch et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>.
Therefore, various privacy-preserving ML techniques have been proposed <cite class="ltx_cite ltx_citemacro_citep">(Archer et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2017</a>; McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite>, where the emerging federated learning (FL) has drawn tremendous attentions <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite>.
The key idea of FL is to employ a set of personal mobile devices, e.g., smartphones, to train an ML model collaboratively under the orchestration of a central parameter server. Since the training process of FL takes place on mobile devices (i.e., on-device training) without uploading user personal data outside devices, it is considered to be quite promising for preserving user privacy in ML applications.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, various emerging FL algorithms have been proposed, e.g., <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>)</cite>, <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">Structured Updates</span> <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>)</cite>, and <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">q-FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>)</cite>.
To evaluate these algorithms, existing FL studies typically take a simulation approach <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>; Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> given the high cost of real deployment.
However, they have no data to describe how devices participate in FL<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Although Google has built a practical FL system  <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>)</cite>, its detailed data are not disclosed.</span></span></span>.
As a result, they usually have an overly ideal assumption, i.e., all the devices are always available for training and equipped with <span id="S1.p2.1.4" class="ltx_text ltx_font_italic">homogeneous</span> hardware specifications (e.g., the same CPU and RAM capacity) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>; Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>; Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>; Mohri
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, these assumptions could be too ideal for FL deployment in practice.
More specifically, FL usually requires a substantial number of devices to collaboratively accomplish a learning task, which poses a unique challenge, namely <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">heterogeneity</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite>.
In practice, the heterogeneity can be attributed to two major aspects:
(1) One is from hardware specifications of devices (called <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">hardware heterogeneity</span>), e.g., different CPU, RAM, and battery life.
(2) Additionally, the state and running environment of participating devices can be various and dynamic (called <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">state heterogeneity</span>), e.g., CPU busy/free, stable/unreliable network connections to the server, etc.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Intuitively, heterogeneity can impact FL in terms of accuracy and training time. For instance, it is not surprising when a device fails to upload its local model updates to the server (called <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">device failure</span>), which can definitely affect the training time to obtain a converged global model.
Furthermore, devices that seldom participate in an FL task due to abnormal states, e.g., CPU busy, can be underrepresented by the global model.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Although some recent studies <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>; Nishio and
Yonetani, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Laguel et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>; Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> have realized the heterogeneity in FL, its impacts have never been comprehensively quantified over large-scale real-world data.
In this paper, we present the first empirical study to demystify the impacts of heterogeneity in FL tasks.
To this end, we develop a holistic platform that complies with the standard and widely-adopted FL protocol  <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2018</a>; Hard et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>, but for the first time, facilitates reproducing existing FL algorithms under <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">heterogeneity-aware</span> settings, i.e., devices have dynamic states and various hardware capacities.
Undoubtedly, conducting such a study requires the data that can faithfully reflect the heterogeneity in real-world settings. Therefore, we collect the device hardware specifications and regular state changes (including the states related to device check-in and drop-out) of 136k smartphones in one week through a commodity input method app (IMA).
We then plug the data into our <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">heterogeneity-aware</span> platform to simulate the device state dynamics and hardware capacity.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Based on the data and platform, we conduct extensive measurement experiments to compare the state-of-the-art FL algorithms’ performance, including model accuracy and training time under heterogeneity-aware and heterogeneity-unaware settings.
We select four typical FL tasks: two image classification tasks and two natural language processing tasks. For every single task, we employ a benchmark dataset for model training. Three of the benchmark datasets <cite class="ltx_cite ltx_citemacro_citep">(PushShift.io, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>; Multimedia Laboratory, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>; Cohen
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite> have been widely used in existing FL-related studies <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>; Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>; Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>; McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>)</cite>, and the last one is a real-world text input dataset collected from the aforementioned IMA.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.6" class="ltx_p"><math id="S1.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p7.1.m1.1a"><mo id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">\bullet</annotation></semantics></math> <span id="S1.p7.6.1" class="ltx_text ltx_font_bold">Findings.</span> Heterogeneity leads to non-trivial impacts on the performance of FL algorithms, including accuracy drop, increased training time, and undermined fairness. For the basic algorithm (<math id="S1.p7.2.m2.1" class="ltx_Math" alttext="\S\ref{subsec:basic}" display="inline"><semantics id="S1.p7.2.m2.1a"><mrow id="S1.p7.2.m2.1.1" xref="S1.p7.2.m2.1.1.cmml"><mi mathvariant="normal" id="S1.p7.2.m2.1.1.2" xref="S1.p7.2.m2.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S1.p7.2.m2.1.1.1" xref="S1.p7.2.m2.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S1.p7.2.m2.1.1.3" xref="S1.p7.2.m2.1.1.3c.cmml"><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S1.p7.2.m2.1b"><apply id="S1.p7.2.m2.1.1.cmml" xref="S1.p7.2.m2.1.1"><times id="S1.p7.2.m2.1.1.1.cmml" xref="S1.p7.2.m2.1.1.1"></times><ci id="S1.p7.2.m2.1.1.2.cmml" xref="S1.p7.2.m2.1.1.2">§</ci><ci id="S1.p7.2.m2.1.1.3c.cmml" xref="S1.p7.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S1.p7.2.m2.1.1.3.cmml" xref="S1.p7.2.m2.1.1.3"><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m2.1c">\S\ref{subsec:basic}</annotation></semantics></math>), i.e., <span id="S1.p7.6.2" class="ltx_text ltx_font_italic">FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>)</cite>, when heterogeneity is considered, its performance is compromised in terms of 3.1% accuracy drop (up to 9.2%) and 1.74<math id="S1.p7.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p7.3.m3.1a"><mo id="S1.p7.3.m3.1.1" xref="S1.p7.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p7.3.m3.1b"><times id="S1.p7.3.m3.1.1.cmml" xref="S1.p7.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.3.m3.1c">\times</annotation></semantics></math> training time (up to 2.32<math id="S1.p7.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p7.4.m4.1a"><mo id="S1.p7.4.m4.1.1" xref="S1.p7.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p7.4.m4.1b"><times id="S1.p7.4.m4.1.1.cmml" xref="S1.p7.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.4.m4.1c">\times</annotation></semantics></math>) on average. For other advanced algorithms (<math id="S1.p7.5.m5.1" class="ltx_Math" alttext="\S\ref{subsec:advance}" display="inline"><semantics id="S1.p7.5.m5.1a"><mrow id="S1.p7.5.m5.1.1" xref="S1.p7.5.m5.1.1.cmml"><mi mathvariant="normal" id="S1.p7.5.m5.1.1.2" xref="S1.p7.5.m5.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S1.p7.5.m5.1.1.1" xref="S1.p7.5.m5.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S1.p7.5.m5.1.1.3" xref="S1.p7.5.m5.1.1.3c.cmml"><a href="#S4.SS2" title="4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S1.p7.5.m5.1b"><apply id="S1.p7.5.m5.1.1.cmml" xref="S1.p7.5.m5.1.1"><times id="S1.p7.5.m5.1.1.1.cmml" xref="S1.p7.5.m5.1.1.1"></times><ci id="S1.p7.5.m5.1.1.2.cmml" xref="S1.p7.5.m5.1.1.2">§</ci><ci id="S1.p7.5.m5.1.1.3c.cmml" xref="S1.p7.5.m5.1.1.3"><mtext class="ltx_mathvariant_italic" id="S1.p7.5.m5.1.1.3.cmml" xref="S1.p7.5.m5.1.1.3"><a href="#S4.SS2" title="4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.5.m5.1c">\S\ref{subsec:advance}</annotation></semantics></math>), i.e., gradient compression algorithms, including <span id="S1.p7.6.3" class="ltx_text ltx_font_italic">Structured Updates</span> <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>)</cite>, <span id="S1.p7.6.4" class="ltx_text ltx_font_italic">Gradient Dropping</span> <cite class="ltx_cite ltx_citemacro_citep">(Aji and Heafield, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>, and <span id="S1.p7.6.5" class="ltx_text ltx_font_italic">SignSGD</span> <cite class="ltx_cite ltx_citemacro_citep">(Bernstein et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, and advanced aggregation algorithms, i.e., <span id="S1.p7.6.6" class="ltx_text ltx_font_italic">q-FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>)</cite> and <span id="S1.p7.6.7" class="ltx_text ltx_font_italic">FedProx</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite>, optimizations are not always effective as reported. For example, heterogeneity hinders <span id="S1.p7.6.8" class="ltx_text ltx_font_italic">q-FedAvg</span> from addressing the fairness issues in FL.
We also find that current gradient compression algorithms can hardly speed up FL convergence under heterogeneity-aware settings.
In the worst case, the training time is lengthened by 3.5<math id="S1.p7.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p7.6.m6.1a"><mo id="S1.p7.6.m6.1.1" xref="S1.p7.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.p7.6.m6.1b"><times id="S1.p7.6.m6.1.1.cmml" xref="S1.p7.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.6.m6.1c">\times</annotation></semantics></math>. These findings indicate that heterogeneity cannot be simply ignored when designing FL algorithms.</p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.4" class="ltx_p"><math id="S1.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p8.1.m1.1a"><mo id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><ci id="S1.p8.1.m1.1.1.cmml" xref="S1.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">\bullet</annotation></semantics></math> <span id="S1.p8.4.1" class="ltx_text ltx_font_bold">Analysis of Potential Impact Factors</span>.
We first break down the heterogeneity to analyze the individual impacts of state heterogeneity and hardware heterogeneity, respectively (<math id="S1.p8.2.m2.1" class="ltx_Math" alttext="\S\ref{subsec:breakdown}" display="inline"><semantics id="S1.p8.2.m2.1a"><mrow id="S1.p8.2.m2.1.1" xref="S1.p8.2.m2.1.1.cmml"><mi mathvariant="normal" id="S1.p8.2.m2.1.1.2" xref="S1.p8.2.m2.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S1.p8.2.m2.1.1.1" xref="S1.p8.2.m2.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S1.p8.2.m2.1.1.3" xref="S1.p8.2.m2.1.1.3c.cmml"><a href="#S5.SS1" title="5.1. Breakdown of Heterogeneity ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><apply id="S1.p8.2.m2.1.1.cmml" xref="S1.p8.2.m2.1.1"><times id="S1.p8.2.m2.1.1.1.cmml" xref="S1.p8.2.m2.1.1.1"></times><ci id="S1.p8.2.m2.1.1.2.cmml" xref="S1.p8.2.m2.1.1.2">§</ci><ci id="S1.p8.2.m2.1.1.3c.cmml" xref="S1.p8.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S1.p8.2.m2.1.1.3.cmml" xref="S1.p8.2.m2.1.1.3"><a href="#S5.SS1" title="5.1. Breakdown of Heterogeneity ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">\S\ref{subsec:breakdown}</annotation></semantics></math>).
We find that both types of heterogeneity can slow down the learning process, while state heterogeneity is often more responsible for the accuracy degradation.
Then we zoom into our experiments and find out two major factors that are particularly obvious under heterogeneity-aware settings.
(1) <span id="S1.p8.4.2" class="ltx_text ltx_font_italic">Device failure</span> (<math id="S1.p8.3.m3.1" class="ltx_Math" alttext="\S\ref{subsec:failure}" display="inline"><semantics id="S1.p8.3.m3.1a"><mrow id="S1.p8.3.m3.1.1" xref="S1.p8.3.m3.1.1.cmml"><mi mathvariant="normal" id="S1.p8.3.m3.1.1.2" xref="S1.p8.3.m3.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S1.p8.3.m3.1.1.1" xref="S1.p8.3.m3.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S1.p8.3.m3.1.1.3" xref="S1.p8.3.m3.1.1.3c.cmml"><a href="#S5.SS2" title="5.2. Device Failure ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.2</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S1.p8.3.m3.1b"><apply id="S1.p8.3.m3.1.1.cmml" xref="S1.p8.3.m3.1.1"><times id="S1.p8.3.m3.1.1.1.cmml" xref="S1.p8.3.m3.1.1.1"></times><ci id="S1.p8.3.m3.1.1.2.cmml" xref="S1.p8.3.m3.1.1.2">§</ci><ci id="S1.p8.3.m3.1.1.3c.cmml" xref="S1.p8.3.m3.1.1.3"><mtext class="ltx_mathvariant_italic" id="S1.p8.3.m3.1.1.3.cmml" xref="S1.p8.3.m3.1.1.3"><a href="#S5.SS2" title="5.2. Device Failure ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.2</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.3.m3.1c">\S\ref{subsec:failure}</annotation></semantics></math>): On average, 11.6% of selected devices fail to upload their model updates per round due to unreliable network, excessive training time, and drop-out caused by user interruption.
This failure slows down the model convergence and wastes valuable hardware resources.
(2) <span id="S1.p8.4.3" class="ltx_text ltx_font_italic">Participant bias</span> (<math id="S1.p8.4.m4.1" class="ltx_Math" alttext="\S\ref{subsec:bias}" display="inline"><semantics id="S1.p8.4.m4.1a"><mrow id="S1.p8.4.m4.1.1" xref="S1.p8.4.m4.1.1.cmml"><mi mathvariant="normal" id="S1.p8.4.m4.1.1.2" xref="S1.p8.4.m4.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S1.p8.4.m4.1.1.1" xref="S1.p8.4.m4.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S1.p8.4.m4.1.1.3" xref="S1.p8.4.m4.1.1.3c.cmml"><a href="#S5.SS3" title="5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.3</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S1.p8.4.m4.1b"><apply id="S1.p8.4.m4.1.1.cmml" xref="S1.p8.4.m4.1.1"><times id="S1.p8.4.m4.1.1.1.cmml" xref="S1.p8.4.m4.1.1.1"></times><ci id="S1.p8.4.m4.1.1.2.cmml" xref="S1.p8.4.m4.1.1.2">§</ci><ci id="S1.p8.4.m4.1.1.3c.cmml" xref="S1.p8.4.m4.1.1.3"><mtext class="ltx_mathvariant_italic" id="S1.p8.4.m4.1.1.3.cmml" xref="S1.p8.4.m4.1.1.3"><a href="#S5.SS3" title="5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.3</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.4.m4.1c">\S\ref{subsec:bias}</annotation></semantics></math>): Devices attend FL process in a biased manner. For instance, we find that more than 30% of devices never participate in the learning process when the model converges and the global model is dominated by active devices (top 30% devices contribute to 81% total computation). State heterogeneity is the major cause for the participant bias.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Our extensive experiments provide several insightful implications as summarized in <math id="S1.p9.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S1.p9.1.m1.1a"><mi mathvariant="normal" id="S1.p9.1.m1.1.1" xref="S1.p9.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S1.p9.1.m1.1b"><ci id="S1.p9.1.m1.1.1.cmml" xref="S1.p9.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p9.1.m1.1c">\S</annotation></semantics></math><a href="#S6" title="6. Implications ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
For instance, FL algorithm designers should consider necessary heterogeneity in the evaluation environment of FL, while <span id="S1.p9.1.1" class="ltx_text ltx_font_italic">FL system providers</span> should design specific mechanisms to mitigate the impacts of heterogeneity.
In summary, the major contributions of this paper are as follows:</p>
</div>
<div id="S1.p10" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We build a heterogeneity-aware FL platform with a large-scale dataset collected from 136k smartphones, which can help simulate the state and hardware heterogeneity for exploring FL in real-world practice<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We have released our dataset and source code at <a target="_blank" href="https://github.com/PKU-Chengxu/FLASH" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PKU-Chengxu/FLASH</a> to facilitate future FL research.</span></span></span>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We conduct extensive measurement experiments to demystify the non-trivial impacts of heterogeneity in existing FL algorithms.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We make an in-depth analysis of possible factors for impacts introduced by heterogeneity. Our results can provide insightful and actionable implications for the research community.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2006.06983/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of our methodology.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background and related work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">FL</span> is an emerging privacy-preserving learning paradigm.
Among different FL scenarios <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>, we focus on a widely studied one, i.e., <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">cross-device</span> FL, which utilizes a federation of <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">client devices<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_upright">3</span></span><span id="footnote3.5" class="ltx_text ltx_font_upright">In the rest of this paper, we use </span><span id="footnote3.6" class="ltx_text">device</span><span id="footnote3.7" class="ltx_text ltx_font_upright"> to refer to </span><span id="footnote3.8" class="ltx_text">client device</span><span id="footnote3.9" class="ltx_text ltx_font_upright">.</span></span></span></span></span>, coordinated by a <span id="S2.p1.1.4" class="ltx_text ltx_font_italic">central server</span>, to train a global ML model.
A typical FL workflow <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>)</cite> consists of many rounds, where each round can be divided into three phases: (1) the central server first selects devices to participate in the FL; (2) each selected device retrieves the latest global model from the server as the local model, re-trains the local model with local data, and uploads the updated weights/gradients of the local model to the server; (3) the server finally aggregates the updates from devices and obtains a new global model.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In practice, FL is typically implemented based on state-of-the-art FL algorithms, such as <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>)</cite>. In <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">FedAvg</span>, devices perform multiple local training epochs, where per round a device updates the weights of its local model using its local data.
Then the central server averages the updated weights of local models as the new weights of the global model.
<span id="S2.p2.1.3" class="ltx_text ltx_font_italic">FedAvg</span> is a representative FL algorithm that has been widely used in existing FL-related studies <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>; Mohri
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>)</cite> and also deployed in the industry, e.g., in Google’s production FL system <cite class="ltx_cite ltx_citemacro_citep">(Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>.
Therefore, we use <span id="S2.p2.1.4" class="ltx_text ltx_font_italic">FedAvg</span> as the basic FL algorithm to study heterogeneity’s impacts (<math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:basic}" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3c.cmml"><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><times id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></times><ci id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">§</ci><ci id="S2.p2.1.m1.1.1.3c.cmml" xref="S2.p2.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3"><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\S\ref{subsec:basic}</annotation></semantics></math>).</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In addition, many advanced algorithms have been proposed to optimize FL, including reducing the communication cost between the central server and devices <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>; Smith
et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2017</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Reisizadeh et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>; Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2019b</a>; Yuan
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020</a>)</cite>, enhancing privacy guarantee <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; McMahan
et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2017b</a>; Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>; Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2019</a>; Nasr
et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2018</a>)</cite>,
ensuring fairness across devices <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>; Mohri
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>, minimizing on-device energy cost <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>, etc. However, most of them have not been well evaluated in a heterogeneity-aware environment, making their benefits unclear in real-world deployment.
Therefore, we make the first attempt to study how heterogeneity impacts the effectiveness of these advanced FL algorithms (<math id="S2.p3.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:advance}" display="inline"><semantics id="S2.p3.1.m1.1a"><mrow id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S2.p3.1.m1.1.1.2" xref="S2.p3.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S2.p3.1.m1.1.1.1" xref="S2.p3.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S2.p3.1.m1.1.1.3" xref="S2.p3.1.m1.1.1.3c.cmml"><a href="#S4.SS2" title="4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><apply id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1"><times id="S2.p3.1.m1.1.1.1.cmml" xref="S2.p3.1.m1.1.1.1"></times><ci id="S2.p3.1.m1.1.1.2.cmml" xref="S2.p3.1.m1.1.1.2">§</ci><ci id="S2.p3.1.m1.1.1.3c.cmml" xref="S2.p3.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S2.p3.1.m1.1.1.3.cmml" xref="S2.p3.1.m1.1.1.3"><a href="#S4.SS2" title="4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\S\ref{subsec:advance}</annotation></semantics></math>).</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Heterogeneity</span> is considered as one of the core challenges in FL <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite>.
Some existing work <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>; Nishio and
Yonetani, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Laguel et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>; Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> has studied the heterogeneity in FL but not in a comprehensive way.
In particular, they ignore state heterogeneity and randomly set training time to simulate hardware heterogeneity, leaving communication capacities unconsidered.
For example, <span id="S2.p4.1.2" class="ltx_text ltx_font_italic">FedProx</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite> handles the hardware heterogeneity by allowing each participating device to perform a variable amount of work, but the hardware capability of each device is randomly set and state changes of devices remain unconsidered.
<span id="S2.p4.1.3" class="ltx_text ltx_font_italic">FedCS</span> <cite class="ltx_cite ltx_citemacro_citep">(Nishio and
Yonetani, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite> accelerates FL by managing devices based on their resource conditions and allowing the server to aggregate as many device updates as possible, but it assumes that the network is stable and not congested, and randomly sets the training time from 5 to 500 seconds.
Chai et al. <cite class="ltx_cite ltx_citemacro_citep">(Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> have studied the impacts of hardware heterogeneity by allocating varied CPU resources when simulating FL, but they leave state heterogeneity unconsidered.
Our study differs from existing work in two aspects: (1) we comprehensively consider hardware heterogeneity and state heterogeneity in the experimental environment powered by real-world data, and (2) we build an FL scenario with a much larger device population (up to 136k).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>The Measurement Approach</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Approach Overview</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the overall workflow of our measurement approach.
It starts from a <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">benchmark dataset</span> that is typically partitioned into thousands or millions of devices holding their own local data for training ( <svg id="S3.SS1.p1.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S3.SS1.p1.1.pic1.1.1.1.1.1" class="ltx_text">1</span></foreignObject></g></g></svg>).
For a fair comparison, we always use the same partition strategy in the heterogeneity-aware settings and heterogeneity-unaware settings, i.e., the local training data on a given device are the same.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p">For heterogeneity-aware settings, we randomly assign a state trace ( <svg id="S3.SS1.p2.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S3.SS1.p2.1.pic1.1.1.1.1.1" class="ltx_text">2</span></foreignObject></g></g></svg>) and a hardware capacity ( <svg id="S3.SS1.p2.2.pic2" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S3.SS1.p2.2.pic2.1.1.1.1.1" class="ltx_text">3</span></foreignObject></g></g></svg>) to each device.
A state trace determines whether a device is available for local training at any simulation timestamp, while the hardware capacity specifies the training speed and communication bandwidth.
Both datasets are collected from large-scale real-world mobile devices through an IMA app (details in <math id="S3.SS1.p2.3.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS1.p2.3.m1.1a"><mi mathvariant="normal" id="S3.SS1.p2.3.m1.1.1" xref="S3.SS1.p2.3.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m1.1b"><ci id="S3.SS1.p2.3.m1.1.1.cmml" xref="S3.SS1.p2.3.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m1.1c">\S</annotation></semantics></math><a href="#S3.SS2" title="3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
As a result, we get a heterogeneous device set with different local training data, hardware capacities, and state change dynamics.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">For heterogeneity-unaware settings, we assign each device with an “ideal” state trace, i.e., the device always stays available for local training and never drops out ( <svg id="S3.SS1.p3.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S3.SS1.p3.1.pic1.1.1.1.1.1" class="ltx_text">4</span></foreignObject></g></g></svg>), and a uniform hardware capacity as the mid-end device in our IMA dataset (Redmi Note 8) ( <svg id="S3.SS1.p3.2.pic2" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S3.SS1.p3.2.pic2.1.1.1.1.1" class="ltx_text">5</span></foreignObject></g></g></svg>).
As a result, we get a homogeneous device set with the same hardware capacity and state change dynamics, as existing FL platforms do.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.4" class="ltx_p">We next deploy the two device sets to our FL simulation platform and execute the FL task (e.g., image classification) under the same configurations ( <svg id="S3.SS1.p4.1.pic1" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S3.SS1.p4.1.pic1.1.1.1.1.1" class="ltx_text">6</span></foreignObject></g></g></svg> and  <svg id="S3.SS1.p4.2.pic2" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S3.SS1.p4.2.pic2.1.1.1.1.1" class="ltx_text">7</span></foreignObject></g></g></svg>).
The simulation platform extends the standard FL protocol with heterogeneity consideration, e.g., a device can quit training due to a state change (details in <math id="S3.SS1.p4.3.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS1.p4.3.m1.1a"><mi mathvariant="normal" id="S3.SS1.p4.3.m1.1.1" xref="S3.SS1.p4.3.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m1.1b"><ci id="S3.SS1.p4.3.m1.1.1.cmml" xref="S3.SS1.p4.3.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m1.1c">\S</annotation></semantics></math><a href="#S3.SS3" title="3.3. The Simulation Platform ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
We finally analyze heterogeneity’s impacts by comparing the metric values achieved by heterogeneous devices and homogeneous devices ( <svg id="S3.SS1.p4.4.pic3" class="ltx_picture" height="15.74" overflow="visible" version="1.1" width="15.74"><g transform="translate(0,15.74) matrix(1 0 0 -1 0 0) translate(7.87,0) translate(0,7.87)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.59 0 C 7.59 4.19 4.19 7.59 0 7.59 C -4.19 7.59 -7.59 4.19 -7.59 0 C -7.59 -4.19 -4.19 -7.59 0 -7.59 C 4.19 -7.59 7.59 -4.19 7.59 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S3.SS1.p4.4.pic3.1.1.1.1.1" class="ltx_text">8</span></foreignObject></g></g></svg>).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>The Datasets</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">As described in <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:workflow}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3c.cmml"><a href="#S3.SS1" title="3.1. Approach Overview ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">3.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">§</ci><ci id="S3.SS2.p1.1.m1.1.1.3c.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><a href="#S3.SS1" title="3.1. Approach Overview ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">3.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\S\ref{subsec:workflow}</annotation></semantics></math>, we use two types of datasets in this study, including (1) the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">IMA dataset</span> describing the heterogeneity in real-world smartphone usage, and (2) benchmark datasets containing devices’ local data used for training and testing ML models.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span><span id="S3.SS2.SSS1.1.1" class="ltx_text ltx_font_bold">IMA dataset</span>
</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">To power the heterogeneity-aware settings, we collect large-scale real-world data from a popular IMA that can be downloaded from Google Play.
The dataset can be divided into two parts, including (1) <span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">device state traces</span> for annotating state heterogeneity, and (2) <span id="S3.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">capacity data</span> for annotating hardware heterogeneity.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p"><math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mo id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Device state traces</span> record the state changes (including battery charging, battery level, network environment, screen locking, etc.) of 136k devices within one week starting from Jan. 31 in 2020.
More specifically, every time the aforementioned state changes, the IMA records it with the timestamp and saves it as a state entry (refer to Table <a href="#S3.T1" title="Table 1 ‣ 3.2.1. IMA dataset ‣ 3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">In total, we collect 136k traces (one for each device) containing 180 million state entries, accounting for 111GB of storage.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.4" class="ltx_p">The state traces are to determine the time intervals when a device is available for local training, which are critical in understanding the FL performance under heterogeneity-aware settings.
Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2.1. IMA dataset ‣ 3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> concretely exemplifies how a trace works during the simulation.
The device becomes available for training at <math id="S3.SS2.SSS1.p4.1.m1.1" class="ltx_Math" alttext="T_{2}" display="inline"><semantics id="S3.SS2.SSS1.p4.1.m1.1a"><msub id="S3.SS2.SSS1.p4.1.m1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p4.1.m1.1.1.2" xref="S3.SS2.SSS1.p4.1.m1.1.1.2.cmml">T</mi><mn id="S3.SS2.SSS1.p4.1.m1.1.1.3" xref="S3.SS2.SSS1.p4.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.1.m1.1b"><apply id="S3.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p4.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.2">𝑇</ci><cn type="integer" id="S3.SS2.SSS1.p4.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.1.m1.1c">T_{2}</annotation></semantics></math> because it meets the state criteria <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>)</cite>, i.e., when a device is idle, charged, and connected to WiFi.
Then after a period of time at <math id="S3.SS2.SSS1.p4.2.m2.1" class="ltx_Math" alttext="T_{3}" display="inline"><semantics id="S3.SS2.SSS1.p4.2.m2.1a"><msub id="S3.SS2.SSS1.p4.2.m2.1.1" xref="S3.SS2.SSS1.p4.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p4.2.m2.1.1.2" xref="S3.SS2.SSS1.p4.2.m2.1.1.2.cmml">T</mi><mn id="S3.SS2.SSS1.p4.2.m2.1.1.3" xref="S3.SS2.SSS1.p4.2.m2.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.2.m2.1b"><apply id="S3.SS2.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p4.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p4.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.2">𝑇</ci><cn type="integer" id="S3.SS2.SSS1.p4.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.2.m2.1c">T_{3}</annotation></semantics></math>, the network environment changes to “4G”, thus the device becomes unavailable.
As a result, we obtain a training-available interval between <math id="S3.SS2.SSS1.p4.3.m3.1" class="ltx_Math" alttext="T_{2}" display="inline"><semantics id="S3.SS2.SSS1.p4.3.m3.1a"><msub id="S3.SS2.SSS1.p4.3.m3.1.1" xref="S3.SS2.SSS1.p4.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p4.3.m3.1.1.2" xref="S3.SS2.SSS1.p4.3.m3.1.1.2.cmml">T</mi><mn id="S3.SS2.SSS1.p4.3.m3.1.1.3" xref="S3.SS2.SSS1.p4.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.3.m3.1b"><apply id="S3.SS2.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p4.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p4.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p4.3.m3.1.1.2">𝑇</ci><cn type="integer" id="S3.SS2.SSS1.p4.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p4.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.3.m3.1c">T_{2}</annotation></semantics></math> and <math id="S3.SS2.SSS1.p4.4.m4.1" class="ltx_Math" alttext="T_{3}" display="inline"><semantics id="S3.SS2.SSS1.p4.4.m4.1a"><msub id="S3.SS2.SSS1.p4.4.m4.1.1" xref="S3.SS2.SSS1.p4.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p4.4.m4.1.1.2" xref="S3.SS2.SSS1.p4.4.m4.1.1.2.cmml">T</mi><mn id="S3.SS2.SSS1.p4.4.m4.1.1.3" xref="S3.SS2.SSS1.p4.4.m4.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.4.m4.1b"><apply id="S3.SS2.SSS1.p4.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p4.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p4.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p4.4.m4.1.1.2">𝑇</ci><cn type="integer" id="S3.SS2.SSS1.p4.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p4.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.4.m4.1c">T_{3}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p">As far as we know, this is the first-of-its-kind device usage dataset collected from large-scale real-world devices, making it much more representative than the datasets covering a small group of devices <cite class="ltx_cite ltx_citemacro_citep">(Kang
et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2011</a>; Yogesh
et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2014</a>)</cite>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:301.8pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.3pt,18.0pt) scale(0.75,0.75) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">Field</td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Description</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Example</td>
</tr>
<tr id="S3.T1.1.1.2" class="ltx_tr">
<td id="S3.T1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">user_id</td>
<td id="S3.T1.1.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Anonymized user id.</td>
<td id="S3.T1.1.1.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">xxxyyyzzz</td>
</tr>
<tr id="S3.T1.1.1.3" class="ltx_tr">
<td id="S3.T1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">device_model</td>
<td id="S3.T1.1.1.3.2" class="ltx_td ltx_align_left ltx_border_r">device type</td>
<td id="S3.T1.1.1.3.3" class="ltx_td ltx_align_left ltx_border_r">SM-A300M</td>
</tr>
<tr id="S3.T1.1.1.4" class="ltx_tr">
<td id="S3.T1.1.1.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">screen_trace</td>
<td id="S3.T1.1.1.4.2" class="ltx_td ltx_align_left ltx_border_r">screen on or off</td>
<td id="S3.T1.1.1.4.3" class="ltx_td ltx_align_left ltx_border_r">screen_on</td>
</tr>
<tr id="S3.T1.1.1.5" class="ltx_tr">
<td id="S3.T1.1.1.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">screen_lock_trace</td>
<td id="S3.T1.1.1.5.2" class="ltx_td ltx_align_left ltx_border_r">screen lock or unlock</td>
<td id="S3.T1.1.1.5.3" class="ltx_td ltx_align_left ltx_border_r">screen_lock</td>
</tr>
<tr id="S3.T1.1.1.6" class="ltx_tr">
<td id="S3.T1.1.1.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">time</td>
<td id="S3.T1.1.1.6.2" class="ltx_td ltx_align_left ltx_border_r">time at current state</td>
<td id="S3.T1.1.1.6.3" class="ltx_td ltx_align_left ltx_border_r">2020-01-29 05:52:16</td>
</tr>
<tr id="S3.T1.1.1.7" class="ltx_tr">
<td id="S3.T1.1.1.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">network_trace</td>
<td id="S3.T1.1.1.7.2" class="ltx_td ltx_align_left ltx_border_r">network condition</td>
<td id="S3.T1.1.1.7.3" class="ltx_td ltx_align_left ltx_border_r">2G/3G/4G/5G/WiFi</td>
</tr>
<tr id="S3.T1.1.1.8" class="ltx_tr">
<td id="S3.T1.1.1.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_l ltx_border_r">battery_trace</td>
<td id="S3.T1.1.1.8.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">battery charging state, battery level</td>
<td id="S3.T1.1.1.8.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">battery_charged_off 96.0%</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Example of a state entry.</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2006.06983/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="137" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>A trace is a series of state changes over time.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p6.1" class="ltx_p"><math id="S3.SS2.SSS1.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.SS2.SSS1.p6.1.m1.1a"><mo id="S3.SS2.SSS1.p6.1.m1.1.1" xref="S3.SS2.SSS1.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p6.1.m1.1b"><ci id="S3.SS2.SSS1.p6.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p6.1.m1.1c">\bullet</annotation></semantics></math> <span id="S3.SS2.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Hardware capacity data</span> indicate the computational and communication capacities of different devices.
This dataset, along with the aforementioned state trace, determines how long and whether a device can successfully finish its local training and upload the model updates to the central server before a deadline.</p>
</div>
<div id="S3.SS2.SSS1.p7" class="ltx_para">
<p id="S3.SS2.SSS1.p7.1" class="ltx_p">For the computational capacity, we seek to obtain the training speed for each given device.
However, our collected IMA dataset contains more than one thousand types of devices, making it rather difficult to profile.
Thus, we employ a “clustering” approach by mapping all device models to a small number of representative device models that we afford to offline profile.
The mapping consists of two steps:
(1) The total device models are first mapped to the device models profiled by AI-Benchmark <cite class="ltx_cite ltx_citemacro_citep">(Ignatov et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>, a comprehensive AI performance benchmark.
For a few device models that AI-Benchmark does not cover, we make a random mapping.
It reduces the number of device models to 296.
(2) The remaining device models are then mapped to what we afford to profile.
So far, we have profiled three representative and widely-used device models (Samsung Note 10, Redmi Note 8, and Nexus 6), and we plan to include more device models in the future.
To profile these devices, we run on-device training using the open-source ML library DL4J <cite class="ltx_cite ltx_citemacro_citep">(Eclipse, <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> and record their training time for each ML model used in our experiments.
We are aware of learning-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Xu
et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2019b</a>)</cite> to obtain the on-device ML performance, but our empirical efforts show that these approaches are not precise enough for on-device training tasks.</p>
</div>
<div id="S3.SS2.SSS1.p8" class="ltx_para">
<p id="S3.SS2.SSS1.p8.1" class="ltx_p">For the communication capacity, we recruit 30 volunteers and deploy a testing app on their devices to periodically obtain (i.e., every two hours) the downstream/upstream bandwidth between the devices and a cloud server.
We fit each volunteer’s data to a normal distribution and randomly assign a distribution to the device during the simulation.
The bandwidth data determine the model uploading/downloading time during simulation.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span><span id="S3.SS2.SSS2.1.1" class="ltx_text ltx_font_bold">Benchmark datasets</span>
</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">We use four benchmark datasets to quantitatively study the impacts of heterogeneity on FL performance.
Three of them (i.e., Reddit <cite class="ltx_cite ltx_citemacro_citep">(PushShift.io, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>, Femnist <cite class="ltx_cite ltx_citemacro_citep">(Cohen
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite>, and Celeba <cite class="ltx_cite ltx_citemacro_citep">(Multimedia Laboratory, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>) are synthetic datasets widely adopted in FL literature <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>, <a href="#bib.bib28" title="" class="ltx_ref">a</a>; Nishio and
Yonetani, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Bagdasaryan et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>, while the remaining one is a real-world input corpus collected from our IMA, named as M-Type.
M-Type contains texts input from the devices covered in the state traces in <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mi mathvariant="normal" id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S3.SS2.SSS1" title="3.2.1. IMA dataset ‣ 3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Due to privacy concerns, we do not include M-Type in our GitHub repository.</span></span></span>
Each dataset can be used for an FL task. Specifically, Femnist and Celeba are for image classification tasks, while Reddit and M-Type are for next-word prediction tasks.
For Femnist and Celeba, we use CNN models, and for Reddit and M-Type, we use LSTM models. The four models are implemented by <span id="S3.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Leaf</span> <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>, a popular FL benchmark.
All the datasets are non-IID datasets, i.e., the data distribution is skewed and unbalanced across devices, which is a common data distribution in FL scenarios <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>.
We randomly split the data in each device into a training/testing set (80%/20%).</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span><span id="S3.SS2.SSS3.1.1" class="ltx_text ltx_font_bold">Ethic considerations</span>
</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">All the data are collected with explicit agreements with users on user-term statements and a strict policy in data collection, transmission, and storage.
The IMA users are given an explicit option to opt-out of having their data collected.
In addition, we take very careful steps to protect user privacy and preserve the ethics of research.
First, our work is approved by the Research Ethical Committee of the institutes that the authors are currently affiliated with.
Second, the users’ identifies are all completely anonymized during the study.
Third, the data are stored and processed on a private, HIPPA-compliant cloud server, with strict access authorized by the company that develops the IMA. The whole process is compliant with the privacy policy of the company.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>The Simulation Platform</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2006.06983/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="264" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>We build our FL simulation platform atop the standard FL protocol <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>)</cite>.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Our platform follows the standard FL protocol <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>)</cite> and divides the simulation into three main following phases as shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3. The Simulation Platform ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
We also follow the Google’s report <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2018</a>)</cite> to configure the FL systems
, e.g., the time that the server waits for devices to check-in.
Given an FL task, a global ML model is trained in a synchronized way and advanced round by round.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Selection.</span>
At the beginning of each round, the server waits for tens of seconds for devices to check-in.
Devices that meet the required state criteria check in to the server (①).
Then the server randomly selects a subset (by default 100) of these training-available devices.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Configuration.</span>
The server sends the global model and configuration to each of the selected devices (②).
The configuration is used to instruct the device to train the model.
The device starts to train the model using its local data once the transmission completed (③).
</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Reporting.</span>
The server waits for the participating devices to report updates.
The time that the server waits is configured by the <span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_italic">reporting deadline</span>.
Each device first checks its “reporting qualification” (④), i.e., whether it has dropped out according to its states over the corresponding time period.
It also checks if it has missed the deadline according to the time needed to finish training and communication.
The preceding checking is powered by our IMA dataset described in <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:datasets}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mrow id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.1.1.1" xref="S3.SS3.p4.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3c.cmml"><a href="#S3.SS2.SSS1" title="3.2.1. IMA dataset ‣ 3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">3.2.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><times id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1.1"></times><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">§</ci><ci id="S3.SS3.p4.1.m1.1.1.3c.cmml" xref="S3.SS3.p4.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3"><a href="#S3.SS2.SSS1" title="3.2.1. IMA dataset ‣ 3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">3.2.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\S\ref{subsec:datasets}</annotation></semantics></math>.
The server validates updates based on the checking results and aggregates the qualified updates (⑤).
Devices that fail to report and those that are not selected will wait until the next round (⑥).
This reporting qualification step is what enables heterogeneity-aware FL and distinguishes our platform from existing ones.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Experimental Settings</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_bold">Algorithms</span>.
We briefly introduce the algorithms explored in our study and leave more details and their hyper-parameters in <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi mathvariant="normal" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S4" title="4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The algorithms can be divided into three categories:
(1) The Basic algorithm, i.e., <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_italic">FedAvg</span>, which has been deployed to real systems <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>)</cite> and is widely used in FL literature <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>; Mohri
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>)</cite>.
(2) Aggregation algorithms that determine how to aggregate the weights/gradients uploaded from multiple devices, including <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_italic">q-FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>)</cite> and <span id="S3.SS4.p1.1.4" class="ltx_text ltx_font_italic">FedProx</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite>.
(3) Compression algorithms, including <span id="S3.SS4.p1.1.5" class="ltx_text ltx_font_italic">Structured Updates</span> <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>)</cite>, <span id="S3.SS4.p1.1.6" class="ltx_text ltx_font_italic">Gradient Dropping</span> (<span id="S3.SS4.p1.1.7" class="ltx_text ltx_font_italic">GDrop</span>) <cite class="ltx_cite ltx_citemacro_citep">(Aji and Heafield, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>, and <span id="S3.SS4.p1.1.8" class="ltx_text ltx_font_italic">SignSGD</span> <cite class="ltx_cite ltx_citemacro_citep">(Bernstein et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, which compress local models’ weights/gradients to reduce the communication cost between devices and the central server.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">Metrics</span>.
In our experiments, we quantify the impacts of heterogeneity by reporting the following metrics:
(1) <span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_italic">Convergence accuracy</span>, which is directly related to the performance of an algorithm.
(2) <span id="S3.SS4.p2.1.3" class="ltx_text ltx_font_italic">Training time/round</span>, which is defined as the time/rounds for the global model to converge.
Noting that the training time reported by our simulation platform is the running time after the FL system is deployed in real world instead of the time to run simulation on the pure cloud.
(3) <span id="S3.SS4.p2.1.4" class="ltx_text ltx_font_italic">Compression ratio</span>, which is defined as the fraction of the size of compressed gradients to the original size <cite class="ltx_cite ltx_citemacro_citep">(Wikipedia, <a href="#bib.bib47" title="" class="ltx_ref">2020b</a>)</cite>.
(4) <span id="S3.SS4.p2.1.5" class="ltx_text ltx_font_italic">Variance of accuracy</span>, which is calculated as the standard deviation of accuracy across all the devices in the benchmark dataset. This metric indicates the cross-device fairness of an algorithm.
Table <a href="#S3.T2" title="Table 2 ‣ 3.4. Experimental Settings ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the algorithms and their corresponding metrics that we measure.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.24" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:318.6pt;height:127pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.1pt,11.2pt) scale(0.85,0.85) ;">
<table id="S3.T2.24.24" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.24.24.25" class="ltx_tr">
<td id="S3.T2.24.24.25.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Algorithms</td>
<td id="S3.T2.24.24.25.2" class="ltx_td ltx_align_center ltx_border_tt">Acc.</td>
<td id="S3.T2.24.24.25.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S3.T2.24.24.25.3.1" class="ltx_text"></span> <span id="S3.T2.24.24.25.3.2" class="ltx_text">
<span id="S3.T2.24.24.25.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.24.24.25.3.2.1.1" class="ltx_tr">
<span id="S3.T2.24.24.25.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Training</span></span>
<span id="S3.T2.24.24.25.3.2.1.2" class="ltx_tr">
<span id="S3.T2.24.24.25.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Time/Round</span></span>
</span></span><span id="S3.T2.24.24.25.3.3" class="ltx_text"></span></td>
<td id="S3.T2.24.24.25.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S3.T2.24.24.25.4.1" class="ltx_text"></span> <span id="S3.T2.24.24.25.4.2" class="ltx_text">
<span id="S3.T2.24.24.25.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.24.24.25.4.2.1.1" class="ltx_tr">
<span id="S3.T2.24.24.25.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Compression</span></span>
<span id="S3.T2.24.24.25.4.2.1.2" class="ltx_tr">
<span id="S3.T2.24.24.25.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Ratio</span></span>
</span></span><span id="S3.T2.24.24.25.4.3" class="ltx_text"></span></td>
<td id="S3.T2.24.24.25.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S3.T2.24.24.25.5.1" class="ltx_text"></span> <span id="S3.T2.24.24.25.5.2" class="ltx_text">
<span id="S3.T2.24.24.25.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.24.24.25.5.2.1.1" class="ltx_tr">
<span id="S3.T2.24.24.25.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Var. of Acc.</span></span>
</span></span><span id="S3.T2.24.24.25.5.3" class="ltx_text"></span></td>
</tr>
<tr id="S3.T2.4.4.4" class="ltx_tr">
<td id="S3.T2.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.4.4.4.5.1" class="ltx_text ltx_font_italic">FedAvg</span></td>
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.2.2.2.2.m1.1a"><mi mathvariant="normal" id="S3.T2.2.2.2.2.m1.1.1" xref="S3.T2.2.2.2.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.2.m1.1b"><ci id="S3.T2.2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.3.3.3.3.m1.1a"><mo id="S3.T2.3.3.3.3.m1.1.1" xref="S3.T2.3.3.3.3.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.3.m1.1b"><minus id="S3.T2.3.3.3.3.m1.1.1.cmml" xref="S3.T2.3.3.3.3.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.3.m1.1c">-</annotation></semantics></math></td>
<td id="S3.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.4.4.4.4.m1.1a"><mo id="S3.T2.4.4.4.4.m1.1.1" xref="S3.T2.4.4.4.4.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.4.m1.1b"><minus id="S3.T2.4.4.4.4.m1.1.1.cmml" xref="S3.T2.4.4.4.4.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.4.m1.1c">-</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.8.8.8" class="ltx_tr">
<td id="S3.T2.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.8.8.8.5.1" class="ltx_text ltx_font_italic">Structured Updates</span></td>
<td id="S3.T2.5.5.5.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.5.5.5.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.5.5.5.1.m1.1a"><mi mathvariant="normal" id="S3.T2.5.5.5.1.m1.1.1" xref="S3.T2.5.5.5.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.1.m1.1b"><ci id="S3.T2.5.5.5.1.m1.1.1.cmml" xref="S3.T2.5.5.5.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.6.6.6.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.6.6.6.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.6.6.6.2.m1.1a"><mi mathvariant="normal" id="S3.T2.6.6.6.2.m1.1.1" xref="S3.T2.6.6.6.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.6.2.m1.1b"><ci id="S3.T2.6.6.6.2.m1.1.1.cmml" xref="S3.T2.6.6.6.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.6.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.7.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.7.7.7.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.7.7.7.3.m1.1a"><mi mathvariant="normal" id="S3.T2.7.7.7.3.m1.1.1" xref="S3.T2.7.7.7.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.7.3.m1.1b"><ci id="S3.T2.7.7.7.3.m1.1.1.cmml" xref="S3.T2.7.7.7.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.7.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.8.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.8.8.8.4.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.8.8.8.4.m1.1a"><mo id="S3.T2.8.8.8.4.m1.1.1" xref="S3.T2.8.8.8.4.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.8.4.m1.1b"><minus id="S3.T2.8.8.8.4.m1.1.1.cmml" xref="S3.T2.8.8.8.4.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.8.4.m1.1c">-</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.12.12.12" class="ltx_tr">
<td id="S3.T2.12.12.12.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.12.12.12.5.1" class="ltx_text ltx_font_italic">GDrop</span></td>
<td id="S3.T2.9.9.9.1" class="ltx_td ltx_align_center"><math id="S3.T2.9.9.9.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.9.9.9.1.m1.1a"><mi mathvariant="normal" id="S3.T2.9.9.9.1.m1.1.1" xref="S3.T2.9.9.9.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.9.1.m1.1b"><ci id="S3.T2.9.9.9.1.m1.1.1.cmml" xref="S3.T2.9.9.9.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.9.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.10.10.10.2" class="ltx_td ltx_align_center"><math id="S3.T2.10.10.10.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.10.10.10.2.m1.1a"><mi mathvariant="normal" id="S3.T2.10.10.10.2.m1.1.1" xref="S3.T2.10.10.10.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.10.10.10.2.m1.1b"><ci id="S3.T2.10.10.10.2.m1.1.1.cmml" xref="S3.T2.10.10.10.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.10.10.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.11.11.11.3" class="ltx_td ltx_align_center"><math id="S3.T2.11.11.11.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.11.11.11.3.m1.1a"><mi mathvariant="normal" id="S3.T2.11.11.11.3.m1.1.1" xref="S3.T2.11.11.11.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.11.11.11.3.m1.1b"><ci id="S3.T2.11.11.11.3.m1.1.1.cmml" xref="S3.T2.11.11.11.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.11.11.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.12.12.12.4" class="ltx_td ltx_align_center"><math id="S3.T2.12.12.12.4.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.12.12.12.4.m1.1a"><mo id="S3.T2.12.12.12.4.m1.1.1" xref="S3.T2.12.12.12.4.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.12.12.12.4.m1.1b"><minus id="S3.T2.12.12.12.4.m1.1.1.cmml" xref="S3.T2.12.12.12.4.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.12.12.12.4.m1.1c">-</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.16.16.16" class="ltx_tr">
<td id="S3.T2.16.16.16.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.16.16.16.5.1" class="ltx_text ltx_font_italic">SignSGD</span></td>
<td id="S3.T2.13.13.13.1" class="ltx_td ltx_align_center"><math id="S3.T2.13.13.13.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.13.13.13.1.m1.1a"><mi mathvariant="normal" id="S3.T2.13.13.13.1.m1.1.1" xref="S3.T2.13.13.13.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.13.13.13.1.m1.1b"><ci id="S3.T2.13.13.13.1.m1.1.1.cmml" xref="S3.T2.13.13.13.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.13.13.13.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.14.14.14.2" class="ltx_td ltx_align_center"><math id="S3.T2.14.14.14.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.14.14.14.2.m1.1a"><mi mathvariant="normal" id="S3.T2.14.14.14.2.m1.1.1" xref="S3.T2.14.14.14.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.14.14.14.2.m1.1b"><ci id="S3.T2.14.14.14.2.m1.1.1.cmml" xref="S3.T2.14.14.14.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.14.14.14.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.15.15.15.3" class="ltx_td ltx_align_center"><math id="S3.T2.15.15.15.3.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.15.15.15.3.m1.1a"><mi mathvariant="normal" id="S3.T2.15.15.15.3.m1.1.1" xref="S3.T2.15.15.15.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.15.15.15.3.m1.1b"><ci id="S3.T2.15.15.15.3.m1.1.1.cmml" xref="S3.T2.15.15.15.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.15.15.15.3.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.16.16.16.4" class="ltx_td ltx_align_center"><math id="S3.T2.16.16.16.4.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.16.16.16.4.m1.1a"><mo id="S3.T2.16.16.16.4.m1.1.1" xref="S3.T2.16.16.16.4.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.16.16.16.4.m1.1b"><minus id="S3.T2.16.16.16.4.m1.1.1.cmml" xref="S3.T2.16.16.16.4.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.16.16.16.4.m1.1c">-</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.20.20.20" class="ltx_tr">
<td id="S3.T2.20.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.20.20.20.5.1" class="ltx_text ltx_font_italic">q-FedAvg</span></td>
<td id="S3.T2.17.17.17.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.17.17.17.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.17.17.17.1.m1.1a"><mi mathvariant="normal" id="S3.T2.17.17.17.1.m1.1.1" xref="S3.T2.17.17.17.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.17.17.17.1.m1.1b"><ci id="S3.T2.17.17.17.1.m1.1.1.cmml" xref="S3.T2.17.17.17.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.17.17.17.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.18.18.18.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.18.18.18.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.18.18.18.2.m1.1a"><mi mathvariant="normal" id="S3.T2.18.18.18.2.m1.1.1" xref="S3.T2.18.18.18.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.18.18.18.2.m1.1b"><ci id="S3.T2.18.18.18.2.m1.1.1.cmml" xref="S3.T2.18.18.18.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.18.18.18.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.19.19.19.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.19.19.19.3.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.19.19.19.3.m1.1a"><mo id="S3.T2.19.19.19.3.m1.1.1" xref="S3.T2.19.19.19.3.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.19.19.19.3.m1.1b"><minus id="S3.T2.19.19.19.3.m1.1.1.cmml" xref="S3.T2.19.19.19.3.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.19.19.19.3.m1.1c">-</annotation></semantics></math></td>
<td id="S3.T2.20.20.20.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T2.20.20.20.4.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.20.20.20.4.m1.1a"><mi mathvariant="normal" id="S3.T2.20.20.20.4.m1.1.1" xref="S3.T2.20.20.20.4.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.20.20.20.4.m1.1b"><ci id="S3.T2.20.20.20.4.m1.1.1.cmml" xref="S3.T2.20.20.20.4.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.20.20.20.4.m1.1c">\checkmark</annotation></semantics></math></td>
</tr>
<tr id="S3.T2.24.24.24" class="ltx_tr">
<td id="S3.T2.24.24.24.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T2.24.24.24.5.1" class="ltx_text ltx_font_italic">FedProx</span></td>
<td id="S3.T2.21.21.21.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T2.21.21.21.1.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.21.21.21.1.m1.1a"><mi mathvariant="normal" id="S3.T2.21.21.21.1.m1.1.1" xref="S3.T2.21.21.21.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.21.21.21.1.m1.1b"><ci id="S3.T2.21.21.21.1.m1.1.1.cmml" xref="S3.T2.21.21.21.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.21.21.21.1.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.22.22.22.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T2.22.22.22.2.m1.1" class="ltx_Math" alttext="\checkmark" display="inline"><semantics id="S3.T2.22.22.22.2.m1.1a"><mi mathvariant="normal" id="S3.T2.22.22.22.2.m1.1.1" xref="S3.T2.22.22.22.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S3.T2.22.22.22.2.m1.1b"><ci id="S3.T2.22.22.22.2.m1.1.1.cmml" xref="S3.T2.22.22.22.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.22.22.22.2.m1.1c">\checkmark</annotation></semantics></math></td>
<td id="S3.T2.23.23.23.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T2.23.23.23.3.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.23.23.23.3.m1.1a"><mo id="S3.T2.23.23.23.3.m1.1.1" xref="S3.T2.23.23.23.3.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.23.23.23.3.m1.1b"><minus id="S3.T2.23.23.23.3.m1.1.1.cmml" xref="S3.T2.23.23.23.3.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.23.23.23.3.m1.1c">-</annotation></semantics></math></td>
<td id="S3.T2.24.24.24.4" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T2.24.24.24.4.m1.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S3.T2.24.24.24.4.m1.1a"><mo id="S3.T2.24.24.24.4.m1.1.1" xref="S3.T2.24.24.24.4.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S3.T2.24.24.24.4.m1.1b"><minus id="S3.T2.24.24.24.4.m1.1.1.cmml" xref="S3.T2.24.24.24.4.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.24.24.24.4.m1.1c">-</annotation></semantics></math></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Three categories of FL algorithms we choose and their corresponding metrics we measure.</figcaption>
</figure>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Computing Environment</span>.
All experiments are performed on a high-performance computing cluster with Red Hat Enterprise Linux Server release 7.3 (Maipo).
The cluster has 10 GPU workers.
Each worker is equipped with 2 Intel Xeon E5-2643 V4 processor, 256G main memory, and 2 NVIDIA Tesla P100 graphics cards.
In total, the reported experiments cost more than 5,700 GPU-hours.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Results</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.2" class="ltx_p">In this section, we report the results on how heterogeneity impacts the performance of the basic <span id="S4.p1.2.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm (<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S4.p1.1.m1.1a"><mi mathvariant="normal" id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and advanced FL algorithms proposed by recent FL-related studies (<math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S4.p1.2.m2.1a"><mi mathvariant="normal" id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\S</annotation></semantics></math><a href="#S4.SS2" title="4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Impacts on Basic Algorithm’s Performance</h3>

<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2006.06983/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="438" height="174" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The testing accuracy over time, across different numbers of local training epochs (denoted as <span id="S4.F4.2.1" class="ltx_text ltx_font_italic">E</span>)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F4.3" class="ltx_p ltx_figure_panel ltx_align_center">.</p>
</div>
</div>
</figure>
<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We first measure the impacts of heterogeneity on the performance (in terms of accuracy and training time/rounds) of the basic <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm.
To obtain a more reliable result, we perform the measurement under different numbers of local training epochs, i.e., different numbers of times that the devices use their local data to update the weights of their local models (refer to <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi mathvariant="normal" id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S2" title="2. Background and related work ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The number of local training epoch is an important hyper-parameter of <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">FedAvg</span> used to balance the communication cost between the server and the devices <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite>. We follow previous work <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>)</cite> to set this number (denoted as <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">E</span>) to 1, 5, and 20.
Also, we use the learning rate and the batch size recommended by <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_italic">Leaf</span> <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> for each ML model.
Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates how accuracy changes with training time and training rounds under different numbers of local training epochs.
We summarize our observations and insights as follows.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\bullet</annotation></semantics></math>
<span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Heterogeneity causes non-trivial accuracy drop in FL.</span>
Under heterogeneity-aware settings, the accuracy drops on each dataset across various local training epoch. Specifically, the accuracy drops by an average of 2.3%, 0.5%, and 4% on the existing Femnist, Celeba, and Reddit datasets, respectively. On our M-Type dataset, the accuracy drop is more significant, with an average of 9.2%.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.12" class="ltx_p"><math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\bullet</annotation></semantics></math>
<span id="S4.SS1.p3.12.1" class="ltx_text ltx_font_bold">Heterogeneity obviously slows down the training process of FL in terms of both training time and training rounds.</span>
We first analyze the results in terms of training time. Under each setting of the local training epoch, the training time increases on each dataset when heterogeneity is considered. The increase ranges from 1.15<math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mo id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><times id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\times</annotation></semantics></math> (Reddit with <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="E=1" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mi id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">E</mi><mo id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><eq id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1"></eq><ci id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">𝐸</ci><cn type="integer" id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">E=1</annotation></semantics></math>) to 2.32<math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mo id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><times id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">\times</annotation></semantics></math> (Celeba with <math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="E=20" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mrow id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml"><mi id="S4.SS1.p3.5.m5.1.1.2" xref="S4.SS1.p3.5.m5.1.1.2.cmml">E</mi><mo id="S4.SS1.p3.5.m5.1.1.1" xref="S4.SS1.p3.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.5.m5.1.1.3" xref="S4.SS1.p3.5.m5.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><apply id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1"><eq id="S4.SS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1.1"></eq><ci id="S4.SS1.p3.5.m5.1.1.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2">𝐸</ci><cn type="integer" id="S4.SS1.p3.5.m5.1.1.3.cmml" xref="S4.SS1.p3.5.m5.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">E=20</annotation></semantics></math>), with an average of 1.74<math id="S4.SS1.p3.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p3.6.m6.1a"><mo id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.1b"><times id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.1c">\times</annotation></semantics></math>. In addition, we find that the training time increases more obviously when the number of local training epochs increases. When we set <math id="S4.SS1.p3.7.m7.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS1.p3.7.m7.1a"><mi id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><ci id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">E</annotation></semantics></math> to 20, the training time even increases by around 12 hours on Femnist and Celeba.
We next analyze the results in terms of training rounds. Similar to the training time, training rounds increase on each dataset when heterogeneity is considered. The increase ranges from 1.02<math id="S4.SS1.p3.8.m8.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p3.8.m8.1a"><mo id="S4.SS1.p3.8.m8.1.1" xref="S4.SS1.p3.8.m8.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.8.m8.1b"><times id="S4.SS1.p3.8.m8.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.8.m8.1c">\times</annotation></semantics></math> (M-Type with <math id="S4.SS1.p3.9.m9.1" class="ltx_Math" alttext="E=20" display="inline"><semantics id="S4.SS1.p3.9.m9.1a"><mrow id="S4.SS1.p3.9.m9.1.1" xref="S4.SS1.p3.9.m9.1.1.cmml"><mi id="S4.SS1.p3.9.m9.1.1.2" xref="S4.SS1.p3.9.m9.1.1.2.cmml">E</mi><mo id="S4.SS1.p3.9.m9.1.1.1" xref="S4.SS1.p3.9.m9.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.9.m9.1.1.3" xref="S4.SS1.p3.9.m9.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.9.m9.1b"><apply id="S4.SS1.p3.9.m9.1.1.cmml" xref="S4.SS1.p3.9.m9.1.1"><eq id="S4.SS1.p3.9.m9.1.1.1.cmml" xref="S4.SS1.p3.9.m9.1.1.1"></eq><ci id="S4.SS1.p3.9.m9.1.1.2.cmml" xref="S4.SS1.p3.9.m9.1.1.2">𝐸</ci><cn type="integer" id="S4.SS1.p3.9.m9.1.1.3.cmml" xref="S4.SS1.p3.9.m9.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.9.m9.1c">E=20</annotation></semantics></math>) to 2.64<math id="S4.SS1.p3.10.m10.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p3.10.m10.1a"><mo id="S4.SS1.p3.10.m10.1.1" xref="S4.SS1.p3.10.m10.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.10.m10.1b"><times id="S4.SS1.p3.10.m10.1.1.cmml" xref="S4.SS1.p3.10.m10.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.10.m10.1c">\times</annotation></semantics></math> (Celeba with <math id="S4.SS1.p3.11.m11.1" class="ltx_Math" alttext="E=20" display="inline"><semantics id="S4.SS1.p3.11.m11.1a"><mrow id="S4.SS1.p3.11.m11.1.1" xref="S4.SS1.p3.11.m11.1.1.cmml"><mi id="S4.SS1.p3.11.m11.1.1.2" xref="S4.SS1.p3.11.m11.1.1.2.cmml">E</mi><mo id="S4.SS1.p3.11.m11.1.1.1" xref="S4.SS1.p3.11.m11.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.11.m11.1.1.3" xref="S4.SS1.p3.11.m11.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.11.m11.1b"><apply id="S4.SS1.p3.11.m11.1.1.cmml" xref="S4.SS1.p3.11.m11.1.1"><eq id="S4.SS1.p3.11.m11.1.1.1.cmml" xref="S4.SS1.p3.11.m11.1.1.1"></eq><ci id="S4.SS1.p3.11.m11.1.1.2.cmml" xref="S4.SS1.p3.11.m11.1.1.2">𝐸</ci><cn type="integer" id="S4.SS1.p3.11.m11.1.1.3.cmml" xref="S4.SS1.p3.11.m11.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.11.m11.1c">E=20</annotation></semantics></math>), with an average of 1.42<math id="S4.SS1.p3.12.m12.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p3.12.m12.1a"><mo id="S4.SS1.p3.12.m12.1.1" xref="S4.SS1.p3.12.m12.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.12.m12.1b"><times id="S4.SS1.p3.12.m12.1.1.cmml" xref="S4.SS1.p3.12.m12.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.12.m12.1c">\times</annotation></semantics></math>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:273.5pt;height:110.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-64.4pt,25.9pt) scale(0.68,0.68) ;">
<table id="S4.T3.5.5" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Heter.</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Algo.</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">Worst 10%</span></td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.7.1" class="ltx_text ltx_font_bold">Best 10%</span></td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Var. <math id="S4.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 10^{-4}" display="inline"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.T3.1.1.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.1.cmml">×</mo><msup id="S4.T3.1.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S4.T3.1.1.1.1.1.m1.1.1.3.2" xref="S4.T3.1.1.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.T3.1.1.1.1.1.m1.1.1.3.3" xref="S4.T3.1.1.1.1.1.m1.1.1.3.3.cmml"><mo id="S4.T3.1.1.1.1.1.m1.1.1.3.3a" xref="S4.T3.1.1.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.T3.1.1.1.1.1.m1.1.1.3.3.2" xref="S4.T3.1.1.1.1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S4.T3.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S4.T3.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.T3.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3.2">10</cn><apply id="S4.T3.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3.3"><minus id="S4.T3.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.T3.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">\times 10^{-4}</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T3.5.5.6" class="ltx_tr">
<td id="S4.T3.5.5.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T3.5.5.6.1.1" class="ltx_text">Femnist</span></td>
<td id="S4.T3.5.5.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.5.5.6.2.1" class="ltx_text">Unaware</span></td>
<td id="S4.T3.5.5.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.5.5.6.3.1" class="ltx_text ltx_font_italic">FedAvg</span></td>
<td id="S4.T3.5.5.6.4" class="ltx_td ltx_align_center ltx_border_t">82.13%</td>
<td id="S4.T3.5.5.6.5" class="ltx_td ltx_align_center ltx_border_t">61.1%</td>
<td id="S4.T3.5.5.6.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.5.5.6.6.1" class="ltx_text ltx_font_bold">97.2%</span></td>
<td id="S4.T3.5.5.6.7" class="ltx_td ltx_align_left ltx_border_t">213</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.2.2.2.2.1" class="ltx_text ltx_font_italic">q-FedAvg</span></td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.2.3.1" class="ltx_text ltx_font_bold">82.66%</span></td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.2.4.1" class="ltx_text ltx_font_bold">64.7%</span></td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center">95.1%</td>
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_left"><span id="S4.T3.2.2.2.1.1" class="ltx_text ltx_font_bold">157 (<math id="S4.T3.2.2.2.1.1.m1.1" class="ltx_Math" alttext="26.3\%\downarrow" display="inline"><semantics id="S4.T3.2.2.2.1.1.m1.1a"><mrow id="S4.T3.2.2.2.1.1.m1.1.1" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml"><mrow id="S4.T3.2.2.2.1.1.m1.1.1.2" xref="S4.T3.2.2.2.1.1.m1.1.1.2.cmml"><mn id="S4.T3.2.2.2.1.1.m1.1.1.2.2" xref="S4.T3.2.2.2.1.1.m1.1.1.2.2.cmml">26.3</mn><mo id="S4.T3.2.2.2.1.1.m1.1.1.2.1" xref="S4.T3.2.2.2.1.1.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T3.2.2.2.1.1.m1.1.1.1" xref="S4.T3.2.2.2.1.1.m1.1.1.1.cmml">↓</mo><mi id="S4.T3.2.2.2.1.1.m1.1.1.3" xref="S4.T3.2.2.2.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.1.m1.1b"><apply id="S4.T3.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1"><ci id="S4.T3.2.2.2.1.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.1">↓</ci><apply id="S4.T3.2.2.2.1.1.m1.1.1.2.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T3.2.2.2.1.1.m1.1.1.2.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T3.2.2.2.1.1.m1.1.1.2.2.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.2.2">26.3</cn></apply><csymbol cd="latexml" id="S4.T3.2.2.2.1.1.m1.1.1.3.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.1.m1.1c">26.3\%\downarrow</annotation></semantics></math>)</span></td>
</tr>
<tr id="S4.T3.5.5.7" class="ltx_tr">
<td id="S4.T3.5.5.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.5.5.7.1.1" class="ltx_text">Aware</span></td>
<td id="S4.T3.5.5.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.5.5.7.2.1" class="ltx_text ltx_font_italic">FedAvg</span></td>
<td id="S4.T3.5.5.7.3" class="ltx_td ltx_align_center ltx_border_t">81.22%</td>
<td id="S4.T3.5.5.7.4" class="ltx_td ltx_align_center ltx_border_t">61.1%</td>
<td id="S4.T3.5.5.7.5" class="ltx_td ltx_align_center ltx_border_t">94.9%</td>
<td id="S4.T3.5.5.7.6" class="ltx_td ltx_align_left ltx_border_t">203</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.3.3.3.2.1" class="ltx_text ltx_font_italic">q-FedAvg</span></td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.3.1" class="ltx_text ltx_font_bold">81.24%</span></td>
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.4.1" class="ltx_text ltx_font_bold">64.7%</span></td>
<td id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.5.1" class="ltx_text ltx_font_bold">95.1%</span></td>
<td id="S4.T3.3.3.3.1" class="ltx_td ltx_align_left"><span id="S4.T3.3.3.3.1.1" class="ltx_text ltx_font_bold">159 (<math id="S4.T3.3.3.3.1.1.m1.1" class="ltx_Math" alttext="21.7\%\downarrow" display="inline"><semantics id="S4.T3.3.3.3.1.1.m1.1a"><mrow id="S4.T3.3.3.3.1.1.m1.1.1" xref="S4.T3.3.3.3.1.1.m1.1.1.cmml"><mrow id="S4.T3.3.3.3.1.1.m1.1.1.2" xref="S4.T3.3.3.3.1.1.m1.1.1.2.cmml"><mn id="S4.T3.3.3.3.1.1.m1.1.1.2.2" xref="S4.T3.3.3.3.1.1.m1.1.1.2.2.cmml">21.7</mn><mo id="S4.T3.3.3.3.1.1.m1.1.1.2.1" xref="S4.T3.3.3.3.1.1.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T3.3.3.3.1.1.m1.1.1.1" xref="S4.T3.3.3.3.1.1.m1.1.1.1.cmml">↓</mo><mi id="S4.T3.3.3.3.1.1.m1.1.1.3" xref="S4.T3.3.3.3.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.1.m1.1b"><apply id="S4.T3.3.3.3.1.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1"><ci id="S4.T3.3.3.3.1.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.1">↓</ci><apply id="S4.T3.3.3.3.1.1.m1.1.1.2.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T3.3.3.3.1.1.m1.1.1.2.1.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T3.3.3.3.1.1.m1.1.1.2.2.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.2.2">21.7</cn></apply><csymbol cd="latexml" id="S4.T3.3.3.3.1.1.m1.1.1.3.cmml" xref="S4.T3.3.3.3.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.1.m1.1c">21.7\%\downarrow</annotation></semantics></math>)</span></td>
</tr>
<tr id="S4.T3.5.5.8" class="ltx_tr">
<td id="S4.T3.5.5.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T3.5.5.8.1.1" class="ltx_text">M-Type</span></td>
<td id="S4.T3.5.5.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.5.5.8.2.1" class="ltx_text">Unaware</span></td>
<td id="S4.T3.5.5.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.5.5.8.3.1" class="ltx_text ltx_font_italic">FedAvg</span></td>
<td id="S4.T3.5.5.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.5.5.8.4.1" class="ltx_text ltx_font_bold">8.15%</span></td>
<td id="S4.T3.5.5.8.5" class="ltx_td ltx_align_center ltx_border_t">2.33%</td>
<td id="S4.T3.5.5.8.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.5.5.8.6.1" class="ltx_text ltx_font_bold">13.5%</span></td>
<td id="S4.T3.5.5.8.7" class="ltx_td ltx_align_left ltx_border_t">19</td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.4.4.4.2.1" class="ltx_text ltx_font_italic">q-FedAvg</span></td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center">7.78%</td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.4.4.4.4.1" class="ltx_text ltx_font_bold">2.33%</span></td>
<td id="S4.T3.4.4.4.5" class="ltx_td ltx_align_center">13.0%</td>
<td id="S4.T3.4.4.4.1" class="ltx_td ltx_align_left"><span id="S4.T3.4.4.4.1.1" class="ltx_text ltx_font_bold">17 (<math id="S4.T3.4.4.4.1.1.m1.1" class="ltx_Math" alttext="10.5\%\downarrow" display="inline"><semantics id="S4.T3.4.4.4.1.1.m1.1a"><mrow id="S4.T3.4.4.4.1.1.m1.1.1" xref="S4.T3.4.4.4.1.1.m1.1.1.cmml"><mrow id="S4.T3.4.4.4.1.1.m1.1.1.2" xref="S4.T3.4.4.4.1.1.m1.1.1.2.cmml"><mn id="S4.T3.4.4.4.1.1.m1.1.1.2.2" xref="S4.T3.4.4.4.1.1.m1.1.1.2.2.cmml">10.5</mn><mo id="S4.T3.4.4.4.1.1.m1.1.1.2.1" xref="S4.T3.4.4.4.1.1.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T3.4.4.4.1.1.m1.1.1.1" xref="S4.T3.4.4.4.1.1.m1.1.1.1.cmml">↓</mo><mi id="S4.T3.4.4.4.1.1.m1.1.1.3" xref="S4.T3.4.4.4.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.1.m1.1b"><apply id="S4.T3.4.4.4.1.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1"><ci id="S4.T3.4.4.4.1.1.m1.1.1.1.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.1">↓</ci><apply id="S4.T3.4.4.4.1.1.m1.1.1.2.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T3.4.4.4.1.1.m1.1.1.2.1.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T3.4.4.4.1.1.m1.1.1.2.2.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.2.2">10.5</cn></apply><csymbol cd="latexml" id="S4.T3.4.4.4.1.1.m1.1.1.3.cmml" xref="S4.T3.4.4.4.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.1.m1.1c">10.5\%\downarrow</annotation></semantics></math>)</span></td>
</tr>
<tr id="S4.T3.5.5.9" class="ltx_tr">
<td id="S4.T3.5.5.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T3.5.5.9.1.1" class="ltx_text">Aware</span></td>
<td id="S4.T3.5.5.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.5.5.9.2.1" class="ltx_text ltx_font_italic">FedAvg</span></td>
<td id="S4.T3.5.5.9.3" class="ltx_td ltx_align_center ltx_border_t">7.47%</td>
<td id="S4.T3.5.5.9.4" class="ltx_td ltx_align_center ltx_border_t">2.27%</td>
<td id="S4.T3.5.5.9.5" class="ltx_td ltx_align_center ltx_border_t">12.3%</td>
<td id="S4.T3.5.5.9.6" class="ltx_td ltx_align_left ltx_border_t">16.2</td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<td id="S4.T3.5.5.5.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T3.5.5.5.2.1" class="ltx_text ltx_font_italic">q-FedAvg</span></td>
<td id="S4.T3.5.5.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.5.5.5.3.1" class="ltx_text ltx_font_bold">7.47%</span></td>
<td id="S4.T3.5.5.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.5.5.5.4.1" class="ltx_text ltx_font_bold">2.33%</span></td>
<td id="S4.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.5.5.5.5.1" class="ltx_text ltx_font_bold">12.4%</span></td>
<td id="S4.T3.5.5.5.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.5.5.5.1.1" class="ltx_text ltx_font_bold">15.6 (<math id="S4.T3.5.5.5.1.1.m1.1" class="ltx_Math" alttext="3.7\%\downarrow" display="inline"><semantics id="S4.T3.5.5.5.1.1.m1.1a"><mrow id="S4.T3.5.5.5.1.1.m1.1.1" xref="S4.T3.5.5.5.1.1.m1.1.1.cmml"><mrow id="S4.T3.5.5.5.1.1.m1.1.1.2" xref="S4.T3.5.5.5.1.1.m1.1.1.2.cmml"><mn id="S4.T3.5.5.5.1.1.m1.1.1.2.2" xref="S4.T3.5.5.5.1.1.m1.1.1.2.2.cmml">3.7</mn><mo id="S4.T3.5.5.5.1.1.m1.1.1.2.1" xref="S4.T3.5.5.5.1.1.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T3.5.5.5.1.1.m1.1.1.1" xref="S4.T3.5.5.5.1.1.m1.1.1.1.cmml">↓</mo><mi id="S4.T3.5.5.5.1.1.m1.1.1.3" xref="S4.T3.5.5.5.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.1.m1.1b"><apply id="S4.T3.5.5.5.1.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1"><ci id="S4.T3.5.5.5.1.1.m1.1.1.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.1">↓</ci><apply id="S4.T3.5.5.5.1.1.m1.1.1.2.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T3.5.5.5.1.1.m1.1.1.2.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T3.5.5.5.1.1.m1.1.1.2.2.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.2.2">3.7</cn></apply><csymbol cd="latexml" id="S4.T3.5.5.5.1.1.m1.1.1.3.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.1.m1.1c">3.7\%\downarrow</annotation></semantics></math>)</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Test accuracy for <span id="S4.T3.8.1" class="ltx_text ltx_font_italic">q-FedAvg</span> and <span id="S4.T3.9.2" class="ltx_text ltx_font_italic">FedAvg</span>. “Var” represents the variance of accuracy across devices.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Impacts on Advanced Algorithms’ Performance</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We now measure the impacts of heterogeneity on advanced FL algorithms, e.g., model aggregation and gradient compression.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.64" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:560.8pt;height:158.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.5pt,13.9pt) scale(0.85,0.85) ;">
<table id="S4.T4.64.64" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.64.64.65" class="ltx_tr">
<td id="S4.T4.64.64.65.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.64.64.65.1.1" class="ltx_text"></span> <span id="S4.T4.64.64.65.1.2" class="ltx_text">
<span id="S4.T4.64.64.65.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.64.64.65.1.2.1.1" class="ltx_tr">
<span id="S4.T4.64.64.65.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Dataset</span></span>
</span></span><span id="S4.T4.64.64.65.1.3" class="ltx_text"></span></td>
<td id="S4.T4.64.64.65.2" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S4.T4.64.64.65.2.1" class="ltx_text"></span> <span id="S4.T4.64.64.65.2.2" class="ltx_text">
<span id="S4.T4.64.64.65.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.64.64.65.2.2.1.1" class="ltx_tr">
<span id="S4.T4.64.64.65.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Algo.</span></span>
</span></span><span id="S4.T4.64.64.65.2.3" class="ltx_text"></span></td>
<td id="S4.T4.64.64.65.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.64.64.65.3.1" class="ltx_text"></span> <span id="S4.T4.64.64.65.3.2" class="ltx_text">
<span id="S4.T4.64.64.65.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.64.64.65.3.2.1.1" class="ltx_tr">
<span id="S4.T4.64.64.65.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Acc (%)</span></span>
<span id="S4.T4.64.64.65.3.2.1.2" class="ltx_tr">
<span id="S4.T4.64.64.65.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Heter-unaware</span></span>
</span></span><span id="S4.T4.64.64.65.3.3" class="ltx_text"></span>
</td>
<td id="S4.T4.64.64.65.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.64.64.65.4.1" class="ltx_text"></span> <span id="S4.T4.64.64.65.4.2" class="ltx_text">
<span id="S4.T4.64.64.65.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.64.64.65.4.2.1.1" class="ltx_tr">
<span id="S4.T4.64.64.65.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Acc (%)</span></span>
<span id="S4.T4.64.64.65.4.2.1.2" class="ltx_tr">
<span id="S4.T4.64.64.65.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Heter-aware</span></span>
</span></span><span id="S4.T4.64.64.65.4.3" class="ltx_text"></span>
</td>
<td id="S4.T4.64.64.65.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.64.64.65.5.1" class="ltx_text"></span> <span id="S4.T4.64.64.65.5.2" class="ltx_text">
<span id="S4.T4.64.64.65.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.64.64.65.5.2.1.1" class="ltx_tr">
<span id="S4.T4.64.64.65.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Acc Change</span></span>
<span id="S4.T4.64.64.65.5.2.1.2" class="ltx_tr">
<span id="S4.T4.64.64.65.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(ratio)</span></span>
</span></span><span id="S4.T4.64.64.65.5.3" class="ltx_text"></span>
</td>
<td id="S4.T4.64.64.65.6" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.64.64.65.6.1" class="ltx_text"></span> <span id="S4.T4.64.64.65.6.2" class="ltx_text">
<span id="S4.T4.64.64.65.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.64.64.65.6.2.1.1" class="ltx_tr">
<span id="S4.T4.64.64.65.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Training time</span></span>
<span id="S4.T4.64.64.65.6.2.1.2" class="ltx_tr">
<span id="S4.T4.64.64.65.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Heter-unaware</span></span>
</span></span><span id="S4.T4.64.64.65.6.3" class="ltx_text"></span>
</td>
<td id="S4.T4.64.64.65.7" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.64.64.65.7.1" class="ltx_text"></span> <span id="S4.T4.64.64.65.7.2" class="ltx_text">
<span id="S4.T4.64.64.65.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.64.64.65.7.2.1.1" class="ltx_tr">
<span id="S4.T4.64.64.65.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Training time</span></span>
<span id="S4.T4.64.64.65.7.2.1.2" class="ltx_tr">
<span id="S4.T4.64.64.65.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Heter-aware</span></span>
</span></span><span id="S4.T4.64.64.65.7.3" class="ltx_text"></span>
</td>
<td id="S4.T4.64.64.65.8" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.64.64.65.8.1" class="ltx_text"></span> <span id="S4.T4.64.64.65.8.2" class="ltx_text">
<span id="S4.T4.64.64.65.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.64.64.65.8.2.1.1" class="ltx_tr">
<span id="S4.T4.64.64.65.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Compression</span></span>
<span id="S4.T4.64.64.65.8.2.1.2" class="ltx_tr">
<span id="S4.T4.64.64.65.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Ratio</span></span>
</span></span><span id="S4.T4.64.64.65.8.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S4.T4.8.8.8" class="ltx_tr">
<td id="S4.T4.8.8.8.9" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T4.8.8.8.9.1" class="ltx_text">Femnist</span></td>
<td id="S4.T4.8.8.8.10" class="ltx_td ltx_align_left ltx_border_t">No Compression</td>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_right ltx_border_t">
<math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="84.1" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mn id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">84.1</mn><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><cn type="float" id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">84.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">84.1</annotation></semantics></math> (<math id="S4.T4.2.2.2.2.m2.1" class="ltx_Math" alttext="0.0\%" display="inline"><semantics id="S4.T4.2.2.2.2.m2.1a"><mrow id="S4.T4.2.2.2.2.m2.1.1" xref="S4.T4.2.2.2.2.m2.1.1.cmml"><mn id="S4.T4.2.2.2.2.m2.1.1.2" xref="S4.T4.2.2.2.2.m2.1.1.2.cmml">0.0</mn><mo id="S4.T4.2.2.2.2.m2.1.1.1" xref="S4.T4.2.2.2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m2.1b"><apply id="S4.T4.2.2.2.2.m2.1.1.cmml" xref="S4.T4.2.2.2.2.m2.1.1"><csymbol cd="latexml" id="S4.T4.2.2.2.2.m2.1.1.1.cmml" xref="S4.T4.2.2.2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.T4.2.2.2.2.m2.1.1.2.cmml" xref="S4.T4.2.2.2.2.m2.1.1.2">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m2.1c">0.0\%</annotation></semantics></math>)</td>
<td id="S4.T4.4.4.4.4" class="ltx_td ltx_align_right ltx_border_t">
<math id="S4.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="83.0" display="inline"><semantics id="S4.T4.3.3.3.3.m1.1a"><mn id="S4.T4.3.3.3.3.m1.1.1" xref="S4.T4.3.3.3.3.m1.1.1.cmml">83.0</mn><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.3.m1.1b"><cn type="float" id="S4.T4.3.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.3.m1.1.1">83.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.3.m1.1c">83.0</annotation></semantics></math> (<math id="S4.T4.4.4.4.4.m2.1" class="ltx_Math" alttext="0.0\%" display="inline"><semantics id="S4.T4.4.4.4.4.m2.1a"><mrow id="S4.T4.4.4.4.4.m2.1.1" xref="S4.T4.4.4.4.4.m2.1.1.cmml"><mn id="S4.T4.4.4.4.4.m2.1.1.2" xref="S4.T4.4.4.4.4.m2.1.1.2.cmml">0.0</mn><mo id="S4.T4.4.4.4.4.m2.1.1.1" xref="S4.T4.4.4.4.4.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.4.m2.1b"><apply id="S4.T4.4.4.4.4.m2.1.1.cmml" xref="S4.T4.4.4.4.4.m2.1.1"><csymbol cd="latexml" id="S4.T4.4.4.4.4.m2.1.1.1.cmml" xref="S4.T4.4.4.4.4.m2.1.1.1">percent</csymbol><cn type="float" id="S4.T4.4.4.4.4.m2.1.1.2.cmml" xref="S4.T4.4.4.4.4.m2.1.1.2">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.4.m2.1c">0.0\%</annotation></semantics></math>)</td>
<td id="S4.T4.5.5.5.5" class="ltx_td ltx_align_right ltx_border_t"><math id="S4.T4.5.5.5.5.m1.1" class="ltx_Math" alttext="1.2\%\downarrow" display="inline"><semantics id="S4.T4.5.5.5.5.m1.1a"><mrow id="S4.T4.5.5.5.5.m1.1.1" xref="S4.T4.5.5.5.5.m1.1.1.cmml"><mrow id="S4.T4.5.5.5.5.m1.1.1.2" xref="S4.T4.5.5.5.5.m1.1.1.2.cmml"><mn id="S4.T4.5.5.5.5.m1.1.1.2.2" xref="S4.T4.5.5.5.5.m1.1.1.2.2.cmml">1.2</mn><mo id="S4.T4.5.5.5.5.m1.1.1.2.1" xref="S4.T4.5.5.5.5.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.5.5.5.5.m1.1.1.1" xref="S4.T4.5.5.5.5.m1.1.1.1.cmml">↓</mo><mi id="S4.T4.5.5.5.5.m1.1.1.3" xref="S4.T4.5.5.5.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.5.m1.1b"><apply id="S4.T4.5.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.5.m1.1.1"><ci id="S4.T4.5.5.5.5.m1.1.1.1.cmml" xref="S4.T4.5.5.5.5.m1.1.1.1">↓</ci><apply id="S4.T4.5.5.5.5.m1.1.1.2.cmml" xref="S4.T4.5.5.5.5.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.5.5.5.5.m1.1.1.2.1.cmml" xref="S4.T4.5.5.5.5.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.5.5.5.5.m1.1.1.2.2.cmml" xref="S4.T4.5.5.5.5.m1.1.1.2.2">1.2</cn></apply><csymbol cd="latexml" id="S4.T4.5.5.5.5.m1.1.1.3.cmml" xref="S4.T4.5.5.5.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.5.m1.1c">1.2\%\downarrow</annotation></semantics></math></td>
<td id="S4.T4.6.6.6.6" class="ltx_td ltx_align_right ltx_border_t">5.56 hours (1.0<math id="S4.T4.6.6.6.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.6.6.6.6.m1.1a"><mo id="S4.T4.6.6.6.6.m1.1.1" xref="S4.T4.6.6.6.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.6.m1.1b"><times id="S4.T4.6.6.6.6.m1.1.1.cmml" xref="S4.T4.6.6.6.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.6.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.7.7.7.7" class="ltx_td ltx_align_right ltx_border_t">5.96 hours (1.0<math id="S4.T4.7.7.7.7.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.7.7.7.7.m1.1a"><mo id="S4.T4.7.7.7.7.m1.1.1" xref="S4.T4.7.7.7.7.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.7.m1.1b"><times id="S4.T4.7.7.7.7.m1.1.1.cmml" xref="S4.T4.7.7.7.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.7.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.8.8.8.8" class="ltx_td ltx_align_right ltx_border_t"><math id="S4.T4.8.8.8.8.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S4.T4.8.8.8.8.m1.1a"><mrow id="S4.T4.8.8.8.8.m1.1.1" xref="S4.T4.8.8.8.8.m1.1.1.cmml"><mn id="S4.T4.8.8.8.8.m1.1.1.2" xref="S4.T4.8.8.8.8.m1.1.1.2.cmml">100</mn><mo id="S4.T4.8.8.8.8.m1.1.1.1" xref="S4.T4.8.8.8.8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.8.m1.1b"><apply id="S4.T4.8.8.8.8.m1.1.1.cmml" xref="S4.T4.8.8.8.8.m1.1.1"><csymbol cd="latexml" id="S4.T4.8.8.8.8.m1.1.1.1.cmml" xref="S4.T4.8.8.8.8.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.T4.8.8.8.8.m1.1.1.2.cmml" xref="S4.T4.8.8.8.8.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.8.m1.1c">100\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.16.16.16" class="ltx_tr">
<td id="S4.T4.16.16.16.9" class="ltx_td ltx_align_left">Structured Updates</td>
<td id="S4.T4.10.10.10.2" class="ltx_td ltx_align_right">
<span id="S4.T4.10.10.10.2.1" class="ltx_text ltx_markedasmath ltx_font_bold">84.2</span> (<math id="S4.T4.10.10.10.2.m2.1" class="ltx_Math" alttext="0.1\%\uparrow" display="inline"><semantics id="S4.T4.10.10.10.2.m2.1a"><mrow id="S4.T4.10.10.10.2.m2.1.1" xref="S4.T4.10.10.10.2.m2.1.1.cmml"><mrow id="S4.T4.10.10.10.2.m2.1.1.2" xref="S4.T4.10.10.10.2.m2.1.1.2.cmml"><mn id="S4.T4.10.10.10.2.m2.1.1.2.2" xref="S4.T4.10.10.10.2.m2.1.1.2.2.cmml">0.1</mn><mo id="S4.T4.10.10.10.2.m2.1.1.2.1" xref="S4.T4.10.10.10.2.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.10.10.10.2.m2.1.1.1" xref="S4.T4.10.10.10.2.m2.1.1.1.cmml">↑</mo><mi id="S4.T4.10.10.10.2.m2.1.1.3" xref="S4.T4.10.10.10.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.2.m2.1b"><apply id="S4.T4.10.10.10.2.m2.1.1.cmml" xref="S4.T4.10.10.10.2.m2.1.1"><ci id="S4.T4.10.10.10.2.m2.1.1.1.cmml" xref="S4.T4.10.10.10.2.m2.1.1.1">↑</ci><apply id="S4.T4.10.10.10.2.m2.1.1.2.cmml" xref="S4.T4.10.10.10.2.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.10.10.10.2.m2.1.1.2.1.cmml" xref="S4.T4.10.10.10.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.10.10.10.2.m2.1.1.2.2.cmml" xref="S4.T4.10.10.10.2.m2.1.1.2.2">0.1</cn></apply><csymbol cd="latexml" id="S4.T4.10.10.10.2.m2.1.1.3.cmml" xref="S4.T4.10.10.10.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.2.m2.1c">0.1\%\uparrow</annotation></semantics></math>)</td>
<td id="S4.T4.12.12.12.4" class="ltx_td ltx_align_right">
<span id="S4.T4.12.12.12.4.1" class="ltx_text ltx_markedasmath ltx_font_bold">83.2</span> (<math id="S4.T4.12.12.12.4.m2.1" class="ltx_Math" alttext="0.3\%\uparrow" display="inline"><semantics id="S4.T4.12.12.12.4.m2.1a"><mrow id="S4.T4.12.12.12.4.m2.1.1" xref="S4.T4.12.12.12.4.m2.1.1.cmml"><mrow id="S4.T4.12.12.12.4.m2.1.1.2" xref="S4.T4.12.12.12.4.m2.1.1.2.cmml"><mn id="S4.T4.12.12.12.4.m2.1.1.2.2" xref="S4.T4.12.12.12.4.m2.1.1.2.2.cmml">0.3</mn><mo id="S4.T4.12.12.12.4.m2.1.1.2.1" xref="S4.T4.12.12.12.4.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.12.12.12.4.m2.1.1.1" xref="S4.T4.12.12.12.4.m2.1.1.1.cmml">↑</mo><mi id="S4.T4.12.12.12.4.m2.1.1.3" xref="S4.T4.12.12.12.4.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.4.m2.1b"><apply id="S4.T4.12.12.12.4.m2.1.1.cmml" xref="S4.T4.12.12.12.4.m2.1.1"><ci id="S4.T4.12.12.12.4.m2.1.1.1.cmml" xref="S4.T4.12.12.12.4.m2.1.1.1">↑</ci><apply id="S4.T4.12.12.12.4.m2.1.1.2.cmml" xref="S4.T4.12.12.12.4.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.12.12.12.4.m2.1.1.2.1.cmml" xref="S4.T4.12.12.12.4.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.12.12.12.4.m2.1.1.2.2.cmml" xref="S4.T4.12.12.12.4.m2.1.1.2.2">0.3</cn></apply><csymbol cd="latexml" id="S4.T4.12.12.12.4.m2.1.1.3.cmml" xref="S4.T4.12.12.12.4.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.4.m2.1c">0.3\%\uparrow</annotation></semantics></math>)</td>
<td id="S4.T4.13.13.13.5" class="ltx_td ltx_align_right"><math id="S4.T4.13.13.13.5.m1.1" class="ltx_Math" alttext="1.1\%\downarrow" display="inline"><semantics id="S4.T4.13.13.13.5.m1.1a"><mrow id="S4.T4.13.13.13.5.m1.1.1" xref="S4.T4.13.13.13.5.m1.1.1.cmml"><mrow id="S4.T4.13.13.13.5.m1.1.1.2" xref="S4.T4.13.13.13.5.m1.1.1.2.cmml"><mn id="S4.T4.13.13.13.5.m1.1.1.2.2" xref="S4.T4.13.13.13.5.m1.1.1.2.2.cmml">1.1</mn><mo id="S4.T4.13.13.13.5.m1.1.1.2.1" xref="S4.T4.13.13.13.5.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.13.13.13.5.m1.1.1.1" xref="S4.T4.13.13.13.5.m1.1.1.1.cmml">↓</mo><mi id="S4.T4.13.13.13.5.m1.1.1.3" xref="S4.T4.13.13.13.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.5.m1.1b"><apply id="S4.T4.13.13.13.5.m1.1.1.cmml" xref="S4.T4.13.13.13.5.m1.1.1"><ci id="S4.T4.13.13.13.5.m1.1.1.1.cmml" xref="S4.T4.13.13.13.5.m1.1.1.1">↓</ci><apply id="S4.T4.13.13.13.5.m1.1.1.2.cmml" xref="S4.T4.13.13.13.5.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.13.13.13.5.m1.1.1.2.1.cmml" xref="S4.T4.13.13.13.5.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.13.13.13.5.m1.1.1.2.2.cmml" xref="S4.T4.13.13.13.5.m1.1.1.2.2">1.1</cn></apply><csymbol cd="latexml" id="S4.T4.13.13.13.5.m1.1.1.3.cmml" xref="S4.T4.13.13.13.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.5.m1.1c">1.1\%\downarrow</annotation></semantics></math></td>
<td id="S4.T4.14.14.14.6" class="ltx_td ltx_align_right"><span id="S4.T4.14.14.14.6.1" class="ltx_text ltx_font_bold">5.23 hours (0.95<math id="S4.T4.14.14.14.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.14.14.14.6.1.m1.1a"><mo id="S4.T4.14.14.14.6.1.m1.1.1" xref="S4.T4.14.14.14.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.6.1.m1.1b"><times id="S4.T4.14.14.14.6.1.m1.1.1.cmml" xref="S4.T4.14.14.14.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.6.1.m1.1c">\times</annotation></semantics></math>)</span></td>
<td id="S4.T4.15.15.15.7" class="ltx_td ltx_align_right"><span id="S4.T4.15.15.15.7.1" class="ltx_text ltx_font_bold">5.56 hours (0.93<math id="S4.T4.15.15.15.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.15.15.15.7.1.m1.1a"><mo id="S4.T4.15.15.15.7.1.m1.1.1" xref="S4.T4.15.15.15.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.15.7.1.m1.1b"><times id="S4.T4.15.15.15.7.1.m1.1.1.cmml" xref="S4.T4.15.15.15.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.15.7.1.m1.1c">\times</annotation></semantics></math>)</span></td>
<td id="S4.T4.16.16.16.8" class="ltx_td ltx_align_right"><math id="S4.T4.16.16.16.8.m1.1" class="ltx_Math" alttext="6.7\%" display="inline"><semantics id="S4.T4.16.16.16.8.m1.1a"><mrow id="S4.T4.16.16.16.8.m1.1.1" xref="S4.T4.16.16.16.8.m1.1.1.cmml"><mn id="S4.T4.16.16.16.8.m1.1.1.2" xref="S4.T4.16.16.16.8.m1.1.1.2.cmml">6.7</mn><mo id="S4.T4.16.16.16.8.m1.1.1.1" xref="S4.T4.16.16.16.8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.16.8.m1.1b"><apply id="S4.T4.16.16.16.8.m1.1.1.cmml" xref="S4.T4.16.16.16.8.m1.1.1"><csymbol cd="latexml" id="S4.T4.16.16.16.8.m1.1.1.1.cmml" xref="S4.T4.16.16.16.8.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T4.16.16.16.8.m1.1.1.2.cmml" xref="S4.T4.16.16.16.8.m1.1.1.2">6.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.16.8.m1.1c">6.7\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.24.24.24" class="ltx_tr">
<td id="S4.T4.24.24.24.9" class="ltx_td ltx_align_left">GDrop</td>
<td id="S4.T4.18.18.18.2" class="ltx_td ltx_align_right">
<math id="S4.T4.17.17.17.1.m1.1" class="ltx_Math" alttext="82.2" display="inline"><semantics id="S4.T4.17.17.17.1.m1.1a"><mn id="S4.T4.17.17.17.1.m1.1.1" xref="S4.T4.17.17.17.1.m1.1.1.cmml">82.2</mn><annotation-xml encoding="MathML-Content" id="S4.T4.17.17.17.1.m1.1b"><cn type="float" id="S4.T4.17.17.17.1.m1.1.1.cmml" xref="S4.T4.17.17.17.1.m1.1.1">82.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.17.17.17.1.m1.1c">82.2</annotation></semantics></math> (<math id="S4.T4.18.18.18.2.m2.1" class="ltx_Math" alttext="2.2\%\downarrow" display="inline"><semantics id="S4.T4.18.18.18.2.m2.1a"><mrow id="S4.T4.18.18.18.2.m2.1.1" xref="S4.T4.18.18.18.2.m2.1.1.cmml"><mrow id="S4.T4.18.18.18.2.m2.1.1.2" xref="S4.T4.18.18.18.2.m2.1.1.2.cmml"><mn id="S4.T4.18.18.18.2.m2.1.1.2.2" xref="S4.T4.18.18.18.2.m2.1.1.2.2.cmml">2.2</mn><mo id="S4.T4.18.18.18.2.m2.1.1.2.1" xref="S4.T4.18.18.18.2.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.18.18.18.2.m2.1.1.1" xref="S4.T4.18.18.18.2.m2.1.1.1.cmml">↓</mo><mi id="S4.T4.18.18.18.2.m2.1.1.3" xref="S4.T4.18.18.18.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.18.18.18.2.m2.1b"><apply id="S4.T4.18.18.18.2.m2.1.1.cmml" xref="S4.T4.18.18.18.2.m2.1.1"><ci id="S4.T4.18.18.18.2.m2.1.1.1.cmml" xref="S4.T4.18.18.18.2.m2.1.1.1">↓</ci><apply id="S4.T4.18.18.18.2.m2.1.1.2.cmml" xref="S4.T4.18.18.18.2.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.18.18.18.2.m2.1.1.2.1.cmml" xref="S4.T4.18.18.18.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.18.18.18.2.m2.1.1.2.2.cmml" xref="S4.T4.18.18.18.2.m2.1.1.2.2">2.2</cn></apply><csymbol cd="latexml" id="S4.T4.18.18.18.2.m2.1.1.3.cmml" xref="S4.T4.18.18.18.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.18.18.18.2.m2.1c">2.2\%\downarrow</annotation></semantics></math>)</td>
<td id="S4.T4.20.20.20.4" class="ltx_td ltx_align_right">
<math id="S4.T4.19.19.19.3.m1.1" class="ltx_Math" alttext="81.5" display="inline"><semantics id="S4.T4.19.19.19.3.m1.1a"><mn id="S4.T4.19.19.19.3.m1.1.1" xref="S4.T4.19.19.19.3.m1.1.1.cmml">81.5</mn><annotation-xml encoding="MathML-Content" id="S4.T4.19.19.19.3.m1.1b"><cn type="float" id="S4.T4.19.19.19.3.m1.1.1.cmml" xref="S4.T4.19.19.19.3.m1.1.1">81.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.19.19.19.3.m1.1c">81.5</annotation></semantics></math> (<math id="S4.T4.20.20.20.4.m2.1" class="ltx_Math" alttext="1.8\%\downarrow" display="inline"><semantics id="S4.T4.20.20.20.4.m2.1a"><mrow id="S4.T4.20.20.20.4.m2.1.1" xref="S4.T4.20.20.20.4.m2.1.1.cmml"><mrow id="S4.T4.20.20.20.4.m2.1.1.2" xref="S4.T4.20.20.20.4.m2.1.1.2.cmml"><mn id="S4.T4.20.20.20.4.m2.1.1.2.2" xref="S4.T4.20.20.20.4.m2.1.1.2.2.cmml">1.8</mn><mo id="S4.T4.20.20.20.4.m2.1.1.2.1" xref="S4.T4.20.20.20.4.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.20.20.20.4.m2.1.1.1" xref="S4.T4.20.20.20.4.m2.1.1.1.cmml">↓</mo><mi id="S4.T4.20.20.20.4.m2.1.1.3" xref="S4.T4.20.20.20.4.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.20.20.20.4.m2.1b"><apply id="S4.T4.20.20.20.4.m2.1.1.cmml" xref="S4.T4.20.20.20.4.m2.1.1"><ci id="S4.T4.20.20.20.4.m2.1.1.1.cmml" xref="S4.T4.20.20.20.4.m2.1.1.1">↓</ci><apply id="S4.T4.20.20.20.4.m2.1.1.2.cmml" xref="S4.T4.20.20.20.4.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.20.20.20.4.m2.1.1.2.1.cmml" xref="S4.T4.20.20.20.4.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.20.20.20.4.m2.1.1.2.2.cmml" xref="S4.T4.20.20.20.4.m2.1.1.2.2">1.8</cn></apply><csymbol cd="latexml" id="S4.T4.20.20.20.4.m2.1.1.3.cmml" xref="S4.T4.20.20.20.4.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.20.20.20.4.m2.1c">1.8\%\downarrow</annotation></semantics></math>)</td>
<td id="S4.T4.21.21.21.5" class="ltx_td ltx_align_right"><math id="S4.T4.21.21.21.5.m1.1" class="ltx_Math" alttext="0.8\%\downarrow" display="inline"><semantics id="S4.T4.21.21.21.5.m1.1a"><mrow id="S4.T4.21.21.21.5.m1.1.1" xref="S4.T4.21.21.21.5.m1.1.1.cmml"><mrow id="S4.T4.21.21.21.5.m1.1.1.2" xref="S4.T4.21.21.21.5.m1.1.1.2.cmml"><mn id="S4.T4.21.21.21.5.m1.1.1.2.2" xref="S4.T4.21.21.21.5.m1.1.1.2.2.cmml">0.8</mn><mo id="S4.T4.21.21.21.5.m1.1.1.2.1" xref="S4.T4.21.21.21.5.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.21.21.21.5.m1.1.1.1" xref="S4.T4.21.21.21.5.m1.1.1.1.cmml">↓</mo><mi id="S4.T4.21.21.21.5.m1.1.1.3" xref="S4.T4.21.21.21.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.21.21.21.5.m1.1b"><apply id="S4.T4.21.21.21.5.m1.1.1.cmml" xref="S4.T4.21.21.21.5.m1.1.1"><ci id="S4.T4.21.21.21.5.m1.1.1.1.cmml" xref="S4.T4.21.21.21.5.m1.1.1.1">↓</ci><apply id="S4.T4.21.21.21.5.m1.1.1.2.cmml" xref="S4.T4.21.21.21.5.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.21.21.21.5.m1.1.1.2.1.cmml" xref="S4.T4.21.21.21.5.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.21.21.21.5.m1.1.1.2.2.cmml" xref="S4.T4.21.21.21.5.m1.1.1.2.2">0.8</cn></apply><csymbol cd="latexml" id="S4.T4.21.21.21.5.m1.1.1.3.cmml" xref="S4.T4.21.21.21.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.21.21.21.5.m1.1c">0.8\%\downarrow</annotation></semantics></math></td>
<td id="S4.T4.22.22.22.6" class="ltx_td ltx_align_right">7.17 hours (1.3<math id="S4.T4.22.22.22.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.22.22.22.6.m1.1a"><mo id="S4.T4.22.22.22.6.m1.1.1" xref="S4.T4.22.22.22.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.22.22.22.6.m1.1b"><times id="S4.T4.22.22.22.6.m1.1.1.cmml" xref="S4.T4.22.22.22.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.22.22.22.6.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.23.23.23.7" class="ltx_td ltx_align_right">7.98 hours (1.3<math id="S4.T4.23.23.23.7.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.23.23.23.7.m1.1a"><mo id="S4.T4.23.23.23.7.m1.1.1" xref="S4.T4.23.23.23.7.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.23.23.23.7.m1.1b"><times id="S4.T4.23.23.23.7.m1.1.1.cmml" xref="S4.T4.23.23.23.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.23.23.23.7.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.24.24.24.8" class="ltx_td ltx_align_right"><math id="S4.T4.24.24.24.8.m1.1" class="ltx_Math" alttext="21.4\%\sim 28.2\%" display="inline"><semantics id="S4.T4.24.24.24.8.m1.1a"><mrow id="S4.T4.24.24.24.8.m1.1.1" xref="S4.T4.24.24.24.8.m1.1.1.cmml"><mrow id="S4.T4.24.24.24.8.m1.1.1.2" xref="S4.T4.24.24.24.8.m1.1.1.2.cmml"><mn id="S4.T4.24.24.24.8.m1.1.1.2.2" xref="S4.T4.24.24.24.8.m1.1.1.2.2.cmml">21.4</mn><mo id="S4.T4.24.24.24.8.m1.1.1.2.1" xref="S4.T4.24.24.24.8.m1.1.1.2.1.cmml">%</mo></mrow><mo id="S4.T4.24.24.24.8.m1.1.1.1" xref="S4.T4.24.24.24.8.m1.1.1.1.cmml">∼</mo><mrow id="S4.T4.24.24.24.8.m1.1.1.3" xref="S4.T4.24.24.24.8.m1.1.1.3.cmml"><mn id="S4.T4.24.24.24.8.m1.1.1.3.2" xref="S4.T4.24.24.24.8.m1.1.1.3.2.cmml">28.2</mn><mo id="S4.T4.24.24.24.8.m1.1.1.3.1" xref="S4.T4.24.24.24.8.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.24.24.24.8.m1.1b"><apply id="S4.T4.24.24.24.8.m1.1.1.cmml" xref="S4.T4.24.24.24.8.m1.1.1"><csymbol cd="latexml" id="S4.T4.24.24.24.8.m1.1.1.1.cmml" xref="S4.T4.24.24.24.8.m1.1.1.1">similar-to</csymbol><apply id="S4.T4.24.24.24.8.m1.1.1.2.cmml" xref="S4.T4.24.24.24.8.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.24.24.24.8.m1.1.1.2.1.cmml" xref="S4.T4.24.24.24.8.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.24.24.24.8.m1.1.1.2.2.cmml" xref="S4.T4.24.24.24.8.m1.1.1.2.2">21.4</cn></apply><apply id="S4.T4.24.24.24.8.m1.1.1.3.cmml" xref="S4.T4.24.24.24.8.m1.1.1.3"><csymbol cd="latexml" id="S4.T4.24.24.24.8.m1.1.1.3.1.cmml" xref="S4.T4.24.24.24.8.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T4.24.24.24.8.m1.1.1.3.2.cmml" xref="S4.T4.24.24.24.8.m1.1.1.3.2">28.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.24.24.24.8.m1.1c">21.4\%\sim 28.2\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.32.32.32" class="ltx_tr">
<td id="S4.T4.32.32.32.9" class="ltx_td ltx_align_left">SignSGD</td>
<td id="S4.T4.26.26.26.2" class="ltx_td ltx_align_right">
<math id="S4.T4.25.25.25.1.m1.1" class="ltx_Math" alttext="79.0" display="inline"><semantics id="S4.T4.25.25.25.1.m1.1a"><mn id="S4.T4.25.25.25.1.m1.1.1" xref="S4.T4.25.25.25.1.m1.1.1.cmml">79.0</mn><annotation-xml encoding="MathML-Content" id="S4.T4.25.25.25.1.m1.1b"><cn type="float" id="S4.T4.25.25.25.1.m1.1.1.cmml" xref="S4.T4.25.25.25.1.m1.1.1">79.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.25.25.25.1.m1.1c">79.0</annotation></semantics></math> (<math id="S4.T4.26.26.26.2.m2.1" class="ltx_Math" alttext="6.1\%\downarrow" display="inline"><semantics id="S4.T4.26.26.26.2.m2.1a"><mrow id="S4.T4.26.26.26.2.m2.1.1" xref="S4.T4.26.26.26.2.m2.1.1.cmml"><mrow id="S4.T4.26.26.26.2.m2.1.1.2" xref="S4.T4.26.26.26.2.m2.1.1.2.cmml"><mn id="S4.T4.26.26.26.2.m2.1.1.2.2" xref="S4.T4.26.26.26.2.m2.1.1.2.2.cmml">6.1</mn><mo id="S4.T4.26.26.26.2.m2.1.1.2.1" xref="S4.T4.26.26.26.2.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.26.26.26.2.m2.1.1.1" xref="S4.T4.26.26.26.2.m2.1.1.1.cmml">↓</mo><mi id="S4.T4.26.26.26.2.m2.1.1.3" xref="S4.T4.26.26.26.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.26.26.26.2.m2.1b"><apply id="S4.T4.26.26.26.2.m2.1.1.cmml" xref="S4.T4.26.26.26.2.m2.1.1"><ci id="S4.T4.26.26.26.2.m2.1.1.1.cmml" xref="S4.T4.26.26.26.2.m2.1.1.1">↓</ci><apply id="S4.T4.26.26.26.2.m2.1.1.2.cmml" xref="S4.T4.26.26.26.2.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.26.26.26.2.m2.1.1.2.1.cmml" xref="S4.T4.26.26.26.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.26.26.26.2.m2.1.1.2.2.cmml" xref="S4.T4.26.26.26.2.m2.1.1.2.2">6.1</cn></apply><csymbol cd="latexml" id="S4.T4.26.26.26.2.m2.1.1.3.cmml" xref="S4.T4.26.26.26.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.26.26.26.2.m2.1c">6.1\%\downarrow</annotation></semantics></math>)</td>
<td id="S4.T4.28.28.28.4" class="ltx_td ltx_align_right">
<math id="S4.T4.27.27.27.3.m1.1" class="ltx_Math" alttext="76.3" display="inline"><semantics id="S4.T4.27.27.27.3.m1.1a"><mn id="S4.T4.27.27.27.3.m1.1.1" xref="S4.T4.27.27.27.3.m1.1.1.cmml">76.3</mn><annotation-xml encoding="MathML-Content" id="S4.T4.27.27.27.3.m1.1b"><cn type="float" id="S4.T4.27.27.27.3.m1.1.1.cmml" xref="S4.T4.27.27.27.3.m1.1.1">76.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.27.27.27.3.m1.1c">76.3</annotation></semantics></math> (<math id="S4.T4.28.28.28.4.m2.1" class="ltx_Math" alttext="8.1\%\downarrow" display="inline"><semantics id="S4.T4.28.28.28.4.m2.1a"><mrow id="S4.T4.28.28.28.4.m2.1.1" xref="S4.T4.28.28.28.4.m2.1.1.cmml"><mrow id="S4.T4.28.28.28.4.m2.1.1.2" xref="S4.T4.28.28.28.4.m2.1.1.2.cmml"><mn id="S4.T4.28.28.28.4.m2.1.1.2.2" xref="S4.T4.28.28.28.4.m2.1.1.2.2.cmml">8.1</mn><mo id="S4.T4.28.28.28.4.m2.1.1.2.1" xref="S4.T4.28.28.28.4.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.28.28.28.4.m2.1.1.1" xref="S4.T4.28.28.28.4.m2.1.1.1.cmml">↓</mo><mi id="S4.T4.28.28.28.4.m2.1.1.3" xref="S4.T4.28.28.28.4.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.28.28.28.4.m2.1b"><apply id="S4.T4.28.28.28.4.m2.1.1.cmml" xref="S4.T4.28.28.28.4.m2.1.1"><ci id="S4.T4.28.28.28.4.m2.1.1.1.cmml" xref="S4.T4.28.28.28.4.m2.1.1.1">↓</ci><apply id="S4.T4.28.28.28.4.m2.1.1.2.cmml" xref="S4.T4.28.28.28.4.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.28.28.28.4.m2.1.1.2.1.cmml" xref="S4.T4.28.28.28.4.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.28.28.28.4.m2.1.1.2.2.cmml" xref="S4.T4.28.28.28.4.m2.1.1.2.2">8.1</cn></apply><csymbol cd="latexml" id="S4.T4.28.28.28.4.m2.1.1.3.cmml" xref="S4.T4.28.28.28.4.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.28.28.28.4.m2.1c">8.1\%\downarrow</annotation></semantics></math>)</td>
<td id="S4.T4.29.29.29.5" class="ltx_td ltx_align_right"><math id="S4.T4.29.29.29.5.m1.1" class="ltx_Math" alttext="3.4\%\downarrow" display="inline"><semantics id="S4.T4.29.29.29.5.m1.1a"><mrow id="S4.T4.29.29.29.5.m1.1.1" xref="S4.T4.29.29.29.5.m1.1.1.cmml"><mrow id="S4.T4.29.29.29.5.m1.1.1.2" xref="S4.T4.29.29.29.5.m1.1.1.2.cmml"><mn id="S4.T4.29.29.29.5.m1.1.1.2.2" xref="S4.T4.29.29.29.5.m1.1.1.2.2.cmml">3.4</mn><mo id="S4.T4.29.29.29.5.m1.1.1.2.1" xref="S4.T4.29.29.29.5.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.29.29.29.5.m1.1.1.1" xref="S4.T4.29.29.29.5.m1.1.1.1.cmml">↓</mo><mi id="S4.T4.29.29.29.5.m1.1.1.3" xref="S4.T4.29.29.29.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.29.29.29.5.m1.1b"><apply id="S4.T4.29.29.29.5.m1.1.1.cmml" xref="S4.T4.29.29.29.5.m1.1.1"><ci id="S4.T4.29.29.29.5.m1.1.1.1.cmml" xref="S4.T4.29.29.29.5.m1.1.1.1">↓</ci><apply id="S4.T4.29.29.29.5.m1.1.1.2.cmml" xref="S4.T4.29.29.29.5.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.29.29.29.5.m1.1.1.2.1.cmml" xref="S4.T4.29.29.29.5.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.29.29.29.5.m1.1.1.2.2.cmml" xref="S4.T4.29.29.29.5.m1.1.1.2.2">3.4</cn></apply><csymbol cd="latexml" id="S4.T4.29.29.29.5.m1.1.1.3.cmml" xref="S4.T4.29.29.29.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.29.29.29.5.m1.1c">3.4\%\downarrow</annotation></semantics></math></td>
<td id="S4.T4.30.30.30.6" class="ltx_td ltx_align_right">7.62 hours (1.4<math id="S4.T4.30.30.30.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.30.30.30.6.m1.1a"><mo id="S4.T4.30.30.30.6.m1.1.1" xref="S4.T4.30.30.30.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.30.30.30.6.m1.1b"><times id="S4.T4.30.30.30.6.m1.1.1.cmml" xref="S4.T4.30.30.30.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.30.30.30.6.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.31.31.31.7" class="ltx_td ltx_align_right">20.5 hours (3.4<math id="S4.T4.31.31.31.7.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.31.31.31.7.m1.1a"><mo id="S4.T4.31.31.31.7.m1.1.1" xref="S4.T4.31.31.31.7.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.31.31.31.7.m1.1b"><times id="S4.T4.31.31.31.7.m1.1.1.cmml" xref="S4.T4.31.31.31.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.31.31.31.7.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.32.32.32.8" class="ltx_td ltx_align_right"><span id="S4.T4.32.32.32.8.1" class="ltx_text ltx_markedasmath ltx_font_bold">3.1%</span></td>
</tr>
<tr id="S4.T4.40.40.40" class="ltx_tr">
<td id="S4.T4.40.40.40.9" class="ltx_td ltx_align_center ltx_border_bb" rowspan="5"><span id="S4.T4.40.40.40.9.1" class="ltx_text">M-Type</span></td>
<td id="S4.T4.40.40.40.10" class="ltx_td ltx_align_left">No Compression</td>
<td id="S4.T4.34.34.34.2" class="ltx_td ltx_align_right">
<math id="S4.T4.33.33.33.1.m1.1" class="ltx_Math" alttext="9.86" display="inline"><semantics id="S4.T4.33.33.33.1.m1.1a"><mn id="S4.T4.33.33.33.1.m1.1.1" xref="S4.T4.33.33.33.1.m1.1.1.cmml">9.86</mn><annotation-xml encoding="MathML-Content" id="S4.T4.33.33.33.1.m1.1b"><cn type="float" id="S4.T4.33.33.33.1.m1.1.1.cmml" xref="S4.T4.33.33.33.1.m1.1.1">9.86</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.33.33.33.1.m1.1c">9.86</annotation></semantics></math> (<math id="S4.T4.34.34.34.2.m2.1" class="ltx_Math" alttext="0.0\%" display="inline"><semantics id="S4.T4.34.34.34.2.m2.1a"><mrow id="S4.T4.34.34.34.2.m2.1.1" xref="S4.T4.34.34.34.2.m2.1.1.cmml"><mn id="S4.T4.34.34.34.2.m2.1.1.2" xref="S4.T4.34.34.34.2.m2.1.1.2.cmml">0.0</mn><mo id="S4.T4.34.34.34.2.m2.1.1.1" xref="S4.T4.34.34.34.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.34.34.34.2.m2.1b"><apply id="S4.T4.34.34.34.2.m2.1.1.cmml" xref="S4.T4.34.34.34.2.m2.1.1"><csymbol cd="latexml" id="S4.T4.34.34.34.2.m2.1.1.1.cmml" xref="S4.T4.34.34.34.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.T4.34.34.34.2.m2.1.1.2.cmml" xref="S4.T4.34.34.34.2.m2.1.1.2">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.34.34.34.2.m2.1c">0.0\%</annotation></semantics></math>)</td>
<td id="S4.T4.36.36.36.4" class="ltx_td ltx_align_right">
<math id="S4.T4.35.35.35.3.m1.1" class="ltx_Math" alttext="9.28\%" display="inline"><semantics id="S4.T4.35.35.35.3.m1.1a"><mrow id="S4.T4.35.35.35.3.m1.1.1" xref="S4.T4.35.35.35.3.m1.1.1.cmml"><mn id="S4.T4.35.35.35.3.m1.1.1.2" xref="S4.T4.35.35.35.3.m1.1.1.2.cmml">9.28</mn><mo id="S4.T4.35.35.35.3.m1.1.1.1" xref="S4.T4.35.35.35.3.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.35.35.35.3.m1.1b"><apply id="S4.T4.35.35.35.3.m1.1.1.cmml" xref="S4.T4.35.35.35.3.m1.1.1"><csymbol cd="latexml" id="S4.T4.35.35.35.3.m1.1.1.1.cmml" xref="S4.T4.35.35.35.3.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T4.35.35.35.3.m1.1.1.2.cmml" xref="S4.T4.35.35.35.3.m1.1.1.2">9.28</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.35.35.35.3.m1.1c">9.28\%</annotation></semantics></math> (<math id="S4.T4.36.36.36.4.m2.1" class="ltx_Math" alttext="0.0\%" display="inline"><semantics id="S4.T4.36.36.36.4.m2.1a"><mrow id="S4.T4.36.36.36.4.m2.1.1" xref="S4.T4.36.36.36.4.m2.1.1.cmml"><mn id="S4.T4.36.36.36.4.m2.1.1.2" xref="S4.T4.36.36.36.4.m2.1.1.2.cmml">0.0</mn><mo id="S4.T4.36.36.36.4.m2.1.1.1" xref="S4.T4.36.36.36.4.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.36.36.36.4.m2.1b"><apply id="S4.T4.36.36.36.4.m2.1.1.cmml" xref="S4.T4.36.36.36.4.m2.1.1"><csymbol cd="latexml" id="S4.T4.36.36.36.4.m2.1.1.1.cmml" xref="S4.T4.36.36.36.4.m2.1.1.1">percent</csymbol><cn type="float" id="S4.T4.36.36.36.4.m2.1.1.2.cmml" xref="S4.T4.36.36.36.4.m2.1.1.2">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.36.36.36.4.m2.1c">0.0\%</annotation></semantics></math>)</td>
<td id="S4.T4.37.37.37.5" class="ltx_td ltx_align_right"><math id="S4.T4.37.37.37.5.m1.1" class="ltx_Math" alttext="5.9\%\downarrow" display="inline"><semantics id="S4.T4.37.37.37.5.m1.1a"><mrow id="S4.T4.37.37.37.5.m1.1.1" xref="S4.T4.37.37.37.5.m1.1.1.cmml"><mrow id="S4.T4.37.37.37.5.m1.1.1.2" xref="S4.T4.37.37.37.5.m1.1.1.2.cmml"><mn id="S4.T4.37.37.37.5.m1.1.1.2.2" xref="S4.T4.37.37.37.5.m1.1.1.2.2.cmml">5.9</mn><mo id="S4.T4.37.37.37.5.m1.1.1.2.1" xref="S4.T4.37.37.37.5.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.37.37.37.5.m1.1.1.1" xref="S4.T4.37.37.37.5.m1.1.1.1.cmml">↓</mo><mi id="S4.T4.37.37.37.5.m1.1.1.3" xref="S4.T4.37.37.37.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.37.37.37.5.m1.1b"><apply id="S4.T4.37.37.37.5.m1.1.1.cmml" xref="S4.T4.37.37.37.5.m1.1.1"><ci id="S4.T4.37.37.37.5.m1.1.1.1.cmml" xref="S4.T4.37.37.37.5.m1.1.1.1">↓</ci><apply id="S4.T4.37.37.37.5.m1.1.1.2.cmml" xref="S4.T4.37.37.37.5.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.37.37.37.5.m1.1.1.2.1.cmml" xref="S4.T4.37.37.37.5.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.37.37.37.5.m1.1.1.2.2.cmml" xref="S4.T4.37.37.37.5.m1.1.1.2.2">5.9</cn></apply><csymbol cd="latexml" id="S4.T4.37.37.37.5.m1.1.1.3.cmml" xref="S4.T4.37.37.37.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.37.37.37.5.m1.1c">5.9\%\downarrow</annotation></semantics></math></td>
<td id="S4.T4.38.38.38.6" class="ltx_td ltx_align_right">0.54 hours (1.0<math id="S4.T4.38.38.38.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.38.38.38.6.m1.1a"><mo id="S4.T4.38.38.38.6.m1.1.1" xref="S4.T4.38.38.38.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.38.38.38.6.m1.1b"><times id="S4.T4.38.38.38.6.m1.1.1.cmml" xref="S4.T4.38.38.38.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.38.38.38.6.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.39.39.39.7" class="ltx_td ltx_align_right">1.23 hours (1.0<math id="S4.T4.39.39.39.7.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.39.39.39.7.m1.1a"><mo id="S4.T4.39.39.39.7.m1.1.1" xref="S4.T4.39.39.39.7.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.39.39.39.7.m1.1b"><times id="S4.T4.39.39.39.7.m1.1.1.cmml" xref="S4.T4.39.39.39.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.39.39.39.7.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.40.40.40.8" class="ltx_td ltx_align_right"><math id="S4.T4.40.40.40.8.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S4.T4.40.40.40.8.m1.1a"><mrow id="S4.T4.40.40.40.8.m1.1.1" xref="S4.T4.40.40.40.8.m1.1.1.cmml"><mn id="S4.T4.40.40.40.8.m1.1.1.2" xref="S4.T4.40.40.40.8.m1.1.1.2.cmml">100</mn><mo id="S4.T4.40.40.40.8.m1.1.1.1" xref="S4.T4.40.40.40.8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.40.40.40.8.m1.1b"><apply id="S4.T4.40.40.40.8.m1.1.1.cmml" xref="S4.T4.40.40.40.8.m1.1.1"><csymbol cd="latexml" id="S4.T4.40.40.40.8.m1.1.1.1.cmml" xref="S4.T4.40.40.40.8.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.T4.40.40.40.8.m1.1.1.2.cmml" xref="S4.T4.40.40.40.8.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.40.40.40.8.m1.1c">100\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.48.48.48" class="ltx_tr">
<td id="S4.T4.48.48.48.9" class="ltx_td ltx_align_left">Structured Updates</td>
<td id="S4.T4.42.42.42.2" class="ltx_td ltx_align_right">
<math id="S4.T4.41.41.41.1.m1.1" class="ltx_Math" alttext="9.93" display="inline"><semantics id="S4.T4.41.41.41.1.m1.1a"><mn id="S4.T4.41.41.41.1.m1.1.1" xref="S4.T4.41.41.41.1.m1.1.1.cmml">9.93</mn><annotation-xml encoding="MathML-Content" id="S4.T4.41.41.41.1.m1.1b"><cn type="float" id="S4.T4.41.41.41.1.m1.1.1.cmml" xref="S4.T4.41.41.41.1.m1.1.1">9.93</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.41.41.41.1.m1.1c">9.93</annotation></semantics></math> (<math id="S4.T4.42.42.42.2.m2.1" class="ltx_Math" alttext="0.6\%\uparrow" display="inline"><semantics id="S4.T4.42.42.42.2.m2.1a"><mrow id="S4.T4.42.42.42.2.m2.1.1" xref="S4.T4.42.42.42.2.m2.1.1.cmml"><mrow id="S4.T4.42.42.42.2.m2.1.1.2" xref="S4.T4.42.42.42.2.m2.1.1.2.cmml"><mn id="S4.T4.42.42.42.2.m2.1.1.2.2" xref="S4.T4.42.42.42.2.m2.1.1.2.2.cmml">0.6</mn><mo id="S4.T4.42.42.42.2.m2.1.1.2.1" xref="S4.T4.42.42.42.2.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.42.42.42.2.m2.1.1.1" xref="S4.T4.42.42.42.2.m2.1.1.1.cmml">↑</mo><mi id="S4.T4.42.42.42.2.m2.1.1.3" xref="S4.T4.42.42.42.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.42.42.42.2.m2.1b"><apply id="S4.T4.42.42.42.2.m2.1.1.cmml" xref="S4.T4.42.42.42.2.m2.1.1"><ci id="S4.T4.42.42.42.2.m2.1.1.1.cmml" xref="S4.T4.42.42.42.2.m2.1.1.1">↑</ci><apply id="S4.T4.42.42.42.2.m2.1.1.2.cmml" xref="S4.T4.42.42.42.2.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.42.42.42.2.m2.1.1.2.1.cmml" xref="S4.T4.42.42.42.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.42.42.42.2.m2.1.1.2.2.cmml" xref="S4.T4.42.42.42.2.m2.1.1.2.2">0.6</cn></apply><csymbol cd="latexml" id="S4.T4.42.42.42.2.m2.1.1.3.cmml" xref="S4.T4.42.42.42.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.42.42.42.2.m2.1c">0.6\%\uparrow</annotation></semantics></math>)</td>
<td id="S4.T4.44.44.44.4" class="ltx_td ltx_align_right">
<math id="S4.T4.43.43.43.3.m1.1" class="ltx_Math" alttext="9.08" display="inline"><semantics id="S4.T4.43.43.43.3.m1.1a"><mn id="S4.T4.43.43.43.3.m1.1.1" xref="S4.T4.43.43.43.3.m1.1.1.cmml">9.08</mn><annotation-xml encoding="MathML-Content" id="S4.T4.43.43.43.3.m1.1b"><cn type="float" id="S4.T4.43.43.43.3.m1.1.1.cmml" xref="S4.T4.43.43.43.3.m1.1.1">9.08</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.43.43.43.3.m1.1c">9.08</annotation></semantics></math> (<math id="S4.T4.44.44.44.4.m2.1" class="ltx_Math" alttext="2.2\%\downarrow" display="inline"><semantics id="S4.T4.44.44.44.4.m2.1a"><mrow id="S4.T4.44.44.44.4.m2.1.1" xref="S4.T4.44.44.44.4.m2.1.1.cmml"><mrow id="S4.T4.44.44.44.4.m2.1.1.2" xref="S4.T4.44.44.44.4.m2.1.1.2.cmml"><mn id="S4.T4.44.44.44.4.m2.1.1.2.2" xref="S4.T4.44.44.44.4.m2.1.1.2.2.cmml">2.2</mn><mo id="S4.T4.44.44.44.4.m2.1.1.2.1" xref="S4.T4.44.44.44.4.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.44.44.44.4.m2.1.1.1" xref="S4.T4.44.44.44.4.m2.1.1.1.cmml">↓</mo><mi id="S4.T4.44.44.44.4.m2.1.1.3" xref="S4.T4.44.44.44.4.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.44.44.44.4.m2.1b"><apply id="S4.T4.44.44.44.4.m2.1.1.cmml" xref="S4.T4.44.44.44.4.m2.1.1"><ci id="S4.T4.44.44.44.4.m2.1.1.1.cmml" xref="S4.T4.44.44.44.4.m2.1.1.1">↓</ci><apply id="S4.T4.44.44.44.4.m2.1.1.2.cmml" xref="S4.T4.44.44.44.4.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.44.44.44.4.m2.1.1.2.1.cmml" xref="S4.T4.44.44.44.4.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.44.44.44.4.m2.1.1.2.2.cmml" xref="S4.T4.44.44.44.4.m2.1.1.2.2">2.2</cn></apply><csymbol cd="latexml" id="S4.T4.44.44.44.4.m2.1.1.3.cmml" xref="S4.T4.44.44.44.4.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.44.44.44.4.m2.1c">2.2\%\downarrow</annotation></semantics></math>)</td>
<td id="S4.T4.45.45.45.5" class="ltx_td ltx_align_right"><math id="S4.T4.45.45.45.5.m1.1" class="ltx_Math" alttext="8.6\%\downarrow" display="inline"><semantics id="S4.T4.45.45.45.5.m1.1a"><mrow id="S4.T4.45.45.45.5.m1.1.1" xref="S4.T4.45.45.45.5.m1.1.1.cmml"><mrow id="S4.T4.45.45.45.5.m1.1.1.2" xref="S4.T4.45.45.45.5.m1.1.1.2.cmml"><mn id="S4.T4.45.45.45.5.m1.1.1.2.2" xref="S4.T4.45.45.45.5.m1.1.1.2.2.cmml">8.6</mn><mo id="S4.T4.45.45.45.5.m1.1.1.2.1" xref="S4.T4.45.45.45.5.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.45.45.45.5.m1.1.1.1" xref="S4.T4.45.45.45.5.m1.1.1.1.cmml">↓</mo><mi id="S4.T4.45.45.45.5.m1.1.1.3" xref="S4.T4.45.45.45.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.45.45.45.5.m1.1b"><apply id="S4.T4.45.45.45.5.m1.1.1.cmml" xref="S4.T4.45.45.45.5.m1.1.1"><ci id="S4.T4.45.45.45.5.m1.1.1.1.cmml" xref="S4.T4.45.45.45.5.m1.1.1.1">↓</ci><apply id="S4.T4.45.45.45.5.m1.1.1.2.cmml" xref="S4.T4.45.45.45.5.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.45.45.45.5.m1.1.1.2.1.cmml" xref="S4.T4.45.45.45.5.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.45.45.45.5.m1.1.1.2.2.cmml" xref="S4.T4.45.45.45.5.m1.1.1.2.2">8.6</cn></apply><csymbol cd="latexml" id="S4.T4.45.45.45.5.m1.1.1.3.cmml" xref="S4.T4.45.45.45.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.45.45.45.5.m1.1c">8.6\%\downarrow</annotation></semantics></math></td>
<td id="S4.T4.46.46.46.6" class="ltx_td ltx_align_right"><span id="S4.T4.46.46.46.6.1" class="ltx_text ltx_font_bold">0.53 hours (0.98<math id="S4.T4.46.46.46.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.46.46.46.6.1.m1.1a"><mo id="S4.T4.46.46.46.6.1.m1.1.1" xref="S4.T4.46.46.46.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.46.46.46.6.1.m1.1b"><times id="S4.T4.46.46.46.6.1.m1.1.1.cmml" xref="S4.T4.46.46.46.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.46.46.46.6.1.m1.1c">\times</annotation></semantics></math>)</span></td>
<td id="S4.T4.47.47.47.7" class="ltx_td ltx_align_right"><span id="S4.T4.47.47.47.7.1" class="ltx_text ltx_font_bold">1.59 hours (1.3<math id="S4.T4.47.47.47.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.47.47.47.7.1.m1.1a"><mo id="S4.T4.47.47.47.7.1.m1.1.1" xref="S4.T4.47.47.47.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.47.47.47.7.1.m1.1b"><times id="S4.T4.47.47.47.7.1.m1.1.1.cmml" xref="S4.T4.47.47.47.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.47.47.47.7.1.m1.1c">\times</annotation></semantics></math>)</span></td>
<td id="S4.T4.48.48.48.8" class="ltx_td ltx_align_right"><math id="S4.T4.48.48.48.8.m1.1" class="ltx_Math" alttext="39.4\%" display="inline"><semantics id="S4.T4.48.48.48.8.m1.1a"><mrow id="S4.T4.48.48.48.8.m1.1.1" xref="S4.T4.48.48.48.8.m1.1.1.cmml"><mn id="S4.T4.48.48.48.8.m1.1.1.2" xref="S4.T4.48.48.48.8.m1.1.1.2.cmml">39.4</mn><mo id="S4.T4.48.48.48.8.m1.1.1.1" xref="S4.T4.48.48.48.8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.48.48.48.8.m1.1b"><apply id="S4.T4.48.48.48.8.m1.1.1.cmml" xref="S4.T4.48.48.48.8.m1.1.1"><csymbol cd="latexml" id="S4.T4.48.48.48.8.m1.1.1.1.cmml" xref="S4.T4.48.48.48.8.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T4.48.48.48.8.m1.1.1.2.cmml" xref="S4.T4.48.48.48.8.m1.1.1.2">39.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.48.48.48.8.m1.1c">39.4\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.56.56.56" class="ltx_tr">
<td id="S4.T4.56.56.56.9" class="ltx_td ltx_align_left">GDrop</td>
<td id="S4.T4.50.50.50.2" class="ltx_td ltx_align_right">
<math id="S4.T4.49.49.49.1.m1.1" class="ltx_Math" alttext="8.09" display="inline"><semantics id="S4.T4.49.49.49.1.m1.1a"><mn id="S4.T4.49.49.49.1.m1.1.1" xref="S4.T4.49.49.49.1.m1.1.1.cmml">8.09</mn><annotation-xml encoding="MathML-Content" id="S4.T4.49.49.49.1.m1.1b"><cn type="float" id="S4.T4.49.49.49.1.m1.1.1.cmml" xref="S4.T4.49.49.49.1.m1.1.1">8.09</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.49.49.49.1.m1.1c">8.09</annotation></semantics></math> (<math id="S4.T4.50.50.50.2.m2.1" class="ltx_Math" alttext="18.0\%\downarrow" display="inline"><semantics id="S4.T4.50.50.50.2.m2.1a"><mrow id="S4.T4.50.50.50.2.m2.1.1" xref="S4.T4.50.50.50.2.m2.1.1.cmml"><mrow id="S4.T4.50.50.50.2.m2.1.1.2" xref="S4.T4.50.50.50.2.m2.1.1.2.cmml"><mn id="S4.T4.50.50.50.2.m2.1.1.2.2" xref="S4.T4.50.50.50.2.m2.1.1.2.2.cmml">18.0</mn><mo id="S4.T4.50.50.50.2.m2.1.1.2.1" xref="S4.T4.50.50.50.2.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.50.50.50.2.m2.1.1.1" xref="S4.T4.50.50.50.2.m2.1.1.1.cmml">↓</mo><mi id="S4.T4.50.50.50.2.m2.1.1.3" xref="S4.T4.50.50.50.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.50.50.50.2.m2.1b"><apply id="S4.T4.50.50.50.2.m2.1.1.cmml" xref="S4.T4.50.50.50.2.m2.1.1"><ci id="S4.T4.50.50.50.2.m2.1.1.1.cmml" xref="S4.T4.50.50.50.2.m2.1.1.1">↓</ci><apply id="S4.T4.50.50.50.2.m2.1.1.2.cmml" xref="S4.T4.50.50.50.2.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.50.50.50.2.m2.1.1.2.1.cmml" xref="S4.T4.50.50.50.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.50.50.50.2.m2.1.1.2.2.cmml" xref="S4.T4.50.50.50.2.m2.1.1.2.2">18.0</cn></apply><csymbol cd="latexml" id="S4.T4.50.50.50.2.m2.1.1.3.cmml" xref="S4.T4.50.50.50.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.50.50.50.2.m2.1c">18.0\%\downarrow</annotation></semantics></math>)</td>
<td id="S4.T4.52.52.52.4" class="ltx_td ltx_align_right">
<math id="S4.T4.51.51.51.3.m1.1" class="ltx_Math" alttext="8.27" display="inline"><semantics id="S4.T4.51.51.51.3.m1.1a"><mn id="S4.T4.51.51.51.3.m1.1.1" xref="S4.T4.51.51.51.3.m1.1.1.cmml">8.27</mn><annotation-xml encoding="MathML-Content" id="S4.T4.51.51.51.3.m1.1b"><cn type="float" id="S4.T4.51.51.51.3.m1.1.1.cmml" xref="S4.T4.51.51.51.3.m1.1.1">8.27</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.51.51.51.3.m1.1c">8.27</annotation></semantics></math> (<math id="S4.T4.52.52.52.4.m2.1" class="ltx_Math" alttext="10.9\%\downarrow" display="inline"><semantics id="S4.T4.52.52.52.4.m2.1a"><mrow id="S4.T4.52.52.52.4.m2.1.1" xref="S4.T4.52.52.52.4.m2.1.1.cmml"><mrow id="S4.T4.52.52.52.4.m2.1.1.2" xref="S4.T4.52.52.52.4.m2.1.1.2.cmml"><mn id="S4.T4.52.52.52.4.m2.1.1.2.2" xref="S4.T4.52.52.52.4.m2.1.1.2.2.cmml">10.9</mn><mo id="S4.T4.52.52.52.4.m2.1.1.2.1" xref="S4.T4.52.52.52.4.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.52.52.52.4.m2.1.1.1" xref="S4.T4.52.52.52.4.m2.1.1.1.cmml">↓</mo><mi id="S4.T4.52.52.52.4.m2.1.1.3" xref="S4.T4.52.52.52.4.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.52.52.52.4.m2.1b"><apply id="S4.T4.52.52.52.4.m2.1.1.cmml" xref="S4.T4.52.52.52.4.m2.1.1"><ci id="S4.T4.52.52.52.4.m2.1.1.1.cmml" xref="S4.T4.52.52.52.4.m2.1.1.1">↓</ci><apply id="S4.T4.52.52.52.4.m2.1.1.2.cmml" xref="S4.T4.52.52.52.4.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.52.52.52.4.m2.1.1.2.1.cmml" xref="S4.T4.52.52.52.4.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.52.52.52.4.m2.1.1.2.2.cmml" xref="S4.T4.52.52.52.4.m2.1.1.2.2">10.9</cn></apply><csymbol cd="latexml" id="S4.T4.52.52.52.4.m2.1.1.3.cmml" xref="S4.T4.52.52.52.4.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.52.52.52.4.m2.1c">10.9\%\downarrow</annotation></semantics></math>)</td>
<td id="S4.T4.53.53.53.5" class="ltx_td ltx_align_right"><math id="S4.T4.53.53.53.5.m1.1" class="ltx_Math" alttext="2.2\%\uparrow" display="inline"><semantics id="S4.T4.53.53.53.5.m1.1a"><mrow id="S4.T4.53.53.53.5.m1.1.1" xref="S4.T4.53.53.53.5.m1.1.1.cmml"><mrow id="S4.T4.53.53.53.5.m1.1.1.2" xref="S4.T4.53.53.53.5.m1.1.1.2.cmml"><mn id="S4.T4.53.53.53.5.m1.1.1.2.2" xref="S4.T4.53.53.53.5.m1.1.1.2.2.cmml">2.2</mn><mo id="S4.T4.53.53.53.5.m1.1.1.2.1" xref="S4.T4.53.53.53.5.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.53.53.53.5.m1.1.1.1" xref="S4.T4.53.53.53.5.m1.1.1.1.cmml">↑</mo><mi id="S4.T4.53.53.53.5.m1.1.1.3" xref="S4.T4.53.53.53.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.53.53.53.5.m1.1b"><apply id="S4.T4.53.53.53.5.m1.1.1.cmml" xref="S4.T4.53.53.53.5.m1.1.1"><ci id="S4.T4.53.53.53.5.m1.1.1.1.cmml" xref="S4.T4.53.53.53.5.m1.1.1.1">↑</ci><apply id="S4.T4.53.53.53.5.m1.1.1.2.cmml" xref="S4.T4.53.53.53.5.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.53.53.53.5.m1.1.1.2.1.cmml" xref="S4.T4.53.53.53.5.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.53.53.53.5.m1.1.1.2.2.cmml" xref="S4.T4.53.53.53.5.m1.1.1.2.2">2.2</cn></apply><csymbol cd="latexml" id="S4.T4.53.53.53.5.m1.1.1.3.cmml" xref="S4.T4.53.53.53.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.53.53.53.5.m1.1c">2.2\%\uparrow</annotation></semantics></math></td>
<td id="S4.T4.54.54.54.6" class="ltx_td ltx_align_right">5.34 hours (10.0<math id="S4.T4.54.54.54.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.54.54.54.6.m1.1a"><mo id="S4.T4.54.54.54.6.m1.1.1" xref="S4.T4.54.54.54.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.54.54.54.6.m1.1b"><times id="S4.T4.54.54.54.6.m1.1.1.cmml" xref="S4.T4.54.54.54.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.54.54.54.6.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.55.55.55.7" class="ltx_td ltx_align_right">4.29 hours (3.5<math id="S4.T4.55.55.55.7.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.55.55.55.7.m1.1a"><mo id="S4.T4.55.55.55.7.m1.1.1" xref="S4.T4.55.55.55.7.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.55.55.55.7.m1.1b"><times id="S4.T4.55.55.55.7.m1.1.1.cmml" xref="S4.T4.55.55.55.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.55.55.55.7.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.56.56.56.8" class="ltx_td ltx_align_right"><math id="S4.T4.56.56.56.8.m1.1" class="ltx_Math" alttext="\textbf{0.1}\%\sim\textbf{2.1}\%" display="inline"><semantics id="S4.T4.56.56.56.8.m1.1a"><mrow id="S4.T4.56.56.56.8.m1.1.1" xref="S4.T4.56.56.56.8.m1.1.1.cmml"><mrow id="S4.T4.56.56.56.8.m1.1.1.2" xref="S4.T4.56.56.56.8.m1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S4.T4.56.56.56.8.m1.1.1.2.2" xref="S4.T4.56.56.56.8.m1.1.1.2.2a.cmml">0.1</mtext><mo id="S4.T4.56.56.56.8.m1.1.1.2.1" xref="S4.T4.56.56.56.8.m1.1.1.2.1.cmml">%</mo></mrow><mo id="S4.T4.56.56.56.8.m1.1.1.1" xref="S4.T4.56.56.56.8.m1.1.1.1.cmml">∼</mo><mrow id="S4.T4.56.56.56.8.m1.1.1.3" xref="S4.T4.56.56.56.8.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S4.T4.56.56.56.8.m1.1.1.3.2" xref="S4.T4.56.56.56.8.m1.1.1.3.2a.cmml">2.1</mtext><mo id="S4.T4.56.56.56.8.m1.1.1.3.1" xref="S4.T4.56.56.56.8.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.56.56.56.8.m1.1b"><apply id="S4.T4.56.56.56.8.m1.1.1.cmml" xref="S4.T4.56.56.56.8.m1.1.1"><csymbol cd="latexml" id="S4.T4.56.56.56.8.m1.1.1.1.cmml" xref="S4.T4.56.56.56.8.m1.1.1.1">similar-to</csymbol><apply id="S4.T4.56.56.56.8.m1.1.1.2.cmml" xref="S4.T4.56.56.56.8.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.56.56.56.8.m1.1.1.2.1.cmml" xref="S4.T4.56.56.56.8.m1.1.1.2.1">percent</csymbol><ci id="S4.T4.56.56.56.8.m1.1.1.2.2a.cmml" xref="S4.T4.56.56.56.8.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S4.T4.56.56.56.8.m1.1.1.2.2.cmml" xref="S4.T4.56.56.56.8.m1.1.1.2.2">0.1</mtext></ci></apply><apply id="S4.T4.56.56.56.8.m1.1.1.3.cmml" xref="S4.T4.56.56.56.8.m1.1.1.3"><csymbol cd="latexml" id="S4.T4.56.56.56.8.m1.1.1.3.1.cmml" xref="S4.T4.56.56.56.8.m1.1.1.3.1">percent</csymbol><ci id="S4.T4.56.56.56.8.m1.1.1.3.2a.cmml" xref="S4.T4.56.56.56.8.m1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S4.T4.56.56.56.8.m1.1.1.3.2.cmml" xref="S4.T4.56.56.56.8.m1.1.1.3.2">2.1</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.56.56.56.8.m1.1c">\textbf{0.1}\%\sim\textbf{2.1}\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T4.64.64.64" class="ltx_tr">
<td id="S4.T4.64.64.64.9" class="ltx_td ltx_align_left ltx_border_bb">SignSGD</td>
<td id="S4.T4.58.58.58.2" class="ltx_td ltx_align_right ltx_border_bb">
<span id="S4.T4.58.58.58.2.1" class="ltx_text ltx_markedasmath ltx_font_bold">10.4</span> (<math id="S4.T4.58.58.58.2.m2.1" class="ltx_Math" alttext="6.0\%\uparrow" display="inline"><semantics id="S4.T4.58.58.58.2.m2.1a"><mrow id="S4.T4.58.58.58.2.m2.1.1" xref="S4.T4.58.58.58.2.m2.1.1.cmml"><mrow id="S4.T4.58.58.58.2.m2.1.1.2" xref="S4.T4.58.58.58.2.m2.1.1.2.cmml"><mn id="S4.T4.58.58.58.2.m2.1.1.2.2" xref="S4.T4.58.58.58.2.m2.1.1.2.2.cmml">6.0</mn><mo id="S4.T4.58.58.58.2.m2.1.1.2.1" xref="S4.T4.58.58.58.2.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.58.58.58.2.m2.1.1.1" xref="S4.T4.58.58.58.2.m2.1.1.1.cmml">↑</mo><mi id="S4.T4.58.58.58.2.m2.1.1.3" xref="S4.T4.58.58.58.2.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.58.58.58.2.m2.1b"><apply id="S4.T4.58.58.58.2.m2.1.1.cmml" xref="S4.T4.58.58.58.2.m2.1.1"><ci id="S4.T4.58.58.58.2.m2.1.1.1.cmml" xref="S4.T4.58.58.58.2.m2.1.1.1">↑</ci><apply id="S4.T4.58.58.58.2.m2.1.1.2.cmml" xref="S4.T4.58.58.58.2.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.58.58.58.2.m2.1.1.2.1.cmml" xref="S4.T4.58.58.58.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.58.58.58.2.m2.1.1.2.2.cmml" xref="S4.T4.58.58.58.2.m2.1.1.2.2">6.0</cn></apply><csymbol cd="latexml" id="S4.T4.58.58.58.2.m2.1.1.3.cmml" xref="S4.T4.58.58.58.2.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.58.58.58.2.m2.1c">6.0\%\uparrow</annotation></semantics></math>)</td>
<td id="S4.T4.60.60.60.4" class="ltx_td ltx_align_right ltx_border_bb">
<span id="S4.T4.60.60.60.4.1" class="ltx_text ltx_markedasmath ltx_font_bold">9.55</span> (<math id="S4.T4.60.60.60.4.m2.1" class="ltx_Math" alttext="2.9\%\uparrow" display="inline"><semantics id="S4.T4.60.60.60.4.m2.1a"><mrow id="S4.T4.60.60.60.4.m2.1.1" xref="S4.T4.60.60.60.4.m2.1.1.cmml"><mrow id="S4.T4.60.60.60.4.m2.1.1.2" xref="S4.T4.60.60.60.4.m2.1.1.2.cmml"><mn id="S4.T4.60.60.60.4.m2.1.1.2.2" xref="S4.T4.60.60.60.4.m2.1.1.2.2.cmml">2.9</mn><mo id="S4.T4.60.60.60.4.m2.1.1.2.1" xref="S4.T4.60.60.60.4.m2.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.60.60.60.4.m2.1.1.1" xref="S4.T4.60.60.60.4.m2.1.1.1.cmml">↑</mo><mi id="S4.T4.60.60.60.4.m2.1.1.3" xref="S4.T4.60.60.60.4.m2.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.60.60.60.4.m2.1b"><apply id="S4.T4.60.60.60.4.m2.1.1.cmml" xref="S4.T4.60.60.60.4.m2.1.1"><ci id="S4.T4.60.60.60.4.m2.1.1.1.cmml" xref="S4.T4.60.60.60.4.m2.1.1.1">↑</ci><apply id="S4.T4.60.60.60.4.m2.1.1.2.cmml" xref="S4.T4.60.60.60.4.m2.1.1.2"><csymbol cd="latexml" id="S4.T4.60.60.60.4.m2.1.1.2.1.cmml" xref="S4.T4.60.60.60.4.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.60.60.60.4.m2.1.1.2.2.cmml" xref="S4.T4.60.60.60.4.m2.1.1.2.2">2.9</cn></apply><csymbol cd="latexml" id="S4.T4.60.60.60.4.m2.1.1.3.cmml" xref="S4.T4.60.60.60.4.m2.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.60.60.60.4.m2.1c">2.9\%\uparrow</annotation></semantics></math>)</td>
<td id="S4.T4.61.61.61.5" class="ltx_td ltx_align_right ltx_border_bb"><math id="S4.T4.61.61.61.5.m1.1" class="ltx_Math" alttext="8.5\%\downarrow" display="inline"><semantics id="S4.T4.61.61.61.5.m1.1a"><mrow id="S4.T4.61.61.61.5.m1.1.1" xref="S4.T4.61.61.61.5.m1.1.1.cmml"><mrow id="S4.T4.61.61.61.5.m1.1.1.2" xref="S4.T4.61.61.61.5.m1.1.1.2.cmml"><mn id="S4.T4.61.61.61.5.m1.1.1.2.2" xref="S4.T4.61.61.61.5.m1.1.1.2.2.cmml">8.5</mn><mo id="S4.T4.61.61.61.5.m1.1.1.2.1" xref="S4.T4.61.61.61.5.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.T4.61.61.61.5.m1.1.1.1" xref="S4.T4.61.61.61.5.m1.1.1.1.cmml">↓</mo><mi id="S4.T4.61.61.61.5.m1.1.1.3" xref="S4.T4.61.61.61.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.61.61.61.5.m1.1b"><apply id="S4.T4.61.61.61.5.m1.1.1.cmml" xref="S4.T4.61.61.61.5.m1.1.1"><ci id="S4.T4.61.61.61.5.m1.1.1.1.cmml" xref="S4.T4.61.61.61.5.m1.1.1.1">↓</ci><apply id="S4.T4.61.61.61.5.m1.1.1.2.cmml" xref="S4.T4.61.61.61.5.m1.1.1.2"><csymbol cd="latexml" id="S4.T4.61.61.61.5.m1.1.1.2.1.cmml" xref="S4.T4.61.61.61.5.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T4.61.61.61.5.m1.1.1.2.2.cmml" xref="S4.T4.61.61.61.5.m1.1.1.2.2">8.5</cn></apply><csymbol cd="latexml" id="S4.T4.61.61.61.5.m1.1.1.3.cmml" xref="S4.T4.61.61.61.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.61.61.61.5.m1.1c">8.5\%\downarrow</annotation></semantics></math></td>
<td id="S4.T4.62.62.62.6" class="ltx_td ltx_align_right ltx_border_bb">1.45 hours (2.7<math id="S4.T4.62.62.62.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.62.62.62.6.m1.1a"><mo id="S4.T4.62.62.62.6.m1.1.1" xref="S4.T4.62.62.62.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.62.62.62.6.m1.1b"><times id="S4.T4.62.62.62.6.m1.1.1.cmml" xref="S4.T4.62.62.62.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.62.62.62.6.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.63.63.63.7" class="ltx_td ltx_align_right ltx_border_bb">3.93 hours (3.2<math id="S4.T4.63.63.63.7.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T4.63.63.63.7.m1.1a"><mo id="S4.T4.63.63.63.7.m1.1.1" xref="S4.T4.63.63.63.7.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T4.63.63.63.7.m1.1b"><times id="S4.T4.63.63.63.7.m1.1.1.cmml" xref="S4.T4.63.63.63.7.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.63.63.63.7.m1.1c">\times</annotation></semantics></math>)</td>
<td id="S4.T4.64.64.64.8" class="ltx_td ltx_align_right ltx_border_bb"><math id="S4.T4.64.64.64.8.m1.1" class="ltx_Math" alttext="3.1\%" display="inline"><semantics id="S4.T4.64.64.64.8.m1.1a"><mrow id="S4.T4.64.64.64.8.m1.1.1" xref="S4.T4.64.64.64.8.m1.1.1.cmml"><mn id="S4.T4.64.64.64.8.m1.1.1.2" xref="S4.T4.64.64.64.8.m1.1.1.2.cmml">3.1</mn><mo id="S4.T4.64.64.64.8.m1.1.1.1" xref="S4.T4.64.64.64.8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.64.64.64.8.m1.1b"><apply id="S4.T4.64.64.64.8.m1.1.1.cmml" xref="S4.T4.64.64.64.8.m1.1.1"><csymbol cd="latexml" id="S4.T4.64.64.64.8.m1.1.1.1.cmml" xref="S4.T4.64.64.64.8.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T4.64.64.64.8.m1.1.1.2.cmml" xref="S4.T4.64.64.64.8.m1.1.1.2">3.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.64.64.64.8.m1.1c">3.1\%</annotation></semantics></math></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>
The performance of different gradients compression algorithms.
Numbers in the brackets indicate the accuracy change compared to the “No Compression” baseline.
“Acc. Change” refers to the accuracy change introduced by heterogeneity.
The compression ratio is the fraction of the size of compressed gradients to the original size.
</figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span><span id="S4.SS2.SSS1.1.1" class="ltx_text ltx_font_bold">Aggregation Algorithms</span>
</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">The aggregation algorithm is a key component in FL that determines how to aggregate the weights or gradients uploaded from multiple devices.
Besides <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">FedAvg</span>, various aggregation algorithms are proposed to improve efficiency <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>; Nishio and
Yonetani, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Niu
et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>, ensure fairness <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>)</cite>, preserve privacy <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Niu
et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>, etc.
To study how heterogeneity affects the performance of aggregation algorithms, we focus on two representative ones: <span id="S4.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">q-FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020c</a>)</cite> and <span id="S4.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_italic">FedProx</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite>, both of which are open-sourced.
<span id="S4.SS2.SSS1.p1.1.4" class="ltx_text ltx_font_italic">q-FedAvg</span> is proposed to address the fairness issues in FL. It minimizes an aggregated reweighted loss so that the devices with higher loss are given higher relative weights.
<span id="S4.SS2.SSS1.p1.1.5" class="ltx_text ltx_font_italic">FedProx</span> is proposed to tackle with hardware heterogeneity in FL.
Compared to <span id="S4.SS2.SSS1.p1.1.6" class="ltx_text ltx_font_italic">FedAvg</span>, <span id="S4.SS2.SSS1.p1.1.7" class="ltx_text ltx_font_italic">FedProx</span> allows devices to perform various amounts of training work based on their available system resources, while <span id="S4.SS2.SSS1.p1.1.8" class="ltx_text ltx_font_italic">FedAvg</span> simply drops the stragglers that fail to upload the model updates.
<span id="S4.SS2.SSS1.p1.1.9" class="ltx_text ltx_font_italic">FedProx</span> also adds a proximal term to the local optimization objective (loss function) to limit the impact of variable local updates.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">We use <span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">FedAvg</span> as the baseline for comparison.
Due to the different optimization goals of <span id="S4.SS2.SSS1.p2.1.2" class="ltx_text ltx_font_italic">q-FedAvg</span> and <span id="S4.SS2.SSS1.p2.1.3" class="ltx_text ltx_font_italic">FedProx</span>, we make the comparison separately. For <span id="S4.SS2.SSS1.p2.1.4" class="ltx_text ltx_font_italic">q-FedAvg</span>, the results are shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which illustrates the the same metrics as evaluated by <span id="S4.SS2.SSS1.p2.1.5" class="ltx_text ltx_font_italic">q-FedAvg</span> (variance of accuracy, worst 10% accuracy, i.e., 10% quantile of accuracy across devices, and best 10% accuracy, i.e., 90% quantile of accuracy across devices). For <span id="S4.SS2.SSS1.p2.1.6" class="ltx_text ltx_font_italic">FedProx</span>, the results are shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2.1. Aggregation Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, which presents the accuracy changes by round.
Due to space limit, we show only the results on two datasets, i.e., one dataset using the CNN model (Femnist) and another dataset using the LSTM model (M-Type). Our observations are as follows.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p3.2" class="ltx_p"><math id="S4.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS1.p3.1.m1.1a"><mo id="S4.SS2.SSS1.p3.1.m1.1.1" xref="S4.SS2.SSS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.1.m1.1b"><ci id="S4.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S4.SS2.SSS1.p3.2.1" class="ltx_text ltx_font_bold ltx_font_italic">q-FedAvg<span id="S4.SS2.SSS1.p3.2.1.1" class="ltx_text ltx_font_upright"> that is supposed to address fairness issues is less effective in ensuring fairness under heterogeneity-aware settings.</span></span>
According to Table <a href="#S4.T3" title="Table 3 ‣ 4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, under heterogeneity-unaware settings, the worst 10% accuracy of <span id="S4.SS2.SSS1.p3.2.2" class="ltx_text ltx_font_italic">q-FedAvg</span> is higher than that of <span id="S4.SS2.SSS1.p3.2.3" class="ltx_text ltx_font_italic">FedAvg</span> and <span id="S4.SS2.SSS1.p3.2.4" class="ltx_text ltx_font_italic">q-FedAvg</span> also obtains lower variance of accuracy on both datasets.
However, under heterogeneity-aware settings, the variance reduction decreases from 26.3% to 21.7% on Femnist and from 10.5% to 3.7% on M-Type, respectively.
It is probably because <span id="S4.SS2.SSS1.p3.2.5" class="ltx_text ltx_font_italic">q-FedAvg</span> cannot tackle the bias in device selection introduced by state heterogeneity (see details in <math id="S4.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="\S\ref{subsec:bias}" display="inline"><semantics id="S4.SS2.SSS1.p3.2.m2.1a"><mrow id="S4.SS2.SSS1.p3.2.m2.1.1" xref="S4.SS2.SSS1.p3.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.SSS1.p3.2.m2.1.1.2" xref="S4.SS2.SSS1.p3.2.m2.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS1.p3.2.m2.1.1.1" xref="S4.SS2.SSS1.p3.2.m2.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS1.p3.2.m2.1.1.3" xref="S4.SS2.SSS1.p3.2.m2.1.1.3c.cmml"><a href="#S5.SS3" title="5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.3</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.2.m2.1b"><apply id="S4.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1"><times id="S4.SS2.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1.1"></times><ci id="S4.SS2.SSS1.p3.2.m2.1.1.2.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1.2">§</ci><ci id="S4.SS2.SSS1.p3.2.m2.1.1.3c.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS1.p3.2.m2.1.1.3.cmml" xref="S4.SS2.SSS1.p3.2.m2.1.1.3"><a href="#S5.SS3" title="5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.3</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.2.m2.1c">\S\ref{subsec:bias}</annotation></semantics></math>), which makes <span id="S4.SS2.SSS1.p3.2.6" class="ltx_text ltx_font_italic">q-FedAvg</span> less effective in ensuring fairness.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2006.06983/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="175" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The training performance of <span id="S4.F5.3.1" class="ltx_text ltx_font_italic">FedProx</span> and <span id="S4.F5.4.2" class="ltx_text ltx_font_italic">FedAvg</span> with and without heterogeneity.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p"><math id="S4.SS2.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS1.p4.1.m1.1a"><mo id="S4.SS2.SSS1.p4.1.m1.1.1" xref="S4.SS2.SSS1.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p4.1.m1.1b"><ci id="S4.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S4.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">FedProx<span id="S4.SS2.SSS1.p4.1.1.1" class="ltx_text ltx_font_upright"> is less effective in improving the training process with heterogeneity considered.</span></span>
According to Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2.1. Aggregation Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, on M-Type, <span id="S4.SS2.SSS1.p4.1.2" class="ltx_text ltx_font_italic">FedProx</span> only slightly outperforms <span id="S4.SS2.SSS1.p4.1.3" class="ltx_text ltx_font_italic">FedAvg</span>, and the heterogeneity causes an accuracy drop of 7.5%.
On Femnist, <span id="S4.SS2.SSS1.p4.1.4" class="ltx_text ltx_font_italic">FedProx</span> achieves the same performance as <span id="S4.SS2.SSS1.p4.1.5" class="ltx_text ltx_font_italic">FedAvg</span> under heterogeneity-unaware settings and slightly underperforms <span id="S4.SS2.SSS1.p4.1.6" class="ltx_text ltx_font_italic">FedAvg</span> under heterogeneity-aware settings. The heterogeneity causes an accuracy drop of 1.2%.
Note that <span id="S4.SS2.SSS1.p4.1.7" class="ltx_text ltx_font_italic">FedProx</span> incorporates hardware heterogeneity into its design while leaving state heterogeneity unsolved.
We manually check the involved devices and find that only 51.3% devices have attended the training when the model reaches the target accuracy.
As a result, the model may have been dominated by these active devices and perform badly on other devices.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span><span id="S4.SS2.SSS2.1.1" class="ltx_text ltx_font_bold">Gradient Compression Algorithms</span>
</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.2" class="ltx_p">The cost of device-server communication is often reported as a major bottleneck in FL <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>, so we first investigate gradient compression algorithms that are extensively studied to reduce the communication cost.
Specifically, we focus on three well-adopted gradient compression algorithms: <span id="S4.SS2.SSS2.p1.2.1" class="ltx_text ltx_font_italic">Structured Updates</span> <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2016b</a>)</cite>, <span id="S4.SS2.SSS2.p1.2.2" class="ltx_text ltx_font_italic">Gradient Dropping</span> (<span id="S4.SS2.SSS2.p1.2.3" class="ltx_text ltx_font_italic">GDrop</span>) <cite class="ltx_cite ltx_citemacro_citep">(Aji and Heafield, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>, and <span id="S4.SS2.SSS2.p1.2.4" class="ltx_text ltx_font_italic">SignSGD</span> <cite class="ltx_cite ltx_citemacro_citep">(Bernstein et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>.
For each of them, we tune the hyper-parameters to achieve the highest accuracy through massive experiments. As a result,
for <span id="S4.SS2.SSS2.p1.2.5" class="ltx_text ltx_font_italic">Structured Updates</span>, we set the max rank of the decomposited matrix to 100;
for <span id="S4.SS2.SSS2.p1.2.6" class="ltx_text ltx_font_italic">GDrop</span>, we set the weights dropout threshold to 0.005;
for <span id="S4.SS2.SSS2.p1.2.7" class="ltx_text ltx_font_italic">SignSGD</span>, we set the learning rate to 0.001, the momentum constant to 0, and the weight decay to 0. We use <span id="S4.SS2.SSS2.p1.2.8" class="ltx_text ltx_font_italic">FedAvg</span> with no compression as the baseline for comparison.
Besides accuracy and training time/rounds, we also use compression ratio (described in <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mi mathvariant="normal" id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><ci id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">\S</annotation></semantics></math><a href="#S3.SS4" title="3.4. Experimental Settings ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>) as the measurement metrics of these algorithms. We present the metric values of the three compression algorithms as well as the baseline under heterogeneity-unaware and heterogeneity-aware settings in Table <a href="#S4.T4" title="Table 4 ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Similar to <math id="S4.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\S\ref{subsec:aggr}" display="inline"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.SSS2.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS2.p1.2.m2.1.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p1.2.m2.1.1.3" xref="S4.SS2.SSS2.p1.2.m2.1.1.3c.cmml"><a href="#S4.SS2.SSS1" title="4.2.1. Aggregation Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><apply id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"><times id="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.1"></times><ci id="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.2">§</ci><ci id="S4.SS2.SSS2.p1.2.m2.1.1.3c.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.3"><a href="#S4.SS2.SSS1" title="4.2.1. Aggregation Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">\S\ref{subsec:aggr}</annotation></semantics></math>, we report only the results on Femnist and M-Type.
We summarize our findings as follows.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p2.4" class="ltx_p"><math id="S4.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p2.1.m1.1a"><mo id="S4.SS2.SSS2.p2.1.m1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.1b"><ci id="S4.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.1c">\bullet</annotation></semantics></math> <span id="S4.SS2.SSS2.p2.4.1" class="ltx_text ltx_font_bold">Heterogeneity introduces a similar accuracy drop to compression algorithms as it does to the basic algorithm.</span>
We measure the accuracy change introduced by heterogeneity (noted as <span id="S4.SS2.SSS2.p2.4.2" class="ltx_text ltx_font_italic">Acc. Change</span> in Table <a href="#S4.T4" title="Table 4 ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
We observe that the introduced accuracy degradation (<math id="S4.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="3.1\%" display="inline"><semantics id="S4.SS2.SSS2.p2.2.m2.1a"><mrow id="S4.SS2.SSS2.p2.2.m2.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.p2.2.m2.1.1.2" xref="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml">3.1</mn><mo id="S4.SS2.SSS2.p2.2.m2.1.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.2.m2.1b"><apply id="S4.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.2">3.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.2.m2.1c">3.1\%</annotation></semantics></math> on average) is similar to the one (<math id="S4.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="3.2\%" display="inline"><semantics id="S4.SS2.SSS2.p2.3.m3.1a"><mrow id="S4.SS2.SSS2.p2.3.m3.1.1" xref="S4.SS2.SSS2.p2.3.m3.1.1.cmml"><mn id="S4.SS2.SSS2.p2.3.m3.1.1.2" xref="S4.SS2.SSS2.p2.3.m3.1.1.2.cmml">3.2</mn><mo id="S4.SS2.SSS2.p2.3.m3.1.1.1" xref="S4.SS2.SSS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.3.m3.1b"><apply id="S4.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1.2">3.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.3.m3.1c">3.2\%</annotation></semantics></math> on average) that we observe in <math id="S4.SS2.SSS2.p2.4.m4.1" class="ltx_Math" alttext="\S\ref{subsec:basic}" display="inline"><semantics id="S4.SS2.SSS2.p2.4.m4.1a"><mrow id="S4.SS2.SSS2.p2.4.m4.1.1" xref="S4.SS2.SSS2.p2.4.m4.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.SSS2.p2.4.m4.1.1.2" xref="S4.SS2.SSS2.p2.4.m4.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS2.p2.4.m4.1.1.1" xref="S4.SS2.SSS2.p2.4.m4.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p2.4.m4.1.1.3" xref="S4.SS2.SSS2.p2.4.m4.1.1.3c.cmml"><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.4.m4.1b"><apply id="S4.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p2.4.m4.1.1"><times id="S4.SS2.SSS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.SSS2.p2.4.m4.1.1.1"></times><ci id="S4.SS2.SSS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.SSS2.p2.4.m4.1.1.2">§</ci><ci id="S4.SS2.SSS2.p2.4.m4.1.1.3c.cmml" xref="S4.SS2.SSS2.p2.4.m4.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.SSS2.p2.4.m4.1.1.3"><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.4.m4.1c">\S\ref{subsec:basic}</annotation></semantics></math>.
On average, the accuracy drops by 1.7% on Femnist and 5.3% on M-Type.
It is reasonable because heterogeneity will not affect the compressed gradients.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p3.5" class="ltx_p"><math id="S4.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.SS2.SSS2.p3.1.m1.1a"><mo id="S4.SS2.SSS2.p3.1.m1.1.1" xref="S4.SS2.SSS2.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.1.m1.1b"><ci id="S4.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S4.SS2.SSS2.p3.5.1" class="ltx_text ltx_font_bold">Gradient compression algorithms can hardly speed up the model convergence under heterogeneity-aware settings.</span>
Although all these algorithms compress the gradients and reduce the communication cost significantly (the compression ratio ranges from 0.1% to 39.4%), the training time is seldom shortened (only <span id="S4.SS2.SSS2.p3.5.2" class="ltx_text ltx_font_italic">Structured Updates</span> shortens the convergence time to 0.93<math id="S4.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS2.p3.2.m2.1a"><mo id="S4.SS2.SSS2.p3.2.m2.1.1" xref="S4.SS2.SSS2.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.2.m2.1b"><times id="S4.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.2.m2.1c">\times</annotation></semantics></math> at most) and lengthened in most cases.
For example, on M-Type under heterogeneity-aware environment, the training time is lengthened by 1.3<math id="S4.SS2.SSS2.p3.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS2.p3.3.m3.1a"><mo id="S4.SS2.SSS2.p3.3.m3.1.1" xref="S4.SS2.SSS2.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.3.m3.1b"><times id="S4.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.3.m3.1c">\times</annotation></semantics></math> to 2.5<math id="S4.SS2.SSS2.p3.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.SSS2.p3.4.m4.1a"><mo id="S4.SS2.SSS2.p3.4.m4.1.1" xref="S4.SS2.SSS2.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.4.m4.1b"><times id="S4.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.4.m4.1c">\times</annotation></semantics></math> for all compression algorithms.
The training time has not been shortened for two reasons.
First, we find that communication accounts for only a small portion of the total learning time compared to on-device training.
Most devices can finish the downloading and uploading in less than 30 seconds for a model around 50M while spending more time (1-5 minutes with 5 epochs) on training.
Second, the accuracy increases slowly when the gradients are compressed and the heterogeneity is introduced (refer to <math id="S4.SS2.SSS2.p3.5.m5.1" class="ltx_Math" alttext="\S\ref{subsec:basic}" display="inline"><semantics id="S4.SS2.SSS2.p3.5.m5.1a"><mrow id="S4.SS2.SSS2.p3.5.m5.1.1" xref="S4.SS2.SSS2.p3.5.m5.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.SSS2.p3.5.m5.1.1.2" xref="S4.SS2.SSS2.p3.5.m5.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS2.p3.5.m5.1.1.1" xref="S4.SS2.SSS2.p3.5.m5.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p3.5.m5.1.1.3" xref="S4.SS2.SSS2.p3.5.m5.1.1.3c.cmml"><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.5.m5.1b"><apply id="S4.SS2.SSS2.p3.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1"><times id="S4.SS2.SSS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.1"></times><ci id="S4.SS2.SSS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.2">§</ci><ci id="S4.SS2.SSS2.p3.5.m5.1.1.3c.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS2.p3.5.m5.1.1.3.cmml" xref="S4.SS2.SSS2.p3.5.m5.1.1.3"><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.5.m5.1c">\S\ref{subsec:basic}</annotation></semantics></math>), thus taking more rounds to reach the target accuracy.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2006.06983/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="114" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>The prevalence of different failure reasons. The optimal deadline (red line) refers to the one that achieves the shortest training time.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2006.06983/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>A breakdown of the impacts of different types of heterogeneity. State heterogeneity causes more performance degradation than hardware heterogeneity. “Heter” is short for heterogeneity.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Analysis of Impact Factors</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.3" class="ltx_p">Given the non-trivial negative impacts of heterogeneity shown in the previous section, we dive deeper to analyze the main factors of these impacts. In this section, we focus on <span id="S5.p1.3.1" class="ltx_text ltx_font_italic">FedAvg</span>, considering its wide usage in practical applications. Specifically, we first break down heterogeneity into two types, i.e., state heterogeneity and hardware heterogeneity, to analyze their individual impacts (<math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:breakdown}" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3c.cmml"><a href="#S5.SS1" title="5.1. Breakdown of Heterogeneity ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></times><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">§</ci><ci id="S5.p1.1.m1.1.1.3c.cmml" xref="S5.p1.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3"><a href="#S5.SS1" title="5.1. Breakdown of Heterogeneity ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\S\ref{subsec:breakdown}</annotation></semantics></math>). Then we report two phenomena that are particularly obvious under heterogeneity-aware settings according to our experiments: (1) selected devices can fail to upload their model updates for several reasons, which we call <span id="S5.p1.3.2" class="ltx_text ltx_font_italic">device failure</span> (<math id="S5.p1.2.m2.1" class="ltx_Math" alttext="\S\ref{subsec:failure}" display="inline"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3c.cmml"><a href="#S5.SS2" title="5.2. Device Failure ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.2</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><times id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></times><ci id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">§</ci><ci id="S5.p1.2.m2.1.1.3c.cmml" xref="S5.p1.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3"><a href="#S5.SS2" title="5.2. Device Failure ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.2</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\S\ref{subsec:failure}</annotation></semantics></math>); (2) devices that succeed in uploading still have biased contribution to the global model, which we call <span id="S5.p1.3.3" class="ltx_text ltx_font_italic">participant bias</span> (<math id="S5.p1.3.m3.1" class="ltx_Math" alttext="\S\ref{subsec:bias}" display="inline"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mi mathvariant="normal" id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S5.p1.3.m3.1.1.1" xref="S5.p1.3.m3.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3c.cmml"><a href="#S5.SS3" title="5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.3</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><times id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1"></times><ci id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">§</ci><ci id="S5.p1.3.m3.1.1.3c.cmml" xref="S5.p1.3.m3.1.1.3"><mtext class="ltx_mathvariant_italic" id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3"><a href="#S5.SS3" title="5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.3</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">\S\ref{subsec:bias}</annotation></semantics></math>).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Breakdown of Heterogeneity</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">The preceding results indicate the joint impacts from two types of heterogeneity.
To analyze their individual impact, we disable the hardware heterogeneity, i.e., all the devices have the same computational and communication capacity (noted as “w/o hardware heter”).
Similarly, we disable the state heterogeneity, i.e., devices are always available at any time and will not drop out (noted as “w/o state heter”).
We show the accuracy changes with the training time in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2.2. Gradient Compression Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.5" class="ltx_p"><math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mo id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS1.p2.5.1" class="ltx_text ltx_font_bold">Both state heterogeneity and hardware heterogeneity slow down the model convergence.</span> According to Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2.2. Gradient Compression Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, state heterogeneity leads to comparable increase of training time to hardware heterogeneity, i.e., 1.72<math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mo id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><times id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">\times</annotation></semantics></math> vs. 1.26<math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mo id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><times id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">\times</annotation></semantics></math> on M-Type and 2.34<math id="S5.SS1.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.4.m4.1a"><mo id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><times id="S5.SS1.p2.4.m4.1.1.cmml" xref="S5.SS1.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">\times</annotation></semantics></math> vs. 2.62<math id="S5.SS1.p2.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.5.m5.1a"><mo id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><times id="S5.SS1.p2.5.m5.1.1.cmml" xref="S5.SS1.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">\times</annotation></semantics></math> on Femnist. It is reasonable because both drop-out (introduced by state heterogeneity) and low-end devices (introduced by hardware heterogeneity) affect the training time.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.3" class="ltx_p"><math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mo id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS1.p3.3.1" class="ltx_text ltx_font_bold">State heterogeneity is more influential than hardware heterogeneity on the model accuracy.</span>
As shown in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2.2. Gradient Compression Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, state heterogeneity leads to a more significant accuracy drop than hardware heterogeneity, i.e., 9.5% vs. 0.4% on M-Type and 1.1% vs. 0.1% on Femnist.
Note that existing FL-related studies usually ignore state heterogeneity and only a small amount of work <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>; Nishio and
Yonetani, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Laguel et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>; Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> explores hardware heterogeneity (refer to <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="\S\ref{sec:background}" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3c.cmml"><a href="#S2" title="2. Background and related work ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">2</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><times id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></times><ci id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">§</ci><ci id="S5.SS1.p3.2.m2.1.1.3c.cmml" xref="S5.SS1.p3.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3"><a href="#S2" title="2. Background and related work ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">2</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">\S\ref{sec:background}</annotation></semantics></math>).
Our results show that state heterogeneity is more responsible for the model accuracy drop, which explains why <span id="S5.SS1.p3.3.2" class="ltx_text ltx_font_italic">FedProx</span> (it considers hardware heterogeneity) is less effective given both types of heterogeneity (refer to <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="\S\ref{subsec:aggr}" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mrow id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><mi mathvariant="normal" id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.3.m3.1.1.1" xref="S5.SS1.p3.3.m3.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S5.SS1.p3.3.m3.1.1.3" xref="S5.SS1.p3.3.m3.1.1.3c.cmml"><a href="#S4.SS2.SSS1" title="4.2.1. Aggregation Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><times id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1.1"></times><ci id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2">§</ci><ci id="S5.SS1.p3.3.m3.1.1.3c.cmml" xref="S5.SS1.p3.3.m3.1.1.3"><mtext class="ltx_mathvariant_italic" id="S5.SS1.p3.3.m3.1.1.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3"><a href="#S4.SS2.SSS1" title="4.2.1. Aggregation Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">\S\ref{subsec:aggr}</annotation></semantics></math>).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Device Failure</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">Device failure refers to the phenomenon that a selected device misses the deadline to upload the model updates in a round.
It can slow down the model convergence and cause a waste of valuable device resources (computations, energy, etc.).
However, device failure is seldom studied in prior work, probably because it is directly related to the FL heterogeneity.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Heuristically, we categorize device failure to three possible causes:
(1) <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Network failure</span> is detected if the device takes excessively long time (default: 3<math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mo id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><times id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\times</annotation></semantics></math> the average) to communicate with the server due to a slow or unreliable network connection.
(2) <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_bold ltx_font_italic">Interruption failure</span> is detected if the device fails to upload the model updates due to the user interruption, e.g., the device is uncharged during training.
(3) <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_bold ltx_font_italic">Training failure</span> refers to the case when the device takes too much time on training.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">To understand device failure, we zoom into the previous experiments under varied round deadlines. We vary the deadline because we find that the proportion of failed devices is greatly affected by it.
Similar to <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mi mathvariant="normal" id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">\S</annotation></semantics></math><a href="#S5.SS1" title="5.1. Breakdown of Heterogeneity ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>, we will also check hardware heterogeneity’s and state heterogeneity’s influence on device failure.
The key questions we want to answer here are: (1) how often the devices may fail and what the corresponding reasons for the failure are;
(2) and which type of heterogeneity is the major factor.
The results are illustrated in Figures <a href="#S4.F6" title="Figure 6 ‣ 4.2.2. Gradient Compression Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S5.F8" title="Figure 8 ‣ 5.2. Device Failure ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, from which we make the following key observations.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p"><math id="S5.SS2.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.p4.1.m1.1a"><mo id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><ci id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Heterogeneity introduces non-trivial device failure even when an optimal deadline setting is given.</span>
The overall proportion of the failed devices reaches 11.6% on average, with an optimal deadline setting that achieves the shortest training time.
A tight deadline increases the failure proportion because devices receive less time to finish their training tasks. We look into three types of failure and find that:
(1) Network failure accounts for a small fraction of device failure (typically less than 5%) and it is more stable than other types of failure.
(2) Interruption failure is affected by the deadline but in a moderate way.
We further break down the interruption failure into three sub-categories corresponding to three restrictions on training <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>)</cite>. Specifically, results show that the training process is interrupted by user interaction, battery charged-off, and network changes with a probability of 46.06%, 36.96%, and 17.78% respectively.
(3) Training failure is heavily affected by the deadline.
This type of failure accounts for the majority of the device failure when the deadline is set too tight.
Even with the optimal deadline setting, this type of failure still occurs because we observe that some low-end devices with too many local data sometimes fail to meet the deadline.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.1" class="ltx_p"><math id="S5.SS2.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS2.p5.1.m1.1a"><mo id="S5.SS2.p5.1.m1.1.1" xref="S5.SS2.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.1.m1.1b"><ci id="S5.SS2.p5.1.m1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">Hardware heterogeneity leads to more device failure than state heterogeneity.</span>
According to Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2. Device Failure ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, hardware heterogeneity is more responsible for the device failure.
For example, on M-Type, hardware heterogeneity causes 14% failed devices on average while state heterogeneity causes only 2.5%.
It is probably because when hardware heterogeneity is considered, there are low-end devices that suffer longer training time.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2006.06983/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Different kinds of heterogeneity’s influence on device failure.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Participant Bias</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Participant bias refers to the phenomenon that devices do not participate in FL with the same probability.
It can lead to different contributions to the global model, thus making some devices under-represented.
Due to state heterogeneity, devices frequently used by users are less likely to check in.
Due to hardware heterogeneity, low-end devices are less likely to upload their updates to the central server.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.2" class="ltx_p">To measure the participant bias introduced by heterogeneity, we run the same FL tasks in <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mi mathvariant="normal" id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><ci id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">\S</annotation></semantics></math><a href="#S4.SS1" title="4.1. Impacts on Basic Algorithm’s Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
We take the amount of computation to reflect the participation degree of different devices.
Since it is difficult to compare the computation of different models directly, we divide them by the amount of computation for a training epoch (noted as computation loads).
Figure <a href="#S5.F9" title="Figure 9 ‣ 5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> illustrates the distribution of computation loads across devices when the global model reaches the target accuracy.
Similar to <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mi mathvariant="normal" id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><ci id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">\S</annotation></semantics></math><a href="#S5.SS1" title="5.1. Breakdown of Heterogeneity ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>, we also break down to explore the impacts of different types of heterogeneity.
We summary our findings as follows.</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2006.06983/assets/x9.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>The distribution of computations across devices during FL training.</figcaption>
</figure>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2006.06983/assets/x10.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="203" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>A breakdown of the impacts of different types of Heterogeneity on participant bias.</figcaption>
</figure>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.4" class="ltx_p"><math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><mo id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><ci id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">\bullet</annotation></semantics></math>
<span id="S5.SS3.p3.4.1" class="ltx_text ltx_font_bold">The computation loads get more uneven under heterogeneity-aware settings.</span>
The variance is increased by 2.4<math id="S5.SS3.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS3.p3.2.m2.1a"><mo id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><times id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">\times</annotation></semantics></math> (Reddit) to 10.7<math id="S5.SS3.p3.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS3.p3.3.m3.1a"><mo id="S5.SS3.p3.3.m3.1.1" xref="S5.SS3.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.1b"><times id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.1c">\times</annotation></semantics></math> (Femnist).
Compared to heterogeneity-unaware environment where every device participates with an equal probability, in the heterogeneity-aware environment, the computation loads have a trend of polarization.
On Celeba, the maximum computation load increases by 1.17<math id="S5.SS3.p3.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS3.p3.4.m4.1a"><mo id="S5.SS3.p3.4.m4.1.1" xref="S5.SS3.p3.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.4.m4.1b"><times id="S5.SS3.p3.4.m4.1.1.cmml" xref="S5.SS3.p3.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.4.m4.1c">\times</annotation></semantics></math>.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.p4.1" class="ltx_p"><math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mo id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><ci id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">\bullet</annotation></semantics></math>
<span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_bold">The number of inactive devices increases significantly under heterogeneity-aware settings.</span>
The median computation load drops by 28% (Femnist) to 75% (Reddit), indicating that more inactive devices appear.
Compared to the heterogeneity-unaware environment where top 30% of the devices contribute 54% of the total computation, in the heterogeneity-aware environment, top 30% of the devices contribute 81% of the total computation, putting the inactive devices at a disadvantage.</p>
</div>
<figure id="S5.F11" class="ltx_figure ltx_align_floatright"><img src="/html/2006.06983/assets/x11.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="364" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Percentage of participating devices over time.</figcaption>
</figure>
<div id="S5.SS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.p5.1" class="ltx_p"><math id="S5.SS3.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS3.p5.1.m1.1a"><mo id="S5.SS3.p5.1.m1.1.1" xref="S5.SS3.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.1.m1.1b"><ci id="S5.SS3.p5.1.m1.1.1.cmml" xref="S5.SS3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.1.m1.1c">\bullet</annotation></semantics></math>
<span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">Up to 30% devices have not participated in FL process when the global model reaches the target accuracy under heterogeneity-aware settings.</span>
To investigate the reasons for these inactive devices, we inspect the percentage of participating devices over time and demonstrate the result in Figure <a href="#S5.F11" title="Figure 11 ‣ 5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.
We find that when the model reaches the target accuracy (6-24 hours in our experiments), more than 30% devices have not participated.
In the heterogeneity-unaware environment, the participating devices accumulate quickly and soon cover the total population in 12 hours.
While in heterogeneity-aware environment, the accumulation speed gets much slower and it takes much longer time to converge (more than 48 hours).</p>
</div>
<div id="S5.SS3.p6" class="ltx_para ltx_noindent">
<p id="S5.SS3.p6.2" class="ltx_p"><math id="S5.SS3.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.SS3.p6.1.m1.1a"><mo id="S5.SS3.p6.1.m1.1.1" xref="S5.SS3.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.1.m1.1b"><ci id="S5.SS3.p6.1.m1.1.1.cmml" xref="S5.SS3.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.1.m1.1c">\bullet</annotation></semantics></math>
<span id="S5.SS3.p6.2.1" class="ltx_text ltx_font_bold">State heterogeneity is more responsible for participant bias.</span>
As shown in Figure <a href="#S5.F10" title="Figure 10 ‣ 5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, state heterogeneity is the main reason for computation bias.
It causes the similar computation distribution as the one in heterogeneity-aware environment.
It is probably because state heterogeneity introduces bias in device selection, i.e., although the server selects devices randomly, the available devices that can be selected highly depend on if the device can meet the state criteria (refer to <math id="S5.SS3.p6.2.m2.1" class="ltx_Math" alttext="\S\ref{subsec:datasets}" display="inline"><semantics id="S5.SS3.p6.2.m2.1a"><mrow id="S5.SS3.p6.2.m2.1.1" xref="S5.SS3.p6.2.m2.1.1.cmml"><mi mathvariant="normal" id="S5.SS3.p6.2.m2.1.1.2" xref="S5.SS3.p6.2.m2.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p6.2.m2.1.1.1" xref="S5.SS3.p6.2.m2.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S5.SS3.p6.2.m2.1.1.3" xref="S5.SS3.p6.2.m2.1.1.3c.cmml"><a href="#S3.SS2.SSS1" title="3.2.1. IMA dataset ‣ 3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">3.2.1</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.2.m2.1b"><apply id="S5.SS3.p6.2.m2.1.1.cmml" xref="S5.SS3.p6.2.m2.1.1"><times id="S5.SS3.p6.2.m2.1.1.1.cmml" xref="S5.SS3.p6.2.m2.1.1.1"></times><ci id="S5.SS3.p6.2.m2.1.1.2.cmml" xref="S5.SS3.p6.2.m2.1.1.2">§</ci><ci id="S5.SS3.p6.2.m2.1.1.3c.cmml" xref="S5.SS3.p6.2.m2.1.1.3"><mtext class="ltx_mathvariant_italic" id="S5.SS3.p6.2.m2.1.1.3.cmml" xref="S5.SS3.p6.2.m2.1.1.3"><a href="#S3.SS2.SSS1" title="3.2.1. IMA dataset ‣ 3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">3.2.1</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.2.m2.1c">\S\ref{subsec:datasets}</annotation></semantics></math>).</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Implications</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this section, we discuss actionable implications for FL algorithm designers and FL system providers based on our above findings.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>For FL Algorithm Designers</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_bold">Taking heterogeneity into consideration.</span> As demonstrated in our study, heterogeneity introduces non-trivial accuracy drop and training slowdown in FL, as well as affects the effectiveness of some proposed methods.
These findings encourage researchers to consider heterogeneity, especially state heterogeneity, when they practice on FL.
On the one hand, when designing approaches or algorithms, researchers should consider circumstances that are common in heterogeneity-aware environment but do not exist in heterogeneity-unaware environment.
For example, when designing a device selection approach, researchers should be aware that some devices can be unavailable at a given time and the server cannot select as it wants.
When designing an aggregation algorithm, researchers should guarantee that the algorithm still works given inevitable device failure.
On the other hand, when evaluating FL algorithms, researchers should add necessary heterogeneity settings in the experiments according to the targeted scenario.
For example, additional system overhead of the algorithm may further widen the gap in training time between different devices, which should be considered during the evaluation.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p"><span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_bold">Reducing device failure by a “proactive alerting” technique.</span>
In <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:failure}" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S6.SS1.p2.1.m1.1.1.3" xref="S6.SS1.p2.1.m1.1.1.3c.cmml"><a href="#S5.SS2" title="5.2. Device Failure ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.2</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><times id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1.1"></times><ci id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2">§</ci><ci id="S6.SS1.p2.1.m1.1.1.3c.cmml" xref="S6.SS1.p2.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S6.SS1.p2.1.m1.1.1.3.cmml" xref="S6.SS1.p2.1.m1.1.1.3"><a href="#S5.SS2" title="5.2. Device Failure ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.2</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">\S\ref{subsec:failure}</annotation></semantics></math>, we find that around 10% of devices fail to upload their model updates under typical settings.
The reasons include excessive training time, unstable network, and device drop-out caused by state changes.
Existing efforts have explored dynamic deadline <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> and tolerating partial work <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite> to handle the device failure.
However, these algorithms are inadequate to handle the failure caused by unstable network and drop-out because they are highly dependent on the device’s states.
One may explore a “proactive alerting” technique by predicting the device’s future states and network condition based on historical data.
The server should assign a low priority to the devices that are likely to drop out.
In this way, the overall device failure can be reduced and more updates can be aggregated thus saving the hardware resource and accelerating learning process.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p"><span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_bold">Resolving bias in device selections.</span>
In <math id="S6.SS1.p3.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:bias}" display="inline"><semantics id="S6.SS1.p3.1.m1.1a"><mrow id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S6.SS1.p3.1.m1.1.1.2" xref="S6.SS1.p3.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p3.1.m1.1.1.1" xref="S6.SS1.p3.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S6.SS1.p3.1.m1.1.1.3" xref="S6.SS1.p3.1.m1.1.1.3c.cmml"><a href="#S5.SS3" title="5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.3</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><apply id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"><times id="S6.SS1.p3.1.m1.1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1.1"></times><ci id="S6.SS1.p3.1.m1.1.1.2.cmml" xref="S6.SS1.p3.1.m1.1.1.2">§</ci><ci id="S6.SS1.p3.1.m1.1.1.3c.cmml" xref="S6.SS1.p3.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S6.SS1.p3.1.m1.1.1.3.cmml" xref="S6.SS1.p3.1.m1.1.1.3"><a href="#S5.SS3" title="5.3. Participant Bias ‣ 5. Analysis of Impact Factors ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">5.3</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\S\ref{subsec:bias}</annotation></semantics></math>, we find that the global model is dominated by some active devices (top 30% of devices can contribute 81% of the total computation).
The reason is that, due to state heterogeneity, devices do not participate in the learning process with the same probability even when they are randomly selected, and some (more than 30% in our experiments) have never participated when the model reaches a local optimum.
To alleviate the bias in device selection, a naive approach is to set a participation time window (e.g., one day) and omit the devices that have participated in this window.
The “fairness” is guaranteed, but this may remarkably increase the training time of an FL task, and the length of the time window should be carefully tuned.
What is more, adjusting the local objective (loss function) or re-weighting updates can be possible alternatives.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>For FL System Providers</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS2.p1.1" class="ltx_p"><span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_bold">Building heterogeneity-aware platforms.</span>
Our results show that a heterogeneity-aware platform is necessary for developers to precisely understand how their model shall perform in real-world settings.
However, existing platforms <cite class="ltx_cite ltx_citemacro_citep">(Tensorflow, <a href="#bib.bib45" title="" class="ltx_ref">2020</a>; Ryffel et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2018</a>; Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>; PaddlePaddle, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; He et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> fail to incorporate heterogeneity into their design.
Our work provides a reference implementation and can be easily integrated into these FL platforms.
We also encourage system providers to collect their own data that fit different scenarios to further help the FL community.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold">Optimizing on-device training time, instead of optimizing compression in unmetered (e.g., WiFi) networks.</span>
In <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:grad_comp}" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mrow id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S6.SS2.p2.1.m1.1.1.1" xref="S6.SS2.p2.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S6.SS2.p2.1.m1.1.1.3" xref="S6.SS2.p2.1.m1.1.1.3c.cmml"><a href="#S4.SS2.SSS2" title="4.2.2. Gradient Compression Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2.2</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><times id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1.1"></times><ci id="S6.SS2.p2.1.m1.1.1.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2">§</ci><ci id="S6.SS2.p2.1.m1.1.1.3c.cmml" xref="S6.SS2.p2.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S6.SS2.p2.1.m1.1.1.3.cmml" xref="S6.SS2.p2.1.m1.1.1.3"><a href="#S4.SS2.SSS2" title="4.2.2. Gradient Compression Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">4.2.2</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">\S\ref{subsec:grad_comp}</annotation></semantics></math>, we find that gradient compression algorithms can hardly speed up model convergence.
The time spent on communication is relatively small in the WiFi environment, compared to the time spent in training.
As a result, an orthogonal way to accelerate FL is to optimize the on-device training time.
Possible solutions include neural architecture search (NAS) <cite class="ltx_cite ltx_citemacro_citep">(Elsken
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2019</a>; Wistuba
et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2019</a>)</cite> and using hardware AI accelerators like mobile GPU and digital signal processor (DSP).</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Discussion</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">We next discuss open problems along with generalizability of our study.</p>
</div>
<figure id="S7.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F13.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:203.8pt;"><img src="/html/2006.06983/assets/x12.png" id="S7.F13.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="360" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>Percentage of available devices over time.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F13.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:203.8pt;"><img src="" id="S7.F13.2.g1" class="ltx_graphics ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>Decoupling is verified on M-Type.</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Bias of our IMA dataset.</span>
The device state traces (<math id="S7.p2.1.m1.1" class="ltx_Math" alttext="\S" display="inline"><semantics id="S7.p2.1.m1.1a"><mi mathvariant="normal" id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml">§</mi><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><ci id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1">§</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">\S</annotation></semantics></math><a href="#S3.SS2.SSS1" title="3.2.1. IMA dataset ‣ 3.2. The Datasets ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) are collected from our IMA (app-specific) whose users mainly reside in Southeast Asia and Latin America (geo-specific).
The traces may not be fully representative to other FL scenarios.
However, we believe that our findings are still faithful because
(1) FL task is always app-specific and improving IMA experience is a key scenario of FL <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2018</a>; Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>; Hard et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>;
(2) our traces are large enough to cover the general state change patterns of smartphones. What is more, the patterns are consistent with prior work <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2018</a>)</cite> as aforementioned.
Furthermore, new user traces can be seamlessly plugged into our platform where researchers can reproduce all experiments mentioned in this paper.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text ltx_font_bold">Consistency with results reported by real-world FL systems.</span>
Similar to all existing FL platforms <cite class="ltx_cite ltx_citemacro_citep">(Tensorflow, <a href="#bib.bib45" title="" class="ltx_ref">2020</a>; Ryffel et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2018</a>; Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>; PaddlePaddle, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; He et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>, our platform (<math id="S7.p3.1.m1.1" class="ltx_Math" alttext="\S\ref{subsec:fl-runtime}" display="inline"><semantics id="S7.p3.1.m1.1a"><mrow id="S7.p3.1.m1.1.1" xref="S7.p3.1.m1.1.1.cmml"><mi mathvariant="normal" id="S7.p3.1.m1.1.1.2" xref="S7.p3.1.m1.1.1.2.cmml">§</mi><mo lspace="0em" rspace="0em" id="S7.p3.1.m1.1.1.1" xref="S7.p3.1.m1.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_italic" id="S7.p3.1.m1.1.1.3" xref="S7.p3.1.m1.1.1.3c.cmml"><a href="#S3.SS3" title="3.3. The Simulation Platform ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">3.3</span></a></mtext></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.1.m1.1b"><apply id="S7.p3.1.m1.1.1.cmml" xref="S7.p3.1.m1.1.1"><times id="S7.p3.1.m1.1.1.1.cmml" xref="S7.p3.1.m1.1.1.1"></times><ci id="S7.p3.1.m1.1.1.2.cmml" xref="S7.p3.1.m1.1.1.2">§</ci><ci id="S7.p3.1.m1.1.1.3c.cmml" xref="S7.p3.1.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S7.p3.1.m1.1.1.3.cmml" xref="S7.p3.1.m1.1.1.3"><a href="#S3.SS3" title="3.3. The Simulation Platform ‣ 3. The Measurement Approach ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref ltx_font_italic"><span class="ltx_text ltx_ref_tag">3.3</span></a></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.1.m1.1c">\S\ref{subsec:fl-runtime}</annotation></semantics></math>) performs FL tasks in a simulation way.
We carefully design the platform to simulate the real-world FL systems by considering the heterogeneity. However, we acknowledge that a gap may still exist for unexpected FL glitches, e.g., software failure.
We plan to further validate our platform with real-world deployment.
Nevertheless, the observed patterns from our platform, e.g., device availability (Figure <a href="#S7.F13" title="Figure 13 ‣ 7. Discussion ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>) and failure proportion (Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2.2. Gradient Compression Algorithms ‣ 4.2. Impacts on Advanced Algorithms’ Performance ‣ 4. Results ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), are consistent with the results reported from a large-scale FL deployment by Google <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019a</a>)</cite>.
Therefore, we believe that our findings are still valid.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p"><span id="S7.p4.1.1" class="ltx_text ltx_font_bold">Validity of randomly assigning state traces and training data to devices.</span>
In practice, the heterogeneity is inherently coupled with the non-IID data distribution <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>.
In this study, we decouple the heterogeneity from the data distribution, i.e., randomly assigning a state trace to each device, to generalize our traces to other benchmark datasets.
We use M-Type to verify this design because it shares the same user population with our traces.
According to Figure <a href="#S7.F13" title="Figure 13 ‣ 7. Discussion ‣ Characterizing Impacts of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, the gap between the coupled case and the decoupled case is trivial compared to the gap between the heterogeneity-unaware and heterogeneity-aware settings.
It justifies our design to decouple heterogeneity from any third-party datasets.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p"><span id="S7.p5.1.1" class="ltx_text ltx_font_bold">Other types of heterogeneity.</span>
In this paper, we focus on the impacts of hardware and state heterogeneity. In fact, there also exist other types of heterogeneity in FL. One is data heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>; Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> that resides in the skewed and unbalanced local data distribution (non-IID data distribution) across devices. Data heterogeneity is one of the basic assumptions in FL and existing work <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017a</a>; Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2016a</a>; Chai et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> has conducted in-depth research on it. Since the benchmark datasets used in our experiments are all non-IID datasets, data heterogeneity is inherently considered in our study. Other types of heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>, like heterogeneity on software or platform, are highly relevant to the implementation of an FL system and hard to generalize. We plan to leave them for future work.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para ltx_noindent">
<p id="S8.p1.1" class="ltx_p">We have collected large-scale real-world data and conducted extensive experiments to first anatomize the impacts of heterogeneity. Results show that
(1) heterogeneity causes non-trivial performance degradation in FL tasks, up to 9.2% accuracy drop and 2.32<math id="S8.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.p1.1.m1.1a"><mo id="S8.p1.1.m1.1.1" xref="S8.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.p1.1.m1.1b"><times id="S8.p1.1.m1.1.1.cmml" xref="S8.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.1.m1.1c">\times</annotation></semantics></math> convergence slowdown;
(2) recent advanced FL algorithms can be compromised and rethought with heterogeneity considered;
(3) state heterogeneity, which is usually ignored in existing studies, is more responsible for the aforementioned performance degradation;
(4) device failure and participant bias are two potential impact factors of performance degradation.
These results suggest that heterogeneity should be taken into consideration in further research work and that optimizations to mitigate the negative impacts of heterogeneity are promising.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by the National Key Research and Development Program of China under the grant number 2020YFB2104100, the National Natural Science Foundation of China under grant numbers 61725201, J1924032, and 62032003, the Beijing Outstanding Young Scientist Program under the grant number BJJWZYJH 01201910001004, and the Alibaba-PKU Joint Research Program.
Mengwei Xu was supported by the NSFC under the grant number 61921003.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aji and Heafield (2017)</span>
<span class="ltx_bibblock">
Alham Fikri Aji and
Kenneth Heafield. 2017.

</span>
<span class="ltx_bibblock">Sparse communication for distributed gradient
descent. In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen,
Denmark, September 9-11, 2017</em>, Martha
Palmer, Rebecca Hwa, and Sebastian
Riedel (Eds.). 440–445.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Archer et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Aaron Archer, Silvio
Lattanzi, Peter Likarish, and Sergei
Vassilvitskii. 2017.

</span>
<span class="ltx_bibblock">Indexing public-private graphs. In
<em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th International Conference on
World Wide Web, WWW 2017</em>. 1461–1470.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan,
Andreas Veit, Yiqing Hua,
Deborah Estrin, and Vitaly Shmatikov.
2018.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.00459</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bernstein et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jeremy Bernstein, Jiawei
Zhao, Kamyar Azizzadenesheli, and Anima
Anandkumar. 2019.

</span>
<span class="ltx_bibblock">signSGD with majority vote is communication
efficient and fault tolerant. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of
7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert
Eichner, Wolfgang Grieskamp, Dzmitry
Huba, Alex Ingerman, Vladimir Ivanov,
Chloé Kiddon, Jakub Konecný,
Stefano Mazzocchi, Brendan McMahan,
Timon Van Overveldt, David Petrou,
Daniel Ramage, and Jason Roselander.
2019a.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: system
design. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and
Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir
Ivanov, Ben Kreuter, Antonio Marcedone,
H Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and
Karn Seth. 2017.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving
machine learning. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em>.
1175–1191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Fariborz
Salehi, Jakub Konečnỳ, Brendan
McMahan, and Marco Gruteser.
2019b.

</span>
<span class="ltx_bibblock">Federated learning with autotuned
communication-efficient secure aggregation.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.00131</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Peter
Wu, Tian Li, Jakub Konečnỳ,
H Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. 2018.

</span>
<span class="ltx_bibblock">Leaf: a benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01097</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Zheng Chai, Hannan
Fayyaz, Zeshan Fayyaz, Ali Anwar,
Yi Zhou, Nathalie Baracaldo,
Heiko Ludwig, and Yue Cheng.
2019.

</span>
<span class="ltx_bibblock">Towards taming the resource and data heterogeneity
in federated learning. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of 2019
USENIX Conference on Operational Machine Learning (OpML 19)</em>.
19–21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yang Chen, Xiaoyan Sun,
and Yaochu Jin. 2019.

</span>
<span class="ltx_bibblock">Communication-efficient federated deep learning
with layerwise asynchronous model update and temporally weighted
aggregation.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and
Learning Systems</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhenpeng Chen, Yanbin
Cao, Yuanqiang Liu, Haoyu Wang,
Tao Xie, and Xuanzhe Liu.
2020.

</span>
<span class="ltx_bibblock">A comprehensive study on challenges in deploying
deep learning based software. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the ACM Joint Meeting on European Software Engineering Conference and
Symposium on the Foundations of Software Engineering, ESEC/FSE 2020</em>.
750–762.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Zhenpeng Chen, Xuan Lu,
Wei Ai, Huoran Li,
Qiaozhu Mei, and Xuanzhe Liu.
2018.

</span>
<span class="ltx_bibblock">Through a gender lens: learning usage patterns of
emojis from large-scale Android users. In
<em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 World Wide Web Conference,
WWW 2018</em>. 763–772.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhenpeng Chen, Huihan
Yao, Yiling Lou, Yanbin Cao,
Yuanqiang Liu, Haoyu Wang, and
Xuanzhe Liu. 2021.

</span>
<span class="ltx_bibblock">An empirical study on deployment faults of deep
learning based mobile applications. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings
of the 43rd International Conference on Software Engineering, ICSE 2021</em>.
Accepted to appear.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen
et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Gregory Cohen, Saeed
Afshar, Jonathan Tapson, and Andre
Van Schaik. 2017.

</span>
<span class="ltx_bibblock">EMNIST: extending MNIST to handwritten letters. In
<em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of 2017 International Joint Conference
on Neural Networks (IJCNN)</em>. 2921–2926.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eclipse (2020)</span>
<span class="ltx_bibblock">
Eclipse. 2020.

</span>
<span class="ltx_bibblock">Deep Learning for Java.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://deeplearning4j.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://deeplearning4j.org/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed Mar 16, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elsken
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Thomas Elsken, Jan Hendrik
Metzen, and Frank Hutter.
2019.

</span>
<span class="ltx_bibblock">Neural architecture search: a survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>
20, 55 (2019),
1–21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka
Rao, Rajiv Mathews, Swaroop Ramaswamy,
Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé
Kiddon, and Daniel Ramage.
2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib19.3.3.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li,
Jinhyun So, Mi Zhang,
Hongyi Wang, Xiaoyang Wang,
Praneeth Vepakomma, Abhishek Singh,
Hang Qiu, Li Shen, et al<span id="bib.bib19.4.1" class="ltx_text">.</span>
2020.

</span>
<span class="ltx_bibblock">Fedml: a research library and benchmark for
federated machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ignatov et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andrey Ignatov, Radu
Timofte, William Chou, Ke Wang,
Max Wu, Tim Hartley, and
Luc Van Gool. 2018.

</span>
<span class="ltx_bibblock">Ai benchmark: running deep neural networks on
android smartphones. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
European Conference on Computer Vision (ECCV)</em>. 288–314.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yihan Jiang, Jakub
Konečnỳ, Keith Rush, and
Sreeram Kannan. 2019.

</span>
<span class="ltx_bibblock">Improving federated learning personalization via
model agnostic meta learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.12488</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz
et al<span id="bib.bib22.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary
Charles, Graham Cormode, Rachel
Cummings, et al<span id="bib.bib22.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.04977</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang
et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
J. Kang, S. Seo,
and J. W. Hong. 2011.

</span>
<span class="ltx_bibblock">Usage pattern analysis of smartphones. In
<em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of 2011 13th Asia-Pacific Network
Operations and Management Symposium</em>. 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2016a)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ,
H Brendan McMahan, Daniel Ramage, and
Peter Richtárik. 2016a.

</span>
<span class="ltx_bibblock">Federated optimization: distributed machine
learning for on-device intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2016b)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ,
H Brendan McMahan, Felix X Yu,
Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon.
2016b.

</span>
<span class="ltx_bibblock">Federated learning: strategies for improving
communication efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laguel et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yassine Laguel, Krishna
Pillutla, Jérôme Malick, and
Zaid Harchaoui. 2020.

</span>
<span class="ltx_bibblock">Device heterogeneity in federated learning: a
superquantile approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.11223</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Li Li, Haoyi Xiong,
Zhishan Guo, Jun Wang, and
Cheng-Zhong Xu. 2019.

</span>
<span class="ltx_bibblock">SmartPC: hierarchical pace control in real-time
federated learning system. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of 2019
IEEE Real-Time Systems Symposium (RTSS)</em>. 406–418.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu,
Ameet Talwalkar, and Virginia Smith.
2020a.

</span>
<span class="ltx_bibblock">Federated learning: challenges, methods, and future
directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>
37, 3 (2020),
50–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu,
Manzil Zaheer, Maziar Sanjabi,
Ameet Talwalkar, and Virginia Smith.
2020b.

</span>
<span class="ltx_bibblock">Federated Optimization in Heterogeneous Networks.
In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems
2020, MLSys 2020</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi,
Ahmad Beirami, and Virginia Smith.
2020c.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning. In
<em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2017a)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Agüera y Arcas.
2017a.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
20th International Conference on Artificial Intelligence and Statistics,
AISTATS 2017</em>. 1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan
et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2017b)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel
Ramage, Kunal Talwar, and Li Zhang.
2017b.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06963</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melis et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng
Song, Emiliano De Cristofaro, and
Vitaly Shmatikov. 2019.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in
collaborative learning. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of 2019
IEEE Symposium on Security and Privacy (SP)</em>. 691–706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meurisch et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Christian Meurisch, Bekir
Bayrak, and Max Mühlhäuser.
2020.

</span>
<span class="ltx_bibblock">Privacy-preserving AI services through data
decentralization. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Web
Conference 2020, WWW 2020</em>. 190–200.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohri
et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary
Sivek, and Ananda Theertha Suresh.
2019.

</span>
<span class="ltx_bibblock">Agnostic federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.00146</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Multimedia Laboratory (2020)</span>
<span class="ltx_bibblock">
The Chinese University of Hong Kong
Multimedia Laboratory. 2020.

</span>
<span class="ltx_bibblock">Large-scale CelebFaces Attributes (CelebA) Dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed May 22, 2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasr
et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Milad Nasr, Reza Shokri,
and Amir Houmansadr. 2018.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning:
stand-alone and federated learning under passive and active white-box
inference attacks.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.00910</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishio and
Yonetani (2019)</span>
<span class="ltx_bibblock">
Takayuki Nishio and Ryo
Yonetani. 2019.

</span>
<span class="ltx_bibblock">Client selection for federated learning with
heterogeneous resources in mobile edge. In
<em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICC 2019-2019 IEEE International
Conference on Communications (ICC)</em>. IEEE, 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu
et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Chaoyue Niu, Fan Wu,
Shaojie Tang, Lifeng Hua,
Rongfei Jia, Chengfei Lv,
Zhihua Wu, and Guihai Chen.
2019.

</span>
<span class="ltx_bibblock">Secure federated submodel learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.02254</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PaddlePaddle (2020)</span>
<span class="ltx_bibblock">
PaddlePaddle.
2020.

</span>
<span class="ltx_bibblock">Federated deep learning in PaddlePaddle.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/PaddlePaddle/PaddleFL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PaddlePaddle/PaddleFL</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed Jan 28, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PushShift.io (2020)</span>
<span class="ltx_bibblock">
PushShift.io.
2020.

</span>
<span class="ltx_bibblock">Reddit Dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://files.pushshift.io/reddit/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://files.pushshift.io/reddit/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed May 22, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reisizadeh et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Amirhossein Reisizadeh,
Aryan Mokhtari, Hamed Hassani,
Ali Jadbabaie, and Ramtin Pedarsani.
2019.

</span>
<span class="ltx_bibblock">Fedpaq: a communication-efficient federated
learning method with periodic averaging and quantization.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.13014</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryffel et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Theo Ryffel, Andrew
Trask, Morten Dahl, Bobby Wagner,
Jason Mancuso, Daniel Rueckert, and
Jonathan Passerat-Palmbach.
2018.

</span>
<span class="ltx_bibblock">A generic framework for privacy preserving deep
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.04017</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith
et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Virginia Smith, Chao-Kai
Chiang, Maziar Sanjabi, and Ameet S
Talwalkar. 2017.

</span>
<span class="ltx_bibblock">Federated multi-task learning. In
<em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">Proceedings of Advances in Neural Information
Processing Systems</em>. 4424–4434.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tensorflow (2020)</span>
<span class="ltx_bibblock">
Tensorflow.
2020.

</span>
<span class="ltx_bibblock">TensorFlow Federated: Machine Learning on
Decentralized Data.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed Jan 28, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wikipedia (2020a)</span>
<span class="ltx_bibblock">
Wikipedia.
2020a.

</span>
<span class="ltx_bibblock">California Consumer Privacy Act.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/California_Consumer_Privacy_Act</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed Feb 5, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wikipedia (2020b)</span>
<span class="ltx_bibblock">
Wikipedia.
2020b.

</span>
<span class="ltx_bibblock">Compression ratio.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://en.wikipedia.org/wiki/Compression_ratio" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/Compression_ratio</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed Oct 18, 2020.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wikipedia (2020c)</span>
<span class="ltx_bibblock">
Wikipedia.
2020c.

</span>
<span class="ltx_bibblock">General Data Protection Regulation.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/General_Data_Protection_Regulation</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed Feb 5, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wistuba
et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Martin Wistuba, Ambrish
Rawat, and Tejaswini Pedapati.
2019.

</span>
<span class="ltx_bibblock">A survey on neural architecture search.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.01392</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu
et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Mengwei Xu, Jiawei Liu,
Yuanqiang Liu, Felix Xiaozhu Lin,
Yunxin Liu, and Xuanzhe Liu.
2019a.

</span>
<span class="ltx_bibblock">A first look at deep learning apps on smartphones.
In <em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">Proceedings of The World Wide Web Conference</em>.
2125–2136.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu
et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Mengwei Xu, Feng Qian,
Qiaozhu Mei, Kang Huang, and
Xuanzhe Liu. 2018a.

</span>
<span class="ltx_bibblock">Deeptype: On-device deep learning for input
personalization service with minimal privacy concern.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies</em> 2,
4 (2018), 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu
et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Mengwei Xu, Feng Qian,
Mengze Zhu, Feifan Huang,
Saumay Pushp, and Xuanzhe Liu.
2019b.

</span>
<span class="ltx_bibblock">Deepwear: Adaptive local offloading for on-wearable
deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em>
19, 2 (2019),
314–330.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu
et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Mengwei Xu, Mengze Zhu,
Yunxin Liu, Felix Xiaozhu Lin, and
Xuanzhe Liu. 2018b.

</span>
<span class="ltx_bibblock">DeepCache: principled cache for mobile deep
vision. In <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th Annual
International Conference on Mobile Computing and Networking</em>.
129–144.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Timothy Yang, Galen
Andrew, Hubert Eichner, Haicheng Sun,
Wei Li, Nicholas Kong,
Daniel Ramage, and Françoise
Beaufays. 2018.

</span>
<span class="ltx_bibblock">Applied federated learning: improving google
keyboard query suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.02903</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yogesh
et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Saxena Yogesh, Shrivastava
Abha, and Singh Priyanka.
2014.

</span>
<span class="ltx_bibblock">Short communication mobile usage and sleep patterns
among medical students.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Indian J Physiol Pharmacol</em>
58, 1 (2014),
100–103.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan
et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jinliang Yuan, Mengwei
Xu, Xiao Ma, Ao Zhou,
Xuanzhe Liu, and Shangguang Wang.
2020.

</span>
<span class="ltx_bibblock">Hierarchical Federated Learning through LAN-WAN
Orchestration.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11612</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2006.06982" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2006.06983" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2006.06983">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2006.06983" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2006.06984" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 09:31:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
