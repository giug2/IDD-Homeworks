<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.18706] The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses</title><meta property="og:description" content="This paper presents a complete processing workflow for extracting information from French census lists from 1836 to 1936. These lists contain information about individuals living in France and their households. We aim …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.18706">

<!--Generated on Sun May  5 18:14:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Handwritten table recognition Large-scale data collection Collaborative annotation.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>TEKLIA, Paris, France 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>boillet@teklia.com</span></span></span> 
<br class="ltx_break"></span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Institut National d’Etudes Démographiques (INED) and Paris School of Economics (PSE), France
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>lionel.kesztenbaum@ined.fr</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mélodie Boillet
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-0618-7852" title="ORCID identifier" class="ltx_ref">0000-0002-0618-7852</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Solène Tarride
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-6174-9865" title="ORCID identifier" class="ltx_ref">0000-0001-6174-9865</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yoann Schneider
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bastien Abadie
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lionel Kesztenbaum
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christopher Kermorvant
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7508-4080" title="ORCID identifier" class="ltx_ref">0000-0002-7508-4080</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This paper presents a complete processing workflow for extracting information from French census lists from 1836 to 1936. These lists contain information about individuals living in France and their households. We aim at extracting all the information contained in these tables using automatic handwritten table recognition. At the end of the Socface project, in which our work is taking place, the extracted information will be redistributed to the departmental archives, and the nominative lists will be freely available to the public, allowing anyone to browse hundreds of millions of records. The extracted data will be used by demographers to analyze social change over time, significantly improving our understanding of French economic and social structures. For this project, we developed a complete processing workflow: large-scale data collection from French departmental archives, collaborative annotation of documents, training of handwritten table text and structure recognition models, and mass processing of millions of images.</p>
<p id="id2.id2" class="ltx_p">We present the tools we have developed to easily collect and process millions of pages. We also show that it is possible to process such a wide variety of tables with a single table recognition model that uses the image of the entire page to recognize information about individuals, categorize them and automatically group them into households. The entire process has been successfully used to process the documents of a departmental archive, representing more than 450,000 images.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Handwritten table recognition Large-scale data collection Collaborative annotation.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>The Socface project</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The Socface project<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://socface.site.ined.fr/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://socface.site.ined.fr/</a></span></span></span> involves archivists, demographers, and computer scientists working together to analyze French census documents and extract information on a very large scale. Its objective is to gather and process all the handwritten nominal census lists from 1836 to 1936 using automatic handwriting recognition. Produced every five year, these lists are organized spatially (municipality; wards, hamlets, or streets; houses; households) and summarize the information from the census, listing each individual with some of his or her characteristics, e.g., name, year of birth, or occupation. The project aims at taking advantage of this archival material to produce a database of all individuals who lived in France between 1836 and 1936, which will be used to analyze social change over the course of 100 years. An important impact of Socface will be public access to the nominal lists: they will be made freely available, allowing anyone to browse hundreds of millions of records.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/moulins_year/1836.jpg" id="S1.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="145" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">1836</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/moulins_year/1886.jpg" id="S1.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="129" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">1886</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/moulins_year/1936.jpg" id="S1.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="144" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S1.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">1936</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">First page of nominal lists for the commune of Moulins (department of Allier) for three census years. The quality of the pages varies greatly from one year to the next. In addition, the table template evolved over the years: in 1881, civil status was replaced by the column marking the position in the household. In 1906, age is replaced by year of birth, as can be seen on the 1936 example.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As depicted on Figure <a href="#S1.F1" title="Figure 1 ‣ 1 The Socface project ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the data are presented in tabular form. A major challenge in this project and in processing these documents is that the tabular formats have evolved over the 100 years studied. As can be seen in the figure, the columns changed (age vs. year of birth), so did their order on the page. In addition, the quality of preservation varies from year to year and from archival deposit to archival deposit. The very large number of writers makes the task even more complex.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The decentralized nature of the source material has been a significant hurdle for prior attempts at a project of this scale. The images required for the Socface project are dispersed across 100 local archive services throughout France, rather than being housed in a single repository. The project is faced with a dual layer of variability due to the dispersion of documents, which requires not only the preliminary collection of images, but also dealing with the diversity of the documents themselves and the differing organizational systems and metadata standards employed by each archival service. The collection and systematic analysis of the data is made difficult by its complexity.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The Socface project faces a significant challenge in processing a vast number of documents, with an estimated count of 30 million images. To address this challenge, access to public supercomputing resources is necessary. However, High-Performance Computing (HPC) architectures are not inherently designed to manage such extensive input and output flows. Tailored development efforts are necessary to ensure that images can be efficiently processed by available computing resources, particularly GPUs, and to seamlessly integrate the results into a document management system. This highlights the need for innovative solutions to bridge the gap between traditional HPC capabilities and the demands of large-scale data analysis projects. 
<br class="ltx_break"></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we describe the methodologies and technological advancements developed in the Socface project, highlighting our contributions to document recognition and historical data analysis. Our work presents a comprehensive approach to processing and analyzing historical census documents on an unprecedented scale. The key contributions of this paper are:</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data collection and normalization:</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">We present a reliable method for collecting, identifying, normalizing, and storing images and metadata from each archival service. This involved developing a standardized protocol for interacting with the various organizational systems found across the 100 local archives, ensuring consistency in the way documents are digitized, classified, and archived. Our approach involves techniques for harmonizing metadata, which facilitates the integration of different data sources into a cohesive dataset.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deep learning model for handwritten table understanding:</h5>

<div id="S1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p1.1" class="ltx_p">A central contribution of our work is the design of a unique deep learning model capable of recognizing and structuring the personal information contained within the handwritten lists, despite the considerable diversity of document formats encountered. This model leverages advancements in full page handwriting recognition to accurately interpret a wide range of handwriting styles and extract structured data from documents whose layouts evolve over time.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph"> High-Performance Computing (HPC) for document processing:</h5>

<div id="S1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px3.p1.1" class="ltx_p">A pivotal advancement made in this project is the extension of Arkindex, an open-source platform for automatic document processing, to facilitate communication with High-Performance Computing (HPC) systems via the SLURM workload manager. This extension grants the document processing community the ability to leverage the vast processing capacities inherent to HPC environments.</p>
</div>
<div id="S1.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px3.p2.1" class="ltx_p">This paper is structured as follows. Section <a href="#S2" title="2 Related work ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of the main approaches for information extraction from digitized handwritten tables. Section <a href="#S3" title="3 Data collection and normalization ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the tool developed during this project to simplify the data collection and normalization from departmental archives. Section <a href="#S4" title="4 Document organization and content ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the census registers and the annotation process. Section <a href="#S5" title="5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> describes the training data, presents each step of the proposed information extraction workflow, and discusses the results. The final section <a href="#S6" title="6 Distributed processing on HPC ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> describes how we distributed the document processing across a cluster of computers using HPC tools.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Several models are available for detecting tables in document images. However, there are few models that can retrieve both the textual content and structure of tables, especially for historical and handwritten documents. The dominant approach for processing such documents is to first detect the rows of a table and then apply a standard character recognition model at the row level. More recently, models have been proposed to process handwritten tables as a whole by analyzing the image of the entire table. Both approaches will be discussed in the following sections.</p>
</div>
<section id="S2.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.1 </span>Table row processing</h4>

<div id="S2.SS0.SSS1.p1" class="ltx_para">
<p id="S2.SS0.SSS1.p1.1" class="ltx_p">In the literature, most analysis focus on 2-step pipelines. First, the table rows are extracted using standard text line detection models. These are usually Fully Convolutional Networks (FCN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, Region-based CNNs (R-CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> or, more recently, Transformer-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> models. Once the rows have been extracted, standard text recognition using an HTR model is applied, and the columns are often recreated in a post-processing step.</p>
</div>
<div id="S2.SS0.SSS1.p2" class="ltx_para">
<p id="S2.SS0.SSS1.p2.1" class="ltx_p">In their work on the POPP dataset, Constum <span id="S2.SS0.SSS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> addressed the problem using a standard line detection model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> followed by a line-level text recognition model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. As their tables followed the same template from image to image, there was no need to segment the tables into columns, as the information was always presented in the same order. To correctly categorize the retrieved information, they added a <span id="S2.SS0.SSS1.p2.1.2" class="ltx_text ltx_font_typewriter">/</span> symbol in the ground truth to separate the information from the different columns.</p>
</div>
<div id="S2.SS0.SSS1.p3" class="ltx_para">
<p id="S2.SS0.SSS1.p3.1" class="ltx_p">In their study, Tarride <span id="S2.SS0.SSS1.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> made this method a little more generic by predicting both the text and the category of information recognized. This makes it possible to apply the same model to multiple table templates. To achieve this, the authors transformed the information in the ground truth by adding a token before the start of the text in each column, representing its category. This allowed them to avoid using the <span id="S2.SS0.SSS1.p3.1.2" class="ltx_text ltx_font_typewriter">/</span> symbol, which was no longer useful. The trained model performs very similarly to the model trained by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, but it is much more general and predicts more information as it categorizes the detected information.</p>
</div>
<div id="S2.SS0.SSS1.p4" class="ltx_para">
<p id="S2.SS0.SSS1.p4.1" class="ltx_p">The TableTransformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> goes one step further by extracting both tables and their structure from PDF document images. This means that it can extract not only the rows of tables, but also their columns and cells. This model works very well on printed data and has shown good performance on handwritten tables <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, as with previous approaches, it cannot recognize the content of the cells directly, so it is necessary to apply a text recognition model afterward.</p>
</div>
</section>
<section id="S2.SS0.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.0.2 </span>Full table processing</h4>

<div id="S2.SS0.SSS2.p1" class="ltx_para">
<p id="S2.SS0.SSS2.p1.1" class="ltx_p">A major disadvantage of processing at the table row level is that, as with conventional text recognition, detection errors have a major impact on the quality of text recognition. Furthermore, if we use a character recognition model that uses context, in particular Transformer models, the context is greatly reduced compared to full page recognition. This is also the case for table processing: when processing at row level only, the context of previous rows is lost, as is the information contained in the table header. Recent advances, particularly in Transformer architectures, make it possible to process and understand the entire page or table without any prior segmentation.</p>
</div>
<div id="S2.SS0.SSS2.p2" class="ltx_para">
<p id="S2.SS0.SSS2.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the authors trained a model on the whole table. The model has been trained to extract the textual content of the table in a structured way: each piece of textual information is extracted with its type, which corresponds to the column label. For this, the authors used the DAN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which combines a convolutional encoder and a transformer decoder, making it possible to process documents at line level, but also at full page level. Due to the small amount of annotated data and the much more complex task, the model performs less well than the table row level model, but is not affected by the quality of the line-detection model, whose impact on text recognition has not been assessed.</p>
</div>
<div id="S2.SS0.SSS2.p3" class="ltx_para">
<p id="S2.SS0.SSS2.p3.1" class="ltx_p">Given the immense size and diversity of the documents in the Socface project, it is impractical to set up a complex workflow with sequential models such as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> or to use several models tailored to specific document templates. We therefore decided to develop a single, comprehensive model capable of processing entire tables. This model is designed to automatically adapt to the variations inherent in documents, ensuring efficient and accurate recognition and structuring of data without the need for prior segmentation or template-specific adjustments. This approach not only streamlines the processing pipeline, but also overcomes the challenge of handling the project’s large and varied dataset with a scalable and flexible solution.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data collection and normalization</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.18706/assets/resources/spider.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="359" height="392" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Configuration interface for retrieving and organizing data from the input CSV file. The ”Name” column indicates the fields present in the CSV file. The ”Type” column indicates how the CSV fields will be used (whether it corresponds to the year or commune, or if the field should be ignored). If the data displayed in the ”Values sample” column is correct, the user will see a preview of the retrieved images with their metadata.</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">A critical component of the Socface project is the comprehensive collection, normalization, and organization of images and metadata from 94 departmental archives services across metropolitan France. The majority of these services have volunteered to participate in the project by providing access to their archival images and associated metadata. In return for their cooperation, they are offered access to all the data automatically extracted from their documents. These archive services use a variety of systems to store their images, including self-hosted solutions, external hosting and the International Image Interoperability Framework (IIIF), as well as different archive management systems. As a result, images and metadata were presented in a variety of formats and organizational hierarchies, including XML-EAD, CSV, XLSX, XLS and ODT, with no standard naming conventions for cities across the services.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To address the challenge of collecting, organizing and normalizing this diverse dataset, we developed a web-based platform called Socface-Spider. This platform is designed with several key functionalities to facilitate the processing of the collected data:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Import metadata from CSV files: In response to the diversity of file formats provided by the archive services, all file formats are first converted to CSV. Following conversion, metadata files are imported into Socface-Spider.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Support for specific CSV formats: Given the variation in the structure of CSV files across different archive services, Socface-Spider includes a feature that allows users to manually select the columns containing the necessary metadata. This selection is facilitated by a user interface, shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Data collection and normalization ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, designed to accommodate the specificities of each CSV format. This process ensures that essential data such as the year, city name, archival ID, and image path are accurately identified and normalized for consistency.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Fuzzy identification of place names: Given the lack of standardized naming of cities across services, the platform uses fuzzy matching techniques to identify city names within the Cassini index <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This index catalogues all official names of communes in France since 1793, facilitating accurate matching of data to specific locations.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Image integrity checks via IIIF: The platform verifies the presence and integrity of images on the storage server via IIIF access, ensuring that digital artifacts are complete and uncorrupted before further processing.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Export and organization of validated data: After validation, the platform exports the data to Arkindex, where the images are organized in a standardized manner by census year, municipality, and register. All the metadata collected is linked to the corresponding census registers, creating a structured and accessible dataset.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">At the current stage of the project, Socface-Spider has proven its effectiveness and versatility by being used in more than 50 projects, successfully validating and organizing more than 9 million images and their metadata according to the specific requirements of each project.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Document organization and content</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Description of census registers</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The census registers provide a unique window on the demographic fabric of France from the mid-nineteenth to the mid-twentieth century. These nominative lists were systematically compiled every five years from 1836 onwards. Exceptions to this five-year rhythm were due to historical contingencies: the census of 1871 was postponed to the following year due to the occupation of parts of the territory by the Prussian army, and those planned for 1916 and 1941 were cancelled due to war conditions. The censuses were carried out within the municipal framework, systematically listing the inhabitants by household. This organization gave priority to the head of the household, followed by his or her spouse, children, other relatives living in the household, and then any servants or apprentices, among others.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/moulins_sample/cover_page.jpg" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="110" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.4.2" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Front page</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/moulins_sample/list_page.jpg" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="106" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.4.2" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">List page</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/moulins_sample/recap_page.jpg" id="S4.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="111" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F3.sf3.4.2" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Recap page</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S4.F3.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/moulins_sample/totals_page.jpg" id="S4.F3.sf4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="108" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F3.sf4.4.2" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Totals page</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Example of digitized pages from the census of the commune of Moulins (department of Allier) in 1881.</span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Over time, the content of these communal nominative lists evolved and typically included first and last names, ages or dates of birth, family positions, occupations, nationalities, and occasionally precise addresses. The images obtained from the archival services are systematically organized into registers, each corresponding to a specific census date and commune. The images are mainly scans of double pages and, for certain years and departments, single pages. These include not only the nominative lists, but also title pages, summaries, totals and even blank pages as presented in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 Description of census registers ‣ 4 Document organization and content ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, with most images in black and white, scanned either from the originals or from microfilm, although a few are in color.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">At present, our project is concentrating exclusively on the pages containing individual information organized by household. These lists are usually 30 lines long, although variations from 29 to 36 lines have been observed.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">The layout of these documents generally begins with columns for street, house and household information, followed by details of individuals such as surname, first name, age (or year of birth) and occupation. At the current stage of the project, we are focusing on the recognition and analysis of the individual information contained in these lists.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Ground-truth generation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Generating ground-truth data is a fundamental step in training deep learning models for automatically extracting individual information from historical census lists. Given the wide variation in documents - including differences in time, format, and scanning conditions - it is imperative to collect and annotate a representative sample that captures the full spectrum of document diversity.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/callico/individuals.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F4.sf1.4.2" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Key-Value<span id="S4.F4.sf1.4.2.1" class="ltx_text ltx_font_upright"> mode to annotate the individuals’ information.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.18706/assets/resources/callico/household.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F4.sf2.4.2" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Grouping<span id="S4.F4.sf2.4.2.1" class="ltx_text ltx_font_upright"> mode to group individuals into households.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Callico interfaces for annotating information on individuals and grouping them into households.</span></figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We selected 100 single pages from 11 pilot departmental archives for annotation. This selection was carefully chosen to include all years covered by the study and to accurately reflect the diversity of page appearance, image quality and table templates present in the archives. These images served as the basis for manual transcription tasks carried out on Callico <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, an open-source document image annotation platform, using two specific annotation modes. First, the <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">Key-Value</span> mode was used to annotate the individual level information. In this mode, the annotator is presented with a full list page, with a highlighted zone corresponding to an individual entry, and is prompted to enter the relevant details into a designated form, as presented on Figure <a href="#S4.F4.sf1" title="In Figure 4 ‣ 4.2 Ground-truth generation ‣ 4 Document organization and content ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>. Second, the <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_smallcaps">Grouping</span> mode was used to construct the household groupings present in the nominative lists. In this mode, the entire page is displayed with individual zones highlighted, as shown on Figure <a href="#S4.F4.sf2" title="In Figure 4 ‣ 4.2 Ground-truth generation ‣ 4 Document organization and content ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>. Annotators are tasked with selecting zones that belong to the same household, in order to reconstruct of household units from the disjointed individual entries. In addition, we assigned a class to each selected page in order to train the page classification model described in Section <a href="#S5.SS1" title="5.1 Page classification ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. 
<br class="ltx_break"></p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Throughout the project, 22 annotation campaigns were conducted - two for each of the 11 selected departmental archives - resulting in an annotated dataset for model training. The first type of campaign, which focused on detailed annotation of individual information, resulted in 33,815 rows of table data. For the household grouping efforts, a total of 532 pages were annotated. Importantly, the majority of these annotations underwent a moderation process where they were either validated or corrected by experts to ensure the highest possible accuracy and reliability of the ground-truth data.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Information extraction workflow</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section describes the different models used to extract the personal data. To process only the list pages and extract the individual information, we start the processing by applying an image classification model, as described in Section <a href="#S5.SS1" title="5.1 Page classification ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. A text recognition model is then applied directly to the pages to extract the individual information. This model and its training parameters and performance are described in detail in Section <a href="#S5.SS2" title="5.2 Handwritten table recognition ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Page classification</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Since our study focuses on the pages of the nominal lists, and in order to save processing time, we only send images containing list pages to the recognizer. We therefore trained an image classification model with the following classes:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">The <span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Front</span> class corresponds to the first page of a register, which contains all the information about the year, the commune, the department and also some instructions for filling in the nominative lists;</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">The <span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">List</span> class corresponds to pages of nominative lists with information on individuals, organized by household and street;</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">The <span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">Recap</span> pages contain various tables summarizing information about the population of the parish;</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">The <span id="S5.I1.i4.p1.1.1" class="ltx_text ltx_font_smallcaps">Totals</span> pages contain the total number of houses, households, individuals, but also men and women in the commune;</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p">The <span id="S5.I1.i5.p1.1.1" class="ltx_text ltx_font_smallcaps">Other</span> class contains all other images such as blank pages, black pages or handwritten tables that do not correspond to nominative lists, summaries, or totals.</p>
</div>
</li>
</ul>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Training configuration</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">To train a model, we chose to fine-tune the classification model pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> available in YOLOv8<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://docs.ultralytics.com/tasks/classify/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.ultralytics.com/tasks/classify/</a></span></span></span>. We started from the <span id="S5.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_smallcaps">YOLOv8x-cls</span> model and fine-tuned on Socface images during 200 epochs with early stopping and a batch size of 4. The model was trained on square images of size <math id="S5.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S5.SS1.SSS1.p1.1.m1.1a"><mrow id="S5.SS1.SSS1.p1.1.m1.1.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS1.p1.1.m1.1.1.2" xref="S5.SS1.SSS1.p1.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.SSS1.p1.1.m1.1.1.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S5.SS1.SSS1.p1.1.m1.1.1.3" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.1.m1.1b"><apply id="S5.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1"><times id="S5.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.2">1024</cn><cn type="integer" id="S5.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.1.m1.1c">1024\times 1024</annotation></semantics></math> pixels. The data used is the same as that selected for Callico annotation to train the recognition model, to which we have added pages from classes other than <span id="S5.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_smallcaps">List</span>. In total, we have 1,285 pages, divided into 899 in the training set, 193 for the validation and 193 in the test set.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Page classification results</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">The performance results of the model were satisfactory, particularly in terms of minimizing classification ambiguities regarding the <span id="S5.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_smallcaps">List</span> class. The accurate identification of this class is critical, as these pages are subsequently processed by the information extraction model, which requires a high degree of precision and recall to ensure comprehensive data capture. As shown in Table <a href="#S5.T1.st1" title="In Table 1 ‣ 5.1.2 Page classification results ‣ 5.1 Page classification ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>, the model demonstrates exceptional efficiency, achieving precision and recall metrics of at least 99% for the <span id="S5.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_smallcaps">List</span> class.</p>
</div>
<figure id="S5.T1" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.3.2" class="ltx_text" style="font-size:90%;">Results obtained by the image classification model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T1.st1" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.T1.st1.3.2" class="ltx_text" style="font-size:90%;">Results obtained by the image classification model for each set and class. The results on the training set are not shown because the model obtained 100% for the precision, recall and F1-score for all classes.</span></figcaption>
<table id="S5.T1.st1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.st1.4.1.1" class="ltx_tr">
<th id="S5.T1.st1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" rowspan="2"><span id="S5.T1.st1.4.1.1.1.1" class="ltx_text ltx_font_bold">Class</span></th>
<th id="S5.T1.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="3"><span id="S5.T1.st1.4.1.1.2.1" class="ltx_text ltx_font_bold">Validation</span></th>
<th id="S5.T1.st1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="3"><span id="S5.T1.st1.4.1.1.3.1" class="ltx_text ltx_font_bold">Test</span></th>
</tr>
<tr id="S5.T1.st1.4.2.2" class="ltx_tr">
<th id="S5.T1.st1.4.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.2.2.1.1" class="ltx_text ltx_font_bold">P</span></th>
<th id="S5.T1.st1.4.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.2.2.2.1" class="ltx_text ltx_font_bold">R</span></th>
<th id="S5.T1.st1.4.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.2.2.3.1" class="ltx_text ltx_font_bold">F1</span></th>
<th id="S5.T1.st1.4.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.2.2.4.1" class="ltx_text ltx_font_bold">P</span></th>
<th id="S5.T1.st1.4.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.2.2.5.1" class="ltx_text ltx_font_bold">R</span></th>
<th id="S5.T1.st1.4.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.2.2.6.1" class="ltx_text ltx_font_bold">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.st1.4.3.1" class="ltx_tr">
<th id="S5.T1.st1.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.3.1.1.1" class="ltx_text ltx_font_smallcaps">Front</span></th>
<td id="S5.T1.st1.4.3.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.3.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.3.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.3.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">0.93</td>
<td id="S5.T1.st1.4.3.1.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.3.1.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">0.97</td>
</tr>
<tr id="S5.T1.st1.4.4.2" class="ltx_tr">
<th id="S5.T1.st1.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.4.2.1.1" class="ltx_text ltx_font_smallcaps">List</span></th>
<td id="S5.T1.st1.4.4.2.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.4.2.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.99</td>
<td id="S5.T1.st1.4.4.2.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.4.2.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.4.2.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.99</td>
<td id="S5.T1.st1.4.4.2.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.99</td>
</tr>
<tr id="S5.T1.st1.4.5.3" class="ltx_tr">
<th id="S5.T1.st1.4.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.5.3.1.1" class="ltx_text ltx_font_smallcaps">Recap</span></th>
<td id="S5.T1.st1.4.5.3.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.92</td>
<td id="S5.T1.st1.4.5.3.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.5.3.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.96</td>
<td id="S5.T1.st1.4.5.3.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.91</td>
<td id="S5.T1.st1.4.5.3.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.83</td>
<td id="S5.T1.st1.4.5.3.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.87</td>
</tr>
<tr id="S5.T1.st1.4.6.4" class="ltx_tr">
<th id="S5.T1.st1.4.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.6.4.1.1" class="ltx_text ltx_font_smallcaps">Totals</span></th>
<td id="S5.T1.st1.4.6.4.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.6.4.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.92</td>
<td id="S5.T1.st1.4.6.4.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.96</td>
<td id="S5.T1.st1.4.6.4.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.6.4.6" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.6.4.7" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
</tr>
<tr id="S5.T1.st1.4.7.5" class="ltx_tr">
<th id="S5.T1.st1.4.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T1.st1.4.7.5.1.1" class="ltx_text ltx_font_smallcaps">Other</span></th>
<td id="S5.T1.st1.4.7.5.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">0.88</td>
<td id="S5.T1.st1.4.7.5.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">1.0</td>
<td id="S5.T1.st1.4.7.5.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">0.93</td>
<td id="S5.T1.st1.4.7.5.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">0.56</td>
<td id="S5.T1.st1.4.7.5.6" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">0.71</td>
<td id="S5.T1.st1.4.7.5.7" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">0.63</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T1.st2" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.T1.st2.3.2" class="ltx_text" style="font-size:90%;">Confusion matrix of the test set.</span></figcaption>
<table id="S5.T1.st2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.st2.4.1.1" class="ltx_tr">
<th id="S5.T1.st2.4.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row" style="padding:2.5pt 4.0pt;"></th>
<th id="S5.T1.st2.4.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row" style="padding:2.5pt 4.0pt;"></th>
<th id="S5.T1.st2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:2.5pt 4.0pt;" colspan="5">Truth</th>
</tr>
<tr id="S5.T1.st2.4.2.2" class="ltx_tr">
<th id="S5.T1.st2.4.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row" style="padding:2.5pt 4.0pt;"></th>
<th id="S5.T1.st2.4.2.2.2" class="ltx_td ltx_th ltx_th_column ltx_th_row" style="padding:2.5pt 4.0pt;"></th>
<th id="S5.T1.st2.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:2.5pt 4.0pt;">
<div id="S5.T1.st2.4.2.2.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:23.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:23.8pt;transform:translate(-8.47pt,-8.47pt) rotate(-90deg) ;">
<p id="S5.T1.st2.4.2.2.3.1.1" class="ltx_p"><span id="S5.T1.st2.4.2.2.3.1.1.1" class="ltx_text ltx_font_smallcaps">Front</span></p>
</span></div>
</th>
<th id="S5.T1.st2.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:2.5pt 4.0pt;">
<div id="S5.T1.st2.4.2.2.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:16.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:16.9pt;transform:translate(-5.01pt,-5.01pt) rotate(-90deg) ;">
<p id="S5.T1.st2.4.2.2.4.1.1" class="ltx_p"><span id="S5.T1.st2.4.2.2.4.1.1.1" class="ltx_text ltx_font_smallcaps">List</span></p>
</span></div>
</th>
<th id="S5.T1.st2.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:2.5pt 4.0pt;">
<div id="S5.T1.st2.4.2.2.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:26.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:26.8pt;transform:translate(-9.01pt,-8.04pt) rotate(-90deg) ;">
<p id="S5.T1.st2.4.2.2.5.1.1" class="ltx_p"><span id="S5.T1.st2.4.2.2.5.1.1.1" class="ltx_text ltx_font_smallcaps">Recap</span></p>
</span></div>
</th>
<th id="S5.T1.st2.4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:2.5pt 4.0pt;">
<div id="S5.T1.st2.4.2.2.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:30.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:30.3pt;transform:translate(-11.69pt,-11.69pt) rotate(-90deg) ;">
<p id="S5.T1.st2.4.2.2.6.1.1" class="ltx_p"><span id="S5.T1.st2.4.2.2.6.1.1.1" class="ltx_text ltx_font_smallcaps">Totals</span></p>
</span></div>
</th>
<th id="S5.T1.st2.4.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:2.5pt 4.0pt;">
<div id="S5.T1.st2.4.2.2.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:25.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:25.6pt;transform:translate(-9.32pt,-9.32pt) rotate(-90deg) ;">
<p id="S5.T1.st2.4.2.2.7.1.1" class="ltx_p"><span id="S5.T1.st2.4.2.2.7.1.1.1" class="ltx_text ltx_font_smallcaps">Other</span></p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.st2.4.3.1" class="ltx_tr">
<th id="S5.T1.st2.4.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" style="padding:2.5pt 4.0pt;" rowspan="5"><span id="S5.T1.st2.4.3.1.1.1" class="ltx_text">
<span id="S5.T1.st2.4.3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:41.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:41.8pt;transform:translate(-17.44pt,-17.44pt) rotate(-90deg) ;">
<span id="S5.T1.st2.4.3.1.1.1.1.1" class="ltx_p">Predicted</span>
</span></span></span></th>
<th id="S5.T1.st2.4.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.3.1.2.1" class="ltx_text ltx_font_smallcaps">Front</span></th>
<td id="S5.T1.st2.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E4EFF9;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.3.1.3.1" class="ltx_text" style="background-color:#E4EFF9;">14</span></td>
<td id="S5.T1.st2.4.3.1.4" class="ltx_td ltx_border_t" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.3.1.5" class="ltx_td ltx_border_t" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.3.1.6" class="ltx_td ltx_border_t" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.3.1.7" class="ltx_td ltx_border_r ltx_border_t" style="padding:2.5pt 4.0pt;"></td>
</tr>
<tr id="S5.T1.st2.4.4.2" class="ltx_tr">
<th id="S5.T1.st2.4.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.4.2.1.1" class="ltx_text ltx_font_smallcaps">List</span></th>
<td id="S5.T1.st2.4.4.2.2" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.4.2.3" class="ltx_td ltx_align_center" style="background-color:#08306B;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.4.2.3.1" class="ltx_text" style="color:#FFFFFF;background-color:#08306B;">145</span></td>
<td id="S5.T1.st2.4.4.2.4" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.4.2.5" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.4.2.6" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F5F9FE;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.4.2.6.1" class="ltx_text" style="background-color:#F5F9FE;">2</span></td>
</tr>
<tr id="S5.T1.st2.4.5.3" class="ltx_tr">
<th id="S5.T1.st2.4.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.5.3.1.1" class="ltx_text ltx_font_smallcaps">Recap</span></th>
<td id="S5.T1.st2.4.5.3.2" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.5.3.3" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.5.3.4" class="ltx_td ltx_align_center" style="background-color:#EAF2FB;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.5.3.4.1" class="ltx_text" style="background-color:#EAF2FB;">10</span></td>
<td id="S5.T1.st2.4.5.3.5" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.5.3.6" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#F5F9FE;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.5.3.6.1" class="ltx_text" style="background-color:#F5F9FE;">2</span></td>
</tr>
<tr id="S5.T1.st2.4.6.4" class="ltx_tr">
<th id="S5.T1.st2.4.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.6.4.1.1" class="ltx_text ltx_font_smallcaps">Totals</span></th>
<td id="S5.T1.st2.4.6.4.2" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.6.4.3" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.6.4.4" class="ltx_td" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.6.4.5" class="ltx_td ltx_align_center" style="background-color:#E6F0F9;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.6.4.5.1" class="ltx_text" style="background-color:#E6F0F9;">13</span></td>
<td id="S5.T1.st2.4.6.4.6" class="ltx_td ltx_border_r" style="padding:2.5pt 4.0pt;"></td>
</tr>
<tr id="S5.T1.st2.4.7.5" class="ltx_tr">
<th id="S5.T1.st2.4.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.7.5.1.1" class="ltx_text ltx_font_smallcaps">Other</span></th>
<td id="S5.T1.st2.4.7.5.2" class="ltx_td ltx_align_center ltx_border_b" style="background-color:#F6FAFF;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.7.5.2.1" class="ltx_text" style="background-color:#F6FAFF;">1</span></td>
<td id="S5.T1.st2.4.7.5.3" class="ltx_td ltx_border_b" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.7.5.4" class="ltx_td ltx_align_center ltx_border_b" style="background-color:#F6FAFF;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.7.5.4.1" class="ltx_text" style="background-color:#F6FAFF;">1</span></td>
<td id="S5.T1.st2.4.7.5.5" class="ltx_td ltx_border_b" style="padding:2.5pt 4.0pt;"></td>
<td id="S5.T1.st2.4.7.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="background-color:#F1F7FD;padding:2.5pt 4.0pt;"><span id="S5.T1.st2.4.7.5.6.1" class="ltx_text" style="background-color:#F1F7FD;">5</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p">Furthermore, analysis of the confusion matrix shown in Table <a href="#S5.T1.st2" title="In Table 1 ‣ 5.1.2 Page classification results ‣ 5.1 Page classification ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a> reveals that the model faces more challenges in classifying the <span id="S5.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_smallcaps">Other</span> class, which is characterized by its considerable diversity. This category combines data that includes tables that are often misidentified as summary pages, as well as pages with printed text that resemble front pages, leading to classification ambiguities and, consequently, reduced performance metrics within this specific class.</p>
</div>
<div id="S5.SS1.SSS2.p3" class="ltx_para">
<p id="S5.SS1.SSS2.p3.1" class="ltx_p">Notwithstanding the model’s limitations in accurately classifying the <span id="S5.SS1.SSS2.p3.1.1" class="ltx_text ltx_font_smallcaps">Other</span> class, its ability to identify list pages remains commendably high, making it sufficiently capable for the purposes of classifying and earmarking pages for subsequent processing by the information extraction model outlined in the following section.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Handwritten table recognition</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The information about the individuals is presented in a table where the individuals are grouped into households.
Given the scale of the project and the diversity of the documents, it was not feasible to develop and maintain a processing chain comprising multiple models. We therefore chose the DAN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to perform a full table recognition, which not only extracts the text from the table, but also tags the extracted text in order to categorize the predicted text at the same time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The advantage of this method is that it does not require any segmentation of the page nor the table, as it works directly on the whole page. In addition, some information is marked by vertical lines or ditto labels, so processing the whole page allows better interpretation of these labels compared to, for example, table row-level processing, where the model has much less context to interpret the content of a cell. A second advantage of this method is that there is no need to apply a second model later to label the information, as it is categorized directly at the same time as the text is recognized.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Finally, this model can also be used to predict data in a structured way. In fact, by adding a token indicating the head of the household before the individual information, we are able to directly structure individuals into households without any other model. This structuring involves a post-processing step consisting of going through all the pages of the register, in the correct reading order, to reconstruct households spanning two pages.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Label generation</h4>

<figure id="S5.F5" class="ltx_figure"><img src="/html/2404.18706/assets/resources/label_generation.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="239" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.4.2" class="ltx_text" style="font-size:90%;">Table header and first rows of a table from the census of the commune of Neuilly-le-Réal (department of Allier) in 1901. The label used to train the model for this part of the table is: 
<br class="ltx_break"><span id="S5.F5.4.2.1" class="ltx_text ltx_font_typewriter" style="font-size:89%;">
<span id="S5.F5.4.2.1.1" class="ltx_text" style="color:#26FFAB;">&lt;s-h&gt;</span>Gendre <span id="S5.F5.4.2.1.2" class="ltx_text" style="color:#9933CC;">&lt;f&gt;</span>Pierre <span id="S5.F5.4.2.1.3" class="ltx_text" style="color:#009900;">&lt;o&gt;</span>cultivateur <span id="S5.F5.4.2.1.4" class="ltx_text" style="color:#FFB529;">&lt;l&gt;</span>chef <span id="S5.F5.4.2.1.5" class="ltx_text" style="color:#FF0A9C;">&lt;e&gt;</span>patron <span id="S5.F5.4.2.1.6" class="ltx_text" style="color:#80FF00;">&lt;a&gt;</span>75 <span id="S5.F5.4.2.1.7" class="ltx_text" style="color:#AD0000;">&lt;n&gt;</span>française
<br class="ltx_break"><span id="S5.F5.4.2.1.8" class="ltx_text" style="color:#0F75FF;">&lt;s&gt;</span>Paraud <span id="S5.F5.4.2.1.9" class="ltx_text" style="color:#9933CC;">&lt;f&gt;</span>Marie <span id="S5.F5.4.2.1.10" class="ltx_text" style="color:#009900;">&lt;o&gt;</span>néant <span id="S5.F5.4.2.1.11" class="ltx_text" style="color:#FFB529;">&lt;l&gt;</span>épouse <span id="S5.F5.4.2.1.12" class="ltx_text" style="color:#FF0A9C;">&lt;e&gt;</span>néant <span id="S5.F5.4.2.1.13" class="ltx_text" style="color:#80FF00;">&lt;a&gt;</span>66 <span id="S5.F5.4.2.1.14" class="ltx_text" style="color:#AD0000;">&lt;n&gt;</span>idem
<br class="ltx_break"><span id="S5.F5.4.2.1.15" class="ltx_text" style="color:#26FFAB;">&lt;s-h&gt;</span>Martin <span id="S5.F5.4.2.1.16" class="ltx_text" style="color:#9933CC;">&lt;f&gt;</span>Pierre <span id="S5.F5.4.2.1.17" class="ltx_text" style="color:#009900;">&lt;o&gt;</span>métayer <span id="S5.F5.4.2.1.18" class="ltx_text" style="color:#FFB529;">&lt;l&gt;</span>chef <span id="S5.F5.4.2.1.19" class="ltx_text" style="color:#FF0A9C;">&lt;e&gt;</span>patron <span id="S5.F5.4.2.1.20" class="ltx_text" style="color:#80FF00;">&lt;a&gt;</span>69 <span id="S5.F5.4.2.1.21" class="ltx_text" style="color:#AD0000;">&lt;n&gt;</span>idem
<br class="ltx_break"><span id="S5.F5.4.2.1.22" class="ltx_text" style="color:#0F75FF;">&lt;s&gt;</span>Joyoz <span id="S5.F5.4.2.1.23" class="ltx_text" style="color:#9933CC;">&lt;f&gt;</span>Suzanne <span id="S5.F5.4.2.1.24" class="ltx_text" style="color:#009900;">&lt;o&gt;</span>néant <span id="S5.F5.4.2.1.25" class="ltx_text" style="color:#FFB529;">&lt;l&gt;</span>mère <span id="S5.F5.4.2.1.26" class="ltx_text" style="color:#FF0A9C;">&lt;e&gt;</span>néant <span id="S5.F5.4.2.1.27" class="ltx_text" style="color:#80FF00;">&lt;a&gt;</span>72 <span id="S5.F5.4.2.1.28" class="ltx_text" style="color:#AD0000;">&lt;n&gt;</span>idem
<br class="ltx_break">...
<br class="ltx_break"></span></span>Note that the order of the entities in the labels is always the same and does not always correspond to the order in which the information appears in the images, as there are multiple templates.</figcaption>
</figure>
<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">To extract information from individuals and group them into households, we use a unique text recognition model. To train it, we constructed the ground truth transcriptions as described below and shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2.1 Label generation ‣ 5.2 Handwritten table recognition ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>:</p>
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">Each piece of information annotated in the Callico form is preceded by a token indicating the type of information (<span id="S5.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#26FFAB;">&lt;s-h&gt;<span id="S5.I2.i1.p1.1.1.1" class="ltx_text" style="color:#000000;">, <span id="S5.I2.i1.p1.1.1.1.1" class="ltx_text" style="color:#0F75FF;">&lt;s&gt;</span>, <span id="S5.I2.i1.p1.1.1.1.2" class="ltx_text" style="color:#9933CC;">&lt;f&gt;</span>, <span id="S5.I2.i1.p1.1.1.1.3" class="ltx_text" style="color:#009900;">&lt;o&gt;</span>, <span id="S5.I2.i1.p1.1.1.1.4" class="ltx_text" style="color:#FFB529;">&lt;l&gt;</span>, <span id="S5.I2.i1.p1.1.1.1.5" class="ltx_text" style="color:#FF0A9C;">&lt;e&gt;</span>, <span id="S5.I2.i1.p1.1.1.1.6" class="ltx_text" style="color:#80FF00;">&lt;a&gt;</span>, <span id="S5.I2.i1.p1.1.1.1.7" class="ltx_text" style="color:#AD0000;">&lt;n&gt;</span></span></span> in the Figure);</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">The names of individuals listed as ’heads of household’ are preceded by a token (<span id="S5.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#26FFAB;">&lt;s-h&gt;</span>) that is different from the other members of the household (<span id="S5.I2.i2.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;color:#0F75FF;">&lt;s&gt;</span>), in order to indicate the start of the household;</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p id="S5.I2.i3.p1.1" class="ltx_p">All information about an individual is concatenated into a single string so that it always follows the same order, even if it is different from the order in the table;</p>
</div>
</li>
<li id="S5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i4.p1" class="ltx_para">
<p id="S5.I2.i4.p1.1" class="ltx_p">The transcriptions for each individual are themselves concatenated to represent the whole page in a single string.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">Empty cells and rows are not annotated and not present in the transcription.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Model and training configuration</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">DAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is an open-source attention-based Transformer model for handwritten text recognition that can work directly on pages. The encoder is fully convolutional, while the decoder is a Transformer network. It is trained with the cross-entropy loss function. The last layer is a linear layer with a softmax activation function that computes probabilities associated with each vocabulary character.
We trained a DAN model on the annotated single pages for 1,000 epochs with early stopping and a batch size of 4. The model was trained on a single GPU A100 with 80Gb. To reduce the memory required for training, the images were resized so that their height was equal to 1900 pixels. Data augmentation was applied during training and the maximum number of tokens to be predicted was set to 2,800 according to the training data.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Full-page recognition results</h4>

<figure id="S5.T2" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.3.2" class="ltx_text" style="font-size:90%;">Results obtained by the information extraction model.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T2.st1" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.T2.st1.3.2" class="ltx_text" style="font-size:90%;">Character Error Rate and Word Error Rate obtained by the information extraction model (%).</span></figcaption>
<table id="S5.T2.st1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.st1.4.1.1" class="ltx_tr">
<th id="S5.T2.st1.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st1.4.1.1.1.1" class="ltx_text ltx_font_bold">Set</span></th>
<th id="S5.T2.st1.4.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st1.4.1.1.2.1" class="ltx_text ltx_font_bold">CER</span></th>
<th id="S5.T2.st1.4.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st1.4.1.1.3.1" class="ltx_text ltx_font_bold">WER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.st1.4.2.1" class="ltx_tr">
<th id="S5.T2.st1.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st1.4.2.1.1.1" class="ltx_text ltx_font_smallcaps">train</span></th>
<td id="S5.T2.st1.4.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">8.94</td>
<td id="S5.T2.st1.4.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">17.18</td>
</tr>
<tr id="S5.T2.st1.4.3.2" class="ltx_tr">
<th id="S5.T2.st1.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st1.4.3.2.1.1" class="ltx_text ltx_font_smallcaps">validation</span></th>
<td id="S5.T2.st1.4.3.2.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">14.30</td>
<td id="S5.T2.st1.4.3.2.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">26.22</td>
</tr>
<tr id="S5.T2.st1.4.4.3" class="ltx_tr">
<th id="S5.T2.st1.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st1.4.4.3.1.1" class="ltx_text ltx_font_smallcaps">test</span></th>
<td id="S5.T2.st1.4.4.3.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">14.47</td>
<td id="S5.T2.st1.4.4.3.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">27.05</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T2.st2" class="ltx_table ltx_figure_panel">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.T2.st2.3.2" class="ltx_text" style="font-size:90%;">Evaluation of entity recognition on the test set.</span></figcaption>
<table id="S5.T2.st2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.st2.4.1.1" class="ltx_tr">
<th id="S5.T2.st2.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.1.1.1.1" class="ltx_text ltx_font_bold">Tag</span></th>
<th id="S5.T2.st2.4.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.1.1.2.1" class="ltx_text ltx_font_bold">P</span></th>
<th id="S5.T2.st2.4.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.1.1.3.1" class="ltx_text ltx_font_bold">R</span></th>
<th id="S5.T2.st2.4.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.1.1.4.1" class="ltx_text ltx_font_bold">F1</span></th>
<th id="S5.T2.st2.4.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.1.1.5.1" class="ltx_text ltx_font_bold">Support</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.st2.4.2.1" class="ltx_tr">
<th id="S5.T2.st2.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.2.1.1.1" class="ltx_text ltx_font_smallcaps">age</span></th>
<td id="S5.T2.st2.4.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">0.87</td>
<td id="S5.T2.st2.4.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">0.87</td>
<td id="S5.T2.st2.4.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">0.87</td>
<td id="S5.T2.st2.4.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1,700</td>
</tr>
<tr id="S5.T2.st2.4.3.2" class="ltx_tr">
<th id="S5.T2.st2.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.3.2.1.1" class="ltx_text ltx_font_smallcaps">birth_date</span></th>
<td id="S5.T2.st2.4.3.2.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.97</td>
<td id="S5.T2.st2.4.3.2.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.99</td>
<td id="S5.T2.st2.4.3.2.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.98</td>
<td id="S5.T2.st2.4.3.2.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">558</td>
</tr>
<tr id="S5.T2.st2.4.4.3" class="ltx_tr">
<th id="S5.T2.st2.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.4.3.1.1" class="ltx_text ltx_font_smallcaps">civil_status</span></th>
<td id="S5.T2.st2.4.4.3.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.95</td>
<td id="S5.T2.st2.4.4.3.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.93</td>
<td id="S5.T2.st2.4.4.3.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.94</td>
<td id="S5.T2.st2.4.4.3.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1,153</td>
</tr>
<tr id="S5.T2.st2.4.5.4" class="ltx_tr">
<th id="S5.T2.st2.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.5.4.1.1" class="ltx_text ltx_font_smallcaps">employer</span></th>
<td id="S5.T2.st2.4.5.4.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.74</td>
<td id="S5.T2.st2.4.5.4.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.76</td>
<td id="S5.T2.st2.4.5.4.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.75</td>
<td id="S5.T2.st2.4.5.4.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">237</td>
</tr>
<tr id="S5.T2.st2.4.6.5" class="ltx_tr">
<th id="S5.T2.st2.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.6.5.1.1" class="ltx_text ltx_font_smallcaps">firstname</span></th>
<td id="S5.T2.st2.4.6.5.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.94</td>
<td id="S5.T2.st2.4.6.5.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.93</td>
<td id="S5.T2.st2.4.6.5.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.94</td>
<td id="S5.T2.st2.4.6.5.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">2,371</td>
</tr>
<tr id="S5.T2.st2.4.7.6" class="ltx_tr">
<th id="S5.T2.st2.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.7.6.1.1" class="ltx_text ltx_font_smallcaps">link</span></th>
<td id="S5.T2.st2.4.7.6.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.85</td>
<td id="S5.T2.st2.4.7.6.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.89</td>
<td id="S5.T2.st2.4.7.6.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.87</td>
<td id="S5.T2.st2.4.7.6.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1,838</td>
</tr>
<tr id="S5.T2.st2.4.8.7" class="ltx_tr">
<th id="S5.T2.st2.4.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.8.7.1.1" class="ltx_text ltx_font_smallcaps">lob</span></th>
<td id="S5.T2.st2.4.8.7.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.74</td>
<td id="S5.T2.st2.4.8.7.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.76</td>
<td id="S5.T2.st2.4.8.7.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.75</td>
<td id="S5.T2.st2.4.8.7.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">788</td>
</tr>
<tr id="S5.T2.st2.4.9.8" class="ltx_tr">
<th id="S5.T2.st2.4.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.9.8.1.1" class="ltx_text ltx_font_smallcaps">nationality</span></th>
<td id="S5.T2.st2.4.9.8.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.67</td>
<td id="S5.T2.st2.4.9.8.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.73</td>
<td id="S5.T2.st2.4.9.8.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.70</td>
<td id="S5.T2.st2.4.9.8.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1,287</td>
</tr>
<tr id="S5.T2.st2.4.10.9" class="ltx_tr">
<th id="S5.T2.st2.4.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.10.9.1.1" class="ltx_text ltx_font_smallcaps">observation</span></th>
<td id="S5.T2.st2.4.10.9.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.37</td>
<td id="S5.T2.st2.4.10.9.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.10</td>
<td id="S5.T2.st2.4.10.9.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.16</td>
<td id="S5.T2.st2.4.10.9.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">141</td>
</tr>
<tr id="S5.T2.st2.4.11.10" class="ltx_tr">
<th id="S5.T2.st2.4.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.11.10.1.1" class="ltx_text ltx_font_smallcaps">occupation</span></th>
<td id="S5.T2.st2.4.11.10.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.83</td>
<td id="S5.T2.st2.4.11.10.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.80</td>
<td id="S5.T2.st2.4.11.10.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.81</td>
<td id="S5.T2.st2.4.11.10.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1,496</td>
</tr>
<tr id="S5.T2.st2.4.12.11" class="ltx_tr">
<th id="S5.T2.st2.4.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.12.11.1.1" class="ltx_text ltx_font_smallcaps">surname</span></th>
<td id="S5.T2.st2.4.12.11.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.86</td>
<td id="S5.T2.st2.4.12.11.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.82</td>
<td id="S5.T2.st2.4.12.11.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.84</td>
<td id="S5.T2.st2.4.12.11.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">1,835</td>
</tr>
<tr id="S5.T2.st2.4.13.12" class="ltx_tr">
<th id="S5.T2.st2.4.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.13.12.1.1" class="ltx_text ltx_font_smallcaps">surname_house.</span></th>
<td id="S5.T2.st2.4.13.12.2" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.72</td>
<td id="S5.T2.st2.4.13.12.3" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.80</td>
<td id="S5.T2.st2.4.13.12.4" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">0.76</td>
<td id="S5.T2.st2.4.13.12.5" class="ltx_td ltx_align_right" style="padding-left:4.0pt;padding-right:4.0pt;">519</td>
</tr>
<tr id="S5.T2.st2.4.14.13" class="ltx_tr">
<th id="S5.T2.st2.4.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.14.13.1.1" class="ltx_text ltx_font_bold">Total</span></th>
<td id="S5.T2.st2.4.14.13.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.14.13.2.1" class="ltx_text ltx_font_bold">0.85</span></td>
<td id="S5.T2.st2.4.14.13.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.14.13.3.1" class="ltx_text ltx_font_bold">0.85</span></td>
<td id="S5.T2.st2.4.14.13.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.14.13.4.1" class="ltx_text ltx_font_bold">0.85</span></td>
<td id="S5.T2.st2.4.14.13.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S5.T2.st2.4.14.13.5.1" class="ltx_text ltx_font_bold">13,923</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">The performance of the text recognition and household grouping model is shown in Tables <a href="#S5.T2.st1" title="In Table 2 ‣ 5.2.3 Full-page recognition results ‣ 5.2 Handwritten table recognition ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> and <a href="#S5.T2.st2" title="In Table 2 ‣ 5.2.3 Full-page recognition results ‣ 5.2 Handwritten table recognition ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. The CER obtained on the validation and test sets are 14.30% and 14.47% respectively. These values, which may seem rather high, reflect the quality of all the categories of information to be extracted at the level of the whole page. As these metrics are strongly affected by a shift in recognition: an extra word, for example, shifts the entire predicted sequence, they are very difficult to interpret. For this reason, the performance of each entity is presented in Table <a href="#S5.T2.st2" title="In Table 2 ‣ 5.2.3 Full-page recognition results ‣ 5.2 Handwritten table recognition ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. From this table, the F1 scores for all the fields, except the ”Observation” category, ranged from 70% for nationality to 98% for year of birth. These high scores show that the model is robust and generic enough to handle a large number of documents, image qualities and table templates.</p>
</div>
<div id="S5.SS2.SSS3.p2" class="ltx_para">
<p id="S5.SS2.SSS3.p2.1" class="ltx_p">From the table, we can also see that the information contained in the ”Observation” columns is very poorly recognized, with an F1 score of 16%. This can be explained by the fact that this category is very poorly represented during training: it appears only 388 times in the manual annotations of the training set, which means that the information is present in about 1% of the table rows.</p>
</div>
<div id="S5.SS2.SSS3.p3" class="ltx_para">
<p id="S5.SS2.SSS3.p3.1" class="ltx_p">Finally, although it definitely plays a role, performance in the other categories does not seem to be directly correlated with the number of elements in the training set. Our hypothesis is that the difference in performance between the different categories can be explained by several other factors:</p>
<ul id="S5.I3" class="ltx_itemize">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i1.p1" class="ltx_para">
<p id="S5.I3.i1.p1.1" class="ltx_p">Some entities are easier to recognize because the possible values are very limited: this is particularly the case for the age and year of birth categories;</p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i2.p1" class="ltx_para">
<p id="S5.I3.i2.p1.1" class="ltx_p">Others entities are more difficult to recognize because they may contain ditto entries, and some annotators have rewritten the text in the corresponding cell rather than annotating it as a ditto.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.SSS3.p4" class="ltx_para">
<p id="S5.SS2.SSS3.p4.1" class="ltx_p">In order to improve performance and make the results easier to interpret, further standardization of the annotations would be necessary, particularly to reduce the impact of this last factor.</p>
</div>
</section>
<section id="S5.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.4 </span>Household extraction</h4>

<div id="S5.SS2.SSS4.p1" class="ltx_para">
<p id="S5.SS2.SSS4.p1.1" class="ltx_p">Table <a href="#S5.T2.st2" title="In Table 2 ‣ 5.2.3 Full-page recognition results ‣ 5.2 Handwritten table recognition ‣ 5 Information extraction workflow ‣ The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a> also shows the performance on the household grouping task, which consists of predicting a different category for the surnames. We can see that 76% of the households were correctly grouped, which seems quite good considering the difficulty of the task. In fact, in some lists, the information is clearly annotated with brackets. But this is not always the case, and sometimes the information is not directly annotated but has to be inferred from the ’link’ category, making the task much more complex.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Distributed processing on HPC</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The Socface project leverages the capabilities of Arkindex, an open source document processing platform that offers a comprehensive suite of functionalities including document organization, visualization, processing, and export. However, we are faced with the monumental task of processing approximately 30 million images. To meet the demanding computational requirements of this huge dataset, we rely on public High-Performance Computing (HPC) resources. However, integration with the HPC infrastructure imposes specific constraints: the compute nodes are isolated from the Internet, requiring pre-staging of data on specialized local storage and orchestration of job submissions through dedicated scheduling systems such as SLURM.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">To overcome these limitations, we have developed a three-step strategy to extend Arkindex to take advantage of HPC resources:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Data preparation and pre-processing: Recognizing the lack of Internet connectivity on HPC computing nodes, the first stage is performed on front-end CPU nodes that do have Internet access. This step involves downloading the required dataset images from the IIIF server, along with essential processing metadata such as image dimensions. These elements are then stored on local storage, making them available to the HPC computing nodes for subsequent processing stages.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Image processing: With the data pre-positioned on local storage, processing shifts to the HPC’s GPU nodes. This stage uses the computing power of the GPUs to efficiently analyze the images. The results of this processing stage are encapsulated in JSON files, providing a structured representation of the results that can be easily transferred and interpreted in subsequent steps.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">Integration and monitoring of results: The final phase moves back to CPU nodes with Internet access. This is where the JSON files containing the processed data are uploaded to the Arkindex database. This step not only secures the processed data within Arkindex, but also facilitates real-time task status updates. Such updates are critical for monitoring the progress and success of processing tasks, providing insight into operational status, and ensuring that any necessary adjustments or re-processing can be addressed in a timely manner.</p>
</div>
</li>
</ul>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">To implement these various steps, Arkindex’s internal distributed task system has been significantly enhanced by integrating the PySlurm library<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://pyslurm.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pyslurm.github.io/</a></span></span></span>, which enables seamless communication with SLURM. This key development has effectively enabled Arkindex to take advantage of the immense computing resources available in HPC environments, significantly increasing its processing capacity to meet the demands of large-scale projects.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">We conducted a processing time evaluation using our distributed processing framework enabled by High Performance Computing (HPC) to manage the extraction of information from a batch of 450,000 images, processed using a distributed architecture that integrated 14 parallel processes on CPU nodes for the initial and final stages of the workflow, and used NVIDIA V100 GPUs to execute the deep learning model responsible for table recognition and entity typing.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">The breakdown of processing times for each stage of the workflow is as follows:</p>
<ul id="S6.I2" class="ltx_itemize">
<li id="S6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i1.p1" class="ltx_para">
<p id="S6.I2.i1.p1.1" class="ltx_p">Preprocessing Phase: This initial phase, mainly focused on image download, was completed in an average time of 1.6 seconds per image.</p>
</div>
</li>
<li id="S6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i2.p1" class="ltx_para">
<p id="S6.I2.i2.p1.1" class="ltx_p">Table Recognition and Entity Typing: The core processing task of recognizing full-page tables and typing entities within these tables using our deep learning model took an average of 12.5 seconds per image.</p>
</div>
</li>
<li id="S6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i3.p1" class="ltx_para">
<p id="S6.I2.i3.p1.1" class="ltx_p">Post-processing stage: The final phase, which included uploading the results to the database along with text position, line-level text recognition and entity tagging, took an average of 7.2 seconds per image.</p>
</div>
</li>
</ul>
<p id="S6.p5.2" class="ltx_p">The entire batch of 450,000 images was processed in less than 8 days, demonstrating the efficiency and scalability of our distributed processing approach using HPC resources.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we have presented a comprehensive workflow designed to automatically extract information from individual census tables spanning 20 censuses over a century, structured to closely follow the original format of the source documents. This methodology has already proven its effectiveness on thousands of images and will be scaled up to process millions more from numerous French departmental archives by the end of the project.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Our achievement lies in the development of a unified model capable of handling a wide variety of image types, table structures and handwriting styles. Using a transformer-based architecture, this model allows direct processing of entire tables without the need for prior segmentation, significantly minimizing the potential for errors commonly associated with multi-step processing approaches. Careful label generation ensures comprehensive information extraction across all table variants, covering both the content and the familial arrangements of the listed individuals.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">However, the current method has limitations, most notably the inability to process complete registers on a page-by-page basis while retaining the context of previously processed pages. This shortcoming requires additional post-processing to reassemble household units that span multiple pages. Future enhancements will focus on overcoming this challenge by enabling sequential processing of entire registers with the aim of preserving contextual continuity. We also plan to extend the processing to include address recognition, thereby facilitating the reconstruction of household compositions within individual houses, streets, hamlets, and sectors.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgments</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">The Socface project is funded by the French National Research Agency (ANR) under the fund ANR-21-CE38-0013. This work was granted access to the HPC resources of IDRIS under the allocation 2022-AD011013446 made by GENCI and was partially funded by the ACADIIE project ”Compréhension automatique des documents d’archives pour l’extraction d’informations individuelles” supported by a grant overseen by the French National Research Agency (ANR) as part of the France Relance program.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ares Oliveira, S., Seguin, B., Kaplan, F.: dhSegment: A Generic Deep-learning Approach for Document Segmentation. In: 16th International Conference on Frontiers in Handwriting Recognition (ICFHR). pp. 7–12 (Aug 2018)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bernard, G., Wall, C., Boillet, M., Coustaty, M., Kermorvant, C., Doucet, A.: Text Line Detection in Historical Index Tables: Evaluations on a New French PArish REcord Survey Dataset (PARES). In: Leveraging Generative Intelligence in Digital Libraries: Towards Human-Machine Collaboration. pp. 59–75. Springer Nature Singapore (Dec 2023). https://doi.org/10.1007/978-981-99-8085-7_6

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Biswas, S., Banerjee, A., Lladós, J., Pal, U.: DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer. In: arXiv preprint arXiv:2201.11438 (2022)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Boillet, M., Kermorvant, C., Paquet, T.: Multiple Document Datasets Pre-training Improves Text Line Detection With Deep Neural Networks. In: 25th International Conference on Pattern Recognition (ICPR). pp. 2134–2141 (Jan 2021)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Constum, T., Kempf, N., Paquet, T., Traounez, P., Chatelain, C., Bree, S., Merveille, F.: Recognition and Information Extraction in Historical Handwritten Tables: Toward Understanding Early 20th Century Paris Census. In: 15th International Workshop on Document Analysis Systems (DAS). p. 143–157 (May 2022). https://doi.org/10.1007/978-3-031-06555-2_10

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Coquenet, D., Chatelain, C., Paquet, T.: DAN: a segmentation-free document attention network for handwritten document recognition. In: IEEE Transactions on Pattern Analysis and Machine Intelligence. pp. 1–17. Institute of Electrical and Electronics Engineers (IEEE) (Jan 2023). https://doi.org/10.1109/tpami.2023.3235826

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Coquenet, D., Chatelain, C., Paquet, T.: End-to-end Handwritten Paragraph Text Recognition Using a Vertical Attention Network. In: IEEE Transactions on Pattern Analysis and Machine Intelligence. pp. 508–524 (Jan 2023). https://doi.org/10.1109/TPAMI.2022.3144899

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L., Kai Li, Li Fei-Fei: ImageNet: A Large-scale Hierarchical Image Database. In: IEEE Conference on Computer Vision and Pattern Recognition (ICPR). pp. 248–255 (Jun 2009). https://doi.org/10.1109/CVPR.2009.5206848

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Grüning, T., Leifert, G., Strauß, T., Labahn, R.: A Two-Stage Method for Text Line Detection in Historical Documents. In: International Journal on Document Analysis and Recognition (IJDAR). pp. 285–302 (Sep 2019)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 770–778 (Jun 2016)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kermorvant, C., Bardou, E., Blanco, M., Abadie, B.: Callico: a Versatile Open-Source Document Image Annotation Platform. In: Sumbitted to ICDAR2024 (2024)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Motte, C., Vouloir, M.C.: Le site cassini.ehess.fr. Un instrument d’observation pour une analyse du peuplement. Bulletin du Comité français de cartographie <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">191</span>, 68–84 (2007)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In: 28th International Conference on Neural Information Processing Systems (NIPS). p. 91–99 (Jun 2015)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634–4642 (Jun 2022)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Tarride, S., Maarand, M., Boillet, M., McGrath, J., Capel, E., Vézina, H., Kermorvant, C.: Large-scale genealogical information extraction from handwritten quebec parish records. Int. J. Doc. Anal. Recognit. <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">26</span>(3), 255–272 (jan 2023). https://doi.org/10.1007/s10032-023-00427-w, <a target="_blank" href="https://doi.org/10.1007/s10032-023-00427-w" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10032-023-00427-w</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Tarride, S., Boillet, M., Kermorvant, C.: Key-Value Information Extraction from Full Handwritten Pages. In: Document Analysis and Recognition - ICDAR 2023. pp. 185–204. Springer Nature Switzerland (Aug 2023). https://doi.org/10.1007/978-3-031-41679-8_11

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is All you Need. In: 31st International Conference on Neural Information Processing Systems (NIPS). p. 6000–6010 (Dec 2017)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.18705" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.18706" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.18706">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.18706" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.18707" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 18:14:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
