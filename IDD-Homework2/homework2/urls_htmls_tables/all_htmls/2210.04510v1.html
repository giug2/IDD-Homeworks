<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2210.04510] Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing</title><meta property="og:description" content="With the new generation of satellite technologies, the archives of remote sensing (RS) images are growing very fast.
To make the intrinsic information of each RS image easily accessible, visual question answering (VQA)â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2210.04510">

<!--Generated on Thu Mar 14 00:43:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\authorinfo</span>
<p id="p1.2" class="ltx_p">Further author information: (Send correspondence to Tim Siebert)
<br class="ltx_break">Tim Siebert: E-mail: siebert@tu-berlin.de
<br class="ltx_break">*Equal Contribution</p>
</div>
<h1 class="ltx_title ltx_title_document">Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tim Siebert<sup id="id3.2.id1" class="ltx_sup">âˆ—</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Technische Universitat Berlin, Einsteinufer 17, 10587, Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kai Norman Clasen<sup id="id4.2.id1" class="ltx_sup">âˆ—</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Technische Universitat Berlin, Einsteinufer 17, 10587, Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mahdyar Ravanbakhsh
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Technische Universitat Berlin, Einsteinufer 17, 10587, Berlin, Germany
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">BegÃ¼m Demir
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Technische Universitat Berlin, Einsteinufer 17, 10587, Berlin, Germany
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">With the new generation of satellite technologies, the archives of remote sensing (RS) images are growing very fast.
To make the intrinsic information of each RS image easily accessible, visual question answering (VQA) has been introduced in RS. VQA allows a user to formulate a free-form question concerning the content of RS images to extract generic information.
It has been shown that the fusion of the input modalities (i.e., image and text) is crucial for the performance of VQA systems.
Most of the current fusion approaches use modality-specific representations in their fusion modules instead of joint representation learning.
However, to discover the underlying relation between both the image and question modality, the model is required to learn the joint representation instead of simply combining (e.g., concatenating, adding, or multiplying) the modality-specific representations. We propose a multi-modal transformer-based architecture to overcome this issue.
Our proposed architecture consists of three main modules: i) the feature extraction module for extracting the modality-specific features; ii) the fusion module, which leverages a user-defined number of multi-modal transformer layers of the VisualBERT model (VB); and iii) the classification module to obtain the answer. In contrast to recently proposed transformer-based models in RS VQA, the presented architecture (called <em id="id5.id1.1" class="ltx_emph ltx_font_italic">VBFusion</em>) is not limited to specific questions, e.g., questions concerning pre-defined objects.
Experimental results obtained on the RSVQAxBEN and RSVQA-LR datasets (which are made up of RGB bands of Sentinel-2 images) demonstrate the effectiveness of VBFusion for VQA tasks in RS. To analyze the importance of using other spectral bands for the description of the complex content of RS images in the framework of VQA, we extend the RSVQAxBEN dataset to include all the spectral bands of Sentinel-2 images with 10m and 20m spatial resolution. Experimental results show the importance of utilizing these bands to characterize the land-use land-cover classes present in the images in the framework of VQA. The code of the proposed method is publicly available at <a target="_blank" href="https://git.tu-berlin.de/rsim/multi-modal-fusion-transformer-for-vqa-in-rs" title="" class="ltx_ref ltx_href">https://git.tu-berlin.de/rsim/multi-modal-fusion-transformer-for-vqa-in-rs</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Multi-modal transformer, visual question answering, deep learning, remote sensing.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With advances in satellite technology, remote sensing (RS) image archives are rapidly growing, providing an unprecedented amount of data, which is a great source for information extraction in the framework of several different Earth observation applications. As a result, there has been an increased demand for systems that provide an intuitive interface to this wealth of information.
For this purpose, the development of accurate visual question answering (VQA) systems has recently become an important research topic in RS [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">1</a></cite>].
VQA defines a framework that allows retrieval of use-case-specific information [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib2" title="" class="ltx_ref">2</a></cite>].
By asking a free-form question to a selected image, the user can query various types of information without any need for remote sensing-related expertise.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Most of the existing VQA architectures consist of three main modules: i) the feature extraction module; ii) the fusion module; and iii) the classification module.
The feature extraction module extracts high-level features for both input modalities (i.e., image and question text).
After encoding the input modalities, the fusion module is required to discover a cross-interaction between both features.
Since the model needs to select the relevant part of the image concerning the question, combining the features in a meaningful way is crucial.
Finally, the output of the fusion module is passed to the classification module to generate the natural language answer.
In RS, relatively few VQA architectures are investigated [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a></cite>].
In [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">1</a></cite>], a convolutional neural network (CNN) is applied as an image encoder in the feature extraction module.
The natural language encoder is based on the skip-thoughts [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib8" title="" class="ltx_ref">8</a></cite>] architecture, while the fusion module is defined based on a simple, non-learnable point-wise multiplication of the feature vectors.
In [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib4" title="" class="ltx_ref">4</a></cite>], image modality representation is provided by a VGG-16 network [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib9" title="" class="ltx_ref">9</a></cite>], which extracts two sets of image features: i) an image feature map extracted from the last convolutional layer; and ii) an image feature vector derived from the final fully connected layer.
The text modality representations are extracted by a gated-recurrent unit (GRU) [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib10" title="" class="ltx_ref">10</a></cite>].
The two modality-specific representations are merged by the fusion module that leverages the more complex mutual-attention component.
In [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib6" title="" class="ltx_ref">6</a></cite>], the authors reveal the potential of an effective fusion module to build competitive RS VQA models.
In the computer vision (CV) community, it has been shown that to discover the underlying relation between the modalities, a model is required to learn a joint representation instead of applying a simple combination (e.g., concatenation, addition, or multiplication) of the modality-specific feature vectors [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib11" title="" class="ltx_ref">11</a></cite>].
In [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib3" title="" class="ltx_ref">3</a></cite>], this knowledge is transferred to the RS VQA domain by introducing the first transformer-based architecture combined with an object detector pre-processor for the image features (called <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">CrossModal</em>).
The object detector is trained on an RS object detection dataset (i.e., the xView dataset [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib12" title="" class="ltx_ref">12</a></cite>]) to recognize target objects such as cars, buildings, and ships.
However, since the CrossModal architecture leverages objects defined in xView, the model is specialized for VQA tasks that contain those objects. In contrast, the largest RSVQA benchmark dataset (RSVQAxBEN [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>]) includes questions regarding objects not included in xView (e.g., the dataset contains questions that require a distinction between coniferous and broad-leaved trees).
In [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib7" title="" class="ltx_ref">7</a></cite>], a model called <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">Prompt-RSVQA</em> is proposed that takes advantage of the language transformer DistilBERT [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib14" title="" class="ltx_ref">14</a></cite>].
To this end, the image feature extraction module uses a pre-trained multi-class classification network to predict image labels, then projects the image labels into a word embedding space. Finally, the image features encoded as words and the question text are merged in the transformer-based fusion module. Using word embeddings of the image labels as image features in Prompt-RSVQA comes with limitations: i) the model is tailored to the dataset that the classifier is trained on; ii) some questions are not covered in this formulation (e.g., counting, comparison) since the relation between the class labels are not presented in the predicted label set.
The aforementioned models provide promising results, guiding the motivation towards fusion modules that exploit transformer-based architectures. However, they are limited to the datasets that the classifier or the object detector is trained on, which makes the model unable to learn questions that are not covered in this formulation (e.g., counting, comparison, and objects that are not in the classifier/object detector training set).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To overcome these issues, in this paper, we present a multi-modal transformer-based architecture that leverages a different number of multi-modal transformer layers of the VisualBERT [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib11" title="" class="ltx_ref">11</a></cite>] model as a fusion module.
Instead of applying an object detector, we utilize <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">BoxExtractor</em>, a more general box extractor which does not overemphasize objects, within the image feature extraction module.
The resulting boxes are fed into a ResNet [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib15" title="" class="ltx_ref">15</a></cite>] to generate image tokens as embeddings of the extracted boxes.
For the text modality, the BertTokenizer [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib16" title="" class="ltx_ref">16</a></cite>] is used to tokenize the questions.
Our fusion module takes modality-specific tokens as input and processes them with a user-defined number <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S1.p3.1.m1.1a"><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">l</annotation></semantics></math> of VisualBERT (VB) layers.
The classification module consists of a <span title="" class="ltx_glossaryref">Multi-Layer Perceptron (MLP)</span> that generates an output vector that represents the answer.
Experimental results obtained on large-scale RS VQA benchmark datasets [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a></cite>] (which only include the RGB bands of Sentinel-2 multispectral images) demonstrate the success of the proposed architecture (called <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">VBFusion</em>) compared to the standard RS VQA model.
To analyze the importance of the other spectral bands for characterization of the complex information content of Sentinel-2 images in the framework of VQA, we add the other 10m and all 20m spectral bands to the RSVQAxBEN [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>] dataset.
The results show that the inclusion of these spectral bands significantly improves the VQA performance, as the additional spectral information helps the model to take better advantage of the complex image data.
To the best of our knowledge, this is the first study to consider multispectral RS VQA that is not limited to the RGB bands.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The remaining sections are organized as follows; <a href="#S2" title="2 Proposed multi-modal transformer-based VQA architecture â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> will introduce the proposed architecture, including the feature extraction pipeline.
In <a href="#S3" title="3 Dataset Description and Experimental Setup â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>, we will introduce the datasets for the experiments, the experimental setup, and in <a href="#S4" title="4 Experimental Results â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> analyze our results.
We will conclude our work in <a href="#S5" title="5 CONCLUSION â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> and provide an outlook for future research directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Proposed multi-modal transformer-based VQA architecture</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2210.04510/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.10.2.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.2.1" class="ltx_text" style="font-size:90%;">A general overview of VBFusion, our multi-modal transformer-based VQA architecture.
Our architecture includes: i) a feature extraction module (<a href="#S2.SS1" title="2.1 Feature Extraction Module â€£ 2 Proposed multi-modal transformer-based VQA architecture â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">2.1</span></a>); ii) a fusion module (<a href="#S2.SS2" title="2.2 Fusion Module â€£ 2 Proposed multi-modal transformer-based VQA architecture â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">2.2</span></a>); and iii) a classification module (<a href="#S2.SS3" title="2.3 Classification Module â€£ 2 Proposed multi-modal transformer-based VQA architecture â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">2.3</span></a>).
An RS image <span id="S2.F1.2.1.1" class="ltx_text ltx_font_bold">I</span> and a question <span id="S2.F1.2.1.2" class="ltx_text ltx_font_bold">Q</span> about this image are considered as input.
Inside the VisualBERT model, the modality-specific features [<span id="S2.F1.2.1.3" class="ltx_text ltx_font_bold">Z</span>, <span id="S2.F1.2.1.4" class="ltx_text ltx_font_bold">T</span>] get projected to a hidden dimension and concatenated afterwards.
The concatenated features are then fed to <math id="S2.F1.2.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.F1.2.1.m1.1b"><mi id="S2.F1.2.1.m1.1.1" xref="S2.F1.2.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.F1.2.1.m1.1c"><ci id="S2.F1.2.1.m1.1.1.cmml" xref="S2.F1.2.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.2.1.m1.1d">l</annotation></semantics></math> Self-Attention Layers, providing an output for further processing in the classification module.
The output of the classification module is a vector representing the final answer <span id="S2.F1.2.1.5" class="ltx_text ltx_font_bold">A</span> to the question <span id="S2.F1.2.1.6" class="ltx_text ltx_font_bold">Q</span>.</span></figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.11" class="ltx_p">Visual question answering systems attempt to answer questions in natural language regarding an image input and can be formulated as a classification problem, where image and question are the input and the answer is considered as a label.
Given an image, question, answer triplet denoted as <math id="S2.p1.1.m1.3" class="ltx_Math" alttext="(\mathbf{I,Q,A})" display="inline"><semantics id="S2.p1.1.m1.3a"><mrow id="S2.p1.1.m1.3.4.2" xref="S2.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="S2.p1.1.m1.3.4.2.1" xref="S2.p1.1.m1.3.4.1.cmml">(</mo><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">ğˆ</mi><mo id="S2.p1.1.m1.3.4.2.2" xref="S2.p1.1.m1.3.4.1.cmml">,</mo><mi id="S2.p1.1.m1.2.2" xref="S2.p1.1.m1.2.2.cmml">ğ</mi><mo id="S2.p1.1.m1.3.4.2.3" xref="S2.p1.1.m1.3.4.1.cmml">,</mo><mi id="S2.p1.1.m1.3.3" xref="S2.p1.1.m1.3.3.cmml">ğ€</mi><mo stretchy="false" id="S2.p1.1.m1.3.4.2.4" xref="S2.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.3b"><vector id="S2.p1.1.m1.3.4.1.cmml" xref="S2.p1.1.m1.3.4.2"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">ğˆ</ci><ci id="S2.p1.1.m1.2.2.cmml" xref="S2.p1.1.m1.2.2">ğ</ci><ci id="S2.p1.1.m1.3.3.cmml" xref="S2.p1.1.m1.3.3">ğ€</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.3c">(\mathbf{I,Q,A})</annotation></semantics></math>, an VQA system inputs <math id="S2.p1.2.m2.2" class="ltx_Math" alttext="(\mathbf{I,Q})" display="inline"><semantics id="S2.p1.2.m2.2a"><mrow id="S2.p1.2.m2.2.3.2" xref="S2.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S2.p1.2.m2.2.3.2.1" xref="S2.p1.2.m2.2.3.1.cmml">(</mo><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">ğˆ</mi><mo id="S2.p1.2.m2.2.3.2.2" xref="S2.p1.2.m2.2.3.1.cmml">,</mo><mi id="S2.p1.2.m2.2.2" xref="S2.p1.2.m2.2.2.cmml">ğ</mi><mo stretchy="false" id="S2.p1.2.m2.2.3.2.3" xref="S2.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.2b"><interval closure="open" id="S2.p1.2.m2.2.3.1.cmml" xref="S2.p1.2.m2.2.3.2"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğˆ</ci><ci id="S2.p1.2.m2.2.2.cmml" xref="S2.p1.2.m2.2.2">ğ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.2c">(\mathbf{I,Q})</annotation></semantics></math> and predicts <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{A}" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">ğ€</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğ€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathbf{A}</annotation></semantics></math>.
We assume that a training set <math id="S2.p1.4.m4.3" class="ltx_math_unparsed" alttext="\mathcal{D}=\{\mathbf{I}_{n},\mathbf{Q}_{n}^{m},\mathbf{A}_{n}^{m}\}^{m=1..M_{n}}_{n=1..N}" display="inline"><semantics id="S2.p1.4.m4.3a"><mrow id="S2.p1.4.m4.3.3"><mi class="ltx_font_mathcaligraphic" id="S2.p1.4.m4.3.3.5">ğ’Ÿ</mi><mo id="S2.p1.4.m4.3.3.4">=</mo><msubsup id="S2.p1.4.m4.3.3.3"><mrow id="S2.p1.4.m4.3.3.3.3.3.3"><mo stretchy="false" id="S2.p1.4.m4.3.3.3.3.3.3.4">{</mo><msub id="S2.p1.4.m4.1.1.1.1.1.1.1"><mi id="S2.p1.4.m4.1.1.1.1.1.1.1.2">ğˆ</mi><mi id="S2.p1.4.m4.1.1.1.1.1.1.1.3">n</mi></msub><mo id="S2.p1.4.m4.3.3.3.3.3.3.5">,</mo><msubsup id="S2.p1.4.m4.2.2.2.2.2.2.2"><mi id="S2.p1.4.m4.2.2.2.2.2.2.2.2.2">ğ</mi><mi id="S2.p1.4.m4.2.2.2.2.2.2.2.2.3">n</mi><mi id="S2.p1.4.m4.2.2.2.2.2.2.2.3">m</mi></msubsup><mo id="S2.p1.4.m4.3.3.3.3.3.3.6">,</mo><msubsup id="S2.p1.4.m4.3.3.3.3.3.3.3"><mi id="S2.p1.4.m4.3.3.3.3.3.3.3.2.2">ğ€</mi><mi id="S2.p1.4.m4.3.3.3.3.3.3.3.2.3">n</mi><mi id="S2.p1.4.m4.3.3.3.3.3.3.3.3">m</mi></msubsup><mo stretchy="false" id="S2.p1.4.m4.3.3.3.3.3.3.7">}</mo></mrow><mrow id="S2.p1.4.m4.3.3.3.5"><mi id="S2.p1.4.m4.3.3.3.5.1">n</mi><mo id="S2.p1.4.m4.3.3.3.5.2">=</mo><mn id="S2.p1.4.m4.3.3.3.5.3">1</mn><mo lspace="0em" rspace="0.0835em" id="S2.p1.4.m4.3.3.3.5.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S2.p1.4.m4.3.3.3.5.5">.</mo><mi id="S2.p1.4.m4.3.3.3.5.6">N</mi></mrow><mrow id="S2.p1.4.m4.3.3.3.3.5"><mi id="S2.p1.4.m4.3.3.3.3.5.1">m</mi><mo id="S2.p1.4.m4.3.3.3.3.5.2">=</mo><mn id="S2.p1.4.m4.3.3.3.3.5.3">1</mn><mo lspace="0em" rspace="0.0835em" id="S2.p1.4.m4.3.3.3.3.5.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S2.p1.4.m4.3.3.3.3.5.5">.</mo><msub id="S2.p1.4.m4.3.3.3.3.5.6"><mi id="S2.p1.4.m4.3.3.3.3.5.6.2">M</mi><mi id="S2.p1.4.m4.3.3.3.3.5.6.3">n</mi></msub></mrow></msubsup></mrow><annotation encoding="application/x-tex" id="S2.p1.4.m4.3b">\mathcal{D}=\{\mathbf{I}_{n},\mathbf{Q}_{n}^{m},\mathbf{A}_{n}^{m}\}^{m=1..M_{n}}_{n=1..N}</annotation></semantics></math> that consists of <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">N</annotation></semantics></math> number of images is available, and for each <math id="S2.p1.6.m6.3" class="ltx_Math" alttext="n\in\{1,...,N\}" display="inline"><semantics id="S2.p1.6.m6.3a"><mrow id="S2.p1.6.m6.3.4" xref="S2.p1.6.m6.3.4.cmml"><mi id="S2.p1.6.m6.3.4.2" xref="S2.p1.6.m6.3.4.2.cmml">n</mi><mo id="S2.p1.6.m6.3.4.1" xref="S2.p1.6.m6.3.4.1.cmml">âˆˆ</mo><mrow id="S2.p1.6.m6.3.4.3.2" xref="S2.p1.6.m6.3.4.3.1.cmml"><mo stretchy="false" id="S2.p1.6.m6.3.4.3.2.1" xref="S2.p1.6.m6.3.4.3.1.cmml">{</mo><mn id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">1</mn><mo id="S2.p1.6.m6.3.4.3.2.2" xref="S2.p1.6.m6.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.p1.6.m6.2.2" xref="S2.p1.6.m6.2.2.cmml">â€¦</mi><mo id="S2.p1.6.m6.3.4.3.2.3" xref="S2.p1.6.m6.3.4.3.1.cmml">,</mo><mi id="S2.p1.6.m6.3.3" xref="S2.p1.6.m6.3.3.cmml">N</mi><mo stretchy="false" id="S2.p1.6.m6.3.4.3.2.4" xref="S2.p1.6.m6.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.3b"><apply id="S2.p1.6.m6.3.4.cmml" xref="S2.p1.6.m6.3.4"><in id="S2.p1.6.m6.3.4.1.cmml" xref="S2.p1.6.m6.3.4.1"></in><ci id="S2.p1.6.m6.3.4.2.cmml" xref="S2.p1.6.m6.3.4.2">ğ‘›</ci><set id="S2.p1.6.m6.3.4.3.1.cmml" xref="S2.p1.6.m6.3.4.3.2"><cn type="integer" id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">1</cn><ci id="S2.p1.6.m6.2.2.cmml" xref="S2.p1.6.m6.2.2">â€¦</ci><ci id="S2.p1.6.m6.3.3.cmml" xref="S2.p1.6.m6.3.3">ğ‘</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.3c">n\in\{1,...,N\}</annotation></semantics></math> the image <math id="S2.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{I}_{n}" display="inline"><semantics id="S2.p1.7.m7.1a"><msub id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml"><mi id="S2.p1.7.m7.1.1.2" xref="S2.p1.7.m7.1.1.2.cmml">ğˆ</mi><mi id="S2.p1.7.m7.1.1.3" xref="S2.p1.7.m7.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><apply id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p1.7.m7.1.1.1.cmml" xref="S2.p1.7.m7.1.1">subscript</csymbol><ci id="S2.p1.7.m7.1.1.2.cmml" xref="S2.p1.7.m7.1.1.2">ğˆ</ci><ci id="S2.p1.7.m7.1.1.3.cmml" xref="S2.p1.7.m7.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">\mathbf{I}_{n}</annotation></semantics></math> is associated with <math id="S2.p1.8.m8.1" class="ltx_Math" alttext="M_{n}" display="inline"><semantics id="S2.p1.8.m8.1a"><msub id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml"><mi id="S2.p1.8.m8.1.1.2" xref="S2.p1.8.m8.1.1.2.cmml">M</mi><mi id="S2.p1.8.m8.1.1.3" xref="S2.p1.8.m8.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><apply id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.p1.8.m8.1.1.1.cmml" xref="S2.p1.8.m8.1.1">subscript</csymbol><ci id="S2.p1.8.m8.1.1.2.cmml" xref="S2.p1.8.m8.1.1.2">ğ‘€</ci><ci id="S2.p1.8.m8.1.1.3.cmml" xref="S2.p1.8.m8.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">M_{n}</annotation></semantics></math> pairs of questions <math id="S2.p1.9.m9.1" class="ltx_Math" alttext="\{\mathbf{Q}_{n}^{m}\}_{m=1}^{M_{n}}" display="inline"><semantics id="S2.p1.9.m9.1a"><msubsup id="S2.p1.9.m9.1.1" xref="S2.p1.9.m9.1.1.cmml"><mrow id="S2.p1.9.m9.1.1.1.1.1" xref="S2.p1.9.m9.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.p1.9.m9.1.1.1.1.1.2" xref="S2.p1.9.m9.1.1.1.1.2.cmml">{</mo><msubsup id="S2.p1.9.m9.1.1.1.1.1.1" xref="S2.p1.9.m9.1.1.1.1.1.1.cmml"><mi id="S2.p1.9.m9.1.1.1.1.1.1.2.2" xref="S2.p1.9.m9.1.1.1.1.1.1.2.2.cmml">ğ</mi><mi id="S2.p1.9.m9.1.1.1.1.1.1.2.3" xref="S2.p1.9.m9.1.1.1.1.1.1.2.3.cmml">n</mi><mi id="S2.p1.9.m9.1.1.1.1.1.1.3" xref="S2.p1.9.m9.1.1.1.1.1.1.3.cmml">m</mi></msubsup><mo stretchy="false" id="S2.p1.9.m9.1.1.1.1.1.3" xref="S2.p1.9.m9.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.p1.9.m9.1.1.1.3" xref="S2.p1.9.m9.1.1.1.3.cmml"><mi id="S2.p1.9.m9.1.1.1.3.2" xref="S2.p1.9.m9.1.1.1.3.2.cmml">m</mi><mo id="S2.p1.9.m9.1.1.1.3.1" xref="S2.p1.9.m9.1.1.1.3.1.cmml">=</mo><mn id="S2.p1.9.m9.1.1.1.3.3" xref="S2.p1.9.m9.1.1.1.3.3.cmml">1</mn></mrow><msub id="S2.p1.9.m9.1.1.3" xref="S2.p1.9.m9.1.1.3.cmml"><mi id="S2.p1.9.m9.1.1.3.2" xref="S2.p1.9.m9.1.1.3.2.cmml">M</mi><mi id="S2.p1.9.m9.1.1.3.3" xref="S2.p1.9.m9.1.1.3.3.cmml">n</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.p1.9.m9.1b"><apply id="S2.p1.9.m9.1.1.cmml" xref="S2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p1.9.m9.1.1.2.cmml" xref="S2.p1.9.m9.1.1">superscript</csymbol><apply id="S2.p1.9.m9.1.1.1.cmml" xref="S2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p1.9.m9.1.1.1.2.cmml" xref="S2.p1.9.m9.1.1">subscript</csymbol><set id="S2.p1.9.m9.1.1.1.1.2.cmml" xref="S2.p1.9.m9.1.1.1.1.1"><apply id="S2.p1.9.m9.1.1.1.1.1.1.cmml" xref="S2.p1.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.9.m9.1.1.1.1.1.1.1.cmml" xref="S2.p1.9.m9.1.1.1.1.1.1">superscript</csymbol><apply id="S2.p1.9.m9.1.1.1.1.1.1.2.cmml" xref="S2.p1.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.9.m9.1.1.1.1.1.1.2.1.cmml" xref="S2.p1.9.m9.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p1.9.m9.1.1.1.1.1.1.2.2.cmml" xref="S2.p1.9.m9.1.1.1.1.1.1.2.2">ğ</ci><ci id="S2.p1.9.m9.1.1.1.1.1.1.2.3.cmml" xref="S2.p1.9.m9.1.1.1.1.1.1.2.3">ğ‘›</ci></apply><ci id="S2.p1.9.m9.1.1.1.1.1.1.3.cmml" xref="S2.p1.9.m9.1.1.1.1.1.1.3">ğ‘š</ci></apply></set><apply id="S2.p1.9.m9.1.1.1.3.cmml" xref="S2.p1.9.m9.1.1.1.3"><eq id="S2.p1.9.m9.1.1.1.3.1.cmml" xref="S2.p1.9.m9.1.1.1.3.1"></eq><ci id="S2.p1.9.m9.1.1.1.3.2.cmml" xref="S2.p1.9.m9.1.1.1.3.2">ğ‘š</ci><cn type="integer" id="S2.p1.9.m9.1.1.1.3.3.cmml" xref="S2.p1.9.m9.1.1.1.3.3">1</cn></apply></apply><apply id="S2.p1.9.m9.1.1.3.cmml" xref="S2.p1.9.m9.1.1.3"><csymbol cd="ambiguous" id="S2.p1.9.m9.1.1.3.1.cmml" xref="S2.p1.9.m9.1.1.3">subscript</csymbol><ci id="S2.p1.9.m9.1.1.3.2.cmml" xref="S2.p1.9.m9.1.1.3.2">ğ‘€</ci><ci id="S2.p1.9.m9.1.1.3.3.cmml" xref="S2.p1.9.m9.1.1.3.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m9.1c">\{\mathbf{Q}_{n}^{m}\}_{m=1}^{M_{n}}</annotation></semantics></math> and answers <math id="S2.p1.10.m10.1" class="ltx_Math" alttext="\{\mathbf{A}_{n}^{m}\}_{m=1}^{M_{n}}" display="inline"><semantics id="S2.p1.10.m10.1a"><msubsup id="S2.p1.10.m10.1.1" xref="S2.p1.10.m10.1.1.cmml"><mrow id="S2.p1.10.m10.1.1.1.1.1" xref="S2.p1.10.m10.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.p1.10.m10.1.1.1.1.1.2" xref="S2.p1.10.m10.1.1.1.1.2.cmml">{</mo><msubsup id="S2.p1.10.m10.1.1.1.1.1.1" xref="S2.p1.10.m10.1.1.1.1.1.1.cmml"><mi id="S2.p1.10.m10.1.1.1.1.1.1.2.2" xref="S2.p1.10.m10.1.1.1.1.1.1.2.2.cmml">ğ€</mi><mi id="S2.p1.10.m10.1.1.1.1.1.1.2.3" xref="S2.p1.10.m10.1.1.1.1.1.1.2.3.cmml">n</mi><mi id="S2.p1.10.m10.1.1.1.1.1.1.3" xref="S2.p1.10.m10.1.1.1.1.1.1.3.cmml">m</mi></msubsup><mo stretchy="false" id="S2.p1.10.m10.1.1.1.1.1.3" xref="S2.p1.10.m10.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.p1.10.m10.1.1.1.3" xref="S2.p1.10.m10.1.1.1.3.cmml"><mi id="S2.p1.10.m10.1.1.1.3.2" xref="S2.p1.10.m10.1.1.1.3.2.cmml">m</mi><mo id="S2.p1.10.m10.1.1.1.3.1" xref="S2.p1.10.m10.1.1.1.3.1.cmml">=</mo><mn id="S2.p1.10.m10.1.1.1.3.3" xref="S2.p1.10.m10.1.1.1.3.3.cmml">1</mn></mrow><msub id="S2.p1.10.m10.1.1.3" xref="S2.p1.10.m10.1.1.3.cmml"><mi id="S2.p1.10.m10.1.1.3.2" xref="S2.p1.10.m10.1.1.3.2.cmml">M</mi><mi id="S2.p1.10.m10.1.1.3.3" xref="S2.p1.10.m10.1.1.3.3.cmml">n</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.p1.10.m10.1b"><apply id="S2.p1.10.m10.1.1.cmml" xref="S2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.p1.10.m10.1.1.2.cmml" xref="S2.p1.10.m10.1.1">superscript</csymbol><apply id="S2.p1.10.m10.1.1.1.cmml" xref="S2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.p1.10.m10.1.1.1.2.cmml" xref="S2.p1.10.m10.1.1">subscript</csymbol><set id="S2.p1.10.m10.1.1.1.1.2.cmml" xref="S2.p1.10.m10.1.1.1.1.1"><apply id="S2.p1.10.m10.1.1.1.1.1.1.cmml" xref="S2.p1.10.m10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.10.m10.1.1.1.1.1.1.1.cmml" xref="S2.p1.10.m10.1.1.1.1.1.1">superscript</csymbol><apply id="S2.p1.10.m10.1.1.1.1.1.1.2.cmml" xref="S2.p1.10.m10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.10.m10.1.1.1.1.1.1.2.1.cmml" xref="S2.p1.10.m10.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p1.10.m10.1.1.1.1.1.1.2.2.cmml" xref="S2.p1.10.m10.1.1.1.1.1.1.2.2">ğ€</ci><ci id="S2.p1.10.m10.1.1.1.1.1.1.2.3.cmml" xref="S2.p1.10.m10.1.1.1.1.1.1.2.3">ğ‘›</ci></apply><ci id="S2.p1.10.m10.1.1.1.1.1.1.3.cmml" xref="S2.p1.10.m10.1.1.1.1.1.1.3">ğ‘š</ci></apply></set><apply id="S2.p1.10.m10.1.1.1.3.cmml" xref="S2.p1.10.m10.1.1.1.3"><eq id="S2.p1.10.m10.1.1.1.3.1.cmml" xref="S2.p1.10.m10.1.1.1.3.1"></eq><ci id="S2.p1.10.m10.1.1.1.3.2.cmml" xref="S2.p1.10.m10.1.1.1.3.2">ğ‘š</ci><cn type="integer" id="S2.p1.10.m10.1.1.1.3.3.cmml" xref="S2.p1.10.m10.1.1.1.3.3">1</cn></apply></apply><apply id="S2.p1.10.m10.1.1.3.cmml" xref="S2.p1.10.m10.1.1.3"><csymbol cd="ambiguous" id="S2.p1.10.m10.1.1.3.1.cmml" xref="S2.p1.10.m10.1.1.3">subscript</csymbol><ci id="S2.p1.10.m10.1.1.3.2.cmml" xref="S2.p1.10.m10.1.1.3.2">ğ‘€</ci><ci id="S2.p1.10.m10.1.1.3.3.cmml" xref="S2.p1.10.m10.1.1.3.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.10.m10.1c">\{\mathbf{A}_{n}^{m}\}_{m=1}^{M_{n}}</annotation></semantics></math>.
Given the training set <math id="S2.p1.11.m11.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S2.p1.11.m11.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.11.m11.1.1" xref="S2.p1.11.m11.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S2.p1.11.m11.1b"><ci id="S2.p1.11.m11.1.1.cmml" xref="S2.p1.11.m11.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.11.m11.1c">\mathcal{D}</annotation></semantics></math>, the proposed multi-modal fusion transformer aims to learn a joint representation (from images and questions) and a classification head to obtain the answer.
To this end, the proposed architecture includes: i) a feature extraction module based on the BoxExtractor and the BertTokenizer; ii) a fusion module based on a user-defined number of multi-modal transformer layers of VisualBERT; and iii) a classification module consisting of an <span title="" class="ltx_glossaryref">MLP</span> projection head.
<a href="#S2.F1" title="In 2 Proposed multi-modal transformer-based VQA architecture â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> shows the general overview of our multi-modal transformer-based VQA architecture VBFusion.
Each module is explained in detail in the following.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Feature Extraction Module</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.2" class="ltx_p">The feature extraction module aims to extract relevant features for text and image modality independently from each other. To this end, the feature extraction module consists of two modality-specific encoder networks: i) image modality encoder <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">f</annotation></semantics></math>; and ii) text modality encoder <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">g</annotation></semantics></math>.
In the case of the image modality, we first utilize a simplified box extractor followed by an image encoder network. Then the output of the image encoder network is transformed with an <span title="" class="ltx_glossaryref">MLP</span> layer into a suitable shape for the fusion module. It is worth noting that, to extract the image features, in CV it is common to guide the focus of a model to the relevant areas of the image [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a></cite>]. For this reason, the state-of-the-art VQA models in CV leverage an object detector to focus on important regions (i.e., objects). For example, in VisualBERT [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib11" title="" class="ltx_ref">11</a></cite>] a Faster R-CNN [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib18" title="" class="ltx_ref">18</a></cite>] backbone is used to extract the relevant regions of the image. To extract regions of interest and detect objects present in RS images, semantic segmentation models are often applied. Such models require the availability of reliable pixel-based ground reference samples to be used in the training phase.
The collection of a sufficient number of reliable labeled samples is time-consuming, complex, and costly in operational scenarios and can significantly affect the final accuracy of object detection. To overcome this issue, we propose <em id="S2.SS1.p1.2.1" class="ltx_emph ltx_font_italic">BoxExtractor</em> which generates rectangular boxes, selecting a region of interest without requiring any labeled training samples.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The image feature extraction pipeline is illustrated in <a href="#S2.F2" title="In 2.1 Feature Extraction Module â€£ 2 Proposed multi-modal transformer-based VQA architecture â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2210.04510/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="137" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.30.13.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.24.12" class="ltx_text" style="font-size:90%;">The image feature extraction module takes an image <span id="S2.F2.24.12.1" class="ltx_text ltx_font_bold">I</span> as input and outputs a feature map <span id="S2.F2.24.12.2" class="ltx_text ltx_font_bold">Z</span>. First, the BoxExtractor <math id="S2.F2.13.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.F2.13.1.m1.1b"><mi id="S2.F2.13.1.m1.1.1" xref="S2.F2.13.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.F2.13.1.m1.1c"><ci id="S2.F2.13.1.m1.1.1.cmml" xref="S2.F2.13.1.m1.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.13.1.m1.1d">B</annotation></semantics></math> selects regions of the input image <span id="S2.F2.24.12.3" class="ltx_text ltx_font_bold">I</span> and creates <math id="S2.F2.14.2.m2.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.F2.14.2.m2.1b"><mi id="S2.F2.14.2.m2.1.1" xref="S2.F2.14.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.F2.14.2.m2.1c"><ci id="S2.F2.14.2.m2.1.1.cmml" xref="S2.F2.14.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.14.2.m2.1d">b</annotation></semantics></math> raw boxes, which has the sizes <math id="S2.F2.15.3.m3.1" class="ltx_Math" alttext="c\times H_{i}^{\prime}\times W_{i}^{\prime}" display="inline"><semantics id="S2.F2.15.3.m3.1b"><mrow id="S2.F2.15.3.m3.1.1" xref="S2.F2.15.3.m3.1.1.cmml"><mi id="S2.F2.15.3.m3.1.1.2" xref="S2.F2.15.3.m3.1.1.2.cmml">c</mi><mo lspace="0.222em" rspace="0.222em" id="S2.F2.15.3.m3.1.1.1" xref="S2.F2.15.3.m3.1.1.1.cmml">Ã—</mo><msubsup id="S2.F2.15.3.m3.1.1.3" xref="S2.F2.15.3.m3.1.1.3.cmml"><mi id="S2.F2.15.3.m3.1.1.3.2.2" xref="S2.F2.15.3.m3.1.1.3.2.2.cmml">H</mi><mi id="S2.F2.15.3.m3.1.1.3.2.3" xref="S2.F2.15.3.m3.1.1.3.2.3.cmml">i</mi><mo id="S2.F2.15.3.m3.1.1.3.3" xref="S2.F2.15.3.m3.1.1.3.3.cmml">â€²</mo></msubsup><mo lspace="0.222em" rspace="0.222em" id="S2.F2.15.3.m3.1.1.1b" xref="S2.F2.15.3.m3.1.1.1.cmml">Ã—</mo><msubsup id="S2.F2.15.3.m3.1.1.4" xref="S2.F2.15.3.m3.1.1.4.cmml"><mi id="S2.F2.15.3.m3.1.1.4.2.2" xref="S2.F2.15.3.m3.1.1.4.2.2.cmml">W</mi><mi id="S2.F2.15.3.m3.1.1.4.2.3" xref="S2.F2.15.3.m3.1.1.4.2.3.cmml">i</mi><mo id="S2.F2.15.3.m3.1.1.4.3" xref="S2.F2.15.3.m3.1.1.4.3.cmml">â€²</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.15.3.m3.1c"><apply id="S2.F2.15.3.m3.1.1.cmml" xref="S2.F2.15.3.m3.1.1"><times id="S2.F2.15.3.m3.1.1.1.cmml" xref="S2.F2.15.3.m3.1.1.1"></times><ci id="S2.F2.15.3.m3.1.1.2.cmml" xref="S2.F2.15.3.m3.1.1.2">ğ‘</ci><apply id="S2.F2.15.3.m3.1.1.3.cmml" xref="S2.F2.15.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.F2.15.3.m3.1.1.3.1.cmml" xref="S2.F2.15.3.m3.1.1.3">superscript</csymbol><apply id="S2.F2.15.3.m3.1.1.3.2.cmml" xref="S2.F2.15.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.F2.15.3.m3.1.1.3.2.1.cmml" xref="S2.F2.15.3.m3.1.1.3">subscript</csymbol><ci id="S2.F2.15.3.m3.1.1.3.2.2.cmml" xref="S2.F2.15.3.m3.1.1.3.2.2">ğ»</ci><ci id="S2.F2.15.3.m3.1.1.3.2.3.cmml" xref="S2.F2.15.3.m3.1.1.3.2.3">ğ‘–</ci></apply><ci id="S2.F2.15.3.m3.1.1.3.3.cmml" xref="S2.F2.15.3.m3.1.1.3.3">â€²</ci></apply><apply id="S2.F2.15.3.m3.1.1.4.cmml" xref="S2.F2.15.3.m3.1.1.4"><csymbol cd="ambiguous" id="S2.F2.15.3.m3.1.1.4.1.cmml" xref="S2.F2.15.3.m3.1.1.4">superscript</csymbol><apply id="S2.F2.15.3.m3.1.1.4.2.cmml" xref="S2.F2.15.3.m3.1.1.4"><csymbol cd="ambiguous" id="S2.F2.15.3.m3.1.1.4.2.1.cmml" xref="S2.F2.15.3.m3.1.1.4">subscript</csymbol><ci id="S2.F2.15.3.m3.1.1.4.2.2.cmml" xref="S2.F2.15.3.m3.1.1.4.2.2">ğ‘Š</ci><ci id="S2.F2.15.3.m3.1.1.4.2.3.cmml" xref="S2.F2.15.3.m3.1.1.4.2.3">ğ‘–</ci></apply><ci id="S2.F2.15.3.m3.1.1.4.3.cmml" xref="S2.F2.15.3.m3.1.1.4.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.15.3.m3.1d">c\times H_{i}^{\prime}\times W_{i}^{\prime}</annotation></semantics></math> with <math id="S2.F2.16.4.m4.3" class="ltx_Math" alttext="i=1,...,b" display="inline"><semantics id="S2.F2.16.4.m4.3b"><mrow id="S2.F2.16.4.m4.3.4" xref="S2.F2.16.4.m4.3.4.cmml"><mi id="S2.F2.16.4.m4.3.4.2" xref="S2.F2.16.4.m4.3.4.2.cmml">i</mi><mo id="S2.F2.16.4.m4.3.4.1" xref="S2.F2.16.4.m4.3.4.1.cmml">=</mo><mrow id="S2.F2.16.4.m4.3.4.3.2" xref="S2.F2.16.4.m4.3.4.3.1.cmml"><mn id="S2.F2.16.4.m4.1.1" xref="S2.F2.16.4.m4.1.1.cmml">1</mn><mo id="S2.F2.16.4.m4.3.4.3.2.1" xref="S2.F2.16.4.m4.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.F2.16.4.m4.2.2" xref="S2.F2.16.4.m4.2.2.cmml">â€¦</mi><mo id="S2.F2.16.4.m4.3.4.3.2.2" xref="S2.F2.16.4.m4.3.4.3.1.cmml">,</mo><mi id="S2.F2.16.4.m4.3.3" xref="S2.F2.16.4.m4.3.3.cmml">b</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.16.4.m4.3c"><apply id="S2.F2.16.4.m4.3.4.cmml" xref="S2.F2.16.4.m4.3.4"><eq id="S2.F2.16.4.m4.3.4.1.cmml" xref="S2.F2.16.4.m4.3.4.1"></eq><ci id="S2.F2.16.4.m4.3.4.2.cmml" xref="S2.F2.16.4.m4.3.4.2">ğ‘–</ci><list id="S2.F2.16.4.m4.3.4.3.1.cmml" xref="S2.F2.16.4.m4.3.4.3.2"><cn type="integer" id="S2.F2.16.4.m4.1.1.cmml" xref="S2.F2.16.4.m4.1.1">1</cn><ci id="S2.F2.16.4.m4.2.2.cmml" xref="S2.F2.16.4.m4.2.2">â€¦</ci><ci id="S2.F2.16.4.m4.3.3.cmml" xref="S2.F2.16.4.m4.3.3">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.16.4.m4.3d">i=1,...,b</annotation></semantics></math>.
These raw boxes are interpolated to the size <math id="S2.F2.17.5.m5.1" class="ltx_Math" alttext="c\times H^{\prime}\times W^{\prime}" display="inline"><semantics id="S2.F2.17.5.m5.1b"><mrow id="S2.F2.17.5.m5.1.1" xref="S2.F2.17.5.m5.1.1.cmml"><mi id="S2.F2.17.5.m5.1.1.2" xref="S2.F2.17.5.m5.1.1.2.cmml">c</mi><mo lspace="0.222em" rspace="0.222em" id="S2.F2.17.5.m5.1.1.1" xref="S2.F2.17.5.m5.1.1.1.cmml">Ã—</mo><msup id="S2.F2.17.5.m5.1.1.3" xref="S2.F2.17.5.m5.1.1.3.cmml"><mi id="S2.F2.17.5.m5.1.1.3.2" xref="S2.F2.17.5.m5.1.1.3.2.cmml">H</mi><mo id="S2.F2.17.5.m5.1.1.3.3" xref="S2.F2.17.5.m5.1.1.3.3.cmml">â€²</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S2.F2.17.5.m5.1.1.1b" xref="S2.F2.17.5.m5.1.1.1.cmml">Ã—</mo><msup id="S2.F2.17.5.m5.1.1.4" xref="S2.F2.17.5.m5.1.1.4.cmml"><mi id="S2.F2.17.5.m5.1.1.4.2" xref="S2.F2.17.5.m5.1.1.4.2.cmml">W</mi><mo id="S2.F2.17.5.m5.1.1.4.3" xref="S2.F2.17.5.m5.1.1.4.3.cmml">â€²</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.17.5.m5.1c"><apply id="S2.F2.17.5.m5.1.1.cmml" xref="S2.F2.17.5.m5.1.1"><times id="S2.F2.17.5.m5.1.1.1.cmml" xref="S2.F2.17.5.m5.1.1.1"></times><ci id="S2.F2.17.5.m5.1.1.2.cmml" xref="S2.F2.17.5.m5.1.1.2">ğ‘</ci><apply id="S2.F2.17.5.m5.1.1.3.cmml" xref="S2.F2.17.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.F2.17.5.m5.1.1.3.1.cmml" xref="S2.F2.17.5.m5.1.1.3">superscript</csymbol><ci id="S2.F2.17.5.m5.1.1.3.2.cmml" xref="S2.F2.17.5.m5.1.1.3.2">ğ»</ci><ci id="S2.F2.17.5.m5.1.1.3.3.cmml" xref="S2.F2.17.5.m5.1.1.3.3">â€²</ci></apply><apply id="S2.F2.17.5.m5.1.1.4.cmml" xref="S2.F2.17.5.m5.1.1.4"><csymbol cd="ambiguous" id="S2.F2.17.5.m5.1.1.4.1.cmml" xref="S2.F2.17.5.m5.1.1.4">superscript</csymbol><ci id="S2.F2.17.5.m5.1.1.4.2.cmml" xref="S2.F2.17.5.m5.1.1.4.2">ğ‘Š</ci><ci id="S2.F2.17.5.m5.1.1.4.3.cmml" xref="S2.F2.17.5.m5.1.1.4.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.17.5.m5.1d">c\times H^{\prime}\times W^{\prime}</annotation></semantics></math> resulting in the vector of boxes <math id="S2.F2.18.6.m6.1" class="ltx_Math" alttext="\textbf{I}^{\prime}=B(\textbf{I})" display="inline"><semantics id="S2.F2.18.6.m6.1b"><mrow id="S2.F2.18.6.m6.1.2" xref="S2.F2.18.6.m6.1.2.cmml"><msup id="S2.F2.18.6.m6.1.2.2" xref="S2.F2.18.6.m6.1.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S2.F2.18.6.m6.1.2.2.2" xref="S2.F2.18.6.m6.1.2.2.2a.cmml">I</mtext><mo id="S2.F2.18.6.m6.1.2.2.3" xref="S2.F2.18.6.m6.1.2.2.3.cmml">â€²</mo></msup><mo id="S2.F2.18.6.m6.1.2.1" xref="S2.F2.18.6.m6.1.2.1.cmml">=</mo><mrow id="S2.F2.18.6.m6.1.2.3" xref="S2.F2.18.6.m6.1.2.3.cmml"><mi id="S2.F2.18.6.m6.1.2.3.2" xref="S2.F2.18.6.m6.1.2.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.F2.18.6.m6.1.2.3.1" xref="S2.F2.18.6.m6.1.2.3.1.cmml">â€‹</mo><mrow id="S2.F2.18.6.m6.1.2.3.3.2" xref="S2.F2.18.6.m6.1.1a.cmml"><mo stretchy="false" id="S2.F2.18.6.m6.1.2.3.3.2.1" xref="S2.F2.18.6.m6.1.1a.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S2.F2.18.6.m6.1.1" xref="S2.F2.18.6.m6.1.1.cmml">I</mtext><mo stretchy="false" id="S2.F2.18.6.m6.1.2.3.3.2.2" xref="S2.F2.18.6.m6.1.1a.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.18.6.m6.1c"><apply id="S2.F2.18.6.m6.1.2.cmml" xref="S2.F2.18.6.m6.1.2"><eq id="S2.F2.18.6.m6.1.2.1.cmml" xref="S2.F2.18.6.m6.1.2.1"></eq><apply id="S2.F2.18.6.m6.1.2.2.cmml" xref="S2.F2.18.6.m6.1.2.2"><csymbol cd="ambiguous" id="S2.F2.18.6.m6.1.2.2.1.cmml" xref="S2.F2.18.6.m6.1.2.2">superscript</csymbol><ci id="S2.F2.18.6.m6.1.2.2.2a.cmml" xref="S2.F2.18.6.m6.1.2.2.2"><mtext class="ltx_mathvariant_bold" id="S2.F2.18.6.m6.1.2.2.2.cmml" xref="S2.F2.18.6.m6.1.2.2.2">I</mtext></ci><ci id="S2.F2.18.6.m6.1.2.2.3.cmml" xref="S2.F2.18.6.m6.1.2.2.3">â€²</ci></apply><apply id="S2.F2.18.6.m6.1.2.3.cmml" xref="S2.F2.18.6.m6.1.2.3"><times id="S2.F2.18.6.m6.1.2.3.1.cmml" xref="S2.F2.18.6.m6.1.2.3.1"></times><ci id="S2.F2.18.6.m6.1.2.3.2.cmml" xref="S2.F2.18.6.m6.1.2.3.2">ğµ</ci><ci id="S2.F2.18.6.m6.1.1a.cmml" xref="S2.F2.18.6.m6.1.2.3.3.2"><mtext class="ltx_mathvariant_bold" id="S2.F2.18.6.m6.1.1.cmml" xref="S2.F2.18.6.m6.1.1">I</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.18.6.m6.1d">\textbf{I}^{\prime}=B(\textbf{I})</annotation></semantics></math>. To further process <math id="S2.F2.19.7.m7.1" class="ltx_Math" alttext="\textbf{I}^{\prime}" display="inline"><semantics id="S2.F2.19.7.m7.1b"><msup id="S2.F2.19.7.m7.1.1" xref="S2.F2.19.7.m7.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.F2.19.7.m7.1.1.2" xref="S2.F2.19.7.m7.1.1.2a.cmml">I</mtext><mo id="S2.F2.19.7.m7.1.1.3" xref="S2.F2.19.7.m7.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S2.F2.19.7.m7.1c"><apply id="S2.F2.19.7.m7.1.1.cmml" xref="S2.F2.19.7.m7.1.1"><csymbol cd="ambiguous" id="S2.F2.19.7.m7.1.1.1.cmml" xref="S2.F2.19.7.m7.1.1">superscript</csymbol><ci id="S2.F2.19.7.m7.1.1.2a.cmml" xref="S2.F2.19.7.m7.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.F2.19.7.m7.1.1.2.cmml" xref="S2.F2.19.7.m7.1.1.2">I</mtext></ci><ci id="S2.F2.19.7.m7.1.1.3.cmml" xref="S2.F2.19.7.m7.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.19.7.m7.1d">\textbf{I}^{\prime}</annotation></semantics></math>, the image encoder network <math id="S2.F2.20.8.m8.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.F2.20.8.m8.1b"><mi id="S2.F2.20.8.m8.1.1" xref="S2.F2.20.8.m8.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.F2.20.8.m8.1c"><ci id="S2.F2.20.8.m8.1.1.cmml" xref="S2.F2.20.8.m8.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.20.8.m8.1d">f</annotation></semantics></math> is leveraged.
The output of <math id="S2.F2.21.9.m9.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.F2.21.9.m9.1b"><mi id="S2.F2.21.9.m9.1.1" xref="S2.F2.21.9.m9.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.F2.21.9.m9.1c"><ci id="S2.F2.21.9.m9.1.1.cmml" xref="S2.F2.21.9.m9.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.21.9.m9.1d">f</annotation></semantics></math>, <math id="S2.F2.22.10.m10.1" class="ltx_Math" alttext="\textbf{I}^{\star}" display="inline"><semantics id="S2.F2.22.10.m10.1b"><msup id="S2.F2.22.10.m10.1.1" xref="S2.F2.22.10.m10.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.F2.22.10.m10.1.1.2" xref="S2.F2.22.10.m10.1.1.2a.cmml">I</mtext><mo id="S2.F2.22.10.m10.1.1.3" xref="S2.F2.22.10.m10.1.1.3.cmml">â‹†</mo></msup><annotation-xml encoding="MathML-Content" id="S2.F2.22.10.m10.1c"><apply id="S2.F2.22.10.m10.1.1.cmml" xref="S2.F2.22.10.m10.1.1"><csymbol cd="ambiguous" id="S2.F2.22.10.m10.1.1.1.cmml" xref="S2.F2.22.10.m10.1.1">superscript</csymbol><ci id="S2.F2.22.10.m10.1.1.2a.cmml" xref="S2.F2.22.10.m10.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.F2.22.10.m10.1.1.2.cmml" xref="S2.F2.22.10.m10.1.1.2">I</mtext></ci><ci id="S2.F2.22.10.m10.1.1.3.cmml" xref="S2.F2.22.10.m10.1.1.3">â‹†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.22.10.m10.1d">\textbf{I}^{\star}</annotation></semantics></math> (size <math id="S2.F2.23.11.m11.1" class="ltx_Math" alttext="b\times K" display="inline"><semantics id="S2.F2.23.11.m11.1b"><mrow id="S2.F2.23.11.m11.1.1" xref="S2.F2.23.11.m11.1.1.cmml"><mi id="S2.F2.23.11.m11.1.1.2" xref="S2.F2.23.11.m11.1.1.2.cmml">b</mi><mo lspace="0.222em" rspace="0.222em" id="S2.F2.23.11.m11.1.1.1" xref="S2.F2.23.11.m11.1.1.1.cmml">Ã—</mo><mi id="S2.F2.23.11.m11.1.1.3" xref="S2.F2.23.11.m11.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.23.11.m11.1c"><apply id="S2.F2.23.11.m11.1.1.cmml" xref="S2.F2.23.11.m11.1.1"><times id="S2.F2.23.11.m11.1.1.1.cmml" xref="S2.F2.23.11.m11.1.1.1"></times><ci id="S2.F2.23.11.m11.1.1.2.cmml" xref="S2.F2.23.11.m11.1.1.2">ğ‘</ci><ci id="S2.F2.23.11.m11.1.1.3.cmml" xref="S2.F2.23.11.m11.1.1.3">ğ¾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.23.11.m11.1d">b\times K</annotation></semantics></math>) is then projected to image features <span id="S2.F2.24.12.4" class="ltx_text ltx_font_bold">Z</span> (size <math id="S2.F2.24.12.m12.1" class="ltx_Math" alttext="b\times v" display="inline"><semantics id="S2.F2.24.12.m12.1b"><mrow id="S2.F2.24.12.m12.1.1" xref="S2.F2.24.12.m12.1.1.cmml"><mi id="S2.F2.24.12.m12.1.1.2" xref="S2.F2.24.12.m12.1.1.2.cmml">b</mi><mo lspace="0.222em" rspace="0.222em" id="S2.F2.24.12.m12.1.1.1" xref="S2.F2.24.12.m12.1.1.1.cmml">Ã—</mo><mi id="S2.F2.24.12.m12.1.1.3" xref="S2.F2.24.12.m12.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.24.12.m12.1c"><apply id="S2.F2.24.12.m12.1.1.cmml" xref="S2.F2.24.12.m12.1.1"><times id="S2.F2.24.12.m12.1.1.1.cmml" xref="S2.F2.24.12.m12.1.1.1"></times><ci id="S2.F2.24.12.m12.1.1.2.cmml" xref="S2.F2.24.12.m12.1.1.2">ğ‘</ci><ci id="S2.F2.24.12.m12.1.1.3.cmml" xref="S2.F2.24.12.m12.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.24.12.m12.1d">b\times v</annotation></semantics></math>) through an <span title="" class="ltx_glossaryref">MLP</span> layer.</span></figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.14" class="ltx_p">To obtain the image features, we first create spatially interpolated rectangular regions of the image <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{I}_{n}" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><msub id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mi id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">ğˆ</mi><mi id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">ğˆ</ci><ci id="S2.SS1.p3.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\mathbf{I}_{n}</annotation></semantics></math> with our BoxExtractor. For this purpose, let <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="c\in\mathbb{N}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mrow id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">c</mi><mo id="S2.SS1.p3.2.m2.1.1.1" xref="S2.SS1.p3.2.m2.1.1.1.cmml">âˆˆ</mo><mi id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml">â„•</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><in id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1.1"></in><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">ğ‘</ci><ci id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3">â„•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">c\in\mathbb{N}</annotation></semantics></math> be the number of spectral bands, <math id="S2.SS1.p3.3.m3.2" class="ltx_Math" alttext="H,W\in\mathbb{N}" display="inline"><semantics id="S2.SS1.p3.3.m3.2a"><mrow id="S2.SS1.p3.3.m3.2.3" xref="S2.SS1.p3.3.m3.2.3.cmml"><mrow id="S2.SS1.p3.3.m3.2.3.2.2" xref="S2.SS1.p3.3.m3.2.3.2.1.cmml"><mi id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">H</mi><mo id="S2.SS1.p3.3.m3.2.3.2.2.1" xref="S2.SS1.p3.3.m3.2.3.2.1.cmml">,</mo><mi id="S2.SS1.p3.3.m3.2.2" xref="S2.SS1.p3.3.m3.2.2.cmml">W</mi></mrow><mo id="S2.SS1.p3.3.m3.2.3.1" xref="S2.SS1.p3.3.m3.2.3.1.cmml">âˆˆ</mo><mi id="S2.SS1.p3.3.m3.2.3.3" xref="S2.SS1.p3.3.m3.2.3.3.cmml">â„•</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.2b"><apply id="S2.SS1.p3.3.m3.2.3.cmml" xref="S2.SS1.p3.3.m3.2.3"><in id="S2.SS1.p3.3.m3.2.3.1.cmml" xref="S2.SS1.p3.3.m3.2.3.1"></in><list id="S2.SS1.p3.3.m3.2.3.2.1.cmml" xref="S2.SS1.p3.3.m3.2.3.2.2"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">ğ»</ci><ci id="S2.SS1.p3.3.m3.2.2.cmml" xref="S2.SS1.p3.3.m3.2.2">ğ‘Š</ci></list><ci id="S2.SS1.p3.3.m3.2.3.3.cmml" xref="S2.SS1.p3.3.m3.2.3.3">â„•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.2c">H,W\in\mathbb{N}</annotation></semantics></math> the height and width of the image, and <math id="S2.SS1.p3.4.m4.2" class="ltx_Math" alttext="H^{\prime},W^{\prime}\in\mathbb{N}" display="inline"><semantics id="S2.SS1.p3.4.m4.2a"><mrow id="S2.SS1.p3.4.m4.2.2" xref="S2.SS1.p3.4.m4.2.2.cmml"><mrow id="S2.SS1.p3.4.m4.2.2.2.2" xref="S2.SS1.p3.4.m4.2.2.2.3.cmml"><msup id="S2.SS1.p3.4.m4.1.1.1.1.1" xref="S2.SS1.p3.4.m4.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.4.m4.1.1.1.1.1.2" xref="S2.SS1.p3.4.m4.1.1.1.1.1.2.cmml">H</mi><mo id="S2.SS1.p3.4.m4.1.1.1.1.1.3" xref="S2.SS1.p3.4.m4.1.1.1.1.1.3.cmml">â€²</mo></msup><mo id="S2.SS1.p3.4.m4.2.2.2.2.3" xref="S2.SS1.p3.4.m4.2.2.2.3.cmml">,</mo><msup id="S2.SS1.p3.4.m4.2.2.2.2.2" xref="S2.SS1.p3.4.m4.2.2.2.2.2.cmml"><mi id="S2.SS1.p3.4.m4.2.2.2.2.2.2" xref="S2.SS1.p3.4.m4.2.2.2.2.2.2.cmml">W</mi><mo id="S2.SS1.p3.4.m4.2.2.2.2.2.3" xref="S2.SS1.p3.4.m4.2.2.2.2.2.3.cmml">â€²</mo></msup></mrow><mo id="S2.SS1.p3.4.m4.2.2.3" xref="S2.SS1.p3.4.m4.2.2.3.cmml">âˆˆ</mo><mi id="S2.SS1.p3.4.m4.2.2.4" xref="S2.SS1.p3.4.m4.2.2.4.cmml">â„•</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.2b"><apply id="S2.SS1.p3.4.m4.2.2.cmml" xref="S2.SS1.p3.4.m4.2.2"><in id="S2.SS1.p3.4.m4.2.2.3.cmml" xref="S2.SS1.p3.4.m4.2.2.3"></in><list id="S2.SS1.p3.4.m4.2.2.2.3.cmml" xref="S2.SS1.p3.4.m4.2.2.2.2"><apply id="S2.SS1.p3.4.m4.1.1.1.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p3.4.m4.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.4.m4.1.1.1.1.1.2">ğ»</ci><ci id="S2.SS1.p3.4.m4.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.4.m4.1.1.1.1.1.3">â€²</ci></apply><apply id="S2.SS1.p3.4.m4.2.2.2.2.2.cmml" xref="S2.SS1.p3.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.2.2.2.2.2.1.cmml" xref="S2.SS1.p3.4.m4.2.2.2.2.2">superscript</csymbol><ci id="S2.SS1.p3.4.m4.2.2.2.2.2.2.cmml" xref="S2.SS1.p3.4.m4.2.2.2.2.2.2">ğ‘Š</ci><ci id="S2.SS1.p3.4.m4.2.2.2.2.2.3.cmml" xref="S2.SS1.p3.4.m4.2.2.2.2.2.3">â€²</ci></apply></list><ci id="S2.SS1.p3.4.m4.2.2.4.cmml" xref="S2.SS1.p3.4.m4.2.2.4">â„•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.2c">H^{\prime},W^{\prime}\in\mathbb{N}</annotation></semantics></math> the height and width of the interpolated boxes.
Let <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="b\in\mathbb{N}" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><mrow id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.2" xref="S2.SS1.p3.5.m5.1.1.2.cmml">b</mi><mo id="S2.SS1.p3.5.m5.1.1.1" xref="S2.SS1.p3.5.m5.1.1.1.cmml">âˆˆ</mo><mi id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml">â„•</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><in id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1.1"></in><ci id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2">ğ‘</ci><ci id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3">â„•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">b\in\mathbb{N}</annotation></semantics></math> be the number of boxes that BoxExtractor creates.
Then the function <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="B:\mathbb{R}^{c\times H\times W}\to\mathbb{R}^{b\times c\times H^{\prime}\times W^{\prime}}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><mrow id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.2" xref="S2.SS1.p3.6.m6.1.1.2.cmml">B</mi><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p3.6.m6.1.1.1" xref="S2.SS1.p3.6.m6.1.1.1.cmml">:</mo><mrow id="S2.SS1.p3.6.m6.1.1.3" xref="S2.SS1.p3.6.m6.1.1.3.cmml"><msup id="S2.SS1.p3.6.m6.1.1.3.2" xref="S2.SS1.p3.6.m6.1.1.3.2.cmml"><mi id="S2.SS1.p3.6.m6.1.1.3.2.2" xref="S2.SS1.p3.6.m6.1.1.3.2.2.cmml">â„</mi><mrow id="S2.SS1.p3.6.m6.1.1.3.2.3" xref="S2.SS1.p3.6.m6.1.1.3.2.3.cmml"><mi id="S2.SS1.p3.6.m6.1.1.3.2.3.2" xref="S2.SS1.p3.6.m6.1.1.3.2.3.2.cmml">c</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.6.m6.1.1.3.2.3.1" xref="S2.SS1.p3.6.m6.1.1.3.2.3.1.cmml">Ã—</mo><mi id="S2.SS1.p3.6.m6.1.1.3.2.3.3" xref="S2.SS1.p3.6.m6.1.1.3.2.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.6.m6.1.1.3.2.3.1a" xref="S2.SS1.p3.6.m6.1.1.3.2.3.1.cmml">Ã—</mo><mi id="S2.SS1.p3.6.m6.1.1.3.2.3.4" xref="S2.SS1.p3.6.m6.1.1.3.2.3.4.cmml">W</mi></mrow></msup><mo stretchy="false" id="S2.SS1.p3.6.m6.1.1.3.1" xref="S2.SS1.p3.6.m6.1.1.3.1.cmml">â†’</mo><msup id="S2.SS1.p3.6.m6.1.1.3.3" xref="S2.SS1.p3.6.m6.1.1.3.3.cmml"><mi id="S2.SS1.p3.6.m6.1.1.3.3.2" xref="S2.SS1.p3.6.m6.1.1.3.3.2.cmml">â„</mi><mrow id="S2.SS1.p3.6.m6.1.1.3.3.3" xref="S2.SS1.p3.6.m6.1.1.3.3.3.cmml"><mi id="S2.SS1.p3.6.m6.1.1.3.3.3.2" xref="S2.SS1.p3.6.m6.1.1.3.3.3.2.cmml">b</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.6.m6.1.1.3.3.3.1" xref="S2.SS1.p3.6.m6.1.1.3.3.3.1.cmml">Ã—</mo><mi id="S2.SS1.p3.6.m6.1.1.3.3.3.3" xref="S2.SS1.p3.6.m6.1.1.3.3.3.3.cmml">c</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.6.m6.1.1.3.3.3.1a" xref="S2.SS1.p3.6.m6.1.1.3.3.3.1.cmml">Ã—</mo><msup id="S2.SS1.p3.6.m6.1.1.3.3.3.4" xref="S2.SS1.p3.6.m6.1.1.3.3.3.4.cmml"><mi id="S2.SS1.p3.6.m6.1.1.3.3.3.4.2" xref="S2.SS1.p3.6.m6.1.1.3.3.3.4.2.cmml">H</mi><mo id="S2.SS1.p3.6.m6.1.1.3.3.3.4.3" xref="S2.SS1.p3.6.m6.1.1.3.3.3.4.3.cmml">â€²</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.6.m6.1.1.3.3.3.1b" xref="S2.SS1.p3.6.m6.1.1.3.3.3.1.cmml">Ã—</mo><msup id="S2.SS1.p3.6.m6.1.1.3.3.3.5" xref="S2.SS1.p3.6.m6.1.1.3.3.3.5.cmml"><mi id="S2.SS1.p3.6.m6.1.1.3.3.3.5.2" xref="S2.SS1.p3.6.m6.1.1.3.3.3.5.2.cmml">W</mi><mo id="S2.SS1.p3.6.m6.1.1.3.3.3.5.3" xref="S2.SS1.p3.6.m6.1.1.3.3.3.5.3.cmml">â€²</mo></msup></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1"><ci id="S2.SS1.p3.6.m6.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1.1">:</ci><ci id="S2.SS1.p3.6.m6.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.2">ğµ</ci><apply id="S2.SS1.p3.6.m6.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3"><ci id="S2.SS1.p3.6.m6.1.1.3.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.1">â†’</ci><apply id="S2.SS1.p3.6.m6.1.1.3.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.3.2.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2">superscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.3.2.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2.2">â„</ci><apply id="S2.SS1.p3.6.m6.1.1.3.2.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2.3"><times id="S2.SS1.p3.6.m6.1.1.3.2.3.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2.3.1"></times><ci id="S2.SS1.p3.6.m6.1.1.3.2.3.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2.3.2">ğ‘</ci><ci id="S2.SS1.p3.6.m6.1.1.3.2.3.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2.3.3">ğ»</ci><ci id="S2.SS1.p3.6.m6.1.1.3.2.3.4.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2.3.4">ğ‘Š</ci></apply></apply><apply id="S2.SS1.p3.6.m6.1.1.3.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.3.3.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3">superscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.3.3.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.2">â„</ci><apply id="S2.SS1.p3.6.m6.1.1.3.3.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3"><times id="S2.SS1.p3.6.m6.1.1.3.3.3.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.1"></times><ci id="S2.SS1.p3.6.m6.1.1.3.3.3.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.2">ğ‘</ci><ci id="S2.SS1.p3.6.m6.1.1.3.3.3.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.3">ğ‘</ci><apply id="S2.SS1.p3.6.m6.1.1.3.3.3.4.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.4"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.3.3.3.4.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.4">superscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.3.3.3.4.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.4.2">ğ»</ci><ci id="S2.SS1.p3.6.m6.1.1.3.3.3.4.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.4.3">â€²</ci></apply><apply id="S2.SS1.p3.6.m6.1.1.3.3.3.5.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.5"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.3.3.3.5.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.5">superscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.3.3.3.5.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.5.2">ğ‘Š</ci><ci id="S2.SS1.p3.6.m6.1.1.3.3.3.5.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3.5.3">â€²</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">B:\mathbb{R}^{c\times H\times W}\to\mathbb{R}^{b\times c\times H^{\prime}\times W^{\prime}}</annotation></semantics></math> defines the BoxExtractor that maps the image <math id="S2.SS1.p3.7.m7.1" class="ltx_Math" alttext="\mathbf{I}_{n}" display="inline"><semantics id="S2.SS1.p3.7.m7.1a"><msub id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml"><mi id="S2.SS1.p3.7.m7.1.1.2" xref="S2.SS1.p3.7.m7.1.1.2.cmml">ğˆ</mi><mi id="S2.SS1.p3.7.m7.1.1.3" xref="S2.SS1.p3.7.m7.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.1b"><apply id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.2.cmml" xref="S2.SS1.p3.7.m7.1.1.2">ğˆ</ci><ci id="S2.SS1.p3.7.m7.1.1.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.1c">\mathbf{I}_{n}</annotation></semantics></math> to its boxes <math id="S2.SS1.p3.8.m8.1" class="ltx_Math" alttext="B(\mathbf{I}_{n})=\mathbf{I}_{n}^{\prime}" display="inline"><semantics id="S2.SS1.p3.8.m8.1a"><mrow id="S2.SS1.p3.8.m8.1.1" xref="S2.SS1.p3.8.m8.1.1.cmml"><mrow id="S2.SS1.p3.8.m8.1.1.1" xref="S2.SS1.p3.8.m8.1.1.1.cmml"><mi id="S2.SS1.p3.8.m8.1.1.1.3" xref="S2.SS1.p3.8.m8.1.1.1.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.8.m8.1.1.1.2" xref="S2.SS1.p3.8.m8.1.1.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p3.8.m8.1.1.1.1.1" xref="S2.SS1.p3.8.m8.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.8.m8.1.1.1.1.1.2" xref="S2.SS1.p3.8.m8.1.1.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p3.8.m8.1.1.1.1.1.1" xref="S2.SS1.p3.8.m8.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.8.m8.1.1.1.1.1.1.2" xref="S2.SS1.p3.8.m8.1.1.1.1.1.1.2.cmml">ğˆ</mi><mi id="S2.SS1.p3.8.m8.1.1.1.1.1.1.3" xref="S2.SS1.p3.8.m8.1.1.1.1.1.1.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS1.p3.8.m8.1.1.1.1.1.3" xref="S2.SS1.p3.8.m8.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p3.8.m8.1.1.2" xref="S2.SS1.p3.8.m8.1.1.2.cmml">=</mo><msubsup id="S2.SS1.p3.8.m8.1.1.3" xref="S2.SS1.p3.8.m8.1.1.3.cmml"><mi id="S2.SS1.p3.8.m8.1.1.3.2.2" xref="S2.SS1.p3.8.m8.1.1.3.2.2.cmml">ğˆ</mi><mi id="S2.SS1.p3.8.m8.1.1.3.2.3" xref="S2.SS1.p3.8.m8.1.1.3.2.3.cmml">n</mi><mo id="S2.SS1.p3.8.m8.1.1.3.3" xref="S2.SS1.p3.8.m8.1.1.3.3.cmml">â€²</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.1b"><apply id="S2.SS1.p3.8.m8.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1"><eq id="S2.SS1.p3.8.m8.1.1.2.cmml" xref="S2.SS1.p3.8.m8.1.1.2"></eq><apply id="S2.SS1.p3.8.m8.1.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1.1"><times id="S2.SS1.p3.8.m8.1.1.1.2.cmml" xref="S2.SS1.p3.8.m8.1.1.1.2"></times><ci id="S2.SS1.p3.8.m8.1.1.1.3.cmml" xref="S2.SS1.p3.8.m8.1.1.1.3">ğµ</ci><apply id="S2.SS1.p3.8.m8.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.8.m8.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.8.m8.1.1.1.1.1.1.2">ğˆ</ci><ci id="S2.SS1.p3.8.m8.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.8.m8.1.1.1.1.1.1.3">ğ‘›</ci></apply></apply><apply id="S2.SS1.p3.8.m8.1.1.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.1.3.1.cmml" xref="S2.SS1.p3.8.m8.1.1.3">superscript</csymbol><apply id="S2.SS1.p3.8.m8.1.1.3.2.cmml" xref="S2.SS1.p3.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.1.3.2.1.cmml" xref="S2.SS1.p3.8.m8.1.1.3">subscript</csymbol><ci id="S2.SS1.p3.8.m8.1.1.3.2.2.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.2">ğˆ</ci><ci id="S2.SS1.p3.8.m8.1.1.3.2.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3.2.3">ğ‘›</ci></apply><ci id="S2.SS1.p3.8.m8.1.1.3.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.1c">B(\mathbf{I}_{n})=\mathbf{I}_{n}^{\prime}</annotation></semantics></math> in a non-deterministic way.
The non-determinism is observed because we first choose random start- and end-points for the height and the width of the boxes, <math id="S2.SS1.p3.9.m9.6" class="ltx_Math" alttext="s_{H}^{i},e_{H}^{i}\in\{0,1,...,H\}" display="inline"><semantics id="S2.SS1.p3.9.m9.6a"><mrow id="S2.SS1.p3.9.m9.6.6" xref="S2.SS1.p3.9.m9.6.6.cmml"><mrow id="S2.SS1.p3.9.m9.6.6.2.2" xref="S2.SS1.p3.9.m9.6.6.2.3.cmml"><msubsup id="S2.SS1.p3.9.m9.5.5.1.1.1" xref="S2.SS1.p3.9.m9.5.5.1.1.1.cmml"><mi id="S2.SS1.p3.9.m9.5.5.1.1.1.2.2" xref="S2.SS1.p3.9.m9.5.5.1.1.1.2.2.cmml">s</mi><mi id="S2.SS1.p3.9.m9.5.5.1.1.1.2.3" xref="S2.SS1.p3.9.m9.5.5.1.1.1.2.3.cmml">H</mi><mi id="S2.SS1.p3.9.m9.5.5.1.1.1.3" xref="S2.SS1.p3.9.m9.5.5.1.1.1.3.cmml">i</mi></msubsup><mo id="S2.SS1.p3.9.m9.6.6.2.2.3" xref="S2.SS1.p3.9.m9.6.6.2.3.cmml">,</mo><msubsup id="S2.SS1.p3.9.m9.6.6.2.2.2" xref="S2.SS1.p3.9.m9.6.6.2.2.2.cmml"><mi id="S2.SS1.p3.9.m9.6.6.2.2.2.2.2" xref="S2.SS1.p3.9.m9.6.6.2.2.2.2.2.cmml">e</mi><mi id="S2.SS1.p3.9.m9.6.6.2.2.2.2.3" xref="S2.SS1.p3.9.m9.6.6.2.2.2.2.3.cmml">H</mi><mi id="S2.SS1.p3.9.m9.6.6.2.2.2.3" xref="S2.SS1.p3.9.m9.6.6.2.2.2.3.cmml">i</mi></msubsup></mrow><mo id="S2.SS1.p3.9.m9.6.6.3" xref="S2.SS1.p3.9.m9.6.6.3.cmml">âˆˆ</mo><mrow id="S2.SS1.p3.9.m9.6.6.4.2" xref="S2.SS1.p3.9.m9.6.6.4.1.cmml"><mo stretchy="false" id="S2.SS1.p3.9.m9.6.6.4.2.1" xref="S2.SS1.p3.9.m9.6.6.4.1.cmml">{</mo><mn id="S2.SS1.p3.9.m9.1.1" xref="S2.SS1.p3.9.m9.1.1.cmml">0</mn><mo id="S2.SS1.p3.9.m9.6.6.4.2.2" xref="S2.SS1.p3.9.m9.6.6.4.1.cmml">,</mo><mn id="S2.SS1.p3.9.m9.2.2" xref="S2.SS1.p3.9.m9.2.2.cmml">1</mn><mo id="S2.SS1.p3.9.m9.6.6.4.2.3" xref="S2.SS1.p3.9.m9.6.6.4.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p3.9.m9.3.3" xref="S2.SS1.p3.9.m9.3.3.cmml">â€¦</mi><mo id="S2.SS1.p3.9.m9.6.6.4.2.4" xref="S2.SS1.p3.9.m9.6.6.4.1.cmml">,</mo><mi id="S2.SS1.p3.9.m9.4.4" xref="S2.SS1.p3.9.m9.4.4.cmml">H</mi><mo stretchy="false" id="S2.SS1.p3.9.m9.6.6.4.2.5" xref="S2.SS1.p3.9.m9.6.6.4.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.9.m9.6b"><apply id="S2.SS1.p3.9.m9.6.6.cmml" xref="S2.SS1.p3.9.m9.6.6"><in id="S2.SS1.p3.9.m9.6.6.3.cmml" xref="S2.SS1.p3.9.m9.6.6.3"></in><list id="S2.SS1.p3.9.m9.6.6.2.3.cmml" xref="S2.SS1.p3.9.m9.6.6.2.2"><apply id="S2.SS1.p3.9.m9.5.5.1.1.1.cmml" xref="S2.SS1.p3.9.m9.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.5.5.1.1.1.1.cmml" xref="S2.SS1.p3.9.m9.5.5.1.1.1">superscript</csymbol><apply id="S2.SS1.p3.9.m9.5.5.1.1.1.2.cmml" xref="S2.SS1.p3.9.m9.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.5.5.1.1.1.2.1.cmml" xref="S2.SS1.p3.9.m9.5.5.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.9.m9.5.5.1.1.1.2.2.cmml" xref="S2.SS1.p3.9.m9.5.5.1.1.1.2.2">ğ‘ </ci><ci id="S2.SS1.p3.9.m9.5.5.1.1.1.2.3.cmml" xref="S2.SS1.p3.9.m9.5.5.1.1.1.2.3">ğ»</ci></apply><ci id="S2.SS1.p3.9.m9.5.5.1.1.1.3.cmml" xref="S2.SS1.p3.9.m9.5.5.1.1.1.3">ğ‘–</ci></apply><apply id="S2.SS1.p3.9.m9.6.6.2.2.2.cmml" xref="S2.SS1.p3.9.m9.6.6.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.6.6.2.2.2.1.cmml" xref="S2.SS1.p3.9.m9.6.6.2.2.2">superscript</csymbol><apply id="S2.SS1.p3.9.m9.6.6.2.2.2.2.cmml" xref="S2.SS1.p3.9.m9.6.6.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.6.6.2.2.2.2.1.cmml" xref="S2.SS1.p3.9.m9.6.6.2.2.2">subscript</csymbol><ci id="S2.SS1.p3.9.m9.6.6.2.2.2.2.2.cmml" xref="S2.SS1.p3.9.m9.6.6.2.2.2.2.2">ğ‘’</ci><ci id="S2.SS1.p3.9.m9.6.6.2.2.2.2.3.cmml" xref="S2.SS1.p3.9.m9.6.6.2.2.2.2.3">ğ»</ci></apply><ci id="S2.SS1.p3.9.m9.6.6.2.2.2.3.cmml" xref="S2.SS1.p3.9.m9.6.6.2.2.2.3">ğ‘–</ci></apply></list><set id="S2.SS1.p3.9.m9.6.6.4.1.cmml" xref="S2.SS1.p3.9.m9.6.6.4.2"><cn type="integer" id="S2.SS1.p3.9.m9.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1">0</cn><cn type="integer" id="S2.SS1.p3.9.m9.2.2.cmml" xref="S2.SS1.p3.9.m9.2.2">1</cn><ci id="S2.SS1.p3.9.m9.3.3.cmml" xref="S2.SS1.p3.9.m9.3.3">â€¦</ci><ci id="S2.SS1.p3.9.m9.4.4.cmml" xref="S2.SS1.p3.9.m9.4.4">ğ»</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.9.m9.6c">s_{H}^{i},e_{H}^{i}\in\{0,1,...,H\}</annotation></semantics></math>, <math id="S2.SS1.p3.10.m10.1" class="ltx_Math" alttext="s_{H}^{i}&lt;e_{H}^{i}" display="inline"><semantics id="S2.SS1.p3.10.m10.1a"><mrow id="S2.SS1.p3.10.m10.1.1" xref="S2.SS1.p3.10.m10.1.1.cmml"><msubsup id="S2.SS1.p3.10.m10.1.1.2" xref="S2.SS1.p3.10.m10.1.1.2.cmml"><mi id="S2.SS1.p3.10.m10.1.1.2.2.2" xref="S2.SS1.p3.10.m10.1.1.2.2.2.cmml">s</mi><mi id="S2.SS1.p3.10.m10.1.1.2.2.3" xref="S2.SS1.p3.10.m10.1.1.2.2.3.cmml">H</mi><mi id="S2.SS1.p3.10.m10.1.1.2.3" xref="S2.SS1.p3.10.m10.1.1.2.3.cmml">i</mi></msubsup><mo id="S2.SS1.p3.10.m10.1.1.1" xref="S2.SS1.p3.10.m10.1.1.1.cmml">&lt;</mo><msubsup id="S2.SS1.p3.10.m10.1.1.3" xref="S2.SS1.p3.10.m10.1.1.3.cmml"><mi id="S2.SS1.p3.10.m10.1.1.3.2.2" xref="S2.SS1.p3.10.m10.1.1.3.2.2.cmml">e</mi><mi id="S2.SS1.p3.10.m10.1.1.3.2.3" xref="S2.SS1.p3.10.m10.1.1.3.2.3.cmml">H</mi><mi id="S2.SS1.p3.10.m10.1.1.3.3" xref="S2.SS1.p3.10.m10.1.1.3.3.cmml">i</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.10.m10.1b"><apply id="S2.SS1.p3.10.m10.1.1.cmml" xref="S2.SS1.p3.10.m10.1.1"><lt id="S2.SS1.p3.10.m10.1.1.1.cmml" xref="S2.SS1.p3.10.m10.1.1.1"></lt><apply id="S2.SS1.p3.10.m10.1.1.2.cmml" xref="S2.SS1.p3.10.m10.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.10.m10.1.1.2.1.cmml" xref="S2.SS1.p3.10.m10.1.1.2">superscript</csymbol><apply id="S2.SS1.p3.10.m10.1.1.2.2.cmml" xref="S2.SS1.p3.10.m10.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.10.m10.1.1.2.2.1.cmml" xref="S2.SS1.p3.10.m10.1.1.2">subscript</csymbol><ci id="S2.SS1.p3.10.m10.1.1.2.2.2.cmml" xref="S2.SS1.p3.10.m10.1.1.2.2.2">ğ‘ </ci><ci id="S2.SS1.p3.10.m10.1.1.2.2.3.cmml" xref="S2.SS1.p3.10.m10.1.1.2.2.3">ğ»</ci></apply><ci id="S2.SS1.p3.10.m10.1.1.2.3.cmml" xref="S2.SS1.p3.10.m10.1.1.2.3">ğ‘–</ci></apply><apply id="S2.SS1.p3.10.m10.1.1.3.cmml" xref="S2.SS1.p3.10.m10.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.10.m10.1.1.3.1.cmml" xref="S2.SS1.p3.10.m10.1.1.3">superscript</csymbol><apply id="S2.SS1.p3.10.m10.1.1.3.2.cmml" xref="S2.SS1.p3.10.m10.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.10.m10.1.1.3.2.1.cmml" xref="S2.SS1.p3.10.m10.1.1.3">subscript</csymbol><ci id="S2.SS1.p3.10.m10.1.1.3.2.2.cmml" xref="S2.SS1.p3.10.m10.1.1.3.2.2">ğ‘’</ci><ci id="S2.SS1.p3.10.m10.1.1.3.2.3.cmml" xref="S2.SS1.p3.10.m10.1.1.3.2.3">ğ»</ci></apply><ci id="S2.SS1.p3.10.m10.1.1.3.3.cmml" xref="S2.SS1.p3.10.m10.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.10.m10.1c">s_{H}^{i}&lt;e_{H}^{i}</annotation></semantics></math> and <math id="S2.SS1.p3.11.m11.6" class="ltx_Math" alttext="s_{W}^{i},e_{W}^{i}\in\{0,1,...,W\}" display="inline"><semantics id="S2.SS1.p3.11.m11.6a"><mrow id="S2.SS1.p3.11.m11.6.6" xref="S2.SS1.p3.11.m11.6.6.cmml"><mrow id="S2.SS1.p3.11.m11.6.6.2.2" xref="S2.SS1.p3.11.m11.6.6.2.3.cmml"><msubsup id="S2.SS1.p3.11.m11.5.5.1.1.1" xref="S2.SS1.p3.11.m11.5.5.1.1.1.cmml"><mi id="S2.SS1.p3.11.m11.5.5.1.1.1.2.2" xref="S2.SS1.p3.11.m11.5.5.1.1.1.2.2.cmml">s</mi><mi id="S2.SS1.p3.11.m11.5.5.1.1.1.2.3" xref="S2.SS1.p3.11.m11.5.5.1.1.1.2.3.cmml">W</mi><mi id="S2.SS1.p3.11.m11.5.5.1.1.1.3" xref="S2.SS1.p3.11.m11.5.5.1.1.1.3.cmml">i</mi></msubsup><mo id="S2.SS1.p3.11.m11.6.6.2.2.3" xref="S2.SS1.p3.11.m11.6.6.2.3.cmml">,</mo><msubsup id="S2.SS1.p3.11.m11.6.6.2.2.2" xref="S2.SS1.p3.11.m11.6.6.2.2.2.cmml"><mi id="S2.SS1.p3.11.m11.6.6.2.2.2.2.2" xref="S2.SS1.p3.11.m11.6.6.2.2.2.2.2.cmml">e</mi><mi id="S2.SS1.p3.11.m11.6.6.2.2.2.2.3" xref="S2.SS1.p3.11.m11.6.6.2.2.2.2.3.cmml">W</mi><mi id="S2.SS1.p3.11.m11.6.6.2.2.2.3" xref="S2.SS1.p3.11.m11.6.6.2.2.2.3.cmml">i</mi></msubsup></mrow><mo id="S2.SS1.p3.11.m11.6.6.3" xref="S2.SS1.p3.11.m11.6.6.3.cmml">âˆˆ</mo><mrow id="S2.SS1.p3.11.m11.6.6.4.2" xref="S2.SS1.p3.11.m11.6.6.4.1.cmml"><mo stretchy="false" id="S2.SS1.p3.11.m11.6.6.4.2.1" xref="S2.SS1.p3.11.m11.6.6.4.1.cmml">{</mo><mn id="S2.SS1.p3.11.m11.1.1" xref="S2.SS1.p3.11.m11.1.1.cmml">0</mn><mo id="S2.SS1.p3.11.m11.6.6.4.2.2" xref="S2.SS1.p3.11.m11.6.6.4.1.cmml">,</mo><mn id="S2.SS1.p3.11.m11.2.2" xref="S2.SS1.p3.11.m11.2.2.cmml">1</mn><mo id="S2.SS1.p3.11.m11.6.6.4.2.3" xref="S2.SS1.p3.11.m11.6.6.4.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p3.11.m11.3.3" xref="S2.SS1.p3.11.m11.3.3.cmml">â€¦</mi><mo id="S2.SS1.p3.11.m11.6.6.4.2.4" xref="S2.SS1.p3.11.m11.6.6.4.1.cmml">,</mo><mi id="S2.SS1.p3.11.m11.4.4" xref="S2.SS1.p3.11.m11.4.4.cmml">W</mi><mo stretchy="false" id="S2.SS1.p3.11.m11.6.6.4.2.5" xref="S2.SS1.p3.11.m11.6.6.4.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.11.m11.6b"><apply id="S2.SS1.p3.11.m11.6.6.cmml" xref="S2.SS1.p3.11.m11.6.6"><in id="S2.SS1.p3.11.m11.6.6.3.cmml" xref="S2.SS1.p3.11.m11.6.6.3"></in><list id="S2.SS1.p3.11.m11.6.6.2.3.cmml" xref="S2.SS1.p3.11.m11.6.6.2.2"><apply id="S2.SS1.p3.11.m11.5.5.1.1.1.cmml" xref="S2.SS1.p3.11.m11.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.11.m11.5.5.1.1.1.1.cmml" xref="S2.SS1.p3.11.m11.5.5.1.1.1">superscript</csymbol><apply id="S2.SS1.p3.11.m11.5.5.1.1.1.2.cmml" xref="S2.SS1.p3.11.m11.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.11.m11.5.5.1.1.1.2.1.cmml" xref="S2.SS1.p3.11.m11.5.5.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.11.m11.5.5.1.1.1.2.2.cmml" xref="S2.SS1.p3.11.m11.5.5.1.1.1.2.2">ğ‘ </ci><ci id="S2.SS1.p3.11.m11.5.5.1.1.1.2.3.cmml" xref="S2.SS1.p3.11.m11.5.5.1.1.1.2.3">ğ‘Š</ci></apply><ci id="S2.SS1.p3.11.m11.5.5.1.1.1.3.cmml" xref="S2.SS1.p3.11.m11.5.5.1.1.1.3">ğ‘–</ci></apply><apply id="S2.SS1.p3.11.m11.6.6.2.2.2.cmml" xref="S2.SS1.p3.11.m11.6.6.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.11.m11.6.6.2.2.2.1.cmml" xref="S2.SS1.p3.11.m11.6.6.2.2.2">superscript</csymbol><apply id="S2.SS1.p3.11.m11.6.6.2.2.2.2.cmml" xref="S2.SS1.p3.11.m11.6.6.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.11.m11.6.6.2.2.2.2.1.cmml" xref="S2.SS1.p3.11.m11.6.6.2.2.2">subscript</csymbol><ci id="S2.SS1.p3.11.m11.6.6.2.2.2.2.2.cmml" xref="S2.SS1.p3.11.m11.6.6.2.2.2.2.2">ğ‘’</ci><ci id="S2.SS1.p3.11.m11.6.6.2.2.2.2.3.cmml" xref="S2.SS1.p3.11.m11.6.6.2.2.2.2.3">ğ‘Š</ci></apply><ci id="S2.SS1.p3.11.m11.6.6.2.2.2.3.cmml" xref="S2.SS1.p3.11.m11.6.6.2.2.2.3">ğ‘–</ci></apply></list><set id="S2.SS1.p3.11.m11.6.6.4.1.cmml" xref="S2.SS1.p3.11.m11.6.6.4.2"><cn type="integer" id="S2.SS1.p3.11.m11.1.1.cmml" xref="S2.SS1.p3.11.m11.1.1">0</cn><cn type="integer" id="S2.SS1.p3.11.m11.2.2.cmml" xref="S2.SS1.p3.11.m11.2.2">1</cn><ci id="S2.SS1.p3.11.m11.3.3.cmml" xref="S2.SS1.p3.11.m11.3.3">â€¦</ci><ci id="S2.SS1.p3.11.m11.4.4.cmml" xref="S2.SS1.p3.11.m11.4.4">ğ‘Š</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.11.m11.6c">s_{W}^{i},e_{W}^{i}\in\{0,1,...,W\}</annotation></semantics></math>, <math id="S2.SS1.p3.12.m12.1" class="ltx_Math" alttext="s_{W}^{i}&lt;e_{W}^{i}" display="inline"><semantics id="S2.SS1.p3.12.m12.1a"><mrow id="S2.SS1.p3.12.m12.1.1" xref="S2.SS1.p3.12.m12.1.1.cmml"><msubsup id="S2.SS1.p3.12.m12.1.1.2" xref="S2.SS1.p3.12.m12.1.1.2.cmml"><mi id="S2.SS1.p3.12.m12.1.1.2.2.2" xref="S2.SS1.p3.12.m12.1.1.2.2.2.cmml">s</mi><mi id="S2.SS1.p3.12.m12.1.1.2.2.3" xref="S2.SS1.p3.12.m12.1.1.2.2.3.cmml">W</mi><mi id="S2.SS1.p3.12.m12.1.1.2.3" xref="S2.SS1.p3.12.m12.1.1.2.3.cmml">i</mi></msubsup><mo id="S2.SS1.p3.12.m12.1.1.1" xref="S2.SS1.p3.12.m12.1.1.1.cmml">&lt;</mo><msubsup id="S2.SS1.p3.12.m12.1.1.3" xref="S2.SS1.p3.12.m12.1.1.3.cmml"><mi id="S2.SS1.p3.12.m12.1.1.3.2.2" xref="S2.SS1.p3.12.m12.1.1.3.2.2.cmml">e</mi><mi id="S2.SS1.p3.12.m12.1.1.3.2.3" xref="S2.SS1.p3.12.m12.1.1.3.2.3.cmml">W</mi><mi id="S2.SS1.p3.12.m12.1.1.3.3" xref="S2.SS1.p3.12.m12.1.1.3.3.cmml">i</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.12.m12.1b"><apply id="S2.SS1.p3.12.m12.1.1.cmml" xref="S2.SS1.p3.12.m12.1.1"><lt id="S2.SS1.p3.12.m12.1.1.1.cmml" xref="S2.SS1.p3.12.m12.1.1.1"></lt><apply id="S2.SS1.p3.12.m12.1.1.2.cmml" xref="S2.SS1.p3.12.m12.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.12.m12.1.1.2.1.cmml" xref="S2.SS1.p3.12.m12.1.1.2">superscript</csymbol><apply id="S2.SS1.p3.12.m12.1.1.2.2.cmml" xref="S2.SS1.p3.12.m12.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.12.m12.1.1.2.2.1.cmml" xref="S2.SS1.p3.12.m12.1.1.2">subscript</csymbol><ci id="S2.SS1.p3.12.m12.1.1.2.2.2.cmml" xref="S2.SS1.p3.12.m12.1.1.2.2.2">ğ‘ </ci><ci id="S2.SS1.p3.12.m12.1.1.2.2.3.cmml" xref="S2.SS1.p3.12.m12.1.1.2.2.3">ğ‘Š</ci></apply><ci id="S2.SS1.p3.12.m12.1.1.2.3.cmml" xref="S2.SS1.p3.12.m12.1.1.2.3">ğ‘–</ci></apply><apply id="S2.SS1.p3.12.m12.1.1.3.cmml" xref="S2.SS1.p3.12.m12.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.12.m12.1.1.3.1.cmml" xref="S2.SS1.p3.12.m12.1.1.3">superscript</csymbol><apply id="S2.SS1.p3.12.m12.1.1.3.2.cmml" xref="S2.SS1.p3.12.m12.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.12.m12.1.1.3.2.1.cmml" xref="S2.SS1.p3.12.m12.1.1.3">subscript</csymbol><ci id="S2.SS1.p3.12.m12.1.1.3.2.2.cmml" xref="S2.SS1.p3.12.m12.1.1.3.2.2">ğ‘’</ci><ci id="S2.SS1.p3.12.m12.1.1.3.2.3.cmml" xref="S2.SS1.p3.12.m12.1.1.3.2.3">ğ‘Š</ci></apply><ci id="S2.SS1.p3.12.m12.1.1.3.3.cmml" xref="S2.SS1.p3.12.m12.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.12.m12.1c">s_{W}^{i}&lt;e_{W}^{i}</annotation></semantics></math>, for <math id="S2.SS1.p3.13.m13.3" class="ltx_Math" alttext="i\in\{1,...,b\}" display="inline"><semantics id="S2.SS1.p3.13.m13.3a"><mrow id="S2.SS1.p3.13.m13.3.4" xref="S2.SS1.p3.13.m13.3.4.cmml"><mi id="S2.SS1.p3.13.m13.3.4.2" xref="S2.SS1.p3.13.m13.3.4.2.cmml">i</mi><mo id="S2.SS1.p3.13.m13.3.4.1" xref="S2.SS1.p3.13.m13.3.4.1.cmml">âˆˆ</mo><mrow id="S2.SS1.p3.13.m13.3.4.3.2" xref="S2.SS1.p3.13.m13.3.4.3.1.cmml"><mo stretchy="false" id="S2.SS1.p3.13.m13.3.4.3.2.1" xref="S2.SS1.p3.13.m13.3.4.3.1.cmml">{</mo><mn id="S2.SS1.p3.13.m13.1.1" xref="S2.SS1.p3.13.m13.1.1.cmml">1</mn><mo id="S2.SS1.p3.13.m13.3.4.3.2.2" xref="S2.SS1.p3.13.m13.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p3.13.m13.2.2" xref="S2.SS1.p3.13.m13.2.2.cmml">â€¦</mi><mo id="S2.SS1.p3.13.m13.3.4.3.2.3" xref="S2.SS1.p3.13.m13.3.4.3.1.cmml">,</mo><mi id="S2.SS1.p3.13.m13.3.3" xref="S2.SS1.p3.13.m13.3.3.cmml">b</mi><mo stretchy="false" id="S2.SS1.p3.13.m13.3.4.3.2.4" xref="S2.SS1.p3.13.m13.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.13.m13.3b"><apply id="S2.SS1.p3.13.m13.3.4.cmml" xref="S2.SS1.p3.13.m13.3.4"><in id="S2.SS1.p3.13.m13.3.4.1.cmml" xref="S2.SS1.p3.13.m13.3.4.1"></in><ci id="S2.SS1.p3.13.m13.3.4.2.cmml" xref="S2.SS1.p3.13.m13.3.4.2">ğ‘–</ci><set id="S2.SS1.p3.13.m13.3.4.3.1.cmml" xref="S2.SS1.p3.13.m13.3.4.3.2"><cn type="integer" id="S2.SS1.p3.13.m13.1.1.cmml" xref="S2.SS1.p3.13.m13.1.1">1</cn><ci id="S2.SS1.p3.13.m13.2.2.cmml" xref="S2.SS1.p3.13.m13.2.2">â€¦</ci><ci id="S2.SS1.p3.13.m13.3.3.cmml" xref="S2.SS1.p3.13.m13.3.3">ğ‘</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.13.m13.3c">i\in\{1,...,b\}</annotation></semantics></math>. In the second step, a vector of boxes to each pair of start- and end-points is created.
We control the distance between the start- and endpoints (e.g., <math id="S2.SS1.p3.14.m14.1" class="ltx_Math" alttext="e_{H}^{i}-s_{H}^{i}" display="inline"><semantics id="S2.SS1.p3.14.m14.1a"><mrow id="S2.SS1.p3.14.m14.1.1" xref="S2.SS1.p3.14.m14.1.1.cmml"><msubsup id="S2.SS1.p3.14.m14.1.1.2" xref="S2.SS1.p3.14.m14.1.1.2.cmml"><mi id="S2.SS1.p3.14.m14.1.1.2.2.2" xref="S2.SS1.p3.14.m14.1.1.2.2.2.cmml">e</mi><mi id="S2.SS1.p3.14.m14.1.1.2.2.3" xref="S2.SS1.p3.14.m14.1.1.2.2.3.cmml">H</mi><mi id="S2.SS1.p3.14.m14.1.1.2.3" xref="S2.SS1.p3.14.m14.1.1.2.3.cmml">i</mi></msubsup><mo id="S2.SS1.p3.14.m14.1.1.1" xref="S2.SS1.p3.14.m14.1.1.1.cmml">âˆ’</mo><msubsup id="S2.SS1.p3.14.m14.1.1.3" xref="S2.SS1.p3.14.m14.1.1.3.cmml"><mi id="S2.SS1.p3.14.m14.1.1.3.2.2" xref="S2.SS1.p3.14.m14.1.1.3.2.2.cmml">s</mi><mi id="S2.SS1.p3.14.m14.1.1.3.2.3" xref="S2.SS1.p3.14.m14.1.1.3.2.3.cmml">H</mi><mi id="S2.SS1.p3.14.m14.1.1.3.3" xref="S2.SS1.p3.14.m14.1.1.3.3.cmml">i</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.14.m14.1b"><apply id="S2.SS1.p3.14.m14.1.1.cmml" xref="S2.SS1.p3.14.m14.1.1"><minus id="S2.SS1.p3.14.m14.1.1.1.cmml" xref="S2.SS1.p3.14.m14.1.1.1"></minus><apply id="S2.SS1.p3.14.m14.1.1.2.cmml" xref="S2.SS1.p3.14.m14.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.14.m14.1.1.2.1.cmml" xref="S2.SS1.p3.14.m14.1.1.2">superscript</csymbol><apply id="S2.SS1.p3.14.m14.1.1.2.2.cmml" xref="S2.SS1.p3.14.m14.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.14.m14.1.1.2.2.1.cmml" xref="S2.SS1.p3.14.m14.1.1.2">subscript</csymbol><ci id="S2.SS1.p3.14.m14.1.1.2.2.2.cmml" xref="S2.SS1.p3.14.m14.1.1.2.2.2">ğ‘’</ci><ci id="S2.SS1.p3.14.m14.1.1.2.2.3.cmml" xref="S2.SS1.p3.14.m14.1.1.2.2.3">ğ»</ci></apply><ci id="S2.SS1.p3.14.m14.1.1.2.3.cmml" xref="S2.SS1.p3.14.m14.1.1.2.3">ğ‘–</ci></apply><apply id="S2.SS1.p3.14.m14.1.1.3.cmml" xref="S2.SS1.p3.14.m14.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.14.m14.1.1.3.1.cmml" xref="S2.SS1.p3.14.m14.1.1.3">superscript</csymbol><apply id="S2.SS1.p3.14.m14.1.1.3.2.cmml" xref="S2.SS1.p3.14.m14.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.14.m14.1.1.3.2.1.cmml" xref="S2.SS1.p3.14.m14.1.1.3">subscript</csymbol><ci id="S2.SS1.p3.14.m14.1.1.3.2.2.cmml" xref="S2.SS1.p3.14.m14.1.1.3.2.2">ğ‘ </ci><ci id="S2.SS1.p3.14.m14.1.1.3.2.3.cmml" xref="S2.SS1.p3.14.m14.1.1.3.2.3">ğ»</ci></apply><ci id="S2.SS1.p3.14.m14.1.1.3.3.cmml" xref="S2.SS1.p3.14.m14.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.14.m14.1c">e_{H}^{i}-s_{H}^{i}</annotation></semantics></math>) to guarantee a minimum amount of information in each box.
To this end, we set:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\text{min}_{H}\leq e_{H}^{i}-s_{H}^{i}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><msub id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mtext id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2a.cmml">min</mtext><mi id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml">H</mi></msub><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">â‰¤</mo><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><msubsup id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2.2" xref="S2.E1.m1.1.1.3.2.2.2.cmml">e</mi><mi id="S2.E1.m1.1.1.3.2.2.3" xref="S2.E1.m1.1.1.3.2.2.3.cmml">H</mi><mi id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml">i</mi></msubsup><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">âˆ’</mo><msubsup id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.3.3.2.2" xref="S2.E1.m1.1.1.3.3.2.2.cmml">s</mi><mi id="S2.E1.m1.1.1.3.3.2.3" xref="S2.E1.m1.1.1.3.3.2.3.cmml">H</mi><mi id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3.cmml">i</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><leq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></leq><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.2.2a.cmml" xref="S2.E1.m1.1.1.2.2"><mtext id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">min</mtext></ci><ci id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3">ğ»</ci></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><minus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></minus><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2">superscript</csymbol><apply id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.2.1.cmml" xref="S2.E1.m1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2.2">ğ‘’</ci><ci id="S2.E1.m1.1.1.3.2.2.3.cmml" xref="S2.E1.m1.1.1.3.2.2.3">ğ»</ci></apply><ci id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3">ğ‘–</ci></apply><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3">superscript</csymbol><apply id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.3.2.1.cmml" xref="S2.E1.m1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.3.3.2.2.cmml" xref="S2.E1.m1.1.1.3.3.2.2">ğ‘ </ci><ci id="S2.E1.m1.1.1.3.3.2.3.cmml" xref="S2.E1.m1.1.1.3.3.2.3">ğ»</ci></apply><ci id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\text{min}_{H}\leq e_{H}^{i}-s_{H}^{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.25" class="ltx_p">and</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\text{min}_{W}\leq e_{W}^{i}-s_{W}^{i}," display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml"><mtext id="S2.E2.m1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.2.2a.cmml">min</mtext><mi id="S2.E2.m1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.2.3.cmml">W</mi></msub><mo id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml">â‰¤</mo><mrow id="S2.E2.m1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.3.cmml"><msubsup id="S2.E2.m1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.3.2.cmml"><mi id="S2.E2.m1.1.1.1.1.3.2.2.2" xref="S2.E2.m1.1.1.1.1.3.2.2.2.cmml">e</mi><mi id="S2.E2.m1.1.1.1.1.3.2.2.3" xref="S2.E2.m1.1.1.1.1.3.2.2.3.cmml">W</mi><mi id="S2.E2.m1.1.1.1.1.3.2.3" xref="S2.E2.m1.1.1.1.1.3.2.3.cmml">i</mi></msubsup><mo id="S2.E2.m1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.1.3.1.cmml">âˆ’</mo><msubsup id="S2.E2.m1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.1.1.3.3.2.2" xref="S2.E2.m1.1.1.1.1.3.3.2.2.cmml">s</mi><mi id="S2.E2.m1.1.1.1.1.3.3.2.3" xref="S2.E2.m1.1.1.1.1.3.3.2.3.cmml">W</mi><mi id="S2.E2.m1.1.1.1.1.3.3.3" xref="S2.E2.m1.1.1.1.1.3.3.3.cmml">i</mi></msubsup></mrow></mrow><mo id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><leq id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"></leq><apply id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.2.2a.cmml" xref="S2.E2.m1.1.1.1.1.2.2"><mtext id="S2.E2.m1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2">min</mtext></ci><ci id="S2.E2.m1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.2.3">ğ‘Š</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.3"><minus id="S2.E2.m1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.1"></minus><apply id="S2.E2.m1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.2">superscript</csymbol><apply id="S2.E2.m1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2.2.2">ğ‘’</ci><ci id="S2.E2.m1.1.1.1.1.3.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.3.2.2.3">ğ‘Š</ci></apply><ci id="S2.E2.m1.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.1.1.3.2.3">ğ‘–</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.3">superscript</csymbol><apply id="S2.E2.m1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.3.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.3.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.3.2.2">ğ‘ </ci><ci id="S2.E2.m1.1.1.1.1.3.3.2.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3.2.3">ğ‘Š</ci></apply><ci id="S2.E2.m1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\text{min}_{W}\leq e_{W}^{i}-s_{W}^{i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.24" class="ltx_p">for <math id="S2.SS1.p3.15.m1.3" class="ltx_Math" alttext="i=1,...,b" display="inline"><semantics id="S2.SS1.p3.15.m1.3a"><mrow id="S2.SS1.p3.15.m1.3.4" xref="S2.SS1.p3.15.m1.3.4.cmml"><mi id="S2.SS1.p3.15.m1.3.4.2" xref="S2.SS1.p3.15.m1.3.4.2.cmml">i</mi><mo id="S2.SS1.p3.15.m1.3.4.1" xref="S2.SS1.p3.15.m1.3.4.1.cmml">=</mo><mrow id="S2.SS1.p3.15.m1.3.4.3.2" xref="S2.SS1.p3.15.m1.3.4.3.1.cmml"><mn id="S2.SS1.p3.15.m1.1.1" xref="S2.SS1.p3.15.m1.1.1.cmml">1</mn><mo id="S2.SS1.p3.15.m1.3.4.3.2.1" xref="S2.SS1.p3.15.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p3.15.m1.2.2" xref="S2.SS1.p3.15.m1.2.2.cmml">â€¦</mi><mo id="S2.SS1.p3.15.m1.3.4.3.2.2" xref="S2.SS1.p3.15.m1.3.4.3.1.cmml">,</mo><mi id="S2.SS1.p3.15.m1.3.3" xref="S2.SS1.p3.15.m1.3.3.cmml">b</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.15.m1.3b"><apply id="S2.SS1.p3.15.m1.3.4.cmml" xref="S2.SS1.p3.15.m1.3.4"><eq id="S2.SS1.p3.15.m1.3.4.1.cmml" xref="S2.SS1.p3.15.m1.3.4.1"></eq><ci id="S2.SS1.p3.15.m1.3.4.2.cmml" xref="S2.SS1.p3.15.m1.3.4.2">ğ‘–</ci><list id="S2.SS1.p3.15.m1.3.4.3.1.cmml" xref="S2.SS1.p3.15.m1.3.4.3.2"><cn type="integer" id="S2.SS1.p3.15.m1.1.1.cmml" xref="S2.SS1.p3.15.m1.1.1">1</cn><ci id="S2.SS1.p3.15.m1.2.2.cmml" xref="S2.SS1.p3.15.m1.2.2">â€¦</ci><ci id="S2.SS1.p3.15.m1.3.3.cmml" xref="S2.SS1.p3.15.m1.3.3">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.15.m1.3c">i=1,...,b</annotation></semantics></math>, where <math id="S2.SS1.p3.16.m2.2" class="ltx_Math" alttext="\text{min}_{H},\text{min}_{W}" display="inline"><semantics id="S2.SS1.p3.16.m2.2a"><mrow id="S2.SS1.p3.16.m2.2.2.2" xref="S2.SS1.p3.16.m2.2.2.3.cmml"><msub id="S2.SS1.p3.16.m2.1.1.1.1" xref="S2.SS1.p3.16.m2.1.1.1.1.cmml"><mtext id="S2.SS1.p3.16.m2.1.1.1.1.2" xref="S2.SS1.p3.16.m2.1.1.1.1.2a.cmml">min</mtext><mi id="S2.SS1.p3.16.m2.1.1.1.1.3" xref="S2.SS1.p3.16.m2.1.1.1.1.3.cmml">H</mi></msub><mo id="S2.SS1.p3.16.m2.2.2.2.3" xref="S2.SS1.p3.16.m2.2.2.3.cmml">,</mo><msub id="S2.SS1.p3.16.m2.2.2.2.2" xref="S2.SS1.p3.16.m2.2.2.2.2.cmml"><mtext id="S2.SS1.p3.16.m2.2.2.2.2.2" xref="S2.SS1.p3.16.m2.2.2.2.2.2a.cmml">min</mtext><mi id="S2.SS1.p3.16.m2.2.2.2.2.3" xref="S2.SS1.p3.16.m2.2.2.2.2.3.cmml">W</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.16.m2.2b"><list id="S2.SS1.p3.16.m2.2.2.3.cmml" xref="S2.SS1.p3.16.m2.2.2.2"><apply id="S2.SS1.p3.16.m2.1.1.1.1.cmml" xref="S2.SS1.p3.16.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.16.m2.1.1.1.1.1.cmml" xref="S2.SS1.p3.16.m2.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.16.m2.1.1.1.1.2a.cmml" xref="S2.SS1.p3.16.m2.1.1.1.1.2"><mtext id="S2.SS1.p3.16.m2.1.1.1.1.2.cmml" xref="S2.SS1.p3.16.m2.1.1.1.1.2">min</mtext></ci><ci id="S2.SS1.p3.16.m2.1.1.1.1.3.cmml" xref="S2.SS1.p3.16.m2.1.1.1.1.3">ğ»</ci></apply><apply id="S2.SS1.p3.16.m2.2.2.2.2.cmml" xref="S2.SS1.p3.16.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.16.m2.2.2.2.2.1.cmml" xref="S2.SS1.p3.16.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p3.16.m2.2.2.2.2.2a.cmml" xref="S2.SS1.p3.16.m2.2.2.2.2.2"><mtext id="S2.SS1.p3.16.m2.2.2.2.2.2.cmml" xref="S2.SS1.p3.16.m2.2.2.2.2.2">min</mtext></ci><ci id="S2.SS1.p3.16.m2.2.2.2.2.3.cmml" xref="S2.SS1.p3.16.m2.2.2.2.2.3">ğ‘Š</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.16.m2.2c">\text{min}_{H},\text{min}_{W}</annotation></semantics></math> are hyperparameters.
The resulting vector of boxes <math id="S2.SS1.p3.17.m3.1" class="ltx_Math" alttext="\textbf{I}_{n}^{\prime}" display="inline"><semantics id="S2.SS1.p3.17.m3.1a"><msubsup id="S2.SS1.p3.17.m3.1.1" xref="S2.SS1.p3.17.m3.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.SS1.p3.17.m3.1.1.2.2" xref="S2.SS1.p3.17.m3.1.1.2.2a.cmml">I</mtext><mi id="S2.SS1.p3.17.m3.1.1.2.3" xref="S2.SS1.p3.17.m3.1.1.2.3.cmml">n</mi><mo id="S2.SS1.p3.17.m3.1.1.3" xref="S2.SS1.p3.17.m3.1.1.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.17.m3.1b"><apply id="S2.SS1.p3.17.m3.1.1.cmml" xref="S2.SS1.p3.17.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.17.m3.1.1.1.cmml" xref="S2.SS1.p3.17.m3.1.1">superscript</csymbol><apply id="S2.SS1.p3.17.m3.1.1.2.cmml" xref="S2.SS1.p3.17.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.17.m3.1.1.2.1.cmml" xref="S2.SS1.p3.17.m3.1.1">subscript</csymbol><ci id="S2.SS1.p3.17.m3.1.1.2.2a.cmml" xref="S2.SS1.p3.17.m3.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.SS1.p3.17.m3.1.1.2.2.cmml" xref="S2.SS1.p3.17.m3.1.1.2.2">I</mtext></ci><ci id="S2.SS1.p3.17.m3.1.1.2.3.cmml" xref="S2.SS1.p3.17.m3.1.1.2.3">ğ‘›</ci></apply><ci id="S2.SS1.p3.17.m3.1.1.3.cmml" xref="S2.SS1.p3.17.m3.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.17.m3.1c">\textbf{I}_{n}^{\prime}</annotation></semantics></math> is obtained by equally resizing the boxes to <math id="S2.SS1.p3.18.m4.1" class="ltx_Math" alttext="H^{\prime}" display="inline"><semantics id="S2.SS1.p3.18.m4.1a"><msup id="S2.SS1.p3.18.m4.1.1" xref="S2.SS1.p3.18.m4.1.1.cmml"><mi id="S2.SS1.p3.18.m4.1.1.2" xref="S2.SS1.p3.18.m4.1.1.2.cmml">H</mi><mo id="S2.SS1.p3.18.m4.1.1.3" xref="S2.SS1.p3.18.m4.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.18.m4.1b"><apply id="S2.SS1.p3.18.m4.1.1.cmml" xref="S2.SS1.p3.18.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.18.m4.1.1.1.cmml" xref="S2.SS1.p3.18.m4.1.1">superscript</csymbol><ci id="S2.SS1.p3.18.m4.1.1.2.cmml" xref="S2.SS1.p3.18.m4.1.1.2">ğ»</ci><ci id="S2.SS1.p3.18.m4.1.1.3.cmml" xref="S2.SS1.p3.18.m4.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.18.m4.1c">H^{\prime}</annotation></semantics></math> and <math id="S2.SS1.p3.19.m5.1" class="ltx_Math" alttext="W^{\prime}" display="inline"><semantics id="S2.SS1.p3.19.m5.1a"><msup id="S2.SS1.p3.19.m5.1.1" xref="S2.SS1.p3.19.m5.1.1.cmml"><mi id="S2.SS1.p3.19.m5.1.1.2" xref="S2.SS1.p3.19.m5.1.1.2.cmml">W</mi><mo id="S2.SS1.p3.19.m5.1.1.3" xref="S2.SS1.p3.19.m5.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.19.m5.1b"><apply id="S2.SS1.p3.19.m5.1.1.cmml" xref="S2.SS1.p3.19.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.19.m5.1.1.1.cmml" xref="S2.SS1.p3.19.m5.1.1">superscript</csymbol><ci id="S2.SS1.p3.19.m5.1.1.2.cmml" xref="S2.SS1.p3.19.m5.1.1.2">ğ‘Š</ci><ci id="S2.SS1.p3.19.m5.1.1.3.cmml" xref="S2.SS1.p3.19.m5.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.19.m5.1c">W^{\prime}</annotation></semantics></math> with bicubic interpolation. Then, we stack the image boxes and pass them to the image encoder network <math id="S2.SS1.p3.20.m6.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.SS1.p3.20.m6.1a"><mi id="S2.SS1.p3.20.m6.1.1" xref="S2.SS1.p3.20.m6.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.20.m6.1b"><ci id="S2.SS1.p3.20.m6.1.1.cmml" xref="S2.SS1.p3.20.m6.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.20.m6.1c">f</annotation></semantics></math>.
Finally, we transform the output <math id="S2.SS1.p3.21.m7.1" class="ltx_Math" alttext="\textbf{I}^{*}=f(\textbf{I}_{n}^{\prime})" display="inline"><semantics id="S2.SS1.p3.21.m7.1a"><mrow id="S2.SS1.p3.21.m7.1.1" xref="S2.SS1.p3.21.m7.1.1.cmml"><msup id="S2.SS1.p3.21.m7.1.1.3" xref="S2.SS1.p3.21.m7.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S2.SS1.p3.21.m7.1.1.3.2" xref="S2.SS1.p3.21.m7.1.1.3.2a.cmml">I</mtext><mo id="S2.SS1.p3.21.m7.1.1.3.3" xref="S2.SS1.p3.21.m7.1.1.3.3.cmml">âˆ—</mo></msup><mo id="S2.SS1.p3.21.m7.1.1.2" xref="S2.SS1.p3.21.m7.1.1.2.cmml">=</mo><mrow id="S2.SS1.p3.21.m7.1.1.1" xref="S2.SS1.p3.21.m7.1.1.1.cmml"><mi id="S2.SS1.p3.21.m7.1.1.1.3" xref="S2.SS1.p3.21.m7.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.21.m7.1.1.1.2" xref="S2.SS1.p3.21.m7.1.1.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p3.21.m7.1.1.1.1.1" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.21.m7.1.1.1.1.1.2" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.SS1.p3.21.m7.1.1.1.1.1.1" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.2" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.2a.cmml">I</mtext><mi id="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.3" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.3.cmml">n</mi><mo id="S2.SS1.p3.21.m7.1.1.1.1.1.1.3" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.3.cmml">â€²</mo></msubsup><mo stretchy="false" id="S2.SS1.p3.21.m7.1.1.1.1.1.3" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.21.m7.1b"><apply id="S2.SS1.p3.21.m7.1.1.cmml" xref="S2.SS1.p3.21.m7.1.1"><eq id="S2.SS1.p3.21.m7.1.1.2.cmml" xref="S2.SS1.p3.21.m7.1.1.2"></eq><apply id="S2.SS1.p3.21.m7.1.1.3.cmml" xref="S2.SS1.p3.21.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.21.m7.1.1.3.1.cmml" xref="S2.SS1.p3.21.m7.1.1.3">superscript</csymbol><ci id="S2.SS1.p3.21.m7.1.1.3.2a.cmml" xref="S2.SS1.p3.21.m7.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S2.SS1.p3.21.m7.1.1.3.2.cmml" xref="S2.SS1.p3.21.m7.1.1.3.2">I</mtext></ci><times id="S2.SS1.p3.21.m7.1.1.3.3.cmml" xref="S2.SS1.p3.21.m7.1.1.3.3"></times></apply><apply id="S2.SS1.p3.21.m7.1.1.1.cmml" xref="S2.SS1.p3.21.m7.1.1.1"><times id="S2.SS1.p3.21.m7.1.1.1.2.cmml" xref="S2.SS1.p3.21.m7.1.1.1.2"></times><ci id="S2.SS1.p3.21.m7.1.1.1.3.cmml" xref="S2.SS1.p3.21.m7.1.1.1.3">ğ‘“</ci><apply id="S2.SS1.p3.21.m7.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.21.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.21.m7.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.21.m7.1.1.1.1.1">superscript</csymbol><apply id="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.21.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.1.cmml" xref="S2.SS1.p3.21.m7.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.2a.cmml" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.2.cmml" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.2">I</mtext></ci><ci id="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.3.cmml" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.2.3">ğ‘›</ci></apply><ci id="S2.SS1.p3.21.m7.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.21.m7.1.1.1.1.1.1.3">â€²</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.21.m7.1c">\textbf{I}^{*}=f(\textbf{I}_{n}^{\prime})</annotation></semantics></math> into a suitable shape for the fusion module.
To this end, we reshape the features and keep the number of boxes as the first dimension, whereas the second dimension is given by flattening the remaining directions, resulting in a matrix.
The matrix is fed into an <span title="" class="ltx_glossaryref">MLP</span>, to project its size to <math id="S2.SS1.p3.22.m8.1" class="ltx_Math" alttext="b\times v" display="inline"><semantics id="S2.SS1.p3.22.m8.1a"><mrow id="S2.SS1.p3.22.m8.1.1" xref="S2.SS1.p3.22.m8.1.1.cmml"><mi id="S2.SS1.p3.22.m8.1.1.2" xref="S2.SS1.p3.22.m8.1.1.2.cmml">b</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p3.22.m8.1.1.1" xref="S2.SS1.p3.22.m8.1.1.1.cmml">Ã—</mo><mi id="S2.SS1.p3.22.m8.1.1.3" xref="S2.SS1.p3.22.m8.1.1.3.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.22.m8.1b"><apply id="S2.SS1.p3.22.m8.1.1.cmml" xref="S2.SS1.p3.22.m8.1.1"><times id="S2.SS1.p3.22.m8.1.1.1.cmml" xref="S2.SS1.p3.22.m8.1.1.1"></times><ci id="S2.SS1.p3.22.m8.1.1.2.cmml" xref="S2.SS1.p3.22.m8.1.1.2">ğ‘</ci><ci id="S2.SS1.p3.22.m8.1.1.3.cmml" xref="S2.SS1.p3.22.m8.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.22.m8.1c">b\times v</annotation></semantics></math>, where <math id="S2.SS1.p3.23.m9.1" class="ltx_Math" alttext="v\in\mathbb{N}" display="inline"><semantics id="S2.SS1.p3.23.m9.1a"><mrow id="S2.SS1.p3.23.m9.1.1" xref="S2.SS1.p3.23.m9.1.1.cmml"><mi id="S2.SS1.p3.23.m9.1.1.2" xref="S2.SS1.p3.23.m9.1.1.2.cmml">v</mi><mo id="S2.SS1.p3.23.m9.1.1.1" xref="S2.SS1.p3.23.m9.1.1.1.cmml">âˆˆ</mo><mi id="S2.SS1.p3.23.m9.1.1.3" xref="S2.SS1.p3.23.m9.1.1.3.cmml">â„•</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.23.m9.1b"><apply id="S2.SS1.p3.23.m9.1.1.cmml" xref="S2.SS1.p3.23.m9.1.1"><in id="S2.SS1.p3.23.m9.1.1.1.cmml" xref="S2.SS1.p3.23.m9.1.1.1"></in><ci id="S2.SS1.p3.23.m9.1.1.2.cmml" xref="S2.SS1.p3.23.m9.1.1.2">ğ‘£</ci><ci id="S2.SS1.p3.23.m9.1.1.3.cmml" xref="S2.SS1.p3.23.m9.1.1.3">â„•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.23.m9.1c">v\in\mathbb{N}</annotation></semantics></math> is the so-called image embedding dimension of the VisualBERT [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib11" title="" class="ltx_ref">11</a></cite>] model (i.e., the required input dimension for the image modality).
Finally, the output image embedding vectors <math id="S2.SS1.p3.24.m10.1" class="ltx_Math" alttext="\mathbf{Z}_{n}=\mathrm{MLP}(\textbf{I}^{*})" display="inline"><semantics id="S2.SS1.p3.24.m10.1a"><mrow id="S2.SS1.p3.24.m10.1.1" xref="S2.SS1.p3.24.m10.1.1.cmml"><msub id="S2.SS1.p3.24.m10.1.1.3" xref="S2.SS1.p3.24.m10.1.1.3.cmml"><mi id="S2.SS1.p3.24.m10.1.1.3.2" xref="S2.SS1.p3.24.m10.1.1.3.2.cmml">ğ™</mi><mi id="S2.SS1.p3.24.m10.1.1.3.3" xref="S2.SS1.p3.24.m10.1.1.3.3.cmml">n</mi></msub><mo id="S2.SS1.p3.24.m10.1.1.2" xref="S2.SS1.p3.24.m10.1.1.2.cmml">=</mo><mrow id="S2.SS1.p3.24.m10.1.1.1" xref="S2.SS1.p3.24.m10.1.1.1.cmml"><mi id="S2.SS1.p3.24.m10.1.1.1.3" xref="S2.SS1.p3.24.m10.1.1.1.3.cmml">MLP</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.24.m10.1.1.1.2" xref="S2.SS1.p3.24.m10.1.1.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p3.24.m10.1.1.1.1.1" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.24.m10.1.1.1.1.1.2" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.cmml">(</mo><msup id="S2.SS1.p3.24.m10.1.1.1.1.1.1" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.SS1.p3.24.m10.1.1.1.1.1.1.2" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.2a.cmml">I</mtext><mo id="S2.SS1.p3.24.m10.1.1.1.1.1.1.3" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.3.cmml">âˆ—</mo></msup><mo stretchy="false" id="S2.SS1.p3.24.m10.1.1.1.1.1.3" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.24.m10.1b"><apply id="S2.SS1.p3.24.m10.1.1.cmml" xref="S2.SS1.p3.24.m10.1.1"><eq id="S2.SS1.p3.24.m10.1.1.2.cmml" xref="S2.SS1.p3.24.m10.1.1.2"></eq><apply id="S2.SS1.p3.24.m10.1.1.3.cmml" xref="S2.SS1.p3.24.m10.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.24.m10.1.1.3.1.cmml" xref="S2.SS1.p3.24.m10.1.1.3">subscript</csymbol><ci id="S2.SS1.p3.24.m10.1.1.3.2.cmml" xref="S2.SS1.p3.24.m10.1.1.3.2">ğ™</ci><ci id="S2.SS1.p3.24.m10.1.1.3.3.cmml" xref="S2.SS1.p3.24.m10.1.1.3.3">ğ‘›</ci></apply><apply id="S2.SS1.p3.24.m10.1.1.1.cmml" xref="S2.SS1.p3.24.m10.1.1.1"><times id="S2.SS1.p3.24.m10.1.1.1.2.cmml" xref="S2.SS1.p3.24.m10.1.1.1.2"></times><ci id="S2.SS1.p3.24.m10.1.1.1.3.cmml" xref="S2.SS1.p3.24.m10.1.1.1.3">MLP</ci><apply id="S2.SS1.p3.24.m10.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.24.m10.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.24.m10.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.24.m10.1.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p3.24.m10.1.1.1.1.1.1.2a.cmml" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.SS1.p3.24.m10.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.2">I</mtext></ci><times id="S2.SS1.p3.24.m10.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.24.m10.1.1.1.1.1.1.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.24.m10.1c">\mathbf{Z}_{n}=\mathrm{MLP}(\textbf{I}^{*})</annotation></semantics></math> are given to the fusion module.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.5" class="ltx_p">To extract the text features from a question <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{Q}_{n}^{m}" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><msubsup id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml"><mi id="S2.SS1.p4.1.m1.1.1.2.2" xref="S2.SS1.p4.1.m1.1.1.2.2.cmml">ğ</mi><mi id="S2.SS1.p4.1.m1.1.1.2.3" xref="S2.SS1.p4.1.m1.1.1.2.3.cmml">n</mi><mi id="S2.SS1.p4.1.m1.1.1.3" xref="S2.SS1.p4.1.m1.1.1.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">superscript</csymbol><apply id="S2.SS1.p4.1.m1.1.1.2.cmml" xref="S2.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.1.m1.1.1.2.1.cmml" xref="S2.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p4.1.m1.1.1.2.2.cmml" xref="S2.SS1.p4.1.m1.1.1.2.2">ğ</ci><ci id="S2.SS1.p4.1.m1.1.1.2.3.cmml" xref="S2.SS1.p4.1.m1.1.1.2.3">ğ‘›</ci></apply><ci id="S2.SS1.p4.1.m1.1.1.3.cmml" xref="S2.SS1.p4.1.m1.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">\mathbf{Q}_{n}^{m}</annotation></semantics></math>, we translate the question into tokens. These tokens are a mapping of the words to a numerical value.
To achieve this and to be aligned with the original VisualBERT model, we utilize a frozen BertTokenizer [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib16" title="" class="ltx_ref">16</a></cite>]â€‰ as text modality encoder network <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">g</annotation></semantics></math>, where <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{T}_{n}^{m}=g(\mathbf{Q}_{n}^{m})" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><mrow id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml"><msubsup id="S2.SS1.p4.3.m3.1.1.3" xref="S2.SS1.p4.3.m3.1.1.3.cmml"><mi id="S2.SS1.p4.3.m3.1.1.3.2.2" xref="S2.SS1.p4.3.m3.1.1.3.2.2.cmml">ğ“</mi><mi id="S2.SS1.p4.3.m3.1.1.3.2.3" xref="S2.SS1.p4.3.m3.1.1.3.2.3.cmml">n</mi><mi id="S2.SS1.p4.3.m3.1.1.3.3" xref="S2.SS1.p4.3.m3.1.1.3.3.cmml">m</mi></msubsup><mo id="S2.SS1.p4.3.m3.1.1.2" xref="S2.SS1.p4.3.m3.1.1.2.cmml">=</mo><mrow id="S2.SS1.p4.3.m3.1.1.1" xref="S2.SS1.p4.3.m3.1.1.1.cmml"><mi id="S2.SS1.p4.3.m3.1.1.1.3" xref="S2.SS1.p4.3.m3.1.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.3.m3.1.1.1.2" xref="S2.SS1.p4.3.m3.1.1.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p4.3.m3.1.1.1.1.1" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p4.3.m3.1.1.1.1.1.2" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.cmml">(</mo><msubsup id="S2.SS1.p4.3.m3.1.1.1.1.1.1" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.2" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.2.cmml">ğ</mi><mi id="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.3" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.3.cmml">n</mi><mi id="S2.SS1.p4.3.m3.1.1.1.1.1.1.3" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.3.cmml">m</mi></msubsup><mo stretchy="false" id="S2.SS1.p4.3.m3.1.1.1.1.1.3" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><apply id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1"><eq id="S2.SS1.p4.3.m3.1.1.2.cmml" xref="S2.SS1.p4.3.m3.1.1.2"></eq><apply id="S2.SS1.p4.3.m3.1.1.3.cmml" xref="S2.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.3.1.cmml" xref="S2.SS1.p4.3.m3.1.1.3">superscript</csymbol><apply id="S2.SS1.p4.3.m3.1.1.3.2.cmml" xref="S2.SS1.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.3.2.1.cmml" xref="S2.SS1.p4.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS1.p4.3.m3.1.1.3.2.2.cmml" xref="S2.SS1.p4.3.m3.1.1.3.2.2">ğ“</ci><ci id="S2.SS1.p4.3.m3.1.1.3.2.3.cmml" xref="S2.SS1.p4.3.m3.1.1.3.2.3">ğ‘›</ci></apply><ci id="S2.SS1.p4.3.m3.1.1.3.3.cmml" xref="S2.SS1.p4.3.m3.1.1.3.3">ğ‘š</ci></apply><apply id="S2.SS1.p4.3.m3.1.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1.1"><times id="S2.SS1.p4.3.m3.1.1.1.2.cmml" xref="S2.SS1.p4.3.m3.1.1.1.2"></times><ci id="S2.SS1.p4.3.m3.1.1.1.3.cmml" xref="S2.SS1.p4.3.m3.1.1.1.3">ğ‘”</ci><apply id="S2.SS1.p4.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1.1.1.1">superscript</csymbol><apply id="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p4.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.1.cmml" xref="S2.SS1.p4.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.2.cmml" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.2">ğ</ci><ci id="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.3.cmml" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.2.3">ğ‘›</ci></apply><ci id="S2.SS1.p4.3.m3.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p4.3.m3.1.1.1.1.1.1.3">ğ‘š</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">\mathbf{T}_{n}^{m}=g(\mathbf{Q}_{n}^{m})</annotation></semantics></math> is the output text embedding vector. Both <math id="S2.SS1.p4.4.m4.1" class="ltx_Math" alttext="\mathbf{Z}_{n}" display="inline"><semantics id="S2.SS1.p4.4.m4.1a"><msub id="S2.SS1.p4.4.m4.1.1" xref="S2.SS1.p4.4.m4.1.1.cmml"><mi id="S2.SS1.p4.4.m4.1.1.2" xref="S2.SS1.p4.4.m4.1.1.2.cmml">ğ™</mi><mi id="S2.SS1.p4.4.m4.1.1.3" xref="S2.SS1.p4.4.m4.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.4.m4.1b"><apply id="S2.SS1.p4.4.m4.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.4.m4.1.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p4.4.m4.1.1.2.cmml" xref="S2.SS1.p4.4.m4.1.1.2">ğ™</ci><ci id="S2.SS1.p4.4.m4.1.1.3.cmml" xref="S2.SS1.p4.4.m4.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.4.m4.1c">\mathbf{Z}_{n}</annotation></semantics></math> and <math id="S2.SS1.p4.5.m5.1" class="ltx_Math" alttext="\mathbf{T}_{n}^{m}" display="inline"><semantics id="S2.SS1.p4.5.m5.1a"><msubsup id="S2.SS1.p4.5.m5.1.1" xref="S2.SS1.p4.5.m5.1.1.cmml"><mi id="S2.SS1.p4.5.m5.1.1.2.2" xref="S2.SS1.p4.5.m5.1.1.2.2.cmml">ğ“</mi><mi id="S2.SS1.p4.5.m5.1.1.2.3" xref="S2.SS1.p4.5.m5.1.1.2.3.cmml">n</mi><mi id="S2.SS1.p4.5.m5.1.1.3" xref="S2.SS1.p4.5.m5.1.1.3.cmml">m</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.5.m5.1b"><apply id="S2.SS1.p4.5.m5.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1">superscript</csymbol><apply id="S2.SS1.p4.5.m5.1.1.2.cmml" xref="S2.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.2.1.cmml" xref="S2.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p4.5.m5.1.1.2.2.cmml" xref="S2.SS1.p4.5.m5.1.1.2.2">ğ“</ci><ci id="S2.SS1.p4.5.m5.1.1.2.3.cmml" xref="S2.SS1.p4.5.m5.1.1.2.3">ğ‘›</ci></apply><ci id="S2.SS1.p4.5.m5.1.1.3.cmml" xref="S2.SS1.p4.5.m5.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.5.m5.1c">\mathbf{T}_{n}^{m}</annotation></semantics></math> is jointly processed to extract their implicit knowledge in the next step.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Fusion Module</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.3" class="ltx_p">Our transformer-based fusion module jointly learns the alignment between the image and text modality.
To learn the joint alignment, we leverage <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">l</annotation></semantics></math> VisualBERT layers.
These layers are an extension of the language transformer BERT with the image modality.
VisualBERT first projects the image features <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{Z}_{n}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">ğ™</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ğ™</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\mathbf{Z}_{n}</annotation></semantics></math> into a suitable shape to concatenate the projected image features with the language modality.
The concatenated image and text features <math id="S2.SS2.p1.3.m3.2" class="ltx_Math" alttext="[\mathbf{Z}_{n},\mathbf{T}_{n}^{m}]" display="inline"><semantics id="S2.SS2.p1.3.m3.2a"><mrow id="S2.SS2.p1.3.m3.2.2.2" xref="S2.SS2.p1.3.m3.2.2.3.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m3.2.2.2.3" xref="S2.SS2.p1.3.m3.2.2.3.cmml">[</mo><msub id="S2.SS2.p1.3.m3.1.1.1.1" xref="S2.SS2.p1.3.m3.1.1.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.1.1.2" xref="S2.SS2.p1.3.m3.1.1.1.1.2.cmml">ğ™</mi><mi id="S2.SS2.p1.3.m3.1.1.1.1.3" xref="S2.SS2.p1.3.m3.1.1.1.1.3.cmml">n</mi></msub><mo id="S2.SS2.p1.3.m3.2.2.2.4" xref="S2.SS2.p1.3.m3.2.2.3.cmml">,</mo><msubsup id="S2.SS2.p1.3.m3.2.2.2.2" xref="S2.SS2.p1.3.m3.2.2.2.2.cmml"><mi id="S2.SS2.p1.3.m3.2.2.2.2.2.2" xref="S2.SS2.p1.3.m3.2.2.2.2.2.2.cmml">ğ“</mi><mi id="S2.SS2.p1.3.m3.2.2.2.2.2.3" xref="S2.SS2.p1.3.m3.2.2.2.2.2.3.cmml">n</mi><mi id="S2.SS2.p1.3.m3.2.2.2.2.3" xref="S2.SS2.p1.3.m3.2.2.2.2.3.cmml">m</mi></msubsup><mo stretchy="false" id="S2.SS2.p1.3.m3.2.2.2.5" xref="S2.SS2.p1.3.m3.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.2b"><interval closure="closed" id="S2.SS2.p1.3.m3.2.2.3.cmml" xref="S2.SS2.p1.3.m3.2.2.2"><apply id="S2.SS2.p1.3.m3.1.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.1.1.2">ğ™</ci><ci id="S2.SS2.p1.3.m3.1.1.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.1.1.3">ğ‘›</ci></apply><apply id="S2.SS2.p1.3.m3.2.2.2.2.cmml" xref="S2.SS2.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.2.2.2.2.1.cmml" xref="S2.SS2.p1.3.m3.2.2.2.2">superscript</csymbol><apply id="S2.SS2.p1.3.m3.2.2.2.2.2.cmml" xref="S2.SS2.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.2.2.2.2.2.1.cmml" xref="S2.SS2.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="S2.SS2.p1.3.m3.2.2.2.2.2.2.cmml" xref="S2.SS2.p1.3.m3.2.2.2.2.2.2">ğ“</ci><ci id="S2.SS2.p1.3.m3.2.2.2.2.2.3.cmml" xref="S2.SS2.p1.3.m3.2.2.2.2.2.3">ğ‘›</ci></apply><ci id="S2.SS2.p1.3.m3.2.2.2.2.3.cmml" xref="S2.SS2.p1.3.m3.2.2.2.2.3">ğ‘š</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.2c">[\mathbf{Z}_{n},\mathbf{T}_{n}^{m}]</annotation></semantics></math> are then fed to the BERT transformer layers.
The output is given by pooling the last hidden state.
The resulting vector is further processed in the classification module.
To summarize the forward procedure, VisualBERT fuses the image and language modalities and feeds them into the BERT architecture, representing a natural extension of BERT to multiple modalities.
Additionally, VisualBERT shows competitive results on VQA benchmark datasets and is thus well suited as a starting point for multi-modal fusion transformers in RS VQA.
Unlike the previous RS VQA models that simply combine both modalities in a non-learnable fashion, our model learns a joint representation.
Note that our approach differs from [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib7" title="" class="ltx_ref">7</a></cite>] because they apply a transformer only to the language modality. In contrast to the model proposed in [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib3" title="" class="ltx_ref">3</a></cite>], we apply exclusively multi-modal self-attention instead of a mixture of single- and multi-modal self-attention.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Classification Module</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The task of the classification module is to create an output vector representing the prediction for the specific answers.
Following [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">1</a></cite>], we utilize an <span title="" class="ltx_glossaryref">MLP</span> as a classification module.
The <span title="" class="ltx_glossaryref">MLP</span> consists of three layers, where the last layer has the dimension of the considered answer set.
The final answer is then obtained by selecting the largest activation of the output vector.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Description and Experimental Setup</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We conducted experiments on the RSVQA-LR [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">1</a></cite>] and the RSVQAxBEN [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>] datasets.
The RSVQA-LR dataset was constructed using 7 Sentinel-2 tiles acquired over the Netherlands, from which only the RGB bands were used.
The tiles are divided into 772 patches of size 256x256 pixels.
To obtain the information needed to create the image-question-answer triplets, knowledge from the publicly available OpenStreetMap database was leveraged.
With this knowledge, the dataset was constructed by taking an image patch and randomly generating question-answer pairs from a given template.
This procedure resulted in 77,232 image-question-answer triplets.
As in [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">1</a></cite>], we used the same tile-based train/validation/test split.
The training split consists of five tiles, whereas the validation and test split consist of one tile each.
For further information on the RSVQA-LR dataset (e.g., the definition of the question classes), the reader is referred to [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">1</a></cite>].</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The RSVQAxBEN [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>] dataset embodies the largest freely available RS VQA benchmark dataset.
It is based on the BigEarthNet (BEN) [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib19" title="" class="ltx_ref">19</a></cite>] archive and contains 590,326 Sentinel-2 L2A image patches with 12 spectral bands of varying resolution (10m, 20m, and 60m).
The content of each patch is described by multiple class labels from the CORINE Land Cover (CLC) 2018 database.
The RSVQAxBEN dataset was constructed as a large-scale benchmark for RS VQA, using only the RGB bands of the BEN patches (which have a spatial resolution of 10m). A stochastic algorithm that utilizes the CLC labels produced the image-question-answer triplets.
The procedure described in [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>] resulted in 14,758,150 image-question-answer triplets,
which include 26,875 unique answers.
To limit the number of possible answers, the modelâ€™s output in [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>] was restricted to the 1,000 most frequent answers, covering <math id="S3.p2.1.m1.3" class="ltx_Math" alttext="98.1\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S3.p2.1.m1.3a"><mrow id="S3.p2.1.m1.3.3" xref="S3.p2.1.m1.3.3.cmml"><mn id="S3.p2.1.m1.1.1.1.1.1.1" xref="S3.p2.1.m1.1.1.1.1.1.1.cmml">98.1</mn><mtext id="S3.p2.1.m1.2.2.2.2.2.2" xref="S3.p2.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S3.p2.1.m1.3.3.3.3.3.3" xref="S3.p2.1.m1.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.3b"><apply id="S3.p2.1.m1.3.3.cmml" xref="S3.p2.1.m1.3.3"><csymbol cd="latexml" id="S3.p2.1.m1.2.2.2.2.2.2.cmml" xref="S3.p2.1.m1.2.2.2.2.2.2">times</csymbol><cn type="float" id="S3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1.1.1.1">98.1</cn><csymbol cd="latexml" id="S3.p2.1.m1.3.3.3.3.3.3.cmml" xref="S3.p2.1.m1.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.3c">98.1\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math> of the answer set.
For the sake of comparability, we also apply this restriction.
We use the same train/validation/test split based on the tilesâ€™ spatial location as in [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>].
For more statistical insights, the reader is referred to [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>]â€‰.
We provide experimental results on the original RSVQAxBEN dataset.
Furthermore, we extended the original three-band RSVQAxBEN dataset to a ten-band version by including all spectral bands with 10m and 20m spatial resolution in addition to the RGB bands.
In our experiments, we resampled the 20m bands to the resolution of the 10m bands by using a cubic interpolation method.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.6" class="ltx_p">We performed experiments with different layer configurations for our proposed VBFusion architecture given by the pre-trained VisualBERT [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib11" title="" class="ltx_ref">11</a></cite>] model. The value of <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">l</annotation></semantics></math> (which defines the number of layers selected from the original VisualBERT model) is varied as <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">l</annotation></semantics></math> <math id="S3.p3.3.m3.5" class="ltx_Math" alttext="\in\{4,6,8,10,12\}" display="inline"><semantics id="S3.p3.3.m3.5a"><mrow id="S3.p3.3.m3.5.6" xref="S3.p3.3.m3.5.6.cmml"><mi id="S3.p3.3.m3.5.6.2" xref="S3.p3.3.m3.5.6.2.cmml"></mi><mo id="S3.p3.3.m3.5.6.1" xref="S3.p3.3.m3.5.6.1.cmml">âˆˆ</mo><mrow id="S3.p3.3.m3.5.6.3.2" xref="S3.p3.3.m3.5.6.3.1.cmml"><mo stretchy="false" id="S3.p3.3.m3.5.6.3.2.1" xref="S3.p3.3.m3.5.6.3.1.cmml">{</mo><mn id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">4</mn><mo id="S3.p3.3.m3.5.6.3.2.2" xref="S3.p3.3.m3.5.6.3.1.cmml">,</mo><mn id="S3.p3.3.m3.2.2" xref="S3.p3.3.m3.2.2.cmml">6</mn><mo id="S3.p3.3.m3.5.6.3.2.3" xref="S3.p3.3.m3.5.6.3.1.cmml">,</mo><mn id="S3.p3.3.m3.3.3" xref="S3.p3.3.m3.3.3.cmml">8</mn><mo id="S3.p3.3.m3.5.6.3.2.4" xref="S3.p3.3.m3.5.6.3.1.cmml">,</mo><mn id="S3.p3.3.m3.4.4" xref="S3.p3.3.m3.4.4.cmml">10</mn><mo id="S3.p3.3.m3.5.6.3.2.5" xref="S3.p3.3.m3.5.6.3.1.cmml">,</mo><mn id="S3.p3.3.m3.5.5" xref="S3.p3.3.m3.5.5.cmml">12</mn><mo stretchy="false" id="S3.p3.3.m3.5.6.3.2.6" xref="S3.p3.3.m3.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.5b"><apply id="S3.p3.3.m3.5.6.cmml" xref="S3.p3.3.m3.5.6"><in id="S3.p3.3.m3.5.6.1.cmml" xref="S3.p3.3.m3.5.6.1"></in><csymbol cd="latexml" id="S3.p3.3.m3.5.6.2.cmml" xref="S3.p3.3.m3.5.6.2">absent</csymbol><set id="S3.p3.3.m3.5.6.3.1.cmml" xref="S3.p3.3.m3.5.6.3.2"><cn type="integer" id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">4</cn><cn type="integer" id="S3.p3.3.m3.2.2.cmml" xref="S3.p3.3.m3.2.2">6</cn><cn type="integer" id="S3.p3.3.m3.3.3.cmml" xref="S3.p3.3.m3.3.3">8</cn><cn type="integer" id="S3.p3.3.m3.4.4.cmml" xref="S3.p3.3.m3.4.4">10</cn><cn type="integer" id="S3.p3.3.m3.5.5.cmml" xref="S3.p3.3.m3.5.5">12</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.5c">\in\{4,6,8,10,12\}</annotation></semantics></math>.
Note that when <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="l=12" display="inline"><semantics id="S3.p3.4.m4.1a"><mrow id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml"><mi id="S3.p3.4.m4.1.1.2" xref="S3.p3.4.m4.1.1.2.cmml">l</mi><mo id="S3.p3.4.m4.1.1.1" xref="S3.p3.4.m4.1.1.1.cmml">=</mo><mn id="S3.p3.4.m4.1.1.3" xref="S3.p3.4.m4.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><apply id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1"><eq id="S3.p3.4.m4.1.1.1.cmml" xref="S3.p3.4.m4.1.1.1"></eq><ci id="S3.p3.4.m4.1.1.2.cmml" xref="S3.p3.4.m4.1.1.2">ğ‘™</ci><cn type="integer" id="S3.p3.4.m4.1.1.3.cmml" xref="S3.p3.4.m4.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">l=12</annotation></semantics></math>, it is identical to the original VisualBERT model.
For the image encoder network <math id="S3.p3.5.m5.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">f</annotation></semantics></math>, we utilized a pre-trained ResNet152 [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib15" title="" class="ltx_ref">15</a></cite>] network architecture with frozen weights.
The image encoder network was pre-trained on ImageNet [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib20" title="" class="ltx_ref">20</a></cite>]â€‰ for RSVQA-LR and three-band RSVQAxBEN and on BEN for the ten-band variant.
We extracted ten boxes for the RSVQA-LR and three-band RSVQAxBEN datasets, and for the ten-band variant five boxes.
The learning rate was set to <math id="S3.p3.6.m6.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S3.p3.6.m6.1a"><msup id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml"><mn id="S3.p3.6.m6.1.1.2" xref="S3.p3.6.m6.1.1.2.cmml">10</mn><mrow id="S3.p3.6.m6.1.1.3" xref="S3.p3.6.m6.1.1.3.cmml"><mo id="S3.p3.6.m6.1.1.3a" xref="S3.p3.6.m6.1.1.3.cmml">âˆ’</mo><mn id="S3.p3.6.m6.1.1.3.2" xref="S3.p3.6.m6.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><apply id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p3.6.m6.1.1.1.cmml" xref="S3.p3.6.m6.1.1">superscript</csymbol><cn type="integer" id="S3.p3.6.m6.1.1.2.cmml" xref="S3.p3.6.m6.1.1.2">10</cn><apply id="S3.p3.6.m6.1.1.3.cmml" xref="S3.p3.6.m6.1.1.3"><minus id="S3.p3.6.m6.1.1.3.1.cmml" xref="S3.p3.6.m6.1.1.3"></minus><cn type="integer" id="S3.p3.6.m6.1.1.3.2.cmml" xref="S3.p3.6.m6.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">10^{-6}</annotation></semantics></math>, while the maximum number of training epochs was set to 300 for RSVQA-LR and 20 for RSVQAxBEN. We chose a batch size of 1024 and 2048 for RSVQA-LR and RSVQAxBEN, respectively.
We performed our experiments on a cluster of 8 NVIDIA Tesla A100 GPUs.
We used the implementation from the Huggingface library [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib21" title="" class="ltx_ref">21</a></cite>] for the VisualBERT layers.
Our architecture was compared with the SkipRes [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>] architecture, which is based on Skip-Thoughts and ResNet152. To analyze the results on the RSVQA-LR dataset we consider the same metrics as given in [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">1</a></cite>]: the accuracy of counting questions (called <span id="S3.p3.6.1" class="ltx_text ltx_inline-quote ltx_outerquote">â€œCountâ€</span>), of presence questions (called <span id="S3.p3.6.2" class="ltx_text ltx_inline-quote ltx_outerquote">â€œPresenceâ€</span>), of comparison questions (called <span id="S3.p3.6.3" class="ltx_text ltx_inline-quote ltx_outerquote">â€œComparisonâ€</span>) and of rural/urban questions (called <span id="S3.p3.6.4" class="ltx_text ltx_inline-quote ltx_outerquote">â€œRural/Urbanâ€</span>). Furthermore, we provide the average accuracy (<span id="S3.p3.6.5" class="ltx_text ltx_inline-quote ltx_outerquote">â€œAAâ€</span>) over the question type accuracies and the overall accuracy (<span id="S3.p3.6.6" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span>).
The metrics used to analyze the results on the RSVQAxBEN dataset are: the accuracy of yes/no questions (called <span id="S3.p3.6.7" class="ltx_text ltx_inline-quote ltx_outerquote">â€œYes/Noâ€</span>) and the accuracy of the land use and land cover questions (called <span id="S3.p3.6.8" class="ltx_text ltx_inline-quote ltx_outerquote">â€œLULCâ€</span>), as well as <span id="S3.p3.6.9" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span> and <span id="S3.p3.6.10" class="ltx_text ltx_inline-quote ltx_outerquote">â€œAAâ€</span>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.5.2.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.2.1" class="ltx_text" style="font-size:90%;">Accuracies obtained by using SkipRes and VBFusion with different number <math id="S3.T1.2.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.T1.2.1.m1.1b"><mi id="S3.T1.2.1.m1.1.1" xref="S3.T1.2.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.T1.2.1.m1.1c"><ci id="S3.T1.2.1.m1.1.1.cmml" xref="S3.T1.2.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.1.m1.1d">l</annotation></semantics></math> of layers (RSVQA-LR dataset).</span></figcaption>
<table id="S3.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.1" class="ltx_tr">
<th id="S3.T1.3.1.2" class="ltx_td ltx_align_left ltx_align_bottom ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S3.T1.3.1.2.1" class="ltx_text">Architecture</span></th>
<th id="S3.T1.3.1.1" class="ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S3.T1.3.1.1.1" class="ltx_text"><math id="S3.T1.3.1.1.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.T1.3.1.1.1.m1.1a"><mi id="S3.T1.3.1.1.1.m1.1.1" xref="S3.T1.3.1.1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.T1.3.1.1.1.m1.1b"><ci id="S3.T1.3.1.1.1.m1.1.1.cmml" xref="S3.T1.3.1.1.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.1.1.1.m1.1c">l</annotation></semantics></math></span></th>
<th id="S3.T1.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Question Type</th>
<th id="S3.T1.3.1.4" class="ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.T1.3.1.4.1" class="ltx_text">AA</span></th>
<th id="S3.T1.3.1.5" class="ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.T1.3.1.5.1" class="ltx_text">OA</span></th>
</tr>
<tr id="S3.T1.3.2.1" class="ltx_tr">
<th id="S3.T1.3.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Count</th>
<th id="S3.T1.3.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Presence</th>
<th id="S3.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Comparison</th>
<th id="S3.T1.3.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Rural/Urban</th>
</tr>
<tr id="S3.T1.3.3.2" class="ltx_tr">
<th id="S3.T1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">SkipRes [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>]</th>
<th id="S3.T1.3.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">â€“</th>
<th id="S3.T1.3.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">67.01</th>
<th id="S3.T1.3.3.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">87.46</th>
<th id="S3.T1.3.3.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">81.50</th>
<th id="S3.T1.3.3.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.3.3.2.6.1" class="ltx_text ltx_font_bold">90.00</span></th>
<th id="S3.T1.3.3.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">81.49</th>
<th id="S3.T1.3.3.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">79.08</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.4.1" class="ltx_tr">
<th id="S3.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row ltx_border_bb" rowspan="5"><span id="S3.T1.3.4.1.1.1" class="ltx_text">VBFusion</span></th>
<th id="S3.T1.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">4</th>
<td id="S3.T1.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">68.17</td>
<td id="S3.T1.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">88.02</td>
<td id="S3.T1.3.4.1.5" class="ltx_td ltx_align_center ltx_border_t">88.51</td>
<td id="S3.T1.3.4.1.6" class="ltx_td ltx_align_center ltx_border_t">87.00</td>
<td id="S3.T1.3.4.1.7" class="ltx_td ltx_align_center ltx_border_t">82.92</td>
<td id="S3.T1.3.4.1.8" class="ltx_td ltx_align_center ltx_border_t">82.36</td>
</tr>
<tr id="S3.T1.3.5.2" class="ltx_tr">
<th id="S3.T1.3.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S3.T1.3.5.2.2" class="ltx_td ltx_align_center">68.17</td>
<td id="S3.T1.3.5.2.3" class="ltx_td ltx_align_center">88.87</td>
<td id="S3.T1.3.5.2.4" class="ltx_td ltx_align_center"><span id="S3.T1.3.5.2.4.1" class="ltx_text ltx_font_bold">88.83</span></td>
<td id="S3.T1.3.5.2.5" class="ltx_td ltx_align_center">84.00</td>
<td id="S3.T1.3.5.2.6" class="ltx_td ltx_align_center">82.47</td>
<td id="S3.T1.3.5.2.7" class="ltx_td ltx_align_center">82.71</td>
</tr>
<tr id="S3.T1.3.6.3" class="ltx_tr">
<th id="S3.T1.3.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">8</th>
<td id="S3.T1.3.6.3.2" class="ltx_td ltx_align_center"><span id="S3.T1.3.6.3.2.1" class="ltx_text ltx_font_bold">69.36</span></td>
<td id="S3.T1.3.6.3.3" class="ltx_td ltx_align_center">89.00</td>
<td id="S3.T1.3.6.3.4" class="ltx_td ltx_align_center">83.46</td>
<td id="S3.T1.3.6.3.5" class="ltx_td ltx_align_center">88.00</td>
<td id="S3.T1.3.6.3.6" class="ltx_td ltx_align_center">82.45</td>
<td id="S3.T1.3.6.3.7" class="ltx_td ltx_align_center">80.99</td>
</tr>
<tr id="S3.T1.3.7.4" class="ltx_tr">
<th id="S3.T1.3.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S3.T1.3.7.4.2" class="ltx_td ltx_align_center">67.73</td>
<td id="S3.T1.3.7.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.3.7.4.3.1" class="ltx_text ltx_font_bold">89.48</span></td>
<td id="S3.T1.3.7.4.4" class="ltx_td ltx_align_center">86.68</td>
<td id="S3.T1.3.7.4.5" class="ltx_td ltx_align_center">88.00</td>
<td id="S3.T1.3.7.4.6" class="ltx_td ltx_align_center"><span id="S3.T1.3.7.4.6.1" class="ltx_text ltx_font_bold">82.97</span></td>
<td id="S3.T1.3.7.4.7" class="ltx_td ltx_align_center">81.94</td>
</tr>
<tr id="S3.T1.3.8.5" class="ltx_tr">
<th id="S3.T1.3.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">12</th>
<td id="S3.T1.3.8.5.2" class="ltx_td ltx_align_center ltx_border_bb">68.14</td>
<td id="S3.T1.3.8.5.3" class="ltx_td ltx_align_center ltx_border_bb">89.27</td>
<td id="S3.T1.3.8.5.4" class="ltx_td ltx_align_center ltx_border_bb">88.71</td>
<td id="S3.T1.3.8.5.5" class="ltx_td ltx_align_center ltx_border_bb">85.00</td>
<td id="S3.T1.3.8.5.6" class="ltx_td ltx_align_center ltx_border_bb">82.78</td>
<td id="S3.T1.3.8.5.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.3.8.5.7.1" class="ltx_text ltx_font_bold">82.78</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.4" class="ltx_p"><a href="#S3.T1" title="In 3 Dataset Description and Experimental Setup â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> shows the results obtained on the RSVQA-LR dataset. By analyzing the table, one can see that our proposed VBFusion architecture outperforms the SkipRes model in all metrics except <span id="S4.p1.4.1" class="ltx_text ltx_inline-quote ltx_outerquote">â€œRural/Urbanâ€</span>, independently of the number of layers.
The most significant difference occurs in the <span id="S4.p1.4.2" class="ltx_text ltx_inline-quote ltx_outerquote">â€œComparisonâ€</span> accuracy, which represents the most challenging question type.
Here, a model is required to count the number of occurrences of a selected object with a specific positional relationship to a reference object.
For this type of question, our 6-layer VBFusion architecture significantly outperforms SkipRes by more than <math id="S4.p1.1.m1.3" class="ltx_Math" alttext="7\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p1.1.m1.3a"><mrow id="S4.p1.1.m1.3.3" xref="S4.p1.1.m1.3.3.cmml"><mn id="S4.p1.1.m1.1.1.1.1.1.1" xref="S4.p1.1.m1.1.1.1.1.1.1.cmml">7</mn><mtext id="S4.p1.1.m1.2.2.2.2.2.2" xref="S4.p1.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p1.1.m1.3.3.3.3.3.3" xref="S4.p1.1.m1.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.3b"><apply id="S4.p1.1.m1.3.3.cmml" xref="S4.p1.1.m1.3.3"><csymbol cd="latexml" id="S4.p1.1.m1.2.2.2.2.2.2.cmml" xref="S4.p1.1.m1.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1.1.1.1">7</cn><csymbol cd="latexml" id="S4.p1.1.m1.3.3.3.3.3.3.cmml" xref="S4.p1.1.m1.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.3c">7\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>.
For the <span id="S4.p1.4.3" class="ltx_text ltx_inline-quote ltx_outerquote">â€œCountâ€</span> questions the differences are smaller, e.g., VBFusion with 10 layers outperforms the SkipRes model by only <math id="S4.p1.2.m2.3" class="ltx_Math" alttext="0.7\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p1.2.m2.3a"><mrow id="S4.p1.2.m2.3.3" xref="S4.p1.2.m2.3.3.cmml"><mn id="S4.p1.2.m2.1.1.1.1.1.1" xref="S4.p1.2.m2.1.1.1.1.1.1.cmml">0.7</mn><mtext id="S4.p1.2.m2.2.2.2.2.2.2" xref="S4.p1.2.m2.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p1.2.m2.3.3.3.3.3.3" xref="S4.p1.2.m2.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.3b"><apply id="S4.p1.2.m2.3.3.cmml" xref="S4.p1.2.m2.3.3"><csymbol cd="latexml" id="S4.p1.2.m2.2.2.2.2.2.2.cmml" xref="S4.p1.2.m2.2.2.2.2.2.2">times</csymbol><cn type="float" id="S4.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1.1.1.1">0.7</cn><csymbol cd="latexml" id="S4.p1.2.m2.3.3.3.3.3.3.cmml" xref="S4.p1.2.m2.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.3c">0.7\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>.
In the summarizing metrics <span id="S4.p1.4.4" class="ltx_text ltx_inline-quote ltx_outerquote">â€œAAâ€</span>, and <span id="S4.p1.4.5" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span>, all VBFusion models, independently of the number of layers, significantly outperform SkipRes.
Furthermore, no significant improvement is observed when increasing the number <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">l</annotation></semantics></math> of VisualBert layers.
Although complexity increases, when using more layers, performance changes only slightly.
This observation aligns with the findings of [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib22" title="" class="ltx_ref">22</a></cite>] that large transformers struggle on small datasets.
To benefit from the larger transformer configurations, more samples are probably required.
In the case of the <span id="S4.p1.4.6" class="ltx_text ltx_inline-quote ltx_outerquote">â€œRural/Urbanâ€</span> questions, the underperformance can be explained with the same argument; there are too few questions of this type.
This is because the <span id="S4.p1.4.7" class="ltx_text ltx_inline-quote ltx_outerquote">â€œRural/Urbanâ€</span> question type makes up the smallest proportion of the dataset (<math id="S4.p1.4.m4.3" class="ltx_Math" alttext="1\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p1.4.m4.3a"><mrow id="S4.p1.4.m4.3.3" xref="S4.p1.4.m4.3.3.cmml"><mn id="S4.p1.4.m4.1.1.1.1.1.1" xref="S4.p1.4.m4.1.1.1.1.1.1.cmml">1</mn><mtext id="S4.p1.4.m4.2.2.2.2.2.2" xref="S4.p1.4.m4.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p1.4.m4.3.3.3.3.3.3" xref="S4.p1.4.m4.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.3b"><apply id="S4.p1.4.m4.3.3.cmml" xref="S4.p1.4.m4.3.3"><csymbol cd="latexml" id="S4.p1.4.m4.2.2.2.2.2.2.cmml" xref="S4.p1.4.m4.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S4.p1.4.m4.1.1.1.1.1.1.cmml" xref="S4.p1.4.m4.1.1.1.1.1.1">1</cn><csymbol cd="latexml" id="S4.p1.4.m4.3.3.3.3.3.3.cmml" xref="S4.p1.4.m4.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.3c">1\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>).
By analyzing the results, one can conclude that the proposed VBFusion architecture is able to improve the performance compared to the SkipRes architecture. It is worth emphasizing that the datasets used in the experiments are benchmarks, whereas in many real applications the VQA is expected to be applied to much larger archives. Due to the nature of the large transformer-based architecture, we expect that the gain achieved by the VBFusion architecture will be increased for large-scale RS VQA problems and also for the cases of when much more complex question types are present.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.5.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.2.1" class="ltx_text" style="font-size:90%;">Accuracies obtained by using SkipRes and VBFusion with different number <math id="S4.T2.2.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.T2.2.1.m1.1b"><mi id="S4.T2.2.1.m1.1.1" xref="S4.T2.2.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.1.m1.1c"><ci id="S4.T2.2.1.m1.1.1.cmml" xref="S4.T2.2.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.1.m1.1d">l</annotation></semantics></math> of layers (RSVQAxBEN dataset).</span></figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1" class="ltx_tr">
<th id="S4.T2.3.1.2" class="ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.2.1" class="ltx_text">Architecture</span></th>
<th id="S4.T2.3.1.1" class="ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.1.1" class="ltx_text"><math id="S4.T2.3.1.1.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.T2.3.1.1.1.m1.1a"><mi id="S4.T2.3.1.1.1.m1.1.1" xref="S4.T2.3.1.1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.1.m1.1b"><ci id="S4.T2.3.1.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.1.m1.1.1">ğ‘™</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.1.m1.1c">l</annotation></semantics></math></span></th>
<th id="S4.T2.3.1.3" class="ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.3.1" class="ltx_text">Number of Bands</span></th>
<td id="S4.T2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Question Type</td>
<td id="S4.T2.3.1.5" class="ltx_td ltx_align_center ltx_align_bottom ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.5.1" class="ltx_text">AA</span></td>
<td id="S4.T2.3.1.6" class="ltx_td ltx_align_center ltx_align_bottom ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.6.1" class="ltx_text">OA</span></td>
</tr>
<tr id="S4.T2.3.2.1" class="ltx_tr">
<td id="S4.T2.3.2.1.1" class="ltx_td ltx_align_center ltx_border_t">LULC</td>
<td id="S4.T2.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Yes/No</td>
</tr>
<tr id="S4.T2.3.3.2" class="ltx_tr">
<th id="S4.T2.3.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">SkipRes [<cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib13" title="" class="ltx_ref">13</a></cite>]</th>
<th id="S4.T2.3.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">â€“</th>
<th id="S4.T2.3.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3</th>
<td id="S4.T2.3.3.2.4" class="ltx_td ltx_align_center ltx_border_t">20.68</td>
<td id="S4.T2.3.3.2.5" class="ltx_td ltx_align_center ltx_border_t">80.02</td>
<td id="S4.T2.3.3.2.6" class="ltx_td ltx_align_center ltx_border_t">50.35</td>
<td id="S4.T2.3.3.2.7" class="ltx_td ltx_align_center ltx_border_t">69.92</td>
</tr>
<tr id="S4.T2.3.4.3" class="ltx_tr">
<th id="S4.T2.3.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" rowspan="10"><span id="S4.T2.3.4.3.1.1" class="ltx_text">VBFusion</span></th>
<th id="S4.T2.3.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T2.3.4.3.2.1" class="ltx_text">4</span></th>
<th id="S4.T2.3.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3</th>
<td id="S4.T2.3.4.3.4" class="ltx_td ltx_align_center ltx_border_t">20.40</td>
<td id="S4.T2.3.4.3.5" class="ltx_td ltx_align_center ltx_border_t">83.86</td>
<td id="S4.T2.3.4.3.6" class="ltx_td ltx_align_center ltx_border_t">52.13</td>
<td id="S4.T2.3.4.3.7" class="ltx_td ltx_align_center ltx_border_t">73.06</td>
</tr>
<tr id="S4.T2.3.5.4" class="ltx_tr">
<th id="S4.T2.3.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S4.T2.3.5.4.2" class="ltx_td ltx_align_center">25.72</td>
<td id="S4.T2.3.5.4.3" class="ltx_td ltx_align_center">85.41</td>
<td id="S4.T2.3.5.4.4" class="ltx_td ltx_align_center">55.56</td>
<td id="S4.T2.3.5.4.5" class="ltx_td ltx_align_center">75.26</td>
</tr>
<tr id="S4.T2.3.6.5" class="ltx_tr">
<th id="S4.T2.3.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.3.6.5.1.1" class="ltx_text">6</span></th>
<th id="S4.T2.3.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3</th>
<td id="S4.T2.3.6.5.3" class="ltx_td ltx_align_center ltx_border_t">21.66</td>
<td id="S4.T2.3.6.5.4" class="ltx_td ltx_align_center ltx_border_t">84.58</td>
<td id="S4.T2.3.6.5.5" class="ltx_td ltx_align_center ltx_border_t">53.12</td>
<td id="S4.T2.3.6.5.6" class="ltx_td ltx_align_center ltx_border_t">73.88</td>
</tr>
<tr id="S4.T2.3.7.6" class="ltx_tr">
<th id="S4.T2.3.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S4.T2.3.7.6.2" class="ltx_td ltx_align_center">25.88</td>
<td id="S4.T2.3.7.6.3" class="ltx_td ltx_align_center">85.48</td>
<td id="S4.T2.3.7.6.4" class="ltx_td ltx_align_center">55.68</td>
<td id="S4.T2.3.7.6.5" class="ltx_td ltx_align_center">75.34</td>
</tr>
<tr id="S4.T2.3.8.7" class="ltx_tr">
<th id="S4.T2.3.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.3.8.7.1.1" class="ltx_text">8</span></th>
<th id="S4.T2.3.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3</th>
<td id="S4.T2.3.8.7.3" class="ltx_td ltx_align_center ltx_border_t">20.07</td>
<td id="S4.T2.3.8.7.4" class="ltx_td ltx_align_center ltx_border_t">84.37</td>
<td id="S4.T2.3.8.7.5" class="ltx_td ltx_align_center ltx_border_t">52.22</td>
<td id="S4.T2.3.8.7.6" class="ltx_td ltx_align_center ltx_border_t">73.43</td>
</tr>
<tr id="S4.T2.3.9.8" class="ltx_tr">
<th id="S4.T2.3.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S4.T2.3.9.8.2" class="ltx_td ltx_align_center">25.04</td>
<td id="S4.T2.3.9.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.9.8.3.1" class="ltx_text ltx_font_bold">86.56</span></td>
<td id="S4.T2.3.9.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.9.8.4.1" class="ltx_text ltx_font_bold">55.80</span></td>
<td id="S4.T2.3.9.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.9.8.5.1" class="ltx_text ltx_font_bold">76.10</span></td>
</tr>
<tr id="S4.T2.3.10.9" class="ltx_tr">
<th id="S4.T2.3.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" rowspan="2"><span id="S4.T2.3.10.9.1.1" class="ltx_text">10</span></th>
<th id="S4.T2.3.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3</th>
<td id="S4.T2.3.10.9.3" class="ltx_td ltx_align_center ltx_border_t">20.82</td>
<td id="S4.T2.3.10.9.4" class="ltx_td ltx_align_center ltx_border_t">85.13</td>
<td id="S4.T2.3.10.9.5" class="ltx_td ltx_align_center ltx_border_t">52.97</td>
<td id="S4.T2.3.10.9.6" class="ltx_td ltx_align_center ltx_border_t">74.19</td>
</tr>
<tr id="S4.T2.3.11.10" class="ltx_tr">
<th id="S4.T2.3.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S4.T2.3.11.10.2" class="ltx_td ltx_align_center">25.19</td>
<td id="S4.T2.3.11.10.3" class="ltx_td ltx_align_center">85.95</td>
<td id="S4.T2.3.11.10.4" class="ltx_td ltx_align_center">55.57</td>
<td id="S4.T2.3.11.10.5" class="ltx_td ltx_align_center">75.61</td>
</tr>
<tr id="S4.T2.3.12.11" class="ltx_tr">
<th id="S4.T2.3.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" rowspan="2"><span id="S4.T2.3.12.11.1.1" class="ltx_text">12</span></th>
<th id="S4.T2.3.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3</th>
<td id="S4.T2.3.12.11.3" class="ltx_td ltx_align_center ltx_border_t">24.33</td>
<td id="S4.T2.3.12.11.4" class="ltx_td ltx_align_center ltx_border_t">85.47</td>
<td id="S4.T2.3.12.11.5" class="ltx_td ltx_align_center ltx_border_t">54.90</td>
<td id="S4.T2.3.12.11.6" class="ltx_td ltx_align_center ltx_border_t">75.07</td>
</tr>
<tr id="S4.T2.3.13.12" class="ltx_tr">
<th id="S4.T2.3.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">10</th>
<td id="S4.T2.3.13.12.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.13.12.2.1" class="ltx_text ltx_font_bold">26.26</span></td>
<td id="S4.T2.3.13.12.3" class="ltx_td ltx_align_center ltx_border_bb">85.34</td>
<td id="S4.T2.3.13.12.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.13.12.4.1" class="ltx_text ltx_font_bold">55.80</span></td>
<td id="S4.T2.3.13.12.5" class="ltx_td ltx_align_center ltx_border_bb">75.29</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.13" class="ltx_p"><a href="#S4.T2" title="In 4 Experimental Results â€£ Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> reports the accuracies for the three- and ten-band RSVQAxBEN dataset variants.
By analyzing the tables one can observe that the proposed architecture VBFusion outperforms the SkipRes architecture, irrespective of the layer configuration, in both summarizing metrics <span id="S4.p2.13.1" class="ltx_text ltx_inline-quote ltx_outerquote">â€œAAâ€</span> and <span id="S4.p2.13.2" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span>.
Even the smallest configuration with 4 layers trained on three bands improves the <span id="S4.p2.13.3" class="ltx_text ltx_inline-quote ltx_outerquote">â€œAAâ€</span> by almost <math id="S4.p2.1.m1.3" class="ltx_Math" alttext="2\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.1.m1.3a"><mrow id="S4.p2.1.m1.3.3" xref="S4.p2.1.m1.3.3.cmml"><mn id="S4.p2.1.m1.1.1.1.1.1.1" xref="S4.p2.1.m1.1.1.1.1.1.1.cmml">2</mn><mtext id="S4.p2.1.m1.2.2.2.2.2.2" xref="S4.p2.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.1.m1.3.3.3.3.3.3" xref="S4.p2.1.m1.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.3b"><apply id="S4.p2.1.m1.3.3.cmml" xref="S4.p2.1.m1.3.3"><csymbol cd="latexml" id="S4.p2.1.m1.2.2.2.2.2.2.cmml" xref="S4.p2.1.m1.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S4.p2.1.m1.1.1.1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1.1.1.1">2</cn><csymbol cd="latexml" id="S4.p2.1.m1.3.3.3.3.3.3.cmml" xref="S4.p2.1.m1.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.3c">2\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math> and the <span id="S4.p2.13.4" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span> by more than <math id="S4.p2.2.m2.3" class="ltx_Math" alttext="3\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.2.m2.3a"><mrow id="S4.p2.2.m2.3.3" xref="S4.p2.2.m2.3.3.cmml"><mn id="S4.p2.2.m2.1.1.1.1.1.1" xref="S4.p2.2.m2.1.1.1.1.1.1.cmml">3</mn><mtext id="S4.p2.2.m2.2.2.2.2.2.2" xref="S4.p2.2.m2.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.2.m2.3.3.3.3.3.3" xref="S4.p2.2.m2.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.3b"><apply id="S4.p2.2.m2.3.3.cmml" xref="S4.p2.2.m2.3.3"><csymbol cd="latexml" id="S4.p2.2.m2.2.2.2.2.2.2.cmml" xref="S4.p2.2.m2.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S4.p2.2.m2.1.1.1.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1.1.1.1">3</cn><csymbol cd="latexml" id="S4.p2.2.m2.3.3.3.3.3.3.cmml" xref="S4.p2.2.m2.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.3c">3\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>.
When we compare the performance of the proposed architecture trained on the three-band variant, most of the improvements are attributed to the <span id="S4.p2.13.5" class="ltx_text ltx_inline-quote ltx_outerquote">â€œYes/Noâ€</span> related questions.
We can also see that in the 3-band scenario, deeper and larger VBFusion models lead to higher <span id="S4.p2.13.6" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span> scores.
This pattern is in contrast to the generally similar performance in the <span id="S4.p2.13.7" class="ltx_text ltx_inline-quote ltx_outerquote">â€œLULCâ€</span> question category among the layer configurations up to <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="l=10" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">l</mi><mo id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><eq id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></eq><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">ğ‘™</ci><cn type="integer" id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">l=10</annotation></semantics></math> trained on three bands.
Interestingly, a performance jump can be observed for the <span id="S4.p2.13.8" class="ltx_text ltx_inline-quote ltx_outerquote">â€œLULCâ€</span> questions with the largest layer configuration (<math id="S4.p2.4.m4.1" class="ltx_Math" alttext="l=12" display="inline"><semantics id="S4.p2.4.m4.1a"><mrow id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mi id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">l</mi><mo id="S4.p2.4.m4.1.1.1" xref="S4.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><eq id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1"></eq><ci id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">ğ‘™</ci><cn type="integer" id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">l=12</annotation></semantics></math>).
A possible reason is that with 12 layers, all pre-trained VisualBERT layers are utilized and none are rejected.
Therefore, the initial weights can produce more meaningful high-level embeddings than the pruned models.
This seems to be especially important for the complex <span id="S4.p2.13.9" class="ltx_text ltx_inline-quote ltx_outerquote">â€œLULCâ€</span> questions, where the model is required to deeply understand the contents of intricate multispectral images and connect them to the associated question.
The performance improvement exists for the three- and ten-band variants, although it is more dominant in the three-band configuration.
With all 12 layers, the <span id="S4.p2.13.10" class="ltx_text ltx_inline-quote ltx_outerquote">â€œLULCâ€</span> accuracy increases by <math id="S4.p2.5.m5.3" class="ltx_Math" alttext="3.5\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.5.m5.3a"><mrow id="S4.p2.5.m5.3.3" xref="S4.p2.5.m5.3.3.cmml"><mn id="S4.p2.5.m5.1.1.1.1.1.1" xref="S4.p2.5.m5.1.1.1.1.1.1.cmml">3.5</mn><mtext id="S4.p2.5.m5.2.2.2.2.2.2" xref="S4.p2.5.m5.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.5.m5.3.3.3.3.3.3" xref="S4.p2.5.m5.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.3b"><apply id="S4.p2.5.m5.3.3.cmml" xref="S4.p2.5.m5.3.3"><csymbol cd="latexml" id="S4.p2.5.m5.2.2.2.2.2.2.cmml" xref="S4.p2.5.m5.2.2.2.2.2.2">times</csymbol><cn type="float" id="S4.p2.5.m5.1.1.1.1.1.1.cmml" xref="S4.p2.5.m5.1.1.1.1.1.1">3.5</cn><csymbol cd="latexml" id="S4.p2.5.m5.3.3.3.3.3.3.cmml" xref="S4.p2.5.m5.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.3c">3.5\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>, compared to the 10 layer configuration, for the three-band training leading to a high <span id="S4.p2.13.11" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span> of <math id="S4.p2.6.m6.3" class="ltx_Math" alttext="75.07\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.6.m6.3a"><mrow id="S4.p2.6.m6.3.3" xref="S4.p2.6.m6.3.3.cmml"><mn id="S4.p2.6.m6.1.1.1.1.1.1" xref="S4.p2.6.m6.1.1.1.1.1.1.cmml">75.07</mn><mtext id="S4.p2.6.m6.2.2.2.2.2.2" xref="S4.p2.6.m6.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.6.m6.3.3.3.3.3.3" xref="S4.p2.6.m6.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.3b"><apply id="S4.p2.6.m6.3.3.cmml" xref="S4.p2.6.m6.3.3"><csymbol cd="latexml" id="S4.p2.6.m6.2.2.2.2.2.2.cmml" xref="S4.p2.6.m6.2.2.2.2.2.2">times</csymbol><cn type="float" id="S4.p2.6.m6.1.1.1.1.1.1.cmml" xref="S4.p2.6.m6.1.1.1.1.1.1">75.07</cn><csymbol cd="latexml" id="S4.p2.6.m6.3.3.3.3.3.3.cmml" xref="S4.p2.6.m6.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.3c">75.07\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>.
The effect is not as significant for the 10-band configuration.
The relatively low impact on the 10-band configuration can be due to a discrepancy between the high-level RGB image feature representation and the ten-band feature representation from the initial weights of the pre-trained VisualBERT model, since VisualBERT was pre-trained on RGB images.
However, analyzing the results for all 10-band configurations, one can observe that these models improve the performance in all metrics compared to their three-band counterparts, except for the 12-layer configuration in the <span id="S4.p2.13.12" class="ltx_text ltx_inline-quote ltx_outerquote">â€œYes/Noâ€</span> category, where the accuracy is slightly lower by less than <math id="S4.p2.7.m7.3" class="ltx_Math" alttext="0.2\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.7.m7.3a"><mrow id="S4.p2.7.m7.3.3" xref="S4.p2.7.m7.3.3.cmml"><mn id="S4.p2.7.m7.1.1.1.1.1.1" xref="S4.p2.7.m7.1.1.1.1.1.1.cmml">0.2</mn><mtext id="S4.p2.7.m7.2.2.2.2.2.2" xref="S4.p2.7.m7.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.7.m7.3.3.3.3.3.3" xref="S4.p2.7.m7.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.3b"><apply id="S4.p2.7.m7.3.3.cmml" xref="S4.p2.7.m7.3.3"><csymbol cd="latexml" id="S4.p2.7.m7.2.2.2.2.2.2.cmml" xref="S4.p2.7.m7.2.2.2.2.2.2">times</csymbol><cn type="float" id="S4.p2.7.m7.1.1.1.1.1.1.cmml" xref="S4.p2.7.m7.1.1.1.1.1.1">0.2</cn><csymbol cd="latexml" id="S4.p2.7.m7.3.3.3.3.3.3.cmml" xref="S4.p2.7.m7.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.3c">0.2\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>.
Most of the observed performance increase is due to the notably higher accuracies in the <span id="S4.p2.13.13" class="ltx_text ltx_inline-quote ltx_outerquote">â€œLULCâ€</span> category.
For example, the smallest configuration trained on 10 bands with <math id="S4.p2.8.m8.1" class="ltx_Math" alttext="l=4" display="inline"><semantics id="S4.p2.8.m8.1a"><mrow id="S4.p2.8.m8.1.1" xref="S4.p2.8.m8.1.1.cmml"><mi id="S4.p2.8.m8.1.1.2" xref="S4.p2.8.m8.1.1.2.cmml">l</mi><mo id="S4.p2.8.m8.1.1.1" xref="S4.p2.8.m8.1.1.1.cmml">=</mo><mn id="S4.p2.8.m8.1.1.3" xref="S4.p2.8.m8.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.8.m8.1b"><apply id="S4.p2.8.m8.1.1.cmml" xref="S4.p2.8.m8.1.1"><eq id="S4.p2.8.m8.1.1.1.cmml" xref="S4.p2.8.m8.1.1.1"></eq><ci id="S4.p2.8.m8.1.1.2.cmml" xref="S4.p2.8.m8.1.1.2">ğ‘™</ci><cn type="integer" id="S4.p2.8.m8.1.1.3.cmml" xref="S4.p2.8.m8.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.8.m8.1c">l=4</annotation></semantics></math> is more than <math id="S4.p2.9.m9.3" class="ltx_Math" alttext="5\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.9.m9.3a"><mrow id="S4.p2.9.m9.3.3" xref="S4.p2.9.m9.3.3.cmml"><mn id="S4.p2.9.m9.1.1.1.1.1.1" xref="S4.p2.9.m9.1.1.1.1.1.1.cmml">5</mn><mtext id="S4.p2.9.m9.2.2.2.2.2.2" xref="S4.p2.9.m9.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.9.m9.3.3.3.3.3.3" xref="S4.p2.9.m9.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.9.m9.3b"><apply id="S4.p2.9.m9.3.3.cmml" xref="S4.p2.9.m9.3.3"><csymbol cd="latexml" id="S4.p2.9.m9.2.2.2.2.2.2.cmml" xref="S4.p2.9.m9.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S4.p2.9.m9.1.1.1.1.1.1.cmml" xref="S4.p2.9.m9.1.1.1.1.1.1">5</cn><csymbol cd="latexml" id="S4.p2.9.m9.3.3.3.3.3.3.cmml" xref="S4.p2.9.m9.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.9.m9.3c">5\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math> better than the same architecture trained on the 3 band variant in the <span id="S4.p2.13.14" class="ltx_text ltx_inline-quote ltx_outerquote">â€œLULCâ€</span> category.
An improved <span id="S4.p2.13.15" class="ltx_text ltx_inline-quote ltx_outerquote">â€œLULCâ€</span> performance is expected, since some LULC classes, such as different types of vegetation, greatly benefit from additional spectral information.
The smallest model trained on ten bands even outperforms the largest and best-performing configuration trained on three bands by <math id="S4.p2.10.m10.3" class="ltx_Math" alttext="0.6\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.10.m10.3a"><mrow id="S4.p2.10.m10.3.3" xref="S4.p2.10.m10.3.3.cmml"><mn id="S4.p2.10.m10.1.1.1.1.1.1" xref="S4.p2.10.m10.1.1.1.1.1.1.cmml">0.6</mn><mtext id="S4.p2.10.m10.2.2.2.2.2.2" xref="S4.p2.10.m10.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.10.m10.3.3.3.3.3.3" xref="S4.p2.10.m10.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.10.m10.3b"><apply id="S4.p2.10.m10.3.3.cmml" xref="S4.p2.10.m10.3.3"><csymbol cd="latexml" id="S4.p2.10.m10.2.2.2.2.2.2.cmml" xref="S4.p2.10.m10.2.2.2.2.2.2">times</csymbol><cn type="float" id="S4.p2.10.m10.1.1.1.1.1.1.cmml" xref="S4.p2.10.m10.1.1.1.1.1.1">0.6</cn><csymbol cd="latexml" id="S4.p2.10.m10.3.3.3.3.3.3.cmml" xref="S4.p2.10.m10.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.10.m10.3c">0.6\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math> in <span id="S4.p2.13.16" class="ltx_text ltx_inline-quote ltx_outerquote">â€œAAâ€</span> and almost by <math id="S4.p2.11.m11.3" class="ltx_Math" alttext="0.2\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.11.m11.3a"><mrow id="S4.p2.11.m11.3.3" xref="S4.p2.11.m11.3.3.cmml"><mn id="S4.p2.11.m11.1.1.1.1.1.1" xref="S4.p2.11.m11.1.1.1.1.1.1.cmml">0.2</mn><mtext id="S4.p2.11.m11.2.2.2.2.2.2" xref="S4.p2.11.m11.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.11.m11.3.3.3.3.3.3" xref="S4.p2.11.m11.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.11.m11.3b"><apply id="S4.p2.11.m11.3.3.cmml" xref="S4.p2.11.m11.3.3"><csymbol cd="latexml" id="S4.p2.11.m11.2.2.2.2.2.2.cmml" xref="S4.p2.11.m11.2.2.2.2.2.2">times</csymbol><cn type="float" id="S4.p2.11.m11.1.1.1.1.1.1.cmml" xref="S4.p2.11.m11.1.1.1.1.1.1">0.2</cn><csymbol cd="latexml" id="S4.p2.11.m11.3.3.3.3.3.3.cmml" xref="S4.p2.11.m11.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.11.m11.3c">0.2\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math> in <span id="S4.p2.13.17" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span>.
When training with ten bands, it can be observed that larger configurations of our proposed VBFusion architecture do not necessarily lead to higher accuracies.
The best performing ten-band trained configuration utilizes 8 layers and reaches an <span id="S4.p2.13.18" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span> of <math id="S4.p2.12.m12.3" class="ltx_Math" alttext="76.10\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.12.m12.3a"><mrow id="S4.p2.12.m12.3.3" xref="S4.p2.12.m12.3.3.cmml"><mn id="S4.p2.12.m12.1.1.1.1.1.1" xref="S4.p2.12.m12.1.1.1.1.1.1.cmml">76.10</mn><mtext id="S4.p2.12.m12.2.2.2.2.2.2" xref="S4.p2.12.m12.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.12.m12.3.3.3.3.3.3" xref="S4.p2.12.m12.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.12.m12.3b"><apply id="S4.p2.12.m12.3.3.cmml" xref="S4.p2.12.m12.3.3"><csymbol cd="latexml" id="S4.p2.12.m12.2.2.2.2.2.2.cmml" xref="S4.p2.12.m12.2.2.2.2.2.2">times</csymbol><cn type="float" id="S4.p2.12.m12.1.1.1.1.1.1.cmml" xref="S4.p2.12.m12.1.1.1.1.1.1">76.10</cn><csymbol cd="latexml" id="S4.p2.12.m12.3.3.3.3.3.3.cmml" xref="S4.p2.12.m12.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.12.m12.3c">76.10\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>, while the full 12 layer configuration has the second lowest <span id="S4.p2.13.19" class="ltx_text ltx_inline-quote ltx_outerquote">â€œOAâ€</span> with <math id="S4.p2.13.m13.3" class="ltx_Math" alttext="75.29\text{\,}\mathrm{\char 37\relax}" display="inline"><semantics id="S4.p2.13.m13.3a"><mrow id="S4.p2.13.m13.3.3" xref="S4.p2.13.m13.3.3.cmml"><mn id="S4.p2.13.m13.1.1.1.1.1.1" xref="S4.p2.13.m13.1.1.1.1.1.1.cmml">75.29</mn><mtext id="S4.p2.13.m13.2.2.2.2.2.2" xref="S4.p2.13.m13.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" mathvariant="normal" id="S4.p2.13.m13.3.3.3.3.3.3" xref="S4.p2.13.m13.3.3.3.3.3.3.cmml">%</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.13.m13.3b"><apply id="S4.p2.13.m13.3.3.cmml" xref="S4.p2.13.m13.3.3"><csymbol cd="latexml" id="S4.p2.13.m13.2.2.2.2.2.2.cmml" xref="S4.p2.13.m13.2.2.2.2.2.2">times</csymbol><cn type="float" id="S4.p2.13.m13.1.1.1.1.1.1.cmml" xref="S4.p2.13.m13.1.1.1.1.1.1">75.29</cn><csymbol cd="latexml" id="S4.p2.13.m13.3.3.3.3.3.3.cmml" xref="S4.p2.13.m13.3.3.3.3.3.3">percent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.13.m13.3c">75.29\text{\,}\mathrm{\char 37\relax}</annotation></semantics></math>.
By analyzing the results, one can conclude that the proposed VBFusion architecture discovers the underlying relationship between both the image and question modality better than models that utilize a simple feature combination as the fusion module.
Furthermore, the results show the importance of utilizing additional spectral bands, when available, to better model the contents of intricate multispectral imagery for VQA systems.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>CONCLUSION</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we have presented a novel architecture in RS VQA that applies a transformer model, which exclusively relies on multi-modal transformer layers to learn the image and text representations jointly.
Our proposed architecture VBFusion includes: i) a feature extraction module based on the BoxExtractor, ResNet152, and the BertTokenizer; ii) a fusion module based on a user-defined number of multi-modal transformer layers of VisualBERT; and iii) a classification module consisting of an <span title="" class="ltx_glossaryref">MLP</span> projection head.
It is worth noting that our architecture is not limited to specific questions, e.g., questions concerning pre-defined objects.
To show the effectiveness of the proposed architecture, we have evaluated the model on the RSVQA-LR dataset, the RSVQAxBEN dataset (which only includes the RGB bands of the Sentinel-2 image patches), and an extended RSVQAxBEN dataset (which includes all the spectral bands of Sentinel-2 images with 10m and 20m spatial resolution).
From the experimental results obtained on the RSVQAxBEN dataset variants, we observe that: i) our architecture leads to significant performance improvements compared to an architecture that simply combines the modality-specific representations in the fusion module as our architecture better discovers the underlying relationship between the modalities;
and ii) exploitation of additional available spectral bands leads to better modeling of the complex spatial and spectral content of RS images in the context of VQA.
Our architecture results in a performance increase for the comparably small RSVQA-LR dataset compared to the SkipRes architecture, which utilizes a simple feature combination as a fusion module. Compared to the relatively large complexity of our models, the slight improvement aligns with the literature findings that large transformers struggle on small datasets. In addition, larger layer configurations do not provide a significant improvement in the RSVQA-LR dataset. These results show that our multi-modal transformer-based fusion module requires larger training sets to realize its full
potential.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We would like to note that although transformer-based models have a potential to provide high VQA performance in RS, they are associated to a high number of model parameters and high training complexity. Thus, as a future work, we plan to investigate efficient transformers and conduct a comparative study in terms of complexity and performance. Furthermore, we plan to extend our feature extraction module to optimize the BoxExtractor or exchange it with a more sophisticated general box extractor.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">ACKNOWLEDGEMENT</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is funded by the European Research Council (ERC) through the ERC-2017-STG BigEarth Project under Grant 759764 and by the German Ministry for Economic Affairs and Climate Action through the AI-Cube Project under Grant 50EE2012B.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Lobry, S., Murray, J., Marcos, D., and Tuia, D., â€œVisual question answering
from remote sensing images,â€ IEEE International Geoscience and Remote
Sensing SymposiumÂ 60, 4951â€“4954 (2019).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.Â L., and
Parikh, D., â€œVQA: Visual Question Answering,â€ International
Conference on Computer Vision (2015).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Felix, R., Repasky, B., Hodge, S., Zolfaghari, R., Abbasnejad, E., and Sherrah,
J., â€œCross-modal visual question answering for remote sensing data,â€
International Conference on Digital Image Computing Techniques and
Applications , 1â€“9 (2021).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Zheng, X., Wang, B., Du, X., and Lu, X., â€œMutual attention inception network
for remote sensing visual question answering,â€ IEEE Transactions on
Geoscience and Remote SensingÂ 60, 1â€“14 (2022).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Lobry, S., Marcos, D., Kellenberger, B., and Tuia, D., â€œBetter generic objects
counting when asking questions to images: A multitask approach for remote
sensing visual question answering,â€ ISPRS Annals of the Photogrammetry,
Remote Sensing and Spatial Information SciencesÂ V-2-2020, 1021â€“1027 (2020).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chappuis, C., Lobry, S., Kellenberger, B., Saux, B.Â L., and Tuia, D., â€œHow to
find a good image-text embedding for remote sensing visual question
answering?,â€ CoRRÂ abs/2109.11848 (2021).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chappuis, C., Zermatten, V., Lobry, S., LeÂ Saux, B., and Tuia, D.,
â€œPrompt-RSVQA: Prompting visual context to a language model for remote
sensing visual question answering,â€ Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops , 1372â€“1381 (2022).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Kiros, R., Zhu, Y., Salakhutdinov, R.Â R., Zemel, R., Urtasun, R., Torralba, A.,
and Fidler, S., â€œSkip-Thought vectors,â€ Advances in Neural Information
Processing SystemsÂ 28 (2015).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Simonyan, K. and Zisserman, A., â€œVery deep convolutional networks for
large-scale image recognition,â€ 3rd International Conference on Learning
Representations (2015).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cho, K., van Merrienboer, B., Bahdanau, D., and Bengio, Y., â€œOn the properties
of neural machine translation: Encoder-decoder approaches,â€
CoRRÂ abs/1409.1259 (2014).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Li, L.Â H., Yatskar, M., Yin, D., Hsieh, C., and Chang, K., â€œVisualBERT: A
simple and performant baseline for vision and language,â€ CoRRÂ abs/1908.03557
(2019).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Lam, D., Kuzma, R., McGee, K., Dooley, S., Laielli, M., Klaric, M., Bulatov,
Y., and McCord, B., â€œxView: Objects in context in overhead imagery,â€
CoRRÂ abs/1802.07856 (2018).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Lobry, S., Demir, B., and Tuia, D., â€œRSVQA meets BigEarthNet: A new,
large-scale, visual question answering dataset for remote sensing,â€ IEEE
International Geoscience and Remote Sensing Symposium , 1218â€“1221 (2021).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Sanh, V., Debut, L., Chaumond, J., and Wolf, T., â€œDistilBERT, a distilled
version of BERT: smaller, faster, cheaper and lighter,â€
CoRRÂ abs/1910.01108 (2019).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., and Sun, J., â€œDeep residual learning for image
recognition,â€ CoRRÂ abs/1512.03385 (2015).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M., Lee, K., and Toutanova, K., â€œBERT: pre-training of
deep bidirectional transformers for language understanding,â€
CoRRÂ abs/1810.04805 (2018).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., and
Zhang, L., â€œBottom-up and top-down attention for image captioning and
VQA,â€ CoRRÂ abs/1707.07998 (2017).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R.Â B., and Sun, J., â€œFaster R-CNN: towards
real-time object detection with region proposal networks,â€
CoRRÂ abs/1506.01497 (2015).

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Sumbul, G., Charfuelan, M., Demir, B., and Markl, V., â€œBigEarthNet: A
large-scale benchmark archive for remote sensing image understanding,â€ IEEE
International Geoscience and Remote Sensing Symposium , 5901â€“5904 (2019).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.,
â€œImageNet: A large-scale hierarchical image database,â€ IEEE Conference on
Computer Vision and Pattern Recognition , 248â€“255 (2009).

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.Â L., Gugger, S., Drame, M.,
Lhoest, Q., and Rush, A.Â M., â€œTransformers: State-of-the-art natural
language processing,â€ Proceedings of the Conference on Empirical Methods in
Natural Language Processing: System Demonstrations , 38â€“45 (2020).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
Uszkoreit, J., and Houlsby, N., â€œAn image is worth 16x16 words: Transformers
for image recognition at scale,â€ CoRRÂ abs/2010.11929 (2020).

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2210.04509" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2210.04510" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2210.04510">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2210.04510" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2210.04511" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 00:43:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
