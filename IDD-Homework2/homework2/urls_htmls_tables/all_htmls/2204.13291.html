<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2204.13291] Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems</title><meta property="og:description" content="Federated machine learning is growing fast in academia and industries as a solution to solve data hungriness and privacy issues in machine learning. Being a widely distributed system, federated machine learning require…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2204.13291">

<!--Generated on Wed Feb 28 06:15:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Software architecture,  federated machine learning,  patterns,  decision models,  artificial intelligence (AI)">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sin Kit Lo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:Kit.Lo@data61.csiro.au">Kit.Lo@data61.csiro.au</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">CSIRO Data61 and University of New South Wales</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_country">Australia</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qinghua Lu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:qinghua.lu@data61.csiro.au">qinghua.lu@data61.csiro.au</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">CSIRO Data61</span><span id="id4.2.id2" class="ltx_text ltx_affiliation_country">Australia</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hye-young Paik
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:h.paik@unsw.edu.au">h.paik@unsw.edu.au</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">University of New South Wales</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country">Australia</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liming Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:Liming.Zhu@data61.csiro.au">Liming.Zhu@data61.csiro.au</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">CSIRO Data61</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country">Australia</span>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id9.id1" class="ltx_p">Federated machine learning is growing fast in academia and industries as a solution to solve data hungriness and privacy issues in machine learning. Being a widely distributed system, federated machine learning requires various system design thinking. To better design a federated machine learning system, researchers have introduced multiple patterns and tactics that cover various system design aspects. However, the multitude of patterns leaves the designers confused about when and which pattern to adopt. In this paper, we present a set of decision models for the selection of patterns for federated machine learning architecture design based on a systematic literature review on federated machine learning, to assist designers and architects who have limited knowledge of federated machine learning. Each decision model maps functional and non-functional requirements of federated machine learning systems to a set of patterns. We also clarify the drawbacks of the patterns. We evaluated the decision models by mapping the decision patterns to concrete federated machine learning architectures by big tech firms to assess the models’ correctness and usefulness. The evaluation results indicate that the proposed decision models are able to bring structure to the federated machine learning architecture design process and help explicitly articulate the design rationale.</p>
</div>
<div class="ltx_keywords">Software architecture, federated machine learning, patterns, decision models, artificial intelligence (AI)
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The growth of the idea of industry 4.0 and cloud computing resulted in the exponential increase in data dimensions and the availability of data to generate useful insights <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. The overall increase in data and computation capability of computers accelerated the adoption of machine learning for data analysis in multiple areas. However, many machine learning systems suffer from insufficient training data due to data privacy concerns. Data privacy as an important ethical principle of machine learning systems <cite class="ltx_cite ltx_citemacro_citep">(Jobin et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> induced the regularisation of access to privacy-sensitive data. Furthermore, trustworthy AI has become an emerging topic lately due to the new ethical, legal, social, and technological challenges brought on by the technology <cite class="ltx_cite ltx_citemacro_citep">(Thiebes et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Google introduced federated machine learning as a new concept for distributed machine learning settings in 2017 <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>. The settings utilize a central server to orchestrate machine learning model training on widely distributed devices using their locally collected data, without central collection and preprocessing of training data. Hence, federated machine learning is able to solve data-sharing, privacy, and resource-sharing restriction challenges. However, a federated machine learning system presents more architectural design challenges <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>, especially when dealing with the interactions between the central server and client devices to manage the drawbacks amongst the software quality attributes. For instance, a federated machine learning system faces architecture challenges such as the need to consider how to actively manage multiple client devices while preventing malicious participants, or how to resolve the statistical and system heterogeneity across the client devices to maintain the model training performance <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>. Various federated machine learning software architectural challenges and propose approaches to tackle the challenges were articulated and presented in our systematic literature review (SLR) <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>)</cite>. We have also summarised a set of software architectural patterns <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>)</cite> to address the different requirements from different research articles and industrial practices. Despite having various patterns and solutions, architects may find it difficult to choose when and how to use them. Hence, we aim to structure the patterns and solutions to assist architects in selecting appropriate patterns during the federated machine learning system design through a series of pattern selection decision models. The goal is to provide guidance for federated machine learning architecture design decisions that meet the intended requirements while taking drawbacks and constraints into consideration.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The remainder of the paper is organised as follows: Section <a href="#S2" title="2. Federated Machine Learning: Next-word Prediction ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives an explanation of federated machine learning. Section <a href="#S3" title="3. Decision Models ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the notation and overview of the proposed decision models and elaborates on the 4 decision models for different aspects of architecture design. The evaluation of the decision models is presented in Section <a href="#S4" title="4. Architecture Design Validation ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Section <a href="#S5" title="5. Related work ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> covers the related work on decision models, machine learning, and federated machine learning patterns. Finally, Section <a href="#S6" title="6. Conclusion and Future Works ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Federated Machine Learning: Next-word Prediction</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2204.13291/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Federated machine learning overview <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>)</cite>
</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Fig. <a href="#S2.F1" title="Figure 1 ‣ 2. Federated Machine Learning: Next-word Prediction ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the overview of federated machine learning, under a next-word prediction example in a mobile phone keyboard application. There are two types of system nodes: (1) central server, and (2) client device. Firstly, the learning coordinator (central server) of a federated machine learning system initiates the keyboard application. The contributor clients (client devices) are mobile phone users. The federated machine learning process begins with the creation of a training task (includes training hyperparameters, epochs, aggregation rounds, deployment strategy, etc.), usually by the central server. For instance, the keyboard application is embedded with an initial global model (including scripts &amp; hyperparameters), which is then sent to the participating client devices.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">On the client side, the global model is received and the training is performed locally across the client devices using the raw client data. In each training round, the client device will perform one for several training epochs to optimise the local model. In this case, the smartphones optimise the model using the keyboard typing data. After that, the updated local model is submitted by each participating client device to the central server to perform model aggregation to form a new version of the global model. The new global model is re-distributed to the client devices for the next aggregation round. The entire process repeats until the global model converges. After the completion of training, the central server deploys the converged global model to the client devices. In this example, the keyboard application provider applies the converged model in the latest version of the application for existing or new application users to perform the next word predictions.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">As the entire process repeats, communication and computation costs will be high. For instance, communication bandwidth is highly consumed by multiple client devices when they communicate with the central server and it increases with the scale of the number of devices connected. Furthermore, client devices may run on different operating systems and have diverse communication and computation resources, which trigger the <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">system heterogeneity</span> challenges. One example is the difference in operating systems and computation resources trigger interoperability and model consistency issues. Similarly, the <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">statistical heterogeneity</span> issue also exists, caused by the difference in data distributions across all client devices. The system is also troubled by <span id="S2.p3.1.3" class="ltx_text ltx_font_italic">system reliability</span> issue with the possibility of adversarial nodes participating in the training process, poisoning the system and the model quality and the central server is being exposed as the <span id="S2.p3.1.4" class="ltx_text ltx_font_italic">single-point-of-failure</span>. Federated machine learning generates multiple versions of the local and global models created that need to be managed. However, the <span id="S2.p3.1.5" class="ltx_text ltx_font_italic">traceability</span> of the system is challenging as model provenance for all the local and global models is difficult as the system scale up. For example, local models are trained by the large scale of privately owned devices, using data that are unseen and not processed by the central server. It is challenging to know which version of local devices are trained with which version of local data has been aggregated into which version of a global model. Finally, due to the limited resources available on each device, the motivation of the client to join the federated machine learning process becomes weak, which induces the <span id="S2.p3.1.6" class="ltx_text ltx_font_italic">client motivatability</span> challenge. Client device owners might not want to use smartphones to train local models without reward as the process consumes battery life, communication bandwidth, and computation resources.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2204.13291/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="276" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Federated machine learning Domain Model</figcaption>
</figure>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Fig. <a href="#S2.F2" title="Figure 2 ‣ 2. Federated Machine Learning: Next-word Prediction ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents a domain model for federated machine learning systems. The dashed line specifically indicates that the association between <span id="S2.p4.1.1" class="ltx_text ltx_font_bold">‘FederatedLearning’</span> and the other classes is a “use” relationship. This means that <span id="S2.p4.1.2" class="ltx_text ltx_font_bold">‘FederatedLearning’</span> applies instances of the other classes to carry out its functionality. In this diagram, <span id="S2.p4.1.3" class="ltx_text ltx_font_bold">‘FederatedLearning’</span> represents the main class for the federated machine learning system. For example, <span id="S2.p4.1.4" class="ltx_text ltx_font_bold">‘FederatedLearning’</span> uses an instance of <span id="S2.p4.1.5" class="ltx_text ltx_font_bold">‘BERTModel’</span> to train the BERT (Bidirectional Encoder Representations from Transformers) <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> model used in the federated machine learning system. Similarly, <span id="S2.p4.1.6" class="ltx_text ltx_font_bold">‘FederatedLearning’</span> uses an instance of <span id="S2.p4.1.7" class="ltx_text ltx_font_bold">‘TCP/IP’</span> to handle the communication between the devices and the server in the federated machine learning system. It contains attributes and methods for various components of the system, such as the model, data source, communication protocol, aggregation strategy, training algorithm, and client selection strategy. <span id="S2.p4.1.8" class="ltx_text ltx_font_bold">‘BERTModel’</span> represents the machine learning model used in the federated machine learning system. It has attributes for the weights of the model and methods for training and evaluation. The <span id="S2.p4.1.9" class="ltx_text ltx_font_bold">‘DataSource’</span> represents the source of data for the federated machine learning system. It has a method for getting the data for a specific client. <span id="S2.p4.1.10" class="ltx_text ltx_font_bold">‘TCP/IP’</span> represents the method of communication between the clients and the server in the federated machine learning system. It has a method for sending messages between clients and the server. The <span id="S2.p4.1.11" class="ltx_text ltx_font_bold">‘FederatedAveraging’</span> represents the method for aggregating the weights of the models from the clients in the federated machine learning system while the <span id="S2.p4.1.12" class="ltx_text ltx_font_bold">‘TrainingAlgorithm’</span> represents the algorithm used for training the machine learning model in the federated machine learning system. The <span id="S2.p4.1.13" class="ltx_text ltx_font_bold">‘DeviceSelectionStrategy’</span> represents the method for selecting the clients to participate in the federated machine learning system. It has a method for selecting clients. Lastly, <span id="S2.p4.1.14" class="ltx_text ltx_font_bold">‘TextData’</span> represents the data used for training and evaluation in the federated machine learning system.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Decision Models</h2>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2204.13291/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="276" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Decision model notations <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite></figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>Overview of architectural patterns for federated machine learning <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>)</cite></figcaption>
<table id="S3.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.6.1.1" class="ltx_tr">
<th id="S3.T1.6.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.6.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.1.1.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.T1.6.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Category</span></span>
</span>
</th>
<th id="S3.T1.6.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.6.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.1.1.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Name</span></span>
</span>
</th>
<th id="S3.T1.6.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.6.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.1.1.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Summary</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.6.2.1" class="ltx_tr">
<td id="S3.T1.6.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.2.1.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.T1.6.2.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Federation management patterns</span></span>
</span>
</td>
<td id="S3.T1.6.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.2.1.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Client registry</span></span>
</span>
</td>
<td id="S3.T1.6.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.2.1.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.2.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Exposes addresses and training capabilities of machine learning devices to the central server and maintains the information of all the participating client devices for federation management.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.3.2" class="ltx_tr">
<td id="S3.T1.6.3.2.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.3.2.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">Client selector</span></span>
</span>
</td>
<td id="S3.T1.6.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.3.2.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.3.2.3.1.1.1" class="ltx_text" style="font-size:80%;">Actively selects the client devices for a certain round of training according to the predefined criteria to increase model quality and computation efficiency.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.4.3" class="ltx_tr">
<td id="S3.T1.6.4.3.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.4.3.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.4.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Client cluster</span></span>
</span>
</td>
<td id="S3.T1.6.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.4.3.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.4.3.3.1.1.1" class="ltx_text" style="font-size:80%;">Groups the client devices (i.e., model trainers) based on their similarity of certain characteristics (e.g., available resources, data distribution, features, geolocation) to increase the model quality and training efficiency.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.5.4" class="ltx_tr">
<td id="S3.T1.6.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.5.4.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.T1.6.5.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model management &amp; configuration patterns</span></span>
</span>
</td>
<td id="S3.T1.6.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.5.4.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.5.4.2.1.1.1" class="ltx_text" style="font-size:80%;">Message compressor</span></span>
</span>
</td>
<td id="S3.T1.6.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.5.4.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.5.4.3.1.1.1" class="ltx_text" style="font-size:80%;">Reduces message (global and local models) data size through different ways (compression, pruning, etc.) before every round of model exchange to increase communication efficiency.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.6.5" class="ltx_tr">
<td id="S3.T1.6.6.5.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.6.5.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.6.5.2.1.1.1" class="ltx_text" style="font-size:80%;">Model co-versioning registry</span></span>
</span>
</td>
<td id="S3.T1.6.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.6.5.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.6.5.3.1.1.1" class="ltx_text" style="font-size:80%;">Stores and aligns the local models from each client with the corresponding global model versions for model provenance and model quality tracking.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.7.6" class="ltx_tr">
<td id="S3.T1.6.7.6.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.7.6.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.7.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Model replacement trigger</span></span>
</span>
</td>
<td id="S3.T1.6.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.7.6.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.7.6.3.1.1.1" class="ltx_text" style="font-size:80%;">Actively monitors the model performance and detects when the degradation in model performance occurs. Replaces degraded models when the degradation persists.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.8.7" class="ltx_tr">
<td id="S3.T1.6.8.7.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.8.7.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.8.7.2.1.1.1" class="ltx_text" style="font-size:80%;">Deployment selector</span></span>
</span>
</td>
<td id="S3.T1.6.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.8.7.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.8.7.3.1.1.1" class="ltx_text" style="font-size:80%;">Selects and matches the converged global models to suitable client devices to maximise the global model quality for different applications and tasks.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.9.8" class="ltx_tr">
<td id="S3.T1.6.9.8.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.9.8.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.9.8.2.1.1.1" class="ltx_text" style="font-size:80%;">Training configurator</span></span>
</span>
</td>
<td id="S3.T1.6.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.9.8.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.9.8.3.1.1.1" class="ltx_text" style="font-size:80%;">Enables users to request, configure and deploy FML training processes and models without the need to code or program, using a user-friendly platform.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.10.9" class="ltx_tr">
<td id="S3.T1.6.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.10.9.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.T1.6.10.9.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model training patterns</span></span>
</span>
</td>
<td id="S3.T1.6.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.10.9.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.10.9.2.1.1.1" class="ltx_text" style="font-size:80%;">Multi-task model trainer</span></span>
</span>
</td>
<td id="S3.T1.6.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.10.9.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.10.9.3.1.1.1" class="ltx_text" style="font-size:80%;">Utilises data from separate but related models on local client devices to improve training efficiency and model quality.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.11.10" class="ltx_tr">
<td id="S3.T1.6.11.10.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.11.10.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.11.10.2.1.1.1" class="ltx_text" style="font-size:80%;">Heterogeneous data handler</span></span>
</span>
</td>
<td id="S3.T1.6.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.11.10.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.11.10.3.1.1.1" class="ltx_text" style="font-size:80%;">Solves the non-IID and skewed data distribution issues through data volume and data class addition while maintaining the local data privacy.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.12.11" class="ltx_tr">
<td id="S3.T1.6.12.11.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.12.11.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.12.11.2.1.1.1" class="ltx_text" style="font-size:80%;">Incentive registry</span></span>
</span>
</td>
<td id="S3.T1.6.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.12.11.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.12.11.3.1.1.1" class="ltx_text" style="font-size:80%;">Measures and records the performance and contributions of each client and provides incentives (e.g., cryptocurrencies) to motivate clients’ participation.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.13.12" class="ltx_tr">
<td id="S3.T1.6.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.13.12.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S3.T1.6.13.12.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model aggregation patterns</span></span>
</span>
</td>
<td id="S3.T1.6.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.13.12.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.13.12.2.1.1.1" class="ltx_text" style="font-size:80%;">Asynchronous aggregator</span></span>
</span>
</td>
<td id="S3.T1.6.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.13.12.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.13.12.3.1.1.1" class="ltx_text" style="font-size:80%;">Performs aggregation asynchronously whenever a model update arrives without waiting for all the model updates every round to reduce aggregation latency.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.14.13" class="ltx_tr">
<td id="S3.T1.6.14.13.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.14.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.14.13.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.14.13.2.1.1.1" class="ltx_text" style="font-size:80%;">Decentralised aggregator</span></span>
</span>
</td>
<td id="S3.T1.6.14.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.14.13.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.14.13.3.1.1.1" class="ltx_text" style="font-size:80%;">Removes the central server from the system and decentralizes its role to prevent single-point-of-failure and increase reliability.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.15.14" class="ltx_tr">
<td id="S3.T1.6.15.14.1" class="ltx_td ltx_align_top"></td>
<td id="S3.T1.6.15.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.15.14.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.15.14.2.1.1.1" class="ltx_text" style="font-size:80%;">Hierarchical aggregator</span></span>
</span>
</td>
<td id="S3.T1.6.15.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.6.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.15.14.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.15.14.3.1.1.1" class="ltx_text" style="font-size:80%;">Adds an edge layer to perform partial aggregation of local models from closely-related client devices to improve model quality and computation efficiency.</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.6.16.15" class="ltx_tr">
<td id="S3.T1.6.16.15.1" class="ltx_td ltx_align_top ltx_border_bb"></td>
<td id="S3.T1.6.16.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.6.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.16.15.2.1.1" class="ltx_p" style="width:130.1pt;"><span id="S3.T1.6.16.15.2.1.1.1" class="ltx_text" style="font-size:80%;">Secure aggregator</span></span>
</span>
</td>
<td id="S3.T1.6.16.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.6.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.16.15.3.1.1" class="ltx_p" style="width:563.7pt;"><span id="S3.T1.6.16.15.3.1.1.1" class="ltx_text" style="font-size:80%;">Adopts secure multiparty computation protocols that manage the model exchange and aggregation security to protect model security.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To design a decision model that elicits the functional and non-functional requirements with the respective patterns, we map the elements of the problem space to the elements of the solution space. The problem space can be presented as a set of functional (FR) or non-functional requirements (NFR), whereas the solution space is a set of patterns targeting to solve the problems. We have adopted the decision model design methodology from <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021a</a>)</cite> and adopted the notation method from <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> that involves the mapping of requirements and patterns, as shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3. Decision Models ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. A single-headed arrow from the pattern to the requirement indicates that the pattern satisfies the requirement. All pattern decisions will have benefits which are indicated by a plus sign (+) and drawbacks which are indicated by a minus sign (-).</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To present patterns combination, a double-headed arrow is used to point from one pattern to another pattern, with the label <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">[complements]</span> for one pattern complementing another, and label <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">[alternatives]</span> for one pattern being an alternative to another pattern. When a pattern complements another pattern it means that the initial pattern is required and the qualities of using the initial pattern also apply to the combination of the patterns. If a system quality is associated with both the initial and the complementary pattern but with different qualifications, the qualification of the complementary pattern overrides the qualification of the initial pattern. A trapezium with a dashed line connected to the respective pattern indicates the conditions or constraints to the adoption of that pattern.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2204.13291/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="128" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>High-level Design Decision Models</figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">We created the elements of the problem and solution spaces based on the categories in the architectural pattern collections that we presented in <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>)</cite>, where the summary of the patterns is displayed in Table <a href="#S3.T1" title="Table 1 ‣ 3. Decision Models ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We have categorised 4 main high-level design decisions of a federated machine learning system and compiled a high-level decision model for federated machine learning system design. As shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3. Decision Models ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>: (i) Federation management patterns decisions, (2) model management and configuration patterns decisions, (3) model aggregation patterns decisions, and (4) model training patterns decisions. For each lower-level decision within the high-level decisions, the designers need to consider how each quality is positively or negatively affected by another.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2204.13291/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="229" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Federation Management Patterns Decision Model</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Federation Management Decision Model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The federation management decision model covers the design decisions that handle device information and the connection between client devices under the federation with the central server, and the selection of client devices for the training process, as shown in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3. Decision Models ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span><span id="S3.SS1.SSS1.1.1" class="ltx_text ltx_font_bold">Client cluster</span>
</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The client cluster pattern targets to fulfill the non-functional requirement on <span id="S3.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">training efficiency</span> by grouping the client devices with similar characteristics and local models of the same group will be aggregated. In contrast, the extra computational cost is required to access and group the client devices which may reduce the <span id="S3.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">computation efficiency</span> of the system. One example is the Iterative Federated Clustering Algorithm (<span id="S3.SS1.SSS1.p1.1.3" class="ltx_text ltx_font_italic">IFCA</span>)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/jichan3751/ifca</span></span></span>. It is a framework introduced by UC Berkley and Google to cluster client devices based on the loss values of the client’s gradient.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span><span id="S3.SS1.SSS2.1.1" class="ltx_text ltx_font_bold">Client registry</span>
</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">client registry</span> pattern could be adopted to enhance the system’s <span id="S3.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">maintainability</span> and <span id="S3.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_italic">reliability</span>. The <span id="S3.SS1.SSS2.p1.1.4" class="ltx_text ltx_font_bold">client registry</span> records the information (smartphones ID, connection uptime/downtime, operating system version, available memory, bandwidth, etc.) of the client devices which are essential for federation management and to schedule when the models are communicated. By entrusting the device information to the central server, the <span id="S3.SS1.SSS2.p1.1.5" class="ltx_text ltx_font_italic">data privacy</span> of the clients’ information is compromised. The <span id="S3.SS1.SSS2.p1.1.6" class="ltx_text ltx_font_bold">client registry</span> pattern complements the patterns that manage the connection between the central server and the client devices. For instance, the client cluster and client selector patterns both are complemented by the client registry with its <span id="S3.SS1.SSS2.p1.1.7" class="ltx_text ltx_font_italic">client information availability</span>. However, this would require the storage of data which induces <span id="S3.SS1.SSS2.p1.1.8" class="ltx_text ltx_font_italic">privacy</span> and <span id="S3.SS1.SSS2.p1.1.9" class="ltx_text ltx_font_italic">storage cost efficiency</span> issues. One example of this pattern is the <span id="S3.SS1.SSS2.p1.1.10" class="ltx_text ltx_font_italic">Party Stack</span> component of <span id="S3.SS1.SSS2.p1.1.11" class="ltx_text ltx_font_italic">IBM Federated Learning<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_upright">2</span></span><span id="footnote2.5" class="ltx_text ltx_font_upright">https://github.com/IBM/federated-learning-lib</span></span></span></span></span> framework that manages the client parties of IBM federated learning framework with sub-components such as protocol handler, connection, model, local training, and data handler for client devices registration and management.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span><span id="S3.SS1.SSS3.1.1" class="ltx_text ltx_font_bold">Client selector</span>
</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">The <span id="S3.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_bold">client selector</span> pattern is adopted to actively select the client devices for the training process. This pattern intends to fulfill the non-functional requirement on the <span id="S3.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_italic">training efficiency</span> when interacting with client devices that have high differences in their available computation, communication, and memory capacity <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>; Kairouz et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. However, the adoption of <span id="S3.SS1.SSS3.p1.1.3" class="ltx_text ltx_font_bold">client selector</span> excludes a portion of data from clients which may induce low model generalisability, or higher model bias to unseen data and harm the <span id="S3.SS1.SSS3.p1.1.4" class="ltx_text ltx_font_italic">model quality</span> <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2021a</a>, <a href="#bib.bib20" title="" class="ltx_ref">2022a</a>)</cite>. The <span id="S3.SS1.SSS3.p1.1.5" class="ltx_text ltx_font_bold">client selector</span> pattern is an alternative pattern to the <span id="S3.SS1.SSS3.p1.1.6" class="ltx_text ltx_font_bold">client cluster</span> pattern. Both patterns improve the <span id="S3.SS1.SSS3.p1.1.7" class="ltx_text ltx_font_italic">training efficiency</span> but the <span id="S3.SS1.SSS3.p1.1.8" class="ltx_text ltx_font_bold">client selector</span> pattern offers better efficiency but may lower <span id="S3.SS1.SSS3.p1.1.9" class="ltx_text ltx_font_italic">model quality</span>. One example of this pattern is <span id="S3.SS1.SSS3.p1.1.10" class="ltx_text ltx_font_italic">IBM’s Helios</span> <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2021b</a>)</cite> which has a training consumption profiling function that fully profiles the resource consumption for model training on client devices. Based on the profiling, a resource-aware scheme accelerate local model training on heterogeneous devices and prevent stragglers from delaying the process.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2204.13291/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Model Management and Configuration Patterns Decision Model</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Model Management and Configuration Decision Model</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span><span id="S3.SS2.SSS1.1.1" class="ltx_text ltx_font_bold">Training configurator</span>
</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.1.3. Client selector ‣ 3.1. Federation Management Decision Model ‣ 3. Decision Models ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the model management and configuration decision model. A <span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">training configurator</span> provides a user-friendly interface, state-of-the-art practices, and technical support to the system owners to configure the training parameters, client devices management, model management and configuration, and model aggregation mechanisms. This pattern enhances the <span id="S3.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">accessibility</span>, <span id="S3.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_italic">computation efficiency</span> and <span id="S3.SS2.SSS1.p1.1.4" class="ltx_text ltx_font_italic">usability</span> of the federated machine learning system. For instance, <span id="S3.SS2.SSS1.p1.1.5" class="ltx_text ltx_font_italic">Microsoft Azure Machine Learning Designer<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_upright">3</span></span><a target="_blank" href="https://azure.microsoft.com/en-au/services/machine-learning/designer/" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://azure.microsoft.com/en-au/services/machine-learning/designer/</a></span></span></span></span> and the <span id="S3.SS2.SSS1.p1.1.6" class="ltx_text ltx_font_italic">Amazon SageMaker<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_upright">4</span></span><a target="_blank" href="https://aws.amazon.com/sagemaker/" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://aws.amazon.com/sagemaker/</a></span></span></span></span> are available for centralised or distributed ML systems configuration. However, a preset system may have a relatively lower <span id="S3.SS2.SSS1.p1.1.7" class="ltx_text ltx_font_italic">flexibility</span>. Moreover, the system may face <span id="S3.SS2.SSS1.p1.1.8" class="ltx_text ltx_font_italic">scalability</span> issues to support more users and devices associated with the expansion of the systems. The <span id="S3.SS2.SSS1.p1.1.9" class="ltx_text ltx_font_bold">training configurator</span> pattern complements the <span id="S3.SS2.SSS1.p1.1.10" class="ltx_text ltx_font_bold">model co-versioning registry</span>, <span id="S3.SS2.SSS1.p1.1.11" class="ltx_text ltx_font_bold">model replacement trigger</span>, and the <span id="S3.SS2.SSS1.p1.1.12" class="ltx_text ltx_font_bold">deployment selector</span> patterns, in terms of <span id="S3.SS2.SSS1.p1.1.13" class="ltx_text ltx_font_italic">usability</span>.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span><span id="S3.SS2.SSS2.1.1" class="ltx_text ltx_font_bold">Model co-versioning registry</span>
</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The <span id="S3.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">model co-versioning registry</span> pattern is adopted for model provenance. This approach uses a registry to actively track and record all the model versions and their performance. This effectively increases the <span id="S3.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_italic">accountability</span>, and <span id="S3.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_italic">traceability</span> of the federated machine learning system. One downside to this approach is the low <span id="S3.SS2.SSS2.p1.1.4" class="ltx_text ltx_font_italic">storage cost efficiency</span> due to the requirement to store highly-complex model architecture with the increase in the number of client devices <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>)</cite>. Furthermore, users’ <span id="S3.SS2.SSS2.p1.1.5" class="ltx_text ltx_font_italic">data privacy</span> may be compromised if the registry is managed solely by the central server <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>; Kairouz et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>, whereas placing the registry in each client device reduces the devices’ <span id="S3.SS2.SSS2.p1.1.6" class="ltx_text ltx_font_italic">storage cost efficiency</span> and <span id="S3.SS2.SSS2.p1.1.7" class="ltx_text ltx_font_italic">computation efficiency</span>. <span id="S3.SS2.SSS2.p1.1.8" class="ltx_text ltx_font_italic">Reliability</span> issue also occurs when only one party holds the registry. A storage-efficient method using blockchain and smart contracts to track and record only the hashed representations of the data and model versions is mentioned in <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2022a</a>)</cite>. In addition, the usage of a combination of decentralised blockchain and database for provenance purposes mentioned in <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2022a</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020</a>)</cite> also resolved the <span id="S3.SS2.SSS2.p1.1.9" class="ltx_text ltx_font_italic">data privacy</span> and <span id="S3.SS2.SSS2.p1.1.10" class="ltx_text ltx_font_italic">trustworthiness</span> issues. Some examples of this pattern include <span id="S3.SS2.SSS2.p1.1.11" class="ltx_text ltx_font_italic">DVC<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span id="footnote5.1.1.1" class="ltx_text ltx_font_upright">5</span></span><span id="footnote5.5" class="ltx_text ltx_font_upright">https://dvc.org/</span></span></span></span></span> which is an online machine learning version control platform built to make models shareable and reproducible, and <span id="S3.SS2.SSS2.p1.1.12" class="ltx_text ltx_font_italic">Managed MLflow<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span id="footnote6.1.1.1" class="ltx_text ltx_font_upright">6</span></span><span id="footnote6.5" class="ltx_text ltx_font_upright">https://databricks.com/product/managed-mlflow</span></span></span></span></span> on Databricks that provides chronological model lineage, model versioning, stage transitions, and descriptions.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span><span id="S3.SS2.SSS3.1.1" class="ltx_text ltx_font_bold">Model replacement trigger</span>
</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">A <span id="S3.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">model replacement trigger</span> pattern monitors the performance of the model that is deployed for real-world usage and when the model’s performance (accuracy, precision, etc.) degrades, a request for a new model will be generated to trigger for a new model training task. This maintains the <span id="S3.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_italic">upgradability</span> and <span id="S3.SS2.SSS3.p1.1.3" class="ltx_text ltx_font_italic">reliability</span> of the systems. However, the continuous monitoring and update of the deployed models will affect the <span id="S3.SS2.SSS3.p1.1.4" class="ltx_text ltx_font_italic">computation efficiency</span>. One example is <span id="S3.SS2.SSS3.p1.1.5" class="ltx_text ltx_font_italic">Microsoft Azure Machine Learning Designer<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span id="footnote7.1.1.1" class="ltx_text ltx_font_upright">7</span></span><span id="footnote7.5" class="ltx_text ltx_font_upright">https://azure.microsoft.com/en-au/services/machine-learning/designer/</span></span></span></span></span> which is a platform for machine learning pipeline creation that enables models to be retrained on new data.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4. </span><span id="S3.SS2.SSS4.1.1" class="ltx_text ltx_font_bold">Deployment selector</span>
</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">A <span id="S3.SS2.SSS4.p1.1.1" class="ltx_text ltx_font_bold">deployment selector</span> pattern fulfills the requirement to deploy the trained model to the client devices for real-world usage. This pattern is complemented by the <span id="S3.SS2.SSS4.p1.1.2" class="ltx_text ltx_font_bold">training configurator</span> to increase the model’s <span id="S3.SS2.SSS4.p1.1.3" class="ltx_text ltx_font_italic">ease of deployment</span>. It deploys converged models based on preset criteria. This is especially crucial in multi-task and multi-model training scenario <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>; Kairouz et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>)</cite> where the local data is used to train the models from multiple, different-but-related applications. The <span id="S3.SS2.SSS4.p1.1.4" class="ltx_text ltx_font_bold">deployment selector</span> pattern selects clients that are most suitable to receive the model, according to their specifications, resources, applications, etc. For instance, a smartphone keyboard application provider can deploy the next-world prediction global model to smartphone users that are frequent typists and the text translation global model to smartphone users that use more translation applications for training. This enhances the <span id="S3.SS2.SSS4.p1.1.5" class="ltx_text ltx_font_italic">model suitability</span> for each client device’s application. Some examples are <span id="S3.SS2.SSS4.p1.1.6" class="ltx_text ltx_font_italic">Azure Machine Learning<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note"><span id="footnote8.1.1.1" class="ltx_text ltx_font_upright">8</span></span><span id="footnote8.5" class="ltx_text ltx_font_upright">https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment</span></span></span></span></span> that supports mass deployment with a step of compute target selection, and <span id="S3.SS2.SSS4.p1.1.7" class="ltx_text ltx_font_italic">Amazon SageMaker<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span id="footnote9.1.1.1" class="ltx_text ltx_font_upright">9</span></span><span id="footnote9.5" class="ltx_text ltx_font_upright">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html</span></span></span></span></span> which can host multiple models with multi-model endpoints. However, the <span id="S3.SS2.SSS4.p1.1.8" class="ltx_text ltx_font_italic">data privacy</span> is compromised as more application and device information are required for the selection criteria and causes lower <span id="S3.SS2.SSS4.p1.1.9" class="ltx_text ltx_font_italic">computational efficiency</span>.</p>
</div>
</section>
<section id="S3.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5. </span><span id="S3.SS2.SSS5.1.1" class="ltx_text ltx_font_bold">Model compressor</span>
</h4>

<div id="S3.SS2.SSS5.p1" class="ltx_para">
<p id="S3.SS2.SSS5.p1.1" class="ltx_p">A <span id="S3.SS2.SSS5.p1.1.1" class="ltx_text ltx_font_bold">model compressor</span> pattern can be adopted to reduce the data size of the model before being transferred between the two parties. Model pruning and compression can increase the <span id="S3.SS2.SSS5.p1.1.2" class="ltx_text ltx_font_italic">communication efficiency</span> but may negatively impact the <span id="S3.SS2.SSS5.p1.1.3" class="ltx_text ltx_font_italic">model quality</span> due to the lower model and data precision <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>; Kairouz et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. Using the smartphone use case as an example, the local models are compressed by the app before being sent to the central server, and after aggregation, the updated global model is also compressed before being distributed to the smartphones. One example is Google’s structured update and sketched update. The structured update directly learns an update from a restricted space that can be parametrised using a smaller number of variables, whereas sketched update compresses the model before sending it to the central server.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2204.13291/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Model Aggregation Patterns Decision Model</figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Model Aggregation Decision Model</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span><span id="S3.SS3.SSS1.1.1" class="ltx_text ltx_font_bold">Asynchronous aggregator</span>
</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.2.5. Model compressor ‣ 3.2. Model Management and Configuration Decision Model ‣ 3. Decision Models ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the model aggregation decision model. The model aggregation time depends on the arrival time of the last local model if synchronous aggregation is performed. To reduce the <span id="S3.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">latency</span> in model aggregation and increase the <span id="S3.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_italic">computation efficiency</span> of the model aggregation process, the <span id="S3.SS3.SSS1.p1.1.3" class="ltx_text ltx_font_bold">asynchronous aggregator</span> pattern can be adopted. This pattern performs model aggregation whenever a local model update is received, with the currently available global model in the central server. The aggregation <span id="S3.SS3.SSS1.p1.1.4" class="ltx_text ltx_font_italic">latency</span> can be reduced which enhances the <span id="S3.SS3.SSS1.p1.1.5" class="ltx_text ltx_font_italic">computation efficiency</span>. For instance, the central server aggregates the local model from a smartphone instantly to the latest global model after receiving it. One example is <span id="S3.SS3.SSS1.p1.1.6" class="ltx_text ltx_font_italic">Asynchronous federated SGD-Vertical Partitioned</span> (<span id="S3.SS3.SSS1.p1.1.7" class="ltx_text ltx_font_italic">AFSGD-VP</span>) <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite> algorithm that uses a tree-structured communication scheme to perform asynchronous aggregation. However, a relatively higher number of aggregation rounds may be required and causes lower <span id="S3.SS3.SSS1.p1.1.8" class="ltx_text ltx_font_italic">communication efficiency</span>. Some client devices with extremely scarce communication bandwidth may also struggle from being too outdated to join the latest aggregation. Thus, the model produced may be biased.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span><span id="S3.SS3.SSS2.1.1" class="ltx_text ltx_font_bold">Hierarchical aggregator</span>
</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">The non-IID (Identically Independently Distributed) data is another main challenge of federated machine learning <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>, <a href="#bib.bib20" title="" class="ltx_ref">2022a</a>)</cite>. A <span id="S3.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_bold">hierarchical aggregator</span> pattern uses intermediate layers between the central server and the client devices to resolve the non-IID issue. For instance, edge servers are deployed to cluster and aggregate the smartphones within the same area and perform intermediate model aggregation among these smartphones. By performing an intermediate aggregation, the <span id="S3.SS3.SSS2.p1.1.2" class="ltx_text ltx_font_italic">statistical</span> and <span id="S3.SS3.SSS2.p1.1.3" class="ltx_text ltx_font_italic">system heterogeneity</span> are improved. The <span id="S3.SS3.SSS2.p1.1.4" class="ltx_text ltx_font_italic">scalability</span> is also reduced. One example is <span id="S3.SS3.SSS2.p1.1.5" class="ltx_text ltx_font_italic">HierFAVG</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> which allows multiple edge servers to perform partial model aggregation incrementally from the collected updates. The hierarchical aggregator is an alternative to the asynchronous aggregator that has better <span id="S3.SS3.SSS2.p1.1.6" class="ltx_text ltx_font_italic">computation efficiency</span> but lower <span id="S3.SS3.SSS2.p1.1.7" class="ltx_text ltx_font_italic">reliability</span>. However, the constraint of this pattern is to have more devices added to the system, which induces higher cost and adds more points-of-failure increase, which compromises <span id="S3.SS3.SSS2.p1.1.8" class="ltx_text ltx_font_italic">reliability</span> and <span id="S3.SS3.SSS2.p1.1.9" class="ltx_text ltx_font_italic">security</span>.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3. </span><span id="S3.SS3.SSS3.1.1" class="ltx_text ltx_font_bold">Decentralised aggregator</span>
</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">Another pattern that aims to solve the single-point-of-failure issue is the <span id="S3.SS3.SSS3.p1.1.1" class="ltx_text ltx_font_bold">decentralised aggregator</span> pattern. It removes the central server entirely but it is constrained by low <span id="S3.SS3.SSS3.p1.1.2" class="ltx_text ltx_font_italic">computation efficiency</span> and <span id="S3.SS3.SSS3.p1.1.3" class="ltx_text ltx_font_italic">cost</span> of using alternatives to execute model aggregation. These alternatives include using peer-to-peer communication between neighboring client devices <cite class="ltx_cite ltx_citemacro_citep">(Roy et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>, or blockchain and smart contract to manage the models <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2019</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2020</a>)</cite>. For example, instead of sending the local models from each smartphone to a central server owned by the app provider, they share the local models with 10 of their nearest available devices and aggregate their models within the group. This pattern increases the <span id="S3.SS3.SSS3.p1.1.4" class="ltx_text ltx_font_italic">reliability</span> and <span id="S3.SS3.SSS3.p1.1.5" class="ltx_text ltx_font_italic">accountability</span> in comparison with the centralised federated machine learning approach but it suffers more in terms of <span id="S3.SS3.SSS3.p1.1.6" class="ltx_text ltx_font_italic">latency</span> and <span id="S3.SS3.SSS3.p1.1.7" class="ltx_text ltx_font_italic">storage cost efficiency</span>, especially due to the peer-to-peer connection and the read/write performance of blockchain. <span id="S3.SS3.SSS3.p1.1.8" class="ltx_text ltx_font_italic">Swarm Learning</span> <cite class="ltx_cite ltx_citemacro_citep">(Warnat-Herresthal et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> is a decentralised aggregator example that utilises edge computing, blockchain-based peer-to-peer networking, and coordination while maintaining confidentiality without the need for a central coordinator. Decentralised aggregator is an alternative to the hierarchical aggregator that increases the <span id="S3.SS3.SSS3.p1.1.9" class="ltx_text ltx_font_italic">reliability</span> of the system but lowers <span id="S3.SS3.SSS3.p1.1.10" class="ltx_text ltx_font_italic">computation efficiency</span>.</p>
</div>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4. </span><span id="S3.SS3.SSS4.1.1" class="ltx_text ltx_font_bold">Secure aggregator</span>
</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p id="S3.SS3.SSS4.p1.1" class="ltx_p">The central server needs to be robust throughout the training process and hence, a <span id="S3.SS3.SSS4.p1.1.1" class="ltx_text ltx_font_bold">secure aggregator</span> can be adopted. This pattern utilises state-of-the-art security approaches for multiparty computation such as homomorphic encryption to encrypt and decrypt the model before exchanges, or local differential privacy that add noise to the models before exchanges. For instance, <span id="S3.SS3.SSS4.p1.1.2" class="ltx_text ltx_font_italic">SecAgg</span> <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite> is a practical protocol by Google for secure aggregation in the federated learning settings. However, differential privacy approaches may reduce the <span id="S3.SS3.SSS4.p1.1.3" class="ltx_text ltx_font_italic">model quality</span> due to the noise added to the models. The <span id="S3.SS3.SSS4.p1.1.4" class="ltx_text ltx_font_italic">computation efficiency</span> drops and <span id="S3.SS3.SSS4.p1.1.5" class="ltx_text ltx_font_italic">latency</span> occurs to perform encryption and decryption for every model update received.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Model Training Decision Model</h3>

<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span><span id="S3.SS4.SSS1.1.1" class="ltx_text ltx_font_bold">Heterogeneous data handler</span>
</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Fig. <a href="#S3.F8" title="Figure 8 ‣ 3.4.3. Multi-task trainer ‣ 3.4. Model Training Decision Model ‣ 3. Decision Models ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the model training decision model. Firstly, to solve the <span id="S3.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_italic">statistical heterogeneity</span> issue, the <span id="S3.SS4.SSS1.p1.1.2" class="ltx_text ltx_font_bold">heterogeneous data handler</span> pattern can be adopted. The pattern can implement data augmentation for federated machine learning <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> to generate more data points to create a more balanced dataset, or adopt the federated knowledge distillation method <cite class="ltx_cite ltx_citemacro_citep">(Ahn et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> that obtains the knowledge from other devices during the distributed training process, without accessing the raw data. For example, the smartphone can use a generative model to create a more balanced dataset (text data with more variety in terms of sentiment) based on the original local data for model training. Apart from solving <span id="S3.SS4.SSS1.p1.1.3" class="ltx_text ltx_font_italic">statistical heterogeneity</span>, this pattern also enhances <span id="S3.SS4.SSS1.p1.1.4" class="ltx_text ltx_font_italic">model quality</span> in terms of fairness, which serves as an alternative to the <span id="S3.SS4.SSS1.p1.1.5" class="ltx_text ltx_font_bold">incentive registry</span> pattern in terms of <span id="S3.SS4.SSS1.p1.1.6" class="ltx_text ltx_font_italic">reliability</span> improvements. For example, federated augmentation (<span id="S3.SS4.SSS1.p1.1.7" class="ltx_text ltx_font_italic">FAug</span>) <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> is a data augmentation scheme that utilises a generative adversarial network (GAN) to generate data that takes the tradeoff between privacy leakage and communication overhead into consideration.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span><span id="S3.SS4.SSS2.1.1" class="ltx_text ltx_font_bold">Incentive registry</span>
</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">To improve the <span id="S3.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_italic">model quality</span> by increasing the participation rate of client devices, the <span id="S3.SS4.SSS2.p1.1.2" class="ltx_text ltx_font_bold">incentive registry</span> can be implemented. By providing a fair amount of reward or compensation to the clients, the overall <span id="S3.SS4.SSS2.p1.1.3" class="ltx_text ltx_font_italic">client motivatability</span> is increased and this translates to better model generalisability and hence, better <span id="S3.SS4.SSS2.p1.1.4" class="ltx_text ltx_font_italic">model quality</span>. Furthermore, giving rewards according to the clients’ contribution can also improve the <span id="S3.SS4.SSS2.p1.1.5" class="ltx_text ltx_font_italic">system fairness</span>. Blockchain and smart contract technology are adapted to realise the incentive registry. For example, the app provider can provide exclusive app features or compensation in the form of cryptocurrencies to users that authorised the usage of data and resources to train the local models. However, the provision of rewards may harm the system <span id="S3.SS4.SSS2.p1.1.6" class="ltx_text ltx_font_italic">security</span> as dishonest clients may submit fraudulent results to earn rewards illegally and distort the training process.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3. </span><span id="S3.SS4.SSS3.1.1" class="ltx_text ltx_font_bold">Multi-task trainer</span>
</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">The <span id="S3.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_bold">multi-task trainer</span> pattern uses local data of different-but-related applications for training to enhance model generalisability for better <span id="S3.SS4.SSS3.p1.1.2" class="ltx_text ltx_font_italic">model quality</span> and <span id="S3.SS4.SSS3.p1.1.3" class="ltx_text ltx_font_italic">robustness</span>. Furthermore, the training of the model on related or overlapping representations improves the <span id="S3.SS4.SSS3.p1.1.4" class="ltx_text ltx_font_italic">training efficiency</span> of the system by reducing the training cost. For instance, a smartphone can use image and text data together to train a multitask model to predict text in an image. However, this pattern is constrained by the requirement to collect and match the data from different applications across all participating client devices to perform multi-task model training. The metadata (features of multiple tasks that are related) of the multiple tasks in the client devices are needed by the central server to create an initial model, which will be challenging in terms of <span id="S3.SS4.SSS3.p1.1.5" class="ltx_text ltx_font_italic">data privacy</span>. One example is <span id="S3.SS4.SSS3.p1.1.6" class="ltx_text ltx_font_italic">MultiModel<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span id="footnote10.1.1.1" class="ltx_text ltx_font_upright">10</span></span><span id="footnote10.5" class="ltx_text ltx_font_upright">https://ai.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html</span></span></span></span></span> by Google which simultaneously solves several problems spanning multiple domains, including image recognition, translation, and speech recognition.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2204.13291/assets/x8.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="276" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Model Training Patterns Decision Model</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Architecture Design Validation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we validate the usability of the decision models by mapping components of existing architectures to the pattern options. We have selected three concrete architectures that are fully maintained and technically supported by top software and web companies: (1) Meta’s federated machine learning architecture<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a target="_blank" href="https://engineering.fb.com/2022/06/14/production-engineering/federated-learning-differential-privacy/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://engineering.fb.com/2022/06/14/production-engineering/federated-learning-differential-privacy/</a></span></span></span>; top hardware company: (2) Intel OpenFL<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://openfl.readthedocs.io/en/latest/index.html</span></span></span>; and top industrial manufacturing company: (3) Siemens Industrial federated learning (IFL) <cite class="ltx_cite ltx_citemacro_citep">(Hiessl et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>. These architectures by companies that lead industry 4.0 have different design aspects. For instance, Meta focuses on efficiency under differential privacy, Intel focuses on chip-level FML security, and Siemens focuses on fulfilling industrial requirements. We sourced the architectures based on their availability and comprehensiveness at the time of conducting this research.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Meta’s federated machine learning Architecture</h3>

<figure id="S4.F9" class="ltx_figure"><img src="/html/2204.13291/assets/x9.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="323" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Meta’s federated machine learning Architecture <cite class="ltx_cite ltx_citemacro_citep">(Stojkovic et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite></figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The mapping of Meta’s components onto the decision models is shown in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1. Meta’s federated machine learning Architecture ‣ 4. Architecture Design Validation ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Secure aggregator:</span> In the trusted execution environment, the federated machine learning server and analytics server applied differential privacy for better <span id="S4.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">security</span>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Training configurator:</span> On client devices, the orchestrator, server data fetcher, signal transformer, and local training loop combine to perform: (1) scheduling, (2) device eligibility checking, (2) server-to-device data flow initialisation (3) sample submission control, and (4) logging and performance metric computation <cite class="ltx_cite ltx_citemacro_citep">(Stojkovic et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Heterogeneous data handler:</span> For better <span id="S4.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">model quality</span> in terms of fairness, data/feature augmentations are performed by the joiner component. On the device, the augmentation process is handled by the signal transformer <cite class="ltx_cite ltx_citemacro_citep">(Stojkovic et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Client registry</span> and <span id="S4.I1.i4.p1.1.2" class="ltx_text ltx_font_bold">model co-versioning registry:</span> Realised by the metadata and feature components under the central server for better <span id="S4.I1.i4.p1.1.3" class="ltx_text ltx_font_italic">maintainability</span>, <span id="S4.I1.i4.p1.1.4" class="ltx_text ltx_font_italic">reliability</span> and <span id="S4.I1.i4.p1.1.5" class="ltx_text ltx_font_italic">traceability</span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Intel OpenFL Architecture</h3>

<figure id="S4.F10" class="ltx_figure"><img src="/html/2204.13291/assets/x10.png" id="S4.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Intel OpenFL Architecture <cite class="ltx_cite ltx_citemacro_citep">(Reina et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The mapping of the pattern decisions to Intel’s components is shown in Figure <a href="#S4.F10" title="Figure 10 ‣ 4.2. Intel OpenFL Architecture ‣ 4. Architecture Design Validation ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Multi-task model trainer:</span> In the collaborator (client device), the DL/ML model executed by the FL plan executor supports the multi-institutional collaboration for model training to improve the <span id="S4.I2.i1.p1.1.2" class="ltx_text ltx_font_italic">robustness</span> of the model <cite class="ltx_cite ltx_citemacro_citep">(Reina et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite>. However, due to the requirement to collect user data, <span id="S4.I2.i1.p1.1.3" class="ltx_text ltx_font_italic">data privacy</span> is compromisable.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Secure aggregator:</span> The TCP client utilises PKI (Public Key Infrastructure) certificates and mutually authenticated transport layer security (TLS) <span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a target="_blank" href="https://en.wikipedia.org/wiki/Transport_Layer_Security" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/Transport_Layer_Security</a></span></span></span> connections for better <span id="S4.I2.i2.p1.1.2" class="ltx_text ltx_font_italic">security</span> and <span id="S4.I2.i2.p1.1.3" class="ltx_text ltx_font_italic">data privacy</span> preservation <cite class="ltx_cite ltx_citemacro_citep">(Reina et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Training configurator:</span> The task coordination component enables the <span id="S4.I2.i3.p1.1.2" class="ltx_text ltx_font_italic">accessibility</span> and <span id="S4.I2.i3.p1.1.3" class="ltx_text ltx_font_italic">usability</span> of the system. The architecture adapted Trusted Execution Environments (TEEs) to provide hardware mechanisms to execute code with various security properties. It also uses the FL plan to define the collaborator and aggregator settings, such as batch size, IP address, and training rounds, and specifies the remote procedure calls for the given federation tasks <cite class="ltx_cite ltx_citemacro_citep">(Reina et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p"><span id="S4.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Deployment selector:</span> Realised by the model updates component to enhance the <span id="S4.I2.i4.p1.1.2" class="ltx_text ltx_font_italic">model suitability</span> for a different collaborator.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Siemens Industrial federated learning Architecture</h3>

<figure id="S4.F11" class="ltx_figure"><img src="/html/2204.13291/assets/x11.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Siemens IFL Architecture <cite class="ltx_cite ltx_citemacro_citep">(Hiessl et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite></figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The mapping of Siemens IFL architecture components onto the decision models is shown in Figure <a href="#S4.F11" title="Figure 11 ‣ 4.3. Siemens Industrial federated learning Architecture ‣ 4. Architecture Design Validation ‣ Decision Models for Selecting Architectural Patterns for Federated Machine Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p"><span id="S4.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Multi-task model trainer:</span> In the FL application (client device), the FL task manager and the FL plan processor realise the <span id="S4.I3.i1.p1.1.2" class="ltx_text ltx_font_bold">multi-task model trainer</span> pattern that identifies learning problems in which multiple FL tasks have in common to increase the <span id="S4.I3.i1.p1.1.3" class="ltx_text ltx_font_italic">robustness</span> of the model. However, the adoption is constrained by the client data collection requirement.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p"><span id="S4.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Client registry:</span> In the FL application (client device), it complements the <span id="S4.I3.i2.p1.1.2" class="ltx_text ltx_font_bold">multi-task model trainer</span> through a device manager to record and manage the client devices. On the server side, it registers the FL clients’ organisation, asset data, and data scheme for respective environmental conditions <cite class="ltx_cite ltx_citemacro_citep">(Hiessl et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>. It also complements all the patterns that require client data, consisting <span id="S4.I3.i2.p1.1.3" class="ltx_text ltx_font_bold">client selector</span>, <span id="S4.I3.i2.p1.1.4" class="ltx_text ltx_font_bold">deployment selector</span>, <span id="S4.I3.i2.p1.1.5" class="ltx_text ltx_font_bold">asynchronous aggregator</span> and <span id="S4.I3.i2.p1.1.6" class="ltx_text ltx_font_bold">model co-versioning registry</span> pattern through the utilisation of client data to support their operations. This increases the <span id="S4.I3.i2.p1.1.7" class="ltx_text ltx_font_italic">maintainability</span> and <span id="S4.I3.i2.p1.1.8" class="ltx_text ltx_font_italic">reliability</span> of the system. However, the pattern induces <span id="S4.I3.i2.p1.1.9" class="ltx_text ltx_font_italic">computation efficiency</span> issues.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p"><span id="S4.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Training configurator:</span> On the server side, the FL scheduler, FL plan processor, FL resource optimiser, FL population &amp; FL task store handle the FL plans, training schedules, and resource optimisation to increase the <span id="S4.I3.i3.p1.1.2" class="ltx_text ltx_font_italic">usability</span> and <span id="S4.I3.i3.p1.1.3" class="ltx_text ltx_font_italic">computation efficiency</span> of the system.</p>
</div>
</li>
<li id="S4.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i4.p1" class="ltx_para">
<p id="S4.I3.i4.p1.1" class="ltx_p"><span id="S4.I3.i4.p1.1.1" class="ltx_text ltx_font_bold">Client selector</span>: The FL cohorts manager component acts as the <span id="S4.I3.i4.p1.1.2" class="ltx_text ltx_font_italic">client selector</span> to reduce the duration of training or evaluation, which enhances <span id="S4.I3.i4.p1.1.3" class="ltx_text ltx_font_italic">computation efficiency</span> and <span id="S4.I3.i4.p1.1.4" class="ltx_text ltx_font_italic">model quality</span>.</p>
</div>
</li>
<li id="S4.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i5.p1" class="ltx_para">
<p id="S4.I3.i5.p1.1" class="ltx_p"><span id="S4.I3.i5.p1.1.1" class="ltx_text ltx_font_bold">Asynchronous aggregator</span>: The FL scheduler schedules the FL tasks also improves the <span id="S4.I3.i5.p1.1.2" class="ltx_text ltx_font_italic">computation efficiency</span> and reduces aggregation <span id="S4.I3.i5.p1.1.3" class="ltx_text ltx_font_italic">latency</span>.</p>
</div>
</li>
<li id="S4.I3.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i6.p1" class="ltx_para">
<p id="S4.I3.i6.p1.1" class="ltx_p"><span id="S4.I3.i6.p1.1.1" class="ltx_text ltx_font_bold">Model co-versioning registry</span>: Device &amp; asset metadata catalog and FL cohorts enhances the model <span id="S4.I3.i6.p1.1.2" class="ltx_text ltx_font_italic">traceability</span>.</p>
</div>
</li>
<li id="S4.I3.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i7.p1" class="ltx_para">
<p id="S4.I3.i7.p1.1" class="ltx_p"><span id="S4.I3.i7.p1.1.1" class="ltx_text ltx_font_bold">Deployment selector:</span> Uses continuous updates to reevaluate data similarity that is needed to ensure high <span id="S4.I3.i7.p1.1.2" class="ltx_text ltx_font_italic">model suitability</span> <cite class="ltx_cite ltx_citemacro_citep">(Hiessl et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Discussions</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The mappings of the decision models’ components and patterns on the existing federated machine learning systems implicated the correctness of the decision models as they mapped the patterns, their benefits, and drawbacks, ideally with the industrial methods and techniques used to address each corresponding requirement through visualisation. However, the is a limitation. The capabilities provided by the pattern have to be there but the components might have different names or also have other responsibilities. Assuming that the architectures are documented properly, it is still possible to find known uses for the patterns but extra efforts are required.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">There are two ways to utilise the decision models in federated machine learning architecture design and validation: (1) Assess the fulfillment of requirements and what are the tradeoffs that might be incurred upon the adaptation of one or more design decisions through visualisation; (2) Extend functionalities of existing architecture to fulfill certain requirements and better identify the tradeoffs that come with the decisions.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Related work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">A decision model is an approach in software engineering that maps the problems to the solution to guide design decision-making. A well-known approach for creating decision models is Questions-Options-Criteria (QOC) <cite class="ltx_cite ltx_citemacro_citep">(MacLean et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">1991</a>)</cite> where the questions represent problems, the options map to solutions, and the criteria are used to determine the options’ suitability concerning the questions. Another popular approach from the field of Software Measurement is Goal Questions Metric (GQM) <cite class="ltx_cite ltx_citemacro_citep">(Caldiera and Rombach, <a href="#bib.bib7" title="" class="ltx_ref">1994</a>)</cite>. It models the problem according to the goals and questions and provides the metrics to be used for assessing an object and subsequently making decisions to improve it. The Architecture Tradeoff Analysis Method (ATAM) <cite class="ltx_cite ltx_citemacro_citep">(Kazman et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2000</a>; Clements et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2003</a>)</cite> is a risk-driven approach to evaluating software architectures that helps identify tradeoffs between competing quality attributes, such as performance, security, reliability, and maintainability.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">There is much research work on multiple software engineering and architecture domains that have adopted decision models. For instance, Lewis et al. <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> proposed a decision model for cyber-foraging systems’ architectural tactics. Capilla et al. <cite class="ltx_cite ltx_citemacro_citep">(Capilla et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> highlighted the importance of collaborative decision-making to
produce more accurate and complete design decisions to improve the quality of the architectures. They explored the behavior of software engineering
students as novice software architects in different roles and promote critical design thinking to produce decisions with better quality and architectures. Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021a</a>)</cite> propose a decision model for selecting appropriate patterns for blockchain-based applications. These researchers designed their decision models in extension to the series of patterns or tactics that they have previously published.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Numerous pieces of research have been conducted on software engineering design decisions for machine learning. For instance, Warnett et al. <cite class="ltx_cite ltx_citemacro_citep">(Warnett and Zdun, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> proposed a series of architectural design decisions for machine learning deployment that covers the decision options, decision drivers, and their relations in the domain of machine learning deployment. Wan et al. <cite class="ltx_cite ltx_citemacro_citep">(Wan et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> studied the effects of machine learning adoptions on software development practices. The work characterizes the differences in various aspects of software engineering and the task involved in machine learning system development and traditional software development. Amershi et al. <cite class="ltx_cite ltx_citemacro_citep">(Amershi et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> expressed that AI components are more difficult to handle as distinct modules than traditional software components and summarise several best practices to tackle the software engineering challenges in machine learning. Lwakatare et al. <cite class="ltx_cite ltx_citemacro_citep">(Lwakatare et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> introduced a taxonomy that depicts machine learning components and their maturity stages of use in the industrial software system by mapping the challenges to the machine learning pipeline stages. Wan et al. <cite class="ltx_cite ltx_citemacro_citep">(Washizaki et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite> studied machine learning design patterns and architectural patterns. Yokohama <cite class="ltx_cite ltx_citemacro_citep">(Yokoyama, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite> proposed a set of architectural patterns to improve the operational stability of machine learning systems. A federated machine learning system design was introduced by Bonawitz et. al <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>. It focuses on the high-level design of a basic federated machine learning system. In terms of federated machine learning patterns, We compiled and presented a comprehensive and systematic collection of federated machine learning patterns to guide practitioners in developing federated machine learning systems in <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022b</a>)</cite>. Furthermore, as an extension to the pattern collections, a pattern-oriented reference architecture for federated machine learning was also proposed in <cite class="ltx_cite ltx_citemacro_citep">(Lo et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2021a</a>)</cite>. Motivated by the aforementioned works, we built decision models for the selection of patterns based on the requirements.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion and Future Works</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper presented a set of decision models based on the findings of a systematic literature review that aims to guide academic and industry software architects for federated machine learning system design. The decision models map various functional and non-functional requirements to the patterns, qualified with the benefits and drawbacks to improve the designers’ understanding of the effects of the decisions. The decision models have been evaluated in terms of correctness and usefulness through architecture design validations. The mappings of concrete architectures’ components to the decision models’ options validated the overall usability, correctness, applicability, and comprehensiveness of the decision models. For future works, we plan to expand the decision models by including more patterns, specifically related to the aggregation algorithms and trustworthy AI domain. We also aim to collect more experts’ feedback to improve the decision models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
J. Ahn, O. Simeone,
and J. Kang. 2019.

</span>
<span class="ltx_bibblock">Wireless Federated Distillation for Distributed
Edge Learning with Heterogeneous Data. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">PIMRC
2019</em>. 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amershi et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Saleema Amershi, Andrew
Begel, Christian Bird, Robert DeLine,
Harald Gall, Ece Kamar,
Nachiappan Nagappan, Besmira Nushi, and
Thomas Zimmermann. 2019.

</span>
<span class="ltx_bibblock">Software Engineering for Machine Learning: A Case
Study. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 41st International
Conference on Software Engineering: Software Engineering in Practice</em>
(Montreal, Quebec, Canada) <em id="bib.bib3.4.2" class="ltx_emph ltx_font_italic">(ICSE-SEIP ’19)</em>.
IEEE Press, 291–300.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
X. Bao, C. Su,
Y. Xiong, W. Huang, and
Y. Hu. 2019.

</span>
<span class="ltx_bibblock">FLChain: A Blockchain for Auditable Federated
Learning with Trust and Incentive. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">BIGCOM
’19</em>. 151–159.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir
Ivanov, Ben Kreuter, Antonio Marcedone,
H. Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and
Karn Seth. 2017.

</span>
<span class="ltx_bibblock">Practical Secure Aggregation for Privacy-Preserving
Machine Learning. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em> (Dallas, Texas,
USA) <em id="bib.bib5.4.2" class="ltx_emph ltx_font_italic">(CCS ’17)</em>. Association for
Computing Machinery, New York, NY, USA,
1175–1191.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
K. A. Bonawitz, Hubert
Eichner, Wolfgang Grieskamp, Dzmitry
Huba, Alex Ingerman, Vladimir Ivanov,
Chloé M Kiddon, Jakub Konečný,
Stefano Mazzocchi, Brendan McMahan,
Timon Van Overveldt, David Petrou,
Daniel Ramage, and Jason Roselander.
2019.

</span>
<span class="ltx_bibblock">Towards Federated Learning at Scale: System
Design. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">SysML 2019</em>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">To appear.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldiera and Rombach (1994)</span>
<span class="ltx_bibblock">
Victor R Basili1 Gianluigi Caldiera and
H Dieter Rombach. 1994.

</span>
<span class="ltx_bibblock">The goal question metric approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Encyclopedia of software engineering</em>
(1994), 528–532.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Capilla et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Rafael Capilla, Olaf
Zimmermann, Carlos Carrillo, and
Hernán Astudillo. 2020.

</span>
<span class="ltx_bibblock">Teaching Students Software Architecture Decision
Making. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Software Architecture</em>,
Anton Jansen, Ivano
Malavolta, Henry Muccini, Ipek Ozkaya,
and Olaf Zimmermann (Eds.). Springer
International Publishing, Cham,
231–246.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clements et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2003)</span>
<span class="ltx_bibblock">
Paul Clements, Rick
Kazman, Mark Klein, et al<span id="bib.bib9.3.1" class="ltx_text">.</span>
2003.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.4.1" class="ltx_emph ltx_font_italic">Evaluating software architectures</em>.

</span>
<span class="ltx_bibblock">Tsinghua University Press Beijing.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers
for Language Understanding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1810.04805 [cs.CL]

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Bin Gu, An Xu,
Zhouyuan Huo, Cheng Deng, and
Heng Huang. 2020.

</span>
<span class="ltx_bibblock">Privacy-Preserving Asynchronous Federated Learning
Algorithms for Multi-Party Vertically Collaborative Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hiessl et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Thomas Hiessl, Daniel
Schall, Jana Kemnitz, and Stefan
Schulte. 2020.

</span>
<span class="ltx_bibblock">Industrial federated learning–requirements and
system design. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">International Conference on
Practical Applications of Agents and Multi-Agent Systems</em>. Springer,
42–53.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Eunjeong Jeong, Seungeun
Oh, Hyesung Kim, Jihong Park,
Mehdi Bennis, and Seong-Lyun Kim.
2018.

</span>
<span class="ltx_bibblock">Communication-efficient on-device machine learning:
Federated distillation and augmentation under non-iid private data.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.11479</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jobin et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Anna Jobin, Marcello
Ienca, and Effy Vayena.
2019.

</span>
<span class="ltx_bibblock">The global landscape of AI ethics guidelines.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>
1, 9 (2019),
389–399.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary
Charles, Graham Cormode, Rachel
Cummings, et al<span id="bib.bib15.3.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.04977</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazman et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2000)</span>
<span class="ltx_bibblock">
Rick Kazman, Mark Klein,
and Paul Clements. 2000.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">ATAM: Method for architecture evaluation</em>.

</span>
<span class="ltx_bibblock">Technical Report.
Carnegie-Mellon Univ Pittsburgh PA Software Engineering
Inst.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Grace A. Lewis, Patricia
Lago, and Paris Avgeriou.
2016.

</span>
<span class="ltx_bibblock">A Decision Model for Cyber-Foraging Systems. In
<em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">2016 13th Working IEEE/IFIP Conference on Software
Architecture (WICSA)</em>. 51–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Lumin Liu, Jun Zhang,
S. H. Song, and Khaled B. Letaief.
2019.

</span>
<span class="ltx_bibblock">Client-Edge-Cloud Hierarchical Federated Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1905.06641 [cs.NI]

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lo et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sin Kit Lo, Chee Sun
Liew, Kok Soon Tey, and Saad
Mekhilef. 2019.

</span>
<span class="ltx_bibblock">An Interoperable Component-Based Architecture for
Data-Driven IoT System.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Sensors</em> 19,
20 (2019).

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lo et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Sin Kit Lo, Yue Liu,
Qinghua Lu, Chen Wang,
Xiwei Xu, Hye-Young Paik, and
Liming Zhu. 2022a.

</span>
<span class="ltx_bibblock">Towards Trustworthy AI: Blockchain-based
Architecture Design for Accountability and Fairness of Federated Learning
Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
(2022), 1–1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lo et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Sin Kit Lo, Qinghua Lu,
Hye-Young Paik, and Liming Zhu.
2021a.

</span>
<span class="ltx_bibblock">FLRA: A Reference Architecture for Federated
Learning Systems. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Software Architecture</em>.
Springer International Publishing,
83–98.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lo et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Sin Kit Lo, Qinghua Lu,
Chen Wang, Hye-Young Paik, and
Liming Zhu. 2021b.

</span>
<span class="ltx_bibblock">A Systematic Literature Review on Federated Machine
Learning: From a Software Engineering Perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em> 54,
5, Article 95 (May
2021), 39 pages.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lo et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Sin Kit Lo, Qinghua Lu,
Liming Zhu, Hye-Young Paik,
Xiwei Xu, and Chen Wang.
2022b.

</span>
<span class="ltx_bibblock">Architectural patterns for the design of federated
learning systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Journal of Systems and Software</em>
191 (2022), 111357.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lwakatare et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Lucy Ellen Lwakatare,
Aiswarya Raj, Jan Bosch,
Helena Holmström Olsson, and Ivica
Crnkovic. 2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">A Taxonomy of Software Engineering
Challenges for Machine Learning Systems: An Empirical Investigation</em>.

</span>
<span class="ltx_bibblock">227–243.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacLean et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (1991)</span>
<span class="ltx_bibblock">
Allan MacLean, Richard M
Young, Victoria ME Bellotti, and
Thomas P Moran. 1991.

</span>
<span class="ltx_bibblock">Questions, options, and criteria: Elements of
design space analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Human–computer interaction</em>
6, 3-4 (1991),
201–250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Agüera y Arcas.
2017.

</span>
<span class="ltx_bibblock">Communication-Efficient Learning of Deep Networks
from Decentralized Data.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1602.05629 [cs.LG]

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reina et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
G Anthony Reina, Alexey
Gruzdev, Patrick Foley, Olga
Perepelkina, Mansi Sharma, Igor
Davidyuk, Ilya Trushkin, Maksim
Radionov, Aleksandr Mokrov, Dmitry
Agapov, et al<span id="bib.bib27.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">OpenFL: An open-source framework for Federated
Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.06413</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roy et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Abhijit Guha Roy, Shayan
Siddiqui, Sebastian Pölsterl, Nassir
Navab, and Christian Wachinger.
2019.

</span>
<span class="ltx_bibblock">BrainTorrent: A Peer-to-Peer Environment for
Decentralized Federated Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1905.06731 [cs.LG]

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stojkovic et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Branislav Stojkovic,
Jonathan Woodbridge, Zhihan Fang,
Jerry Cai, Andrey Petrov,
Sathya Iyer, Daoyu Huang,
Patrick Yau, Arvind Sastha Kumar,
Hitesh Jawa, and Anamita Guha.
2022.

</span>
<span class="ltx_bibblock">Applied Federated Learning: Architectural Design for
Robust and Efficient Learning in Privacy Aware Settings.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.2206.00807" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.2206.00807</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thiebes et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Scott Thiebes, Sebastian
Lins, and Ali Sunyaev. 2020.

</span>
<span class="ltx_bibblock">Trustworthy artificial intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Electronic Markets</em> (2020).

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Z. Wan, X. Xia,
D. Lo, and G. C. Murphy.
2019.

</span>
<span class="ltx_bibblock">How does Machine Learning Change Software
Development Practices?

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">IEEE Trans. Softw. Eng.</em>
(2019), 1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warnat-Herresthal et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Stefanie Warnat-Herresthal,
Hartmut Schultze,
Krishnaprasad Lingadahalli Shastry,
Sathyanarayanan Manamohan, Saikat
Mukherjee, Vishesh Garg, Ravi
Sarveswara, Kristian Händler, Peter
Pickkers, N Ahmad Aziz, et al<span id="bib.bib32.3.1" class="ltx_text">.</span>
2021.

</span>
<span class="ltx_bibblock">Swarm Learning for decentralized and confidential
clinical machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.4.1" class="ltx_emph ltx_font_italic">Nature</em> 594,
7862 (2021), 265–270.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warnett and Zdun (2022)</span>
<span class="ltx_bibblock">
Stephen John Warnett and
Uwe Zdun. 2022.

</span>
<span class="ltx_bibblock">Architectural Design Decisions for Machine Learning
Deployment. In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2022 IEEE 19th International
Conference on Software Architecture (ICSA)</em>. 90–100.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Washizaki et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
H. Washizaki, H.
Uchida, F. Khomh, and Y.
Guéhéneuc. 2019.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Xiwei Xu, H.M.N.
Dilum Bandara, Qinghua Lu, Ingo Weber,
Len Bass, and Liming Zhu.
2021a.

</span>
<span class="ltx_bibblock">A Decision Model for Choosing Patterns in
Blockchain-Based Applications. In <em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">2021 IEEE 18th
International Conference on Software Architecture (ICSA)</em>.
47–57.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Zirui Xu, Fuxun Yu,
Jinjun Xiong, and Xiang Chen.
2021b.

</span>
<span class="ltx_bibblock">Helios: Heterogeneity-Aware Federated Learning with
Dynamically Balanced Collaboration. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">2021 58th
ACM/IEEE Design Automation Conference (DAC)</em>. 997–1002.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/DAC18074.2021.9586241" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/DAC18074.2021.9586241</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu,
Tianjian Chen, and Yongxin Tong.
2019.

</span>
<span class="ltx_bibblock">Federated Machine Learning: Concept and
Applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Intell. Syst. Technol.</em>
10, 2, Article 12
(jan 2019), 19 pages.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yokoyama (2019)</span>
<span class="ltx_bibblock">
H. Yokoyama.
2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Machine Learning System Architectural
Pattern for Improving Operational Stability</em>.

</span>
<span class="ltx_bibblock">267–274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
W. Zhang, Q. Lu,
Q. Yu, Z. Li, Y.
Liu, S. K. Lo, S. Chen,
X. Xu, and L. Zhu.
2020.

</span>
<span class="ltx_bibblock">Blockchain-based Federated Learning for Device
Failure Detection in Industrial IoT.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">IEEE Internet Things J.</em>
(2020), 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2204.13290" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2204.13291" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2204.13291">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2204.13291" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2204.13292" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 06:15:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
