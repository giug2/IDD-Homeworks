<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.10567] CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) is a multi-discipline research task. To produce the right answer, it requires an understanding of the visual content of images, the natural language questions, as well as commonsense rea…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.10567">

<!--Generated on Thu Mar 14 08:09:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yao Zhang <sup id="id1.1.id1" class="ltx_sup">1</sup>,
  Haokun Chen <sup id="id2.2.id2" class="ltx_sup">1,4</sup>,
  Ahmed Frikha <sup id="id3.3.id3" class="ltx_sup">1,4</sup>,
  Yezi Yang <sup id="id4.4.id4" class="ltx_sup">2</sup>,
<br class="ltx_break">  Denis Krompass <sup id="id5.5.id5" class="ltx_sup">1,4</sup>,
  Gengyuan Zhang <sup id="id6.6.id6" class="ltx_sup">1</sup>,
  Jindong Gu <sup id="id7.7.id7" class="ltx_sup">3</sup>,
  Volker Tresp <sup id="id8.8.id8" class="ltx_sup">1,4</sup>
<br class="ltx_break"><sup id="id9.9.id9" class="ltx_sup">1</sup> Institute of Informatics, LMU Munich     <sup id="id10.10.id10" class="ltx_sup">2</sup> Department of Informatics, Technical University of Munich 
<br class="ltx_break"><sup id="id11.11.id11" class="ltx_sup">3</sup> Torr Vision Group, University of Oxford     <sup id="id12.12.id12" class="ltx_sup">4</sup> Corporate Technology, Siemens AG 
<br class="ltx_break">yzhang@dbs.ifi.lmu.de,   volker.tresp@siemens.com
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">Visual Question Answering (VQA) is a multi-discipline research task. To produce the right answer, it requires an understanding of the visual content of images, the natural language questions, as well as commonsense reasoning over the information contained in the image and world knowledge. Recently, large-scale Vision-and-Language Pre-trained Models (VLPMs) have been the mainstream approach to VQA tasks due to their superior performance. The standard practice is to fine-tune large-scale VLPMs pre-trained on huge general-domain datasets using the domain-specific VQA datasets. However, in reality, the application domain can change over time, necessitating VLPMs to continually learn and adapt to new domains without forgetting previously acquired knowledge. Most existing continual learning (CL) research concentrates on unimodal tasks, whereas a more practical application scenario, <em id="id13.id1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="id13.id1.2" class="ltx_text"></span>, CL on cross-domain VQA, has not been studied. Motivated by this, we introduce <span id="id13.id1.3" class="ltx_text ltx_font_bold">CL-CrossVQA</span>, a rigorous <span id="id13.id1.4" class="ltx_text ltx_font_bold">C</span>ontinual <span id="id13.id1.5" class="ltx_text ltx_font_bold">L</span>earning benchmark for <span id="id13.id1.6" class="ltx_text ltx_font_bold">Cross</span>-domain <span id="id13.id1.7" class="ltx_text ltx_font_bold">V</span>isual <span id="id13.id1.8" class="ltx_text ltx_font_bold">Q</span>uestion <span id="id13.id1.9" class="ltx_text ltx_font_bold">A</span>nswering, through which we conduct extensive experiments on 4 VLPMs, 4 CL approaches, and 5 VQA datasets from different domains. In addition, by probing the forgetting phenomenon of the intermediate layers, we provide insights into how model architecture affects CL performance, why CL approaches can help mitigate forgetting in VLPMs to some extent, and how to design CL approaches suitable for VLPMs in this challenging continual learning environment. To facilitate future work on CL for cross-domain VQA, we will release our datasets and code.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2211.10567/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="258" height="240" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">Illustration of CL-CrossVQA benchmark. Each VQA task is framed as a classification problem. The VQA system is trained on successive tasks in different domains and evaluated on all encountered tasks. The number of candidate answers, <em id="S1.F1.5.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.F1.5.2.2" class="ltx_text"></span>, classes, is shown between the image and the question.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is an exhilarating reasoning task at the crossroads of Natural Language Processing (NLP) and Computer Vision (CV), with the objective of building a system that responds to natural language inquiries regarding an image. The challenge of VQA stems from the necessity to comprehend the semantic information in the textual and visual channels and their interplay.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Large-scale Vision-and-Language Pre-trained Models (VLPMs) have been shown to be a viable part of modern VQA systems, as it yields state-of-the-art results by fine-tuning a single VQA dataset. In reality, however, the application scenario changes over time, requiring the VQA system to cope with non-stationary data, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, data from new domains become available over time. A trivial workaround is to repeat the training process each time a new domain is encountered. However, training large VLPMs is computationally intensive and requires a vast amount of training data, which might not be available in the application domain of interest. Moreover, the rapidly growing model size of VLPMs makes it impractical to store a new model for each incoming task. Thus, VLPMs are required to continually learn new concepts without forgetting previously acquired knowledge. Concretely, VLPMs need to maintain a balance between plasticity, <em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.4" class="ltx_text"></span>, the ability to acquire new knowledge, and stability, <em id="S1.p2.1.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.6" class="ltx_text"></span>, the ability to retain prior knowledge, which is known as the <span id="S1.p2.1.7" class="ltx_text ltx_font_italic">stability-plasticity dilemma</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In contrast to CV and NLP, CL is still underexplored in the field of Vision-and-Language (V-L) learning. While a few works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> address VQA in a continual learning environment, they focus on single-domain VQA tasks. However, the more practical application scenario, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p3.1.2" class="ltx_text"></span>, CL on cross-domain VQA, has not been studied yet.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To systematically investigate how large-scale VLPMs learn a stream of cross-domain VQA tasks, we construct <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">CL-CrossVQA</span>, a <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">C</span>ontinual <span id="S1.p4.1.3" class="ltx_text ltx_font_bold">L</span>earning benchmark for <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">Cross</span>-domain <span id="S1.p4.1.5" class="ltx_text ltx_font_bold">V</span>isual <span id="S1.p4.1.6" class="ltx_text ltx_font_bold">Q</span>uestion <span id="S1.p4.1.7" class="ltx_text ltx_font_bold">A</span>nswering, which covers 4 large-scale VLPMs, 4 CL approaches, and 5 cross-domain VQA datasets. Through comprehensive analysis of a variety of VLPMs and CL methods, we investigate how model design along multiple dimensions, <em id="S1.p4.1.8" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p4.1.9" class="ltx_text"></span>, text encoder, vision encoder, multimodal fusion module, affects CL performance, and which type of CL approach is more effective for VLPMs. Additionally, by dissecting the inner architecture of VLPMs using intermediate representation probing, we provide insights into
why CL approaches can help mitigate forgetting in VLPMs to some extent, and how to design CL approaches suitable for VLPMs in this challenging, yet practical continual learning environment.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our key observations are as follows: (1) All VLPMs suffer from severe forgetting. Dual-stream encoder-decoder architecture exhibits alleviation of forgetting compared to single-stream encoder-only models; (2) For single-stream encoder-only models, the deeper layers are most affected by forgetting. For the dual-stream encoder-decoder model, the multimodal fusion module is less affected by forgetting; (3) Different VLPMs require different CL approaches to tackle the <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">stability-plasticity dilemma</span>. Replay-based approaches are the most effective for VLPMs and are less sensitive to task orders; (4) For replay-based approaches, the sampling strategy used for the replay buffer has a great impact on CL performance; (5) Adapter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is a strong competitor in this cross-domain multimodal continual learning scenario.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our work lies at the intersection of VLPMs, VQA, cross-domain learning, and CL. We provide here a compendious overview of the relevant background.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Vision-and-Language Pre-trained Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In recent years, pre-trained models have progressed at a dizzying speed in tandem with the evolution of transformers. Due to the significance of single-modal language/vision pre-training, some pioneering works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> have recently attempted to explore the joint representation of language and vision by pre-training large-scale models on vision and language modalities, which are referred to as Vision-and-Language Pre-trained Models (VLPMs). There are three primary components in VLPMs, namely vision encoder (VE), text encoder (TE), and modality fusion module (MF). VE and TE are pre-trained with images and texts, respectively. MF is pre-trained using image-text pairs and amalgamates the output embeddings of VE and TE. While almost all VLPMs extract text embeddings from a pre-trained language model, <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p1.1.2" class="ltx_text"></span>, BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, the pre-trained model used on the vision side is different. Unlike most existing VLPMs, which are built upon region-level features extracted from pre-trained object detectors, <em id="S2.SS1.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p1.1.4" class="ltx_text"></span>, Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. All VLPMs evaluated in our work directly utilize patch features without bounding box annotations, which allows them to escape from capacity limitations due to the imperfections of the object detectors.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.2" class="ltx_inline-block ltx_transformed_outer" style="width:306.0pt;height:70.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-82.4pt,18.9pt) scale(0.65,0.65) ;">
<table id="S2.T1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.2.1.1" class="ltx_tr">
<td id="S2.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S2.T1.2.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S2.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Vision</td>
<td id="S2.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Text</td>
<td id="S2.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Modality Fusion</td>
<td id="S2.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S2.T1.2.1.1.5.1" class="ltx_text">Decoder</span></td>
<td id="S2.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Pre-training</td>
</tr>
<tr id="S2.T1.2.1.2" class="ltx_tr">
<td id="S2.T1.2.1.2.1" class="ltx_td ltx_align_center">Encoder</td>
<td id="S2.T1.2.1.2.2" class="ltx_td ltx_align_center">Encoder</td>
<td id="S2.T1.2.1.2.3" class="ltx_td ltx_align_center">Module</td>
<td id="S2.T1.2.1.2.4" class="ltx_td ltx_align_center">Dataset Size</td>
</tr>
<tr id="S2.T1.2.1.3" class="ltx_tr">
<td id="S2.T1.2.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S2.T1.2.1.3.2" class="ltx_td ltx_align_center ltx_border_t">Patch Emb.</td>
<td id="S2.T1.2.1.3.3" class="ltx_td ltx_align_center ltx_border_t">Emb.</td>
<td id="S2.T1.2.1.3.4" class="ltx_td ltx_align_center ltx_border_t">Single-stream</td>
<td id="S2.T1.2.1.3.5" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S2.T1.2.1.3.6" class="ltx_td ltx_align_center ltx_border_t">5M</td>
</tr>
<tr id="S2.T1.2.1.4" class="ltx_tr">
<td id="S2.T1.2.1.4.1" class="ltx_td ltx_align_center ltx_border_r">VAuLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S2.T1.2.1.4.2" class="ltx_td ltx_align_center">Patch Emb.</td>
<td id="S2.T1.2.1.4.3" class="ltx_td ltx_align_center">BERT</td>
<td id="S2.T1.2.1.4.4" class="ltx_td ltx_align_center">Single-stream</td>
<td id="S2.T1.2.1.4.5" class="ltx_td ltx_align_center">✗</td>
<td id="S2.T1.2.1.4.6" class="ltx_td ltx_align_center">5M</td>
</tr>
<tr id="S2.T1.2.1.5" class="ltx_tr">
<td id="S2.T1.2.1.5.1" class="ltx_td ltx_align_center ltx_border_r">FLAVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S2.T1.2.1.5.2" class="ltx_td ltx_align_center">ViT</td>
<td id="S2.T1.2.1.5.3" class="ltx_td ltx_align_center">ViT</td>
<td id="S2.T1.2.1.5.4" class="ltx_td ltx_align_center">Single-stream</td>
<td id="S2.T1.2.1.5.5" class="ltx_td ltx_align_center">✗</td>
<td id="S2.T1.2.1.5.6" class="ltx_td ltx_align_center">70M</td>
</tr>
<tr id="S2.T1.2.1.6" class="ltx_tr">
<td id="S2.T1.2.1.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">ALBEF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S2.T1.2.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">ViT</td>
<td id="S2.T1.2.1.6.3" class="ltx_td ltx_align_center ltx_border_bb">BERT</td>
<td id="S2.T1.2.1.6.4" class="ltx_td ltx_align_center ltx_border_bb">Dual-stream</td>
<td id="S2.T1.2.1.6.5" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S2.T1.2.1.6.6" class="ltx_td ltx_align_center ltx_border_bb">14M</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison of evaluated VLPMs.</span></figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To thoroughly investigate the effects of model design in a CL setting, we evaluate 4 different types of VLPMs, which are summarized in <a href="#S2.T1" title="In 2.1 Vision-and-Language Pre-trained Models ‣ 2 Related Work ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Specifically, <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">ViLT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> directly feeds image patch features and text token embeddings into a pre-trained ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> model that learns the vision-language alignment with self-attention across both modalities. To address the impoverished language representations of ViLT, <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_bold">VAuLT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, an extension of ViLT, propagates the output representations of a large language model, <em id="S2.SS1.p2.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS1.p2.1.4" class="ltx_text"></span>, BERT, to the language input of ViLT. <span id="S2.SS1.p2.1.5" class="ltx_text ltx_font_bold">FLAVA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, a foundational language and vision alignment model, learns strong representations through joint pre-training on both unimodal and multimodal data while encompassing cross-modal alignment objectives and multimodal fusion objectives. FLAVA adopts the ViT architecture for the visual encoder, text encoder, as well as multimodal encoder. Unlike the aforementioned VLPMs, FLAVA is pre-trained on Public Multimodal Datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, which consist of 70M image-text pairs. As opposed to all aforementioned single-stream VLPMs, which learn one joint V+L representation, <span id="S2.SS1.p2.1.6" class="ltx_text ltx_font_bold">ALBEF</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> adopts dual-stream architecture to map vision and language embeddings into the same semantic space. Specifically, ALBEF employs two separate transformers for images, <em id="S2.SS1.p2.1.7" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS1.p2.1.8" class="ltx_text"></span>, ViT, and texts, <em id="S2.SS1.p2.1.9" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS1.p2.1.10" class="ltx_text"></span>, BERT, and introduces a contrastive loss to align the image and text representation before fusing them through cross-modal attention. Furthermore, all aforementioned VLPMs adopt the encoder-only architecture, where the cross-modal representations are directly fed into an output layer to generate the final outputs. ALBEF, on the other hand, utilizes an encoder-decoder architecture, where cross-modal representations are first fed into a decoder and then into an output layer.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Cross-Domain Learning in VQA</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The cross-domain learning remains underinvestigated in VQA, only a few works research in this direction: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> found that domain shift across VQA datasets mostly lies in their questions and answers and proposed an algorithm to align the source and target feature embeddings. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> trained an additional domain discriminator to adversarially penalize the mismatch between the multi-modal embedding from the source domain and the target domain. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> focused on domain shift in the visual space and proposed a domain-adaptive visual feature extractor to better capture image semantics from different domains. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> generated synthetic images and incorporated them in the training by object-level feature swapping across domains.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Continual Learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The main goal of continual learning research is to mitigate the problem of catastrophic forgetting (CF), <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS3.p1.1.2" class="ltx_text"></span>, the tendency of neural networks to forget existing knowledge when learning new tasks with novel input patterns. To tackle this problem, numerous studies have been conducted in different directions.</p>
</div>
<section id="S2.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Rehearsal-based Methods</h5>

<div id="S2.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px1.p1.1" class="ltx_p">To mitigate forgetting, Replay methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> store and replay samples from previous tasks when learning new tasks. Dark Experience Replay (DER and DERPP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> expands ER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> by distilling the dark knowledge from models on previous tasks using replayed samples. Gradient Episodic Memory (GEM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and its lightweight variant A-GEM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> constrain the model to update on new tasks without interfering with the previous tasks. Instead of storing samples from previous tasks, pseudo replay methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> optimize a generative network to produce synthetic samples depicting the knowledge of previous tasks.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Regularization-based Methods</h5>

<div id="S2.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px2.p1.1" class="ltx_p">Regularization-based methods focus on estimating a distribution over the model parameters, and use prior when learning new tasks. Elastic Weight Consolidation (EWC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is the first to estimate the importance of network parameters and penalize parameter drifting accordingly when learning new tasks. Instead of measuring the parameter importance after each task, Synaptic Intelligence (SI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> extends EWC by maintaining an online importance estimation. There are also methods that obtain priors by conducting unsupervised estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> or incrementally merging Gaussian posteriors for the task parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multimodal Continual Learning Benchmarks</h5>

<div id="S2.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px3.p1.1" class="ltx_p">Existing CL on VQA works primarily consider answer- and question-type incremental learning or scene- and function-incremental learning. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> divides CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> into Wh-question and polar question, and examines the effect of task difficulty on CL, and if the sequence in which a kid obtains question types is beneficial to computational models. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> reorganizes the existing VQA dataset to construct CLOVE, which contains two CL setting <em id="S2.SS3.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS3.SSS0.Px3.p1.1.2" class="ltx_text"></span>, scene-incremental and function incremental, to test the ability of the model to adapt to new scenes (<em id="S2.SS3.SSS0.Px3.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS3.SSS0.Px3.p1.1.4" class="ltx_text"></span> ShopAndDinning, Workplace <em id="S2.SS3.SSS0.Px3.p1.1.5" class="ltx_emph ltx_font_italic">etc</em>.<span id="S2.SS3.SSS0.Px3.p1.1.6" class="ltx_text"></span>) and acquire new features (<em id="S2.SS3.SSS0.Px3.p1.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS3.SSS0.Px3.p1.1.8" class="ltx_text"></span> object recognition, attribute recognition <em id="S2.SS3.SSS0.Px3.p1.1.9" class="ltx_emph ltx_font_italic">etc</em>.<span id="S2.SS3.SSS0.Px3.p1.1.10" class="ltx_text"></span>). In addition to these VQA benchmarks, a multimodal multi-task CL benchmark CLiMB<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> has recently been introduced. In this benchmark, VLPMs are trained on a sequence of different V+L tasks.
In this paper, we consider a more practical CL setting, in which a VQA system is required to continually adapt to new domains.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>CL-CrossVQA Benchmark</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We first illustrate the CL-CrossVQA benchmark, then we introduce the metrics to measure the knowledge preservation and transfer abilities of different CL models.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Task Formulation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Given an image-question pair, the VQA system is required to answer the question based on the given image. Following previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, we consider VQA as a classification task and require the model to select a correct answer from a given answer pool, <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p1.1.2" class="ltx_text"></span>, a list of answers. As mentioned before, the concurrent works investigating CL+VQA focus on single-domain VQA tasks, and CL on cross-domain VQA remains unexplored.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.20" class="ltx_p">We formalize our goal of CL on a sequence of <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">K</annotation></semantics></math> VQA tasks <math id="S3.SS1.p2.2.m2.3" class="ltx_Math" alttext="\{T_{1},...,T_{K}\}" display="inline"><semantics id="S3.SS1.p2.2.m2.3a"><mrow id="S3.SS1.p2.2.m2.3.3.2" xref="S3.SS1.p2.2.m2.3.3.3.cmml"><mo stretchy="false" id="S3.SS1.p2.2.m2.3.3.2.3" xref="S3.SS1.p2.2.m2.3.3.3.cmml">{</mo><msub id="S3.SS1.p2.2.m2.2.2.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.2.2.1.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.cmml">T</mi><mn id="S3.SS1.p2.2.m2.2.2.1.1.3" xref="S3.SS1.p2.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.2.m2.3.3.2.4" xref="S3.SS1.p2.2.m2.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p2.2.m2.3.3.2.5" xref="S3.SS1.p2.2.m2.3.3.3.cmml">,</mo><msub id="S3.SS1.p2.2.m2.3.3.2.2" xref="S3.SS1.p2.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.p2.2.m2.3.3.2.2.2" xref="S3.SS1.p2.2.m2.3.3.2.2.2.cmml">T</mi><mi id="S3.SS1.p2.2.m2.3.3.2.2.3" xref="S3.SS1.p2.2.m2.3.3.2.2.3.cmml">K</mi></msub><mo stretchy="false" id="S3.SS1.p2.2.m2.3.3.2.6" xref="S3.SS1.p2.2.m2.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.3b"><set id="S3.SS1.p2.2.m2.3.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3.2"><apply id="S3.SS1.p2.2.m2.2.2.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2">𝑇</ci><cn type="integer" id="S3.SS1.p2.2.m2.2.2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">…</ci><apply id="S3.SS1.p2.2.m2.3.3.2.2.cmml" xref="S3.SS1.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p2.2.m2.3.3.2.2.2">𝑇</ci><ci id="S3.SS1.p2.2.m2.3.3.2.2.3.cmml" xref="S3.SS1.p2.2.m2.3.3.2.2.3">𝐾</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.3c">\{T_{1},...,T_{K}\}</annotation></semantics></math>, where each task <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="T_{k}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">T</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝑇</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">T_{k}</annotation></semantics></math> contains <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">N</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑁</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">N_{k}</annotation></semantics></math> image-question-answer triplets <math id="S3.SS1.p2.5.m5.1" class="ltx_math_unparsed" alttext="\{(v_{k}^{i},q_{k}^{i},a_{k}^{i})|\ i\in\{1,..,N_{k}\}\}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mrow id="S3.SS1.p2.5.m5.1b"><mo stretchy="false" id="S3.SS1.p2.5.m5.1.1">{</mo><mrow id="S3.SS1.p2.5.m5.1.2"><mo stretchy="false" id="S3.SS1.p2.5.m5.1.2.1">(</mo><msubsup id="S3.SS1.p2.5.m5.1.2.2"><mi id="S3.SS1.p2.5.m5.1.2.2.2.2">v</mi><mi id="S3.SS1.p2.5.m5.1.2.2.2.3">k</mi><mi id="S3.SS1.p2.5.m5.1.2.2.3">i</mi></msubsup><mo id="S3.SS1.p2.5.m5.1.2.3">,</mo><msubsup id="S3.SS1.p2.5.m5.1.2.4"><mi id="S3.SS1.p2.5.m5.1.2.4.2.2">q</mi><mi id="S3.SS1.p2.5.m5.1.2.4.2.3">k</mi><mi id="S3.SS1.p2.5.m5.1.2.4.3">i</mi></msubsup><mo id="S3.SS1.p2.5.m5.1.2.5">,</mo><msubsup id="S3.SS1.p2.5.m5.1.2.6"><mi id="S3.SS1.p2.5.m5.1.2.6.2.2">a</mi><mi id="S3.SS1.p2.5.m5.1.2.6.2.3">k</mi><mi id="S3.SS1.p2.5.m5.1.2.6.3">i</mi></msubsup><mo stretchy="false" id="S3.SS1.p2.5.m5.1.2.7">)</mo></mrow><mo fence="false" rspace="0.667em" stretchy="false" id="S3.SS1.p2.5.m5.1.3">|</mo><mi id="S3.SS1.p2.5.m5.1.4">i</mi><mo id="S3.SS1.p2.5.m5.1.5">∈</mo><mrow id="S3.SS1.p2.5.m5.1.6"><mo stretchy="false" id="S3.SS1.p2.5.m5.1.6.1">{</mo><mn id="S3.SS1.p2.5.m5.1.6.2">1</mn><mo id="S3.SS1.p2.5.m5.1.6.3">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS1.p2.5.m5.1.6.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS1.p2.5.m5.1.6.5">.</mo><mo id="S3.SS1.p2.5.m5.1.6.6">,</mo><msub id="S3.SS1.p2.5.m5.1.6.7"><mi id="S3.SS1.p2.5.m5.1.6.7.2">N</mi><mi id="S3.SS1.p2.5.m5.1.6.7.3">k</mi></msub><mo stretchy="false" id="S3.SS1.p2.5.m5.1.6.8">}</mo></mrow><mo stretchy="false" id="S3.SS1.p2.5.m5.1.7">}</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\{(v_{k}^{i},q_{k}^{i},a_{k}^{i})|\ i\in\{1,..,N_{k}\}\}</annotation></semantics></math> from a specific domain <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="D_{k}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">D</mi><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝐷</ci><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">D_{k}</annotation></semantics></math>. Here, domain <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="D_{k}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">D</mi><mi id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">𝐷</ci><ci id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">D_{k}</annotation></semantics></math> is specified by the joint distribution of <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="\mathcal{P}_{VQA}^{k}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><msubsup id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.8.m8.1.1.2.2" xref="S3.SS1.p2.8.m8.1.1.2.2.cmml">𝒫</mi><mrow id="S3.SS1.p2.8.m8.1.1.2.3" xref="S3.SS1.p2.8.m8.1.1.2.3.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2.3.2" xref="S3.SS1.p2.8.m8.1.1.2.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.8.m8.1.1.2.3.1" xref="S3.SS1.p2.8.m8.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.8.m8.1.1.2.3.3" xref="S3.SS1.p2.8.m8.1.1.2.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.8.m8.1.1.2.3.1a" xref="S3.SS1.p2.8.m8.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.8.m8.1.1.2.3.4" xref="S3.SS1.p2.8.m8.1.1.2.3.4.cmml">A</mi></mrow><mi id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">superscript</csymbol><apply id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.2.1.cmml" xref="S3.SS1.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2.2">𝒫</ci><apply id="S3.SS1.p2.8.m8.1.1.2.3.cmml" xref="S3.SS1.p2.8.m8.1.1.2.3"><times id="S3.SS1.p2.8.m8.1.1.2.3.1.cmml" xref="S3.SS1.p2.8.m8.1.1.2.3.1"></times><ci id="S3.SS1.p2.8.m8.1.1.2.3.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2.3.2">𝑉</ci><ci id="S3.SS1.p2.8.m8.1.1.2.3.3.cmml" xref="S3.SS1.p2.8.m8.1.1.2.3.3">𝑄</ci><ci id="S3.SS1.p2.8.m8.1.1.2.3.4.cmml" xref="S3.SS1.p2.8.m8.1.1.2.3.4">𝐴</ci></apply></apply><ci id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">\mathcal{P}_{VQA}^{k}</annotation></semantics></math>, where <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="\mathcal{P}^{k}_{V}" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><msubsup id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.9.m9.1.1.2.2" xref="S3.SS1.p2.9.m9.1.1.2.2.cmml">𝒫</mi><mi id="S3.SS1.p2.9.m9.1.1.3" xref="S3.SS1.p2.9.m9.1.1.3.cmml">V</mi><mi id="S3.SS1.p2.9.m9.1.1.2.3" xref="S3.SS1.p2.9.m9.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><apply id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">subscript</csymbol><apply id="S3.SS1.p2.9.m9.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.1.2.1.cmml" xref="S3.SS1.p2.9.m9.1.1">superscript</csymbol><ci id="S3.SS1.p2.9.m9.1.1.2.2.cmml" xref="S3.SS1.p2.9.m9.1.1.2.2">𝒫</ci><ci id="S3.SS1.p2.9.m9.1.1.2.3.cmml" xref="S3.SS1.p2.9.m9.1.1.2.3">𝑘</ci></apply><ci id="S3.SS1.p2.9.m9.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">\mathcal{P}^{k}_{V}</annotation></semantics></math>, <math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="\mathcal{P}^{k}_{Q}" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><msubsup id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.10.m10.1.1.2.2" xref="S3.SS1.p2.10.m10.1.1.2.2.cmml">𝒫</mi><mi id="S3.SS1.p2.10.m10.1.1.3" xref="S3.SS1.p2.10.m10.1.1.3.cmml">Q</mi><mi id="S3.SS1.p2.10.m10.1.1.2.3" xref="S3.SS1.p2.10.m10.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><apply id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">subscript</csymbol><apply id="S3.SS1.p2.10.m10.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.2.1.cmml" xref="S3.SS1.p2.10.m10.1.1">superscript</csymbol><ci id="S3.SS1.p2.10.m10.1.1.2.2.cmml" xref="S3.SS1.p2.10.m10.1.1.2.2">𝒫</ci><ci id="S3.SS1.p2.10.m10.1.1.2.3.cmml" xref="S3.SS1.p2.10.m10.1.1.2.3">𝑘</ci></apply><ci id="S3.SS1.p2.10.m10.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">\mathcal{P}^{k}_{Q}</annotation></semantics></math>, <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="\mathcal{P}^{k}_{A}" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><msubsup id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.11.m11.1.1.2.2" xref="S3.SS1.p2.11.m11.1.1.2.2.cmml">𝒫</mi><mi id="S3.SS1.p2.11.m11.1.1.3" xref="S3.SS1.p2.11.m11.1.1.3.cmml">A</mi><mi id="S3.SS1.p2.11.m11.1.1.2.3" xref="S3.SS1.p2.11.m11.1.1.2.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><apply id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">subscript</csymbol><apply id="S3.SS1.p2.11.m11.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.2.1.cmml" xref="S3.SS1.p2.11.m11.1.1">superscript</csymbol><ci id="S3.SS1.p2.11.m11.1.1.2.2.cmml" xref="S3.SS1.p2.11.m11.1.1.2.2">𝒫</ci><ci id="S3.SS1.p2.11.m11.1.1.2.3.cmml" xref="S3.SS1.p2.11.m11.1.1.2.3">𝑘</ci></apply><ci id="S3.SS1.p2.11.m11.1.1.3.cmml" xref="S3.SS1.p2.11.m11.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">\mathcal{P}^{k}_{A}</annotation></semantics></math> are the marginal distribution of the images, questions, and answers from that domain, respectively. We define our cross-domain learning by setting <math id="S3.SS1.p2.12.m12.1" class="ltx_Math" alttext="\mathcal{P}_{VQA}^{k}\neq\mathcal{P}_{VQA}^{j}" display="inline"><semantics id="S3.SS1.p2.12.m12.1a"><mrow id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml"><msubsup id="S3.SS1.p2.12.m12.1.1.2" xref="S3.SS1.p2.12.m12.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.12.m12.1.1.2.2.2" xref="S3.SS1.p2.12.m12.1.1.2.2.2.cmml">𝒫</mi><mrow id="S3.SS1.p2.12.m12.1.1.2.2.3" xref="S3.SS1.p2.12.m12.1.1.2.2.3.cmml"><mi id="S3.SS1.p2.12.m12.1.1.2.2.3.2" xref="S3.SS1.p2.12.m12.1.1.2.2.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.12.m12.1.1.2.2.3.1" xref="S3.SS1.p2.12.m12.1.1.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.12.m12.1.1.2.2.3.3" xref="S3.SS1.p2.12.m12.1.1.2.2.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.12.m12.1.1.2.2.3.1a" xref="S3.SS1.p2.12.m12.1.1.2.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.12.m12.1.1.2.2.3.4" xref="S3.SS1.p2.12.m12.1.1.2.2.3.4.cmml">A</mi></mrow><mi id="S3.SS1.p2.12.m12.1.1.2.3" xref="S3.SS1.p2.12.m12.1.1.2.3.cmml">k</mi></msubsup><mo id="S3.SS1.p2.12.m12.1.1.1" xref="S3.SS1.p2.12.m12.1.1.1.cmml">≠</mo><msubsup id="S3.SS1.p2.12.m12.1.1.3" xref="S3.SS1.p2.12.m12.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.12.m12.1.1.3.2.2" xref="S3.SS1.p2.12.m12.1.1.3.2.2.cmml">𝒫</mi><mrow id="S3.SS1.p2.12.m12.1.1.3.2.3" xref="S3.SS1.p2.12.m12.1.1.3.2.3.cmml"><mi id="S3.SS1.p2.12.m12.1.1.3.2.3.2" xref="S3.SS1.p2.12.m12.1.1.3.2.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.12.m12.1.1.3.2.3.1" xref="S3.SS1.p2.12.m12.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.12.m12.1.1.3.2.3.3" xref="S3.SS1.p2.12.m12.1.1.3.2.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.12.m12.1.1.3.2.3.1a" xref="S3.SS1.p2.12.m12.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS1.p2.12.m12.1.1.3.2.3.4" xref="S3.SS1.p2.12.m12.1.1.3.2.3.4.cmml">A</mi></mrow><mi id="S3.SS1.p2.12.m12.1.1.3.3" xref="S3.SS1.p2.12.m12.1.1.3.3.cmml">j</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><apply id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1"><neq id="S3.SS1.p2.12.m12.1.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1.1"></neq><apply id="S3.SS1.p2.12.m12.1.1.2.cmml" xref="S3.SS1.p2.12.m12.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m12.1.1.2.1.cmml" xref="S3.SS1.p2.12.m12.1.1.2">superscript</csymbol><apply id="S3.SS1.p2.12.m12.1.1.2.2.cmml" xref="S3.SS1.p2.12.m12.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m12.1.1.2.2.1.cmml" xref="S3.SS1.p2.12.m12.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.12.m12.1.1.2.2.2.cmml" xref="S3.SS1.p2.12.m12.1.1.2.2.2">𝒫</ci><apply id="S3.SS1.p2.12.m12.1.1.2.2.3.cmml" xref="S3.SS1.p2.12.m12.1.1.2.2.3"><times id="S3.SS1.p2.12.m12.1.1.2.2.3.1.cmml" xref="S3.SS1.p2.12.m12.1.1.2.2.3.1"></times><ci id="S3.SS1.p2.12.m12.1.1.2.2.3.2.cmml" xref="S3.SS1.p2.12.m12.1.1.2.2.3.2">𝑉</ci><ci id="S3.SS1.p2.12.m12.1.1.2.2.3.3.cmml" xref="S3.SS1.p2.12.m12.1.1.2.2.3.3">𝑄</ci><ci id="S3.SS1.p2.12.m12.1.1.2.2.3.4.cmml" xref="S3.SS1.p2.12.m12.1.1.2.2.3.4">𝐴</ci></apply></apply><ci id="S3.SS1.p2.12.m12.1.1.2.3.cmml" xref="S3.SS1.p2.12.m12.1.1.2.3">𝑘</ci></apply><apply id="S3.SS1.p2.12.m12.1.1.3.cmml" xref="S3.SS1.p2.12.m12.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m12.1.1.3.1.cmml" xref="S3.SS1.p2.12.m12.1.1.3">superscript</csymbol><apply id="S3.SS1.p2.12.m12.1.1.3.2.cmml" xref="S3.SS1.p2.12.m12.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m12.1.1.3.2.1.cmml" xref="S3.SS1.p2.12.m12.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.12.m12.1.1.3.2.2.cmml" xref="S3.SS1.p2.12.m12.1.1.3.2.2">𝒫</ci><apply id="S3.SS1.p2.12.m12.1.1.3.2.3.cmml" xref="S3.SS1.p2.12.m12.1.1.3.2.3"><times id="S3.SS1.p2.12.m12.1.1.3.2.3.1.cmml" xref="S3.SS1.p2.12.m12.1.1.3.2.3.1"></times><ci id="S3.SS1.p2.12.m12.1.1.3.2.3.2.cmml" xref="S3.SS1.p2.12.m12.1.1.3.2.3.2">𝑉</ci><ci id="S3.SS1.p2.12.m12.1.1.3.2.3.3.cmml" xref="S3.SS1.p2.12.m12.1.1.3.2.3.3">𝑄</ci><ci id="S3.SS1.p2.12.m12.1.1.3.2.3.4.cmml" xref="S3.SS1.p2.12.m12.1.1.3.2.3.4">𝐴</ci></apply></apply><ci id="S3.SS1.p2.12.m12.1.1.3.3.cmml" xref="S3.SS1.p2.12.m12.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">\mathcal{P}_{VQA}^{k}\neq\mathcal{P}_{VQA}^{j}</annotation></semantics></math> with <math id="S3.SS1.p2.13.m13.1" class="ltx_math_unparsed" alttext="\forall j,k\in\{1,..,K\}" display="inline"><semantics id="S3.SS1.p2.13.m13.1a"><mrow id="S3.SS1.p2.13.m13.1b"><mo rspace="0.167em" id="S3.SS1.p2.13.m13.1.2">∀</mo><mi id="S3.SS1.p2.13.m13.1.3">j</mi><mo id="S3.SS1.p2.13.m13.1.4">,</mo><mi id="S3.SS1.p2.13.m13.1.1">k</mi><mo id="S3.SS1.p2.13.m13.1.5">∈</mo><mrow id="S3.SS1.p2.13.m13.1.6"><mo stretchy="false" id="S3.SS1.p2.13.m13.1.6.1">{</mo><mn id="S3.SS1.p2.13.m13.1.6.2">1</mn><mo id="S3.SS1.p2.13.m13.1.6.3">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS1.p2.13.m13.1.6.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS1.p2.13.m13.1.6.5">.</mo><mo id="S3.SS1.p2.13.m13.1.6.6">,</mo><mi id="S3.SS1.p2.13.m13.1.6.7">K</mi><mo stretchy="false" id="S3.SS1.p2.13.m13.1.6.8">}</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.1c">\forall j,k\in\{1,..,K\}</annotation></semantics></math>, where <math id="S3.SS1.p2.14.m14.1" class="ltx_Math" alttext="j\neq k" display="inline"><semantics id="S3.SS1.p2.14.m14.1a"><mrow id="S3.SS1.p2.14.m14.1.1" xref="S3.SS1.p2.14.m14.1.1.cmml"><mi id="S3.SS1.p2.14.m14.1.1.2" xref="S3.SS1.p2.14.m14.1.1.2.cmml">j</mi><mo id="S3.SS1.p2.14.m14.1.1.1" xref="S3.SS1.p2.14.m14.1.1.1.cmml">≠</mo><mi id="S3.SS1.p2.14.m14.1.1.3" xref="S3.SS1.p2.14.m14.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m14.1b"><apply id="S3.SS1.p2.14.m14.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1"><neq id="S3.SS1.p2.14.m14.1.1.1.cmml" xref="S3.SS1.p2.14.m14.1.1.1"></neq><ci id="S3.SS1.p2.14.m14.1.1.2.cmml" xref="S3.SS1.p2.14.m14.1.1.2">𝑗</ci><ci id="S3.SS1.p2.14.m14.1.1.3.cmml" xref="S3.SS1.p2.14.m14.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m14.1c">j\neq k</annotation></semantics></math>. The answer pool <math id="S3.SS1.p2.15.m15.1" class="ltx_Math" alttext="A_{k}" display="inline"><semantics id="S3.SS1.p2.15.m15.1a"><msub id="S3.SS1.p2.15.m15.1.1" xref="S3.SS1.p2.15.m15.1.1.cmml"><mi id="S3.SS1.p2.15.m15.1.1.2" xref="S3.SS1.p2.15.m15.1.1.2.cmml">A</mi><mi id="S3.SS1.p2.15.m15.1.1.3" xref="S3.SS1.p2.15.m15.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m15.1b"><apply id="S3.SS1.p2.15.m15.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m15.1.1.1.cmml" xref="S3.SS1.p2.15.m15.1.1">subscript</csymbol><ci id="S3.SS1.p2.15.m15.1.1.2.cmml" xref="S3.SS1.p2.15.m15.1.1.2">𝐴</ci><ci id="S3.SS1.p2.15.m15.1.1.3.cmml" xref="S3.SS1.p2.15.m15.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m15.1c">A_{k}</annotation></semantics></math> of each task <math id="S3.SS1.p2.16.m16.1" class="ltx_Math" alttext="T_{k}" display="inline"><semantics id="S3.SS1.p2.16.m16.1a"><msub id="S3.SS1.p2.16.m16.1.1" xref="S3.SS1.p2.16.m16.1.1.cmml"><mi id="S3.SS1.p2.16.m16.1.1.2" xref="S3.SS1.p2.16.m16.1.1.2.cmml">T</mi><mi id="S3.SS1.p2.16.m16.1.1.3" xref="S3.SS1.p2.16.m16.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.16.m16.1b"><apply id="S3.SS1.p2.16.m16.1.1.cmml" xref="S3.SS1.p2.16.m16.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.16.m16.1.1.1.cmml" xref="S3.SS1.p2.16.m16.1.1">subscript</csymbol><ci id="S3.SS1.p2.16.m16.1.1.2.cmml" xref="S3.SS1.p2.16.m16.1.1.2">𝑇</ci><ci id="S3.SS1.p2.16.m16.1.1.3.cmml" xref="S3.SS1.p2.16.m16.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.16.m16.1c">T_{k}</annotation></semantics></math> include <math id="S3.SS1.p2.17.m17.1" class="ltx_Math" alttext="C_{k}" display="inline"><semantics id="S3.SS1.p2.17.m17.1a"><msub id="S3.SS1.p2.17.m17.1.1" xref="S3.SS1.p2.17.m17.1.1.cmml"><mi id="S3.SS1.p2.17.m17.1.1.2" xref="S3.SS1.p2.17.m17.1.1.2.cmml">C</mi><mi id="S3.SS1.p2.17.m17.1.1.3" xref="S3.SS1.p2.17.m17.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.17.m17.1b"><apply id="S3.SS1.p2.17.m17.1.1.cmml" xref="S3.SS1.p2.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.17.m17.1.1.1.cmml" xref="S3.SS1.p2.17.m17.1.1">subscript</csymbol><ci id="S3.SS1.p2.17.m17.1.1.2.cmml" xref="S3.SS1.p2.17.m17.1.1.2">𝐶</ci><ci id="S3.SS1.p2.17.m17.1.1.3.cmml" xref="S3.SS1.p2.17.m17.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.17.m17.1c">C_{k}</annotation></semantics></math> ground-truth answers, <em id="S3.SS1.p2.20.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p2.20.2" class="ltx_text"></span>, <math id="S3.SS1.p2.18.m18.3" class="ltx_Math" alttext="A_{k}=\{a_{k}^{1},...,a_{k}^{C_{k}}\}" display="inline"><semantics id="S3.SS1.p2.18.m18.3a"><mrow id="S3.SS1.p2.18.m18.3.3" xref="S3.SS1.p2.18.m18.3.3.cmml"><msub id="S3.SS1.p2.18.m18.3.3.4" xref="S3.SS1.p2.18.m18.3.3.4.cmml"><mi id="S3.SS1.p2.18.m18.3.3.4.2" xref="S3.SS1.p2.18.m18.3.3.4.2.cmml">A</mi><mi id="S3.SS1.p2.18.m18.3.3.4.3" xref="S3.SS1.p2.18.m18.3.3.4.3.cmml">k</mi></msub><mo id="S3.SS1.p2.18.m18.3.3.3" xref="S3.SS1.p2.18.m18.3.3.3.cmml">=</mo><mrow id="S3.SS1.p2.18.m18.3.3.2.2" xref="S3.SS1.p2.18.m18.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS1.p2.18.m18.3.3.2.2.3" xref="S3.SS1.p2.18.m18.3.3.2.3.cmml">{</mo><msubsup id="S3.SS1.p2.18.m18.2.2.1.1.1" xref="S3.SS1.p2.18.m18.2.2.1.1.1.cmml"><mi id="S3.SS1.p2.18.m18.2.2.1.1.1.2.2" xref="S3.SS1.p2.18.m18.2.2.1.1.1.2.2.cmml">a</mi><mi id="S3.SS1.p2.18.m18.2.2.1.1.1.2.3" xref="S3.SS1.p2.18.m18.2.2.1.1.1.2.3.cmml">k</mi><mn id="S3.SS1.p2.18.m18.2.2.1.1.1.3" xref="S3.SS1.p2.18.m18.2.2.1.1.1.3.cmml">1</mn></msubsup><mo id="S3.SS1.p2.18.m18.3.3.2.2.4" xref="S3.SS1.p2.18.m18.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.18.m18.1.1" xref="S3.SS1.p2.18.m18.1.1.cmml">…</mi><mo id="S3.SS1.p2.18.m18.3.3.2.2.5" xref="S3.SS1.p2.18.m18.3.3.2.3.cmml">,</mo><msubsup id="S3.SS1.p2.18.m18.3.3.2.2.2" xref="S3.SS1.p2.18.m18.3.3.2.2.2.cmml"><mi id="S3.SS1.p2.18.m18.3.3.2.2.2.2.2" xref="S3.SS1.p2.18.m18.3.3.2.2.2.2.2.cmml">a</mi><mi id="S3.SS1.p2.18.m18.3.3.2.2.2.2.3" xref="S3.SS1.p2.18.m18.3.3.2.2.2.2.3.cmml">k</mi><msub id="S3.SS1.p2.18.m18.3.3.2.2.2.3" xref="S3.SS1.p2.18.m18.3.3.2.2.2.3.cmml"><mi id="S3.SS1.p2.18.m18.3.3.2.2.2.3.2" xref="S3.SS1.p2.18.m18.3.3.2.2.2.3.2.cmml">C</mi><mi id="S3.SS1.p2.18.m18.3.3.2.2.2.3.3" xref="S3.SS1.p2.18.m18.3.3.2.2.2.3.3.cmml">k</mi></msub></msubsup><mo stretchy="false" id="S3.SS1.p2.18.m18.3.3.2.2.6" xref="S3.SS1.p2.18.m18.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.18.m18.3b"><apply id="S3.SS1.p2.18.m18.3.3.cmml" xref="S3.SS1.p2.18.m18.3.3"><eq id="S3.SS1.p2.18.m18.3.3.3.cmml" xref="S3.SS1.p2.18.m18.3.3.3"></eq><apply id="S3.SS1.p2.18.m18.3.3.4.cmml" xref="S3.SS1.p2.18.m18.3.3.4"><csymbol cd="ambiguous" id="S3.SS1.p2.18.m18.3.3.4.1.cmml" xref="S3.SS1.p2.18.m18.3.3.4">subscript</csymbol><ci id="S3.SS1.p2.18.m18.3.3.4.2.cmml" xref="S3.SS1.p2.18.m18.3.3.4.2">𝐴</ci><ci id="S3.SS1.p2.18.m18.3.3.4.3.cmml" xref="S3.SS1.p2.18.m18.3.3.4.3">𝑘</ci></apply><set id="S3.SS1.p2.18.m18.3.3.2.3.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2"><apply id="S3.SS1.p2.18.m18.2.2.1.1.1.cmml" xref="S3.SS1.p2.18.m18.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.18.m18.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.18.m18.2.2.1.1.1">superscript</csymbol><apply id="S3.SS1.p2.18.m18.2.2.1.1.1.2.cmml" xref="S3.SS1.p2.18.m18.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.18.m18.2.2.1.1.1.2.1.cmml" xref="S3.SS1.p2.18.m18.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.18.m18.2.2.1.1.1.2.2.cmml" xref="S3.SS1.p2.18.m18.2.2.1.1.1.2.2">𝑎</ci><ci id="S3.SS1.p2.18.m18.2.2.1.1.1.2.3.cmml" xref="S3.SS1.p2.18.m18.2.2.1.1.1.2.3">𝑘</ci></apply><cn type="integer" id="S3.SS1.p2.18.m18.2.2.1.1.1.3.cmml" xref="S3.SS1.p2.18.m18.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS1.p2.18.m18.1.1.cmml" xref="S3.SS1.p2.18.m18.1.1">…</ci><apply id="S3.SS1.p2.18.m18.3.3.2.2.2.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.18.m18.3.3.2.2.2.1.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2">superscript</csymbol><apply id="S3.SS1.p2.18.m18.3.3.2.2.2.2.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.18.m18.3.3.2.2.2.2.1.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.p2.18.m18.3.3.2.2.2.2.2.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2.2.2">𝑎</ci><ci id="S3.SS1.p2.18.m18.3.3.2.2.2.2.3.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2.2.3">𝑘</ci></apply><apply id="S3.SS1.p2.18.m18.3.3.2.2.2.3.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.18.m18.3.3.2.2.2.3.1.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2.3">subscript</csymbol><ci id="S3.SS1.p2.18.m18.3.3.2.2.2.3.2.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2.3.2">𝐶</ci><ci id="S3.SS1.p2.18.m18.3.3.2.2.2.3.3.cmml" xref="S3.SS1.p2.18.m18.3.3.2.2.2.3.3">𝑘</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.18.m18.3c">A_{k}=\{a_{k}^{1},...,a_{k}^{C_{k}}\}</annotation></semantics></math>. Note that, the size of the answer pool could be different for different tasks. The goal is to optimize a set of parameters <math id="S3.SS1.p2.19.m19.1" class="ltx_Math" alttext="\widetilde{\Theta}" display="inline"><semantics id="S3.SS1.p2.19.m19.1a"><mover accent="true" id="S3.SS1.p2.19.m19.1.1" xref="S3.SS1.p2.19.m19.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p2.19.m19.1.1.2" xref="S3.SS1.p2.19.m19.1.1.2.cmml">Θ</mi><mo id="S3.SS1.p2.19.m19.1.1.1" xref="S3.SS1.p2.19.m19.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.19.m19.1b"><apply id="S3.SS1.p2.19.m19.1.1.cmml" xref="S3.SS1.p2.19.m19.1.1"><ci id="S3.SS1.p2.19.m19.1.1.1.cmml" xref="S3.SS1.p2.19.m19.1.1.1">~</ci><ci id="S3.SS1.p2.19.m19.1.1.2.cmml" xref="S3.SS1.p2.19.m19.1.1.2">Θ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.19.m19.1c">\widetilde{\Theta}</annotation></semantics></math> of the learner towards an objective on each task <math id="S3.SS1.p2.20.m20.1" class="ltx_Math" alttext="T_{k}" display="inline"><semantics id="S3.SS1.p2.20.m20.1a"><msub id="S3.SS1.p2.20.m20.1.1" xref="S3.SS1.p2.20.m20.1.1.cmml"><mi id="S3.SS1.p2.20.m20.1.1.2" xref="S3.SS1.p2.20.m20.1.1.2.cmml">T</mi><mi id="S3.SS1.p2.20.m20.1.1.3" xref="S3.SS1.p2.20.m20.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.20.m20.1b"><apply id="S3.SS1.p2.20.m20.1.1.cmml" xref="S3.SS1.p2.20.m20.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.20.m20.1.1.1.cmml" xref="S3.SS1.p2.20.m20.1.1">subscript</csymbol><ci id="S3.SS1.p2.20.m20.1.1.2.cmml" xref="S3.SS1.p2.20.m20.1.1.2">𝑇</ci><ci id="S3.SS1.p2.20.m20.1.1.3.cmml" xref="S3.SS1.p2.20.m20.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.20.m20.1c">T_{k}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Datasets</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">CL-CrossVQA consists of 5 datasets across different domains, namely the abstract domain, <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p1.1.2" class="ltx_text"></span>, VQA Abstract<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the general domain, <em id="S3.SS2.p1.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p1.1.4" class="ltx_text"></span>, Toronto COCO QA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, the artistic domain, <em id="S3.SS2.p1.1.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p1.1.6" class="ltx_text"></span>, AQUA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, the pathological domain, <em id="S3.SS2.p1.1.7" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p1.1.8" class="ltx_text"></span>, PathVQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and the radiological domain, <em id="S3.SS2.p1.1.9" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p1.1.10" class="ltx_text"></span>, VQA-Med-2019<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Examples from these datasets are provided in <a href="#S1.F1" title="In 1 Introduction ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">VQA Abstract</span> consists of images of abstract scenes.
Each image is accompanied by three questions, with 10 human-provided ground truth answers, ensuring a diverse and interesting set of questions and answers. By its abstraction, it requires high-level reasoning skills for VQA.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">COCO QA</span> contains questions automatically generated from captions in Microsoft COCO dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and therefore may contain syntactic and semantic errors. There are in total 78,736 training questions and 38,948 test questions, of which the answers are all one word.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">AQUA</span> Artwork has been an important part of human history. Different styles of artwork such as naturalism, realism, and cubism represent different levels of abstraction, posing an extra challenge for VQA system. Aiming to investigate this, AQUA is built on the SemArt dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and its QA pairs are generated from the associated comments of the paintings. We use the Visual QA subset from AQUA, including 29,568, 1,507, and 1,270 question-answer (QA) pairs for training, validation, and testing, respectively.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">PathVQA</span> is created with the goal to develop an AI pathologist and foster research in pathological VQA. It consists of 32,799 question-answer pairs automatically generated from captions of 4998 pathology images in pathology textbooks and online
digital libraries. We note that the answers in PathVQA can be long, clinical expressions, adding to the difficulty of the questions.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">VQA-Med-2019</span> aims to support clinical education, clinical decision, and patient education. It focuses on radiology images selected from the MedPix database and the QA pairs are generated based on patterns from naturally asked and validated questions by medical students. There are 3,200 images and 12,792 QA pairs for the training set and 500 images with 2,000 QA pairs for the validation set. The test set includes 500 images with 500 questions.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">To avoid bias caused by different dataset sizes, we downsample the training sets of all datasets to be consistent with VQA-Med-2019 while maintaining their original class distributions. The same train-test split ratio is adopted for all datasets. Detailed dataset pre-processing is provided in the Appendix. To better reflect the challenging real-world scenarios, the selected cross-domain datasets depict minor overlap in the answer space, <em id="S3.SS2.p7.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p7.1.2" class="ltx_text"></span>, only a few answers are shared across datasets.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Metrics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To evaluate the knowledge transfer ability of the models and their possible catastrophic forgetting of the previously learned tasks, we assume access to the test set of each task and report the following three evaluation metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.4" class="ltx_Math" alttext="\textbf{Forward Transfer}\quad FWT=\frac{1}{T-1}\sum_{i=1}^{T-1}S_{i,i+1}-\overline{b}_{i+1}," display="block"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.4.1" xref="S3.Ex1.m1.4.4.1.1.cmml"><mrow id="S3.Ex1.m1.4.4.1.1" xref="S3.Ex1.m1.4.4.1.1.cmml"><mrow id="S3.Ex1.m1.4.4.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3a.cmml">Forward Transfer</mtext><mspace width="1em" id="S3.Ex1.m1.4.4.1.1.1.1.2" xref="S3.Ex1.m1.4.4.1.1.1.2.cmml"></mspace><mrow id="S3.Ex1.m1.4.4.1.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.4.4.1.1.1.1.1.2" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.1.1.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.Ex1.m1.4.4.1.1.1.1.1.3" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.1.1.1.1.1.1a" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.Ex1.m1.4.4.1.1.1.1.1.4" xref="S3.Ex1.m1.4.4.1.1.1.1.1.4.cmml">T</mi></mrow></mrow><mo id="S3.Ex1.m1.4.4.1.1.2" xref="S3.Ex1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.4.4.1.1.3" xref="S3.Ex1.m1.4.4.1.1.3.cmml"><mrow id="S3.Ex1.m1.4.4.1.1.3.2" xref="S3.Ex1.m1.4.4.1.1.3.2.cmml"><mfrac id="S3.Ex1.m1.4.4.1.1.3.2.2" xref="S3.Ex1.m1.4.4.1.1.3.2.2.cmml"><mn id="S3.Ex1.m1.4.4.1.1.3.2.2.2" xref="S3.Ex1.m1.4.4.1.1.3.2.2.2.cmml">1</mn><mrow id="S3.Ex1.m1.4.4.1.1.3.2.2.3" xref="S3.Ex1.m1.4.4.1.1.3.2.2.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.3.2.2.3.2" xref="S3.Ex1.m1.4.4.1.1.3.2.2.3.2.cmml">T</mi><mo id="S3.Ex1.m1.4.4.1.1.3.2.2.3.1" xref="S3.Ex1.m1.4.4.1.1.3.2.2.3.1.cmml">−</mo><mn id="S3.Ex1.m1.4.4.1.1.3.2.2.3.3" xref="S3.Ex1.m1.4.4.1.1.3.2.2.3.3.cmml">1</mn></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.1.1.3.2.1" xref="S3.Ex1.m1.4.4.1.1.3.2.1.cmml">​</mo><mrow id="S3.Ex1.m1.4.4.1.1.3.2.3" xref="S3.Ex1.m1.4.4.1.1.3.2.3.cmml"><munderover id="S3.Ex1.m1.4.4.1.1.3.2.3.1" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.cmml"><mo movablelimits="false" id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.2" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.2.cmml">∑</mo><mrow id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.2" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.2.cmml">i</mi><mo id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.1" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.1.cmml">=</mo><mn id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.3" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.Ex1.m1.4.4.1.1.3.2.3.1.3" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.2" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.2.cmml">T</mi><mo id="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.1" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.1.cmml">−</mo><mn id="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.3" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.3.cmml">1</mn></mrow></munderover><msub id="S3.Ex1.m1.4.4.1.1.3.2.3.2" xref="S3.Ex1.m1.4.4.1.1.3.2.3.2.cmml"><mi id="S3.Ex1.m1.4.4.1.1.3.2.3.2.2" xref="S3.Ex1.m1.4.4.1.1.3.2.3.2.2.cmml">S</mi><mrow id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">i</mi><mo id="S3.Ex1.m1.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.3.cmml">,</mo><mrow id="S3.Ex1.m1.2.2.2.2.1" xref="S3.Ex1.m1.2.2.2.2.1.cmml"><mi id="S3.Ex1.m1.2.2.2.2.1.2" xref="S3.Ex1.m1.2.2.2.2.1.2.cmml">i</mi><mo id="S3.Ex1.m1.2.2.2.2.1.1" xref="S3.Ex1.m1.2.2.2.2.1.1.cmml">+</mo><mn id="S3.Ex1.m1.2.2.2.2.1.3" xref="S3.Ex1.m1.2.2.2.2.1.3.cmml">1</mn></mrow></mrow></msub></mrow></mrow><mo id="S3.Ex1.m1.4.4.1.1.3.1" xref="S3.Ex1.m1.4.4.1.1.3.1.cmml">−</mo><msub id="S3.Ex1.m1.4.4.1.1.3.3" xref="S3.Ex1.m1.4.4.1.1.3.3.cmml"><mover accent="true" id="S3.Ex1.m1.4.4.1.1.3.3.2" xref="S3.Ex1.m1.4.4.1.1.3.3.2.cmml"><mi id="S3.Ex1.m1.4.4.1.1.3.3.2.2" xref="S3.Ex1.m1.4.4.1.1.3.3.2.2.cmml">b</mi><mo id="S3.Ex1.m1.4.4.1.1.3.3.2.1" xref="S3.Ex1.m1.4.4.1.1.3.3.2.1.cmml">¯</mo></mover><mrow id="S3.Ex1.m1.4.4.1.1.3.3.3" xref="S3.Ex1.m1.4.4.1.1.3.3.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.3.3.3.2" xref="S3.Ex1.m1.4.4.1.1.3.3.3.2.cmml">i</mi><mo id="S3.Ex1.m1.4.4.1.1.3.3.3.1" xref="S3.Ex1.m1.4.4.1.1.3.3.3.1.cmml">+</mo><mn id="S3.Ex1.m1.4.4.1.1.3.3.3.3" xref="S3.Ex1.m1.4.4.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S3.Ex1.m1.4.4.1.2" xref="S3.Ex1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.4.1.1.cmml" xref="S3.Ex1.m1.4.4.1"><eq id="S3.Ex1.m1.4.4.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.2"></eq><list id="S3.Ex1.m1.4.4.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1"><ci id="S3.Ex1.m1.3.3a.cmml" xref="S3.Ex1.m1.3.3"><mtext class="ltx_mathvariant_bold" id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3">Forward Transfer</mtext></ci><apply id="S3.Ex1.m1.4.4.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1"><times id="S3.Ex1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.1"></times><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2">𝐹</ci><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3">𝑊</ci><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.4">𝑇</ci></apply></list><apply id="S3.Ex1.m1.4.4.1.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3"><minus id="S3.Ex1.m1.4.4.1.1.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.1"></minus><apply id="S3.Ex1.m1.4.4.1.1.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2"><times id="S3.Ex1.m1.4.4.1.1.3.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.1"></times><apply id="S3.Ex1.m1.4.4.1.1.3.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.2"><divide id="S3.Ex1.m1.4.4.1.1.3.2.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.2"></divide><cn type="integer" id="S3.Ex1.m1.4.4.1.1.3.2.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.2.2">1</cn><apply id="S3.Ex1.m1.4.4.1.1.3.2.2.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.2.3"><minus id="S3.Ex1.m1.4.4.1.1.3.2.2.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.2.3.1"></minus><ci id="S3.Ex1.m1.4.4.1.1.3.2.2.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.2.3.2">𝑇</ci><cn type="integer" id="S3.Ex1.m1.4.4.1.1.3.2.2.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.2.3.3">1</cn></apply></apply><apply id="S3.Ex1.m1.4.4.1.1.3.2.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3"><apply id="S3.Ex1.m1.4.4.1.1.3.2.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.3.2.3.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1">superscript</csymbol><apply id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1">subscript</csymbol><sum id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.2"></sum><apply id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3"><eq id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.1"></eq><ci id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.2.3.3">1</cn></apply></apply><apply id="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.3"><minus id="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.1"></minus><ci id="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.2">𝑇</ci><cn type="integer" id="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.1.3.3">1</cn></apply></apply><apply id="S3.Ex1.m1.4.4.1.1.3.2.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.3.2.3.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.2">subscript</csymbol><ci id="S3.Ex1.m1.4.4.1.1.3.2.3.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.2.3.2.2">𝑆</ci><list id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2"><ci id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">𝑖</ci><apply id="S3.Ex1.m1.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.1"><plus id="S3.Ex1.m1.2.2.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1"></plus><ci id="S3.Ex1.m1.2.2.2.2.1.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.2">𝑖</ci><cn type="integer" id="S3.Ex1.m1.2.2.2.2.1.3.cmml" xref="S3.Ex1.m1.2.2.2.2.1.3">1</cn></apply></list></apply></apply></apply><apply id="S3.Ex1.m1.4.4.1.1.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.3.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3">subscript</csymbol><apply id="S3.Ex1.m1.4.4.1.1.3.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3.2"><ci id="S3.Ex1.m1.4.4.1.1.3.3.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3.2.1">¯</ci><ci id="S3.Ex1.m1.4.4.1.1.3.3.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3.2.2">𝑏</ci></apply><apply id="S3.Ex1.m1.4.4.1.1.3.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3.3"><plus id="S3.Ex1.m1.4.4.1.1.3.3.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3.3.1"></plus><ci id="S3.Ex1.m1.4.4.1.1.3.3.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3.3.2">𝑖</ci><cn type="integer" id="S3.Ex1.m1.4.4.1.1.3.3.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">\textbf{Forward Transfer}\quad FWT=\frac{1}{T-1}\sum_{i=1}^{T-1}S_{i,i+1}-\overline{b}_{i+1},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.6" class="ltx_Math" alttext="\textbf{Backward Transfer}\quad BWT=\frac{1}{T-1}\sum_{i=1}^{T-1}S_{T,i}-S_{i,i}," display="block"><semantics id="S3.Ex2.m1.6a"><mrow id="S3.Ex2.m1.6.6.1" xref="S3.Ex2.m1.6.6.1.1.cmml"><mrow id="S3.Ex2.m1.6.6.1.1" xref="S3.Ex2.m1.6.6.1.1.cmml"><mrow id="S3.Ex2.m1.6.6.1.1.1.1" xref="S3.Ex2.m1.6.6.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.5.5" xref="S3.Ex2.m1.5.5a.cmml">Backward Transfer</mtext><mspace width="1em" id="S3.Ex2.m1.6.6.1.1.1.1.2" xref="S3.Ex2.m1.6.6.1.1.1.2.cmml"></mspace><mrow id="S3.Ex2.m1.6.6.1.1.1.1.1" xref="S3.Ex2.m1.6.6.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.6.6.1.1.1.1.1.2" xref="S3.Ex2.m1.6.6.1.1.1.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.1.1.1.1" xref="S3.Ex2.m1.6.6.1.1.1.1.1.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.1.1.1.3" xref="S3.Ex2.m1.6.6.1.1.1.1.1.3.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.1.1.1.1a" xref="S3.Ex2.m1.6.6.1.1.1.1.1.1.cmml">​</mo><mi id="S3.Ex2.m1.6.6.1.1.1.1.1.4" xref="S3.Ex2.m1.6.6.1.1.1.1.1.4.cmml">T</mi></mrow></mrow><mo id="S3.Ex2.m1.6.6.1.1.2" xref="S3.Ex2.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.Ex2.m1.6.6.1.1.3" xref="S3.Ex2.m1.6.6.1.1.3.cmml"><mrow id="S3.Ex2.m1.6.6.1.1.3.2" xref="S3.Ex2.m1.6.6.1.1.3.2.cmml"><mfrac id="S3.Ex2.m1.6.6.1.1.3.2.2" xref="S3.Ex2.m1.6.6.1.1.3.2.2.cmml"><mn id="S3.Ex2.m1.6.6.1.1.3.2.2.2" xref="S3.Ex2.m1.6.6.1.1.3.2.2.2.cmml">1</mn><mrow id="S3.Ex2.m1.6.6.1.1.3.2.2.3" xref="S3.Ex2.m1.6.6.1.1.3.2.2.3.cmml"><mi id="S3.Ex2.m1.6.6.1.1.3.2.2.3.2" xref="S3.Ex2.m1.6.6.1.1.3.2.2.3.2.cmml">T</mi><mo id="S3.Ex2.m1.6.6.1.1.3.2.2.3.1" xref="S3.Ex2.m1.6.6.1.1.3.2.2.3.1.cmml">−</mo><mn id="S3.Ex2.m1.6.6.1.1.3.2.2.3.3" xref="S3.Ex2.m1.6.6.1.1.3.2.2.3.3.cmml">1</mn></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.6.6.1.1.3.2.1" xref="S3.Ex2.m1.6.6.1.1.3.2.1.cmml">​</mo><mrow id="S3.Ex2.m1.6.6.1.1.3.2.3" xref="S3.Ex2.m1.6.6.1.1.3.2.3.cmml"><munderover id="S3.Ex2.m1.6.6.1.1.3.2.3.1" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.cmml"><mo movablelimits="false" id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.2" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.2.cmml">∑</mo><mrow id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.cmml"><mi id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.2" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.2.cmml">i</mi><mo id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.1" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.1.cmml">=</mo><mn id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.3" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.Ex2.m1.6.6.1.1.3.2.3.1.3" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.cmml"><mi id="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.2" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.2.cmml">T</mi><mo id="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.1" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.1.cmml">−</mo><mn id="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.3" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.3.cmml">1</mn></mrow></munderover><msub id="S3.Ex2.m1.6.6.1.1.3.2.3.2" xref="S3.Ex2.m1.6.6.1.1.3.2.3.2.cmml"><mi id="S3.Ex2.m1.6.6.1.1.3.2.3.2.2" xref="S3.Ex2.m1.6.6.1.1.3.2.3.2.2.cmml">S</mi><mrow id="S3.Ex2.m1.2.2.2.4" xref="S3.Ex2.m1.2.2.2.3.cmml"><mi id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml">T</mi><mo id="S3.Ex2.m1.2.2.2.4.1" xref="S3.Ex2.m1.2.2.2.3.cmml">,</mo><mi id="S3.Ex2.m1.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.cmml">i</mi></mrow></msub></mrow></mrow><mo id="S3.Ex2.m1.6.6.1.1.3.1" xref="S3.Ex2.m1.6.6.1.1.3.1.cmml">−</mo><msub id="S3.Ex2.m1.6.6.1.1.3.3" xref="S3.Ex2.m1.6.6.1.1.3.3.cmml"><mi id="S3.Ex2.m1.6.6.1.1.3.3.2" xref="S3.Ex2.m1.6.6.1.1.3.3.2.cmml">S</mi><mrow id="S3.Ex2.m1.4.4.2.4" xref="S3.Ex2.m1.4.4.2.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1" xref="S3.Ex2.m1.3.3.1.1.cmml">i</mi><mo id="S3.Ex2.m1.4.4.2.4.1" xref="S3.Ex2.m1.4.4.2.3.cmml">,</mo><mi id="S3.Ex2.m1.4.4.2.2" xref="S3.Ex2.m1.4.4.2.2.cmml">i</mi></mrow></msub></mrow></mrow><mo id="S3.Ex2.m1.6.6.1.2" xref="S3.Ex2.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.6b"><apply id="S3.Ex2.m1.6.6.1.1.cmml" xref="S3.Ex2.m1.6.6.1"><eq id="S3.Ex2.m1.6.6.1.1.2.cmml" xref="S3.Ex2.m1.6.6.1.1.2"></eq><list id="S3.Ex2.m1.6.6.1.1.1.2.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1"><ci id="S3.Ex2.m1.5.5a.cmml" xref="S3.Ex2.m1.5.5"><mtext class="ltx_mathvariant_bold" id="S3.Ex2.m1.5.5.cmml" xref="S3.Ex2.m1.5.5">Backward Transfer</mtext></ci><apply id="S3.Ex2.m1.6.6.1.1.1.1.1.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1"><times id="S3.Ex2.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1.1"></times><ci id="S3.Ex2.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1.2">𝐵</ci><ci id="S3.Ex2.m1.6.6.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1.3">𝑊</ci><ci id="S3.Ex2.m1.6.6.1.1.1.1.1.4.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1.4">𝑇</ci></apply></list><apply id="S3.Ex2.m1.6.6.1.1.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3"><minus id="S3.Ex2.m1.6.6.1.1.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.1"></minus><apply id="S3.Ex2.m1.6.6.1.1.3.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2"><times id="S3.Ex2.m1.6.6.1.1.3.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.1"></times><apply id="S3.Ex2.m1.6.6.1.1.3.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.2"><divide id="S3.Ex2.m1.6.6.1.1.3.2.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.2"></divide><cn type="integer" id="S3.Ex2.m1.6.6.1.1.3.2.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.2.2">1</cn><apply id="S3.Ex2.m1.6.6.1.1.3.2.2.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.2.3"><minus id="S3.Ex2.m1.6.6.1.1.3.2.2.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.2.3.1"></minus><ci id="S3.Ex2.m1.6.6.1.1.3.2.2.3.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.2.3.2">𝑇</ci><cn type="integer" id="S3.Ex2.m1.6.6.1.1.3.2.2.3.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.2.3.3">1</cn></apply></apply><apply id="S3.Ex2.m1.6.6.1.1.3.2.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3"><apply id="S3.Ex2.m1.6.6.1.1.3.2.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.3.2.3.1.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1">superscript</csymbol><apply id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1">subscript</csymbol><sum id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.2"></sum><apply id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3"><eq id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.1"></eq><ci id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.2.3.3">1</cn></apply></apply><apply id="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.3"><minus id="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.1"></minus><ci id="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.2">𝑇</ci><cn type="integer" id="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.1.3.3">1</cn></apply></apply><apply id="S3.Ex2.m1.6.6.1.1.3.2.3.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.3.2.3.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.2">subscript</csymbol><ci id="S3.Ex2.m1.6.6.1.1.3.2.3.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.2.3.2.2">𝑆</ci><list id="S3.Ex2.m1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.4"><ci id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1">𝑇</ci><ci id="S3.Ex2.m1.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2">𝑖</ci></list></apply></apply></apply><apply id="S3.Ex2.m1.6.6.1.1.3.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.3.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3">subscript</csymbol><ci id="S3.Ex2.m1.6.6.1.1.3.3.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3.2">𝑆</ci><list id="S3.Ex2.m1.4.4.2.3.cmml" xref="S3.Ex2.m1.4.4.2.4"><ci id="S3.Ex2.m1.3.3.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1">𝑖</ci><ci id="S3.Ex2.m1.4.4.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2">𝑖</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.6c">\textbf{Backward Transfer}\quad BWT=\frac{1}{T-1}\sum_{i=1}^{T-1}S_{T,i}-S_{i,i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.4" class="ltx_Math" alttext="\textbf{Average Accuracy}\quad Acc=\frac{1}{T}\sum_{i=1}^{T}S_{T,i}," display="block"><semantics id="S3.Ex3.m1.4a"><mrow id="S3.Ex3.m1.4.4.1" xref="S3.Ex3.m1.4.4.1.1.cmml"><mrow id="S3.Ex3.m1.4.4.1.1" xref="S3.Ex3.m1.4.4.1.1.cmml"><mrow id="S3.Ex3.m1.4.4.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.Ex3.m1.3.3" xref="S3.Ex3.m1.3.3a.cmml">Average Accuracy</mtext><mspace width="1em" id="S3.Ex3.m1.4.4.1.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.2.cmml"></mspace><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.cmml"><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.3" xref="S3.Ex3.m1.4.4.1.1.1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.1.1.1.1a" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.cmml">​</mo><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.4" xref="S3.Ex3.m1.4.4.1.1.1.1.1.4.cmml">c</mi></mrow></mrow><mo id="S3.Ex3.m1.4.4.1.1.2" xref="S3.Ex3.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.Ex3.m1.4.4.1.1.3" xref="S3.Ex3.m1.4.4.1.1.3.cmml"><mfrac id="S3.Ex3.m1.4.4.1.1.3.2" xref="S3.Ex3.m1.4.4.1.1.3.2.cmml"><mn id="S3.Ex3.m1.4.4.1.1.3.2.2" xref="S3.Ex3.m1.4.4.1.1.3.2.2.cmml">1</mn><mi id="S3.Ex3.m1.4.4.1.1.3.2.3" xref="S3.Ex3.m1.4.4.1.1.3.2.3.cmml">T</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.3.1" xref="S3.Ex3.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S3.Ex3.m1.4.4.1.1.3.3" xref="S3.Ex3.m1.4.4.1.1.3.3.cmml"><munderover id="S3.Ex3.m1.4.4.1.1.3.3.1" xref="S3.Ex3.m1.4.4.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.Ex3.m1.4.4.1.1.3.3.1.2.2" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.Ex3.m1.4.4.1.1.3.3.1.2.3" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.cmml"><mi id="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.2" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.1" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.3" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex3.m1.4.4.1.1.3.3.1.3" xref="S3.Ex3.m1.4.4.1.1.3.3.1.3.cmml">T</mi></munderover><msub id="S3.Ex3.m1.4.4.1.1.3.3.2" xref="S3.Ex3.m1.4.4.1.1.3.3.2.cmml"><mi id="S3.Ex3.m1.4.4.1.1.3.3.2.2" xref="S3.Ex3.m1.4.4.1.1.3.3.2.2.cmml">S</mi><mrow id="S3.Ex3.m1.2.2.2.4" xref="S3.Ex3.m1.2.2.2.3.cmml"><mi id="S3.Ex3.m1.1.1.1.1" xref="S3.Ex3.m1.1.1.1.1.cmml">T</mi><mo id="S3.Ex3.m1.2.2.2.4.1" xref="S3.Ex3.m1.2.2.2.3.cmml">,</mo><mi id="S3.Ex3.m1.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.cmml">i</mi></mrow></msub></mrow></mrow></mrow><mo id="S3.Ex3.m1.4.4.1.2" xref="S3.Ex3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.4b"><apply id="S3.Ex3.m1.4.4.1.1.cmml" xref="S3.Ex3.m1.4.4.1"><eq id="S3.Ex3.m1.4.4.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.2"></eq><list id="S3.Ex3.m1.4.4.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1"><ci id="S3.Ex3.m1.3.3a.cmml" xref="S3.Ex3.m1.3.3"><mtext class="ltx_mathvariant_bold" id="S3.Ex3.m1.3.3.cmml" xref="S3.Ex3.m1.3.3">Average Accuracy</mtext></ci><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1"><times id="S3.Ex3.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1"></times><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.2">𝐴</ci><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.3">𝑐</ci><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.4.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.4">𝑐</ci></apply></list><apply id="S3.Ex3.m1.4.4.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3"><times id="S3.Ex3.m1.4.4.1.1.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3.1"></times><apply id="S3.Ex3.m1.4.4.1.1.3.2.cmml" xref="S3.Ex3.m1.4.4.1.1.3.2"><divide id="S3.Ex3.m1.4.4.1.1.3.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3.2"></divide><cn type="integer" id="S3.Ex3.m1.4.4.1.1.3.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.3.2.2">1</cn><ci id="S3.Ex3.m1.4.4.1.1.3.2.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3.2.3">𝑇</ci></apply><apply id="S3.Ex3.m1.4.4.1.1.3.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3"><apply id="S3.Ex3.m1.4.4.1.1.3.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.3.3.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1">superscript</csymbol><apply id="S3.Ex3.m1.4.4.1.1.3.3.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.3.3.1.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1">subscript</csymbol><sum id="S3.Ex3.m1.4.4.1.1.3.3.1.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.2"></sum><apply id="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.3"><eq id="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.1"></eq><ci id="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.2.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.Ex3.m1.4.4.1.1.3.3.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.1.3">𝑇</ci></apply><apply id="S3.Ex3.m1.4.4.1.1.3.3.2.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.3.3.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.2">subscript</csymbol><ci id="S3.Ex3.m1.4.4.1.1.3.3.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3.2.2">𝑆</ci><list id="S3.Ex3.m1.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.2.4"><ci id="S3.Ex3.m1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1">𝑇</ci><ci id="S3.Ex3.m1.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2">𝑖</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.4c">\textbf{Average Accuracy}\quad Acc=\frac{1}{T}\sum_{i=1}^{T}S_{T,i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.6" class="ltx_p">where <math id="S3.SS3.p2.1.m1.2" class="ltx_Math" alttext="S_{i,j}" display="inline"><semantics id="S3.SS3.p2.1.m1.2a"><msub id="S3.SS3.p2.1.m1.2.3" xref="S3.SS3.p2.1.m1.2.3.cmml"><mi id="S3.SS3.p2.1.m1.2.3.2" xref="S3.SS3.p2.1.m1.2.3.2.cmml">S</mi><mrow id="S3.SS3.p2.1.m1.2.2.2.4" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p2.1.m1.2.2.2.4.1" xref="S3.SS3.p2.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p2.1.m1.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><apply id="S3.SS3.p2.1.m1.2.3.cmml" xref="S3.SS3.p2.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.3.1.cmml" xref="S3.SS3.p2.1.m1.2.3">subscript</csymbol><ci id="S3.SS3.p2.1.m1.2.3.2.cmml" xref="S3.SS3.p2.1.m1.2.3.2">𝑆</ci><list id="S3.SS3.p2.1.m1.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.4"><ci id="S3.SS3.p2.1.m1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1">𝑖</ci><ci id="S3.SS3.p2.1.m1.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">S_{i,j}</annotation></semantics></math> is the evaluation score of the model on the test set of <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">j</annotation></semantics></math>-th task after training on <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">i</annotation></semantics></math>-th task, and <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\overline{b}_{i}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><msub id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mover accent="true" id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2.2" xref="S3.SS3.p2.4.m4.1.1.2.2.cmml">b</mi><mo id="S3.SS3.p2.4.m4.1.1.2.1" xref="S3.SS3.p2.4.m4.1.1.2.1.cmml">¯</mo></mover><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><apply id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2"><ci id="S3.SS3.p2.4.m4.1.1.2.1.cmml" xref="S3.SS3.p2.4.m4.1.1.2.1">¯</ci><ci id="S3.SS3.p2.4.m4.1.1.2.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2.2">𝑏</ci></apply><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\overline{b}_{i}</annotation></semantics></math> is the evaluation score of the pre-trained model, <em id="S3.SS3.p2.6.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS3.p2.6.2" class="ltx_text"></span>, model without any specific fine-tuning for VQA tasks, on the test set of <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mi id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><ci id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">i</annotation></semantics></math>-th task. Specifically, we feed the task-specific answer list into the decoder of pre-trained ALBEF to compute <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="\overline{b}_{i}" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><msub id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mover accent="true" id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2.2" xref="S3.SS3.p2.6.m6.1.1.2.2.cmml">b</mi><mo id="S3.SS3.p2.6.m6.1.1.2.1" xref="S3.SS3.p2.6.m6.1.1.2.1.cmml">¯</mo></mover><mi id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">subscript</csymbol><apply id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2"><ci id="S3.SS3.p2.6.m6.1.1.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1.2.1">¯</ci><ci id="S3.SS3.p2.6.m6.1.1.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2">𝑏</ci></apply><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\overline{b}_{i}</annotation></semantics></math>, while for the encoder-only backbones, we attach a randomly initialized classification head on the top for each task.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We provide a comprehensive analysis of VLPMs with our CL-CrossVQA benchmark. Concretely, we investigate a CL baseline and 4 CL approaches from different categories: (1) <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Sequential</span> uses the model learned on previous tasks as initialization and then optimizes the parameters for the current task. (2) <span id="S4.p1.1.2" class="ltx_text ltx_font_bold">ER</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> is a rehearsal-based method, which stores a portion of samples from the previous task and replays them regularly with the training batch of the current task. (3) <span id="S4.p1.1.3" class="ltx_text ltx_font_bold">DER</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is an extension of ER, which additionally stores the model prediction of the replayed samples at the end of the previous task as dark knowledge regularization. (4) <span id="S4.p1.1.4" class="ltx_text ltx_font_bold">DERPP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> combines ER and DER and is a strong hybrid method.
(5) <span id="S4.p1.1.5" class="ltx_text ltx_font_bold">EWC</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is a regularization-based method, which penalizes the drifting of important network parameters when training on new tasks.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">To analyze the effects of VLPM architecture in the CL setting, we train 4 representative VLPMs with the aforementioned CL methods using our benchmark. To handle the distribution shift in the ground-truth answers of different tasks, we incorporate task-specific classification heads with the encoder-only backbones, <em id="S4.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.p2.1.2" class="ltx_text"></span>, <em id="S4.p2.1.3" class="ltx_emph ltx_font_italic">ViLT, VAuLT, and FLAVA</em>. For the encoder-decoder learner, <em id="S4.p2.1.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.p2.1.5" class="ltx_text"></span>, <em id="S4.p2.1.6" class="ltx_emph ltx_font_italic">ALBEF</em>, we implement a static model architecture for all tasks and utilize different answer lists for each task. The hyperparameters used in the training are detailed in the Appendix.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.60" class="ltx_inline-block ltx_transformed_outer" style="width:499.4pt;height:80pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-146.6pt,23.3pt) scale(0.63,0.63) ;">
<table id="S4.T2.60.60" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.60.60.61" class="ltx_tr">
<td id="S4.T2.60.60.61.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.60.60.61.1.1" class="ltx_text">Model</span></td>
<td id="S4.T2.60.60.61.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">ViLT</td>
<td id="S4.T2.60.60.61.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">VAuLT</td>
<td id="S4.T2.60.60.61.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">FLAVA</td>
<td id="S4.T2.60.60.61.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">ALBEF</td>
</tr>
<tr id="S4.T2.60.60.62" class="ltx_tr">
<td id="S4.T2.60.60.62.1" class="ltx_td ltx_align_center ltx_border_t">Acc</td>
<td id="S4.T2.60.60.62.2" class="ltx_td ltx_align_center ltx_border_t">BWT</td>
<td id="S4.T2.60.60.62.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FWT</td>
<td id="S4.T2.60.60.62.4" class="ltx_td ltx_align_center ltx_border_t">Acc</td>
<td id="S4.T2.60.60.62.5" class="ltx_td ltx_align_center ltx_border_t">BWT</td>
<td id="S4.T2.60.60.62.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FWT</td>
<td id="S4.T2.60.60.62.7" class="ltx_td ltx_align_center ltx_border_t">Acc</td>
<td id="S4.T2.60.60.62.8" class="ltx_td ltx_align_center ltx_border_t">BWT</td>
<td id="S4.T2.60.60.62.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FWT</td>
<td id="S4.T2.60.60.62.10" class="ltx_td ltx_align_center ltx_border_t">Acc</td>
<td id="S4.T2.60.60.62.11" class="ltx_td ltx_align_center ltx_border_t">BWT</td>
<td id="S4.T2.60.60.62.12" class="ltx_td ltx_align_center ltx_border_t">FWT</td>
</tr>
<tr id="S4.T2.12.12.12" class="ltx_tr">
<td id="S4.T2.12.12.12.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Sequential</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">26.82<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\pm</annotation></semantics></math>2.29</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">-42.49<math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\pm</annotation></semantics></math>3.41</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-0.05<math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mo id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\pm</annotation></semantics></math>0.06</td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">26.21<math id="S4.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.4.4.4.4.m1.1a"><mo id="S4.T2.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.m1.1b"><csymbol cd="latexml" id="S4.T2.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.m1.1c">\pm</annotation></semantics></math>4.79</td>
<td id="S4.T2.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">-42.58<math id="S4.T2.5.5.5.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.5.5.5.5.m1.1a"><mo id="S4.T2.5.5.5.5.m1.1.1" xref="S4.T2.5.5.5.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.m1.1b"><csymbol cd="latexml" id="S4.T2.5.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.m1.1c">\pm</annotation></semantics></math>5.64</td>
<td id="S4.T2.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-0.14<math id="S4.T2.6.6.6.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.6.6.6.6.m1.1a"><mo id="S4.T2.6.6.6.6.m1.1.1" xref="S4.T2.6.6.6.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.6.m1.1b"><csymbol cd="latexml" id="S4.T2.6.6.6.6.m1.1.1.cmml" xref="S4.T2.6.6.6.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.6.m1.1c">\pm</annotation></semantics></math>0.24</td>
<td id="S4.T2.7.7.7.7" class="ltx_td ltx_align_center ltx_border_t">26.61<math id="S4.T2.7.7.7.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.7.7.7.7.m1.1a"><mo id="S4.T2.7.7.7.7.m1.1.1" xref="S4.T2.7.7.7.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.7.m1.1b"><csymbol cd="latexml" id="S4.T2.7.7.7.7.m1.1.1.cmml" xref="S4.T2.7.7.7.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.7.m1.1c">\pm</annotation></semantics></math>2.78</td>
<td id="S4.T2.8.8.8.8" class="ltx_td ltx_align_center ltx_border_t">-34.31<math id="S4.T2.8.8.8.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.8.8.8.8.m1.1a"><mo id="S4.T2.8.8.8.8.m1.1.1" xref="S4.T2.8.8.8.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.8.m1.1b"><csymbol cd="latexml" id="S4.T2.8.8.8.8.m1.1.1.cmml" xref="S4.T2.8.8.8.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.8.m1.1c">\pm</annotation></semantics></math>2.36</td>
<td id="S4.T2.9.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-0.02<math id="S4.T2.9.9.9.9.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.9.9.9.9.m1.1a"><mo id="S4.T2.9.9.9.9.m1.1.1" xref="S4.T2.9.9.9.9.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.9.m1.1b"><csymbol cd="latexml" id="S4.T2.9.9.9.9.m1.1.1.cmml" xref="S4.T2.9.9.9.9.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.9.m1.1c">\pm</annotation></semantics></math>0.40</td>
<td id="S4.T2.10.10.10.10" class="ltx_td ltx_align_center ltx_border_t">45.71<math id="S4.T2.10.10.10.10.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.10.10.10.10.m1.1a"><mo id="S4.T2.10.10.10.10.m1.1.1" xref="S4.T2.10.10.10.10.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.10.m1.1b"><csymbol cd="latexml" id="S4.T2.10.10.10.10.m1.1.1.cmml" xref="S4.T2.10.10.10.10.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.10.m1.1c">\pm</annotation></semantics></math>3.43</td>
<td id="S4.T2.11.11.11.11" class="ltx_td ltx_align_center ltx_border_t">-27.13<math id="S4.T2.11.11.11.11.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.11.11.11.11.m1.1a"><mo id="S4.T2.11.11.11.11.m1.1.1" xref="S4.T2.11.11.11.11.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.11.m1.1b"><csymbol cd="latexml" id="S4.T2.11.11.11.11.m1.1.1.cmml" xref="S4.T2.11.11.11.11.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.11.m1.1c">\pm</annotation></semantics></math>3.69</td>
<td id="S4.T2.12.12.12.12" class="ltx_td ltx_align_center ltx_border_t">9.77<math id="S4.T2.12.12.12.12.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.12.12.12.12.m1.1a"><mo id="S4.T2.12.12.12.12.m1.1.1" xref="S4.T2.12.12.12.12.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.12.m1.1b"><csymbol cd="latexml" id="S4.T2.12.12.12.12.m1.1.1.cmml" xref="S4.T2.12.12.12.12.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.12.m1.1c">\pm</annotation></semantics></math>2.83</td>
</tr>
<tr id="S4.T2.24.24.24" class="ltx_tr">
<td id="S4.T2.24.24.24.13" class="ltx_td ltx_align_center ltx_border_r">ER</td>
<td id="S4.T2.13.13.13.1" class="ltx_td ltx_align_center">54.15<math id="S4.T2.13.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.13.13.13.1.m1.1a"><mo id="S4.T2.13.13.13.1.m1.1.1" xref="S4.T2.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T2.13.13.13.1.m1.1.1.cmml" xref="S4.T2.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.13.1.m1.1c">\pm</annotation></semantics></math>1.36</td>
<td id="S4.T2.14.14.14.2" class="ltx_td ltx_align_center">-12.38<math id="S4.T2.14.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.14.14.14.2.m1.1a"><mo id="S4.T2.14.14.14.2.m1.1.1" xref="S4.T2.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T2.14.14.14.2.m1.1.1.cmml" xref="S4.T2.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.14.2.m1.1c">\pm</annotation></semantics></math>1.82</td>
<td id="S4.T2.15.15.15.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.15.15.15.3.1" class="ltx_text ltx_font_bold">0.03<math id="S4.T2.15.15.15.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.15.15.15.3.1.m1.1a"><mo id="S4.T2.15.15.15.3.1.m1.1.1" xref="S4.T2.15.15.15.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.15.3.1.m1.1b"><csymbol cd="latexml" id="S4.T2.15.15.15.3.1.m1.1.1.cmml" xref="S4.T2.15.15.15.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.15.3.1.m1.1c">\pm</annotation></semantics></math>0.06</span></td>
<td id="S4.T2.16.16.16.4" class="ltx_td ltx_align_center"><span id="S4.T2.16.16.16.4.1" class="ltx_text ltx_font_bold">51.51<math id="S4.T2.16.16.16.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.16.16.16.4.1.m1.1a"><mo id="S4.T2.16.16.16.4.1.m1.1.1" xref="S4.T2.16.16.16.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.16.4.1.m1.1b"><csymbol cd="latexml" id="S4.T2.16.16.16.4.1.m1.1.1.cmml" xref="S4.T2.16.16.16.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.16.4.1.m1.1c">\pm</annotation></semantics></math>0.91</span></td>
<td id="S4.T2.17.17.17.5" class="ltx_td ltx_align_center"><span id="S4.T2.17.17.17.5.1" class="ltx_text ltx_font_bold">-12.67<math id="S4.T2.17.17.17.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.17.17.17.5.1.m1.1a"><mo id="S4.T2.17.17.17.5.1.m1.1.1" xref="S4.T2.17.17.17.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.17.5.1.m1.1b"><csymbol cd="latexml" id="S4.T2.17.17.17.5.1.m1.1.1.cmml" xref="S4.T2.17.17.17.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.17.5.1.m1.1c">\pm</annotation></semantics></math>1.07</span></td>
<td id="S4.T2.18.18.18.6" class="ltx_td ltx_align_center ltx_border_r">0.08<math id="S4.T2.18.18.18.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.18.18.18.6.m1.1a"><mo id="S4.T2.18.18.18.6.m1.1.1" xref="S4.T2.18.18.18.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.18.6.m1.1b"><csymbol cd="latexml" id="S4.T2.18.18.18.6.m1.1.1.cmml" xref="S4.T2.18.18.18.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.18.6.m1.1c">\pm</annotation></semantics></math>0.12</td>
<td id="S4.T2.19.19.19.7" class="ltx_td ltx_align_center">44.52<math id="S4.T2.19.19.19.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.19.19.19.7.m1.1a"><mo id="S4.T2.19.19.19.7.m1.1.1" xref="S4.T2.19.19.19.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.19.19.19.7.m1.1b"><csymbol cd="latexml" id="S4.T2.19.19.19.7.m1.1.1.cmml" xref="S4.T2.19.19.19.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.19.19.7.m1.1c">\pm</annotation></semantics></math>0.80</td>
<td id="S4.T2.20.20.20.8" class="ltx_td ltx_align_center"><span id="S4.T2.20.20.20.8.1" class="ltx_text ltx_font_bold">-10.53<math id="S4.T2.20.20.20.8.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.20.20.20.8.1.m1.1a"><mo id="S4.T2.20.20.20.8.1.m1.1.1" xref="S4.T2.20.20.20.8.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.20.20.20.8.1.m1.1b"><csymbol cd="latexml" id="S4.T2.20.20.20.8.1.m1.1.1.cmml" xref="S4.T2.20.20.20.8.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.20.20.8.1.m1.1c">\pm</annotation></semantics></math>1.14</span></td>
<td id="S4.T2.21.21.21.9" class="ltx_td ltx_align_center ltx_border_r">-0.11<math id="S4.T2.21.21.21.9.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.21.21.21.9.m1.1a"><mo id="S4.T2.21.21.21.9.m1.1.1" xref="S4.T2.21.21.21.9.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.21.21.21.9.m1.1b"><csymbol cd="latexml" id="S4.T2.21.21.21.9.m1.1.1.cmml" xref="S4.T2.21.21.21.9.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.21.21.9.m1.1c">\pm</annotation></semantics></math>0.18</td>
<td id="S4.T2.22.22.22.10" class="ltx_td ltx_align_center"><span id="S4.T2.22.22.22.10.1" class="ltx_text ltx_font_bold">60.79<math id="S4.T2.22.22.22.10.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.22.22.22.10.1.m1.1a"><mo id="S4.T2.22.22.22.10.1.m1.1.1" xref="S4.T2.22.22.22.10.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.22.22.22.10.1.m1.1b"><csymbol cd="latexml" id="S4.T2.22.22.22.10.1.m1.1.1.cmml" xref="S4.T2.22.22.22.10.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.22.22.10.1.m1.1c">\pm</annotation></semantics></math>0.54</span></td>
<td id="S4.T2.23.23.23.11" class="ltx_td ltx_align_center"><span id="S4.T2.23.23.23.11.1" class="ltx_text ltx_font_bold">-9.77<math id="S4.T2.23.23.23.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.23.23.23.11.1.m1.1a"><mo id="S4.T2.23.23.23.11.1.m1.1.1" xref="S4.T2.23.23.23.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.23.23.23.11.1.m1.1b"><csymbol cd="latexml" id="S4.T2.23.23.23.11.1.m1.1.1.cmml" xref="S4.T2.23.23.23.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.23.23.11.1.m1.1c">\pm</annotation></semantics></math>1.02</span></td>
<td id="S4.T2.24.24.24.12" class="ltx_td ltx_align_center">11.90<math id="S4.T2.24.24.24.12.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.24.24.24.12.m1.1a"><mo id="S4.T2.24.24.24.12.m1.1.1" xref="S4.T2.24.24.24.12.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.24.24.24.12.m1.1b"><csymbol cd="latexml" id="S4.T2.24.24.24.12.m1.1.1.cmml" xref="S4.T2.24.24.24.12.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.24.24.12.m1.1c">\pm</annotation></semantics></math>2.39</td>
</tr>
<tr id="S4.T2.36.36.36" class="ltx_tr">
<td id="S4.T2.36.36.36.13" class="ltx_td ltx_align_center ltx_border_r">DER</td>
<td id="S4.T2.25.25.25.1" class="ltx_td ltx_align_center">51.42<math id="S4.T2.25.25.25.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.25.25.25.1.m1.1a"><mo id="S4.T2.25.25.25.1.m1.1.1" xref="S4.T2.25.25.25.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.25.25.25.1.m1.1b"><csymbol cd="latexml" id="S4.T2.25.25.25.1.m1.1.1.cmml" xref="S4.T2.25.25.25.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.25.25.1.m1.1c">\pm</annotation></semantics></math>1.71</td>
<td id="S4.T2.26.26.26.2" class="ltx_td ltx_align_center">-12.56<math id="S4.T2.26.26.26.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.26.26.26.2.m1.1a"><mo id="S4.T2.26.26.26.2.m1.1.1" xref="S4.T2.26.26.26.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.26.26.26.2.m1.1b"><csymbol cd="latexml" id="S4.T2.26.26.26.2.m1.1.1.cmml" xref="S4.T2.26.26.26.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.26.26.2.m1.1c">\pm</annotation></semantics></math>1.58</td>
<td id="S4.T2.27.27.27.3" class="ltx_td ltx_align_center ltx_border_r">-0.15<math id="S4.T2.27.27.27.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.27.27.27.3.m1.1a"><mo id="S4.T2.27.27.27.3.m1.1.1" xref="S4.T2.27.27.27.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.27.27.27.3.m1.1b"><csymbol cd="latexml" id="S4.T2.27.27.27.3.m1.1.1.cmml" xref="S4.T2.27.27.27.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.27.27.3.m1.1c">\pm</annotation></semantics></math>0.38</td>
<td id="S4.T2.28.28.28.4" class="ltx_td ltx_align_center">49.35<math id="S4.T2.28.28.28.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.28.28.28.4.m1.1a"><mo id="S4.T2.28.28.28.4.m1.1.1" xref="S4.T2.28.28.28.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.28.28.28.4.m1.1b"><csymbol cd="latexml" id="S4.T2.28.28.28.4.m1.1.1.cmml" xref="S4.T2.28.28.28.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.28.28.28.4.m1.1c">\pm</annotation></semantics></math>1.29</td>
<td id="S4.T2.29.29.29.5" class="ltx_td ltx_align_center">-14.86<math id="S4.T2.29.29.29.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.29.29.29.5.m1.1a"><mo id="S4.T2.29.29.29.5.m1.1.1" xref="S4.T2.29.29.29.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.29.29.29.5.m1.1b"><csymbol cd="latexml" id="S4.T2.29.29.29.5.m1.1.1.cmml" xref="S4.T2.29.29.29.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.29.29.29.5.m1.1c">\pm</annotation></semantics></math>1.63</td>
<td id="S4.T2.30.30.30.6" class="ltx_td ltx_align_center ltx_border_r">-0.15<math id="S4.T2.30.30.30.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.30.30.30.6.m1.1a"><mo id="S4.T2.30.30.30.6.m1.1.1" xref="S4.T2.30.30.30.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.30.30.30.6.m1.1b"><csymbol cd="latexml" id="S4.T2.30.30.30.6.m1.1.1.cmml" xref="S4.T2.30.30.30.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.30.30.30.6.m1.1c">\pm</annotation></semantics></math>0.41</td>
<td id="S4.T2.31.31.31.7" class="ltx_td ltx_align_center"><span id="S4.T2.31.31.31.7.1" class="ltx_text ltx_font_bold">44.82<math id="S4.T2.31.31.31.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.31.31.31.7.1.m1.1a"><mo id="S4.T2.31.31.31.7.1.m1.1.1" xref="S4.T2.31.31.31.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.31.31.31.7.1.m1.1b"><csymbol cd="latexml" id="S4.T2.31.31.31.7.1.m1.1.1.cmml" xref="S4.T2.31.31.31.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.31.31.31.7.1.m1.1c">\pm</annotation></semantics></math>1.09</span></td>
<td id="S4.T2.32.32.32.8" class="ltx_td ltx_align_center">-10.91<math id="S4.T2.32.32.32.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.32.32.32.8.m1.1a"><mo id="S4.T2.32.32.32.8.m1.1.1" xref="S4.T2.32.32.32.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.32.32.32.8.m1.1b"><csymbol cd="latexml" id="S4.T2.32.32.32.8.m1.1.1.cmml" xref="S4.T2.32.32.32.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.32.32.32.8.m1.1c">\pm</annotation></semantics></math>2.75</td>
<td id="S4.T2.33.33.33.9" class="ltx_td ltx_align_center ltx_border_r">-0.07<math id="S4.T2.33.33.33.9.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.33.33.33.9.m1.1a"><mo id="S4.T2.33.33.33.9.m1.1.1" xref="S4.T2.33.33.33.9.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.33.33.33.9.m1.1b"><csymbol cd="latexml" id="S4.T2.33.33.33.9.m1.1.1.cmml" xref="S4.T2.33.33.33.9.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.33.33.33.9.m1.1c">\pm</annotation></semantics></math>0.16</td>
<td id="S4.T2.34.34.34.10" class="ltx_td ltx_align_center">51.49<math id="S4.T2.34.34.34.10.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.34.34.34.10.m1.1a"><mo id="S4.T2.34.34.34.10.m1.1.1" xref="S4.T2.34.34.34.10.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.34.34.34.10.m1.1b"><csymbol cd="latexml" id="S4.T2.34.34.34.10.m1.1.1.cmml" xref="S4.T2.34.34.34.10.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.34.34.34.10.m1.1c">\pm</annotation></semantics></math>2.08</td>
<td id="S4.T2.35.35.35.11" class="ltx_td ltx_align_center">-21.18<math id="S4.T2.35.35.35.11.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.35.35.35.11.m1.1a"><mo id="S4.T2.35.35.35.11.m1.1.1" xref="S4.T2.35.35.35.11.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.35.35.35.11.m1.1b"><csymbol cd="latexml" id="S4.T2.35.35.35.11.m1.1.1.cmml" xref="S4.T2.35.35.35.11.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.35.35.35.11.m1.1c">\pm</annotation></semantics></math>2.29</td>
<td id="S4.T2.36.36.36.12" class="ltx_td ltx_align_center"><span id="S4.T2.36.36.36.12.1" class="ltx_text ltx_font_bold">12.48<math id="S4.T2.36.36.36.12.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.36.36.36.12.1.m1.1a"><mo id="S4.T2.36.36.36.12.1.m1.1.1" xref="S4.T2.36.36.36.12.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.36.36.36.12.1.m1.1b"><csymbol cd="latexml" id="S4.T2.36.36.36.12.1.m1.1.1.cmml" xref="S4.T2.36.36.36.12.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.36.36.36.12.1.m1.1c">\pm</annotation></semantics></math>1.26</span></td>
</tr>
<tr id="S4.T2.48.48.48" class="ltx_tr">
<td id="S4.T2.48.48.48.13" class="ltx_td ltx_align_center ltx_border_r">DERPP</td>
<td id="S4.T2.37.37.37.1" class="ltx_td ltx_align_center"><span id="S4.T2.37.37.37.1.1" class="ltx_text ltx_font_bold">54.21<math id="S4.T2.37.37.37.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.37.37.37.1.1.m1.1a"><mo id="S4.T2.37.37.37.1.1.m1.1.1" xref="S4.T2.37.37.37.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.37.37.37.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.37.37.37.1.1.m1.1.1.cmml" xref="S4.T2.37.37.37.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.37.37.37.1.1.m1.1c">\pm</annotation></semantics></math>1.31</span></td>
<td id="S4.T2.38.38.38.2" class="ltx_td ltx_align_center"><span id="S4.T2.38.38.38.2.1" class="ltx_text ltx_font_bold">-12.34<math id="S4.T2.38.38.38.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.38.38.38.2.1.m1.1a"><mo id="S4.T2.38.38.38.2.1.m1.1.1" xref="S4.T2.38.38.38.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.38.38.38.2.1.m1.1b"><csymbol cd="latexml" id="S4.T2.38.38.38.2.1.m1.1.1.cmml" xref="S4.T2.38.38.38.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.38.38.38.2.1.m1.1c">\pm</annotation></semantics></math>1.70</span></td>
<td id="S4.T2.39.39.39.3" class="ltx_td ltx_align_center ltx_border_r">0.00<math id="S4.T2.39.39.39.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.39.39.39.3.m1.1a"><mo id="S4.T2.39.39.39.3.m1.1.1" xref="S4.T2.39.39.39.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.39.39.39.3.m1.1b"><csymbol cd="latexml" id="S4.T2.39.39.39.3.m1.1.1.cmml" xref="S4.T2.39.39.39.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.39.39.39.3.m1.1c">\pm</annotation></semantics></math>0.28</td>
<td id="S4.T2.40.40.40.4" class="ltx_td ltx_align_center">51.30<math id="S4.T2.40.40.40.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.40.40.40.4.m1.1a"><mo id="S4.T2.40.40.40.4.m1.1.1" xref="S4.T2.40.40.40.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.40.40.40.4.m1.1b"><csymbol cd="latexml" id="S4.T2.40.40.40.4.m1.1.1.cmml" xref="S4.T2.40.40.40.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.40.40.40.4.m1.1c">\pm</annotation></semantics></math>1.03</td>
<td id="S4.T2.41.41.41.5" class="ltx_td ltx_align_center">-13.08<math id="S4.T2.41.41.41.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.41.41.41.5.m1.1a"><mo id="S4.T2.41.41.41.5.m1.1.1" xref="S4.T2.41.41.41.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.41.41.41.5.m1.1b"><csymbol cd="latexml" id="S4.T2.41.41.41.5.m1.1.1.cmml" xref="S4.T2.41.41.41.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.41.41.41.5.m1.1c">\pm</annotation></semantics></math>1.12</td>
<td id="S4.T2.42.42.42.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.42.42.42.6.1" class="ltx_text ltx_font_bold">0.23<math id="S4.T2.42.42.42.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.42.42.42.6.1.m1.1a"><mo id="S4.T2.42.42.42.6.1.m1.1.1" xref="S4.T2.42.42.42.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.42.42.42.6.1.m1.1b"><csymbol cd="latexml" id="S4.T2.42.42.42.6.1.m1.1.1.cmml" xref="S4.T2.42.42.42.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.42.42.42.6.1.m1.1c">\pm</annotation></semantics></math>0.31</span></td>
<td id="S4.T2.43.43.43.7" class="ltx_td ltx_align_center">44.52<math id="S4.T2.43.43.43.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.43.43.43.7.m1.1a"><mo id="S4.T2.43.43.43.7.m1.1.1" xref="S4.T2.43.43.43.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.43.43.43.7.m1.1b"><csymbol cd="latexml" id="S4.T2.43.43.43.7.m1.1.1.cmml" xref="S4.T2.43.43.43.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.43.43.43.7.m1.1c">\pm</annotation></semantics></math>1.57</td>
<td id="S4.T2.44.44.44.8" class="ltx_td ltx_align_center">-11.30<math id="S4.T2.44.44.44.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.44.44.44.8.m1.1a"><mo id="S4.T2.44.44.44.8.m1.1.1" xref="S4.T2.44.44.44.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.44.44.44.8.m1.1b"><csymbol cd="latexml" id="S4.T2.44.44.44.8.m1.1.1.cmml" xref="S4.T2.44.44.44.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.44.44.44.8.m1.1c">\pm</annotation></semantics></math>2.64</td>
<td id="S4.T2.45.45.45.9" class="ltx_td ltx_align_center ltx_border_r">-0.06<math id="S4.T2.45.45.45.9.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.45.45.45.9.m1.1a"><mo id="S4.T2.45.45.45.9.m1.1.1" xref="S4.T2.45.45.45.9.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.45.45.45.9.m1.1b"><csymbol cd="latexml" id="S4.T2.45.45.45.9.m1.1.1.cmml" xref="S4.T2.45.45.45.9.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.45.45.45.9.m1.1c">\pm</annotation></semantics></math>0.33</td>
<td id="S4.T2.46.46.46.10" class="ltx_td ltx_align_center">59.84<math id="S4.T2.46.46.46.10.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.46.46.46.10.m1.1a"><mo id="S4.T2.46.46.46.10.m1.1.1" xref="S4.T2.46.46.46.10.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.46.46.46.10.m1.1b"><csymbol cd="latexml" id="S4.T2.46.46.46.10.m1.1.1.cmml" xref="S4.T2.46.46.46.10.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.46.46.46.10.m1.1c">\pm</annotation></semantics></math>0.97</td>
<td id="S4.T2.47.47.47.11" class="ltx_td ltx_align_center">-10.89<math id="S4.T2.47.47.47.11.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.47.47.47.11.m1.1a"><mo id="S4.T2.47.47.47.11.m1.1.1" xref="S4.T2.47.47.47.11.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.47.47.47.11.m1.1b"><csymbol cd="latexml" id="S4.T2.47.47.47.11.m1.1.1.cmml" xref="S4.T2.47.47.47.11.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.47.47.47.11.m1.1c">\pm</annotation></semantics></math>1.27</td>
<td id="S4.T2.48.48.48.12" class="ltx_td ltx_align_center">11.91<math id="S4.T2.48.48.48.12.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.48.48.48.12.m1.1a"><mo id="S4.T2.48.48.48.12.m1.1.1" xref="S4.T2.48.48.48.12.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.48.48.48.12.m1.1b"><csymbol cd="latexml" id="S4.T2.48.48.48.12.m1.1.1.cmml" xref="S4.T2.48.48.48.12.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.48.48.48.12.m1.1c">\pm</annotation></semantics></math>3.14</td>
</tr>
<tr id="S4.T2.60.60.60" class="ltx_tr">
<td id="S4.T2.60.60.60.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">EWC</td>
<td id="S4.T2.49.49.49.1" class="ltx_td ltx_align_center ltx_border_bb">26.94<math id="S4.T2.49.49.49.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.49.49.49.1.m1.1a"><mo id="S4.T2.49.49.49.1.m1.1.1" xref="S4.T2.49.49.49.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.49.49.49.1.m1.1b"><csymbol cd="latexml" id="S4.T2.49.49.49.1.m1.1.1.cmml" xref="S4.T2.49.49.49.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.49.49.49.1.m1.1c">\pm</annotation></semantics></math>2.75</td>
<td id="S4.T2.50.50.50.2" class="ltx_td ltx_align_center ltx_border_bb">-42.86<math id="S4.T2.50.50.50.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.50.50.50.2.m1.1a"><mo id="S4.T2.50.50.50.2.m1.1.1" xref="S4.T2.50.50.50.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.50.50.50.2.m1.1b"><csymbol cd="latexml" id="S4.T2.50.50.50.2.m1.1.1.cmml" xref="S4.T2.50.50.50.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.50.50.50.2.m1.1c">\pm</annotation></semantics></math>3.96</td>
<td id="S4.T2.51.51.51.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.03<math id="S4.T2.51.51.51.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.51.51.51.3.m1.1a"><mo id="S4.T2.51.51.51.3.m1.1.1" xref="S4.T2.51.51.51.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.51.51.51.3.m1.1b"><csymbol cd="latexml" id="S4.T2.51.51.51.3.m1.1.1.cmml" xref="S4.T2.51.51.51.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.51.51.51.3.m1.1c">\pm</annotation></semantics></math>0.06</td>
<td id="S4.T2.52.52.52.4" class="ltx_td ltx_align_center ltx_border_bb">25.43<math id="S4.T2.52.52.52.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.52.52.52.4.m1.1a"><mo id="S4.T2.52.52.52.4.m1.1.1" xref="S4.T2.52.52.52.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.52.52.52.4.m1.1b"><csymbol cd="latexml" id="S4.T2.52.52.52.4.m1.1.1.cmml" xref="S4.T2.52.52.52.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.52.52.52.4.m1.1c">\pm</annotation></semantics></math>3.79</td>
<td id="S4.T2.53.53.53.5" class="ltx_td ltx_align_center ltx_border_bb">-43.64<math id="S4.T2.53.53.53.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.53.53.53.5.m1.1a"><mo id="S4.T2.53.53.53.5.m1.1.1" xref="S4.T2.53.53.53.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.53.53.53.5.m1.1b"><csymbol cd="latexml" id="S4.T2.53.53.53.5.m1.1.1.cmml" xref="S4.T2.53.53.53.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.53.53.53.5.m1.1c">\pm</annotation></semantics></math>4.02</td>
<td id="S4.T2.54.54.54.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-0.03<math id="S4.T2.54.54.54.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.54.54.54.6.m1.1a"><mo id="S4.T2.54.54.54.6.m1.1.1" xref="S4.T2.54.54.54.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.54.54.54.6.m1.1b"><csymbol cd="latexml" id="S4.T2.54.54.54.6.m1.1.1.cmml" xref="S4.T2.54.54.54.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.54.54.54.6.m1.1c">\pm</annotation></semantics></math>0.60</td>
<td id="S4.T2.55.55.55.7" class="ltx_td ltx_align_center ltx_border_bb">25.83<math id="S4.T2.55.55.55.7.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.55.55.55.7.m1.1a"><mo id="S4.T2.55.55.55.7.m1.1.1" xref="S4.T2.55.55.55.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.55.55.55.7.m1.1b"><csymbol cd="latexml" id="S4.T2.55.55.55.7.m1.1.1.cmml" xref="S4.T2.55.55.55.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.55.55.55.7.m1.1c">\pm</annotation></semantics></math>4.89</td>
<td id="S4.T2.56.56.56.8" class="ltx_td ltx_align_center ltx_border_bb">-34.09<math id="S4.T2.56.56.56.8.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.56.56.56.8.m1.1a"><mo id="S4.T2.56.56.56.8.m1.1.1" xref="S4.T2.56.56.56.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.56.56.56.8.m1.1b"><csymbol cd="latexml" id="S4.T2.56.56.56.8.m1.1.1.cmml" xref="S4.T2.56.56.56.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.56.56.56.8.m1.1c">\pm</annotation></semantics></math>5.87</td>
<td id="S4.T2.57.57.57.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T2.57.57.57.9.1" class="ltx_text ltx_font_bold">-0.01<math id="S4.T2.57.57.57.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.57.57.57.9.1.m1.1a"><mo id="S4.T2.57.57.57.9.1.m1.1.1" xref="S4.T2.57.57.57.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.57.57.57.9.1.m1.1b"><csymbol cd="latexml" id="S4.T2.57.57.57.9.1.m1.1.1.cmml" xref="S4.T2.57.57.57.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.57.57.57.9.1.m1.1c">\pm</annotation></semantics></math>0.18</span></td>
<td id="S4.T2.58.58.58.10" class="ltx_td ltx_align_center ltx_border_bb">46.57<math id="S4.T2.58.58.58.10.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.58.58.58.10.m1.1a"><mo id="S4.T2.58.58.58.10.m1.1.1" xref="S4.T2.58.58.58.10.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.58.58.58.10.m1.1b"><csymbol cd="latexml" id="S4.T2.58.58.58.10.m1.1.1.cmml" xref="S4.T2.58.58.58.10.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.58.58.58.10.m1.1c">\pm</annotation></semantics></math>5.82</td>
<td id="S4.T2.59.59.59.11" class="ltx_td ltx_align_center ltx_border_bb">-27.67<math id="S4.T2.59.59.59.11.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.59.59.59.11.m1.1a"><mo id="S4.T2.59.59.59.11.m1.1.1" xref="S4.T2.59.59.59.11.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.59.59.59.11.m1.1b"><csymbol cd="latexml" id="S4.T2.59.59.59.11.m1.1.1.cmml" xref="S4.T2.59.59.59.11.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.59.59.59.11.m1.1c">\pm</annotation></semantics></math>7.07</td>
<td id="S4.T2.60.60.60.12" class="ltx_td ltx_align_center ltx_border_bb">9.62<math id="S4.T2.60.60.60.12.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.60.60.60.12.m1.1a"><mo id="S4.T2.60.60.60.12.m1.1.1" xref="S4.T2.60.60.60.12.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.60.60.60.12.m1.1b"><csymbol cd="latexml" id="S4.T2.60.60.60.12.m1.1.1.cmml" xref="S4.T2.60.60.60.12.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.60.60.60.12.m1.1c">\pm</annotation></semantics></math>3.53</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.64.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.62.1" class="ltx_text" style="font-size:90%;">Evaluation of selected VLPMs combined with different CL approaches. Average accuracy (Acc), backward transfer (BWT), and forward transfer (FWT) is reported. 6 task orders are adopted to produce the mean <math id="S4.T2.62.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.62.1.m1.1b"><mo id="S4.T2.62.1.m1.1.1" xref="S4.T2.62.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.62.1.m1.1c"><csymbol cd="latexml" id="S4.T2.62.1.m1.1.1.cmml" xref="S4.T2.62.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.62.1.m1.1d">\pm</annotation></semantics></math> standard deviation. All models are fine-tuned for a maximum of 15 epochs. We apply a random sampling strategy for ER, DER, and DERPP. For all replay-based approaches, the memory size is set to 1% of training data. We tune the weight for logits distillation and the weight for experience replay of DERPP by grid search in {0.1, 0.5, 1, 10, 100}. We tune the regularization coefficient of EWC by grid search in {0.1, 1, 10, 100, 1000, 10,000, 100,000, 1,000,000}.</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Results and Discussion</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In <a href="#S4.T2" title="In 4 Experiments ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we provide the evaluation results of the selected VLPMs in our CL-CrossVQA benchmark. We report three metrics, <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p1.1.2" class="ltx_text"></span>, average accuracy (Acc), backward transfer (BWT), and forward transfer (FWT). All experiments are conducted with 6 randomly sampled task orders. In the following, we discuss our findings.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">All VLPMs evaluated suffer from severe catastrophic forgetting.</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">Comparing the backward transfer of all CL methods, we observe that sequential fine-tuning of all VLPMs results in the largest forgetting, indicating that the selected VLPMs cannot preserve the knowledge learned on the previous tasks. Therefore, continually fine-tuning VLPMs without applying any CL approaches would result in a considerable performance loss.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Replay-based approaches are the most effective for VLPMs.</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">Comparing the four different CL methods, we can observe that replay-based CL approaches, <em id="S4.SS1.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS0.Px2.p1.1.2" class="ltx_text"></span>, ER, DER, and DERPP, achieve higher accuracy and higher backward transfer than the regularization-based method, <em id="S4.SS1.SSS0.Px2.p1.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS0.Px2.p1.1.4" class="ltx_text"></span>, EWC, for all VLPMs. ER, and DERPP, which adopts both regularization and experience replay, achieve comparable results on our benchmark. Note that, we only use 1% of the training data, which implies that a small number of replayed data is already able to alleviate forgetting to a large extent. Despite the careful tuning of the regularization coefficient, EWC performs as poorly as the sequential fine-tuning baseline, demonstrating that EWC is not suitable for VLPMs in the cross-domain continual learning setting.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Dual-stream encoder-decoder architecture is more resistant to forgetting.</h5>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">ALBEF shows the highest CL performance among all VLPMs, suggesting that the dual-stream encoder-decoder model is significantly more resistant to forgetting than the encoder-only model. Note that, ALBEF allows one to perform continual learning without adding new task-specific parameters. However, for single-stream models, new classification heads have to be added each time we train on a new task since each task in our benchmark has a different number of classes. Such differences may partially explain why ALBEF is more suitable for CL. Also, ALBEF produces the highest FWT score among all backbones, which further indicates the benefit of applying the same parameters for all tasks. In order to examine whether adding task-specific parameters is responsible for the low performance of single-stream models, we take steps toward understanding why ALBEF seems to be resistant to forgetting by probing the inner layer of ALBEF and comparing it to the single-stream representative VLPM, <em id="S4.SS1.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS0.Px3.p1.1.2" class="ltx_text"></span>, ViLT in <a href="#S5" title="5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p2.1" class="ltx_p">Compared with the dual-stream model, single-stream models benefit more from replay-based approaches. Among all single-stream models, ViLT reaches the highest accuracy and backward transfer, implying that VAuLT, which extracts rich language representations from a pre-trained frozen BERT that serves as input embeddings for ViLT, cannot reap benefits for CL. Counterintuitively, FLAVA, which is pre-trained on the largest dataset with 70M image-text pairs, shows the worst performance in our experimental setting, suggesting that a diverse large-scale pre-training dataset is not a useful ingredient in alleviating forgetting when the domain shift of different CL tasks is large. We hypothesize that large data distribution shifts between the pre-training data of FLAVA and the datasets of our benchmark impair the transfer of the rich knowledge gained from the large pre-training dataset to the new domain, especially when the data in the new domain is limited. This phenomenon suggests that ViLT already contains rich multimodal representations, and a relatively small number of parameters makes it more efficient when transferring to a new domain and better suited for CL with limited data per task.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Impact of task orders.</h5>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">To investigate how task orders affect CL performance, we evaluate 6 task orders selected at random from all possible combinations. We find that replay-based approaches are less sensitive to task orders in comparison to</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/0.png" id="S4.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="496" height="382" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Acc</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/2.png" id="S4.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="511" height="381" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">BWT</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Analysis of model sensitivity to task order for ViLT and ALBEF with different CL methods.</span></figcaption>
</figure>
<div id="S4.SS1.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p2.1" class="ltx_p">regularization-based approaches (see standard errors in <a href="#S4.T2" title="In 4 Experiments ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>). Among all VLPMs, ALBEF is more robust to task orders using the best-performing CL approaches, <em id="S4.SS1.SSS0.Px4.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS0.Px4.p2.1.2" class="ltx_text"></span>, ER, and DERPP (see <a href="#S4.F2" title="In Impact of task orders. ‣ 4.1 Results and Discussion ‣ 4 Experiments ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Impact of memory buffer.</h5>

<div id="S4.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px5.p1.1" class="ltx_p">To investigate the effect of the answer distribution in the replay buffer on the CL performance, we compare two different methods for sampling replay instances: random sampling, <em id="S4.SS1.SSS0.Px5.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS0.Px5.p1.1.2" class="ltx_text"></span>, sampling all instances from the previous tasks with equal probability, and balanced sampling, <em id="S4.SS1.SSS0.Px5.p1.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS0.Px5.p1.1.4" class="ltx_text"></span>, memory buffer exhibits a class-balanced distribution regardless of the ground-truth class distribution in previous answer lists. The evaluation results are shown in <a href="#S4.F3" title="In Impact of memory buffer. ‣ 4.1 Results and Discussion ‣ 4 Experiments ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>. We observe a clear performance gap between the two sampling strategies for all 3 CL methods, indicating that replaying samples based on the class distribution of previous tasks is beneficial for continual learning. It is also important to note that this effect of the replay buffer on CL performance does not seem to be architecture-dependent. This shows that the selection of sampling strategy has a great impact on CL performance, and improving the sampling strategy could be one direction to obtain better CL performance.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/balance_acc.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="492" height="340" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Acc</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/balance_bwt.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="508" height="338" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">BWT</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.6.3.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text" style="font-size:90%;">Comparison of accuracy (Acc<math id="S4.F3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.F3.3.1.m1.1b"><mo stretchy="false" id="S4.F3.3.1.m1.1.1" xref="S4.F3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.3.1.m1.1c"><ci id="S4.F3.3.1.m1.1.1.cmml" xref="S4.F3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.3.1.m1.1d">\uparrow</annotation></semantics></math>) and backward transfer (BWT<math id="S4.F3.4.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.F3.4.2.m2.1b"><mo stretchy="false" id="S4.F3.4.2.m2.1.1" xref="S4.F3.4.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.F3.4.2.m2.1c"><ci id="S4.F3.4.2.m2.1.1.cmml" xref="S4.F3.4.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.4.2.m2.1d">\downarrow</annotation></semantics></math>) of replay-based approaches using different sampling strategies.</span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Comparison with Adapter</h5>

<div id="S4.SS1.SSS0.Px6.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px6.p1.1" class="ltx_p">In addition to conventional CL approaches, we also conduct experiments with Adapter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which adds new trainable modules between the intermediate layers of VLPMs. We implement the same adapter architecture for two representative VLPMs, <em id="S4.SS1.SSS0.Px6.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS0.Px6.p1.1.2" class="ltx_text"></span>, ALBEF and ViLT. We freeze the parameters of all layers and continually optimize the task-specific adapters and the classification head for ViLT. As shown in <a href="#S4.T3" title="In Comparison with Adapter ‣ 4.1 Results and Discussion ‣ 4 Experiments ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S5.T4" title="In 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, Adapter enables both models to learn new tasks without any performance degradation on the previously learned tasks, <em id="S4.SS1.SSS0.Px6.p1.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.SSS0.Px6.p1.1.4" class="ltx_text"></span>, no catastrophic forgetting. Adapter is beneficial for the single-stream model as it boosts performance by ca. 11% accuracy while being ca. 3 times more computationally efficient. In contrast, for the dual-stream model, Adapter leads to a drop of 8% in retained accuracy compared to the best-performing CL approach. Therefore, Adapter is a strong competitor for the more computationally expensive CL methods.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_transformed_outer" style="width:253.4pt;height:50.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.1pt,1.6pt) scale(0.94,0.94) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<td id="S4.T3.2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Method</td>
<td id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Trainable Parameters</td>
<td id="S4.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Acc</td>
<td id="S4.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">BWT</td>
<td id="S4.T3.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">FWT</td>
</tr>
<tr id="S4.T3.2.1.2" class="ltx_tr">
<td id="S4.T3.2.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DERPP</td>
<td id="S4.T3.2.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">125.97M (100.00 %)</td>
<td id="S4.T3.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">54.74</td>
<td id="S4.T3.2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">-12.22</td>
<td id="S4.T3.2.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.11</td>
</tr>
<tr id="S4.T3.2.1.3" class="ltx_tr">
<td id="S4.T3.2.1.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Adapter</td>
<td id="S4.T3.2.1.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">42.72M (33.91%)</td>
<td id="S4.T3.2.1.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.2.1.3.3.1" class="ltx_text ltx_font_bold">65.10</span></td>
<td id="S4.T3.2.1.3.4" class="ltx_td ltx_align_center ltx_border_bb">/</td>
<td id="S4.T3.2.1.3.5" class="ltx_td ltx_align_center ltx_border_bb">/</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Comparison of Adapter and the best-performing CL approach for ViLT.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Model Analysis</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we provide a comprehensive analysis of the impact of each individual module of VLPMs when trained in a CL environment. Specifically, we investigate the influence of vision and text encoders in <a href="#S5.SS1" title="5.1 Impact of Vision and Text Encoder ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>, and examine the impact of multimodal fusion designs in <a href="#S5.SS2" title="5.2 Analysis of Multimodal Fusion Module ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>. Finally, we provide some Grad-CAM visualizations for VQA after training on each task in <a href="#S5.SS3" title="5.3 Visualization ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>. For ablation and analysis, we mainly focus on ViLT and ALBEF. The task order we use is illustrated in <a href="#S1.F1" title="In 1 Introduction ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.2" class="ltx_inline-block ltx_transformed_outer" style="width:252.5pt;height:136.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.0pt,13.0pt) scale(0.84,0.84) ;">
<table id="S5.T4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.2.1.1" class="ltx_tr">
<td id="S5.T4.2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Method</td>
<td id="S5.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Trainable Parameters</td>
<td id="S5.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Acc</td>
<td id="S5.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">BWT</td>
<td id="S5.T4.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">FWT</td>
</tr>
<tr id="S5.T4.2.1.2" class="ltx_tr">
<td id="S5.T4.2.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Seq.</td>
<td id="S5.T4.2.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">290.34M (100.00%)</td>
<td id="S5.T4.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">43.32</td>
<td id="S5.T4.2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">-30.16</td>
<td id="S5.T4.2.1.2.5" class="ltx_td ltx_align_center ltx_border_t">10.16</td>
</tr>
<tr id="S5.T4.2.1.3" class="ltx_tr">
<td id="S5.T4.2.1.3.1" class="ltx_td ltx_align_center ltx_border_r">ER</td>
<td id="S5.T4.2.1.3.2" class="ltx_td ltx_align_center ltx_border_r">290.34M (100.00%)</td>
<td id="S5.T4.2.1.3.3" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.3.3.1" class="ltx_text ltx_font_bold">60.19</span></td>
<td id="S5.T4.2.1.3.4" class="ltx_td ltx_align_center">-10.73</td>
<td id="S5.T4.2.1.3.5" class="ltx_td ltx_align_center">12.71</td>
</tr>
<tr id="S5.T4.2.1.4" class="ltx_tr">
<td id="S5.T4.2.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Freeze VE</td>
<td id="S5.T4.2.1.4.2" class="ltx_td ltx_align_center ltx_border_r">204.25M (70.35%)</td>
<td id="S5.T4.2.1.4.3" class="ltx_td ltx_align_center">38.50</td>
<td id="S5.T4.2.1.4.4" class="ltx_td ltx_align_center">-36.12</td>
<td id="S5.T4.2.1.4.5" class="ltx_td ltx_align_center">11.53</td>
</tr>
<tr id="S5.T4.2.1.5" class="ltx_tr">
<td id="S5.T4.2.1.5.1" class="ltx_td ltx_align_center ltx_border_r">Freeze TE</td>
<td id="S5.T4.2.1.5.2" class="ltx_td ltx_align_center ltx_border_r">223.97M (77.14%)</td>
<td id="S5.T4.2.1.5.3" class="ltx_td ltx_align_center">39.15</td>
<td id="S5.T4.2.1.5.4" class="ltx_td ltx_align_center">-36.73</td>
<td id="S5.T4.2.1.5.5" class="ltx_td ltx_align_center">11.60</td>
</tr>
<tr id="S5.T4.2.1.6" class="ltx_tr">
<td id="S5.T4.2.1.6.1" class="ltx_td ltx_align_center ltx_border_r">Freeze VE+TE</td>
<td id="S5.T4.2.1.6.2" class="ltx_td ltx_align_center ltx_border_r">137.88M (47.49%)</td>
<td id="S5.T4.2.1.6.3" class="ltx_td ltx_align_center">40.57</td>
<td id="S5.T4.2.1.6.4" class="ltx_td ltx_align_center">-34.20</td>
<td id="S5.T4.2.1.6.5" class="ltx_td ltx_align_center">11.42</td>
</tr>
<tr id="S5.T4.2.1.7" class="ltx_tr">
<td id="S5.T4.2.1.7.1" class="ltx_td ltx_align_center ltx_border_r">Freeze B9</td>
<td id="S5.T4.2.1.7.2" class="ltx_td ltx_align_center ltx_border_r">109.53M (37.72%)</td>
<td id="S5.T4.2.1.7.3" class="ltx_td ltx_align_center">38.72</td>
<td id="S5.T4.2.1.7.4" class="ltx_td ltx_align_center">-34.10</td>
<td id="S5.T4.2.1.7.5" class="ltx_td ltx_align_center">11.35</td>
</tr>
<tr id="S5.T4.2.1.8" class="ltx_tr">
<td id="S5.T4.2.1.8.1" class="ltx_td ltx_align_center ltx_border_r">Freeze B12</td>
<td id="S5.T4.2.1.8.2" class="ltx_td ltx_align_center ltx_border_r">81.17M (27.96%)</td>
<td id="S5.T4.2.1.8.3" class="ltx_td ltx_align_center">34.69</td>
<td id="S5.T4.2.1.8.4" class="ltx_td ltx_align_center">-33.23</td>
<td id="S5.T4.2.1.8.5" class="ltx_td ltx_align_center">8.46</td>
</tr>
<tr id="S5.T4.2.1.9" class="ltx_tr">
<td id="S5.T4.2.1.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Adapter</td>
<td id="S5.T4.2.1.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">85.67M (29.51%)</td>
<td id="S5.T4.2.1.9.3" class="ltx_td ltx_align_center ltx_border_bb">52.44</td>
<td id="S5.T4.2.1.9.4" class="ltx_td ltx_align_center ltx_border_bb">/</td>
<td id="S5.T4.2.1.9.5" class="ltx_td ltx_align_center ltx_border_bb">/</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.4.2" class="ltx_text" style="font-size:90%;">Model analysis of ALBEF with partial freeze.</span></figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/Sequential_vilt_layer.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="508" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">ViLT + Seq</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/ER_vilt_layer.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="508" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">ViLT + ER</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F4.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/EWC_vilt_layer.png" id="S5.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="508" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F4.sf3.3.2" class="ltx_text" style="font-size:90%;">ViLT + EWC</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F4.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/seq_albef_layer.png" id="S5.F4.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="508" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F4.sf4.3.2" class="ltx_text" style="font-size:90%;">ALBEF + Seq</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F4.sf5" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/er_albef_layer.png" id="S5.F4.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="508" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S5.F4.sf5.3.2" class="ltx_text" style="font-size:90%;">ALBEF + ER</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F4.sf6" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/ewc_albef_layer.png" id="S5.F4.sf6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="508" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S5.F4.sf6.3.2" class="ltx_text" style="font-size:90%;">ALBEF + EWC</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.4.2" class="ltx_text" style="font-size:90%;">Layer-wise probing results for ViLT and ALBEF with different CL methods. Each line is the <span id="S5.F4.4.2.1" class="ltx_text ltx_font_italic">representation forgetting</span> after the model is trained with task i. (a-c): x-axis is the layer of ViLT encoder. (d-e): x-axis is the layer of the multimodal encoder of ALBEF.</span></figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/Sequential_vilt_task.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="564" height="635" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">ViLT + Seq</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/ER_vilt_task.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="564" height="635" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">ViLT + ER</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F5.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/EWC_vilt_task.png" id="S5.F5.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="564" height="635" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">ViLT + EWC</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F5.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/er_albef_task.png" id="S5.F5.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="564" height="635" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F5.sf4.3.2" class="ltx_text" style="font-size:90%;">ALBEF + ER</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.11.5.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.8.4" class="ltx_text" style="font-size:90%;">Task-wise probing results for ViLT and ALBEF model with different CL methods. The x-axis represents the model after training with task i, while the y-axis shows the <span id="S5.F5.8.4.1" class="ltx_text ltx_font_italic">representation accuracy</span>. (a-c): Line <math id="S5.F5.5.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.F5.5.1.m1.1b"><mi id="S5.F5.5.1.m1.1.1" xref="S5.F5.5.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.F5.5.1.m1.1c"><ci id="S5.F5.5.1.m1.1.1.cmml" xref="S5.F5.5.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.5.1.m1.1d">i</annotation></semantics></math> indicates the <math id="S5.F5.6.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.F5.6.2.m2.1b"><mi id="S5.F5.6.2.m2.1.1" xref="S5.F5.6.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.F5.6.2.m2.1c"><ci id="S5.F5.6.2.m2.1.1.cmml" xref="S5.F5.6.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.6.2.m2.1d">i</annotation></semantics></math>-th layer of ViLT encoder. (d): Line <math id="S5.F5.7.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.F5.7.3.m3.1b"><mi id="S5.F5.7.3.m3.1.1" xref="S5.F5.7.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.F5.7.3.m3.1c"><ci id="S5.F5.7.3.m3.1.1.cmml" xref="S5.F5.7.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.7.3.m3.1d">i</annotation></semantics></math> indicates the <math id="S5.F5.8.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.F5.8.4.m4.1b"><mi id="S5.F5.8.4.m4.1.1" xref="S5.F5.8.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.F5.8.4.m4.1c"><ci id="S5.F5.8.4.m4.1.1.cmml" xref="S5.F5.8.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.8.4.m4.1d">i</annotation></semantics></math>-th layer in the ALBEF multimodal encoder.</span></figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Impact of Vision and Text Encoder</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In this section, we focus exclusively on vision and text encoders adopted in ALBEF. In order to further investigate the roles of the individual modules of ALBEF, we design the following experiments: 1) <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">Freeze VE/TE</span>: we freeze the pre-trained vision encoder (or text encoder) and fine-tune the rest; 2) <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">Freeze VE+TE</span>: we freeze both vision and text encoders, and fine-tune the rest; 3) <span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_italic">Freeze B9</span>: besides vision and text encoder, we also freeze the bottom 3 layers of the multimodal encoder; 4) <span id="S5.SS1.p1.1.4" class="ltx_text ltx_font_italic">Freeze B12</span>: we freeze the full encoder, and only fine-tune the decoder.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">As shown in <a href="#S5.T4" title="In 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, the more modules are frozen, the less accurate the model is.
<span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">Freeze B12</span> has the lowest accuracy, reflecting that due to its low plasticity, <span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">Freeze B12</span> prevents its feature extractor from adapting to new domains by learning new representations. On the other hand, thanks to its high stability, it suffers less from forgetting.
Besides, its backward transfer performance indicates that the decoder also suffers from severe forgetting. Freezing the vision encoder or text encoder results in a similar performance to freezing the multimodal encoder.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Analysis of Multimodal Fusion Module</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this section, we investigate the impact of <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">multimodal fusion module</span> on CL performance.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Forgetting Analysis at Representation Level</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we first compute the average embedding for each class using the ground-truth label of all embeddings. Afterward, we re-classify all the embeddings by assigning the label of the closest class-wise embedding prototype and report the classification accuracy. We use the test set of each task and assume the ground-truth labels are available. Here, we provide the probing results for all 12 layers of ViLT as well as 6 layers in the multimodal encoder of ALBEF. We report the <span id="S5.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">representation forgetting</span>, <em id="S5.SS2.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS2.SSS1.p1.1.3" class="ltx_text"></span>, the average accuracy drop from the best performance on previous tasks, in <a href="#S5.F4" title="In 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Here, numbers smaller than zero mean there is positive knowledge transferred and benefits the performance of previous tasks. In <a href="#S5.F5" title="In 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, we provide the <span id="S5.SS2.SSS1.p1.1.4" class="ltx_text ltx_font_italic">representation accuracy</span> of different methods and discuss our findings in the following section.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/er_0.jpg" id="S5.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="99" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">Seq: Model after VQA Abstract</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/seq_1.jpg" id="S5.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="99" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">Seq: Model after PathVQA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/seq_2.jpg" id="S5.F6.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="99" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F6.sf3.3.2" class="ltx_text" style="font-size:90%;">Seq: Model after VQA-Med-2019</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/seq_4.jpg" id="S5.F6.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="99" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F6.sf4.3.2" class="ltx_text" style="font-size:90%;">Seq: Model after AQUA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf5" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/er_0.jpg" id="S5.F6.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="99" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S5.F6.sf5.3.2" class="ltx_text" style="font-size:90%;">ER: Model after VQA Abstract</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf6" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/er_1.jpg" id="S5.F6.sf6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="99" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S5.F6.sf6.3.2" class="ltx_text" style="font-size:90%;">ER: Model after PathVQA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf7" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/er_2.jpg" id="S5.F6.sf7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="99" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf7.2.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="S5.F6.sf7.3.2" class="ltx_text" style="font-size:90%;">ER: Model after VQA-Med-2019</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf8" class="ltx_figure ltx_figure_panel"><img src="/html/2211.10567/assets/Figures/er_4.jpg" id="S5.F6.sf8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="99" height="57" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf8.2.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span><span id="S5.F6.sf8.3.2" class="ltx_text" style="font-size:90%;">ER: Model after AQUA</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Grad-CAM visualization on the cross-attention maps of the multimodal encoder of ALBEF. Seq: sequential fine-tuning baseline. The task order is shown in <a href="#S1.F1" title="In 1 Introduction ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Question: What color is the bench? Ground truth answer: Brown. </span></figcaption>
</figure>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Results and Discussions</h4>

<section id="S5.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">The deeper layers of the single-stream model are most affected by forgetting.</h5>

<div id="S5.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS2.Px1.p1.1" class="ltx_p">For single-stream architecture ViLT, the amount of representation forgetting increases with the increase of layers (see <a href="#S5.F4.sf1" title="In Figure 4 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a>). For the lower layers, the representation forgetting stays relatively flat or decreases slightly, showing that the bottom layers barely suffer from forgetting and a positive backward transfer is occurring. However, the representation forgetting increases significantly at layer 7, and then the deeper the layer, the more representation forgetting we observe. This phenomenon may be due to the fact that in the lower layers, the model only learns general knowledge. However, the model tends to learn more task-specific features in the upper layers, which are easily forgotten and corrupted when constantly learning new tasks. Moreover, we find that the representation accuracy of the last layer is lower than the second last layer (see <a href="#S5.F5.sf1" title="In Figure 5 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(a)</span></a> and <a href="#S5.F5.sf3" title="In Figure 5 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(c)</span></a>), suggesting that applying another classification method, <em id="S5.SS2.SSS2.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS2.SSS2.Px1.p1.1.2" class="ltx_text"></span>, metric learning, on the representations extracted from the second to last layer might be a more viable approach to adapt the single-stream models to the CL environment. We assume that this is caused by the introduction of a task-specific classifier, which harms the representation ability of the final layer embedding when switching between tasks.</p>
</div>
</section>
<section id="S5.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">ER helps align the single-stream encoder with the task-specific classifier.</h5>

<div id="S5.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.Px2.p1.1" class="ltx_p">Comparing <a href="#S5.F4.sf1" title="In Figure 4 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a> with <a href="#S5.F4.sf2" title="In Figure 4 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(b)</span></a>, we notice that the representation forgetting of ER is similar to that of the sequential fine-tuning baseline, however, ER achieves much higher backward transfer than the baseline (see <a href="#S4.T2" title="In 4 Experiments ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>). We believe that ER and other replay-based approaches may assist in aligning the encoder with the task-specific classifier. In particular, it is worth noting that, although the forgetting in terms of the traditional measure is high for EWC (see <a href="#S4.T2" title="In 4 Experiments ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>), the representation forgetting shown in <a href="#S5.F4.sf3" title="In Figure 4 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(c)</span></a> indicates that EWC is more effective at mitigating forgetting of the middle layers. However, it lacks the ability to constrain the classifier when a task-specific classifier needs to be utilized.</p>
</div>
</section>
<section id="S5.SS2.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fusion module of a dual-stream model forgets less than that of a single-stream model.</h5>

<div id="S5.SS2.SSS2.Px3.p1" class="ltx_para">
<p id="S5.SS2.SSS2.Px3.p1.1" class="ltx_p">Comparing <a href="#S5.F4.sf2" title="In Figure 4 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(b)</span></a> <a href="#S5.F4.sf3" title="In Figure 4 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(c)</span></a> with <a href="#S5.F4.sf5" title="In Figure 4 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(e)</span></a> and <a href="#S5.F4.sf6" title="In Figure 4 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4(f)</span></a>, we observe that ALBEF shows much milder representation forgetting than ViLT, which is also evidenced by the best CL performance achieved by ALBEF (see <a href="#S4.T2" title="In 4 Experiments ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>). Besides, we notice that across all CL methods, the representativeness of the layers within the multimodal encoder of ALBEF does not vary as dramatically as ViLT, possibly because the forgetting of ALBEF occurs mainly in the vision encoder, text encoder, and the decoder, which is also evidenced by <a href="#S5.T4" title="In 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Also, as shown in <a href="#S5.F5.sf4" title="In Figure 5 ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(d)</span></a>, the multimodal encoder of ALBEF extracts representations with higher separability.</p>
</div>
</section>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Visualization</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p"><a href="#S5.F6" title="In 5.2.1 Forgetting Analysis at Representation Level ‣ 5.2 Analysis of Multimodal Fusion Module ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows the Grad-CAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> visualization changes of the continually adapted VLPM on a selected image from the first task. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, we compute the Grad-CAM on the cross-attention maps in the 3rd layer of the multimodel encoder in ALBEF for the VQA task. From the first row in <a href="#S5.F6" title="In 5.2.1 Forgetting Analysis at Representation Level ‣ 5.2 Analysis of Multimodal Fusion Module ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we see that the model attention shifts from the target object to almost every possible object existing in the image after being trained on 2 medical datasets, which we assume are due to the large domain shift in the visual aspect. <a href="#S5.F6.sf4" title="In Figure 6 ‣ 5.2.1 Forgetting Analysis at Representation Level ‣ 5.2 Analysis of Multimodal Fusion Module ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6(d)</span></a> shows that the model attention again focuses on a certain object but fails to target the correct one, which is a predictable behavior since, in the AQUA dataset, objects are also abstractions of real-life objects. The second row of <a href="#S5.F6" title="In 5.2.1 Forgetting Analysis at Representation Level ‣ 5.2 Analysis of Multimodal Fusion Module ‣ 5 Model Analysis ‣ CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows the same Grad-CAM visualization of ALBEF optimized with ER. We notice that by replaying instances from the VQA Abstract dataset, the model maintains the ability to distinguish abstract objects and connect them with the questions while learning new tasks.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we present CL-CrossVQA, the first VQA benchmark for cross-domain continual learning. We systematically investigate which type of CL method is the most effective for VLPMs, and how model design impacts CL performance. Our comprehensive experimental results reveal that: 1) Dual-stream encoder-decoder model exhibits alleviation of forgetting; (2) The deeper layers of single-stream encoder-only models are most affected by forgetting; 3) Replay-based approaches are the most effective and are less sensitive to task orders; 4) Adapter is a promising approach in this cross-domain continual learning scenario. We believe that the insights gained by dissecting the inner architecture of VLPMs will inform the development of CL approaches suitable for multimodal continual learning.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence
Zitnick, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
Tinne Tuytelaars.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Memory aware synapses: Learning what (not) to forget.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 139–154, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 2425–2433, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Craig Atkinson, Brendan McCane, Lech Szymanski, and Anthony Robins.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Pseudo-recursal: Solving the catastrophic forgetting problem in deep
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.03875</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">
Asma Ben Abacha, Sadid A. Hasan, Vivek V. Datla, Joey Liu, Dina
Demner-Fushman, and Henning Müller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">Vqa-med: Overview of the medical visual question answering task at
imageclef 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.6.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CLEF 2019 Working Notes</span><span id="bib.bib5.7.3" class="ltx_text" style="font-size:90%;">, CEUR Workshop Proceedings,
Lugano, Switzerland, September 9-12 2019. CEUR-WS.org
</span><math id="bib.bib5.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="bib.bib5.1.m1.1a"><mo mathsize="90%" id="bib.bib5.1.m1.1.1" xref="bib.bib5.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="bib.bib5.1.m1.1b"><lt id="bib.bib5.1.m1.1.1.cmml" xref="bib.bib5.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="bib.bib5.1.m1.1c">&lt;</annotation></semantics></math><span id="bib.bib5.8.4" class="ltx_text" style="font-size:90%;">http://ceur-ws.org</span><math id="bib.bib5.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="bib.bib5.2.m2.1a"><mo mathsize="90%" id="bib.bib5.2.m2.1.1" xref="bib.bib5.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="bib.bib5.2.m2.1b"><gt id="bib.bib5.2.m2.1.1.cmml" xref="bib.bib5.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="bib.bib5.2.m2.1c">&gt;</annotation></semantics></math><span id="bib.bib5.9.5" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
Calderara.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Dark experience for general continual learning: a strong, simple
baseline.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">,
33:15920–15930, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Paola Cascante-Bonilla, Hui Wu, Letao Wang, Rogerio S Feris, and Vicente
Ordonez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Simvqa: Exploring simulated environments for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 5056–5066, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Wei-Lun Chao, Hexiang Hu, and Fei Sha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Cross-dataset adaptation for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 5716–5725, 2018.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Efficient lifelong learning with a-gem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.00420</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan,
Puneet K Dokania, Philip HS Torr, and M Ranzato.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Continual learning with tiny episodic memories.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu
Cheng, and Jingjing Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Uniter: Learning universal image-text representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Georgios Chochlakis, Tejas Srinivasan, Jesse Thomason, and Shrikanth Narayanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Vault: Augmenting the vision-and-language transformer with the
propagation of deep language representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.09021</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
MohammadReza Davari, Nader Asadi, Sudhir Mudur, Rahaf Aljundi, and Eugene
Belilovsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Probing representation forgetting in supervised and unsupervised
continual learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 16712–16721, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.04805</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Noa Garcia and George Vogiatzis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">How to read paintings: Semantic art understanding with multi-modal
retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Noa Garcia, Chentao Ye, Zihua Liu, Qingtao Hu, Mayu Otani, Chenhui Chu, Yuta
Nakashima, and Teruko Mitamura.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">A dataset and baselines for visual question answering on art.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference in Computer Vision
Workshops</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Claudio Greco, Barbara Plank, Raquel Fernández, and Raffaella Bernardi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Psycholinguistics meets continual learning: Measuring catastrophic
forgetting in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.04229</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Pathvqa: 30000+ questions for medical visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.10286</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Parameter-efficient transfer learning for nlp.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages
2790–2799. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
David Isele and Akansel Cosgun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Selective experience replay for lifelong learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, volume 32, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C
Lawrence Zitnick, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 2901–2910, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Heechul Jung, Jeongwoo Ju, Minju Jung, and Junmo Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Less-forgetting learning in deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1607.00122</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Wonjae Kim, Bokyung Son, and Ildoo Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Vilt: Vision-and-language transformer without convolution or region
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages
5583–5594. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Overcoming catastrophic forgetting in neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the national academy of sciences</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">,
114(13):3521–3526, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Frantzeska Lavda, Jason Ramapuram, Magda Gregorova, and Alexandros Kalousis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Continual classification learning using generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.10612</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Overcoming catastrophic forgetting by incremental moment matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Stan Weixian Lei, Difei Gao, Jay Zhangjie Wu, Yuxuan Wang, Wei Liu, Mengmi
Zhang, and Mike Zheng Shou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Symbolic replay: Scene graph as prompt for continual learning on vqa
task.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.12037</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
and Steven Chu Hong Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Align before fuse: Vision and language representation learning with
momentum distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">,
34:9694–9705, 2021.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Visualbert: A simple and performant baseline for vision and language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.03557</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Zhizhong Li and Derek Hoiem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Learning without forgetting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">,
40(12):2935–2947, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B.
Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll’a r, and
C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Microsoft COCO: common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, abs/1405.0312, 2014.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
David Lopez-Paz and Marc’Aurelio Ranzato.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Gradient episodic memory for continual learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Martial Mermillod, Aurélia Bugaiska, and Patrick Bonin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">The stability-plasticity dilemma: Investigating the continuum from
catastrophic forgetting to age-limited learning effects, 2013.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H
Lampert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">icarl: Incremental classifier and representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 2001–2010, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Mengye Ren, Ryan Kiros, and Richard S. Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Image question answering: A visual semantic embedding model and a
new dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, abs/1505.02074, 2015.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 28, 2015.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
Wayne.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Experience replay for continual learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell,
Devi Parikh, and Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Grad-cam: Why did you say that? visual explanations from deep
networks via gradient-based localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, abs/1610.02391, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Continual learning with deep generative replay.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech
Galuba, Marcus Rohrbach, and Douwe Kiela.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Flava: A foundational language and vision alignment model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 15638–15650, 2022.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Tejas Srinivasan, Ting-Yun Chang, Leticia Leonor Pinto Alva, Georgios
Chochlakis, Mohammad Rostami, and Jesse Thomason.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Climb: A continual learning benchmark for vision-and-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.09059</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Vl-bert: Pre-training of generic visual-linguistic representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.08530</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Hao Tan and Mohit Bansal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Lxmert: Learning cross-modality encoder representations from
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.07490</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Simvlm: Simple visual language model pretraining with weak
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2108.10904</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Yiming Xu, Lin Chen, Zhongwei Cheng, Lixin Duan, and Jiebo Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Open-ended visual question answering by multi-modal domain
adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.04058</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Friedemann Zenke, Ben Poole, and Surya Ganguli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Continual learning through synaptic intelligence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages
3987–3995. PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry
Heck, Heming Zhang, and C-C Jay Kuo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Class-incremental learning via deep model consolidation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 1131–1140, 2020.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Mingda Zhang, Tristan Maidment, Ahmad Diab, Adriana Kovashka, and Rebecca Hwa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Domain-robust vqa with diverse datasets and methods but no target
labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 7046–7056, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.10565" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.10567" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.10567">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.10567" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.10568" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 08:09:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
