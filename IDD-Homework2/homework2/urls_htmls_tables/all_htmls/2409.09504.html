<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.09504] Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language</title><meta property="og:description" content="With the increasing popularity of daily information sharing and acquisition on the Internet, this paper introduces an innovative approach for intent classification in Bangla language, focusing on social media posts whe…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.09504">

<!--Generated on Sun Oct  6 01:38:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">Fatema Tuj Johora Faria <sup id="id1.1.id1" class="ltx_sup">*</sup>,
Mukaffi Bin Moin,
Md. Mahfuzur Rahman,
Md Morshed Alam Shanto,

<br class="ltx_break">Asif Iftekher Fahim,
Md. Moinul Hoque

<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">Ahsanullah University of Science and Technology, Dhaka, Bangladesh.

<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">*Corresponding author(s). E-mail(s): <span id="id2.2.id2" class="ltx_text ltx_font_typewriter" style="color:#0000FF;">fatema.faria142@gmail.com
<br class="ltx_break"></span>Contributing authors: <span id="id3.3.id3" class="ltx_text ltx_font_typewriter" style="color:#0000FF;">mukaffi28@gmail.com</span>; <span id="id4.4.id4" class="ltx_text ltx_font_typewriter" style="color:#0000FF;">mahim1066@gmail.com</span>; 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter" style="color:#0000FF;">mdmorshedalamlipson@gmail.com</span>; <span id="id6.6.id6" class="ltx_text ltx_font_typewriter" style="color:#0000FF;">fahimthescientist@gmail.com</span>; <span id="id7.7.id7" class="ltx_text ltx_font_typewriter" style="color:#0000FF;">moinul.cse@aust.edu</span>;

<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">With the increasing popularity of daily information sharing and acquisition on the Internet, this paper introduces an innovative approach for intent classification in Bangla language, focusing on social media posts where individuals share their thoughts and opinions. The proposed method leverages multimodal data with particular emphasis on authorship identification, aiming to understand the underlying purpose behind textual content, especially in the context of varied user-generated posts on social media. Current methods often face challenges in low-resource languages like Bangla, particularly when author traits intricately link with intent, as observed in social media posts. To address this, we present the Multimodal-based Author Bangla Intent Classification (MABIC) framework, utilizing text and images to gain deeper insights into the conveyed intentions. We have created a dataset named “Uddessho,” comprising 3,048 instances sourced from social media. Our methodology comprises two approaches for classifying textual intent and multimodal author intent, incorporating early fusion and late fusion techniques. In our experiments, the unimodal approach achieved an accuracy of 64.53% in interpreting Bangla textual intent. In contrast, our multimodal approach significantly outperformed traditional unimodal methods, achieving an accuracy of 76.19%. This represents an improvement of 11.66%. To our best knowledge, this is the first research work on multimodal-based author intent classification for low-resource Bangla language social media posts.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.4" class="ltx_p"><em id="p1.4.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.4.2" class="ltx_text ltx_font_bold">eywords</span> Author Intent Classification  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Low Resource Language  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Multimodal Deep Learning  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
Early Fusion  <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
Late Fusion.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">In the era of digital communication, understanding the underlying intentions behind textual content has become increasingly vital, especially in the context of social media where users express a wide range of thoughts and emotions. Multimodal social media platforms like Facebook, Instagram, and X (previously known as Twitter) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> allow content creators to merge visual and textual components. The extensive use of text and images in social media posts underscores the necessity of accurately determining author intent within multimodal content, particularly in the context of the Bangla language. This poses a significant challenge for Natural Language Processing (NLP) in document understanding. Traditional approaches to intent classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> primarily rely on text-based unimodal methods, which often struggle to fully capture the context and nuanced meanings inherent in multimodal posts. This limitation becomes particularly apparent when interpreting author intentions embedded within both visual and textual components.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">This paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> addressed Bangla intent classification in natural language understanding with BNIntent30, a Bengali dataset of 30 intent classes. The GAN-BnBERT approach, integrating Generative Adversarial BERT with contextual embeddings, outperformed existing models. However, the focus on unimodal text-based methods limited the model’s ability to capture contextual information from visual cues in social media posts. In another study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, a ensemble learning classifier was utilized for precise intent discernment in identifying hate speech on Tumblr. However, because it was limited to posts in English, it ignored subtleties in Bangla content, revealing a lack of linguistic inclusivity. Conversely, the paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> on intent detection and slot filling in Sylheti and Bangla gave preference to unimodal text-based techniques and GPT-3.5 over JointBERT. However, the absence of investigation into multimodal techniques in this study limited the possibility of interpreting the author’s aim in a more complex way.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Our research proposes a more comprehensive approach to author intent classification, combining textual and visual analysis to capture hidden meanings. We highlight the importance of MABIC over typical unimodal approaches for analyzing social media postings. By addressing the constraints of text-only techniques, we emphasize the value of multimodal methodologies in revealing the multifaceted characteristics of author intent. The contributions of this research paper can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix1.1.1.m1.1b"><mo id="S1.I1.ix1.1.1.m1.1.1" xref="S1.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix1.1.1.m1.1c"><ci id="S1.I1.ix1.1.1.m1.1.1.cmml" xref="S1.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix1.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.ix1.p1.1" class="ltx_p">We created the <span id="S1.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">“Uddessho”</span> or “Ued/dshY” in Bangla, which means “Intent” in English, dataset for multimodal author intent classification, containing 3048 post instances across six categories: Informative, Advocative, Promotive, Exhibitionist, Expressive, and Controversial. The dataset is split into a training set with 2423 posts, a testing set with 313 posts, and a validation set with 312 posts, totaling 3048 posts.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix2.1.1.m1.1b"><mo id="S1.I1.ix2.1.1.m1.1.1" xref="S1.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix2.1.1.m1.1c"><ci id="S1.I1.ix2.1.1.m1.1.1.cmml" xref="S1.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix2.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.ix2.p1.1" class="ltx_p">We investigated fusion techniques within the <span id="S1.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">MABIC framework</span>, such as Early and Late Fusion, which integrate features from multiple modalities. Our exploration showcased the superiority of multimodal approaches over text-based unimodal methods, underscoring the significance of incorporating diverse modalities for comprehensive author intent analysis.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.I1.ix3.1.1.m1.1b"><mo id="S1.I1.ix3.1.1.m1.1.1" xref="S1.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix3.1.1.m1.1c"><ci id="S1.I1.ix3.1.1.m1.1.1.cmml" xref="S1.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S1.I1.ix3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.ix3.p1.1" class="ltx_p">In our <span id="S1.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">qualitative error</span> investigation, We discovered that it was quite difficult to understand confusing textual and visual content. These challenges are especially noticeable when attempting to understand the author’s hidden intentions, which are distinctive to the low resource Bangla language.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Julia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> examined text-image dynamics in social media, unveiling “meaning multiplication.” Their annotated dataset and fusion techniques highlighted the importance of understanding nuanced interactions between text and image. Additionally, their findings advocate for deeper exploration of divergent meanings in future investigations. Lu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, on the other hand, proposed the MMIA approach, revolutionizing multimodal marketing intent analysis. Their method surpassed previous techniques in detecting intent within social news, outperforming SupDocNADE. Furthermore, MMIA excelled in topic identification, signifying a significant advancement in understanding and deciphering complex multimodal cues within marketing contexts. Shaozu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> introduced OCRBERT and VisualBERT for multimodal conversational intent classification in E-commerce. Both models, leveraging the MCIC dataset, outperformed baseline BERT in accuracy. Notably, VisualBERT showcased superior performance in capturing image regions, while OCRBERT emphasized the importance of OCR text integration for effective classification. Lastly, Mehedi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> introduced BNIntent30, a Bengali intent classification dataset emphasizing linguistic diversity. They proposed GAN-BnBERT, integrating GAN with BERT to enhance classification. Experimental results showcased GAN-BnBERT’s superiority. Although previous research has made considerable progress in unimodal and multimodal intent classification across multiple domains, there is still a significant gap in implementing these techniques to low-resource languages such as Bangla.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Creation</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_bold">Data Collection</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">For our study, we manually collected a dataset of Bangla language social media posts from Facebook, X, and Instagram. We targeted popular accounts and groups with high engagement and diverse content to capture various author intents. Posts were selected based on the presence of both text and image, relevance to common Bangla social media topics, and diversity of expressed intents. Themes covered include personal updates, food reviews, sports, political news, entertainment, tourism, technology, pet, and promotional content.
</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_bold">Dataset Characteristics</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The dataset comprises “Image_ID” for image identification, “Image_Caption” for associated text, and “Intent_Taxonomy” for categorizing intentions. In our research, we defined six categories within the “Uddessho” (“Ued/dshY”) dataset. Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Dataset Characteristics ‣ 3 Dataset Creation ‣ Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> displays the statistics of the dataset. this research is publicly accessible through: <a target="_blank" href="https://data.mendeley.com/datasets/mzxmt8tfjs/1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://data.mendeley.com/datasets/mzxmt8tfjs/1</a></p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix1.1.1.m1.1b"><mo id="S3.I1.ix1.1.1.m1.1.1" xref="S3.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix1.1.1.m1.1c"><ci id="S3.I1.ix1.1.1.m1.1.1.cmml" xref="S3.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Advocative:</span> Posts advocating for a cause, figure, or idea, aiming to persuade others to support the advocated position.</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix2.1.1.m1.1b"><mo id="S3.I1.ix2.1.1.m1.1.1" xref="S3.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix2.1.1.m1.1c"><ci id="S3.I1.ix2.1.1.m1.1.1.cmml" xref="S3.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p"><span id="S3.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Promotive:</span> Posts promoting events, products, or organizations to raise awareness and encourage participation.</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix3.1.1.m1.1b"><mo id="S3.I1.ix3.1.1.m1.1.1" xref="S3.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix3.1.1.m1.1c"><ci id="S3.I1.ix3.1.1.m1.1.1.cmml" xref="S3.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p"><span id="S3.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Exhibitionist:</span> Posts showcasing personal identity, lifestyle, or belongings, such as selfies or pictures of pets.</p>
</div>
</li>
<li id="S3.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix4.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix4.1.1.m1.1b"><mo id="S3.I1.ix4.1.1.m1.1.1" xref="S3.I1.ix4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix4.1.1.m1.1c"><ci id="S3.I1.ix4.1.1.m1.1.1.cmml" xref="S3.I1.ix4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix4.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix4.p1" class="ltx_para">
<p id="S3.I1.ix4.p1.1" class="ltx_p"><span id="S3.I1.ix4.p1.1.1" class="ltx_text ltx_font_bold">Expressive:</span> Posts expressing emotions, attachments, or admiration towards others.</p>
</div>
</li>
<li id="S3.I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix5.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix5.1.1.m1.1b"><mo id="S3.I1.ix5.1.1.m1.1.1" xref="S3.I1.ix5.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix5.1.1.m1.1c"><ci id="S3.I1.ix5.1.1.m1.1.1.cmml" xref="S3.I1.ix5.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix5.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix5.p1" class="ltx_para">
<p id="S3.I1.ix5.p1.1" class="ltx_p"><span id="S3.I1.ix5.p1.1.1" class="ltx_text ltx_font_bold">Informative:</span> Posts providing factual information about subjects or events, enriching understanding.</p>
</div>
</li>
<li id="S3.I1.ix6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S3.I1.ix6.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S3.I1.ix6.1.1.m1.1b"><mo id="S3.I1.ix6.1.1.m1.1.1" xref="S3.I1.ix6.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S3.I1.ix6.1.1.m1.1c"><ci id="S3.I1.ix6.1.1.m1.1.1.cmml" xref="S3.I1.ix6.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.ix6.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S3.I1.ix6.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.ix6.p1.1" class="ltx_p"><span id="S3.I1.ix6.p1.1.1" class="ltx_text ltx_font_bold">Controversial:</span> Posts aiming to provoke debate or shock viewers by addressing taboo or contentious subjects.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Distribution of Dataset Splits Across Different Intent Categories</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_bold">Intent_Taxonomy</span></td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.2.1" class="ltx_text ltx_font_bold">Train</span></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.3.1" class="ltx_text ltx_font_bold">Test</span></td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.4.1" class="ltx_text ltx_font_bold">Validation</span></td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Informative</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">514</td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">67</td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t">67</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_left">Advocative</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_nopad_l ltx_align_center">386</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_nopad_l ltx_align_center">49</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">49</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_left">Promotive</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_nopad_l ltx_align_center">315</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_nopad_l ltx_align_center">43</td>
<td id="S3.T1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">42</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_left">Exhibitionist</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_nopad_l ltx_align_center">371</td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_nopad_l ltx_align_center">47</td>
<td id="S3.T1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">48</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_left">Expressive</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_nopad_l ltx_align_center">518</td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_nopad_l ltx_align_center">66</td>
<td id="S3.T1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">66</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">Controversial</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">319</td>
<td id="S3.T1.1.7.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">41</td>
<td id="S3.T1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb">40</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_bold">Annotation Guideline and Quality Maintenance</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">We recruited six annotators, aged (22-25) years, all undergraduates active on social media with a strong understanding of intention posts. They were compensated according to the Bangladesh wage payscale for their work. We provided annotators with clear guidelines for Multimodal Bangla Author Intent Classification. Aligning captions and images is essential for contextual comprehension, with any discrepancies documented for research. Our hired annotators classified postings based on language, visuals, and context, using a predefined taxonomy. They evaluated posts in their entirety, focusing on consistency while adhering to norms and conducting common validation tests. Each annotation includes transparent evidence of the rationale and decision-making process. Ongoing feedback sessions improve annotation skills and strengthen understanding of Bangla social media content intent. We achieved a Fleiss Kappa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> value of 0.84, suggesting strong agreement among annotators.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed Methodology</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We introduce the <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">“MABIC”</span> framework, which combines unimodal text-based and multimodal-based classification approaches. The MABIC framework, which comprises the Early Fusion and Late Fusion fusion procedures, is depicted in Figure <a href="#S4.F1" title="Figure 1 ‣ 4.2 Approach 2: Multimodal Bangla Author Intent Classification ‣ 4 Proposed Methodology ‣ Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The codes and additional coding files for this research are available to the public at this link: <a target="_blank" href="https://github.com/fatemafaria142/Uddessho-An-Benchmark-Dataset-for-Multimodal-Author-Intent-Classification-in-Bangla-Language" title="" class="ltx_ref ltx_href">Github</a></p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_bold">Approach 1: Bangla Author Intent Text Classification</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">In this approach, we begin with text preprocessing to clean and standardize Bangla text, which involves tasks such as removing emojis, normalizing whitespaces, and stripping punctuation and special symbols for consistency. Next, we develop text-based intent classification models using state-of-the-art (SOTA) pre-trained language models like mBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, DistilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and XLM-RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. These models are fine-tuned on our dataset to leverage their advanced semantic comprehension capabilities. Following model development, we perform hyperparameter tuning with a batch size of 16, learning rate set to 0.001, and epochs ranging from 15 to 25 to optimize model performance. Finally, we evaluate our classification models, analyzing the results as presented in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Quantitative Analysis ‣ 5 Result Analysis ‣ Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> to gain valuable insights.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_bold">Approach 2: Multimodal Bangla Author Intent Classification</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">In order to comprehend author intent in Bangla content, a thorough examination of both textual and visual data is necessary. Our approach makes use of cutting-edge CNN architectures and pre-trained language models to accurately capture the complex patterns and representations required for intent classification. Our goal is to improve the classification model’s accuracy by combining features from both modalities. We have divided our methodology into distinct steps to systematically address the various components of the classification process.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2409.09504/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="145" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The diagram illustrates the MABIC framework, which has two fusion approaches: Early Fusion and Late Fusion.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Step 1) Text Features:</span>
We utilize pre-trained language models such as mBERT, DistilBERT, and XLM-RoBERTa. These models are adept at capturing intricate linguistic patterns in Bangla content, providing a comprehensive representation of textual data.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Step 2) Image Features:</span>
For visual representations, we employ eight different types of CNN architectures: ResNet 50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, ResNet 101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, ResNet 152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, DenseNet 121 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, DenseNet 169 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, DenseNet 201 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, MobileNet V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and MobileNet V3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. These CNNs enable us to extract detailed features from images, providing the dataset with valuable visual context.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Step 3) Fusion Techniques:</span>
We integrate textual and visual data using Early Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and Late Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> methods to enhance classification accuracy:</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix1.1.1.m1.1b"><mo id="S4.I1.ix1.1.1.m1.1.1" xref="S4.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.1.1.m1.1c"><ci id="S4.I1.ix1.1.1.m1.1.1.cmml" xref="S4.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I1.ix1.p1" class="ltx_para ltx_noindent">
<p id="S4.I1.ix1.p1.2" class="ltx_p"><span id="S4.I1.ix1.p1.2.1" class="ltx_text ltx_font_bold">Early Fusion</span>: This method combines text and image features at the input level, enabling the model to jointly learn from both modalities from the outset. This integrated learning process helps capture complex interactions between text and visual data. Mathematically, if <math id="S4.I1.ix1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}" display="inline"><semantics id="S4.I1.ix1.p1.1.m1.1a"><mi id="S4.I1.ix1.p1.1.m1.1.1" xref="S4.I1.ix1.p1.1.m1.1.1.cmml">𝐓</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.p1.1.m1.1b"><ci id="S4.I1.ix1.p1.1.m1.1.1.cmml" xref="S4.I1.ix1.p1.1.m1.1.1">𝐓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.p1.1.m1.1c">\mathbf{T}</annotation></semantics></math> represents the text feature vector and <math id="S4.I1.ix1.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S4.I1.ix1.p1.2.m2.1a"><mi id="S4.I1.ix1.p1.2.m2.1.1" xref="S4.I1.ix1.p1.2.m2.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.p1.2.m2.1b"><ci id="S4.I1.ix1.p1.2.m2.1.1.cmml" xref="S4.I1.ix1.p1.2.m2.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.p1.2.m2.1c">\mathbf{I}</annotation></semantics></math> represents the image feature vector, Early Fusion can be expressed as:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\mathbf{F}_{\text{early}}=f(\mathbf{W}_{T}\mathbf{T}+\mathbf{W}_{I}\mathbf{I}+\mathbf{b})" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><msub id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">𝐅</mi><mtext id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3a.cmml">early</mtext></msub><mo id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.cmml"><msub id="S4.E1.m1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.1.1.1.1.1.1.2.2.2.cmml">𝐖</mi><mi id="S4.E1.m1.1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3.cmml">T</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.1.1.2.1" xref="S4.E1.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.1.1.2.3.cmml">𝐓</mi></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.3.cmml"><msub id="S4.E1.m1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.3.2.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.3.2.2" xref="S4.E1.m1.1.1.1.1.1.1.3.2.2.cmml">𝐖</mi><mi id="S4.E1.m1.1.1.1.1.1.1.3.2.3" xref="S4.E1.m1.1.1.1.1.1.1.3.2.3.cmml">I</mi></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.3.3.cmml">𝐈</mi></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.1a" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S4.E1.m1.1.1.1.1.1.1.4" xref="S4.E1.m1.1.1.1.1.1.1.4.cmml">𝐛</mi></mrow><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"></eq><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">𝐅</ci><ci id="S4.E1.m1.1.1.3.3a.cmml" xref="S4.E1.m1.1.1.3.3"><mtext mathsize="70%" id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3">early</mtext></ci></apply><apply id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><times id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.2"></times><ci id="S4.E1.m1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.3">𝑓</ci><apply id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><plus id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"></plus><apply id="S4.E1.m1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2"><times id="S4.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.1"></times><apply id="S4.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2.2">𝐖</ci><ci id="S4.E1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.2.3">𝑇</ci></apply><ci id="S4.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2.3">𝐓</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3"><times id="S4.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.1"></times><apply id="S4.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.2.2">𝐖</ci><ci id="S4.E1.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.2.3">𝐼</ci></apply><ci id="S4.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3.3">𝐈</ci></apply><ci id="S4.E1.m1.1.1.1.1.1.1.4.cmml" xref="S4.E1.m1.1.1.1.1.1.1.4">𝐛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\mathbf{F}_{\text{early}}=f(\mathbf{W}_{T}\mathbf{T}+\mathbf{W}_{I}\mathbf{I}+\mathbf{b})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.I1.ix1.p2" class="ltx_para ltx_noindent">
<p id="S4.I1.ix1.p2.4" class="ltx_p">where <math id="S4.I1.ix1.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{W}_{T}" display="inline"><semantics id="S4.I1.ix1.p2.1.m1.1a"><msub id="S4.I1.ix1.p2.1.m1.1.1" xref="S4.I1.ix1.p2.1.m1.1.1.cmml"><mi id="S4.I1.ix1.p2.1.m1.1.1.2" xref="S4.I1.ix1.p2.1.m1.1.1.2.cmml">𝐖</mi><mi id="S4.I1.ix1.p2.1.m1.1.1.3" xref="S4.I1.ix1.p2.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.p2.1.m1.1b"><apply id="S4.I1.ix1.p2.1.m1.1.1.cmml" xref="S4.I1.ix1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I1.ix1.p2.1.m1.1.1.1.cmml" xref="S4.I1.ix1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.I1.ix1.p2.1.m1.1.1.2.cmml" xref="S4.I1.ix1.p2.1.m1.1.1.2">𝐖</ci><ci id="S4.I1.ix1.p2.1.m1.1.1.3.cmml" xref="S4.I1.ix1.p2.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.p2.1.m1.1c">\mathbf{W}_{T}</annotation></semantics></math> and <math id="S4.I1.ix1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{W}_{I}" display="inline"><semantics id="S4.I1.ix1.p2.2.m2.1a"><msub id="S4.I1.ix1.p2.2.m2.1.1" xref="S4.I1.ix1.p2.2.m2.1.1.cmml"><mi id="S4.I1.ix1.p2.2.m2.1.1.2" xref="S4.I1.ix1.p2.2.m2.1.1.2.cmml">𝐖</mi><mi id="S4.I1.ix1.p2.2.m2.1.1.3" xref="S4.I1.ix1.p2.2.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.p2.2.m2.1b"><apply id="S4.I1.ix1.p2.2.m2.1.1.cmml" xref="S4.I1.ix1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.I1.ix1.p2.2.m2.1.1.1.cmml" xref="S4.I1.ix1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.I1.ix1.p2.2.m2.1.1.2.cmml" xref="S4.I1.ix1.p2.2.m2.1.1.2">𝐖</ci><ci id="S4.I1.ix1.p2.2.m2.1.1.3.cmml" xref="S4.I1.ix1.p2.2.m2.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.p2.2.m2.1c">\mathbf{W}_{I}</annotation></semantics></math> are weight matrices for text and image features respectively, <math id="S4.I1.ix1.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{b}" display="inline"><semantics id="S4.I1.ix1.p2.3.m3.1a"><mi id="S4.I1.ix1.p2.3.m3.1.1" xref="S4.I1.ix1.p2.3.m3.1.1.cmml">𝐛</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.p2.3.m3.1b"><ci id="S4.I1.ix1.p2.3.m3.1.1.cmml" xref="S4.I1.ix1.p2.3.m3.1.1">𝐛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.p2.3.m3.1c">\mathbf{b}</annotation></semantics></math> is the bias term, and <math id="S4.I1.ix1.p2.4.m4.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.I1.ix1.p2.4.m4.1a"><mi id="S4.I1.ix1.p2.4.m4.1.1" xref="S4.I1.ix1.p2.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.p2.4.m4.1b"><ci id="S4.I1.ix1.p2.4.m4.1.1.cmml" xref="S4.I1.ix1.p2.4.m4.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.p2.4.m4.1c">f</annotation></semantics></math> is the activation function.
<br class="ltx_break"></p>
</div>
</li>
<li id="S4.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix2.1.1.m1.1b"><mo id="S4.I1.ix2.1.1.m1.1.1" xref="S4.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.1.1.m1.1c"><ci id="S4.I1.ix2.1.1.m1.1.1.cmml" xref="S4.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I1.ix2.p1" class="ltx_para ltx_noindent">
<p id="S4.I1.ix2.p1.2" class="ltx_p"><span id="S4.I1.ix2.p1.2.1" class="ltx_text ltx_font_bold">Late Fusion</span>: In this approach, text and image features are processed separately before being merged. This allows each modality to be independently refined, preserving their distinct nuances and contributing to a more nuanced final representation. If <math id="S4.I1.ix2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}" display="inline"><semantics id="S4.I1.ix2.p1.1.m1.1a"><mi id="S4.I1.ix2.p1.1.m1.1.1" xref="S4.I1.ix2.p1.1.m1.1.1.cmml">𝐓</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p1.1.m1.1b"><ci id="S4.I1.ix2.p1.1.m1.1.1.cmml" xref="S4.I1.ix2.p1.1.m1.1.1">𝐓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p1.1.m1.1c">\mathbf{T}</annotation></semantics></math> and <math id="S4.I1.ix2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{I}" display="inline"><semantics id="S4.I1.ix2.p1.2.m2.1a"><mi id="S4.I1.ix2.p1.2.m2.1.1" xref="S4.I1.ix2.p1.2.m2.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p1.2.m2.1b"><ci id="S4.I1.ix2.p1.2.m2.1.1.cmml" xref="S4.I1.ix2.p1.2.m2.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p1.2.m2.1c">\mathbf{I}</annotation></semantics></math> are the independently processed text and image feature vectors, Late Fusion can be represented as:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.3" class="ltx_Math" alttext="\mathbf{F}_{\text{late}}=g(\mathbf{W}_{T}^{\prime}h(\mathbf{T})+\mathbf{W}_{I}^{\prime}k(\mathbf{I})+\mathbf{b}^{\prime})" display="block"><semantics id="S4.E2.m1.3a"><mrow id="S4.E2.m1.3.3" xref="S4.E2.m1.3.3.cmml"><msub id="S4.E2.m1.3.3.3" xref="S4.E2.m1.3.3.3.cmml"><mi id="S4.E2.m1.3.3.3.2" xref="S4.E2.m1.3.3.3.2.cmml">𝐅</mi><mtext id="S4.E2.m1.3.3.3.3" xref="S4.E2.m1.3.3.3.3a.cmml">late</mtext></msub><mo id="S4.E2.m1.3.3.2" xref="S4.E2.m1.3.3.2.cmml">=</mo><mrow id="S4.E2.m1.3.3.1" xref="S4.E2.m1.3.3.1.cmml"><mi id="S4.E2.m1.3.3.1.3" xref="S4.E2.m1.3.3.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.2" xref="S4.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S4.E2.m1.3.3.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.3.3.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.cmml"><mrow id="S4.E2.m1.3.3.1.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.1.2.cmml"><msubsup id="S4.E2.m1.3.3.1.1.1.1.2.2" xref="S4.E2.m1.3.3.1.1.1.1.2.2.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.2.2.2.2" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.2.cmml">𝐖</mi><mi id="S4.E2.m1.3.3.1.1.1.1.2.2.2.3" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.3.cmml">T</mi><mo id="S4.E2.m1.3.3.1.1.1.1.2.2.3" xref="S4.E2.m1.3.3.1.1.1.1.2.2.3.cmml">′</mo></msubsup><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1.2.1" xref="S4.E2.m1.3.3.1.1.1.1.2.1.cmml">​</mo><mi id="S4.E2.m1.3.3.1.1.1.1.2.3" xref="S4.E2.m1.3.3.1.1.1.1.2.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1.2.1a" xref="S4.E2.m1.3.3.1.1.1.1.2.1.cmml">​</mo><mrow id="S4.E2.m1.3.3.1.1.1.1.2.4.2" xref="S4.E2.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.1.2.4.2.1" xref="S4.E2.m1.3.3.1.1.1.1.2.cmml">(</mo><mi id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">𝐓</mi><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.1.2.4.2.2" xref="S4.E2.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.3.3.1.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.1.cmml">+</mo><mrow id="S4.E2.m1.3.3.1.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.1.3.cmml"><msubsup id="S4.E2.m1.3.3.1.1.1.1.3.2" xref="S4.E2.m1.3.3.1.1.1.1.3.2.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.3.2.2.2" xref="S4.E2.m1.3.3.1.1.1.1.3.2.2.2.cmml">𝐖</mi><mi id="S4.E2.m1.3.3.1.1.1.1.3.2.2.3" xref="S4.E2.m1.3.3.1.1.1.1.3.2.2.3.cmml">I</mi><mo id="S4.E2.m1.3.3.1.1.1.1.3.2.3" xref="S4.E2.m1.3.3.1.1.1.1.3.2.3.cmml">′</mo></msubsup><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1.3.1" xref="S4.E2.m1.3.3.1.1.1.1.3.1.cmml">​</mo><mi id="S4.E2.m1.3.3.1.1.1.1.3.3" xref="S4.E2.m1.3.3.1.1.1.1.3.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1.3.1a" xref="S4.E2.m1.3.3.1.1.1.1.3.1.cmml">​</mo><mrow id="S4.E2.m1.3.3.1.1.1.1.3.4.2" xref="S4.E2.m1.3.3.1.1.1.1.3.cmml"><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.1.3.4.2.1" xref="S4.E2.m1.3.3.1.1.1.1.3.cmml">(</mo><mi id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml">𝐈</mi><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.1.3.4.2.2" xref="S4.E2.m1.3.3.1.1.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.3.3.1.1.1.1.1a" xref="S4.E2.m1.3.3.1.1.1.1.1.cmml">+</mo><msup id="S4.E2.m1.3.3.1.1.1.1.4" xref="S4.E2.m1.3.3.1.1.1.1.4.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.4.2" xref="S4.E2.m1.3.3.1.1.1.1.4.2.cmml">𝐛</mi><mo id="S4.E2.m1.3.3.1.1.1.1.4.3" xref="S4.E2.m1.3.3.1.1.1.1.4.3.cmml">′</mo></msup></mrow><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.3b"><apply id="S4.E2.m1.3.3.cmml" xref="S4.E2.m1.3.3"><eq id="S4.E2.m1.3.3.2.cmml" xref="S4.E2.m1.3.3.2"></eq><apply id="S4.E2.m1.3.3.3.cmml" xref="S4.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.3.1.cmml" xref="S4.E2.m1.3.3.3">subscript</csymbol><ci id="S4.E2.m1.3.3.3.2.cmml" xref="S4.E2.m1.3.3.3.2">𝐅</ci><ci id="S4.E2.m1.3.3.3.3a.cmml" xref="S4.E2.m1.3.3.3.3"><mtext mathsize="70%" id="S4.E2.m1.3.3.3.3.cmml" xref="S4.E2.m1.3.3.3.3">late</mtext></ci></apply><apply id="S4.E2.m1.3.3.1.cmml" xref="S4.E2.m1.3.3.1"><times id="S4.E2.m1.3.3.1.2.cmml" xref="S4.E2.m1.3.3.1.2"></times><ci id="S4.E2.m1.3.3.1.3.cmml" xref="S4.E2.m1.3.3.1.3">𝑔</ci><apply id="S4.E2.m1.3.3.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1"><plus id="S4.E2.m1.3.3.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1"></plus><apply id="S4.E2.m1.3.3.1.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2"><times id="S4.E2.m1.3.3.1.1.1.1.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.1"></times><apply id="S4.E2.m1.3.3.1.1.1.1.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2">superscript</csymbol><apply id="S4.E2.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.2.2.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.1.2.2.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.2">𝐖</ci><ci id="S4.E2.m1.3.3.1.1.1.1.2.2.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2.2.3">𝑇</ci></apply><ci id="S4.E2.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.2.3">′</ci></apply><ci id="S4.E2.m1.3.3.1.1.1.1.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.2.3">ℎ</ci><ci id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">𝐓</ci></apply><apply id="S4.E2.m1.3.3.1.1.1.1.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3"><times id="S4.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.1"></times><apply id="S4.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.2">superscript</csymbol><apply id="S4.E2.m1.3.3.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.3.2.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.1.3.2.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.2.2.2">𝐖</ci><ci id="S4.E2.m1.3.3.1.1.1.1.3.2.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.2.2.3">𝐼</ci></apply><ci id="S4.E2.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.2.3">′</ci></apply><ci id="S4.E2.m1.3.3.1.1.1.1.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3.3">𝑘</ci><ci id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2">𝐈</ci></apply><apply id="S4.E2.m1.3.3.1.1.1.1.4.cmml" xref="S4.E2.m1.3.3.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.4.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.4">superscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.1.4.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.4.2">𝐛</ci><ci id="S4.E2.m1.3.3.1.1.1.1.4.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.4.3">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.3c">\mathbf{F}_{\text{late}}=g(\mathbf{W}_{T}^{\prime}h(\mathbf{T})+\mathbf{W}_{I}^{\prime}k(\mathbf{I})+\mathbf{b}^{\prime})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.I1.ix2.p2" class="ltx_para ltx_noindent">
<p id="S4.I1.ix2.p2.6" class="ltx_p">where <math id="S4.I1.ix2.p2.1.m1.1" class="ltx_Math" alttext="h(\mathbf{T})" display="inline"><semantics id="S4.I1.ix2.p2.1.m1.1a"><mrow id="S4.I1.ix2.p2.1.m1.1.2" xref="S4.I1.ix2.p2.1.m1.1.2.cmml"><mi id="S4.I1.ix2.p2.1.m1.1.2.2" xref="S4.I1.ix2.p2.1.m1.1.2.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S4.I1.ix2.p2.1.m1.1.2.1" xref="S4.I1.ix2.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S4.I1.ix2.p2.1.m1.1.2.3.2" xref="S4.I1.ix2.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S4.I1.ix2.p2.1.m1.1.2.3.2.1" xref="S4.I1.ix2.p2.1.m1.1.2.cmml">(</mo><mi id="S4.I1.ix2.p2.1.m1.1.1" xref="S4.I1.ix2.p2.1.m1.1.1.cmml">𝐓</mi><mo stretchy="false" id="S4.I1.ix2.p2.1.m1.1.2.3.2.2" xref="S4.I1.ix2.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p2.1.m1.1b"><apply id="S4.I1.ix2.p2.1.m1.1.2.cmml" xref="S4.I1.ix2.p2.1.m1.1.2"><times id="S4.I1.ix2.p2.1.m1.1.2.1.cmml" xref="S4.I1.ix2.p2.1.m1.1.2.1"></times><ci id="S4.I1.ix2.p2.1.m1.1.2.2.cmml" xref="S4.I1.ix2.p2.1.m1.1.2.2">ℎ</ci><ci id="S4.I1.ix2.p2.1.m1.1.1.cmml" xref="S4.I1.ix2.p2.1.m1.1.1">𝐓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p2.1.m1.1c">h(\mathbf{T})</annotation></semantics></math> and <math id="S4.I1.ix2.p2.2.m2.1" class="ltx_Math" alttext="k(\mathbf{I})" display="inline"><semantics id="S4.I1.ix2.p2.2.m2.1a"><mrow id="S4.I1.ix2.p2.2.m2.1.2" xref="S4.I1.ix2.p2.2.m2.1.2.cmml"><mi id="S4.I1.ix2.p2.2.m2.1.2.2" xref="S4.I1.ix2.p2.2.m2.1.2.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S4.I1.ix2.p2.2.m2.1.2.1" xref="S4.I1.ix2.p2.2.m2.1.2.1.cmml">​</mo><mrow id="S4.I1.ix2.p2.2.m2.1.2.3.2" xref="S4.I1.ix2.p2.2.m2.1.2.cmml"><mo stretchy="false" id="S4.I1.ix2.p2.2.m2.1.2.3.2.1" xref="S4.I1.ix2.p2.2.m2.1.2.cmml">(</mo><mi id="S4.I1.ix2.p2.2.m2.1.1" xref="S4.I1.ix2.p2.2.m2.1.1.cmml">𝐈</mi><mo stretchy="false" id="S4.I1.ix2.p2.2.m2.1.2.3.2.2" xref="S4.I1.ix2.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p2.2.m2.1b"><apply id="S4.I1.ix2.p2.2.m2.1.2.cmml" xref="S4.I1.ix2.p2.2.m2.1.2"><times id="S4.I1.ix2.p2.2.m2.1.2.1.cmml" xref="S4.I1.ix2.p2.2.m2.1.2.1"></times><ci id="S4.I1.ix2.p2.2.m2.1.2.2.cmml" xref="S4.I1.ix2.p2.2.m2.1.2.2">𝑘</ci><ci id="S4.I1.ix2.p2.2.m2.1.1.cmml" xref="S4.I1.ix2.p2.2.m2.1.1">𝐈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p2.2.m2.1c">k(\mathbf{I})</annotation></semantics></math> are the functions processing the text and image features respectively, <math id="S4.I1.ix2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{W}_{T}^{\prime}" display="inline"><semantics id="S4.I1.ix2.p2.3.m3.1a"><msubsup id="S4.I1.ix2.p2.3.m3.1.1" xref="S4.I1.ix2.p2.3.m3.1.1.cmml"><mi id="S4.I1.ix2.p2.3.m3.1.1.2.2" xref="S4.I1.ix2.p2.3.m3.1.1.2.2.cmml">𝐖</mi><mi id="S4.I1.ix2.p2.3.m3.1.1.2.3" xref="S4.I1.ix2.p2.3.m3.1.1.2.3.cmml">T</mi><mo id="S4.I1.ix2.p2.3.m3.1.1.3" xref="S4.I1.ix2.p2.3.m3.1.1.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p2.3.m3.1b"><apply id="S4.I1.ix2.p2.3.m3.1.1.cmml" xref="S4.I1.ix2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.I1.ix2.p2.3.m3.1.1.1.cmml" xref="S4.I1.ix2.p2.3.m3.1.1">superscript</csymbol><apply id="S4.I1.ix2.p2.3.m3.1.1.2.cmml" xref="S4.I1.ix2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.I1.ix2.p2.3.m3.1.1.2.1.cmml" xref="S4.I1.ix2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.I1.ix2.p2.3.m3.1.1.2.2.cmml" xref="S4.I1.ix2.p2.3.m3.1.1.2.2">𝐖</ci><ci id="S4.I1.ix2.p2.3.m3.1.1.2.3.cmml" xref="S4.I1.ix2.p2.3.m3.1.1.2.3">𝑇</ci></apply><ci id="S4.I1.ix2.p2.3.m3.1.1.3.cmml" xref="S4.I1.ix2.p2.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p2.3.m3.1c">\mathbf{W}_{T}^{\prime}</annotation></semantics></math> and <math id="S4.I1.ix2.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{W}_{I}^{\prime}" display="inline"><semantics id="S4.I1.ix2.p2.4.m4.1a"><msubsup id="S4.I1.ix2.p2.4.m4.1.1" xref="S4.I1.ix2.p2.4.m4.1.1.cmml"><mi id="S4.I1.ix2.p2.4.m4.1.1.2.2" xref="S4.I1.ix2.p2.4.m4.1.1.2.2.cmml">𝐖</mi><mi id="S4.I1.ix2.p2.4.m4.1.1.2.3" xref="S4.I1.ix2.p2.4.m4.1.1.2.3.cmml">I</mi><mo id="S4.I1.ix2.p2.4.m4.1.1.3" xref="S4.I1.ix2.p2.4.m4.1.1.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p2.4.m4.1b"><apply id="S4.I1.ix2.p2.4.m4.1.1.cmml" xref="S4.I1.ix2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.I1.ix2.p2.4.m4.1.1.1.cmml" xref="S4.I1.ix2.p2.4.m4.1.1">superscript</csymbol><apply id="S4.I1.ix2.p2.4.m4.1.1.2.cmml" xref="S4.I1.ix2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.I1.ix2.p2.4.m4.1.1.2.1.cmml" xref="S4.I1.ix2.p2.4.m4.1.1">subscript</csymbol><ci id="S4.I1.ix2.p2.4.m4.1.1.2.2.cmml" xref="S4.I1.ix2.p2.4.m4.1.1.2.2">𝐖</ci><ci id="S4.I1.ix2.p2.4.m4.1.1.2.3.cmml" xref="S4.I1.ix2.p2.4.m4.1.1.2.3">𝐼</ci></apply><ci id="S4.I1.ix2.p2.4.m4.1.1.3.cmml" xref="S4.I1.ix2.p2.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p2.4.m4.1c">\mathbf{W}_{I}^{\prime}</annotation></semantics></math> are weight matrices for the processed features, <math id="S4.I1.ix2.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{b}^{\prime}" display="inline"><semantics id="S4.I1.ix2.p2.5.m5.1a"><msup id="S4.I1.ix2.p2.5.m5.1.1" xref="S4.I1.ix2.p2.5.m5.1.1.cmml"><mi id="S4.I1.ix2.p2.5.m5.1.1.2" xref="S4.I1.ix2.p2.5.m5.1.1.2.cmml">𝐛</mi><mo id="S4.I1.ix2.p2.5.m5.1.1.3" xref="S4.I1.ix2.p2.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p2.5.m5.1b"><apply id="S4.I1.ix2.p2.5.m5.1.1.cmml" xref="S4.I1.ix2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.I1.ix2.p2.5.m5.1.1.1.cmml" xref="S4.I1.ix2.p2.5.m5.1.1">superscript</csymbol><ci id="S4.I1.ix2.p2.5.m5.1.1.2.cmml" xref="S4.I1.ix2.p2.5.m5.1.1.2">𝐛</ci><ci id="S4.I1.ix2.p2.5.m5.1.1.3.cmml" xref="S4.I1.ix2.p2.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p2.5.m5.1c">\mathbf{b}^{\prime}</annotation></semantics></math> is the bias term, and <math id="S4.I1.ix2.p2.6.m6.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S4.I1.ix2.p2.6.m6.1a"><mi id="S4.I1.ix2.p2.6.m6.1.1" xref="S4.I1.ix2.p2.6.m6.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.p2.6.m6.1b"><ci id="S4.I1.ix2.p2.6.m6.1.1.cmml" xref="S4.I1.ix2.p2.6.m6.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.p2.6.m6.1c">g</annotation></semantics></math> is the activation function.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Step 4) Hyperparameter Tuning and Model Evaluation:</span>
We adapt hyperparameters to optimize model performance and get the best results. Our approach uses the AdamW optimizer, a batch size of 16, and epochs of 30 to 60. The learning rate is adjusted between 0.001 and 0.01. Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Quantitative Analysis ‣ 5 Result Analysis ‣ Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists measures we use to evaluate our Multimodal Bangla Author Intent Classification technique after hyperparameter tuning. The analysis helps modify and optimize the model, boosting accuracy.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Result Analysis</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Quantitative Analysis</h3>

<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Detailed Performance Metrics of Various Pre-trained Language Models</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T2.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S5.T2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S5.T2.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.3.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S5.T2.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.4.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S5.T2.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.5.1" class="ltx_text ltx_font_bold">F1-Score</span></td>
</tr>
<tr id="S5.T2.1.2" class="ltx_tr">
<td id="S5.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:2.0pt;">mBERT</td>
<td id="S5.T2.1.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-bottom:2.0pt;">0.6300</td>
<td id="S5.T2.1.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-bottom:2.0pt;">0.6251</td>
<td id="S5.T2.1.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-bottom:2.0pt;">0.6324</td>
<td id="S5.T2.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-bottom:2.0pt;">0.6272</td>
</tr>
<tr id="S5.T2.1.3" class="ltx_tr">
<td id="S5.T2.1.3.1" class="ltx_td ltx_align_center" style="padding-bottom:2.0pt;"><span id="S5.T2.1.3.1.1" class="ltx_text ltx_font_bold">XLM-RoBERTa</span></td>
<td id="S5.T2.1.3.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-bottom:2.0pt;"><span id="S5.T2.1.3.2.1" class="ltx_text ltx_font_bold">0.6453</span></td>
<td id="S5.T2.1.3.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-bottom:2.0pt;"><span id="S5.T2.1.3.3.1" class="ltx_text ltx_font_bold">0.6418</span></td>
<td id="S5.T2.1.3.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-bottom:2.0pt;"><span id="S5.T2.1.3.4.1" class="ltx_text ltx_font_bold">0.6411</span></td>
<td id="S5.T2.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-bottom:2.0pt;"><span id="S5.T2.1.3.5.1" class="ltx_text ltx_font_bold">0.6415</span></td>
</tr>
<tr id="S5.T2.1.4" class="ltx_tr">
<td id="S5.T2.1.4.1" class="ltx_td ltx_align_center ltx_border_bb">DistilBERT</td>
<td id="S5.T2.1.4.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">0.6016</td>
<td id="S5.T2.1.4.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">0.5983</td>
<td id="S5.T2.1.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">0.5898</td>
<td id="S5.T2.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb">0.5930</td>
</tr>
</table>
</figure>
<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">In the unimodal approach Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Quantitative Analysis ‣ 5 Result Analysis ‣ Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, XLM-RoBERTa has the highest accuracy at 64.53%, while DistilBERT has the lowest at 60.16%.Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Quantitative Analysis ‣ 5 Result Analysis ‣ Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> compares early and late fusion strategies in multimodal approaches. For the early fusion, the combination of ResNet50 and XLM-RoBERTa achieves the highest accuracy at 76.19%, whereas ResNet101 and mBERT have the lowest accuracy at 63.58%. In the late fusion approach, MobileNet V2 paired with mBERT reaches the highest accuracy of 75.55%, with ResNet101 and XLM-RoBERTa gave the lowest at 62.67%.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparative Performance Metrics of Early and Late Fusion Approaches Using Various Deep Learning Model Combinations</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T3.1.1.1.1" class="ltx_text ltx_font_bold">Approach</span></td>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T3.1.1.2.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">   <span id="S5.T3.1.1.3.1" class="ltx_text ltx_font_bold">Accuracy</span>
</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">   <span id="S5.T3.1.1.4.1" class="ltx_text ltx_font_bold">Precision</span>
</td>
<td id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">   <span id="S5.T3.1.1.5.1" class="ltx_text ltx_font_bold">Recall</span>
</td>
<td id="S5.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">   <span id="S5.T3.1.1.6.1" class="ltx_text ltx_font_bold">F1-Score</span>
</td>
</tr>
<tr id="S5.T3.1.2" class="ltx_tr">
<td id="S5.T3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="24"><span id="S5.T3.1.2.1.1" class="ltx_text">Early Fusion</span></td>
<td id="S5.T3.1.2.2" class="ltx_td ltx_align_left ltx_border_t">ResNet50 + mBERT</td>
<td id="S5.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">   0.7540</td>
<td id="S5.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">   0.7500</td>
<td id="S5.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_t">   0.7497</td>
<td id="S5.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_t">   0.7488</td>
</tr>
<tr id="S5.T3.1.3" class="ltx_tr">
<td id="S5.T3.1.3.1" class="ltx_td ltx_align_left">ResNet101 + mBERT</td>
<td id="S5.T3.1.3.2" class="ltx_td ltx_align_center">   0.6358</td>
<td id="S5.T3.1.3.3" class="ltx_td ltx_align_center">   0.6420</td>
<td id="S5.T3.1.3.4" class="ltx_td ltx_align_center">   0.6597</td>
<td id="S5.T3.1.3.5" class="ltx_td ltx_align_center">   0.6376</td>
</tr>
<tr id="S5.T3.1.4" class="ltx_tr">
<td id="S5.T3.1.4.1" class="ltx_td ltx_align_left">ResNet152 + mBERT</td>
<td id="S5.T3.1.4.2" class="ltx_td ltx_align_center">   0.7002</td>
<td id="S5.T3.1.4.3" class="ltx_td ltx_align_center">   0.7040</td>
<td id="S5.T3.1.4.4" class="ltx_td ltx_align_center">   0.7250</td>
<td id="S5.T3.1.4.5" class="ltx_td ltx_align_center">   0.7120</td>
</tr>
<tr id="S5.T3.1.5" class="ltx_tr">
<td id="S5.T3.1.5.1" class="ltx_td ltx_align_left">DenseNet121 + mBERT</td>
<td id="S5.T3.1.5.2" class="ltx_td ltx_align_center">   0.7560</td>
<td id="S5.T3.1.5.3" class="ltx_td ltx_align_center">   0.7640</td>
<td id="S5.T3.1.5.4" class="ltx_td ltx_align_center">   0.7580</td>
<td id="S5.T3.1.5.5" class="ltx_td ltx_align_center">   0.7603</td>
</tr>
<tr id="S5.T3.1.6" class="ltx_tr">
<td id="S5.T3.1.6.1" class="ltx_td ltx_align_left">DenseNet169 + mBERT</td>
<td id="S5.T3.1.6.2" class="ltx_td ltx_align_center">   0.7380</td>
<td id="S5.T3.1.6.3" class="ltx_td ltx_align_center">   0.7455</td>
<td id="S5.T3.1.6.4" class="ltx_td ltx_align_center">   0.7440</td>
<td id="S5.T3.1.6.5" class="ltx_td ltx_align_center">   0.7333</td>
</tr>
<tr id="S5.T3.1.7" class="ltx_tr">
<td id="S5.T3.1.7.1" class="ltx_td ltx_align_left">DenseNet201 + mBERT</td>
<td id="S5.T3.1.7.2" class="ltx_td ltx_align_center">   0.7476</td>
<td id="S5.T3.1.7.3" class="ltx_td ltx_align_center">   0.7523</td>
<td id="S5.T3.1.7.4" class="ltx_td ltx_align_center">   0.8220</td>
<td id="S5.T3.1.7.5" class="ltx_td ltx_align_center">   0.7264</td>
</tr>
<tr id="S5.T3.1.8" class="ltx_tr">
<td id="S5.T3.1.8.1" class="ltx_td ltx_align_left">MobileNet V2 + mBERT</td>
<td id="S5.T3.1.8.2" class="ltx_td ltx_align_center">   0.7170</td>
<td id="S5.T3.1.8.3" class="ltx_td ltx_align_center">   0.7185</td>
<td id="S5.T3.1.8.4" class="ltx_td ltx_align_center">   0.7770</td>
<td id="S5.T3.1.8.5" class="ltx_td ltx_align_center">   0.7466</td>
</tr>
<tr id="S5.T3.1.9" class="ltx_tr">
<td id="S5.T3.1.9.1" class="ltx_td ltx_align_left">MobileNet V3 + mBERT</td>
<td id="S5.T3.1.9.2" class="ltx_td ltx_align_center">   0.7476</td>
<td id="S5.T3.1.9.3" class="ltx_td ltx_align_center">   0.7439</td>
<td id="S5.T3.1.9.4" class="ltx_td ltx_align_center">   0.7434</td>
<td id="S5.T3.1.9.5" class="ltx_td ltx_align_center">   0.7428</td>
</tr>
<tr id="S5.T3.1.10" class="ltx_tr">
<td id="S5.T3.1.10.1" class="ltx_td ltx_align_left"><span id="S5.T3.1.10.1.1" class="ltx_text ltx_font_bold">ResNet50 + XLM-RoBERTa</span></td>
<td id="S5.T3.1.10.2" class="ltx_td ltx_align_center">   <span id="S5.T3.1.10.2.1" class="ltx_text ltx_font_bold">0.7619</span>
</td>
<td id="S5.T3.1.10.3" class="ltx_td ltx_align_center">   <span id="S5.T3.1.10.3.1" class="ltx_text ltx_font_bold">0.7537</span>
</td>
<td id="S5.T3.1.10.4" class="ltx_td ltx_align_center">   <span id="S5.T3.1.10.4.1" class="ltx_text ltx_font_bold">0.7636</span>
</td>
<td id="S5.T3.1.10.5" class="ltx_td ltx_align_center">   <span id="S5.T3.1.10.5.1" class="ltx_text ltx_font_bold">0.7573</span>
</td>
</tr>
<tr id="S5.T3.1.11" class="ltx_tr">
<td id="S5.T3.1.11.1" class="ltx_td ltx_align_left">ResNet101 + XLM-RoBERTa</td>
<td id="S5.T3.1.11.2" class="ltx_td ltx_align_center">   0.7340</td>
<td id="S5.T3.1.11.3" class="ltx_td ltx_align_center">   0.71955</td>
<td id="S5.T3.1.11.4" class="ltx_td ltx_align_center">   0.7359</td>
<td id="S5.T3.1.11.5" class="ltx_td ltx_align_center">   0.7355</td>
</tr>
<tr id="S5.T3.1.12" class="ltx_tr">
<td id="S5.T3.1.12.1" class="ltx_td ltx_align_left">ResNet152 + XLM-RoBERTa</td>
<td id="S5.T3.1.12.2" class="ltx_td ltx_align_center">   0.6624</td>
<td id="S5.T3.1.12.3" class="ltx_td ltx_align_center">   0.6556</td>
<td id="S5.T3.1.12.4" class="ltx_td ltx_align_center">   0.6562</td>
<td id="S5.T3.1.12.5" class="ltx_td ltx_align_center">   0.6533</td>
</tr>
<tr id="S5.T3.1.13" class="ltx_tr">
<td id="S5.T3.1.13.1" class="ltx_td ltx_align_left">DenseNet121 + XLM-RoBERTa</td>
<td id="S5.T3.1.13.2" class="ltx_td ltx_align_center">   0.6773</td>
<td id="S5.T3.1.13.3" class="ltx_td ltx_align_center">   0.6730</td>
<td id="S5.T3.1.13.4" class="ltx_td ltx_align_center">   0.6677</td>
<td id="S5.T3.1.13.5" class="ltx_td ltx_align_center">   0.6694</td>
</tr>
<tr id="S5.T3.1.14" class="ltx_tr">
<td id="S5.T3.1.14.1" class="ltx_td ltx_align_left">DenseNet169 + XLM-RoBERTa</td>
<td id="S5.T3.1.14.2" class="ltx_td ltx_align_center">   0.6506</td>
<td id="S5.T3.1.14.3" class="ltx_td ltx_align_center">   0.6490</td>
<td id="S5.T3.1.14.4" class="ltx_td ltx_align_center">   0.6480</td>
<td id="S5.T3.1.14.5" class="ltx_td ltx_align_center">   0.6467</td>
</tr>
<tr id="S5.T3.1.15" class="ltx_tr">
<td id="S5.T3.1.15.1" class="ltx_td ltx_align_left">DenseNet201 + XLM-RoBERTa</td>
<td id="S5.T3.1.15.2" class="ltx_td ltx_align_center">   0.6770</td>
<td id="S5.T3.1.15.3" class="ltx_td ltx_align_center">   0.6185</td>
<td id="S5.T3.1.15.4" class="ltx_td ltx_align_center">   0.6770</td>
<td id="S5.T3.1.15.5" class="ltx_td ltx_align_center">   0.6741</td>
</tr>
<tr id="S5.T3.1.16" class="ltx_tr">
<td id="S5.T3.1.16.1" class="ltx_td ltx_align_left">MobileNet V2 + XLM-RoBERTa</td>
<td id="S5.T3.1.16.2" class="ltx_td ltx_align_center">   0.7604</td>
<td id="S5.T3.1.16.3" class="ltx_td ltx_align_center">   0.7642</td>
<td id="S5.T3.1.16.4" class="ltx_td ltx_align_center">   0.7633</td>
<td id="S5.T3.1.16.5" class="ltx_td ltx_align_center">   0.7579</td>
</tr>
<tr id="S5.T3.1.17" class="ltx_tr">
<td id="S5.T3.1.17.1" class="ltx_td ltx_align_left">MobileNet V3 + XLM-RoBERTa</td>
<td id="S5.T3.1.17.2" class="ltx_td ltx_align_center">   0.6901</td>
<td id="S5.T3.1.17.3" class="ltx_td ltx_align_center">   0.6843</td>
<td id="S5.T3.1.17.4" class="ltx_td ltx_align_center">   0.7236</td>
<td id="S5.T3.1.17.5" class="ltx_td ltx_align_center">   0.6948</td>
</tr>
<tr id="S5.T3.1.18" class="ltx_tr">
<td id="S5.T3.1.18.1" class="ltx_td ltx_align_left">ResNet50 + DistilBERT</td>
<td id="S5.T3.1.18.2" class="ltx_td ltx_align_center">   0.7016</td>
<td id="S5.T3.1.18.3" class="ltx_td ltx_align_center">   0.6986</td>
<td id="S5.T3.1.18.4" class="ltx_td ltx_align_center">   0.7025</td>
<td id="S5.T3.1.18.5" class="ltx_td ltx_align_center">   0.6933</td>
</tr>
<tr id="S5.T3.1.19" class="ltx_tr">
<td id="S5.T3.1.19.1" class="ltx_td ltx_align_left">ResNet101 + DistilBERT</td>
<td id="S5.T3.1.19.2" class="ltx_td ltx_align_center">   0.6825</td>
<td id="S5.T3.1.19.3" class="ltx_td ltx_align_center">   0.6840</td>
<td id="S5.T3.1.19.4" class="ltx_td ltx_align_center">   0.6835</td>
<td id="S5.T3.1.19.5" class="ltx_td ltx_align_center">   0.6765</td>
</tr>
<tr id="S5.T3.1.20" class="ltx_tr">
<td id="S5.T3.1.20.1" class="ltx_td ltx_align_left">ResNet152 + DistilBERT</td>
<td id="S5.T3.1.20.2" class="ltx_td ltx_align_center">   0.6783</td>
<td id="S5.T3.1.20.3" class="ltx_td ltx_align_center">   0.6717</td>
<td id="S5.T3.1.20.4" class="ltx_td ltx_align_center">   0.6710</td>
<td id="S5.T3.1.20.5" class="ltx_td ltx_align_center">   0.6679</td>
</tr>
<tr id="S5.T3.1.21" class="ltx_tr">
<td id="S5.T3.1.21.1" class="ltx_td ltx_align_left">DenseNet121 + DistilBERT</td>
<td id="S5.T3.1.21.2" class="ltx_td ltx_align_center">   0.7170</td>
<td id="S5.T3.1.21.3" class="ltx_td ltx_align_center">   0.7185</td>
<td id="S5.T3.1.21.4" class="ltx_td ltx_align_center">   0.7770</td>
<td id="S5.T3.1.21.5" class="ltx_td ltx_align_center">   0.7466</td>
</tr>
<tr id="S5.T3.1.22" class="ltx_tr">
<td id="S5.T3.1.22.1" class="ltx_td ltx_align_left">DenseNet169 + DistilBERT</td>
<td id="S5.T3.1.22.2" class="ltx_td ltx_align_center">   0.6847</td>
<td id="S5.T3.1.22.3" class="ltx_td ltx_align_center">   0.6892</td>
<td id="S5.T3.1.22.4" class="ltx_td ltx_align_center">   0.6871</td>
<td id="S5.T3.1.22.5" class="ltx_td ltx_align_center">   0.6829</td>
</tr>
<tr id="S5.T3.1.23" class="ltx_tr">
<td id="S5.T3.1.23.1" class="ltx_td ltx_align_left">DenseNet201 + DistilBERT</td>
<td id="S5.T3.1.23.2" class="ltx_td ltx_align_center">   0.6454</td>
<td id="S5.T3.1.23.3" class="ltx_td ltx_align_center">   0.6237</td>
<td id="S5.T3.1.23.4" class="ltx_td ltx_align_center">   0.6300</td>
<td id="S5.T3.1.23.5" class="ltx_td ltx_align_center">   0.6245</td>
</tr>
<tr id="S5.T3.1.24" class="ltx_tr">
<td id="S5.T3.1.24.1" class="ltx_td ltx_align_left">MobileNet V2 + DistilBERT</td>
<td id="S5.T3.1.24.2" class="ltx_td ltx_align_center">   0.6550</td>
<td id="S5.T3.1.24.3" class="ltx_td ltx_align_center">   0.6360</td>
<td id="S5.T3.1.24.4" class="ltx_td ltx_align_center">   0.6366</td>
<td id="S5.T3.1.24.5" class="ltx_td ltx_align_center">   0.6328</td>
</tr>
<tr id="S5.T3.1.25" class="ltx_tr">
<td id="S5.T3.1.25.1" class="ltx_td ltx_align_left">MobileNet V3 + DistilBERT</td>
<td id="S5.T3.1.25.2" class="ltx_td ltx_align_center">   0.6901</td>
<td id="S5.T3.1.25.3" class="ltx_td ltx_align_center">   0.6817</td>
<td id="S5.T3.1.25.4" class="ltx_td ltx_align_center">   0.6870</td>
<td id="S5.T3.1.25.5" class="ltx_td ltx_align_center">   0.6715</td>
</tr>
<tr id="S5.T3.1.26" class="ltx_tr">
<td id="S5.T3.1.26.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="24"><span id="S5.T3.1.26.1.1" class="ltx_text">Late Fusion</span></td>
<td id="S5.T3.1.26.2" class="ltx_td ltx_align_left ltx_border_t">ResNet50 + mBERT</td>
<td id="S5.T3.1.26.3" class="ltx_td ltx_align_center ltx_border_t">   0.7188</td>
<td id="S5.T3.1.26.4" class="ltx_td ltx_align_center ltx_border_t">   0.7189</td>
<td id="S5.T3.1.26.5" class="ltx_td ltx_align_center ltx_border_t">   0.7173</td>
<td id="S5.T3.1.26.6" class="ltx_td ltx_align_center ltx_border_t">   0.7175</td>
</tr>
<tr id="S5.T3.1.27" class="ltx_tr">
<td id="S5.T3.1.27.1" class="ltx_td ltx_align_left">ResNet101 + mBERT</td>
<td id="S5.T3.1.27.2" class="ltx_td ltx_align_center">   0.6773</td>
<td id="S5.T3.1.27.3" class="ltx_td ltx_align_center">   0.6670</td>
<td id="S5.T3.1.27.4" class="ltx_td ltx_align_center">   0.6630</td>
<td id="S5.T3.1.27.5" class="ltx_td ltx_align_center">   0.6613</td>
</tr>
<tr id="S5.T3.1.28" class="ltx_tr">
<td id="S5.T3.1.28.1" class="ltx_td ltx_align_left">ResNet152 + mBERT</td>
<td id="S5.T3.1.28.2" class="ltx_td ltx_align_center">   0.6581</td>
<td id="S5.T3.1.28.3" class="ltx_td ltx_align_center">   0.6482</td>
<td id="S5.T3.1.28.4" class="ltx_td ltx_align_center">   0.6484</td>
<td id="S5.T3.1.28.5" class="ltx_td ltx_align_center">   0.6447</td>
</tr>
<tr id="S5.T3.1.29" class="ltx_tr">
<td id="S5.T3.1.29.1" class="ltx_td ltx_align_left">DenseNet121 + mBERT</td>
<td id="S5.T3.1.29.2" class="ltx_td ltx_align_center">   0.6677</td>
<td id="S5.T3.1.29.3" class="ltx_td ltx_align_center">   0.6637</td>
<td id="S5.T3.1.29.4" class="ltx_td ltx_align_center">   0.6600</td>
<td id="S5.T3.1.29.5" class="ltx_td ltx_align_center">   0.6620</td>
</tr>
<tr id="S5.T3.1.30" class="ltx_tr">
<td id="S5.T3.1.30.1" class="ltx_td ltx_align_left">DenseNet169 + mBERT</td>
<td id="S5.T3.1.30.2" class="ltx_td ltx_align_center">   0.6698</td>
<td id="S5.T3.1.30.3" class="ltx_td ltx_align_center">   0.6723</td>
<td id="S5.T3.1.30.4" class="ltx_td ltx_align_center">   0.6746</td>
<td id="S5.T3.1.30.5" class="ltx_td ltx_align_center">   0.6671</td>
</tr>
<tr id="S5.T3.1.31" class="ltx_tr">
<td id="S5.T3.1.31.1" class="ltx_td ltx_align_left">DenseNet201 + mBERT</td>
<td id="S5.T3.1.31.2" class="ltx_td ltx_align_center">   0.6825</td>
<td id="S5.T3.1.31.3" class="ltx_td ltx_align_center">   0.6890</td>
<td id="S5.T3.1.31.4" class="ltx_td ltx_align_center">   0.6889</td>
<td id="S5.T3.1.31.5" class="ltx_td ltx_align_center">   0.6795</td>
</tr>
<tr id="S5.T3.1.32" class="ltx_tr">
<td id="S5.T3.1.32.1" class="ltx_td ltx_align_left"><span id="S5.T3.1.32.1.1" class="ltx_text ltx_font_bold">MobileNet V2 + mBERT</span></td>
<td id="S5.T3.1.32.2" class="ltx_td ltx_align_center">   <span id="S5.T3.1.32.2.1" class="ltx_text ltx_font_bold">0.7555</span>
</td>
<td id="S5.T3.1.32.3" class="ltx_td ltx_align_center">   <span id="S5.T3.1.32.3.1" class="ltx_text ltx_font_bold">0.7520</span>
</td>
<td id="S5.T3.1.32.4" class="ltx_td ltx_align_center">   <span id="S5.T3.1.32.4.1" class="ltx_text ltx_font_bold">0.7623</span>
</td>
<td id="S5.T3.1.32.5" class="ltx_td ltx_align_center">   <span id="S5.T3.1.32.5.1" class="ltx_text ltx_font_bold">0.7551</span>
</td>
</tr>
<tr id="S5.T3.1.33" class="ltx_tr">
<td id="S5.T3.1.33.1" class="ltx_td ltx_align_left">MobileNet V3 + mBERT</td>
<td id="S5.T3.1.33.2" class="ltx_td ltx_align_center">   0.6805</td>
<td id="S5.T3.1.33.3" class="ltx_td ltx_align_center">   0.6840</td>
<td id="S5.T3.1.33.4" class="ltx_td ltx_align_center">   0.6873</td>
<td id="S5.T3.1.33.5" class="ltx_td ltx_align_center">   0.6838</td>
</tr>
<tr id="S5.T3.1.34" class="ltx_tr">
<td id="S5.T3.1.34.1" class="ltx_td ltx_align_left">ResNet50 + XLM-RoBERTa</td>
<td id="S5.T3.1.34.2" class="ltx_td ltx_align_center">   0.7157</td>
<td id="S5.T3.1.34.3" class="ltx_td ltx_align_center">   0.7167</td>
<td id="S5.T3.1.34.4" class="ltx_td ltx_align_center">   0.7200</td>
<td id="S5.T3.1.34.5" class="ltx_td ltx_align_center">   0.7166</td>
</tr>
<tr id="S5.T3.1.35" class="ltx_tr">
<td id="S5.T3.1.35.1" class="ltx_td ltx_align_left">ResNet101 + XLM-RoBERTa</td>
<td id="S5.T3.1.35.2" class="ltx_td ltx_align_center">   0.6267</td>
<td id="S5.T3.1.35.3" class="ltx_td ltx_align_center">   0.6445</td>
<td id="S5.T3.1.35.4" class="ltx_td ltx_align_center">   0.6400</td>
<td id="S5.T3.1.35.5" class="ltx_td ltx_align_center">   0.6344</td>
</tr>
<tr id="S5.T3.1.36" class="ltx_tr">
<td id="S5.T3.1.36.1" class="ltx_td ltx_align_left">ResNet152 + XLM-RoBERTa</td>
<td id="S5.T3.1.36.2" class="ltx_td ltx_align_center">   0.7029</td>
<td id="S5.T3.1.36.3" class="ltx_td ltx_align_center">   0.697</td>
<td id="S5.T3.1.36.4" class="ltx_td ltx_align_center">   0.7124</td>
<td id="S5.T3.1.36.5" class="ltx_td ltx_align_center">   0.7008</td>
</tr>
<tr id="S5.T3.1.37" class="ltx_tr">
<td id="S5.T3.1.37.1" class="ltx_td ltx_align_left">DenseNet121 + XLM-RoBERTa</td>
<td id="S5.T3.1.37.2" class="ltx_td ltx_align_center">   0.7316</td>
<td id="S5.T3.1.37.3" class="ltx_td ltx_align_center">   0.7281</td>
<td id="S5.T3.1.37.4" class="ltx_td ltx_align_center">   0.7361</td>
<td id="S5.T3.1.37.5" class="ltx_td ltx_align_center">   0.7300</td>
</tr>
<tr id="S5.T3.1.38" class="ltx_tr">
<td id="S5.T3.1.38.1" class="ltx_td ltx_align_left">DenseNet169 + XLM-RoBERTa</td>
<td id="S5.T3.1.38.2" class="ltx_td ltx_align_center">   0.7412</td>
<td id="S5.T3.1.38.3" class="ltx_td ltx_align_center">   0.7572</td>
<td id="S5.T3.1.38.4" class="ltx_td ltx_align_center">   0.7455</td>
<td id="S5.T3.1.38.5" class="ltx_td ltx_align_center">   0.7470</td>
</tr>
<tr id="S5.T3.1.39" class="ltx_tr">
<td id="S5.T3.1.39.1" class="ltx_td ltx_align_left">DenseNet201 + XLM-RoBERTa</td>
<td id="S5.T3.1.39.2" class="ltx_td ltx_align_center">   0.6869</td>
<td id="S5.T3.1.39.3" class="ltx_td ltx_align_center">   0.6846</td>
<td id="S5.T3.1.39.4" class="ltx_td ltx_align_center">   0.6837</td>
<td id="S5.T3.1.39.5" class="ltx_td ltx_align_center">   0.6836</td>
</tr>
<tr id="S5.T3.1.40" class="ltx_tr">
<td id="S5.T3.1.40.1" class="ltx_td ltx_align_left">MobileNet V2 + XLM-RoBERTa</td>
<td id="S5.T3.1.40.2" class="ltx_td ltx_align_center">   0.7158</td>
<td id="S5.T3.1.40.3" class="ltx_td ltx_align_center">   0.7267</td>
<td id="S5.T3.1.40.4" class="ltx_td ltx_align_center">   0.7043</td>
<td id="S5.T3.1.40.5" class="ltx_td ltx_align_center">   0.7153</td>
</tr>
<tr id="S5.T3.1.41" class="ltx_tr">
<td id="S5.T3.1.41.1" class="ltx_td ltx_align_left">MobileNet V3 + XLM-RoBERTa</td>
<td id="S5.T3.1.41.2" class="ltx_td ltx_align_center">   0.6645</td>
<td id="S5.T3.1.41.3" class="ltx_td ltx_align_center">   0.6660</td>
<td id="S5.T3.1.41.4" class="ltx_td ltx_align_center">   0.7695</td>
<td id="S5.T3.1.41.5" class="ltx_td ltx_align_center">   0.6634</td>
</tr>
<tr id="S5.T3.1.42" class="ltx_tr">
<td id="S5.T3.1.42.1" class="ltx_td ltx_align_left">ResNet50 + DistilBERT</td>
<td id="S5.T3.1.42.2" class="ltx_td ltx_align_center">   0.6900</td>
<td id="S5.T3.1.42.3" class="ltx_td ltx_align_center">   0.6944</td>
<td id="S5.T3.1.42.4" class="ltx_td ltx_align_center">   0.6867</td>
<td id="S5.T3.1.42.5" class="ltx_td ltx_align_center">   0.6883</td>
</tr>
<tr id="S5.T3.1.43" class="ltx_tr">
<td id="S5.T3.1.43.1" class="ltx_td ltx_align_left">ResNet101 + DistilBERT</td>
<td id="S5.T3.1.43.2" class="ltx_td ltx_align_center">   0.6571</td>
<td id="S5.T3.1.43.3" class="ltx_td ltx_align_center">   0.658</td>
<td id="S5.T3.1.43.4" class="ltx_td ltx_align_center">   0.6536</td>
<td id="S5.T3.1.43.5" class="ltx_td ltx_align_center">   0.6543</td>
</tr>
<tr id="S5.T3.1.44" class="ltx_tr">
<td id="S5.T3.1.44.1" class="ltx_td ltx_align_left">ResNet152 + DistilBERT</td>
<td id="S5.T3.1.44.2" class="ltx_td ltx_align_center">   0.7492</td>
<td id="S5.T3.1.44.3" class="ltx_td ltx_align_center">   0.7609</td>
<td id="S5.T3.1.44.4" class="ltx_td ltx_align_center">   0.7539</td>
<td id="S5.T3.1.44.5" class="ltx_td ltx_align_center">   0.7561</td>
</tr>
<tr id="S5.T3.1.45" class="ltx_tr">
<td id="S5.T3.1.45.1" class="ltx_td ltx_align_left">DenseNet121 + DistilBERT</td>
<td id="S5.T3.1.45.2" class="ltx_td ltx_align_center">   0.7413</td>
<td id="S5.T3.1.45.3" class="ltx_td ltx_align_center">   0.7390</td>
<td id="S5.T3.1.45.4" class="ltx_td ltx_align_center">   0.7439</td>
<td id="S5.T3.1.45.5" class="ltx_td ltx_align_center">   0.7407</td>
</tr>
<tr id="S5.T3.1.46" class="ltx_tr">
<td id="S5.T3.1.46.1" class="ltx_td ltx_align_left">DenseNet169 + DistilBERT</td>
<td id="S5.T3.1.46.2" class="ltx_td ltx_align_center">   0.7438</td>
<td id="S5.T3.1.46.3" class="ltx_td ltx_align_center">   0.7426</td>
<td id="S5.T3.1.46.4" class="ltx_td ltx_align_center">   0.7520</td>
<td id="S5.T3.1.46.5" class="ltx_td ltx_align_center">   0.7442</td>
</tr>
<tr id="S5.T3.1.47" class="ltx_tr">
<td id="S5.T3.1.47.1" class="ltx_td ltx_align_left">DenseNet201 + DistilBERT</td>
<td id="S5.T3.1.47.2" class="ltx_td ltx_align_center">   0.7380</td>
<td id="S5.T3.1.47.3" class="ltx_td ltx_align_center">   0.7358</td>
<td id="S5.T3.1.47.4" class="ltx_td ltx_align_center">   0.7464</td>
<td id="S5.T3.1.47.5" class="ltx_td ltx_align_center">   0.7378</td>
</tr>
<tr id="S5.T3.1.48" class="ltx_tr">
<td id="S5.T3.1.48.1" class="ltx_td ltx_align_left">MobileNet V2 + DistilBERT</td>
<td id="S5.T3.1.48.2" class="ltx_td ltx_align_center">   0.7412</td>
<td id="S5.T3.1.48.3" class="ltx_td ltx_align_center">   0.7394</td>
<td id="S5.T3.1.48.4" class="ltx_td ltx_align_center">   0.7456</td>
<td id="S5.T3.1.48.5" class="ltx_td ltx_align_center">   0.7401</td>
</tr>
<tr id="S5.T3.1.49" class="ltx_tr">
<td id="S5.T3.1.49.1" class="ltx_td ltx_align_left ltx_border_bb">MobileNet V3 + DistilBERT</td>
<td id="S5.T3.1.49.2" class="ltx_td ltx_align_center ltx_border_bb">   0.7492</td>
<td id="S5.T3.1.49.3" class="ltx_td ltx_align_center ltx_border_bb">   0.7522</td>
<td id="S5.T3.1.49.4" class="ltx_td ltx_align_center ltx_border_bb">   0.7523</td>
<td id="S5.T3.1.49.5" class="ltx_td ltx_align_center ltx_border_bb">   0.7506</td>
</tr>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Gaining Insight into Model Performance Through Error Analysis</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">In Figure <a href="#S5.F2" title="Figure 2 ‣ 5.2 Gaining Insight into Model Performance Through Error Analysis ‣ 5 Result Analysis ‣ Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the model predicted “Informative” for Image 1, focusing on factual details about shawarma. However, the post’s emotional tone suggests misclassification, likely due to its descriptive emphasis over emotional content. Moving to Image 2, labeled “Expressive,” the model interpreted a description of Debotakhum in Bandarban as conveying enthusiasm. However, the primary intent was informative, highlighting the location’s attributes rather than expressing personal feelings. Lastly, in Image 3, classified as “Exhibitionist,” the model focused on Ronaldo’s provocative comments generating debate. Yet, the misclassification occurred because the model emphasized attention-seeking rather than recognizing the controversial nature of Ronaldo’s comments.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2409.09504/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="178" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Error Analysis of MABIC framework results, illustrating both early fusion and late fusion techniques.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations and Future Works</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Our research into understanding multimodal Bangla author intent has been limited by the complexities of Bangla language, such as dialects and variations, ambiguity in visuals, which impedes goal decision-making, and continuous uncertainty despite contextual cues from captions. In our future goal, we will investigate advanced multimodal fusion methods for integrating textual and visual data in order to improve intent identification. This includes trying out different transformer architectures to obtain insights. Furthermore, our primary focus will be on creating fine-grained taxonomies adapted to certain domains or circumstances, with the goal of improving accuracy. We will employ semiotic categories such as “divergent,” “additive,” and “parallel” to acquire more insights about user intent expression. Additionally, we will enhance model transparency and reliability by employing explainable AI techniques like GradCAM, GradCAM++, and ScoreCAM.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In summary, our study represents a significant advancement in multimodal author intent classification, particularly within the Bangla language domain. Through the introduction of the “Uddessho” dataset and an examination of both unimodal and multimodal classification techniques, we achieved a 64.53% accuracy in Bangla textual intent classification. Our proposed multimodal approach significantly outperformed this by achieving a 76.19% accuracy. By employing cutting-edge language models and exploring fusion techniques for integrating textual and visual information, we have advanced author intent classification. Our findings not only offer valuable insights into understanding author intent through diverse modalities but also contribute to improving the accuracy of classification models. Moreover, our extensive evaluation of both text-based and multimodal classification approaches highlights the efficacy of incorporating visual features alongside textual data. While unimodal approaches provide substantial insights into author intent, our results demonstrate that the fusion of text and image features in multimodal approaches significantly enhances classification accuracy, offering an improvement of 11.66% over traditional methods.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> Agarwal, S., &amp; Sureka, A. (2017). Characterizing linguistic attributes for automatic classification of intent based racist/radicalized posts on tumblr micro-blogging website. arXiv preprint arXiv:1701.04931.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Meng, L., Huang, M. (2018). Dialogue Intent Classification with Long Short-Term Memory Networks. In: Huang, X., Jiang, J., Zhao, D., Feng, Y., Hong, Y. (eds) Natural Language Processing and Chinese Computing. NLPCC 2017. Lecture Notes in Computer Science(), vol 10619. Springer, Cham. https://doi.org/10.1007/978-3-319-73618-1_4

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> Purohit, Hemant &amp; Dong, Guozhu &amp; Shalin, Valerie &amp; Thirunarayan, Krishnaprasad &amp; Sheth, Amit. (2015). Intent Classification of Short-Text on Social Media. 222-228. 10.1109/SmartCity.2015.75.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> Zhang, H., Xu, H., &amp; Lin, T. E. (2021, May). Deep open intent classification with adaptive decision boundary. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 16, pp. 14374-14382).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> Hasan, M., Basher, M. J. I., &amp; Shawon, M. T. R. (2023, December). Bengali Intent Classification with Generative Adversarial BERT. In 2023 26th International Conference on Computer and Information Technology (ICCIT) (pp. 1-6). IEEE.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> Agarwal, S., &amp; Sureka, A. (2017). Characterizing linguistic attributes for automatic classification of intent based racist/radicalized posts on tumblr micro-blogging website. arXiv preprint arXiv:1701.04931.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> Sakib, Fardin Ahsan &amp; Karim, A. H. M. Rezaul &amp; Khan, Saadat &amp; Rahman, Md. Mushfiqur. (2023). Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti. 10.18653/v1/2023.banglalp-1.6.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Jurafsky, and Ajay Divakaran. 2019. Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4622–4632, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> Zhang, Lu &amp; Shen, Jialie &amp; Zhang, Jian &amp; Xu, Jingsong &amp; Li, Zhibin &amp; Yao, Yazhou &amp; Yu, Litao. (2021). Multimodal Marketing Intent Analysis for Effective Targeted Advertising. IEEE Transactions on Multimedia. PP. 1-1. 10.1109/TMM.2021.3073267.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Shaozu, Yuan &amp; Shen, Xin &amp; Zhao, Yuming &amp; Liu, Hang &amp; Yan, Zhiling &amp; Liu, Ruixue &amp; Chen, Meng. (2022). MCIC: Multimodal Conversational Intent Classification for E-commerce Customer Service. 10.1007/978-3-031-17120-8_58.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> Falotico, R., Quatto, P. Fleiss’ kappa statistic without paradoxes. Qual Quant 49, 463–470 (2015). https://doi.org/10.1007/s11135-014-0003-1

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> Sanh, V., Debut, L., Chaumond, J., &amp; Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> Conneau, Alexis, et al. "Unsupervised cross-lingual representation learning at scale." arXiv preprint arXiv:1911.02116 (2019).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> Huang, G., Liu, Z., Van Der Maaten, L., &amp; Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. "Mobilenets: Efficient convolutional neural networks for mobile vision applications." arXiv preprint arXiv:1704.04861 (2017).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> Barnum, G., Talukder, S., &amp; Yue, Y. (2020). On the benefits of early fusion in multimodal representation learning. arXiv preprint arXiv:2011.07191.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> Pandeya, Y.R., Lee, J. Deep learning-based late fusion of multimodal information for emotion classification of music video. Multimed Tools Appl 80, 2887–2905 (2021). https://doi.org/10.1007/s11042-020-08836-3

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.09503" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.09504" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.09504">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.09504" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.09505" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:38:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
