<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.00994] SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems</title><meta property="og:description" content="We live in an era of data floods, and deep neural networks play a pivotal role in this moment.
Natural data inherently exhibits several challenges such as long-tailed distribution and model fairness, where data imbalan…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.00994">

<!--Generated on Wed Feb 28 10:00:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<span id="id6.6.6" class="ltx_tabular ltx_align_middle">
<span id="id2.2.2.2" class="ltx_tr">
<span id="id2.2.2.2.3" class="ltx_td"></span>
<span id="id1.1.1.1.1" class="ltx_td ltx_align_center">Moon Ye-Bin<sup id="id1.1.1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">1</span></sup></span>
<span id="id2.2.2.2.2" class="ltx_td ltx_align_center">Nam Hyeon-Woo<sup id="id2.2.2.2.2.1" class="ltx_sup"><span id="id2.2.2.2.2.1.1" class="ltx_text ltx_font_italic">1</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span></span>
<span id="id2.2.2.2.4" class="ltx_td"></span></span>
<span id="id6.6.6.6" class="ltx_tr">
<span id="id3.3.3.3.1" class="ltx_td ltx_align_center">Wonseok Choi<sup id="id3.3.3.3.1.1" class="ltx_sup"><span id="id3.3.3.3.1.1.1" class="ltx_text ltx_font_italic">1</span></sup></span>
<span id="id4.4.4.4.2" class="ltx_td ltx_align_center">Nayeong Kim<sup id="id4.4.4.4.2.1" class="ltx_sup"><span id="id4.4.4.4.2.1.1" class="ltx_text ltx_font_italic">1</span></sup></span>
<span id="id5.5.5.5.3" class="ltx_td ltx_align_center">Suha Kwak<sup id="id5.5.5.5.3.1" class="ltx_sup"><span id="id5.5.5.5.3.1.1" class="ltx_text ltx_font_italic">1</span></sup></span>
<span id="id6.6.6.6.4" class="ltx_td ltx_align_center">Tae-Hyun Oh<sup id="id6.6.6.6.4.1" class="ltx_sup"><span id="id6.6.6.6.4.1.1" class="ltx_text ltx_font_italic">1,2</span></sup></span></span>
</span>
<br class="ltx_break"><sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>POSTECH,  <sup id="id10.10.id2" class="ltx_sup"><span id="id10.10.id2.1" class="ltx_text ltx_font_italic">2</span></sup>Yonsei University
<br class="ltx_break"><span id="id11.11.id3" class="ltx_text ltx_font_typewriter" style="font-size:80%;">{ybmoon, hyeonw.nam, wonseok.c, nayeong.kim, suha.kwak, taehyun}@postech.ac.kr</span>
</span><span class="ltx_author_notes">Authors contributed equally to this work.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">We live in an era of data floods, and deep neural networks play a pivotal role in this moment.
Natural data inherently exhibits several challenges such as long-tailed distribution and model fairness, where data imbalance is at the center of fundamental issues.
This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social problems.
To address these problems, we leverage the recent generative models advanced in generating high-quality images.
In this work, we propose SYNAuG, which utilizes synthetic data to uniformize the given imbalance distribution followed by a simple post-calibration step considering the domain gap between real and synthetic data.
This straightforward approach yields impressive performance on datasets for distinctive data imbalance problems such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbirds, surpassing the performance of existing task-specific methods.
While we do not claim that our approach serves as a complete solution to the problem of data imbalance, we argue that supplementing the existing data with synthetic data proves to be an effective and crucial step in addressing data imbalance concerns.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep neural networks (DNNs) have achieved strong performance on visual tasks.
The outstanding performance has been demonstrated by training a model with abundant and diverse labeled data
 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
Despite the importance of data, machine learning researchers have focused mainly on the model and algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>.
We should care about the data for training DNNs because unexpected influences can occur by data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We commonly encounter data imbalance problems categorized into <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">class</em> or <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">group</em> imbalance problems.
Class imbalance means different data amounts in the classes.
Suppose that we collect animal images on the internet.
Images of rare animals may be found on search engines less than cats or dogs because of human bias for uploading their photograph <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For example, the number of search results of Tarsier is about 102 times less than Maltese in Google search engine.</span></span></span>.
Group imbalance, on the other hand, stands for different data amounts in the groups.
We may collect data depending on our environments, including preferences, country, and cultural backgrounds.
Suppose we collect a picture of human hands; then, the skin tones can be biased.
If we do not care about these biases, the collected dataset becomes imbalanced in terms of classes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, groups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, or both.
With such a dataset and the standard supervised learning algorithms based on empirical risk minimization (ERM) principle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, the classifier will be trained to
be biased to majority classes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
Since these problems yield not only substantial performance degradation but also social or ethical issues with biases, researchers have independently developed various algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> to overcome these respective problems.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2308.00994/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="263" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Overview of SYNAuG process.<span id="S1.F1.4.2.1" class="ltx_text ltx_font_medium">
Given the imbalanced real-world data with the class labels, we first uniformize the imbalanced real data distribution by generating the synthetic samples that are conditioned on the class label.
Second, we train a model with the uniformized training data.
Finally, we fine-tune the last layer with the uniformly subsampled real-world data.</span></span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we first uniformize the number of samples in each class using the recent text-to-image generative models before applying off-the-shelf task-specific algorithms.
The prior studies
work the limited, fixed, and bounded original dataset without adding more additional data and mainly focus on algorithmic approaches, such as reweighting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, resampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, or augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
In contrast to the prior arts, we go beyond the fixed original dataset by exploiting
the generative diffusion models to synthesize data, which have recently shown potential as synthetic training data generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
This allows us to tackle the fundamental bottleneck of data imbalance, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>., data, rather than indirect ways of tackling learning algorithms or architectures.
It is a more natural way than restricting training data to the fixed dataset as in the prior arts.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we propose SYNAuG, exploiting the generative diffusion model to augment and make the original data distribution to be uniform distribution, <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">i.e</em>., uniformization.
After training on the uniformized data composed of the original and synthetic data, we found that it is effective to use simple fine-tuning of the last layer with uniformly sub-sampled original data.
This outperforms the other strong baselines, including the baseline using the additional external web data as well as the competing methods on the long-tailed recognition benchmark, CIFAR100-LT, and the fairness benchmark, UTKFace.
In addition, we demonstrate the effectiveness of our method for improving the robustness of the classifier to spurious correlation.
We summarize our contributions as follows:
</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Proposing SYNAuG that uniformizes the given data distribution with synthetic data, beyond the given datasets;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Demonstrating the effectiveness of SYNAuG on three distinctive data imbalance tasks: long-tailed recognition, model fairness, and robustness to spurious correlation;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Reporting the observation of the importance of a few original samples when we use synthetic data together.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Data imbalance can lead to
suboptimal generalization and many challenges
in practical application scenarios, <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>., finance, healthcare, and autonomous driving.
The data imbalance problem is a common source of different imbalance sub-problems: long-tailed recognition, model fairness, and model robustness to spurious correlation.
We brief the related work on the associated sub-problems and on using synthetic data for machine learning tasks.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Long-tailed recognition.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Long-tailed distribution is inherent to the real world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.
There are two main streams in the realm of re-balancing classes, including re-sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and re-weighting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
The re-weighting methods share a similar mechanism to weighting minority classes inverse-proportionally to the number of instances.
The re-sampling methods weight the samples in minority classes by more frequently sampling with replacement so that the training model can see the uniform number of samples across classes.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">There are other approaches
by designing loss functions.
Ryou <em id="S2.SS0.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and Lin <em id="S2.SS0.SSS0.Px1.p2.1.2" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
induce adaptive re-weighting effects during training.
The others take into account margin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> or balance of softmax <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> in the loss design.
Wang <em id="S2.SS0.SSS0.Px1.p2.1.3" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> take a completely different approach; model selection given diversely pre-trained classifiers.
In addition, Ye-Bin <em id="S2.SS0.SSS0.Px1.p2.1.4" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> propose
TextManiA, visual feature augmentation for sparse samples, which shows improved performance in long-tailed distribution.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model fairness.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">In fairness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, researchers have tackled the issue of model bias, where accuracy varies based on
sensitive attributes such as race, age, and ethnicity.
Model fairness is also related to data imbalance because the number of samples of some sensitive groups is lower than that of the major groups.
Fairness has predominantly been tackled using loss weighting and batch sampling.
A loss weighting algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> proposes fairness optimization, where they minimize the worst-case loss of the group by adaptively weighting losses.
Batch sampling approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> take an adaptive sampling strategy by considering sensitive information rather than uniform sampling.
Zeng <em id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> take a post-calibration approach after training
to calibrate the classifiers.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Spurious correlation.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">The spurious correlation problem is related to the robustness of models against misleading correlations.
DNNs are susceptible to falling into shortcuts that capture the most frequently observed patterns in a class regardless of true causality;
it is called spurious correlation or shortcut problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
It is never desirable to rely on spurious features that degrade the generalizability of DNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
The spurious correlation problem is also dealt with similar approaches to the above two tasks: weighting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and post-calibration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Summary of data imbalance problems.</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">While researchers have developed algorithms for each task separately, three different tasks sourced from data imbalance have mainly been tackled in the shared perspective, <em id="S2.SS0.SSS0.Px4.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.,
up-weight loss values or sampling probabilities of minor groups using group or sensitive information.
However, they have focused only on algorithmic parts by limiting their methods to exploit the given imbalance dataset, where the inherent imbalance still remains.</p>
</div>
<div id="S2.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p2.1" class="ltx_p">In this work, we shed light on the overlooked convention to go beyond the given bounded dataset.
We exploit the synthetic data from the generative foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> to take flexibility and controllability so that we can populate the long-tailed training data distribution to become a uniform distribution, which mitigates the imbalance problem itself.
We observe that this simple correction of class distribution with synthetic data can significantly improve the worst-case accuracy and fairness of DNNs.
To our best knowledge, our work is the first work that demonstrates improved or competitive performance with generated synthetic data for both class imbalance and fairness tasks.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Using synthetic data in machine learning tasks.</h4>

<div id="S2.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px5.p1.1" class="ltx_p">To overcome the lack of data or sensitive issues of data, <em id="S2.SS0.SSS0.Px5.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>., licensing and privacy concerns, recent approaches have started to leverage synthetic data for their tasks of interest:
classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, re-identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, motion estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, computational photography <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, and representation learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Recently, deep generative models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> have shown promising results in generating realistic and high-quality samples, stemming from the goal of modeling the real data distribution.
In particular, the image generation conditioned on text provides great controllability and flexibility, which has the potential to be used for a variety of tasks, such as 3D reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and image recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
In this work, we explore the use of a pre-trained foundation diffusion model to mitigate data imbalance problems.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We first present our motivation for using synthetic data to address data imbalance problems based on experimental findings (Sec. <a href="#S3.SS1" title="3.1 Motivations ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
Building on these empirical insights, we propose to exploit the synthetic data (SYNAuG) as a means to uniformize the given training data distribution
(Sec. <a href="#S3.SS2" title="3.2 SYNAuG ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2308.00994/assets/x2.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Class-wise replacement</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.00994/assets/x3.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Instance-wise replacement</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Replacement test.<span id="S3.F2.4.2.1" class="ltx_text ltx_font_medium">
To investigate the effect on model performance when using original and synthetic data together, we replace the original data with synthetic ones in two ways: (a) class-wise and (b) the same ratio of instances across all classes.
We use CIFAR100, which has 500 samples per class and 100 classes.
</span></span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Motivations</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">During training, we consider how to curate the data, train the model, and evaluate it.
As aforementioned, prior methods addressing data imbalance problems have explored in various ways, including data re-sampling, loss function design, and model architecture.
Instead,
we emphasize the importance of data curation and the controllability of data, as data curation significantly affects the training and the subsequent evaluation despite its position as the first step.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Before incorporating synthetic data into our proposed method, we delve into the influence of training with synthetic and original data together.
We establish two settings by controlling the ratio of original and synthetic data.
We use the generated images from the Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> for synthetic data.
In the <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">first setting</span>, we take an extreme approach by replacing whole original data belonging to specific classes with synthetic data.
It means that certain classes have no real samples but only synthetic samples.
In the <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_bold">second setting</span>, we uniformly replace the original data with synthetic data, which means all classes have the same ratio of original and synthetic data.
This approach ensures that every class at least has a few original data.
The significance of original samples becomes apparent through observing the performance change.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The results of the two settings are in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The first setting shows the linear performance degradation as the number of classes with no original data increases (See Fig. <a href="#S3.F2.sf1" title="Figure 2(a) ‣ Figure 2 ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>).
However, the second setting shows the log-like performance degradation as more original data are replaced with synthetic data uniformly (See Fig. <a href="#S3.F2.sf2" title="Figure 2(b) ‣ Figure 2 ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>).
We achieve 41.11% when using 1% of real data in the second setting, which is similar to the result of 43.96% when using 50% of real data in the first setting.
The results suggest that at least a few original samples are necessary as an anchor, as
the domain gap may still exist even with high-quality synthetic data.
</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.T1.st1" class="ltx_table ltx_figure_panel">
<div id="S3.T1.st1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:331.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(169.7pt,-129.5pt) scale(4.59802705837277,4.59802705837277) ;">
<table id="S3.T1.st1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.st1.2.1.1" class="ltx_tr">
<td id="S3.T1.st1.2.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.st1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.st1.2.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S3.T1.st1.2.1.2" class="ltx_tr">
<td id="S3.T1.st1.2.1.2.1" class="ltx_td ltx_align_center ltx_border_t">Real</td>
<td id="S3.T1.st1.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">77.76</td>
</tr>
<tr id="S3.T1.st1.2.1.3" class="ltx_tr">
<td id="S3.T1.st1.2.1.3.1" class="ltx_td ltx_align_center">Syn.</td>
<td id="S3.T1.st1.2.1.3.2" class="ltx_td ltx_align_center">70.56</td>
</tr>
<tr id="S3.T1.st1.2.1.4" class="ltx_tr">
<td id="S3.T1.st1.2.1.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Total</td>
<td id="S3.T1.st1.2.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">74.16</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.st1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.T1.st1.4.2" class="ltx_text" style="font-size:90%;">Binary domain classification</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2308.00994/assets/x4.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="349" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Feature visualization</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Domain gap between real and synthetic data.<span id="S3.F3.4.2.1" class="ltx_text ltx_font_medium">
We test the domain gap empirically with (a) binary domain classification and (b) feature visualization.
For binary classification, we use 2.5k samples for each real and synthetic domain and train only one fully-connected layer with the extracted features.
For visualization, the features are extracted from the pre-trained model on CIFAR100.
C1 and C2 denote different classes.
</span></span></figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">To check the presence of a domain gap, we conduct domain classification and visualization of the features from both real and synthetic data (See Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1 Motivations ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
As shown in Fig. <a href="#S3.T1.st1" title="Table 1(a) ‣ Figure 3 ‣ 3.1 Motivations ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>, the classification performance is 74.16%.
This indicates the existence of a domain gap, considering that 50% means no domain gap.
As shown in Fig. <a href="#S3.F3.sf1" title="Figure 3(a) ‣ Figure 3 ‣ 3.1 Motivations ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>, the features of Syn C2 are more closer to Syn C1 rather than Real C2.
This observation provides empirical evidence of a domain gap existing between real and synthetic data.
</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">In summary, (1) at least, a few real samples are important when we supplement the real samples with the synthetic samples,
(2) synthetic samples are still insufficient to fully replace the original samples, although the deep generative models show impressive performance,
thus, (3) there might be additional room for improvement due to the domain gap between the original and the synthetic data.
It is desirable that the remaining original samples serve as an anchor role, and synthetic data support and populate the insufficient samples.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>SYNAuG</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Given the preliminary experiments, we propose SYNAuG, which leverages synthetic data to mitigate the imbalance and domain gap from the data perspective.
Our approach is applied to
three distinct tasks: long-tailed recognition, model fairness, and robustness to spurious correlation.
While these tasks differ in their ultimate objectives and evaluation metrics, the common underlying factor is the presence of data imbalance.
SYNAuG is an integrated approach designed to mitigate data imbalance across diverse tasks.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As illustrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we first uniformize the imbalance data by generating synthetic data, train the model on the uniformized data, and finally fine-tune the last layer with a few original data uniformly subsampled from each class.
We exploit recent powerful generative models, <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>., Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, to generate the synthetic data of corresponding classes or attributes with the controllable prompt.
Since they are trained on a large number of web data, it would be considered to cover and model the wide distribution of the real world.
Exploiting these favorable properties, we generate supporting data to alleviate the imbalance of the data distribution.
We generate the samples with diverse prompts like “a photo of {<span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">modifier</span>} {<span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">class</span>}”.
We find list of proper modifiers by ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> to make our pipeline automatic.
We train the model on uniformized data with Cross Entropy (CE) loss.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">While SYNAuG is simple and effective, there is still room to improve its performance because of the domain gap identified
in Sec. <a href="#S3.SS1" title="3.1 Motivations ‣ 3 Method ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
To bring further improvement by mitigating the gap, we propose to utilize two simple methods.
First, we propose to leverage
Mixup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> during training to augment the samples to be interpolated samples between real and synthetic samples, <em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>., domain Mixup.
Second, we propose to fine-tune the classifier on the subsampled uniform original data from the original training data after the first training stage.
The fine-tuned classifier would lead to more accurate recognition of the target data by alleviating the domain gap.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">In summary, the process of SYNAuG is as follows:
(1) uniformize the original data distribution with synthetic data from the generative model, (2) train the model with uniformized data using Mixup, and (3) fine-tune the last layer with the uniformly subsampled real data.
</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section,
we evaluate our method for three sub-tasks: long-tailed recognition task (Sec. <a href="#S4.SS1" title="4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), model fairness (Sec. <a href="#S4.SS2" title="4.2 Model Fairness ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), and model robustness to spurious correlation (Sec. <a href="#S4.SS3" title="4.3 Model Robustness to Spurious Correlation ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).
Through these results, we demonstrate the effectiveness of SYNAuG for data imbalance problems.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Long-tailed Recognition</h3>

<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:281.9pt;height:121.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-88.7pt,38.2pt) scale(0.613685017370806,0.613685017370806) ;">
<table id="S3.T1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<td id="S3.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S3.T1.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S3.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S3.T1.2.1.1.2.1" class="ltx_text ltx_font_bold">IF=100</span></td>
<td id="S3.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S3.T1.2.1.1.3.1" class="ltx_text ltx_font_bold">50</span></td>
<td id="S3.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S3.T1.2.1.1.4.1" class="ltx_text ltx_font_bold">10</span></td>
</tr>
<tr id="S3.T1.2.1.2" class="ltx_tr">
<td id="S3.T1.2.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.1.2.1.1" class="ltx_text ltx_font_bold">Many</span></td>
<td id="S3.T1.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.1.2.2.1" class="ltx_text ltx_font_bold">Medium</span></td>
<td id="S3.T1.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.1.2.3.1" class="ltx_text ltx_font_bold">Few</span></td>
<td id="S3.T1.2.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.2.1.2.4.1" class="ltx_text ltx_font_bold">All</span></td>
</tr>
<tr id="S3.T1.2.1.3" class="ltx_tr">
<td id="S3.T1.2.1.3.1" class="ltx_td ltx_align_left ltx_border_t">CE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S3.T1.2.1.3.2" class="ltx_td ltx_align_center ltx_border_t">68.31</td>
<td id="S3.T1.2.1.3.3" class="ltx_td ltx_align_center ltx_border_t">36.88</td>
<td id="S3.T1.2.1.3.4" class="ltx_td ltx_align_center ltx_border_t">4.87</td>
<td id="S3.T1.2.1.3.5" class="ltx_td ltx_align_center ltx_border_t">37.96</td>
<td id="S3.T1.2.1.3.6" class="ltx_td ltx_align_center ltx_border_t">43.54</td>
<td id="S3.T1.2.1.3.7" class="ltx_td ltx_align_center ltx_border_t">59.50</td>
</tr>
<tr id="S3.T1.2.1.4" class="ltx_tr">
<td id="S3.T1.2.1.4.1" class="ltx_td ltx_align_left ltx_border_t">SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S3.T1.2.1.4.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.2.1.4.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.2.1.4.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.2.1.4.5" class="ltx_td ltx_align_center ltx_border_t">46.0</td>
<td id="S3.T1.2.1.4.6" class="ltx_td ltx_align_center ltx_border_t">50.5</td>
<td id="S3.T1.2.1.4.7" class="ltx_td ltx_align_center ltx_border_t">62.3</td>
</tr>
<tr id="S3.T1.2.1.5" class="ltx_tr">
<td id="S3.T1.2.1.5.1" class="ltx_td ltx_align_left">PaCo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S3.T1.2.1.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.5.5" class="ltx_td ltx_align_center">52.0</td>
<td id="S3.T1.2.1.5.6" class="ltx_td ltx_align_center">56.0</td>
<td id="S3.T1.2.1.5.7" class="ltx_td ltx_align_center">64.2</td>
</tr>
<tr id="S3.T1.2.1.6" class="ltx_tr">
<td id="S3.T1.2.1.6.1" class="ltx_td ltx_align_left">RISDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S3.T1.2.1.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.6.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.6.5" class="ltx_td ltx_align_center">50.16</td>
<td id="S3.T1.2.1.6.6" class="ltx_td ltx_align_center">53.84</td>
<td id="S3.T1.2.1.6.7" class="ltx_td ltx_align_center">62.38</td>
</tr>
<tr id="S3.T1.2.1.7" class="ltx_tr">
<td id="S3.T1.2.1.7.1" class="ltx_td ltx_align_left">CE + CMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S3.T1.2.1.7.2" class="ltx_td ltx_align_center">70.4</td>
<td id="S3.T1.2.1.7.3" class="ltx_td ltx_align_center">42.5</td>
<td id="S3.T1.2.1.7.4" class="ltx_td ltx_align_center">14.4</td>
<td id="S3.T1.2.1.7.5" class="ltx_td ltx_align_center">43.9</td>
<td id="S3.T1.2.1.7.6" class="ltx_td ltx_align_center">48.3</td>
<td id="S3.T1.2.1.7.7" class="ltx_td ltx_align_center">59.5</td>
</tr>
<tr id="S3.T1.2.1.8" class="ltx_tr">
<td id="S3.T1.2.1.8.1" class="ltx_td ltx_align_left">LDAM + CMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S3.T1.2.1.8.2" class="ltx_td ltx_align_center">61.5</td>
<td id="S3.T1.2.1.8.3" class="ltx_td ltx_align_center">48.6</td>
<td id="S3.T1.2.1.8.4" class="ltx_td ltx_align_center">28.8</td>
<td id="S3.T1.2.1.8.5" class="ltx_td ltx_align_center">47.2</td>
<td id="S3.T1.2.1.8.6" class="ltx_td ltx_align_center">51.7</td>
<td id="S3.T1.2.1.8.7" class="ltx_td ltx_align_center">58.4</td>
</tr>
<tr id="S3.T1.2.1.9" class="ltx_tr">
<td id="S3.T1.2.1.9.1" class="ltx_td ltx_align_left">RIDE (3 experts) + CMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S3.T1.2.1.9.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.9.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.9.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.1.9.5" class="ltx_td ltx_align_center">50.0</td>
<td id="S3.T1.2.1.9.6" class="ltx_td ltx_align_center">53.0</td>
<td id="S3.T1.2.1.9.7" class="ltx_td ltx_align_center">60.2</td>
</tr>
<tr id="S3.T1.2.1.10" class="ltx_tr">
<td id="S3.T1.2.1.10.1" class="ltx_td ltx_align_left">Weight Balancing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S3.T1.2.1.10.2" class="ltx_td ltx_align_center">72.60</td>
<td id="S3.T1.2.1.10.3" class="ltx_td ltx_align_center">51.86</td>
<td id="S3.T1.2.1.10.4" class="ltx_td ltx_align_center">32.63</td>
<td id="S3.T1.2.1.10.5" class="ltx_td ltx_align_center">53.35</td>
<td id="S3.T1.2.1.10.6" class="ltx_td ltx_align_center">57.71</td>
<td id="S3.T1.2.1.10.7" class="ltx_td ltx_align_center">68.67</td>
</tr>
<tr id="S3.T1.2.1.11" class="ltx_tr">
<td id="S3.T1.2.1.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">SYNAuG</td>
<td id="S3.T1.2.1.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.2.1.11.2.1" class="ltx_text ltx_font_bold">74.06</span></td>
<td id="S3.T1.2.1.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.2.1.11.3.1" class="ltx_text ltx_font_bold">56.63</span></td>
<td id="S3.T1.2.1.11.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.2.1.11.4.1" class="ltx_text ltx_font_bold">42.83</span></td>
<td id="S3.T1.2.1.11.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.2.1.11.5.1" class="ltx_text ltx_font_bold">58.59</span></td>
<td id="S3.T1.2.1.11.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.2.1.11.6.1" class="ltx_text ltx_font_bold">61.36</span></td>
<td id="S3.T1.2.1.11.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.2.1.11.7.1" class="ltx_text ltx_font_bold">69.01</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.5.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Long-tailed recognition performance on CIFAR100-LT.<span id="S3.T1.6.2.1" class="ltx_text ltx_font_medium">
We compare our SYNAuG with recent works in long-tailed recognition.
We report the Top-1 accuracy (%) with different imbalance factors, <em id="S3.T1.6.2.1.1" class="ltx_emph ltx_font_italic">i.e</em>., IF={100, 50, 10}.
</span></span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:193.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(55.3pt,-24.6pt) scale(1.34235925861247,1.34235925861247) ;">
<table id="S4.T2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<td id="S4.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S4.T2.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.2.1.1.2.1" class="ltx_text"><span id="S4.T2.2.1.1.2.1.1" class="ltx_text"></span><span id="S4.T2.2.1.1.2.1.2" class="ltx_text ltx_font_bold"> <span id="S4.T2.2.1.1.2.1.2.1" class="ltx_text">
<span id="S4.T2.2.1.1.2.1.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.2.1.1.2.1.2.1.1.1" class="ltx_tr">
<span id="S4.T2.2.1.1.2.1.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Additional</span></span>
<span id="S4.T2.2.1.1.2.1.2.1.1.2" class="ltx_tr">
<span id="S4.T2.2.1.1.2.1.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Data Type</span></span>
</span></span> <span id="S4.T2.2.1.1.2.1.2.2" class="ltx_text"></span></span></span></td>
<td id="S4.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T2.2.1.1.3.1" class="ltx_text ltx_font_bold">IF</span></td>
</tr>
<tr id="S4.T2.2.1.2" class="ltx_tr">
<td id="S4.T2.2.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.1.2.1.1" class="ltx_text ltx_font_bold">100</span></td>
<td id="S4.T2.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.1.2.2.1" class="ltx_text ltx_font_bold">50</span></td>
<td id="S4.T2.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.1.2.3.1" class="ltx_text ltx_font_bold">10</span></td>
</tr>
<tr id="S4.T2.2.1.3" class="ltx_tr">
<td id="S4.T2.2.1.3.1" class="ltx_td ltx_align_left ltx_border_t">CE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S4.T2.2.1.3.2" class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td id="S4.T2.2.1.3.3" class="ltx_td ltx_align_center ltx_border_t">37.96</td>
<td id="S4.T2.2.1.3.4" class="ltx_td ltx_align_center ltx_border_t">43.54</td>
<td id="S4.T2.2.1.3.5" class="ltx_td ltx_align_center ltx_border_t">59.50</td>
</tr>
<tr id="S4.T2.2.1.4" class="ltx_tr">
<td id="S4.T2.2.1.4.1" class="ltx_td ltx_align_left ltx_border_t">Web crawled images</td>
<td id="S4.T2.2.1.4.2" class="ltx_td ltx_align_center ltx_border_t">Real</td>
<td id="S4.T2.2.1.4.3" class="ltx_td ltx_align_center ltx_border_t">54.06</td>
<td id="S4.T2.2.1.4.4" class="ltx_td ltx_align_center ltx_border_t">56.40</td>
<td id="S4.T2.2.1.4.5" class="ltx_td ltx_align_center ltx_border_t">63.86</td>
</tr>
<tr id="S4.T2.2.1.5" class="ltx_tr">
<td id="S4.T2.2.1.5.1" class="ltx_td ltx_align_left ltx_border_t">Intra-class Image Translation</td>
<td id="S4.T2.2.1.5.2" class="ltx_td ltx_align_center ltx_border_t">Syn.</td>
<td id="S4.T2.2.1.5.3" class="ltx_td ltx_align_center ltx_border_t">47.87</td>
<td id="S4.T2.2.1.5.4" class="ltx_td ltx_align_center ltx_border_t">53.33</td>
<td id="S4.T2.2.1.5.5" class="ltx_td ltx_align_center ltx_border_t">64.95</td>
</tr>
<tr id="S4.T2.2.1.6" class="ltx_tr">
<td id="S4.T2.2.1.6.1" class="ltx_td ltx_align_left">Inter-class Image Translation</td>
<td id="S4.T2.2.1.6.2" class="ltx_td ltx_align_center">Syn.</td>
<td id="S4.T2.2.1.6.3" class="ltx_td ltx_align_center">47.17</td>
<td id="S4.T2.2.1.6.4" class="ltx_td ltx_align_center">51.33</td>
<td id="S4.T2.2.1.6.5" class="ltx_td ltx_align_center">64.11</td>
</tr>
<tr id="S4.T2.2.1.7" class="ltx_tr">
<td id="S4.T2.2.1.7.1" class="ltx_td ltx_align_left">Class Distribution Fitting</td>
<td id="S4.T2.2.1.7.2" class="ltx_td ltx_align_center">Syn.</td>
<td id="S4.T2.2.1.7.3" class="ltx_td ltx_align_center">51.53</td>
<td id="S4.T2.2.1.7.4" class="ltx_td ltx_align_center">55.60</td>
<td id="S4.T2.2.1.7.5" class="ltx_td ltx_align_center">65.60</td>
</tr>
<tr id="S4.T2.2.1.8" class="ltx_tr">
<td id="S4.T2.2.1.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">SYNAuG</td>
<td id="S4.T2.2.1.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Syn.</td>
<td id="S4.T2.2.1.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.8.3.1" class="ltx_text ltx_font_bold">58.59</span></td>
<td id="S4.T2.2.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.8.4.1" class="ltx_text ltx_font_bold">61.36</span></td>
<td id="S4.T2.2.1.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.8.5.1" class="ltx_text ltx_font_bold">69.01</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison with the baselines.<span id="S4.T2.5.2.1" class="ltx_text ltx_font_medium">
We use CIFAR100-LT.
The second column denotes the data type used in uniformization.
</span></span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:168.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(54.5pt,-21.1pt) scale(1.33558285714715,1.33558285714715) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.2.1.1" class="ltx_tr">
<td id="S4.T3.2.1.1.1" class="ltx_td ltx_border_tt" rowspan="2"></td>
<td id="S4.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.2.1.1.2.1" class="ltx_text ltx_font_bold">Modifier</span></td>
<td id="S4.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.2.1.1.3.1" class="ltx_text ltx_font_bold">Mixup</span></td>
<td id="S4.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.2.1.1.4.1" class="ltx_text ltx_font_bold">Re-train</span></td>
<td id="S4.T3.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.2.1.1.5.1" class="ltx_text ltx_font_bold">Finetune</span></td>
<td id="S4.T3.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T3.2.1.1.6.1" class="ltx_text ltx_font_bold">IF</span></td>
</tr>
<tr id="S4.T3.2.1.2" class="ltx_tr">
<td id="S4.T3.2.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.1" class="ltx_text ltx_font_bold">100</span></td>
<td id="S4.T3.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.2.1" class="ltx_text ltx_font_bold">50</span></td>
<td id="S4.T3.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.3.1" class="ltx_text ltx_font_bold">10</span></td>
</tr>
<tr id="S4.T3.2.1.3" class="ltx_tr">
<td id="S4.T3.2.1.3.1" class="ltx_td ltx_align_center ltx_border_t">(a)</td>
<td id="S4.T3.2.1.3.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.2.1.3.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.2.1.3.4" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.2.1.3.5" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.2.1.3.6" class="ltx_td ltx_align_center ltx_border_t">52.41</td>
<td id="S4.T3.2.1.3.7" class="ltx_td ltx_align_center ltx_border_t">56.99</td>
<td id="S4.T3.2.1.3.8" class="ltx_td ltx_align_center ltx_border_t">66.34</td>
</tr>
<tr id="S4.T3.2.1.4" class="ltx_tr">
<td id="S4.T3.2.1.4.1" class="ltx_td ltx_align_center">(b)</td>
<td id="S4.T3.2.1.4.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.2.1.4.3" class="ltx_td"></td>
<td id="S4.T3.2.1.4.4" class="ltx_td"></td>
<td id="S4.T3.2.1.4.5" class="ltx_td"></td>
<td id="S4.T3.2.1.4.6" class="ltx_td ltx_align_center">53.54</td>
<td id="S4.T3.2.1.4.7" class="ltx_td ltx_align_center">57.09</td>
<td id="S4.T3.2.1.4.8" class="ltx_td ltx_align_center">66.66</td>
</tr>
<tr id="S4.T3.2.1.5" class="ltx_tr">
<td id="S4.T3.2.1.5.1" class="ltx_td ltx_align_center">(c)</td>
<td id="S4.T3.2.1.5.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.2.1.5.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.2.1.5.4" class="ltx_td"></td>
<td id="S4.T3.2.1.5.5" class="ltx_td"></td>
<td id="S4.T3.2.1.5.6" class="ltx_td ltx_align_center">55.45</td>
<td id="S4.T3.2.1.5.7" class="ltx_td ltx_align_center">58.69</td>
<td id="S4.T3.2.1.5.8" class="ltx_td ltx_align_center">66.84</td>
</tr>
<tr id="S4.T3.2.1.6" class="ltx_tr">
<td id="S4.T3.2.1.6.1" class="ltx_td ltx_align_center">(d)</td>
<td id="S4.T3.2.1.6.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.2.1.6.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.2.1.6.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.2.1.6.5" class="ltx_td"></td>
<td id="S4.T3.2.1.6.6" class="ltx_td ltx_align_center">57.31</td>
<td id="S4.T3.2.1.6.7" class="ltx_td ltx_align_center">60.34</td>
<td id="S4.T3.2.1.6.8" class="ltx_td ltx_align_center">67.90</td>
</tr>
<tr id="S4.T3.2.1.7" class="ltx_tr">
<td id="S4.T3.2.1.7.1" class="ltx_td ltx_align_center ltx_border_bb">(e)</td>
<td id="S4.T3.2.1.7.2" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S4.T3.2.1.7.3" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S4.T3.2.1.7.4" class="ltx_td ltx_border_bb"></td>
<td id="S4.T3.2.1.7.5" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S4.T3.2.1.7.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.2.1.7.6.1" class="ltx_text ltx_font_bold">58.59</span></td>
<td id="S4.T3.2.1.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.2.1.7.7.1" class="ltx_text ltx_font_bold">61.36</span></td>
<td id="S4.T3.2.1.7.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.2.1.7.8.1" class="ltx_text ltx_font_bold">69.01</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study of SYNAuG.<span id="S4.T3.5.2.1" class="ltx_text ltx_font_medium">
We use CIFAR100-LT.
Each component, Modifier, Mixup, Re-train, and Finetune, means we use the class-related modifiers in the prompt, use Mixup augmentation during training, and re-train or finetune the last layer after training, respectively.
(e) stands for our SYNAuG.
</span></span></figcaption>
</figure>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Experimental setting.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.4" class="ltx_p">We employ two long-tail datasets: CIFAR100-LT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and ImageNet100-LT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
CIFAR100-LT and ImageNet100-LT have train sets that are artificially curated to make class imbalance from the original datasets, CIFAR100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and ImageNet100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
The test sets for them are the same as the original one.
The classes in the long-tailed datasets are divided into three groups: Many-shot (more than 100 samples), Medium-shot (20-100 samples), and Few-shot (less than 20 samples).
For CIFAR100-LT, the imbalance factor (IF) can be controlled by computing the ratio of samples in the head to tail class, <math id="S4.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="N_{1}/N_{K}" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><msub id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">N</mi><mn id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">/</mo><msub id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">N</mi><mi id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">K</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1"><divide id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.1"></divide><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.2">𝑁</ci><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.2.3">1</cn></apply><apply id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.2">𝑁</ci><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.3.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.1c">N_{1}/N_{K}</annotation></semantics></math>, where <math id="S4.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="N_{k}=\left|\mathcal{D}_{k}\right|" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><msub id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">N</mi><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">k</mi></msub><mo id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">=</mo><mrow id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.2.cmml"><mo id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.2.1.cmml">|</mo><msub id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.2.cmml">𝒟</mi><mi id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1"><eq id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.2"></eq><apply id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.2">𝑁</ci><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.3.3">𝑘</ci></apply><apply id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1"><abs id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.2.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.2"></abs><apply id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.2">𝒟</ci><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.1.1.1.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.2.m2.1c">N_{k}=\left|\mathcal{D}_{k}\right|</annotation></semantics></math>, and <math id="S4.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{k}" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.3.m3.1a"><msub id="S4.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml">𝒟</mi><mi id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.2">𝒟</ci><ci id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.3.m3.1c">\mathcal{D}_{k}</annotation></semantics></math> is the set of samples belonging to the class <math id="S4.SS1.SSS0.Px1.p1.4.m4.3" class="ltx_Math" alttext="k\in\{1,\cdots,K\}" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.4.m4.3a"><mrow id="S4.SS1.SSS0.Px1.p1.4.m4.3.4" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.cmml"><mi id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.2.cmml">k</mi><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.1.cmml">∈</mo><mrow id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.1.cmml"><mo stretchy="false" id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.1.cmml">{</mo><mn id="S4.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">1</mn><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.SSS0.Px1.p1.4.m4.2.2" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2.cmml">⋯</mi><mo id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.1.cmml">,</mo><mi id="S4.SS1.SSS0.Px1.p1.4.m4.3.3" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3.cmml">K</mi><mo stretchy="false" id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.4" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.4.m4.3b"><apply id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4"><in id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.1"></in><ci id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.2.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.2">𝑘</ci><set id="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.4.3.2"><cn type="integer" id="S4.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.1.1">1</cn><ci id="S4.SS1.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.2.2">⋯</ci><ci id="S4.SS1.SSS0.Px1.p1.4.m4.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.4.m4.3.3">𝐾</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.4.m4.3c">k\in\{1,\cdots,K\}</annotation></semantics></math>.
As the IF value increases, the skewness of the training data becomes more severe, which makes it more challenging.
We evaluate under the standard IFs of 100, 50, and 10, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
We use ResNet32 for CIFAR100-LT and ResNet50 for ImageNet100-LT.
Further details can be found in the supplementary material.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Competing methods and baselines.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">We compare with recent prior arts: SSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and PaCo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> for self-supervised learning, RISDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and CMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> for data augmentation, and Weight Balancing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> for the rebalance classifier.
They are state-of-the-art in each perspective and propose methods only using
the original long-tailed data without external data sources.</p>
</div>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p2.1" class="ltx_p">We present other
variants of generation methods as baselines:
1) Motivated by the recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> using the few-shot original samples as guidance during the generation process, we first introduce
<em id="S4.SS1.SSS0.Px2.p2.1.1" class="ltx_emph ltx_font_italic">Intra-class Image Translation</em>, where we use the original samples from the original training data as a class-wise guidance image for generation,
2) Inspired by the M2m <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> translating an image of the major class to the minor class for leveraging the diversity of the majority information, we introduce
<em id="S4.SS1.SSS0.Px2.p2.1.2" class="ltx_emph ltx_font_italic">Inter-class Image Translation</em>, where we utilize random samples in the dataset as guidance regardless of the class,
3) As an advanced version motivated by DreamBooth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, we fine-tune the diffusion model with the samples in each class to model the class-wise distribution, named <em id="S4.SS1.SSS0.Px2.p2.1.3" class="ltx_emph ltx_font_italic">Class Distribution Fitting</em>,
and 4) As a strong baseline, we collect the real data from the internet instead of generating synthetic images, <em id="S4.SS1.SSS0.Px2.p2.1.4" class="ltx_emph ltx_font_italic">i.e</em>., <em id="S4.SS1.SSS0.Px2.p2.1.5" class="ltx_emph ltx_font_italic">Web crawled images</em>.
Details are in the supplementary material.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison results.</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">We compare SYNAuG with the prior arts in Table <a href="#S3.T1" title="Table 1 ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Compared to the CE method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> trained with the Cross Entropy loss on the original data, we achieve large improvements when exploiting the generated samples regardless of the skewness of the training data.
Our method also outperforms
most of the competing methods.
This is stunning results in that it
suggests
that relieving the imbalance from the data point of view is simple but more effective than the conventional complex algorithmic methods.
</p>
</div>
<div id="S4.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p2.1" class="ltx_p">In Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare our method with our proposed baselines.
Compared to the case that uses real-world web data<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We collected image from Google image search. Google image search returns images very favorable to DNNs, because Google has used CNN-based image search since March 2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Thus, using web data is analogous to the distillation of a Google internal model, <em id="footnote2.1" class="ltx_emph ltx_font_italic">i.e</em>., very strong baseline.</span></span></span>, it shows that the generated images are of sufficient quality to mitigate the class imbalance problem.
Also, we evaluate additional baselines, which apply the variant methods during the generation process.
While they are better than training only with the original long-tailed data (CE method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>), the performance is lower than SYNAuG.
The results imply that the domain gap between the original and synthetic data is hard to narrow during the generation process.
Thus, we propose to leverage
Mixup during training and fine-tuning the classifier as a more straightforward way.
Note that naïvely applying Mixup to imbalanced data cases is known to be detrimental <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>; thus, we distinctively apply Mixup after uniformizing data distribution which makes a noticeable difference.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ablation study.</h4>

<div id="S4.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px4.p1.1" class="ltx_p">In Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we conduct an ablation study to investigate the influence of each component of our SYNAuG.
When we use modifiers in the prompt, we can get diverse generated samples, which result in the gain between (a) and (b).
We can achieve further improvement by utilizing Mixup (c) to interpolate between original and synthetic data, whereby the domain gap is mitigated by bridging two different domain data.
Despite the domain Mixup,
the classifier still has room to be more adjusted toward the target data.
To do so, we can re-train (d) or fine-tune (e) the last layer on the uniform distribution data sampling from the original training data, <em id="S4.SS1.SSS0.Px4.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>., we set the number of samples in each data class to the smallest number of samples in the original long-tail training data class.
As shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>-(d,e),
we can achieve an additional improvement by adjusting the classifier towards the targeted real data and found that fine-tuning is more effective than re-training.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2308.00994/assets/x5.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Accuracy [%] (y-axis) vs. number of samples [K] (x-axis).<span id="S4.F4.4.2.1" class="ltx_text ltx_font_medium">
As expected, the performance improves as more synthetic samples are added.
Additionally, it is improved significantly when the Few class disappears as the number of samples per class increases.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Performance according to the number of synthetic data.</h4>

<div id="S4.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px5.p1.2" class="ltx_p">We explore the performance according to
varying number of synthetic data (See Fig. <a href="#S4.F4" title="Figure 4 ‣ Ablation study. ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
We use CIFAR100-LT with the imbalance factor IF=100, <em id="S4.SS1.SSS0.Px5.p1.2.1" class="ltx_emph ltx_font_italic">i.e</em>.,
the total number of the original samples is 10,847.
For LT+<math id="S4.SS1.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS0.Px5.p1.1.m1.1a"><mi id="S4.SS1.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p1.1.m1.1b"><ci id="S4.SS1.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px5.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p1.1.m1.1c">\alpha</annotation></semantics></math>, we uniformly allocate
synthetic data across all classes disregarding the distinction between Many, Medium, and Few classes.
In this case, the absolute difference in sample amount between classes is kept unchanged.
For U<math id="S4.SS1.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS1.SSS0.Px5.p1.2.m2.1a"><mi id="S4.SS1.SSS0.Px5.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px5.p1.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p1.2.m2.1b"><ci id="S4.SS1.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px5.p1.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p1.2.m2.1c">\beta</annotation></semantics></math>, we ensure an equal number of samples in each class by either adding synthetic data or trimming some of the original samples.</p>
</div>
<div id="S4.SS1.SSS0.Px5.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px5.p2.6" class="ltx_p">In Fig. <a href="#S4.F4" title="Figure 4 ‣ Ablation study. ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the performance is improved as the number of samples increases regardless of the data distribution.
As the quantity of synthetic data increases, accuracies of LT+<math id="S4.SS1.SSS0.Px5.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS0.Px5.p2.1.m1.1a"><mi id="S4.SS1.SSS0.Px5.p2.1.m1.1.1" xref="S4.SS1.SSS0.Px5.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p2.1.m1.1b"><ci id="S4.SS1.SSS0.Px5.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px5.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p2.1.m1.1c">\alpha</annotation></semantics></math> and U<math id="S4.SS1.SSS0.Px5.p2.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS1.SSS0.Px5.p2.2.m2.1a"><mi id="S4.SS1.SSS0.Px5.p2.2.m2.1.1" xref="S4.SS1.SSS0.Px5.p2.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p2.2.m2.1b"><ci id="S4.SS1.SSS0.Px5.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px5.p2.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p2.2.m2.1c">\beta</annotation></semantics></math> become quite similar.
We think that LT+<math id="S4.SS1.SSS0.Px5.p2.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS0.Px5.p2.3.m3.1a"><mi id="S4.SS1.SSS0.Px5.p2.3.m3.1.1" xref="S4.SS1.SSS0.Px5.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p2.3.m3.1b"><ci id="S4.SS1.SSS0.Px5.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px5.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p2.3.m3.1c">\alpha</annotation></semantics></math> tends to deviate from the long-tailed distribution as the number
of synthetic data increases, <em id="S4.SS1.SSS0.Px5.p2.6.1" class="ltx_emph ltx_font_italic">i.e</em>., the Few class becomes not Few anymore. Although the disparities
in data quantities across classes exist in LT+<math id="S4.SS1.SSS0.Px5.p2.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS0.Px5.p2.4.m4.1a"><mi id="S4.SS1.SSS0.Px5.p2.4.m4.1.1" xref="S4.SS1.SSS0.Px5.p2.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p2.4.m4.1b"><ci id="S4.SS1.SSS0.Px5.p2.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px5.p2.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p2.4.m4.1c">\alpha</annotation></semantics></math>, this effect diminishes the difference
between LT+<math id="S4.SS1.SSS0.Px5.p2.5.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.SSS0.Px5.p2.5.m5.1a"><mi id="S4.SS1.SSS0.Px5.p2.5.m5.1.1" xref="S4.SS1.SSS0.Px5.p2.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p2.5.m5.1b"><ci id="S4.SS1.SSS0.Px5.p2.5.m5.1.1.cmml" xref="S4.SS1.SSS0.Px5.p2.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p2.5.m5.1c">\alpha</annotation></semantics></math> and U<math id="S4.SS1.SSS0.Px5.p2.6.m6.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS1.SSS0.Px5.p2.6.m6.1a"><mi id="S4.SS1.SSS0.Px5.p2.6.m6.1.1" xref="S4.SS1.SSS0.Px5.p2.6.m6.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px5.p2.6.m6.1b"><ci id="S4.SS1.SSS0.Px5.p2.6.m6.1.1.cmml" xref="S4.SS1.SSS0.Px5.p2.6.m6.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px5.p2.6.m6.1c">\beta</annotation></semantics></math> with more synthetic data.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Performance according to the quality of synthetic data.</h4>

<div id="S4.SS1.SSS0.Px6.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px6.p1.1" class="ltx_p">We evaluate SYNAuG on ImageNet100-LT.
We conduct an ablation study to investigate the impact of data quality of
SYNAuG by controlling the diffusion step parameter
of Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, which is known to affect the quality of generated images.
As shown in
Fig. <a href="#S4.F5" title="Figure 5 ‣ Performance according to the quality of synthetic data. ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>-(Top), the generation quality is low when the number of steps is very small, but there is no big difference to the naked eye as it goes up to a certain number.
Figure <a href="#S4.F5" title="Figure 5 ‣ Performance according to the quality of synthetic data. ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>-(Bottom) shows the quantitative results.
Compared to the CE method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> trained on the original long-tailed data, while the accuracy of the Many class is degraded, we achieve large improvement in the Medium, Few, and even All cases
regardless of the synthetic image quality.
However, there is a certain level of quality that exhibits a surge point in performance.
The difference becomes negligible when
the step value exceeds a certain threshold, <em id="S4.SS1.SSS0.Px6.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>., quality.</p>
</div>
<figure id="S4.F5" class="ltx_figure ltx_figure_panel"><img src="/html/2308.00994/assets/x6.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="183" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figure id="S4.F5.tab1" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.F5.tab1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:175.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(61.5pt,-24.9pt) scale(1.39590975451287,1.39590975451287) ;">
<table id="S4.F5.tab1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.F5.tab1.1.1.1" class="ltx_tr">
<td id="S4.F5.tab1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.F5.tab1.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.F5.tab1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F5.tab1.1.1.1.2.1" class="ltx_text ltx_font_bold"># step</span></td>
<td id="S4.F5.tab1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F5.tab1.1.1.1.3.1" class="ltx_text ltx_font_bold">Many</span></td>
<td id="S4.F5.tab1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F5.tab1.1.1.1.4.1" class="ltx_text ltx_font_bold">Medium</span></td>
<td id="S4.F5.tab1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F5.tab1.1.1.1.5.1" class="ltx_text ltx_font_bold">Few</span></td>
<td id="S4.F5.tab1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.F5.tab1.1.1.1.6.1" class="ltx_text ltx_font_bold">All</span></td>
</tr>
<tr id="S4.F5.tab1.1.1.2" class="ltx_tr">
<td id="S4.F5.tab1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">CE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S4.F5.tab1.1.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="S4.F5.tab1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.F5.tab1.1.1.2.3.1" class="ltx_text ltx_font_bold">61.85</span></td>
<td id="S4.F5.tab1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">15.83</td>
<td id="S4.F5.tab1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.29</td>
<td id="S4.F5.tab1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">32.06</td>
</tr>
<tr id="S4.F5.tab1.1.1.3" class="ltx_tr">
<td id="S4.F5.tab1.1.1.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="5"><span id="S4.F5.tab1.1.1.3.1.1" class="ltx_text">SYNAuG</span></td>
<td id="S4.F5.tab1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.F5.tab1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">48.23</td>
<td id="S4.F5.tab1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">46.29</td>
<td id="S4.F5.tab1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">40.07</td>
<td id="S4.F5.tab1.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t">45.10</td>
</tr>
<tr id="S4.F5.tab1.1.1.4" class="ltx_tr">
<td id="S4.F5.tab1.1.1.4.1" class="ltx_td ltx_align_center">10</td>
<td id="S4.F5.tab1.1.1.4.2" class="ltx_td ltx_align_center">53.89</td>
<td id="S4.F5.tab1.1.1.4.3" class="ltx_td ltx_align_center"><span id="S4.F5.tab1.1.1.4.3.1" class="ltx_text ltx_font_bold">49.49</span></td>
<td id="S4.F5.tab1.1.1.4.4" class="ltx_td ltx_align_center">43.87</td>
<td id="S4.F5.tab1.1.1.4.5" class="ltx_td ltx_align_center"><span id="S4.F5.tab1.1.1.4.5.1" class="ltx_text ltx_font_bold">49.34</span></td>
</tr>
<tr id="S4.F5.tab1.1.1.5" class="ltx_tr">
<td id="S4.F5.tab1.1.1.5.1" class="ltx_td ltx_align_center">50</td>
<td id="S4.F5.tab1.1.1.5.2" class="ltx_td ltx_align_center">52.91</td>
<td id="S4.F5.tab1.1.1.5.3" class="ltx_td ltx_align_center">48.63</td>
<td id="S4.F5.tab1.1.1.5.4" class="ltx_td ltx_align_center"><span id="S4.F5.tab1.1.1.5.4.1" class="ltx_text ltx_font_bold">45.27</span></td>
<td id="S4.F5.tab1.1.1.5.5" class="ltx_td ltx_align_center">49.12</td>
</tr>
<tr id="S4.F5.tab1.1.1.6" class="ltx_tr">
<td id="S4.F5.tab1.1.1.6.1" class="ltx_td ltx_align_center">100</td>
<td id="S4.F5.tab1.1.1.6.2" class="ltx_td ltx_align_center">53.03</td>
<td id="S4.F5.tab1.1.1.6.3" class="ltx_td ltx_align_center">49.20</td>
<td id="S4.F5.tab1.1.1.6.4" class="ltx_td ltx_align_center">44.47</td>
<td id="S4.F5.tab1.1.1.6.5" class="ltx_td ltx_align_center">49.12</td>
</tr>
<tr id="S4.F5.tab1.1.1.7" class="ltx_tr">
<td id="S4.F5.tab1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_bb">300</td>
<td id="S4.F5.tab1.1.1.7.2" class="ltx_td ltx_align_center ltx_border_bb">54.11</td>
<td id="S4.F5.tab1.1.1.7.3" class="ltx_td ltx_align_center ltx_border_bb">47.71</td>
<td id="S4.F5.tab1.1.1.7.4" class="ltx_td ltx_align_center ltx_border_bb">44.73</td>
<td id="S4.F5.tab1.1.1.7.5" class="ltx_td ltx_align_center ltx_border_bb">49.06</td>
</tr>
</table>
</span></div>
</figure>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.6.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study according to sample quality.<span id="S4.F5.7.2.1" class="ltx_text ltx_font_medium">
</span>(Top)<span id="S4.F5.7.2.2" class="ltx_text ltx_font_medium"> quality of the generated samples according to the number of steps, </span>(Bottom)<span id="S4.F5.7.2.3" class="ltx_text ltx_font_medium"> long-tailed recognition performance (%)
according to the different times of steps for generating synthetic data, which affects sample quality.
We use ImageNet100-LT with ResNet50.
</span></span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Fairness</h3>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2308.00994/assets/x7.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Influence of the class and group imbalance on classifier during training.<span id="S4.F6.4.2.1" class="ltx_text ltx_font_medium">
The 2D data are sampled from the normal distributions with four different means and the same covariance.
We simulate 4 different experiments with the latent group imbalance (sensitive attributes) by adjusting the number of data in each group.
The total number of samples is the same.
We train classifiers for the classes on different imbalance settings and visualize the learned classifiers (bold black lines).
The fairer the classifiers, the more vertically aligned.
The classifier trained on the class imbalance is more unfair than the one on the group imbalance.
</span></span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T5.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T5.st1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:221.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(93.3pt,-47.6pt) scale(1.75524726389227,1.75524726389227) ;">
<table id="S4.T5.st1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.st1.2.1.1" class="ltx_tr">
<td id="S4.T5.st1.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T5.st1.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T5.st1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st1.2.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S4.T5.st1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st1.2.1.1.3.1" class="ltx_text ltx_font_bold">DP</span></td>
<td id="S4.T5.st1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st1.2.1.1.4.1" class="ltx_text ltx_font_bold">ED</span></td>
<td id="S4.T5.st1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st1.2.1.1.5.1" class="ltx_text ltx_font_bold">EO</span></td>
</tr>
<tr id="S4.T5.st1.2.1.2" class="ltx_tr">
<td id="S4.T5.st1.2.1.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="5">ResNet18</td>
</tr>
<tr id="S4.T5.st1.2.1.3" class="ltx_tr">
<td id="S4.T5.st1.2.1.3.1" class="ltx_td ltx_align_left ltx_border_t">ERM</td>
<td id="S4.T5.st1.2.1.3.2" class="ltx_td ltx_align_center ltx_border_t">93.9</td>
<td id="S4.T5.st1.2.1.3.3" class="ltx_td ltx_align_center ltx_border_t">0.0817</td>
<td id="S4.T5.st1.2.1.3.4" class="ltx_td ltx_align_center ltx_border_t">0.0632</td>
<td id="S4.T5.st1.2.1.3.5" class="ltx_td ltx_align_center ltx_border_t">0.0779</td>
</tr>
<tr id="S4.T5.st1.2.1.4" class="ltx_tr">
<td id="S4.T5.st1.2.1.4.1" class="ltx_td ltx_align_left">SYNAuG</td>
<td id="S4.T5.st1.2.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T5.st1.2.1.4.2.1" class="ltx_text ltx_font_bold">94.1</span></td>
<td id="S4.T5.st1.2.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.st1.2.1.4.3.1" class="ltx_text ltx_font_bold">0.060</span></td>
<td id="S4.T5.st1.2.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T5.st1.2.1.4.4.1" class="ltx_text ltx_font_bold">0.0462</span></td>
<td id="S4.T5.st1.2.1.4.5" class="ltx_td ltx_align_center"><span id="S4.T5.st1.2.1.4.5.1" class="ltx_text ltx_font_bold">0.0434</span></td>
</tr>
<tr id="S4.T5.st1.2.1.5" class="ltx_tr">
<td id="S4.T5.st1.2.1.5.1" class="ltx_td ltx_align_center ltx_border_t" colspan="5">ResNet50</td>
</tr>
<tr id="S4.T5.st1.2.1.6" class="ltx_tr">
<td id="S4.T5.st1.2.1.6.1" class="ltx_td ltx_align_left ltx_border_t">ERM</td>
<td id="S4.T5.st1.2.1.6.2" class="ltx_td ltx_align_center ltx_border_t">94.0</td>
<td id="S4.T5.st1.2.1.6.3" class="ltx_td ltx_align_center ltx_border_t">0.07272</td>
<td id="S4.T5.st1.2.1.6.4" class="ltx_td ltx_align_center ltx_border_t">0.0585</td>
<td id="S4.T5.st1.2.1.6.5" class="ltx_td ltx_align_center ltx_border_t">0.07269</td>
</tr>
<tr id="S4.T5.st1.2.1.7" class="ltx_tr">
<td id="S4.T5.st1.2.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">SYNAuG</td>
<td id="S4.T5.st1.2.1.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.st1.2.1.7.2.1" class="ltx_text ltx_font_bold">94.4</span></td>
<td id="S4.T5.st1.2.1.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.st1.2.1.7.3.1" class="ltx_text ltx_font_bold">0.05432</span></td>
<td id="S4.T5.st1.2.1.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.st1.2.1.7.4.1" class="ltx_text ltx_font_bold">0.03936</span></td>
<td id="S4.T5.st1.2.1.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.st1.2.1.7.5.1" class="ltx_text ltx_font_bold">0.05472</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.st1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T5.st1.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Fairness performance.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T5.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T5.st2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:202.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(81.6pt,-38.0pt) scale(1.6036050036716,1.6036050036716) ;">
<table id="S4.T5.st2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.st2.2.1.1" class="ltx_tr">
<td id="S4.T5.st2.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T5.st2.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T5.st2.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st2.2.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S4.T5.st2.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st2.2.1.1.3.1" class="ltx_text ltx_font_bold">DP</span></td>
<td id="S4.T5.st2.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st2.2.1.1.4.1" class="ltx_text ltx_font_bold">ED</span></td>
<td id="S4.T5.st2.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st2.2.1.1.5.1" class="ltx_text ltx_font_bold">EO</span></td>
</tr>
<tr id="S4.T5.st2.2.1.2" class="ltx_tr">
<td id="S4.T5.st2.2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">ERM</td>
<td id="S4.T5.st2.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">93.9</td>
<td id="S4.T5.st2.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.08174</td>
<td id="S4.T5.st2.2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.0632</td>
<td id="S4.T5.st2.2.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.0779</td>
</tr>
<tr id="S4.T5.st2.2.1.3" class="ltx_tr">
<td id="S4.T5.st2.2.1.3.1" class="ltx_td ltx_align_left">+ Group-DRO</td>
<td id="S4.T5.st2.2.1.3.2" class="ltx_td ltx_align_center">93.9</td>
<td id="S4.T5.st2.2.1.3.3" class="ltx_td ltx_align_center">0.07266</td>
<td id="S4.T5.st2.2.1.3.4" class="ltx_td ltx_align_center">0.05819</td>
<td id="S4.T5.st2.2.1.3.5" class="ltx_td ltx_align_center">0.06954</td>
</tr>
<tr id="S4.T5.st2.2.1.4" class="ltx_tr">
<td id="S4.T5.st2.2.1.4.1" class="ltx_td ltx_align_left">+ RS</td>
<td id="S4.T5.st2.2.1.4.2" class="ltx_td ltx_align_center">93.8</td>
<td id="S4.T5.st2.2.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.st2.2.1.4.3.1" class="ltx_text ltx_font_bold">0.0636</span></td>
<td id="S4.T5.st2.2.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T5.st2.2.1.4.4.1" class="ltx_text ltx_font_bold">0.04663</span></td>
<td id="S4.T5.st2.2.1.4.5" class="ltx_td ltx_align_center"><span id="S4.T5.st2.2.1.4.5.1" class="ltx_text ltx_font_bold">0.05808</span></td>
</tr>
<tr id="S4.T5.st2.2.1.5" class="ltx_tr">
<td id="S4.T5.st2.2.1.5.1" class="ltx_td ltx_align_left ltx_border_t">SYNAuG</td>
<td id="S4.T5.st2.2.1.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.st2.2.1.5.2.1" class="ltx_text ltx_font_bold">94.0</span></td>
<td id="S4.T5.st2.2.1.5.3" class="ltx_td ltx_align_center ltx_border_t">0.06995</td>
<td id="S4.T5.st2.2.1.5.4" class="ltx_td ltx_align_center ltx_border_t">0.0563</td>
<td id="S4.T5.st2.2.1.5.5" class="ltx_td ltx_align_center ltx_border_t">0.06825</td>
</tr>
<tr id="S4.T5.st2.2.1.6" class="ltx_tr">
<td id="S4.T5.st2.2.1.6.1" class="ltx_td ltx_align_left">+ Group-DRO</td>
<td id="S4.T5.st2.2.1.6.2" class="ltx_td ltx_align_center">93.9</td>
<td id="S4.T5.st2.2.1.6.3" class="ltx_td ltx_align_center">0.0711</td>
<td id="S4.T5.st2.2.1.6.4" class="ltx_td ltx_align_center">0.05433</td>
<td id="S4.T5.st2.2.1.6.5" class="ltx_td ltx_align_center">0.07045</td>
</tr>
<tr id="S4.T5.st2.2.1.7" class="ltx_tr">
<td id="S4.T5.st2.2.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">+ RS</td>
<td id="S4.T5.st2.2.1.7.2" class="ltx_td ltx_align_center ltx_border_bb">93.6</td>
<td id="S4.T5.st2.2.1.7.3" class="ltx_td ltx_align_center ltx_border_bb">0.06786</td>
<td id="S4.T5.st2.2.1.7.4" class="ltx_td ltx_align_center ltx_border_bb">0.06439</td>
<td id="S4.T5.st2.2.1.7.5" class="ltx_td ltx_align_center ltx_border_bb">0.0812</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.st2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T5.st2.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation with other algorithms</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T5.st3" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T5.st3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:218pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(91.5pt,-46.0pt) scale(1.73043905041084,1.73043905041084) ;">
<table id="S4.T5.st3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.st3.2.1.1" class="ltx_tr">
<td id="S4.T5.st3.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T5.st3.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T5.st3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st3.2.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S4.T5.st3.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st3.2.1.1.3.1" class="ltx_text ltx_font_bold">DP</span></td>
<td id="S4.T5.st3.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st3.2.1.1.4.1" class="ltx_text ltx_font_bold">ED</span></td>
<td id="S4.T5.st3.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st3.2.1.1.5.1" class="ltx_text ltx_font_bold">EO</span></td>
</tr>
<tr id="S4.T5.st3.2.1.2" class="ltx_tr">
<td id="S4.T5.st3.2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">ERM</td>
<td id="S4.T5.st3.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">93.9</td>
<td id="S4.T5.st3.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.08174</td>
<td id="S4.T5.st3.2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.0632</td>
<td id="S4.T5.st3.2.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.0779</td>
</tr>
<tr id="S4.T5.st3.2.1.3" class="ltx_tr">
<td id="S4.T5.st3.2.1.3.1" class="ltx_td ltx_align_left">+ Mixup</td>
<td id="S4.T5.st3.2.1.3.2" class="ltx_td ltx_align_center">93.9</td>
<td id="S4.T5.st3.2.1.3.3" class="ltx_td ltx_align_center">0.07266</td>
<td id="S4.T5.st3.2.1.3.4" class="ltx_td ltx_align_center">0.05819</td>
<td id="S4.T5.st3.2.1.3.5" class="ltx_td ltx_align_center">0.06954</td>
</tr>
<tr id="S4.T5.st3.2.1.4" class="ltx_tr">
<td id="S4.T5.st3.2.1.4.1" class="ltx_td ltx_align_left">+ CutMix</td>
<td id="S4.T5.st3.2.1.4.2" class="ltx_td ltx_align_center">94.7</td>
<td id="S4.T5.st3.2.1.4.3" class="ltx_td ltx_align_center">0.08265</td>
<td id="S4.T5.st3.2.1.4.4" class="ltx_td ltx_align_center">0.06245</td>
<td id="S4.T5.st3.2.1.4.5" class="ltx_td ltx_align_center">0.08333</td>
</tr>
<tr id="S4.T5.st3.2.1.5" class="ltx_tr">
<td id="S4.T5.st3.2.1.5.1" class="ltx_td ltx_align_left ltx_border_t">SYNAuG</td>
<td id="S4.T5.st3.2.1.5.2" class="ltx_td ltx_align_center ltx_border_t">94.0</td>
<td id="S4.T5.st3.2.1.5.3" class="ltx_td ltx_align_center ltx_border_t">0.06995</td>
<td id="S4.T5.st3.2.1.5.4" class="ltx_td ltx_align_center ltx_border_t">0.0563</td>
<td id="S4.T5.st3.2.1.5.5" class="ltx_td ltx_align_center ltx_border_t">0.06825</td>
</tr>
<tr id="S4.T5.st3.2.1.6" class="ltx_tr">
<td id="S4.T5.st3.2.1.6.1" class="ltx_td ltx_align_left">+ Mixup</td>
<td id="S4.T5.st3.2.1.6.2" class="ltx_td ltx_align_center">94.2</td>
<td id="S4.T5.st3.2.1.6.3" class="ltx_td ltx_align_center"><span id="S4.T5.st3.2.1.6.3.1" class="ltx_text ltx_font_bold">0.064</span></td>
<td id="S4.T5.st3.2.1.6.4" class="ltx_td ltx_align_center"><span id="S4.T5.st3.2.1.6.4.1" class="ltx_text ltx_font_bold">0.03793</span></td>
<td id="S4.T5.st3.2.1.6.5" class="ltx_td ltx_align_center"><span id="S4.T5.st3.2.1.6.5.1" class="ltx_text ltx_font_bold">0.04658</span></td>
</tr>
<tr id="S4.T5.st3.2.1.7" class="ltx_tr">
<td id="S4.T5.st3.2.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">+ CutMix</td>
<td id="S4.T5.st3.2.1.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.st3.2.1.7.2.1" class="ltx_text ltx_font_bold">94.9</span></td>
<td id="S4.T5.st3.2.1.7.3" class="ltx_td ltx_align_center ltx_border_bb">0.0743</td>
<td id="S4.T5.st3.2.1.7.4" class="ltx_td ltx_align_center ltx_border_bb">0.04745</td>
<td id="S4.T5.st3.2.1.7.5" class="ltx_td ltx_align_center ltx_border_bb">0.06319</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.st3.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.T5.st3.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Augmentation abalation</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T5.st4" class="ltx_table ltx_figure_panel ltx_align_center">
<div id="S4.T5.st4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:125pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(91.9pt,-26.5pt) scale(1.73557605013371,1.73557605013371) ;">
<table id="S4.T5.st4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.st4.1.1.2" class="ltx_tr">
<td id="S4.T5.st4.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T5.st4.1.1.2.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T5.st4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st4.1.1.2.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S4.T5.st4.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st4.1.1.2.3.1" class="ltx_text ltx_font_bold">DP</span></td>
<td id="S4.T5.st4.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st4.1.1.2.4.1" class="ltx_text ltx_font_bold">ED</span></td>
<td id="S4.T5.st4.1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.st4.1.1.2.5.1" class="ltx_text ltx_font_bold">EO</span></td>
</tr>
<tr id="S4.T5.st4.1.1.1" class="ltx_tr">
<td id="S4.T5.st4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t">SYNAuG<sup id="S4.T5.st4.1.1.1.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T5.st4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.st4.1.1.1.2.1" class="ltx_text ltx_font_bold">94.0</span></td>
<td id="S4.T5.st4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">0.07342</td>
<td id="S4.T5.st4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">0.05805</td>
<td id="S4.T5.st4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">0.07253</td>
</tr>
<tr id="S4.T5.st4.1.1.3" class="ltx_tr">
<td id="S4.T5.st4.1.1.3.1" class="ltx_td ltx_align_left">SYNAuG</td>
<td id="S4.T5.st4.1.1.3.2" class="ltx_td ltx_align_center"><span id="S4.T5.st4.1.1.3.2.1" class="ltx_text ltx_font_bold">94.0</span></td>
<td id="S4.T5.st4.1.1.3.3" class="ltx_td ltx_align_center"><span id="S4.T5.st4.1.1.3.3.1" class="ltx_text ltx_font_bold">0.06995</span></td>
<td id="S4.T5.st4.1.1.3.4" class="ltx_td ltx_align_center"><span id="S4.T5.st4.1.1.3.4.1" class="ltx_text ltx_font_bold">0.0563</span></td>
<td id="S4.T5.st4.1.1.3.5" class="ltx_td ltx_align_center"><span id="S4.T5.st4.1.1.3.5.1" class="ltx_text ltx_font_bold">0.06825</span></td>
</tr>
<tr id="S4.T5.st4.1.1.4" class="ltx_tr">
<td id="S4.T5.st4.1.1.4.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="5">*Not use the sensitivity attribute</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.st4.4.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.T5.st4.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Sensitivity abalation</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.4.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Fairness performance.<span id="S4.T5.5.2.1" class="ltx_text ltx_font_medium">
(a) accuracy and model fairness results of our SYNAuG,
(b) compatibility with other fairness algorithms, Group-DRO and Re-Sampling (RS),
(c) ablation study with data augmentation, Mixup and Cuxmix,
and (d) ablation study using the prior about sensitive attribute. </span>Bold<span id="S4.T5.5.2.2" class="ltx_text ltx_font_medium"> means the highest accuracy and the best fairness performance in a table.
Higher is better in accuracy, and lower is better in fairness metrics.</span></span></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Group imbalance stands for the data imbalance between groups, such as ethnicity.
We empirically observe that the group imbalance with the class imbalance amplifies
unfair classifiers, as shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.2 Model Fairness ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
The class imbalance affects the unfair classifier more than the group imbalance.
Both class and group imbalance contribute to the unfair classifier.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.6" class="ltx_p">Model fairness, one of the problems caused by group imbalance, is essential to prevent unexpected social confusion.
Fairness metrics have been proposed to measure the fairness performance of models:
Demographic Parity (DP) <math id="S4.SS2.p2.1.m1.1" class="ltx_math_unparsed" alttext="=\max_{z}|P(y_{p}{=}1|z){-}P(y_{p}{=}1)|" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1b"><mo id="S4.SS2.p2.1.m1.1.1">=</mo><msub id="S4.SS2.p2.1.m1.1.2"><mi id="S4.SS2.p2.1.m1.1.2.2">max</mi><mi id="S4.SS2.p2.1.m1.1.2.3">z</mi></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S4.SS2.p2.1.m1.1.3">|</mo><mi id="S4.SS2.p2.1.m1.1.4">P</mi><mrow id="S4.SS2.p2.1.m1.1.5"><mo stretchy="false" id="S4.SS2.p2.1.m1.1.5.1">(</mo><msub id="S4.SS2.p2.1.m1.1.5.2"><mi id="S4.SS2.p2.1.m1.1.5.2.2">y</mi><mi id="S4.SS2.p2.1.m1.1.5.2.3">p</mi></msub><mo id="S4.SS2.p2.1.m1.1.5.3">=</mo><mn id="S4.SS2.p2.1.m1.1.5.4">1</mn><mo fence="false" rspace="0.167em" stretchy="false" id="S4.SS2.p2.1.m1.1.5.5">|</mo><mi id="S4.SS2.p2.1.m1.1.5.6">z</mi><mo stretchy="false" id="S4.SS2.p2.1.m1.1.5.7">)</mo></mrow><mo id="S4.SS2.p2.1.m1.1.6">−</mo><mi id="S4.SS2.p2.1.m1.1.7">P</mi><mrow id="S4.SS2.p2.1.m1.1.8"><mo stretchy="false" id="S4.SS2.p2.1.m1.1.8.1">(</mo><msub id="S4.SS2.p2.1.m1.1.8.2"><mi id="S4.SS2.p2.1.m1.1.8.2.2">y</mi><mi id="S4.SS2.p2.1.m1.1.8.2.3">p</mi></msub><mo id="S4.SS2.p2.1.m1.1.8.3">=</mo><mn id="S4.SS2.p2.1.m1.1.8.4">1</mn><mo stretchy="false" id="S4.SS2.p2.1.m1.1.8.5">)</mo></mrow><mo fence="false" stretchy="false" id="S4.SS2.p2.1.m1.1.9">|</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">=\max_{z}|P(y_{p}{=}1|z){-}P(y_{p}{=}1)|</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>,
Equal Opportunity (EO) <math id="S4.SS2.p2.2.m2.4" class="ltx_math_unparsed" alttext="=\max_{z_{i},z_{j},y,y_{p}}|P_{z_{i}}(y_{p}|y){-}P_{z_{j}}(y_{p}|y)|" display="inline"><semantics id="S4.SS2.p2.2.m2.4a"><mrow id="S4.SS2.p2.2.m2.4b"><mo id="S4.SS2.p2.2.m2.4.5">=</mo><msub id="S4.SS2.p2.2.m2.4.6"><mi id="S4.SS2.p2.2.m2.4.6.2">max</mi><mrow id="S4.SS2.p2.2.m2.4.4.4.4"><msub id="S4.SS2.p2.2.m2.2.2.2.2.1"><mi id="S4.SS2.p2.2.m2.2.2.2.2.1.2">z</mi><mi id="S4.SS2.p2.2.m2.2.2.2.2.1.3">i</mi></msub><mo id="S4.SS2.p2.2.m2.4.4.4.4.4">,</mo><msub id="S4.SS2.p2.2.m2.3.3.3.3.2"><mi id="S4.SS2.p2.2.m2.3.3.3.3.2.2">z</mi><mi id="S4.SS2.p2.2.m2.3.3.3.3.2.3">j</mi></msub><mo id="S4.SS2.p2.2.m2.4.4.4.4.5">,</mo><mi id="S4.SS2.p2.2.m2.1.1.1.1">y</mi><mo id="S4.SS2.p2.2.m2.4.4.4.4.6">,</mo><msub id="S4.SS2.p2.2.m2.4.4.4.4.3"><mi id="S4.SS2.p2.2.m2.4.4.4.4.3.2">y</mi><mi id="S4.SS2.p2.2.m2.4.4.4.4.3.3">p</mi></msub></mrow></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S4.SS2.p2.2.m2.4.7">|</mo><msub id="S4.SS2.p2.2.m2.4.8"><mi id="S4.SS2.p2.2.m2.4.8.2">P</mi><msub id="S4.SS2.p2.2.m2.4.8.3"><mi id="S4.SS2.p2.2.m2.4.8.3.2">z</mi><mi id="S4.SS2.p2.2.m2.4.8.3.3">i</mi></msub></msub><mrow id="S4.SS2.p2.2.m2.4.9"><mo stretchy="false" id="S4.SS2.p2.2.m2.4.9.1">(</mo><msub id="S4.SS2.p2.2.m2.4.9.2"><mi id="S4.SS2.p2.2.m2.4.9.2.2">y</mi><mi id="S4.SS2.p2.2.m2.4.9.2.3">p</mi></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S4.SS2.p2.2.m2.4.9.3">|</mo><mi id="S4.SS2.p2.2.m2.4.9.4">y</mi><mo stretchy="false" id="S4.SS2.p2.2.m2.4.9.5">)</mo></mrow><mo id="S4.SS2.p2.2.m2.4.10">−</mo><msub id="S4.SS2.p2.2.m2.4.11"><mi id="S4.SS2.p2.2.m2.4.11.2">P</mi><msub id="S4.SS2.p2.2.m2.4.11.3"><mi id="S4.SS2.p2.2.m2.4.11.3.2">z</mi><mi id="S4.SS2.p2.2.m2.4.11.3.3">j</mi></msub></msub><mrow id="S4.SS2.p2.2.m2.4.12"><mo stretchy="false" id="S4.SS2.p2.2.m2.4.12.1">(</mo><msub id="S4.SS2.p2.2.m2.4.12.2"><mi id="S4.SS2.p2.2.m2.4.12.2.2">y</mi><mi id="S4.SS2.p2.2.m2.4.12.2.3">p</mi></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S4.SS2.p2.2.m2.4.12.3">|</mo><mi id="S4.SS2.p2.2.m2.4.12.4">y</mi><mo stretchy="false" id="S4.SS2.p2.2.m2.4.12.5">)</mo></mrow><mo fence="false" stretchy="false" id="S4.SS2.p2.2.m2.4.13">|</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.4c">=\max_{z_{i},z_{j},y,y_{p}}|P_{z_{i}}(y_{p}|y){-}P_{z_{j}}(y_{p}|y)|</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>,
and Equalized Odds (ED) <math id="S4.SS2.p2.3.m3.5" class="ltx_math_unparsed" alttext="=\max_{z,y,y_{p}}|P(y_{p}|z,y){-}P(y_{p}|y)|" display="inline"><semantics id="S4.SS2.p2.3.m3.5a"><mrow id="S4.SS2.p2.3.m3.5b"><mo id="S4.SS2.p2.3.m3.5.6">=</mo><msub id="S4.SS2.p2.3.m3.5.7"><mi id="S4.SS2.p2.3.m3.5.7.2">max</mi><mrow id="S4.SS2.p2.3.m3.3.3.3.3"><mi id="S4.SS2.p2.3.m3.1.1.1.1">z</mi><mo id="S4.SS2.p2.3.m3.3.3.3.3.2">,</mo><mi id="S4.SS2.p2.3.m3.2.2.2.2">y</mi><mo id="S4.SS2.p2.3.m3.3.3.3.3.3">,</mo><msub id="S4.SS2.p2.3.m3.3.3.3.3.1"><mi id="S4.SS2.p2.3.m3.3.3.3.3.1.2">y</mi><mi id="S4.SS2.p2.3.m3.3.3.3.3.1.3">p</mi></msub></mrow></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S4.SS2.p2.3.m3.5.8">|</mo><mi id="S4.SS2.p2.3.m3.5.9">P</mi><mrow id="S4.SS2.p2.3.m3.5.10"><mo stretchy="false" id="S4.SS2.p2.3.m3.5.10.1">(</mo><msub id="S4.SS2.p2.3.m3.5.10.2"><mi id="S4.SS2.p2.3.m3.5.10.2.2">y</mi><mi id="S4.SS2.p2.3.m3.5.10.2.3">p</mi></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S4.SS2.p2.3.m3.5.10.3">|</mo><mi id="S4.SS2.p2.3.m3.4.4">z</mi><mo id="S4.SS2.p2.3.m3.5.10.4">,</mo><mi id="S4.SS2.p2.3.m3.5.5">y</mi><mo stretchy="false" id="S4.SS2.p2.3.m3.5.10.5">)</mo></mrow><mo id="S4.SS2.p2.3.m3.5.11">−</mo><mi id="S4.SS2.p2.3.m3.5.12">P</mi><mrow id="S4.SS2.p2.3.m3.5.13"><mo stretchy="false" id="S4.SS2.p2.3.m3.5.13.1">(</mo><msub id="S4.SS2.p2.3.m3.5.13.2"><mi id="S4.SS2.p2.3.m3.5.13.2.2">y</mi><mi id="S4.SS2.p2.3.m3.5.13.2.3">p</mi></msub><mo fence="false" rspace="0.167em" stretchy="false" id="S4.SS2.p2.3.m3.5.13.3">|</mo><mi id="S4.SS2.p2.3.m3.5.13.4">y</mi><mo stretchy="false" id="S4.SS2.p2.3.m3.5.13.5">)</mo></mrow><mo fence="false" stretchy="false" id="S4.SS2.p2.3.m3.5.14">|</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.5c">=\max_{z,y,y_{p}}|P(y_{p}|z,y){-}P(y_{p}|y)|</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, where <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="y_{p}" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><msub id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mi id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">y</mi><mi id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">𝑦</ci><ci id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">y_{p}</annotation></semantics></math> is the prediction, <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mi id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><ci id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">y</annotation></semantics></math> is the class label, and <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mi id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><ci id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">z</annotation></semantics></math> is the sensitive attribute.
These metrics are based on the difference in the performance of the learned classifiers depending on groups, <em id="S4.SS2.p2.6.1" class="ltx_emph ltx_font_italic">i.e</em>., the sensitive attributes.
Lower values of fairness metrics indicate that the model is fairer.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Experiments on UTKFace.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We employ UTKFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> composed of 23,708 images with age, gender, and race labels.
We use race annotation as a sensitive attribute (group label) and gender as the class label.
For SYNAuG, we augment the data to mitigate the class imbalance across the sensitive attribute;
the female and male ratio of each sensitive attribute becomes equal.
We evaluate the accuracy and fairness metrics from the model at the last epoch.
Firstly, we validate the effectiveness of SYNAuG in Table <a href="#S4.T5.st1" title="Table 5(a) ‣ Table 5 ‣ 4.2 Model Fairness ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>.
The result shows that SYNAuG outperforms ERM in accuracy and fairness metrics on both ResNet18 and ResNet50.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ablation with other algorithms.</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">In Table <a href="#S4.T5.st2" title="Table 5(b) ‣ Table 5 ‣ 4.2 Model Fairness ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a>, we evaluate the performance of the two algorithms Group-DRO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> and Re-Sampling (RS).
Note that we do not apply Mixup and fine-tuning in this experiment.
Group-DRO and RS improve the fairness metrics of ERM at the same time.
SYNAuG without Group-DRO outperforms the accuracy and two fairness metrics, ED and EO, compared to Group-DRO.
Developing a fairness algorithm with synthetic data might be a promising direction toward a fair model.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Augmentation ablation.</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">In Table <a href="#S4.T5.st3" title="Table 5(c) ‣ Table 5 ‣ 4.2 Model Fairness ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(c)</span></a>, we compare the effect of data augmentations, Mixup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> and CutMix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.
In this ablation study, we do not apply fine-tuning for clear comparison.
Both augmentations improve the accuracy of ERM; Mixup also works
in the fairness metrics.
Compared to ERM with Mixup, SYNAuG shows higher accuracy and better fairness metrics.
SYNAuG with Mixup outperforms more in accuracy and fairness metrics compared to ERM with Mixup.
</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">No prior of sensitive attribute.</h4>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.2" class="ltx_p">Labeling sensitive attributes might be expensive.
In this ablation study, we augment the synthetic data to mitigate the class imbalance regardless of sensitive attributes.
We denote this setting as SYNAuG<sup id="S4.SS2.SSS0.Px4.p1.2.1" class="ltx_sup">∗</sup>.
As shown in Table <a href="#S4.T5.st4" title="Table 5(d) ‣ Table 5 ‣ 4.2 Model Fairness ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(d)</span></a>, SYNAuG<sup id="S4.SS2.SSS0.Px4.p1.2.2" class="ltx_sup">∗</sup> shows better fairness metrics compared to ERM.
However, exploiting the knowledge of sensitive attributes is more effective.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Summary.</h4>

<div id="S4.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px5.p1.1" class="ltx_p">Class imbalance in fairness can easily cause an unfair model.
This motivates us to balance the class imbalance using synthetic data before tackling fairness directly.
We observe that the synthetic data improves both model accuracy and fairness simultaneously.
The experimental results also demonstrate that SYNAuG is compatible with other training algorithms, data augmentation, and network architecture.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Robustness to Spurious Correlation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">A class may include dominant patterns, <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>., spurious correlations.
For example, waterbirds are usually on water rather than land.
It leads DNNs to rely heavily on these spurious features rather than reasoning; thereby, DNNs classify water images as waterbirds regardless of the existence of waterbirds.
This spurious correlation is also caused by data imbalance because fewer waterbirds are located on land.
While spurious correlation occurs naturally, we can mitigate their impact by resolving data imbalance
similar to model fairness.
We demonstrate whether SYNAuG can mitigate the data imbalance problem of spurious correlations.
</p>
</div>
<figure id="S4.T6" class="ltx_table">
<div id="S4.T6.19" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:257.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(31.6pt,-20.9pt) scale(1.1934010906446,1.1934010906446) ;">
<table id="S4.T6.19.19" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.19.19.20" class="ltx_tr">
<td id="S4.T6.19.19.20.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S4.T6.19.19.20.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S4.T6.19.19.20.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T6.19.19.20.2.1" class="ltx_text ltx_font_bold">Waterbirds</span></td>
</tr>
<tr id="S4.T6.19.19.21" class="ltx_tr">
<td id="S4.T6.19.19.21.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.19.19.21.1.1" class="ltx_text ltx_font_bold">Worst</span></td>
<td id="S4.T6.19.19.21.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.19.19.21.2.1" class="ltx_text ltx_font_bold">Mean</span></td>
</tr>
<tr id="S4.T6.19.19.22" class="ltx_tr">
<td id="S4.T6.19.19.22.1" class="ltx_td ltx_align_left ltx_border_t">ERM</td>
<td id="S4.T6.19.19.22.2" class="ltx_td ltx_align_center ltx_border_t">72.6</td>
<td id="S4.T6.19.19.22.3" class="ltx_td ltx_align_center ltx_border_t">97.3</td>
</tr>
<tr id="S4.T6.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_align_left">JTT<math id="S4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\texttt{ ICML'21}}" display="inline"><semantics id="S4.T6.1.1.1.1.m1.1a"><msub id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml"><mi id="S4.T6.1.1.1.1.m1.1.1a" xref="S4.T6.1.1.1.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_monospace" id="S4.T6.1.1.1.1.m1.1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.1a.cmml"> ICML’21</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><apply id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1"><ci id="S4.T6.1.1.1.1.m1.1.1.1a.cmml" xref="S4.T6.1.1.1.1.m1.1.1.1"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.T6.1.1.1.1.m1.1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1.1"> ICML’21</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">{}_{\texttt{ ICML'21}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center">86.7</td>
<td id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center">93.3</td>
</tr>
<tr id="S4.T6.2.2.2" class="ltx_tr">
<td id="S4.T6.2.2.2.1" class="ltx_td ltx_align_left">Group-DRO<math id="S4.T6.2.2.2.1.m1.1" class="ltx_Math" alttext="{}_{\texttt{ ICLR'20}}" display="inline"><semantics id="S4.T6.2.2.2.1.m1.1a"><msub id="S4.T6.2.2.2.1.m1.1.1" xref="S4.T6.2.2.2.1.m1.1.1.cmml"><mi id="S4.T6.2.2.2.1.m1.1.1a" xref="S4.T6.2.2.2.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_monospace" id="S4.T6.2.2.2.1.m1.1.1.1" xref="S4.T6.2.2.2.1.m1.1.1.1a.cmml"> ICLR’20</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.1.m1.1b"><apply id="S4.T6.2.2.2.1.m1.1.1.cmml" xref="S4.T6.2.2.2.1.m1.1.1"><ci id="S4.T6.2.2.2.1.m1.1.1.1a.cmml" xref="S4.T6.2.2.2.1.m1.1.1.1"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.T6.2.2.2.1.m1.1.1.1.cmml" xref="S4.T6.2.2.2.1.m1.1.1.1"> ICLR’20</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.1.m1.1c">{}_{\texttt{ ICLR'20}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S4.T6.2.2.2.2" class="ltx_td ltx_align_center">91.4</td>
<td id="S4.T6.2.2.2.3" class="ltx_td ltx_align_center">93.5</td>
</tr>
<tr id="S4.T6.4.4.4" class="ltx_tr">
<td id="S4.T6.3.3.3.1" class="ltx_td ltx_align_left">SUBG<math id="S4.T6.3.3.3.1.m1.1" class="ltx_Math" alttext="{}_{\texttt{ ICML'20}}" display="inline"><semantics id="S4.T6.3.3.3.1.m1.1a"><msub id="S4.T6.3.3.3.1.m1.1.1" xref="S4.T6.3.3.3.1.m1.1.1.cmml"><mi id="S4.T6.3.3.3.1.m1.1.1a" xref="S4.T6.3.3.3.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_monospace" id="S4.T6.3.3.3.1.m1.1.1.1" xref="S4.T6.3.3.3.1.m1.1.1.1a.cmml"> ICML’20</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.1.m1.1b"><apply id="S4.T6.3.3.3.1.m1.1.1.cmml" xref="S4.T6.3.3.3.1.m1.1.1"><ci id="S4.T6.3.3.3.1.m1.1.1.1a.cmml" xref="S4.T6.3.3.3.1.m1.1.1.1"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.T6.3.3.3.1.m1.1.1.1.cmml" xref="S4.T6.3.3.3.1.m1.1.1.1"> ICML’20</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.1.m1.1c">{}_{\texttt{ ICML'20}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S4.T6.4.4.4.2" class="ltx_td ltx_align_center">89.1<sub id="S4.T6.4.4.4.2.1" class="ltx_sub"><span id="S4.T6.4.4.4.2.1.1" class="ltx_text ltx_font_italic">±1.1</span></sub>
</td>
<td id="S4.T6.4.4.4.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T6.7.7.7" class="ltx_tr">
<td id="S4.T6.5.5.5.1" class="ltx_td ltx_align_left">SSA<math id="S4.T6.5.5.5.1.m1.1" class="ltx_Math" alttext="{}_{\texttt{ ICLR'22}}" display="inline"><semantics id="S4.T6.5.5.5.1.m1.1a"><msub id="S4.T6.5.5.5.1.m1.1.1" xref="S4.T6.5.5.5.1.m1.1.1.cmml"><mi id="S4.T6.5.5.5.1.m1.1.1a" xref="S4.T6.5.5.5.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_monospace" id="S4.T6.5.5.5.1.m1.1.1.1" xref="S4.T6.5.5.5.1.m1.1.1.1a.cmml"> ICLR’22</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.5.1.m1.1b"><apply id="S4.T6.5.5.5.1.m1.1.1.cmml" xref="S4.T6.5.5.5.1.m1.1.1"><ci id="S4.T6.5.5.5.1.m1.1.1.1a.cmml" xref="S4.T6.5.5.5.1.m1.1.1.1"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.T6.5.5.5.1.m1.1.1.1.cmml" xref="S4.T6.5.5.5.1.m1.1.1.1"> ICLR’22</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.5.1.m1.1c">{}_{\texttt{ ICLR'22}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S4.T6.6.6.6.2" class="ltx_td ltx_align_center">89.0<sub id="S4.T6.6.6.6.2.1" class="ltx_sub"><span id="S4.T6.6.6.6.2.1.1" class="ltx_text ltx_font_italic">±0.55</span></sub>
</td>
<td id="S4.T6.7.7.7.3" class="ltx_td ltx_align_center">92.2<sub id="S4.T6.7.7.7.3.1" class="ltx_sub"><span id="S4.T6.7.7.7.3.1.1" class="ltx_text ltx_font_italic">±0.87</span></sub>
</td>
</tr>
<tr id="S4.T6.9.9.9" class="ltx_tr">
<td id="S4.T6.9.9.9.3" class="ltx_td ltx_align_left ltx_border_t">BaseModel</td>
<td id="S4.T6.8.8.8.1" class="ltx_td ltx_align_center ltx_border_t">73.7<sub id="S4.T6.8.8.8.1.1" class="ltx_sub"><span id="S4.T6.8.8.8.1.1.1" class="ltx_text ltx_font_italic">±3.04</span></sub>
</td>
<td id="S4.T6.9.9.9.2" class="ltx_td ltx_align_center ltx_border_t">90.4<sub id="S4.T6.9.9.9.2.1" class="ltx_sub"><span id="S4.T6.9.9.9.2.1.1" class="ltx_text ltx_font_italic">±0.21</span></sub>
</td>
</tr>
<tr id="S4.T6.11.11.11" class="ltx_tr">
<td id="S4.T6.11.11.11.3" class="ltx_td ltx_align_left">SYNAuG</td>
<td id="S4.T6.10.10.10.1" class="ltx_td ltx_align_center">79.9<sub id="S4.T6.10.10.10.1.1" class="ltx_sub"><span id="S4.T6.10.10.10.1.1.1" class="ltx_text ltx_font_italic">±2.22</span></sub>
</td>
<td id="S4.T6.11.11.11.2" class="ltx_td ltx_align_center">91.52<sub id="S4.T6.11.11.11.2.1" class="ltx_sub"><span id="S4.T6.11.11.11.2.1.1" class="ltx_text ltx_font_italic">±0.98</span></sub>
</td>
</tr>
<tr id="S4.T6.15.15.15" class="ltx_tr">
<td id="S4.T6.13.13.13.2" class="ltx_td ltx_align_left ltx_border_t">DFR<math id="S4.T6.12.12.12.1.m1.1" class="ltx_Math" alttext="{}^{\text{Tr}}_{\text{Val}}" display="inline"><semantics id="S4.T6.12.12.12.1.m1.1a"><mmultiscripts id="S4.T6.12.12.12.1.m1.1.1" xref="S4.T6.12.12.12.1.m1.1.1.cmml"><mi id="S4.T6.12.12.12.1.m1.1.1.2.2" xref="S4.T6.12.12.12.1.m1.1.1.2.2.cmml"></mi><mprescripts id="S4.T6.12.12.12.1.m1.1.1a" xref="S4.T6.12.12.12.1.m1.1.1.cmml"></mprescripts><mtext id="S4.T6.12.12.12.1.m1.1.1.3" xref="S4.T6.12.12.12.1.m1.1.1.3a.cmml">Val</mtext><mrow id="S4.T6.12.12.12.1.m1.1.1b" xref="S4.T6.12.12.12.1.m1.1.1.cmml"></mrow><mrow id="S4.T6.12.12.12.1.m1.1.1c" xref="S4.T6.12.12.12.1.m1.1.1.cmml"></mrow><mtext id="S4.T6.12.12.12.1.m1.1.1.2.3" xref="S4.T6.12.12.12.1.m1.1.1.2.3a.cmml">Tr</mtext></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.T6.12.12.12.1.m1.1b"><apply id="S4.T6.12.12.12.1.m1.1.1.cmml" xref="S4.T6.12.12.12.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.12.12.12.1.m1.1.1.1.cmml" xref="S4.T6.12.12.12.1.m1.1.1">subscript</csymbol><apply id="S4.T6.12.12.12.1.m1.1.1.2.cmml" xref="S4.T6.12.12.12.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T6.12.12.12.1.m1.1.1.2.1.cmml" xref="S4.T6.12.12.12.1.m1.1.1">superscript</csymbol><csymbol cd="latexml" id="S4.T6.12.12.12.1.m1.1.1.2.2.cmml" xref="S4.T6.12.12.12.1.m1.1.1.2.2">absent</csymbol><ci id="S4.T6.12.12.12.1.m1.1.1.2.3a.cmml" xref="S4.T6.12.12.12.1.m1.1.1.2.3"><mtext mathsize="70%" id="S4.T6.12.12.12.1.m1.1.1.2.3.cmml" xref="S4.T6.12.12.12.1.m1.1.1.2.3">Tr</mtext></ci></apply><ci id="S4.T6.12.12.12.1.m1.1.1.3a.cmml" xref="S4.T6.12.12.12.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T6.12.12.12.1.m1.1.1.3.cmml" xref="S4.T6.12.12.12.1.m1.1.1.3">Val</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.12.12.12.1.m1.1c">{}^{\text{Tr}}_{\text{Val}}</annotation></semantics></math><math id="S4.T6.13.13.13.2.m2.1" class="ltx_Math" alttext="{}_{\texttt{ ICLR'23}}" display="inline"><semantics id="S4.T6.13.13.13.2.m2.1a"><msub id="S4.T6.13.13.13.2.m2.1.1" xref="S4.T6.13.13.13.2.m2.1.1.cmml"><mi id="S4.T6.13.13.13.2.m2.1.1a" xref="S4.T6.13.13.13.2.m2.1.1.cmml"></mi><mtext class="ltx_mathvariant_monospace" id="S4.T6.13.13.13.2.m2.1.1.1" xref="S4.T6.13.13.13.2.m2.1.1.1a.cmml"> ICLR’23</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T6.13.13.13.2.m2.1b"><apply id="S4.T6.13.13.13.2.m2.1.1.cmml" xref="S4.T6.13.13.13.2.m2.1.1"><ci id="S4.T6.13.13.13.2.m2.1.1.1a.cmml" xref="S4.T6.13.13.13.2.m2.1.1.1"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.T6.13.13.13.2.m2.1.1.1.cmml" xref="S4.T6.13.13.13.2.m2.1.1.1"> ICLR’23</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.13.13.13.2.m2.1c">{}_{\texttt{ ICLR'23}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S4.T6.14.14.14.3" class="ltx_td ltx_align_center ltx_border_t">91.2<sub id="S4.T6.14.14.14.3.1" class="ltx_sub"><span id="S4.T6.14.14.14.3.1.1" class="ltx_text ltx_font_italic">±1.92</span></sub>
</td>
<td id="S4.T6.15.15.15.4" class="ltx_td ltx_align_center ltx_border_t">93.1<sub id="S4.T6.15.15.15.4.1" class="ltx_sub"><span id="S4.T6.15.15.15.4.1.1" class="ltx_text ltx_font_italic">±0.91</span></sub>
</td>
</tr>
<tr id="S4.T6.17.17.17" class="ltx_tr">
<td id="S4.T6.17.17.17.3" class="ltx_td ltx_align_left">SYNAuG (re-train)</td>
<td id="S4.T6.16.16.16.1" class="ltx_td ltx_align_center">92.9<sub id="S4.T6.16.16.16.1.1" class="ltx_sub"><span id="S4.T6.16.16.16.1.1.1" class="ltx_text ltx_font_italic">±0.45</span></sub>
</td>
<td id="S4.T6.17.17.17.2" class="ltx_td ltx_align_center">93.6<sub id="S4.T6.17.17.17.2.1" class="ltx_sub"><span id="S4.T6.17.17.17.2.1.1" class="ltx_text ltx_font_italic">±0.35</span></sub>
</td>
</tr>
<tr id="S4.T6.19.19.19" class="ltx_tr">
<td id="S4.T6.19.19.19.3" class="ltx_td ltx_align_left ltx_border_bb">SYNAuG (fine-tuning)</td>
<td id="S4.T6.18.18.18.1" class="ltx_td ltx_align_center ltx_border_bb">93.2<sub id="S4.T6.18.18.18.1.1" class="ltx_sub"><span id="S4.T6.18.18.18.1.1.1" class="ltx_text ltx_font_italic">±0.42</span></sub>
</td>
<td id="S4.T6.19.19.19.2" class="ltx_td ltx_align_center ltx_border_bb">94.8<sub id="S4.T6.19.19.19.2.1" class="ltx_sub"><span id="S4.T6.19.19.19.2.1.1" class="ltx_text ltx_font_italic">±0.11</span></sub>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.22.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.23.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Robustness to spurious correlation on Waterbirds.<span id="S4.T6.23.2.1" class="ltx_text ltx_font_medium">
SYNAuG outperforms DFR consistently in worst-group accuracy.</span></span></figcaption>
</figure>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Experiments.</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.8" class="ltx_p">We use Waterbirds dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, which is a synthetic dataset created by combining images of birds from the CUB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> with backgrounds.
The birds are grouped into two categories: waterbirds, which include seabirds and waterfowl, and landbirds.
Land and water background are spurious features.
Let <math id="S4.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="G_{\text{background}}^{\text{class}}" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.1.m1.1a"><msubsup id="S4.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.3a.cmml">background</mtext><mtext id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3a.cmml">class</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.3">background</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3">class</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.1.m1.1c">G_{\text{background}}^{\text{class}}</annotation></semantics></math> be the class with the background, <em id="S4.SS3.SSS0.Px1.p1.8.1" class="ltx_emph ltx_font_italic">e.g</em>., <math id="S4.SS3.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="G_{\text{water}}^{\text{landbird}}" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.2.m2.1a"><msubsup id="S4.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3a.cmml">water</mtext><mtext id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3a.cmml">landbird</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.2.3">water</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.2.m2.1.1.3">landbird</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.2.m2.1c">G_{\text{water}}^{\text{landbird}}</annotation></semantics></math> is the landbird with water background.
In the Waterbirds dataset, <math id="S4.SS3.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="G_{\text{land}}^{\text{landbird}}" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.3.m3.1a"><msubsup id="S4.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.3a.cmml">land</mtext><mtext id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3a.cmml">landbird</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.2.3">land</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.3.m3.1.1.3">landbird</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.3.m3.1c">G_{\text{land}}^{\text{landbird}}</annotation></semantics></math> has more samples than <math id="S4.SS3.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="G_{\text{water}}^{\text{landbird}}" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.4.m4.1a"><msubsup id="S4.SS3.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.3a.cmml">water</mtext><mtext id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3a.cmml">landbird</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.4.m4.1b"><apply id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.2.3">water</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.4.m4.1.1.3">landbird</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.4.m4.1c">G_{\text{water}}^{\text{landbird}}</annotation></semantics></math>, and <math id="S4.SS3.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="G_{\text{water}}^{\text{waterbird}}" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.5.m5.1a"><msubsup id="S4.SS3.SSS0.Px1.p1.5.m5.1.1" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.3a.cmml">water</mtext><mtext id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3a.cmml">waterbird</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.5.m5.1b"><apply id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.2.3">water</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.5.m5.1.1.3">waterbird</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.5.m5.1c">G_{\text{water}}^{\text{waterbird}}</annotation></semantics></math> has more samples than <math id="S4.SS3.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="G_{\text{land}}^{\text{waterbird}}" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.6.m6.1a"><msubsup id="S4.SS3.SSS0.Px1.p1.6.m6.1.1" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.3a.cmml">land</mtext><mtext id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3a.cmml">waterbird</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.6.m6.1b"><apply id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.2.3">land</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.6.m6.1.1.3">waterbird</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.6.m6.1c">G_{\text{land}}^{\text{waterbird}}</annotation></semantics></math>.
We generate the samples to match the number of samples such as <math id="S4.SS3.SSS0.Px1.p1.7.m7.2" class="ltx_Math" alttext="|G_{\text{land}}^{\text{landbird}}|=|G_{\text{water}}^{\text{landbird}}|" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.7.m7.2a"><mrow id="S4.SS3.SSS0.Px1.p1.7.m7.2.2" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.cmml"><mrow id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.2" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.2.1.cmml">|</mo><msubsup id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.3a.cmml">land</mtext><mtext id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.3a.cmml">landbird</mtext></msubsup><mo stretchy="false" id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.2.1.cmml">|</mo></mrow><mo id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.3.cmml">=</mo><mrow id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.2" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.2.1.cmml">|</mo><msubsup id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.3a.cmml">water</mtext><mtext id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.3a.cmml">landbird</mtext></msubsup><mo stretchy="false" id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.3" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.7.m7.2b"><apply id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2"><eq id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.3"></eq><apply id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1"><abs id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.2"></abs><apply id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.2.3">land</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.1.1.1.1.1.3">landbird</mtext></ci></apply></apply><apply id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1"><abs id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.2"></abs><apply id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.2.3">water</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.7.m7.2.2.2.1.1.3">landbird</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.7.m7.2c">|G_{\text{land}}^{\text{landbird}}|=|G_{\text{water}}^{\text{landbird}}|</annotation></semantics></math> and <math id="S4.SS3.SSS0.Px1.p1.8.m8.2" class="ltx_Math" alttext="|G_{\text{water}}^{\text{waterbird}}|=|G_{\text{land}}^{\text{waterbird}}|" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.8.m8.2a"><mrow id="S4.SS3.SSS0.Px1.p1.8.m8.2.2" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.cmml"><mrow id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.2" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.2.1.cmml">|</mo><msubsup id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.3a.cmml">water</mtext><mtext id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.3a.cmml">waterbird</mtext></msubsup><mo stretchy="false" id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.2.1.cmml">|</mo></mrow><mo id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.3.cmml">=</mo><mrow id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.2" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.2.1.cmml">|</mo><msubsup id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.2" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.2.cmml">G</mi><mtext id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.3a.cmml">land</mtext><mtext id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.3a.cmml">waterbird</mtext></msubsup><mo stretchy="false" id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.3" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.8.m8.2b"><apply id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2"><eq id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.3"></eq><apply id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1"><abs id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.2"></abs><apply id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.2.3">water</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.1.1.1.1.1.3">waterbird</mtext></ci></apply></apply><apply id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1"><abs id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.2"></abs><apply id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1">superscript</csymbol><apply id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.1.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1">subscript</csymbol><ci id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.2.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.2">𝐺</ci><ci id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.3.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.2.3">land</mtext></ci></apply><ci id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.3a.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.3"><mtext mathsize="70%" id="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.8.m8.2.2.2.1.1.3">waterbird</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.8.m8.2c">|G_{\text{water}}^{\text{waterbird}}|=|G_{\text{land}}^{\text{waterbird}}|</annotation></semantics></math>.
We report the result over 5 independent runs using the code from DFR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
We reproduce the BaseModel and DFR and report the performance at the last epoch<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/PolinaKirichenko/deep_feature_reweighting" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PolinaKirichenko/deep_feature_reweighting</a></span></span></span>.</p>
</div>
<div id="S4.SS3.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p2.1" class="ltx_p">In Table <a href="#S4.T6" title="Table 6 ‣ 4.3 Model Robustness to Spurious Correlation ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, our SYNAuG generates samples not to be correlated with spurious features, which
improves the performance in BaseModel both on worst and mean accuracies.
When applying DFR, the synthetic data can increase the worst and mean accuracy consistently.
We also observe that fine-tuning is more effective
compared to re-train, which is consistent with Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Long-tailed Recognition ‣ 4 Experiments ‣ SYNAuG: Exploiting Synthetic Data for Data Imbalance Problems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
The overall results demonstrate that the synthetic data from the generative model can be exploited to mitigate the spurious correlation.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We propose SYNAuG which deals with long-tailed recognition, model fairness, and robustness to spurious correlations as data imbalance problems.
The development process of the machine learning model
can be roughly divided into
data curation, model training, and model management.
Since the data comes first in the process,
a flaw in the dataset affects the subsequent phases; thus, it is crucial.
Our study suggests the importance of controlling imbalance from the data perspective.
We believe that taking the controllability of data is a promising research direction to resolve the early bottleneck in machine learning model development.
While we focus on the data perspective, improving the model in multiple views is necessary for effective solutions to data imbalance.
We conclude our work with the following discussions.
</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Other perspectives.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">We have suggested the usage of synthetic data from pre-trained generative models as a new data perspective baseline for the data imbalance problem, but there may be other perspectives.
We observed a gradual performance decline when substituting real data with synthetic data, suggesting the potential need for domain adaptation.
There could be future research directions, <em id="S5.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>., more sophisticated data augmentation, automated data curation, transfer learning, the usage of differentiability of the generative models, and comprehending taxonomies across classes.
While we emphasize that our work suggests a promising way to redraw the direction to overcome the data imbalance problems in the data perspective, more interesting future work will come with integrating multiple levels.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitations of using generative models.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">The generation of synthetic data demands additional time and computational resources.
While the curation of a real dataset requires enormous time, human, and financial resources, the process of generating synthetic data becomes increasingly challenging as the volume of data needed increases.
Also, the quality of the synthesized data varies depending on factors such as the prompt, guidance level, and step value of the diffusion model, impacting the overall performance of the model.
However, since generative models have been continuously developed in terms of sample quality, time efficiency, and controllability,
we believe that exploiting generation models as a data source is a promising research direction as the performance of generation models is improved.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Long-tailed recognition via weight balancing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Antreas Antoniou, Amos Storkey, and Harrison Edwards.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Data augmentation generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Invariant risk minimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.02893</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and
David J Fleet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Synthetic data from diffusion models improves imagenet
classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.08466</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Learning de-biased representations with biased representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning (ICML)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages
528–539. PMLR, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Robust optimization</span><span id="bib.bib6.3.2" class="ltx_text" style="font-size:90%;">, volume 28.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">Princeton university press, 2009.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Learning imbalanced datasets with label-distribution-aware margin
loss.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Fantasia3d: Disentangling geometry and appearance for high-quality
text-to-3d content creation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.13873</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Webly supervised learning of convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">,
2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Xiaohua Chen, Yucan Zhou, Dayan Wu, Wanqian Zhang, Yu Zhou, Bo Li, and Weiping
Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Imagine by reasoning: A reasoning-based implicit semantic data
augmentation for long-tailed classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI Conference on Artificial Intelligence (AAAI)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Ching-Yao Chuang and Youssef Mroueh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Fair mixup: Fairness via interpolation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Jiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Parametric contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Class-balanced loss based on effective number of samples.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas,
Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Flownet: Learning optical flow with convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">,
2015.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and
Suresh Venkatasubramanian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Certifying and removing disparate impact.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">KDD</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, page 259–268, 2015.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,
Wieland Brendel, Matthias Bethge, and Felix A Wichmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Shortcut learning in deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature Machine Intelligence</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Yunhui Han, Kunming Luo, Ao Luo, Jiangyu Liu, Haoqiang Fan, Guiming Luo, and
Shuaicheng Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Realflow: Em-based realistic optical flow dataset generation from
videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Moritz Hardt, Eric Price, and Nati Srebro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Equality of opportunity in supervised learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">,
2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Haibo He and Edwardo A Garcia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Learning from imbalanced data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on knowledge and data engineering</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">,
21:1263–1284, 2009.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song
Bai, and Xiaojuan Qi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Is synthetic data from generative models ready for image recognition?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">,
2023.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Deep imbalanced learning for face recognition and attribute
prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI)</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 42(11):2781–2794, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Inwoo Hwang, Sangjun Lee, Yunhyeok Kwak, Seong Joon Oh, Damien Teney, Jin-Hwa
Kim, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Selecmix: Debiased learning by contradicting-pair sampling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Simple data balancing achieves competitive worst-group-accuracy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Causal Learning and Reasoning</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 336–351,
2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Generative models as a data source for multiview representation
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Ziyu Jiang, Tianlong Chen, Bobak J Mortazavi, and Zhangyang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Self-damaging contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning (ICML)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Sangwon Jung, Sanghyuk Chun, and Taesup Moon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Learning fair classifiers with partially annotated group labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Sangwon Jung, Taeeon Park, Sanghyuk Chun, and Taesup Moon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Re-weighting based group fairness regularization via classwise robust
optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">,
2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Faisal Kamiran and Toon Calders.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Data preprocessing techniques for classification without
discrimination.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Knowledge and information systems</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 33:1–33, 2012.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Jaehyung Kim, Jongheon Jeong, and Jinwoo Shin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">M2m: Imbalanced classification via major-to-minor translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Nayeong Kim, Sehyun Hwang, Sungsoo Ahn, Jaesik Park, and Suha Kwak.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Learning debiased classifier with biased committee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Last layer re-training is sufficient for robustness to spurious
correlations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">,
2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky, Geoffrey Hinton, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Learning multiple layers of features from tiny images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">2009.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and Jaegul Choo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Learning debiased representation via disentangled feature
augmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">,
volume 34, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang,
and Chelsea Finn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Surgical fine-tuning improves adaptation to distribution shifts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Tianhao Li, Limin Wang, and Gangshan Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Self supervision to distillation for long-tailed visual recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh,
Shiori Sagawa, Percy Liang, and Chelsea Finn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Just train twice: Improving group robustness without training group
information.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning (ICML)</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Deep learning face attributes in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 3730–3738, 2015.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X
Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Large-scale long-tailed recognition in an open world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers,
Alexey Dosovitskiy, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">A large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Learning from failure: Training debiased classifier from biased
classifier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Spread spurious attribute: Improving worst-group accuracy with
spurious attribute estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Arvind Narayanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Translation tutorial: 21 fairness definitions and their politics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">FAccT</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, volume 1170, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
Bob McGrew, Ilya Sutskever, and Mark Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Glide: Towards photorealistic image generation and editing with
text-guided diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning (ICML)</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib, Fr’edo Durand,
William T Freeman, and Wojciech Matusik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Learning-based video motion magnification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Training language models to follow instructions with human feedback.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Liyuan Pan, Shah Chowdhury, Richard Hartley, Miaomiao Liu, Hongguang Zhang, and
Hongdong Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Dual pixel exploration: Simultaneous depth estimation and image
restoration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 4340–4349, 2021.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Seulki Park, Youngkyu Hong, Byeongho Heo, Sangdoo Yun, and Jin Young Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">The majority can help the minority: Context-rich minority
oversampling for long-tailed classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Dreamfusion: Text-to-3d using 2d diffusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">,
2023.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Ben Mildenhall, Nataniel
Ruiz, Shiran Zada, Kfir Aberman, Michael Rubenstein, Jonathan Barron,
Yuanzhen Li, and Varun Jampani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Dreambooth3d: Subject-driven text-to-3d generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Jiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Balanced meta-softmax for long-tailed visual recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Fairbatch: Batch selection for model fairness.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and
Kfir Aberman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Dreambooth: Fine tuning text-to-image diffusion models for
subject-driven generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Serim Ryou, Seong-Gyun Jeong, and Pietro Perona.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Anchor loss: Modulating loss scale based on prediction difficulty.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Distributionally robust neural networks for group shifts: On the
importance of regularization for worst-case generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">An investigation of why overparameterization exacerbates spurious
correlations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning (ICML)</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim
Salimans, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Photorealistic text-to-image diffusion models with deep language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen
Paritosh, and Lora M Aroyo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">“everyone wants to do the model work, not the data work”: Data
cascades in high-stakes ai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Dvir Samuel and Gal Chechik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Distributional robustness loss for long-tail learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Veit Sandfort, Ke Yan, Perry J Pickhardt, and Ronald M Summers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Data augmentation using generative adversarial networks (cyclegan) to
improve generalizability in ct segmentation tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Scientific reports</span><span id="bib.bib63.4.2" class="ltx_text" style="font-size:90%;">, 9:16884, 2019.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, Michael Poli, and Sangdoo Yun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Which shortcut cues will dnns choose? a study from the
parameter-space perspective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Li Shen, Zhouchen Lin, and Qingming Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Relay backpropagation for effective learning of deep convolutional
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun Jampani, Michael Krainin,
Huiwen Chang, Ramin Zabih, William T Freeman, and Ce Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Autoflow: Learning a better training set for optical flow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Enzo Tartaglione, Carlo Alberto Barbano, and Marco Grangetto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">End: Entangling and disentangling deep representations for bias
correction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, pages 13508–13517, 2021.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Unshuffling data for improved generalization in visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">,
pages 1417–1427, 2021.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Contrastive multiview coding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib69.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Effective data augmentation with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib70.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and Ian Reid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">A bayesian data augmentation approach for learning deep models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib71.4.2" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Vladimir Vapnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The nature of statistical learning theory</span><span id="bib.bib72.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.4.1" class="ltx_text" style="font-size:90%;">Springer science &amp; business media, 1999.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Long-tailed recognition by routing diverse distribution-aware
experts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib73.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib73.5.3" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P.
Perona.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Caltech-UCSD Birds 200.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">Technical Report CNS-TR-2010-001, California Institute of Technology,
2010.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
Steven Euijong Whang, Ki Hyun Tae, Yuji Roh, and Geon Heo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.2.1" class="ltx_text" style="font-size:90%;">Responsible ai challenges in end-to-end machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2101.05967</span><span id="bib.bib75.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text" style="font-size:90%;">
Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea
Finn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.2.1" class="ltx_text" style="font-size:90%;">Improving out-of-distribution robustness via selective augmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib76.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning (ICML)</span><span id="bib.bib76.5.3" class="ltx_text" style="font-size:90%;">, pages
25407–25437. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text" style="font-size:90%;">
Moon Ye-Bin, Jisoo Kim, Hongyeob Kim, Kilho Son, and Tae-Hyun Oh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.2.1" class="ltx_text" style="font-size:90%;">Textmania: Enriching visual feature by text-driven manifold
augmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib77.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib77.5.3" class="ltx_text" style="font-size:90%;">,
2023.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text" style="font-size:90%;">
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and
Youngjoon Yoo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.2.1" class="ltx_text" style="font-size:90%;">Cutmix: Regularization strategy to train strong classifiers with
localizable features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib78.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib78.5.3" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text" style="font-size:90%;">
Xianli Zeng, Edgar Dobriban, and Guang Cheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.2.1" class="ltx_text" style="font-size:90%;">Fair bayes-optimal classifiers under predictive parity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib79.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NeurIPS)</span><span id="bib.bib79.5.3" class="ltx_text" style="font-size:90%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text" style="font-size:90%;">
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.2.1" class="ltx_text" style="font-size:90%;">mixup: Beyond empirical risk minimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib80.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</span><span id="bib.bib80.5.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text" style="font-size:90%;">
Michael Zhang, Nimit S Sohoni, Hongyang R Zhang, Chelsea Finn, and Christopher
Ré.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.2.1" class="ltx_text" style="font-size:90%;">Correct-n-contrast: A contrastive approach for improving robustness
to spurious correlations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib81.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning (ICML)</span><span id="bib.bib81.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text" style="font-size:90%;">
Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.2.1" class="ltx_text" style="font-size:90%;">Deep long-tailed learning: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.3.1" class="ltx_text" style="font-size:90%;">2023.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text" style="font-size:90%;">
Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela
Barriuso, Antonio Torralba, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.2.1" class="ltx_text" style="font-size:90%;">Datasetgan: Efficient labeled data factory with minimal human effort.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib83.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib83.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text" style="font-size:90%;">
Zhifei Zhang, Yang Song, and Hairong Qi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.2.1" class="ltx_text" style="font-size:90%;">Age progression/regression by conditional adversarial autoencoder.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib84.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib84.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text" style="font-size:90%;">
Zhedong Zheng, Liang Zheng, and Yi Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.2.1" class="ltx_text" style="font-size:90%;">Unlabeled samples generated by gan improve the person
re-identification baseline in vitro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib85.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib85.5.3" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.00993" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.00994" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.00994">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.00994" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.00995" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 10:00:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
