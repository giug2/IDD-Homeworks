<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit</title>
<!--Generated on Mon Sep 30 14:43:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.20435v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S1" title="In ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S2" title="In ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S2.SS1" title="In II Related Work ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Datasets for Anomaly Detection and Space Operations</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S2.SS1.SSS0.Px1" title="In II-A Datasets for Anomaly Detection and Space Operations ‣ II Related Work ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title">Space Operations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S2.SS1.SSS0.Px2" title="In II-A Datasets for Anomaly Detection and Space Operations ‣ II Related Work ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title">Industrial Defect Inspection</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S2.SS2" title="In II Related Work ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Anomaly Detection Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S2.SS2.SSS1" title="In II-B Anomaly Detection Methods ‣ II Related Work ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span>Representation-based methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S2.SS2.SSS2" title="In II-B Anomaly Detection Methods ‣ II Related Work ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span>Reconstruction-based methods</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3" title="In ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Dataset Description</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.SS1" title="In III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Blender Model Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.SS2" title="In III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Image Rendering</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.SS3" title="In III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Dataset Validation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4" title="In ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.SS1" title="In IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.SS2" title="In IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.SS3" title="In IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.SS4" title="In IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Ablation Studies</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.SS5" title="In IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Discussion</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S5" title="In ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">ALLO: A Photorealistic Dataset and Data Generation Pipeline for
<br class="ltx_break"/>Anomaly<span class="ltx_ERROR undefined" id="id11.id1">\sss</span>Detection<span class="ltx_ERROR undefined" id="id12.id2">\sss</span>During<span class="ltx_ERROR undefined" id="id13.id3">\sss</span>Robotic<span class="ltx_ERROR undefined" id="id14.id4">\sss</span>Proximity<span class="ltx_ERROR undefined" id="id15.id5">\sss</span>Operations<span class="ltx_ERROR undefined" id="id16.id6">\sss</span>in<span class="ltx_ERROR undefined" id="id17.id7">\sss</span>Lunar<span class="ltx_ERROR undefined" id="id18.id8">\sss</span>Orbit</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Selina Leveugle<sup class="ltx_sup" id="id19.11.id1">1</sup>, Chang Won Lee<sup class="ltx_sup" id="id20.12.id2">2</sup>, Svetlana Stolpner<sup class="ltx_sup" id="id21.13.id3">3</sup>, Chris Langley<sup class="ltx_sup" id="id22.14.id4">3</sup>,
<br class="ltx_break"/>Paul Grouchy<sup class="ltx_sup" id="id23.15.id5">3</sup>, Steven Waslander<sup class="ltx_sup" id="id24.16.id6">2</sup>, and Jonathan Kelly<sup class="ltx_sup" id="id25.17.id7">1</sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id26.18.id1">1</sup>Space and Terrestrial Autonomous Robotic Systems (STARS) Laboratory at the University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Canada. Email: <span class="ltx_text ltx_font_typewriter" id="id27.19.id2">&lt;firstname&gt;.&lt;lastname&gt;@robotics.utias.utoronto.ca</span><sup class="ltx_sup" id="id28.20.id1">2</sup>Toronto Robotics and AI Laboratory (TRAIL) at the University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Canada. Email: <span class="ltx_text ltx_font_typewriter" id="id29.21.id2">john.lee@robotics.utias.utoronto.ca; steven.waslander@robotics.utias.utoronto.ca</span><sup class="ltx_sup" id="id30.22.id1">3</sup>MDA Space Inc. Email: <span class="ltx_text ltx_font_typewriter" id="id31.23.id2">&lt;firstname&gt;.&lt;lastname&gt;@mda.space</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id32.id1">NASA’s forthcoming Lunar Gateway space station, which will be uncrewed most of the time, will need to operate with an unprecedented level of autonomy. Enhancing autonomy on the Gateway presents several unique challenges, one of which is to equip the Canadarm3, the Gateway’s external robotic system, with the capability to perform worksite monitoring.
Monitoring will involve using the arm’s inspection cameras to detect any anomalies within the operating environment, a task complicated by the widely-varying lighting conditions in space.
In this paper, we introduce the visual anomaly detection and localization task for space applications and establish a benchmark with our novel synthetic dataset called ALLO (for Anomaly Localization in Lunar Orbit).
We develop a complete data generation pipeline to create ALLO, which we use to evaluate the performance of state-of-the-art visual anomaly detection algorithms.
Given the low tolerance for risk during space operations and the lack of relevant data, we emphasize the need for novel, robust, and accurate anomaly detection methods to handle the challenging visual conditions found in lunar orbit and beyond.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Over the past two decades, the international space community has begun to focus its efforts on extending human space exploration beyond low-Earth orbit.
NASA’s Artemis program, for example, aims to deploy the Lunar Gateway, the first space station in lunar orbit, that will test new technologies needed for extended deep-space missions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib1" title="">1</a>]</cite>.
Unlike the International Space Station (ISS), the Gateway will be required to operate autonomously and without an on-board crew for long periods.
Its distance from the Earth and its orbit will hinder rapid and stable communication for control <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib2" title="">2</a>]</cite>.
Autonomy is especially vital for the Canadarm3, the external robotic system under development by MDA Space Ltd. and the Canadian Space Agency, which will play a multifaceted role on the Gateway, including station maintenance, inspection, and the capture of visiting vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Solutions that utilize the arm’s cameras to autonomously detect potential collision hazards, such as loose tools or debris, are of particular interest, as they would significantly reduce the risk of damage to the arm during autonomous operations. Since the type and appearance of all possible hazards cannot be known in advance, it is beneficial for the vision system to perform <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">anomaly detection</em>, recognizing any off-nominal situations that are different from expected norms.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S1.F1.g1" src="extracted/5890171/camera_setup.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Blender rendering of the ISS model used in the ALLO dataset as a surrogate for the Lunar Gateway. The Earth, the Moon, and the Sun are shown in the background; reference camera positions around the station are highlighted in orange.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Anomaly detection and localization involves identifying and pinpointing the locations of unexpected content in images that deviate from a set of expected inputs.
Algorithms for anomaly detection aim to find pixels, features, or objects in an image that differ statistically from an established baseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib4" title="">4</a>]</cite>.
Anomaly detection in the space domain is challenging due to complex lighting conditions that result from the black background coupled with harsh direct solar illumination.
Furthermore, the varied camera viewpoints used during robotic operations on a space station can cause anomalies to blend in with the background, making anomaly detection more difficult.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Although reliable anomaly detection is highly valuable to prevent the Canadarm3 from collisions, current methods do not directly address this task.
Moreover, existing space image datasets from on-orbit operations, such as those from the Canadarm2 on the ISS, are limited and insufficient for developing an anomaly detection algorithm for the Canadarm3.
Images from past missions lack ground truth reference and generally cannot be applied to operations in lunar orbit.
To bridge this gap, we create an open-source anomaly detection dataset for robotic operations on an orbital space station and use it to evaluate state-of-the-art anomaly detection algorithms. To this end, our main contributions are as follows.</p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a new and relevant visual anomaly detection task that goes beyond standard terrestrial applications.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We develop a simulator and automated data generation pipeline, which makes use of Blender’s Cycles rendering engine.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We introduce the ALLO (Anomaly Localization in Lunar Orbit) dataset, an open-source dataset for anomaly detection as part of vision-based proximity operations for space-based robotic systems. The dataset comprises 94,890 anomaly-free images and 17,527 anomalous images with pixel-level ground-truth maps.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We establish a new anomaly detection benchmark by evaluating existing anomaly detection algorithms on the ALLO dataset, and discuss how the existing methods are insufficient for the space domain.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">We validate the fidelity of the rendering process by reconstructing images captured from Earth orbit by ISS cameras and comparing them to existing images.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The remainder of this paper is structured as follows. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S2" title="II Related Work ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">II</span></a> reviews anomaly detection datasets and algorithms. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3" title="III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">III</span></a> details the simulation and rendering of the ALLO dataset. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4" title="IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">IV</span></a> evaluates state-of-the-art anomaly detection algorithms on the ALLO dataset.
The ALLO dataset and the data generation pipeline and benchmarking code are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/utiasSTARS/ALLO.git" title="">https://github.com/utiasSTARS/ALLO.git</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Although anomaly detection has not yet been applied to the space domain, extensive literature exists on vision-based navigation datasets for space exploration and, separately, on anomaly detection for industrial inspection.
In this section, we review existing visual datasets for space missions and current approaches to anomaly detection.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Datasets for Anomaly Detection and Space Operations</span>
</h3>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Space Operations</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">In space exploration, visual navigation datasets are often generated using computer rendering software.
Existing data from past missions are usually unsuitable for algorithm development since the images may be inapplicable to new missions, lack ground truth references, or have incorrect sensor configurations.
Additionally, acquiring new images may be impossible, as in the case of the Gateway, or inadequate, due to limitations regarding what can be replicated on Earth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib5" title="">5</a>]</cite>.
Therefore, rendering programs are used to make synthetic space image datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib5" title="">5</a>]</cite> as this allows an extensive range of scenarios to be realistically created.
Rendering programs use engines such as Cycles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib11" title="">11</a>]</cite> or Unreal Engine 5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib12" title="">12</a>]</cite> to accurately replicate the scenes they expect to encounter during their respective missions. Unlike existing space datasets, the ALLO dataset contains images of a space station in lunar orbit and is open source.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Industrial Defect Inspection</h5>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">Recent advances in anomaly detection have focused on finding manufacturing defects during industrial inspection. The MVTec 2D anomaly detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib13" title="">13</a>]</cite> is the most widely used benchmark for this application due to its pixel-level annotations and diverse range of objects/textures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib14" title="">14</a>]</cite>. However, like other anomaly detection datasets such as BTech <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib15" title="">15</a>]</cite> and Kolector <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib16" title="">16</a>]</cite>, MVTec features static viewpoints, consistent lighting, and simple backgrounds. In contrast, the ALLO dataset offers a more comprehensive and diverse set of scenes for space anomaly detection, including multiple views of the ISS with varied lighting conditions and backgrounds. This diversity sets ALLO apart from these other datasets, providing a broader range of scenarios for anomaly detection in space.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Anomaly Detection Methods</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Anomaly detection involves identifying abnormal samples that deviate from the expected distribution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib17" title="">17</a>]</cite>.
Generally, an anomaly can be described as an irregular or unexpected instance that deviates from an established pattern <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib18" title="">18</a>]</cite>.
Differentiating between normal and anomalous instances can be challenging due to two key issues <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib19" title="">19</a>]</cite>:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Anomaly uncertainty: it is usually not known in advance what an anomaly might look like.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Anomaly scarcity: anomalies are rare and diverse, making it difficult to identify all of them correctly.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.SS2.p1.2">To tackle these challenges, many anomaly detection algorithms have leveraged deep learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib18" title="">18</a>]</cite>.
Most methods are unsupervised because of the absence of representative anomalous data for supervision, though self-supervised methods also exist.
Unsupervised methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib22" title="">22</a>]</cite> learn from anomaly-free images and classify anomalies based on deviations from this learned distribution, while self-supervised methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib24" title="">24</a>]</cite> add synthetic anomalies to normal images during training.
Overall, learned anomaly detection algorithms fall into two categories: representation-based and reconstruction-based methods.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.5.1.1">II-B</span>1 </span>Representation-based methods</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Representation-based methods leverage embeddings from pre-trained feature extractors, such as ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib25" title="">25</a>]</cite> or ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib26" title="">26</a>]</cite>, combined with an outlier detection framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib27" title="">27</a>]</cite>.
PaDiM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib20" title="">20</a>]</cite> uses a pre-trained CNN to create a bag-of-features, reduces dimensions through random selection, fits a multivariate Gaussian distribution to each patch embedding, and detects anomalies based on the Mahalanobis distance between a test feature and the learned distribution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib27" title="">27</a>]</cite>.
Similarly, DFM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib28" title="">28</a>]</cite> employs a bag-of-features approach but uses PCA for dimensionality reduction and fits a Gaussian mixture model to normal features, identifying anomalies via log-likelihood.
Alternatively, PatchCore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib27" title="">27</a>]</cite> and CFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib22" title="">22</a>]</cite> use memory banks to store normal features, scoring anomalies based on the distance between test features and stored features.
PatchCore uses an ImageNet pre-trained network for patch-level storage, whereas CFA applies transfer learning to the target dataset to learn high-probability features.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">Some representation methods use normalizing flows that learn an invertible mapping between the input features and a known reference distribution (eg. Multivariate Gaussian distribution) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib29" title="">29</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib30" title="">30</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib31" title="">31</a>]</cite>.
For instance, CFLOW-AD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib32" title="">32</a>]</cite> uses a pre-trained feature extractor to extract feature maps from image patches, then learns a conditional normalizing flow that maps these vectors to a multivariate Gaussian distribution, calculating anomaly scores via the Mahalanobis distance.
FastFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib33" title="">33</a>]</cite> extends this idea to two dimensions to preserve spatial information, and classifies anomalies based on feature likelihood.
U-Flow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib26" title="">26</a>]</cite> employs a U-net architecture where the encoder is a feature extractor and the decoder is a normalizing flow model, and anomalies are scored based on feature likelihood and the number of false alarms.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.5.1.1">II-B</span>2 </span>Reconstruction-based methods</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Reconstruction-based anomaly detection methods use generative models to learn how to reconstruct only anomaly-free images.
These methods operate on the assumption that anomalous features will be incorrectly reconstructed as they were not seen during training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib14" title="">14</a>]</cite>.
Possible types of image generators include generative adversarial networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib34" title="">34</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib35" title="">35</a>]</cite>, auto-encoders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib36" title="">36</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib37" title="">37</a>]</cite>, and student-teacher models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib38" title="">38</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">Student-teacher methods use knowledge distillation to train a student network to reconstruct anomaly-free images.
Architectures differ between methods: STFPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib38" title="">38</a>]</cite> employs identical architectures for both the student and teacher networks, whereas Reverse Distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib21" title="">21</a>]</cite> uses a mirrored architecture, where the student network is a reversed version of the teacher network.
GAN-based reconstruction methods adversarially train an auto-encoder to reconstruct only normal images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib39" title="">39</a>]</cite>.
Ganomaly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib34" title="">34</a>]</cite> uses encoder-decoder-encoder sub-networks and classifies the latent space representation of the reconstructed image with a discriminator.
Both DRAEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib23" title="">23</a>]</cite> and DSR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib40" title="">40</a>]</cite> generate synthetic anomalies by adding noise to the normal images to enhance robustness.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Dataset Description</span>
</h2>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.1">Dataset Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.2.1">Training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.3.1">Test</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.4.1">Total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.2.2.1.1">Camera Poses</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.1.2">1-40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.1.3">41-50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.1.4">50</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.3.2.1">Normal Images</th>
<td class="ltx_td ltx_align_center" id="S3.T1.2.3.2.2">87,600</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.3.2.3">7,290</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.3.2.4">94,890</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.2.4.3.1">Anomalous Images</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.4.3.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.4.3.3">17,527</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.4.3.4">17,527</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Number of images in the ALLO dataset.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The images captured by Canadarm3 are expected to contain diverse lighting conditions resulting from the combination of the black background of space, direct sunlight, indirect reflections from the Moon or station, and the arm’s inspection lights.
Blender was chosen to create the ALLO dataset and replicate these lighting conditions because its Cycles rendering engine uses path tracing to calculate highly realistic lighting.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Blender Model Setup</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Images of the Gateway’s orbit around the Moon were created using NASA’s model of the ISS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib41" title="">41</a>]</cite> because a realistic Gateway model is not yet available.
The ISS model’s textures and structure are representative of what the Canadarm3’s cameras will capture, as the Gateway is expected to resemble the ISS in appearance.
The Gateway’s orbit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib42" title="">42</a>]</cite> was approximated in Blender with an ellipse whose dimensions matched the perilune and apolune of the station’s orbit.
Then, the positions of the Earth and Sun relative to the Moon were calculated with the Skyfield library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib43" title="">43</a>]</cite> using ephemeris data from the year 2030.
The positions of the Moon, Earth, and Sun relative to the station were simulated over 365 days, replicating the lighting and background conditions the arm’s cameras will experience during operation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In the Blender model, 50 unique camera poses were manually defined around the station to simulate key positions of the arm-mounted cameras. During the rendering process, random Gaussian noise was added to each pose, with a standard deviation of 1 meter for location and 0.2 radians in roll, pitch, and yaw. This process ensured that all relevant regions of the station body were captured and varied between views.
The station model and some of the reference camera positions are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S1.F1" title="In I Introduction ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
Furthermore, to enhance the dataset’s generalizability, several scene parameters were modified during the rendering process.
Each scene was rendered multiple times with varying sun strength intensities, which were carefully adjusted to achieve the desired effect.
Each anomalous scene was also rendered with the anomaly at three different depths: the initial randomly assigned depth, as well as 1 metre closer to and 1 metre farther from the camera, resulting in a total of nine images for each anomalous scene. Furthermore, the anomaly’s size in each scene was randomly varied to one of three scales: its original size, 20% smaller, or 20% larger, to introduce additional diversity.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> ALLO Dataset Rendering Process</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">1:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l1.2">for</span> day in ephemeris data <span class="ltx_text ltx_font_bold" id="alg1.l1.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">2:</span></span>     Position the Moon, Earth, and Sun

</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">3:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l3.2">if</span> Anomalous <span class="ltx_text ltx_font_bold" id="alg1.l3.3">then</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">4:</span></span>         CameraList = Cameras 40-49

</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">5:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l5.2">else</span> <span class="ltx_text ltx_font_bold" id="alg1.l5.3">if</span> Normal <span class="ltx_text ltx_font_bold" id="alg1.l5.4">then</span>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">6:</span></span>         CameraList = Cameras 0-39

</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l7.1.1.1" style="font-size:80%;">7:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l7.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l7.3">if</span>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l8.1.1.1" style="font-size:80%;">8:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l8.2">for</span> Camera in CameraList <span class="ltx_text ltx_font_bold" id="alg1.l8.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l9.1.1.1" style="font-size:80%;">9:</span></span>         <span class="ltx_text ltx_font_bold" id="alg1.l9.2">for</span> Set of sun light and anomaly parameters <span class="ltx_text ltx_font_bold" id="alg1.l9.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l10.1.1.1" style="font-size:80%;">10:</span></span>              Place a camera with slight pose perturbation

</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l11.1.1.1" style="font-size:80%;">11:</span></span>              Add a spotlight near the camera

</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l12.1.1.1" style="font-size:80%;">12:</span></span>              <span class="ltx_text ltx_font_bold" id="alg1.l12.2">if</span> Anomalous <span class="ltx_text ltx_font_bold" id="alg1.l12.3">then</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l13.1.1.1" style="font-size:80%;">13:</span></span>                  <span class="ltx_text ltx_font_bold" id="alg1.l13.2">while</span> Anomaly <math alttext="&lt;" class="ltx_Math" display="inline" id="alg1.l13.m1.1"><semantics id="alg1.l13.m1.1a"><mo id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.1b"><lt id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="alg1.l13.m1.1d">&lt;</annotation></semantics></math> 0.1% of Image <span class="ltx_text ltx_font_bold" id="alg1.l13.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l14.1.1.1" style="font-size:80%;">14:</span></span>                       Add anomaly inside camera frustum

</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l15.1.1.1" style="font-size:80%;">15:</span></span>                  <span class="ltx_text ltx_font_bold" id="alg1.l15.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l15.3">while</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l16.1.1.1" style="font-size:80%;">16:</span></span>                  Render three-class segmentation mask

</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l17.1.1.1" style="font-size:80%;">17:</span></span>              <span class="ltx_text ltx_font_bold" id="alg1.l17.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l17.3">if</span>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l18.1.1.1" style="font-size:80%;">18:</span></span>              Render the image using Cycles

</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l19.1.1.1" style="font-size:80%;">19:</span></span>              Add noise and glare

</div>
<div class="ltx_listingline" id="alg1.l20">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l20.1.1.1" style="font-size:80%;">20:</span></span>         <span class="ltx_text ltx_font_bold" id="alg1.l20.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l20.3">for</span>
</div>
<div class="ltx_listingline" id="alg1.l21">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l21.1.1.1" style="font-size:80%;">21:</span></span>     <span class="ltx_text ltx_font_bold" id="alg1.l21.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l21.3">for</span>
</div>
<div class="ltx_listingline" id="alg1.l22">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l22.1.1.1" style="font-size:80%;">22:</span></span><span class="ltx_text ltx_font_bold" id="alg1.l22.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l22.3">for</span>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="380" id="S3.F2.sf1.g1" src="extracted/5890171/blender_setup.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F2.sf1.3.2" style="font-size:90%;">Scene setup.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="378" id="S3.F2.sf2.g1" src="extracted/5890171/with_anomaly.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F2.sf2.3.2" style="font-size:90%;">Adding camera and anomaly.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="380" id="S3.F2.sf3.g1" src="extracted/5890171/anomaly_ex.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F2.sf3.3.2" style="font-size:90%;">Anomalous image render.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="380" id="S3.F2.sf4.g1" src="extracted/5890171/segmentation_ex.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S3.F2.sf4.3.2" style="font-size:90%;">Segmentation mask.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Rendering process for an anomalous image in the ALLO dataset. The setup of the station and celestial bodies are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.F2.sf1" title="Fig. 2(a) ‣ Fig. 2 ‣ III-A Blender Model Setup ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">2(a)</span></a>, the placement of the camera and anomalous cable are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.F2.sf2" title="Fig. 2(b) ‣ Fig. 2 ‣ III-A Blender Model Setup ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">2(b)</span></a>, and the rendered anomalous image and the corresponding segmentation mask are presented in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.F2.sf3" title="Fig. 2(c) ‣ Fig. 2 ‣ III-A Blender Model Setup ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">2(c)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.F2.sf4" title="Fig. 2(d) ‣ Fig. 2 ‣ III-A Blender Model Setup ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">2(d)</span></a> respectively.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="400" id="S3.F3.sf1.g1" src="extracted/5890171/blanket.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F3.sf1.3.2" style="font-size:90%;">Thermal blanket</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="400" id="S3.F3.sf2.g1" src="extracted/5890171/cable.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F3.sf2.3.2" style="font-size:90%;">Cable</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="400" id="S3.F3.sf3.g1" src="extracted/5890171/drill.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F3.sf3.3.2" style="font-size:90%;">Drill</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Examples of models of anomalous objects used in the ALLO dataset.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Image Rendering</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The normal and anomalous images were generated from different camera positions to ensure variation between the training and testing sets.
Models of thermal blankets, cables, and maintenance tools, such as those shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.F3" title="In III-A Blender Model Setup ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, were used as anomalies.
Each anomalous image contained only one anomaly, as performance on a single anomaly reflects how algorithms would perform on images with multiple anomalies.
For all images, a corresponding three-class segmentation mask was generated that labelled the anomaly (none if normal image), all non-anomalous foreground objects (e.g., the station, celestial bodies), and the background (e.g. space).
The rendering process for the ALLO dataset is outlined in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#alg1" title="In III-A Blender Model Setup ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">1</span></a>, with an example shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.F2" title="In III-A Blender Model Setup ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">All images in the ALLO dataset were rendered to a resolution of <math alttext="1,920\times 1,080" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.3"><semantics id="S3.SS2.p2.1.m1.3a"><mrow id="S3.SS2.p2.1.m1.3.3.1" xref="S3.SS2.p2.1.m1.3.3.2.cmml"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">1</mn><mo id="S3.SS2.p2.1.m1.3.3.1.2" xref="S3.SS2.p2.1.m1.3.3.2.cmml">,</mo><mrow id="S3.SS2.p2.1.m1.3.3.1.1" xref="S3.SS2.p2.1.m1.3.3.1.1.cmml"><mn id="S3.SS2.p2.1.m1.3.3.1.1.2" xref="S3.SS2.p2.1.m1.3.3.1.1.2.cmml">920</mn><mo id="S3.SS2.p2.1.m1.3.3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.1.m1.3.3.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.1.m1.3.3.1.1.3" xref="S3.SS2.p2.1.m1.3.3.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p2.1.m1.3.3.1.3" xref="S3.SS2.p2.1.m1.3.3.2.cmml">,</mo><mn id="S3.SS2.p2.1.m1.2.2" xref="S3.SS2.p2.1.m1.2.2.cmml">080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.3b"><list id="S3.SS2.p2.1.m1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.3.3.1"><cn id="S3.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1">1</cn><apply id="S3.SS2.p2.1.m1.3.3.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1"><times id="S3.SS2.p2.1.m1.3.3.1.1.1.cmml" xref="S3.SS2.p2.1.m1.3.3.1.1.1"></times><cn id="S3.SS2.p2.1.m1.3.3.1.1.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.3.3.1.1.2">920</cn><cn id="S3.SS2.p2.1.m1.3.3.1.1.3.cmml" type="integer" xref="S3.SS2.p2.1.m1.3.3.1.1.3">1</cn></apply><cn id="S3.SS2.p2.1.m1.2.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.2.2">080</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.3c">1,920\times 1,080</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.3d">1 , 920 × 1 , 080</annotation></semantics></math> pixels.
The rendering process was repeated with two seeds and the breakdown of images in the training and testing sets are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.T1" title="In III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a>.
Sample images from the ALLO dataset are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.F4" title="In III-B Image Rendering ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> and demonstrate how these images can be quite crowded, may contain both illuminated and shadowed structures, and have large black portions of space.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F4.1.g1" src="extracted/5890171/01047.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F4.2.g1" src="extracted/5890171/31049.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F4.3.g1" src="extracted/5890171/16495.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F4.4.g1" src="extracted/5890171/37078.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="336" id="S3.F4.5.g1" src="extracted/5890171/09541.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="380" id="S3.F4.6.g1" src="extracted/5890171/Slide1.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="380" id="S3.F4.7.g1" src="extracted/5890171/Slide2.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="380" id="S3.F4.8.g1" src="extracted/5890171/Slide3.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="380" id="S3.F4.9.g1" src="extracted/5890171/Slide4.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.10"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="380" id="S3.F4.10.g1" src="extracted/5890171/Slide5.jpg" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.12.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.13.2" style="font-size:90%;">Sample images from the ALLO dataset. Normal (anomaly-free) images are shown in the top row and anomalous images in the bottom row with the anomalies circled in red.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Dataset Validation</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We demonstrate that our synthetic images accurately represent the scenes expected to be captured by the Canadarm3 by replicating real ISS images using the Blender model.
Two pairs of replicated images are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S3.F5" title="In III-C Dataset Validation ‣ III Dataset Description ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, showing that both the lighting and textures of the scene are well captured by the Blender model.
Seven replica images were compared to real ISS images using the Structural Similarity Index Measure (SSIM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib44" title="">44</a>]</cite>.
While the average SSIM score was 0.32 due to illumination differences caused by the station’s colour, ALLO images are visually similar to real ISS images.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F5.1.g1" src="extracted/5890171/iss_img1_ref.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F5.2.g1" src="extracted/5890171/iss_img1.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F5.3.g1" src="extracted/5890171/iss_img2_ref.jpg" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F5.4.g1" src="extracted/5890171/iss_img2.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.6.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.7.2" style="font-size:90%;">Images taken by the ISS (left) and their synthetic recreations (right).</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Current approaches to anomaly detection have not yet been applied to the space domain.
Therefore, we use the ALLO dataset to evaluate how state-of-the-art anomaly detection algorithms perform on more challenging imagery.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Experimental Setup</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We used Intel’s Anomalib repository <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib45" title="">45</a>]</cite> to test state-of-the-art anomaly detection algorithms on the ALLO dataset.
Anomalib was selected because it supports a large number of algorithms and because it enables the comprehensive testing and tuning of these algorithms on our data.
We modified the Anomalib code to load and train on the ALLO dataset, and the following seven algorithms were evaluated: STFPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib38" title="">38</a>]</cite>, CFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib22" title="">22</a>]</cite>, Reverse Distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib21" title="">21</a>]</cite>, DRAEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib23" title="">23</a>]</cite>, FastFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib33" title="">33</a>]</cite>, U-Flow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib26" title="">26</a>]</cite>, and DSR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib40" title="">40</a>]</cite>.
The MVTec dataset results for the evaluated algorithms are presented in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.T3" title="In IV-D Ablation Studies ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">III</span></a>.
It should be noted that representation-based methods, which rely on discriminative image features, usually use fixed pre-trained encoders, often trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib46" title="">46</a>]</cite>, while training the remaining network layers from scratch on the target dataset.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Each algorithm was first trained with its default hyperparameters.
Then, the mean and standard deviation values of the ALLO training set were computed for image normalization unlike previous works that use ImageNet values.
Additionally, horizontal and vertical flipping, as well as random brightness and contrast augmentations, were applied randomly with a 50% probability during training.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Evaluation Metrics</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Three metrics were used to evaluate anomaly detection algorithms on the ALLO dataset: image AUROC (I.AUROC), pixel AUROC (P.AUROC), and average precision (P.AP). Image AUROC and pixel AUROC (area under receiver operating characteristic) were chosen for their effectiveness in assessing binary classifiers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib26" title="">26</a>]</cite>, but pixel AUROC can be misleading.
A high pixel AUROC often reflects a strong true negative rate, particularly for small anomalies, indicating better detection of normal pixels rather than anomalies. A low score suggests difficulty with normal pixels due to a high false positive rate, with random classifiers scoring 0.5.
Average precision (AP), which considers both precision and recall, better assesses pixel-level localization, especially for imbalanced datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib28" title="">28</a>]</cite>.
Due to the heavy class imbalance between anomalous and non-anomalous pixels in ALLO test images, the best-performing algorithm was primarily determined by the highest pixel AP. However, pixel AUROC was also given consideration, as a low score indicates a high false positive rate, which is critical in evaluating anomaly detection performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Results</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The results of all algorithms on the ALLO test set are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.T3" title="In IV-D Ablation Studies ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">III</span></a>.
Reverse Distillation (Rev. Dist.) achieved the highest pixel AP at 48.4%, followed by STFPM at 32.4% and FastFlow at 31.7%.
Compared to its performance on the MVTec dataset, Reverse Distillation’s image AUROC score decreased by 40.4%, and its pixel AUROC dropped by 46.0% on the ALLO dataset. While Reverse Distillation had the highest pixel AP score, it had a fairly low pixel AUROC score indicating a high false positive rate.
FastFlow had the highest image and pixel AUROC scores at 66.9% and 86.0% respectively.
Generally, the student-teacher methods (STFPM and Rev. Dist.) performed best, followed by the normalizing flow methods (FastFlow and UFlow), while the semi-supervised methods (DRAEM and DSR) performed worse. Example inferences are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.F6" title="In IV-D Ablation Studies ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Ablation Studies</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">While all algorithms struggled to generalize to the space domain, we improved the performance of most algorithms using dataset-specific tuning, described in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.SS1" title="IV-A Experimental Setup ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>.
The effect of data augmentation and custom normalization on FastFlow (the algorithm with the highest pixel AUROC) is shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.T2" title="In IV-D Ablation Studies ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a>.
FastFlow was able to achieve higher pixel AUROC and pixel AP scores when using custom image statistics than with ImageNet-based normalization values. This reflects how different the scenes in the ALLO dataset are to those in ImageNet and other datasets that use ImageNet normalization values.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Due to the significant performance drop on the ALLO dataset compared to the MVTec dataset, we tested the algorithms on more visually distinct anomalies. We created a smaller, secondary test set where the anomalies’ colours were altered from their default colours (which usually resemble the station) to more distinct colours (e.g. red, blue, yellow). This experiment helped to determine how much the algorithms depend on the visual characteristics of anomalies.
The results on the coloured test set are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.T4" title="In IV-D Ablation Studies ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">IV</span></a> and inference examples are visualized in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.F7" title="In IV-D Ablation Studies ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>; all algorithms showed a significant increase in performance.
UFlow achieved the highest pixel AP score of 91.7%, representing a 63.3% increase over its performance on the primary test set. FastFlow recorded the best image AUROC score at 98.8% and the highest pixel AUROC score at 99.4%, showing 31.9% and 13.4% improvements, respectively.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.4.1">Augmentations</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1">I.AUROC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.1">P.AUROC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.m1.1"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><ci id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.1">P.AP <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.1.m1.1"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><ci id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.6.6.4">MVTec</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.4.1"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.1.1">67.6 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.4.4.1.1.m1.1"><semantics id="S4.T2.4.4.1.1.m1.1a"><mo id="S4.T2.4.4.1.1.m1.1.1" xref="S4.T2.4.4.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.4.4.1.1.m1.1.1.cmml" xref="S4.T2.4.4.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.1.1.m1.1d">±</annotation></semantics></math> 0.7</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.5.2">81.5 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.5.5.2.m1.1"><semantics id="S4.T2.5.5.2.m1.1a"><mo id="S4.T2.5.5.2.m1.1.1" xref="S4.T2.5.5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.2.m1.1b"><csymbol cd="latexml" id="S4.T2.5.5.2.m1.1.1.cmml" xref="S4.T2.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.2.m1.1d">±</annotation></semantics></math> 1.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.6.6.3">21.8 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.6.6.3.m1.1"><semantics id="S4.T2.6.6.3.m1.1a"><mo id="S4.T2.6.6.3.m1.1.1" xref="S4.T2.6.6.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.3.m1.1b"><csymbol cd="latexml" id="S4.T2.6.6.3.m1.1.1.cmml" xref="S4.T2.6.6.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.3.m1.1d">±</annotation></semantics></math> 2.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T2.9.9.4">ALLO</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T2.7.7.1">66.9 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.7.7.1.m1.1"><semantics id="S4.T2.7.7.1.m1.1a"><mo id="S4.T2.7.7.1.m1.1.1" xref="S4.T2.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T2.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.1.m1.1d">±</annotation></semantics></math> 2.3</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.8.8.2"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.2.1">86.0 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.8.8.2.1.m1.1"><semantics id="S4.T2.8.8.2.1.m1.1a"><mo id="S4.T2.8.8.2.1.m1.1.1" xref="S4.T2.8.8.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.2.1.m1.1b"><csymbol cd="latexml" id="S4.T2.8.8.2.1.m1.1.1.cmml" xref="S4.T2.8.8.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.2.1.m1.1d">±</annotation></semantics></math> 2.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.9.9.3"><span class="ltx_text ltx_font_bold" id="S4.T2.9.9.3.1">31.7 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.9.9.3.1.m1.1"><semantics id="S4.T2.9.9.3.1.m1.1a"><mo id="S4.T2.9.9.3.1.m1.1.1" xref="S4.T2.9.9.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.3.1.m1.1b"><csymbol cd="latexml" id="S4.T2.9.9.3.1.m1.1.1.cmml" xref="S4.T2.9.9.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.3.1.m1.1d">±</annotation></semantics></math> 1.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.11.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.12.2" style="font-size:90%;">The effect of tuned augmentations and normalization on FastFlow’s performance on the ALLO test set.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.27">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.27.28.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.27.28.1.1">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.27.28.1.2">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.27.28.1.3">FastFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib33" title="">33</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.27.28.1.4">UFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib26" title="">26</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.27.28.1.5">CFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib22" title="">22</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.27.28.1.6">DRAEM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib23" title="">23</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.27.28.1.7">DSR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib40" title="">40</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.27.28.1.8">Rev. Dist. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib21" title="">21</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.27.28.1.9">STFPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib38" title="">38</a>]</cite>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.2" rowspan="3"><span class="ltx_text" id="S4.T3.1.1.2.1">MVTec</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.1">Image AUROC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.1.1.3.1">99.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4">98.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.1">99.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.6">98.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7">98.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.8">98.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.9">95.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.2.2.1">Pixel AUROC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.2.2.1.m1.1"><semantics id="S4.T3.2.2.1.m1.1a"><mo id="S4.T3.2.2.1.m1.1.1" stretchy="false" xref="S4.T3.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><ci id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.2.2.2.1">98.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1">98.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.2.2.4.1">98.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.5">97.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.7">97.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.8">97.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.3.3.1">Pixel AP <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.3.3.1.m1.1"><semantics id="S4.T3.3.3.1.m1.1a"><mo id="S4.T3.3.3.1.m1.1.1" stretchy="false" xref="S4.T3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.1.m1.1b"><ci id="S4.T3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.3.3.5.1">68.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.6"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.6.1">70.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.7">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.8">-</td>
</tr>
<tr class="ltx_tr" id="S4.T3.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T3.11.11.9" rowspan="3"><span class="ltx_text" id="S4.T3.11.11.9.1">ALLO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.4.4.1">Image AUROC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.4.4.1.m1.1"><semantics id="S4.T3.4.4.1.m1.1a"><mo id="S4.T3.4.4.1.m1.1.1" stretchy="false" xref="S4.T3.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.1.m1.1b"><ci id="S4.T3.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.2.1">66.9 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.5.5.2.1.m1.1"><semantics id="S4.T3.5.5.2.1.m1.1a"><mo id="S4.T3.5.5.2.1.m1.1.1" xref="S4.T3.5.5.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.2.1.m1.1b"><csymbol cd="latexml" id="S4.T3.5.5.2.1.m1.1.1.cmml" xref="S4.T3.5.5.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.2.1.m1.1d">±</annotation></semantics></math> 2.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.6.6.3.1">65.5 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.6.6.3.1.m1.1"><semantics id="S4.T3.6.6.3.1.m1.1a"><mo id="S4.T3.6.6.3.1.m1.1.1" xref="S4.T3.6.6.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.3.1.m1.1b"><csymbol cd="latexml" id="S4.T3.6.6.3.1.m1.1.1.cmml" xref="S4.T3.6.6.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.3.1.m1.1d">±</annotation></semantics></math> 1.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.7.4">58.6 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.7.7.4.m1.1"><semantics id="S4.T3.7.7.4.m1.1a"><mo id="S4.T3.7.7.4.m1.1.1" xref="S4.T3.7.7.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.4.m1.1b"><csymbol cd="latexml" id="S4.T3.7.7.4.m1.1.1.cmml" xref="S4.T3.7.7.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.4.m1.1d">±</annotation></semantics></math> 0.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.8.5">55.9 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.8.8.5.m1.1"><semantics id="S4.T3.8.8.5.m1.1a"><mo id="S4.T3.8.8.5.m1.1.1" xref="S4.T3.8.8.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.5.m1.1b"><csymbol cd="latexml" id="S4.T3.8.8.5.m1.1.1.cmml" xref="S4.T3.8.8.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.8.5.m1.1d">±</annotation></semantics></math> 3.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.9.6">53.9 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.9.9.6.m1.1"><semantics id="S4.T3.9.9.6.m1.1a"><mo id="S4.T3.9.9.6.m1.1.1" xref="S4.T3.9.9.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.6.m1.1b"><csymbol cd="latexml" id="S4.T3.9.9.6.m1.1.1.cmml" xref="S4.T3.9.9.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.9.6.m1.1d">±</annotation></semantics></math> 2.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.10.7">58.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.10.10.7.m1.1"><semantics id="S4.T3.10.10.7.m1.1a"><mo id="S4.T3.10.10.7.m1.1.1" xref="S4.T3.10.10.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.7.m1.1b"><csymbol cd="latexml" id="S4.T3.10.10.7.m1.1.1.cmml" xref="S4.T3.10.10.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.10.7.m1.1d">±</annotation></semantics></math> 0.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.11.11.8">63.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.11.11.8.m1.1"><semantics id="S4.T3.11.11.8.m1.1a"><mo id="S4.T3.11.11.8.m1.1.1" xref="S4.T3.11.11.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.8.m1.1b"><csymbol cd="latexml" id="S4.T3.11.11.8.m1.1.1.cmml" xref="S4.T3.11.11.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.8.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.11.8.m1.1d">±</annotation></semantics></math> 0.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.12.12.1">Pixel AUROC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.12.12.1.m1.1"><semantics id="S4.T3.12.12.1.m1.1a"><mo id="S4.T3.12.12.1.m1.1.1" stretchy="false" xref="S4.T3.12.12.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.1.m1.1b"><ci id="S4.T3.12.12.1.m1.1.1.cmml" xref="S4.T3.12.12.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.12.12.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.13.13.2"><span class="ltx_text ltx_font_bold" id="S4.T3.13.13.2.1">86.0 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.13.13.2.1.m1.1"><semantics id="S4.T3.13.13.2.1.m1.1a"><mo id="S4.T3.13.13.2.1.m1.1.1" xref="S4.T3.13.13.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.13.2.1.m1.1b"><csymbol cd="latexml" id="S4.T3.13.13.2.1.m1.1.1.cmml" xref="S4.T3.13.13.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.13.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.13.2.1.m1.1d">±</annotation></semantics></math> 2.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.14.14.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.14.14.3.1">85.6 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.14.14.3.1.m1.1"><semantics id="S4.T3.14.14.3.1.m1.1a"><mo id="S4.T3.14.14.3.1.m1.1.1" xref="S4.T3.14.14.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.14.3.1.m1.1b"><csymbol cd="latexml" id="S4.T3.14.14.3.1.m1.1.1.cmml" xref="S4.T3.14.14.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.14.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.14.14.3.1.m1.1d">±</annotation></semantics></math> 0.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.15.15.4">85.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.15.15.4.m1.1"><semantics id="S4.T3.15.15.4.m1.1a"><mo id="S4.T3.15.15.4.m1.1.1" xref="S4.T3.15.15.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.15.15.4.m1.1b"><csymbol cd="latexml" id="S4.T3.15.15.4.m1.1.1.cmml" xref="S4.T3.15.15.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.15.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.15.15.4.m1.1d">±</annotation></semantics></math> 0.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.16.16.5">67.9 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.16.16.5.m1.1"><semantics id="S4.T3.16.16.5.m1.1a"><mo id="S4.T3.16.16.5.m1.1.1" xref="S4.T3.16.16.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.16.5.m1.1b"><csymbol cd="latexml" id="S4.T3.16.16.5.m1.1.1.cmml" xref="S4.T3.16.16.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.16.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.16.16.5.m1.1d">±</annotation></semantics></math> 1.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.17.17.6">55.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.17.17.6.m1.1"><semantics id="S4.T3.17.17.6.m1.1a"><mo id="S4.T3.17.17.6.m1.1.1" xref="S4.T3.17.17.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.17.6.m1.1b"><csymbol cd="latexml" id="S4.T3.17.17.6.m1.1.1.cmml" xref="S4.T3.17.17.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.17.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.17.17.6.m1.1d">±</annotation></semantics></math> 1.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.18.18.7">51.8 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.18.18.7.m1.1"><semantics id="S4.T3.18.18.7.m1.1a"><mo id="S4.T3.18.18.7.m1.1.1" xref="S4.T3.18.18.7.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.18.7.m1.1b"><csymbol cd="latexml" id="S4.T3.18.18.7.m1.1.1.cmml" xref="S4.T3.18.18.7.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.18.7.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.18.18.7.m1.1d">±</annotation></semantics></math> 0.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.19.19.8">58.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.19.19.8.m1.1"><semantics id="S4.T3.19.19.8.m1.1a"><mo id="S4.T3.19.19.8.m1.1.1" xref="S4.T3.19.19.8.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.19.19.8.m1.1b"><csymbol cd="latexml" id="S4.T3.19.19.8.m1.1.1.cmml" xref="S4.T3.19.19.8.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.19.19.8.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.19.19.8.m1.1d">±</annotation></semantics></math> 6.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.27.27">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T3.20.20.1">Pixel AP <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.20.20.1.m1.1"><semantics id="S4.T3.20.20.1.m1.1a"><mo id="S4.T3.20.20.1.m1.1.1" stretchy="false" xref="S4.T3.20.20.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.20.20.1.m1.1b"><ci id="S4.T3.20.20.1.m1.1.1.cmml" xref="S4.T3.20.20.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.20.20.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.20.20.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.21.21.2">31.7 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.21.21.2.m1.1"><semantics id="S4.T3.21.21.2.m1.1a"><mo id="S4.T3.21.21.2.m1.1.1" xref="S4.T3.21.21.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.21.21.2.m1.1b"><csymbol cd="latexml" id="S4.T3.21.21.2.m1.1.1.cmml" xref="S4.T3.21.21.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.21.21.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.21.21.2.m1.1d">±</annotation></semantics></math> 1.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.22.22.3">28.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.22.22.3.m1.1"><semantics id="S4.T3.22.22.3.m1.1a"><mo id="S4.T3.22.22.3.m1.1.1" xref="S4.T3.22.22.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.22.22.3.m1.1b"><csymbol cd="latexml" id="S4.T3.22.22.3.m1.1.1.cmml" xref="S4.T3.22.22.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.22.22.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.22.22.3.m1.1d">±</annotation></semantics></math> 1.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.23.23.4">16.3 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.23.23.4.m1.1"><semantics id="S4.T3.23.23.4.m1.1a"><mo id="S4.T3.23.23.4.m1.1.1" xref="S4.T3.23.23.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.23.23.4.m1.1b"><csymbol cd="latexml" id="S4.T3.23.23.4.m1.1.1.cmml" xref="S4.T3.23.23.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.23.23.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.23.23.4.m1.1d">±</annotation></semantics></math> 1.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.24.24.5">9.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.24.24.5.m1.1"><semantics id="S4.T3.24.24.5.m1.1a"><mo id="S4.T3.24.24.5.m1.1.1" xref="S4.T3.24.24.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.24.24.5.m1.1b"><csymbol cd="latexml" id="S4.T3.24.24.5.m1.1.1.cmml" xref="S4.T3.24.24.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.24.24.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.24.24.5.m1.1d">±</annotation></semantics></math> 2.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.25.25.6">26.9 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.25.25.6.m1.1"><semantics id="S4.T3.25.25.6.m1.1a"><mo id="S4.T3.25.25.6.m1.1.1" xref="S4.T3.25.25.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.25.25.6.m1.1b"><csymbol cd="latexml" id="S4.T3.25.25.6.m1.1.1.cmml" xref="S4.T3.25.25.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.25.25.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.25.25.6.m1.1d">±</annotation></semantics></math> 3.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.26.26.7"><span class="ltx_text ltx_font_bold" id="S4.T3.26.26.7.1">48.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.26.26.7.1.m1.1"><semantics id="S4.T3.26.26.7.1.m1.1a"><mo id="S4.T3.26.26.7.1.m1.1.1" xref="S4.T3.26.26.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.26.26.7.1.m1.1b"><csymbol cd="latexml" id="S4.T3.26.26.7.1.m1.1.1.cmml" xref="S4.T3.26.26.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.26.26.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.26.26.7.1.m1.1d">±</annotation></semantics></math> 0.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.27.27.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.27.27.8.1">32.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.27.27.8.1.m1.1"><semantics id="S4.T3.27.27.8.1.m1.1a"><mo id="S4.T3.27.27.8.1.m1.1.1" xref="S4.T3.27.27.8.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T3.27.27.8.1.m1.1b"><csymbol cd="latexml" id="S4.T3.27.27.8.1.m1.1.1.cmml" xref="S4.T3.27.27.8.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.27.27.8.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T3.27.27.8.1.m1.1d">±</annotation></semantics></math> 12.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.29.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S4.T3.30.2" style="font-size:90%;">Performance of state-of-the-art anomaly detection algorithms on the ALLO and MVTEC test sets. For each metric, the algorithm that performed best and second-best on that dataset are bolded and underlined, respectively.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.15">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.4.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1">I.AUROC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1">P.AUROC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.1.m1.1a"><mo id="S4.T4.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T4.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><ci id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.3.1">P.AP <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.m1.1"><semantics id="S4.T4.3.3.3.1.m1.1a"><mo id="S4.T4.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T4.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><ci id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.6.6.4">FastFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib33" title="">33</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.1"><span class="ltx_text ltx_font_bold" id="S4.T4.4.4.1.1">98.8 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.4.4.1.1.m1.1"><semantics id="S4.T4.4.4.1.1.m1.1a"><mo id="S4.T4.4.4.1.1.m1.1.1" xref="S4.T4.4.4.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.1.1.m1.1b"><csymbol cd="latexml" id="S4.T4.4.4.1.1.m1.1.1.cmml" xref="S4.T4.4.4.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.1.1.m1.1d">±</annotation></semantics></math> 0.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.5.2"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.2.1">99.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.5.5.2.1.m1.1"><semantics id="S4.T4.5.5.2.1.m1.1a"><mo id="S4.T4.5.5.2.1.m1.1.1" xref="S4.T4.5.5.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.2.1.m1.1b"><csymbol cd="latexml" id="S4.T4.5.5.2.1.m1.1.1.cmml" xref="S4.T4.5.5.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.2.1.m1.1d">±</annotation></semantics></math> 0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.6.6.3.1">87.9 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.6.6.3.1.m1.1"><semantics id="S4.T4.6.6.3.1.m1.1a"><mo id="S4.T4.6.6.3.1.m1.1.1" xref="S4.T4.6.6.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.3.1.m1.1b"><csymbol cd="latexml" id="S4.T4.6.6.3.1.m1.1.1.cmml" xref="S4.T4.6.6.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.3.1.m1.1d">±</annotation></semantics></math> 0.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.9.9.4">UFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib26" title="">26</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T4.7.7.1"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.7.7.1.1">97.3 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.7.7.1.1.m1.1"><semantics id="S4.T4.7.7.1.1.m1.1a"><mo id="S4.T4.7.7.1.1.m1.1.1" xref="S4.T4.7.7.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.1.1.m1.1b"><csymbol cd="latexml" id="S4.T4.7.7.1.1.m1.1.1.cmml" xref="S4.T4.7.7.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.1.1.m1.1d">±</annotation></semantics></math> 1.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T4.8.8.2.1">98.8 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.8.8.2.1.m1.1"><semantics id="S4.T4.8.8.2.1.m1.1a"><mo id="S4.T4.8.8.2.1.m1.1.1" xref="S4.T4.8.8.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.2.1.m1.1b"><csymbol cd="latexml" id="S4.T4.8.8.2.1.m1.1.1.cmml" xref="S4.T4.8.8.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.8.2.1.m1.1d">±</annotation></semantics></math> 0.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.9.9.3"><span class="ltx_text ltx_font_bold" id="S4.T4.9.9.3.1">91.7 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.9.9.3.1.m1.1"><semantics id="S4.T4.9.9.3.1.m1.1a"><mo id="S4.T4.9.9.3.1.m1.1.1" xref="S4.T4.9.9.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.3.1.m1.1b"><csymbol cd="latexml" id="S4.T4.9.9.3.1.m1.1.1.cmml" xref="S4.T4.9.9.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.9.3.1.m1.1d">±</annotation></semantics></math> 0.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.12.12.4">Rev. Dist. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib21" title="">21</a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.1">94.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.10.10.1.m1.1"><semantics id="S4.T4.10.10.1.m1.1a"><mo id="S4.T4.10.10.1.m1.1.1" xref="S4.T4.10.10.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.1.m1.1b"><csymbol cd="latexml" id="S4.T4.10.10.1.m1.1.1.cmml" xref="S4.T4.10.10.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.10.1.m1.1d">±</annotation></semantics></math> 0.6</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.11.2">96.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.11.11.2.m1.1"><semantics id="S4.T4.11.11.2.m1.1a"><mo id="S4.T4.11.11.2.m1.1.1" xref="S4.T4.11.11.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.2.m1.1b"><csymbol cd="latexml" id="S4.T4.11.11.2.m1.1.1.cmml" xref="S4.T4.11.11.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.11.11.2.m1.1d">±</annotation></semantics></math> 0.1</td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.3">74.3 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.12.12.3.m1.1"><semantics id="S4.T4.12.12.3.m1.1a"><mo id="S4.T4.12.12.3.m1.1.1" xref="S4.T4.12.12.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.3.m1.1b"><csymbol cd="latexml" id="S4.T4.12.12.3.m1.1.1.cmml" xref="S4.T4.12.12.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.12.12.3.m1.1d">±</annotation></semantics></math> 0.9</td>
</tr>
<tr class="ltx_tr" id="S4.T4.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T4.15.15.4">STFPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib38" title="">38</a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.13.13.1">95.3 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.13.13.1.m1.1"><semantics id="S4.T4.13.13.1.m1.1a"><mo id="S4.T4.13.13.1.m1.1.1" xref="S4.T4.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T4.13.13.1.m1.1.1.cmml" xref="S4.T4.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.13.13.1.m1.1d">±</annotation></semantics></math> 1.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.14.14.2">93.4 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.14.14.2.m1.1"><semantics id="S4.T4.14.14.2.m1.1a"><mo id="S4.T4.14.14.2.m1.1.1" xref="S4.T4.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T4.14.14.2.m1.1.1.cmml" xref="S4.T4.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.14.14.2.m1.1d">±</annotation></semantics></math> 2.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.15.15.3">81.8 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.15.15.3.m1.1"><semantics id="S4.T4.15.15.3.m1.1a"><mo id="S4.T4.15.15.3.m1.1.1" xref="S4.T4.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.3.m1.1b"><csymbol cd="latexml" id="S4.T4.15.15.3.m1.1.1.cmml" xref="S4.T4.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.15.15.3.m1.1d">±</annotation></semantics></math> 2.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.17.1.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="S4.T4.18.2" style="font-size:90%;">Performance of best four anomaly detection algorithms from the benchmark on the secondary colour test set. For each metric, the algorithm that performed best and second-best are bolded and underlined, respectively.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.1">
<figcaption class="ltx_caption">Input Image</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="336" id="S4.F6.1.g1" src="extracted/5890171/110_cable2_10_0.5_0_default.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.2">
<figcaption class="ltx_caption">Ground Truth</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F6.2.g1" src="extracted/5890171/110_cable2_10_0.5_0_default_mask.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.3">
<figcaption class="ltx_caption">DRAEM Mask</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F6.3.g1" src="extracted/5890171/110_cable2_10_0.5_0_default_draem.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.4">
<figcaption class="ltx_caption">Rev. Dist. Mask</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F6.4.g1" src="extracted/5890171/110_cable2_10_0.5_0_default_revdist.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.5">
<figcaption class="ltx_caption">STFPM Mask</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F6.5.g1" src="extracted/5890171/110_cable2_10_0.5_0_default_stfpm.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="336" id="S4.F6.6.g1" src="extracted/5890171/170_ingenuity_10_0.5_0_default.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F6.7.g1" src="extracted/5890171/170_ingenuity_10_0.5_0_default_mask.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F6.8.g1" src="extracted/5890171/170_ingenuity_10_0.5_0_default_draem.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F6.9.g1" src="extracted/5890171/170_ingenuity_10_0.5_0_default_revdist.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.10"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F6.10.g1" src="extracted/5890171/170_ingenuity_10_0.5_0_default_stfpm.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.12.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.13.2" style="font-size:90%;">Example predictions from anomaly detection algorithms.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.1">
<figcaption class="ltx_caption">Input Image</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="338" id="S4.F7.1.g1" src="extracted/5890171/img7_input.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.2">
<figcaption class="ltx_caption">Ground Truth</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F7.2.g1" src="extracted/5890171/img7_gt.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.3">
<figcaption class="ltx_caption">FastFlow Mask</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F7.3.g1" src="extracted/5890171/img7_fastflow.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.4">
<figcaption class="ltx_caption">UFlow Mask</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F7.4.g1" src="extracted/5890171/img7_uflow.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.5">
<figcaption class="ltx_caption">STFPM Mask</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F7.5.g1" src="extracted/5890171/img7_stfpm.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="338" id="S4.F7.6.g1" src="extracted/5890171/img6_input.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F7.7.g1" src="extracted/5890171/img6_gt.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F7.8.g1" src="extracted/5890171/img6_fastflow.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F7.9.g1" src="extracted/5890171/img6_uflow.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F7.10"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S4.F7.10.g1" src="extracted/5890171/img6_stfpm.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.12.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.13.2" style="font-size:90%;">Example inference from anomaly detection algorithms on colourful anomalies.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.5.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.6.2">Discussion</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Our results in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.SS3" title="IV-C Results ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a> show that existing anomaly detection algorithms are ill-suited for the space domain.
They assume commonly seen features such as those found in ImageNet, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib27" title="">27</a>]</cite> which are not found in ALLO.
Without adapting to the ALLO dataset, the feature extraction networks pre-trained on ImageNet extract non-discriminative feature representations, which hinders the ability of anomaly detection algorithms to effectively learn the dataset’s distribution</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Some algorithms assume features will follow a uni-modal distribution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib20" title="">20</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib33" title="">33</a>]</cite> of anomaly-free features.
This assumption can be problematic for diverse datasets where anomalies have subtle features close to the normal distribution or when the normal data is multi-modal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib23" title="">23</a>]</cite>.
The handling of a more complex distribution is why the normalizing flow method FastFlow detects anomalies well without introducing too many false positives.
As shown by the examples in <a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#S4.F6" title="In IV-D Ablation Studies ‣ IV Experiments ‣ ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly\sssDetection\sssDuring\sssRobotic\sssProximity\sssOperations\sssin\sssLunar\sssOrbit"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, even the best-performing algorithms struggled to find anomalies whose colour resembled that of the station.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">The most significant limitation of existing anomaly detection algorithms is their assumption of image consistency.
The methods evaluated in this benchmark were developed for applications in which images are taken under consistent, monochrome lighting from the same viewpoint (e.g. industrial defect inspection, medical imaging).
This means that the anomaly score may be incorrect if there is a misalignment between the normal training images and the testing image <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.20435v1#bib.bib47" title="">47</a>]</cite>.
In the space domain, lighting varies greatly based on the position of the Moon, Earth, and Sun, and the camera viewpoints in the ALLO dataset differ due to the variety of operations conducted by Canadarm3.
These variations make anomaly detection in the space domain a challenging problem and existing algorithms cannot be directly applied to this task with much success.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we addressed the task of visual anomaly detection and localization for a space station in lunar orbit, a previously unexplored area that is extremely valuable for autonomous space operations.
The varying lighting conditions and complex scene geometry of space imagery must be addressed before an algorithm can be deployed on a space station.
To tackle these challenges, we introduce the ALLO dataset and an automated data generation pipeline, marking the first open-source anomaly detection dataset featuring images from lunar orbit.
We evaluate state-of-the-art methods on this dataset, establishing a new benchmark to guide future research.
We highlight that the similarity between normal and anomalous features in the space domain presents a major challenge for current anomaly detection methods.
As autonomy becomes increasingly important in future missions, we aim for this dataset and data generation pipeline to be foundational in developing robust anomaly detection algorithms for space exploration.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. C. Crusan, R. M. Smith, D. A. Craig, J. M. Caram, J. Guidi, M. Gates, J. M.
Krezel, and N. B. Herrmann, “Deep space gateway concept: Extending human
presence into cislunar space,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2018 IEEE Aerospace Conference</em>,
pp. 1–10. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8396541" title="">https://ieeexplore.ieee.org/document/8396541</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
NASA’s gateway program - NASA. Section: Humans in Space. [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nasa.gov/reference/nasas-gateway-program/" title="">https://www.nasa.gov/reference/nasas-gateway-program/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
(2020, June) About Canadarm3. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.asc-csa.gc.ca/eng/canadarm3/about.asp" title="">https://www.asc-csa.gc.ca/eng/canadarm3/about.asp</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. Ehret, A. Davy, J.-M. Morel, and M. Delbracio, “Image anomalies: a review
and synthesis of detection methods,” vol. 61, no. 5, pp. 710–743. [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1808.02564" title="">http://arxiv.org/abs/1808.02564</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Pajusalu, I. Iakubivskyi, G. J. Schwarzkopf, O. Knuuttila, T. Väisänen,
M. Bührer, H. Teras, G. L. Bonhomme, M. F. Palos, J. Praks, and
A. Slavinskis, “SISPO: Space imaging simulator for proximity operations,”
vol. 17, no. 3, p. e0263882. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2105.06771" title="">http://arxiv.org/abs/2105.06771</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Parkes, I. Martin, M. Dunstan, and D. Matthews, “Planet surface simulation
with PANGU.”

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L. Bingham, J. Kincaid, B. Weno, N. Davis, E. Paddock, and C. Foreman,
“Digital lunar exploration sites unreal simulation tool (DUST),” in
<em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2023 IEEE Aerospace Conference</em>.   IEEE, pp. 1–12. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/10115607/" title="">https://ieeexplore.ieee.org/document/10115607/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
E. Z. Crues, P. Bielski, E. Paddock, C. Foreman, B. Bell, C. Raymond, T. Hunt,
and D. Bulikhov, “Approaches for validation of lighting environments in
realtime lunar south pole simulations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">2023 IEEE Aerospace
Conference</em>, pp. 1–18, ISSN: 1095-323X.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. Brochard, J. Lebreton, C. Robin, K. Kanani, G. Jonniaux, A. Masson,
N. Despré, and A. Berjaoui, “Scientific image rendering for space scenes
with the SurRender software.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1810.01423" title="">http://arxiv.org/abs/1810.01423</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. T. Eapen, R. R. Bhaskara, and M. Majji, “NaRPA: Navigation and rendering
pipeline for astronautics.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2211.01566" title="">http://arxiv.org/abs/2211.01566</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B. Foundation. blender.org - home of the blender project - free and open 3d
creation software. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.blender.org/" title="">https://www.blender.org/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Unreal engine 5. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.unrealengine.com/en-US/unreal-engine-5" title="">https://www.unrealengine.com/en-US/unreal-engine-5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, “MVTec AD — a
comprehensive real-world dataset for unsupervised anomaly detection,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR)</em>, pp. 9584–9592, ISSN: 2575-7075. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8954181" title="">https://ieeexplore.ieee.org/document/8954181</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y. Zheng, X. Wang, Y. Qi, W. Li, and L. Wu, “Benchmarking unsupervised anomaly
detection and localization,” publication Title: arXiv e-prints ADS
Bibcode: 2022arXiv220514852Z. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ui.adsabs.harvard.edu/abs/2022arXiv220514852Z" title="">https://ui.adsabs.harvard.edu/abs/2022arXiv220514852Z</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
MyNew, “Btech dataset,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://universe.roboflow.com/mynew/btech" title="">https://universe.roboflow.com/mynew/btech</a>,
feb 2024, visited on 2024-09-11. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://universe.roboflow.com/mynew/btech" title="">https://universe.roboflow.com/mynew/btech</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. Tabernik, S. Šela, J. Skvarč, and D. Skočaj,
“Segmentation-Based Deep-Learning Approach for Surface-Defect Detection,”
<em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Journal of Intelligent Manufacturing</em>, May 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. G. Mehrotra, C. K. Mohan, and H. Huang, <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Anomaly Detection Principles
and Algorithms</em>, ser. Terrorism, Security, and Computation.   Springer International Publishing. [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://link.springer.com/10.1007/978-3-319-67526-8" title="">http://link.springer.com/10.1007/978-3-319-67526-8</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T. M. Tran, T. N. Vu, N. D. Vo, T. V. Nguyen, and K. Nguyen, “Anomaly analysis
in images and videos: A comprehensive review,” vol. 55, no. 7, pp.
148:1–148:37. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3544014" title="">https://dl.acm.org/doi/10.1145/3544014</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
G. Pang, C. Shen, L. Cao, and A. v. d. Hengel, “Deep learning for anomaly
detection: A review,” vol. 54, no. 2, pp. 1–38. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2007.02500" title="">http://arxiv.org/abs/2007.02500</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T. Defard, A. Setkov, A. Loesch, and R. Audigier, “PaDiM: a patch
distribution modeling framework for anomaly detection and localization.”
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2011.08785" title="">http://arxiv.org/abs/2011.08785</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H. Deng and X. Li, “Anomaly detection via reverse distillation from one-class
embedding.” [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2201.10703" title="">http://arxiv.org/abs/2201.10703</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Lee, S. Lee, and B. C. Song, “CFA: Coupled-hypersphere-based feature
adaptation for target-oriented anomaly localization.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2206.04325" title="">http://arxiv.org/abs/2206.04325</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V. Zavrtanik, M. Kristan, and D. Skočaj, “DRAEM – a discriminatively
trained reconstruction embedding for surface anomaly detection.” [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2108.07610" title="">http://arxiv.org/abs/2108.07610</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
C.-L. Li, K. Sohn, J. Yoon, and T. Pfister, “CutPaste: Self-supervised
learning for anomaly detection and localization.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2104.04015" title="">http://arxiv.org/abs/2104.04015</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Tailanian, P. Muse, and A. Pardo, “A multi-scale a contrario method for
unsupervised image anomaly detection.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2110.02407" title="">http://arxiv.org/abs/2110.02407</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Tailanian, A. Pardo, and P. Muse, “U-flow: A u-shaped normalizing flow for
anomaly detection with unsupervised threshold,” vol. 66, no. 4, pp.
678–696. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s10851-024-01193-y" title="">https://doi.org/10.1007/s10851-024-01193-y</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K. Roth, L. Pemula, J. Zepeda, B. Schölkopf, T. Brox, and P. Gehler, “Towards
total recall in industrial anomaly detection.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2106.08265" title="">http://arxiv.org/abs/2106.08265</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
N. A. Ahuja, I. Ndiour, T. Kalyanpur, and O. Tickoo, “Probabilistic modeling
of deep features for out-of-distribution and adversarial detection.”
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.11786" title="">http://arxiv.org/abs/1909.11786</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
E. D. Cook, M.-A. Lavoie, and S. L. Waslander, “Feature density estimation for
out-of-distribution detection via normalizing flows,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of
the Conference on Robots and Vision</em>, February 2024. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://crv.pubpub.org/pub/3rxx60zs/release/1" title="">https://crv.pubpub.org/pub/3rxx60zs/release/1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
N. Marchal, C. Moraldo, H. Blum, R. Siegwart, C. Cadena, and A. Gawel,
“Learning densities in feature space for reliable segmentation of indoor
scenes,” vol. 5, no. 2, pp. 1032–1038, conference Name: IEEE Robotics and
Automation Letters. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8962043" title="">https://ieeexplore.ieee.org/document/8962043</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
H. Zhang, Z. Wang, Z. Wu, and Y.-G. Jiang, “DiffusionAD: Norm-guided
one-step denoising diffusion for anomaly detection.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.08730" title="">http://arxiv.org/abs/2303.08730</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
D. Gudovskiy, S. Ishizaka, and K. Kozuka, “CFLOW-AD: Real-time
unsupervised anomaly detection with localization via conditional normalizing
flows.” [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2107.12571" title="">http://arxiv.org/abs/2107.12571</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Yu, Y. Zheng, X. Wang, W. Li, Y. Wu, R. Zhao, and L. Wu, “FastFlow:
Unsupervised anomaly detection and localization via 2d normalizing flows.”
[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2111.07677" title="">http://arxiv.org/abs/2111.07677</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon, “GANomaly:
Semi-supervised anomaly detection via adversarial training.” [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1805.06725" title="">http://arxiv.org/abs/1805.06725</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T. Schlegl, P. Seeböck, S. Waldstein, G. Langs, and U. Schmidt-Erfurth,
“f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial
networks,” vol. 54.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A.-S. Collin and C. De Vleeschouwer, “Improved anomaly detection by training
an autoencoder with skip connections on images corrupted with stain-shaped
noise,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">2020 25th International Conference on Pattern Recognition
(ICPR)</em>, pp. 7915–7922, ISSN: 1051-4651. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/abstract/document/9412842" title="">https://ieeexplore.ieee.org/abstract/document/9412842</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Bauer, S. Nakajima, and K.-R. Müller, “Self-supervised training with
autoencoders for visual anomaly detection.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2206.11723" title="">http://arxiv.org/abs/2206.11723</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
G. Wang, S. Han, E. Ding, and D. Huang, “Student-teacher feature pyramid
matching for anomaly detection.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2103.04257" title="">http://arxiv.org/abs/2103.04257</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
M. Sabokrou, M. Khalooei, M. Fathy, and E. Adeli, “Adversarially learned
one-class classifier for novelty detection.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1802.09088" title="">http://arxiv.org/abs/1802.09088</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
V. Zavrtanik, M. Kristan, and D. Skočaj, “DSR – a dual subspace
re-projection network for surface anomaly detection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Computer
Vision – ECCV 2022</em>, ser. Lecture Notes in Computer Science, S. Avidan,
G. Brostow, M. Cissé, G. M. Farinella, and T. Hassner, Eds.   Springer Nature Switzerland, pp. 539–554.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
International space station 3d model - NASA science. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://science.nasa.gov/resource/international-space-station-3d-model/" title="">https://science.nasa.gov/resource/international-space-station-3d-model/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
D. E. Lee, “White paper: Gateway destination orbit model: A continuous 15 year
NRHO reference trajectory,” NTRS Author Affiliations: NASA Johnson
Space Center NTRS Report/Patent Number: JSC-E-DAA-TN72594 NTRS
Document ID: 20190030294 NTRS Research Center: Johnson Space Center
(JSC). [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ntrs.nasa.gov/citations/20190030294" title="">https://ntrs.nasa.gov/citations/20190030294</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
ASCL.net - skyfield: High precision research-grade positions for planets and
earth satellites generator. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ascl.net/1907.024" title="">https://ascl.net/1907.024</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, “Image quality assessment:
from error visibility to structural similarity,” vol. 13, no. 4, pp.
600–612, conference Name: IEEE Transactions on Image Processing. [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/1284395" title="">https://ieeexplore.ieee.org/document/1284395</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
S. Akcay, D. Ameln, A. Vaidya, B. Lakshmanan, N. Ahuja, and U. Genc,
“Anomalib: A deep learning library for anomaly detection.” [Online].
Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2202.08341" title="">http://arxiv.org/abs/2202.08341</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification with
deep convolutional neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Advances in Neural Information
Processing Systems</em>, vol. 25.   Curran
Associates, Inc., 2012. [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html" title="">https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J. Jang, E. Hwang, and S.-H. Park, “N-pad : Neighboring pixel-based industrial
anomaly detection.” [Online]. Available:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2210.08768" title="">http://arxiv.org/abs/2210.08768</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 14:43:24 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
