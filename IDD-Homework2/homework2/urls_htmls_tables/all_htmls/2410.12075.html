<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation</title>
<!--Generated on Tue Oct 15 21:28:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.12075v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S1" title="In WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S2" title="In WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3" title="In WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Proposed Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS1" title="In III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS2" title="In III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">SD Fine-tuning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS3" title="In III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Procedural Prompt Generation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS3.SSS0.Px1" title="In III-C Procedural Prompt Generation ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title">Instance sampler</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS3.SSS0.Px2" title="In III-C Procedural Prompt Generation ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title">Scene composer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS3.SSS0.Px3" title="In III-C Procedural Prompt Generation ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title">Scene descripitor</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS3.SSS0.Px4" title="In III-C Procedural Prompt Generation ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title">Procedural generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS4" title="In III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Sample Generation &amp; Model training</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S4" title="In WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S4.SS1" title="In IV Experiments ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S4.SS2" title="In IV Experiments ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Implementation details</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5" title="In WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Evaluation and Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.SS1" title="In V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Comparison with State-of-the-art </span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.SS2" title="In V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Influence of UDA methods for training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.SS3" title="In V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Influence of SD Fine-tuning.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.SS4" title="In V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Influence of Procedural Prompt Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.SS5" title="In V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">Influence of Numbers of Generated Images</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S6" title="In WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document"> WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Chenghao Qian<sup class="ltx_sup" id="id4.4.id1">∗</sup>,
Yuhu Guo<sup class="ltx_sup" id="id5.5.id2">∗</sup>,
Yuhong Mo,
Wenjing Li
</span><span class="ltx_author_notes">C. Qian, and W. Li are with the Transport Studies at Institute at University of Leeds, UKY. Guo and Y. Mo are with Department of Electrical and Computer Engineering, Carnegie Mellon University, USA<sup class="ltx_sup" id="id6.6.id1">∗</sup> denotes equal contribution</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">In this work, we propose a novel approach, namely WeatherDG, that can generate realistic, weather-diverse, and driving-screen images based on the cooperation of two foundation models, i.e, Stable Diffusion (SD) and Large Language Model (LLM). Specifically, we first fine-tune the SD with source data, aligning the content and layout of generated samples with real-world driving scenarios. Then, we propose a procedural prompt generation method based on LLM, which can enrich scenario descriptions and help SD automatically generate more diverse, detailed images. In addition, we introduce a balanced generation strategy, which encourages the SD to generate high-quality objects of tailed classes under various weather conditions, such as riders and motorcycles. This segmentation-model-agnostic method can improve the generalization ability of existing models by additionally adapting them with the generated synthetic data. Experiments on three challenging datasets show that our method can significantly improve the segmentation performance of different state-of-the-art models on target domains. Notably, in the setting of ”Cityscapes to ACDC”, our method improves the baseline HRDA by 13.9% in mIoU. See the project page for more results:<a class="ltx_ref ltx_href" href="https://jumponthemoon.github.io/WeatherDG.github.io/" title="">weatherDG.github.io.</a></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Semantic segmentation is a fundamental task in autonomous driving. Despite significant achievements in this field, existing models still face serious challenges when deploying in unseen domains due to the well-known domain shift problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib1" title="">1</a>]</cite>. In addition, this issue will be more serious when the unseen domains are with adverse weather conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib3" title="">3</a>]</cite>, such as foggy, rainy, snowy, and nighttime scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">One naive way to solve the above problem is collecting more diverse training data. However, labelling segmentation is a time-consuming process, as we need to annotate every pixel in an image. Hence, domain generalization becomes popular in solving the domain shift problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib5" title="">5</a>]</cite>, in which the goal is to train a model that can generalize to unseen domains using only the given source data. Existing domain generalization methods can be generally divided into two categories: normalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib7" title="">7</a>]</cite> and data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib9" title="">9</a>]</cite>. In this paper, we focus on the latter category, which is more flexible to different model structures and can be easily integrated with the former techniques.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="S1.F1.g1" src="extracted/5929420/pics/figure_1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="font-size:90%;">Visualization of domain-generalized semantic segmentation results: MIC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib10" title="">10</a>]</cite> (red box) vs. WeatherDG (green box).<span class="ltx_text ltx_font_medium" id="S1.F1.4.2.1"> The tested images include foggy, nighttime, snowy, and rainy scenarios.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Data augmentation for domain generalization aims to generate new, diverse and realistic synthetic images for augmenting the training data. Previous methods commonly adopt simulators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib13" title="">13</a>]</cite> or image translation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib15" title="">15</a>]</cite> to generate new samples. Despite their effectiveness, these methods still suffer from diversity and authenticity, particularly in generating samples with adverse weather conditions (shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S2.F2" title="In II Related work ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>). In recent years, Stable Diffusion (SD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib16" title="">16</a>]</cite> has shown its strong ability to generate realistic, diverse, and high-quality images. This inspires us to leverage SD to solve the drawbacks of previous data augmentation methods in domain generalization. However, directly applying SD in our task will lead to a critical issue: the styles and layouts are very different from the driving-screen samples (see <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S2.F2" title="In II Related work ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S1.p3.1.1" style="color:#0000FF;">b</span>)). Hence, training with such synthetic samples will hamper the performance of the model on unseen domains. This problem is mainly caused by that the SD was trained with various types of samples, such as natural images and artistic images, instead of specifically with the driving-screen samples. As such, the SD cannot well generate on-the-hand driving-screen-aware samples without a detailed and specific guide.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To solve the above drawback, we propose a novel data augmentation approach named WeatherDG to generate realistic, weather-diverse, and driving-screen images based on Stable Diffusion (SD) and Large Language Model (LLM). Our method is composed of three steps. <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Step-I: SD Fine-tuning</span>. We fine-tune the SD with source data. This enables us to align the content and layout of generated samples with real-world driving scenarios. <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">Step-II: Procedural Prompt Generation</span>. We propose a prompt generation method based on LLM, where we leverage the LLM agents to procedurally enrich scenario descriptions (prompt). The generated prompt can help SD to automatically generate more diverse, detailed images. In addition, we introduce a balanced generation strategy to enrich tailed classes. <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">Step-III: Sample Generation and model training.</span> Given the fine-tuned SD and the generated prompts, we can then generate new, diverse samples for model training. The generated samples are used to train the model together with the source data. In sum, our contributions are threefold:</p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a novel data augmentation framework based on SD and LLM for domain generalization in adverse weather conditions. Our method can generate realistic, diverse samples for improving the model’s generalization ability in unseen domains.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose two novel strategies for prompt generation and sample generation, which encourage SD to generate diverse and driving-screen samples that are beneficial to our segmentation task.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our method is segmentation-model-agnostic. Experiments on three challenging datasets demonstrate that our method can consistently improve the performance of state-of-the-art methods.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related work</span>
</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Domain Generalization for Semantic Segmentation (DGSS)</span>. DGSS aims to train deep neural networks that perform well on semantic segmentation tasks across multiple unseen domains. Existing DGSS methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib5" title="">5</a>]</cite> attempt to address the domain gap problem through two main approaches: normalization and data augmentation. Normalization-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib7" title="">7</a>]</cite> train by normalizing the mean and standard deviation of source features or whitening the covariance of these features. Data augmentation-based method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib5" title="">5</a>]</cite> transform source images into randomly stylized versions, guiding the model to capture domain-invariant shape features as texture cues are replaced with random styles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib19" title="">19</a>]</cite>. For instance, SHADE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib20" title="">20</a>]</cite> creates new styles derived from the foundational styles of the source domain, while MoDify <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib5" title="">5</a>]</cite> utilizes difficulty-aware photometric augmentations.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">With the advent of generative diffusion models, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib9" title="">9</a>]</cite> have proposed content augmentation to enhance generalization. However, these approaches often either lack realism or fail to adequately consider variations in weather and lighting conditions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Unsupervised Domain Adaptation (UDA).</span> UDA aims to boosts model performance on domain-specific data without needing labeled examples. Existing UDA techniques can be categorized into three main approaches: discrepancy minimization, adversarial training, and self-training. Discrepancy minimization reduces the differences between domains by using statistical distance functions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib21" title="">21</a>]</cite>. Adversarial training involves a domain discriminator within a GAN framework to encourage domain-invariant input <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib22" title="">22</a>]</cite>, feature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib23" title="">23</a>]</cite> or output <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib24" title="">24</a>]</cite>. Self-training generates pseudo-labels for the target domain based on predictions made using confidence thresholds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib25" title="">25</a>]</cite> or pseudo-label prototypes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib26" title="">26</a>]</cite>.
Recently, DATUM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib27" title="">27</a>]</cite> introduces a one-shot domain adaptation method that generates a dataset using a single image from the target domain and pairs it with unsupervised domain adaptation training methods to bridge the sim-to-real gap. Futher, PODA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib28" title="">28</a>]</cite> leveraged the capabilities of the CLIP model to enable zero-shot domain adaptation using prompts.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="423" id="S2.F2.g1" src="extracted/5929420/pics/figure_2.jpg" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F2.4.2" style="font-size:90%;">Comparison between synthetic and real-world images under adverse weather conditions.<span class="ltx_text ltx_font_medium" id="S2.F2.4.2.1"> The results reveal that images generated by (a) driving simulator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib29" title="">29</a>]</cite> lack intricate details and natural lighting, whereas (b) Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib16" title="">16</a>]</cite> typically produces images with an artistic flair. In contrast, (c) our method produces the most realistic images, closely resembling (d) diverse real-world driving scenes.</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Text-based Image Synthesis.</span> The current text-to-image task is predominantly driven by diffusion-based and LLM-oriented methods. Diffusion models, a breakthrough in producing photorealistic images, prompting studies to explore their use in enriching source domain datasets and improving semantic segmentation. For example, DIDEX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib8" title="">8</a>]</cite> employs ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib30" title="">30</a>]</cite> to convert synthetic images into real-world styles. Nevertheless, this method often lacks realism and replicates the spatial layout of the training data, limiting the diversity.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">On the other hand, large language models also play a crucial role. CuPL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib31" title="">31</a>]</cite> leverages GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib32" title="">32</a>]</cite> to generate text descriptions that enhancing zero-shot image classification. CLOUDS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib9" title="">9</a>]</cite> uses Llama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib33" title="">33</a>]</cite> to create prompts for diffusion models.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">However, they fail to adequately consider the complexities introduced by varying weather and lighting conditions. In contrast, our approach employs a chain of LLMs acting as agents to not only craft detailed descriptions of complex real-world scenarios but also implement a tailored generation strategy. This ensures that the generated images are both diverse and realistic, and address the class imbalance problem in challenging conditions.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="172" id="S2.F3.g1" src="extracted/5929420/pics/figure_3.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F3.4.2" style="font-size:90%;">The overview of WeatherDG pipeline.<span class="ltx_text ltx_font_medium" id="S2.F3.4.2.1"> (a) We first fine-tune a text-to-image diffusion model to integrate scene priors from the source domain. This ensures the images generated by the diffusion model accurately depict driving scenes. (b) Next, we employ a chain of LLM agents to procedurally construct detailed prompts that can enrich tailed classes and generate diverse weather and lighting effects with the fine-tuned model. (c) After generating images with these prompts, we utilize UDA techniques to train these images with the source domain dataset, followed by evaluation on real-world target datasets.</span></span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Proposed Method</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Overview</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">WeatherDG aims to generate images tailored to weather-specific autonomous driving scenes, enhancing semantic segmentation in challenging conditions. We begin by fine-tuning a diffusion model to adapt scene priors from the source domain, ensuring the generated images are within a driving scene (<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS2" title="III-B SD Fine-tuning ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>). Next, we employ a procedural prompt generation method to create detailed prompts that enable the diffusion model to produce realistic and diverse weather and lighting effects (<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS3" title="III-C Procedural Prompt Generation ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>). Additionally, we incorporate a probability-oriented sampling strategy for prompt generation. Following this, we use UDA training methods to leverage these generated images for semantic segmentation training (<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.SS4" title="III-D Sample Generation &amp; Model training ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>). The overview of the proposed approach is shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S2.F3" title="In II Related work ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">SD Fine-tuning</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Recently, diffusion models have advanced the field of generative domain adaptation, showcasing exceptional capabilities in producing photo-realistic images conditioned on text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib30" title="">30</a>]</cite>. However, directly applying diffusion models in autonomous driving setting presents challenges due to shifts in scene priors like style and layout. For example, the model often generates artistic images or adopt a bird’s eye view perspective, which is different from the images in real-world autonomous driving datasets, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ III-B SD Fine-tuning ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">4(b)</span></a>. When these images are incorporated during training, they can disrupt the knowledge the model has acquired from the labeled source domain, thereby harming the semantic segmentation performance.
To address these issues, we finetune a diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib16" title="">16</a>]</cite> to generate diverse images that retain content and layouts relevant to source domain. The input consists of a clean image paired with a corresponding prompt text, while the output is an image tailored to the autonomous driving domain.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.3">We follow similar text-to-image diffusion model training procedures described in the relevant work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib27" title="">27</a>]</cite>. Specifically, we use a unique identifier in the prompt to link priors to one single image choosen from the source domain dataset. For instance, the prompt “A photo of <math alttext="V^{*}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">V</mi><mo id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑉</ci><times id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">V^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_V start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> driving scene” associates with image patches cropped from the selected image, where <math alttext="V^{*}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><msup id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">V</mi><mo id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑉</ci><times id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">V^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_V start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> identifies the scene, and “driving scene” describes it broadly. This prevents language drift and improves model performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib34" title="">34</a>]</cite>. The training procedure is presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.F4.sf1" title="In Figure 4 ‣ III-B SD Fine-tuning ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a>. After training, the model captures scene priors through the unique identifier <math alttext="V_{*}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">V</mi><mo id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">∗</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝑉</ci><times id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">V_{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">italic_V start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT</annotation></semantics></math> and can generate new instances in similar contexts within autonomous driving datasets, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ III-B SD Fine-tuning ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">4(b)</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="279" id="S3.F4.sf1.g1" src="extracted/5929420/pics/figure_4a.jpg" width="596"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf1.4.2.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F4.sf1.2.1" style="font-size:90%;">Training stage: A unique identifier, <math alttext="V^{*}" class="ltx_Math" display="inline" id="S3.F4.sf1.2.1.m1.1"><semantics id="S3.F4.sf1.2.1.m1.1b"><msup id="S3.F4.sf1.2.1.m1.1.1" xref="S3.F4.sf1.2.1.m1.1.1.cmml"><mi id="S3.F4.sf1.2.1.m1.1.1.2" xref="S3.F4.sf1.2.1.m1.1.1.2.cmml">V</mi><mo id="S3.F4.sf1.2.1.m1.1.1.3" xref="S3.F4.sf1.2.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.F4.sf1.2.1.m1.1c"><apply id="S3.F4.sf1.2.1.m1.1.1.cmml" xref="S3.F4.sf1.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F4.sf1.2.1.m1.1.1.1.cmml" xref="S3.F4.sf1.2.1.m1.1.1">superscript</csymbol><ci id="S3.F4.sf1.2.1.m1.1.1.2.cmml" xref="S3.F4.sf1.2.1.m1.1.1.2">𝑉</ci><times id="S3.F4.sf1.2.1.m1.1.1.3.cmml" xref="S3.F4.sf1.2.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.sf1.2.1.m1.1d">V^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.F4.sf1.2.1.m1.1e">italic_V start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math>, links prompts to specific image patches from the dataset, following related work to maintain language-scene association and prevent drift.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="267" id="S3.F4.sf2.g1" src="extracted/5929420/pics/figure_4b.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf2.4.2.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F4.sf2.2.1" style="font-size:90%;">Inference stage: After training, the model uses the identifier <math alttext="V^{*}" class="ltx_Math" display="inline" id="S3.F4.sf2.2.1.m1.1"><semantics id="S3.F4.sf2.2.1.m1.1b"><msup id="S3.F4.sf2.2.1.m1.1.1" xref="S3.F4.sf2.2.1.m1.1.1.cmml"><mi id="S3.F4.sf2.2.1.m1.1.1.2" xref="S3.F4.sf2.2.1.m1.1.1.2.cmml">V</mi><mo id="S3.F4.sf2.2.1.m1.1.1.3" xref="S3.F4.sf2.2.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.F4.sf2.2.1.m1.1c"><apply id="S3.F4.sf2.2.1.m1.1.1.cmml" xref="S3.F4.sf2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F4.sf2.2.1.m1.1.1.1.cmml" xref="S3.F4.sf2.2.1.m1.1.1">superscript</csymbol><ci id="S3.F4.sf2.2.1.m1.1.1.2.cmml" xref="S3.F4.sf2.2.1.m1.1.1.2">𝑉</ci><times id="S3.F4.sf2.2.1.m1.1.1.3.cmml" xref="S3.F4.sf2.2.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.sf2.2.1.m1.1d">V^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.F4.sf2.2.1.m1.1e">italic_V start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> to generate new instances in autonomous driving contexts, ensuring coherent scene synthesis.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">The detailed process of Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib16" title="">16</a>]</cite> fine-tuning.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Procedural Prompt Generation</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">To enable the diffusion model to generate weather and lighting effects, it is essential to integrate specific weather conditions and times of day into the prompt. However, simplistic templates such as “A photo of [CLS], [WEATHER], [TIME]” often do not yield diversity and details, necessitating the need for more nuanced descriptions. Manually writing these descriptions is labor-intensive; therefore, we consider adopting LLM models to automate this process. Additionally, given the scarcity of dynamic object samples in adverse weather conditions, we need to consider a balanced generation strategy during the prompt generation to enrich these objects. To this end, our requirements for the generated prompts are threefold: 1) it should incorporate a balanced generation strategy, 2) introduce different weather and lighting conditions in an even manner, and 3) provide detailed descriptions of these conditions. Most importantly, all of the prompts are automatically generated by an LLM model to reduce human effort. During the implementation, we found that directly giving a single instruction for one LLM model fails to meet all of our requirements. Specifically, the generated prompts frequently do not align with our designed generation strategy or achieve the level of detailed description we expect. To address this, we develop a procedural prompt generation method involving a sequence of three LLM agents—namely, instance sampler, scene composer, and scene descriptor. This hierarchical approach enables us to generate precisely tailored text prompts for image generation, ensuring each aspect of the prompt aligns with our intended outcomes.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Instance sampler</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">During the inference phase, we can generate a specific instance by simply adding the instance name &lt;CLS &gt;to the prompt, which serves as the task for the instance sampler. However, we notice that the semantic segmentation evaluation metrics for “thing” classes are significantly lower compared to “stuff” classes in most autonomous driving datasets. This disparity can be attributed to the class imbalance problem where “stuff” classes have more occurances in the images than “thing” classes. In particular, this issue is exacerbated under adverse weather conditions , as dynamic objects are less frequently present on the road in snowy or nightime scenes.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p2.1">Therefore, we aim to enhance the instance sampler agent with the capability to employ a probability-oriented sampling strategy, giving underrepresented classes under adverse weathers higher sampling probabilities. This increases the likelihood of these rare classes presented in the generated images. In detail, we first compute the semantic label distribution of i-th thing classes <math alttext="E_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p2.1.m1.1a"><msub id="S3.SS3.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml">E</mi><mi id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.2">𝐸</ci><ci id="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.1.m1.1c">E_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.1.m1.1d">italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in a typical adverse weather dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib35" title="">35</a>]</cite> by:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E_{i}=\frac{D_{\text{i}}}{D_{\text{thing}}}," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">E</mi><mi id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">D</mi><mtext id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3a.cmml">i</mtext></msub><msub id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml">D</mi><mtext id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3a.cmml">thing</mtext></msub></mfrac></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">𝐸</ci><ci id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3"></divide><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">𝐷</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3"><mtext id="S3.E1.m1.1.1.1.1.3.2.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.2.3">i</mtext></ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">𝐷</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3"><mtext id="S3.E1.m1.1.1.1.1.3.3.3.cmml" mathsize="70%" xref="S3.E1.m1.1.1.1.1.3.3.3">thing</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">E_{i}=\frac{D_{\text{i}}}{D_{\text{thing}}},</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG italic_D start_POSTSUBSCRIPT i end_POSTSUBSCRIPT end_ARG start_ARG italic_D start_POSTSUBSCRIPT thing end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p2.4">where <math alttext="D_{\text{i}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.2.m1.1"><semantics id="S3.SS3.SSS0.Px1.p2.2.m1.1a"><msub id="S3.SS3.SSS0.Px1.p2.2.m1.1.1" xref="S3.SS3.SSS0.Px1.p2.2.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.2.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p2.2.m1.1.1.2.cmml">D</mi><mtext id="S3.SS3.SSS0.Px1.p2.2.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p2.2.m1.1.1.3a.cmml">i</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.2.m1.1b"><apply id="S3.SS3.SSS0.Px1.p2.2.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.2.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.2.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m1.1.1.2">𝐷</ci><ci id="S3.SS3.SSS0.Px1.p2.2.m1.1.1.3a.cmml" xref="S3.SS3.SSS0.Px1.p2.2.m1.1.1.3"><mtext id="S3.SS3.SSS0.Px1.p2.2.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS0.Px1.p2.2.m1.1.1.3">i</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.2.m1.1c">D_{\text{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.2.m1.1d">italic_D start_POSTSUBSCRIPT i end_POSTSUBSCRIPT</annotation></semantics></math> represents number of semantics labels of each thing class,<math alttext="D_{\text{ thing}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.3.m2.1"><semantics id="S3.SS3.SSS0.Px1.p2.3.m2.1a"><msub id="S3.SS3.SSS0.Px1.p2.3.m2.1.1" xref="S3.SS3.SSS0.Px1.p2.3.m2.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.3.m2.1.1.2" xref="S3.SS3.SSS0.Px1.p2.3.m2.1.1.2.cmml">D</mi><mtext id="S3.SS3.SSS0.Px1.p2.3.m2.1.1.3" xref="S3.SS3.SSS0.Px1.p2.3.m2.1.1.3a.cmml"> thing</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.3.m2.1b"><apply id="S3.SS3.SSS0.Px1.p2.3.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.3.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.3.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m2.1.1.2">𝐷</ci><ci id="S3.SS3.SSS0.Px1.p2.3.m2.1.1.3a.cmml" xref="S3.SS3.SSS0.Px1.p2.3.m2.1.1.3"><mtext id="S3.SS3.SSS0.Px1.p2.3.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS0.Px1.p2.3.m2.1.1.3"> thing</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.3.m2.1c">D_{\text{ thing}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.3.m2.1d">italic_D start_POSTSUBSCRIPT thing end_POSTSUBSCRIPT</annotation></semantics></math> stands for number of all thing classes. Then the sampling probability <math alttext="P_{\text{i}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p2.4.m3.1"><semantics id="S3.SS3.SSS0.Px1.p2.4.m3.1a"><msub id="S3.SS3.SSS0.Px1.p2.4.m3.1.1" xref="S3.SS3.SSS0.Px1.p2.4.m3.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p2.4.m3.1.1.2" xref="S3.SS3.SSS0.Px1.p2.4.m3.1.1.2.cmml">P</mi><mtext id="S3.SS3.SSS0.Px1.p2.4.m3.1.1.3" xref="S3.SS3.SSS0.Px1.p2.4.m3.1.1.3a.cmml">i</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p2.4.m3.1b"><apply id="S3.SS3.SSS0.Px1.p2.4.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p2.4.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p2.4.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m3.1.1.2">𝑃</ci><ci id="S3.SS3.SSS0.Px1.p2.4.m3.1.1.3a.cmml" xref="S3.SS3.SSS0.Px1.p2.4.m3.1.1.3"><mtext id="S3.SS3.SSS0.Px1.p2.4.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS0.Px1.p2.4.m3.1.1.3">i</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p2.4.m3.1c">P_{\text{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p2.4.m3.1d">italic_P start_POSTSUBSCRIPT i end_POSTSUBSCRIPT</annotation></semantics></math> can be formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P_{i}=\frac{1}{\sum_{j=1}^{n}E_{j}}\times\frac{1}{E_{i}}." class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.cmml">P</mi><mi id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mfrac id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml"><mn id="S3.E2.m1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.3.2.2.cmml">1</mn><mrow id="S3.E2.m1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.3.2.3.cmml"><msubsup id="S3.E2.m1.1.1.1.1.3.2.3.1" xref="S3.E2.m1.1.1.1.1.3.2.3.1.cmml"><mo id="S3.E2.m1.1.1.1.1.3.2.3.1.2.2" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.1.1.3.2.3.1.2.3" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.2" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.2.cmml">j</mi><mo id="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.1" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.3" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.1.3.2.3.1.3" xref="S3.E2.m1.1.1.1.1.3.2.3.1.3.cmml">n</mi></msubsup><msub id="S3.E2.m1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.1.1.3.2.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2.3.2.2" xref="S3.E2.m1.1.1.1.1.3.2.3.2.2.cmml">E</mi><mi id="S3.E2.m1.1.1.1.1.3.2.3.2.3" xref="S3.E2.m1.1.1.1.1.3.2.3.2.3.cmml">j</mi></msub></mrow></mfrac><mo id="S3.E2.m1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.1.1.1.1.3.1.cmml">×</mo><mfrac id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mn id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">1</mn><msub id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.3.2.cmml">E</mi><mi id="S3.E2.m1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.3.cmml">i</mi></msub></mfrac></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" lspace="0em" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"></eq><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2">𝑃</ci><ci id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1"></times><apply id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2"><divide id="S3.E2.m1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2"></divide><cn id="S3.E2.m1.1.1.1.1.3.2.2.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.3.2.2">1</cn><apply id="S3.E2.m1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3"><apply id="S3.E2.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.3.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.3.2.3.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.3.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.3.2.3.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.3"><eq id="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.2">𝑗</ci><cn id="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.3.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.3.2.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.3.2.3.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.1.3">𝑛</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.2.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.2.2">𝐸</ci><ci id="S3.E2.m1.1.1.1.1.3.2.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.2.3.2.3">𝑗</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><divide id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3"></divide><cn id="S3.E2.m1.1.1.1.1.3.3.2.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.3.3.2">1</cn><apply id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.2">𝐸</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">P_{i}=\frac{1}{\sum_{j=1}^{n}E_{j}}\times\frac{1}{E_{i}}.</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_E start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG × divide start_ARG 1 end_ARG start_ARG italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p2.5">Then, we allow the instance sampler agent to query the sampling probability to generate prompts. In this way, we can alleviate the shortage of instances that rarely appear in adverse weather conditions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Scene composer</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">Intuitively, to enable the generation of images covering a wide range of weather effects and different times of the day, we can design the prompt template as ‘A photo of &lt;CLS &gt;, &lt;WEATHER &gt;, &lt;TIME &gt;’ to describe the image to be generated. Here, &lt;CLS &gt; refers to the classes in the typical autonomous driving dataset, &lt;WEATHER &gt; specifies the weather conditions, and &lt;TIME &gt; indicates the time of the day. To enhance the balance and diversity of the generated dataset, we include three common weather conditions: snowy, rainy, and foggy, along with two distinct times of day: daytime and nighttime. Each condition and time period is equally represented in the dataset to ensure comprehensive coverage and variability.
However, merely incorporating categories of weather and time category does not guarantee the desired diversity in the effects, it often generates a singular subject with minimal environmental context and a limited range of effects in relatively simple scenes. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S3.F5" title="Figure 5 ‣ Scene descripitor ‣ III-C Procedural Prompt Generation ‣ III Proposed Method ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>, with the prompt “A photo of motorcycle, rainy, daytime” generated by <math alttext="\mathcal{E}_{\mathit{SC}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">ℰ</mi><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">𝑆𝐶</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2">ℰ</ci><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3">𝑆𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">\mathcal{E}_{\mathit{SC}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.1.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT</annotation></semantics></math>, the model adds reflections on the road in daytime lighting, indicating a humid, rainy day, but the effect is subtle and the scene details are limited. Although the result does not fully meet expectations, it provides a solid foundation for further improvement.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Scene descripitor</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">To ensure the generated images reflect a variety of environmental effects and capture intricate details that resemble real-world scenes, we found that providing detailed descriptions of weather conditions and lighting is useful. For example, for the prompt “A photo of a motorcycle in rainy, daytime”, we enhance the “rainy” aspect by including details such as “under a grey and overcast sky with raindrops on the pavement”. Similarly, for “daytime”, we add specifics like “streetlights casting a warm glow in the late morning rain”. Using the crafted prompt, the model incorporates elements like buildings, streetlights, a reflective road, and a misty sky into the scene, significantly enhancing the complexity and realism of the generated images. This enables the model trained with these images to generalize better to real-world scenarios.
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure ltx_align_center" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="432" id="S3.F5.1.g1" src="extracted/5929420/pics/figure_5.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.9.4.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.7.3" style="font-size:90%;">The process of prompt generation by gradually introducing LLM agents: <math alttext="\mathcal{E}_{\mathit{IS}}" class="ltx_Math" display="inline" id="S3.F5.5.1.m1.1"><semantics id="S3.F5.5.1.m1.1b"><msub id="S3.F5.5.1.m1.1.1" xref="S3.F5.5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.F5.5.1.m1.1.1.2" xref="S3.F5.5.1.m1.1.1.2.cmml">ℰ</mi><mi id="S3.F5.5.1.m1.1.1.3" xref="S3.F5.5.1.m1.1.1.3.cmml">𝐼𝑆</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F5.5.1.m1.1c"><apply id="S3.F5.5.1.m1.1.1.cmml" xref="S3.F5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F5.5.1.m1.1.1.1.cmml" xref="S3.F5.5.1.m1.1.1">subscript</csymbol><ci id="S3.F5.5.1.m1.1.1.2.cmml" xref="S3.F5.5.1.m1.1.1.2">ℰ</ci><ci id="S3.F5.5.1.m1.1.1.3.cmml" xref="S3.F5.5.1.m1.1.1.3">𝐼𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.5.1.m1.1d">\mathcal{E}_{\mathit{IS}}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.5.1.m1.1e">caligraphic_E start_POSTSUBSCRIPT italic_IS end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathcal{E}_{\mathit{SC}}" class="ltx_Math" display="inline" id="S3.F5.6.2.m2.1"><semantics id="S3.F5.6.2.m2.1b"><msub id="S3.F5.6.2.m2.1.1" xref="S3.F5.6.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.F5.6.2.m2.1.1.2" xref="S3.F5.6.2.m2.1.1.2.cmml">ℰ</mi><mi id="S3.F5.6.2.m2.1.1.3" xref="S3.F5.6.2.m2.1.1.3.cmml">𝑆𝐶</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F5.6.2.m2.1c"><apply id="S3.F5.6.2.m2.1.1.cmml" xref="S3.F5.6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F5.6.2.m2.1.1.1.cmml" xref="S3.F5.6.2.m2.1.1">subscript</csymbol><ci id="S3.F5.6.2.m2.1.1.2.cmml" xref="S3.F5.6.2.m2.1.1.2">ℰ</ci><ci id="S3.F5.6.2.m2.1.1.3.cmml" xref="S3.F5.6.2.m2.1.1.3">𝑆𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.6.2.m2.1d">\mathcal{E}_{\mathit{SC}}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.6.2.m2.1e">caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{E}_{\mathit{SD}}" class="ltx_Math" display="inline" id="S3.F5.7.3.m3.1"><semantics id="S3.F5.7.3.m3.1b"><msub id="S3.F5.7.3.m3.1.1" xref="S3.F5.7.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.F5.7.3.m3.1.1.2" xref="S3.F5.7.3.m3.1.1.2.cmml">ℰ</mi><mi id="S3.F5.7.3.m3.1.1.3" xref="S3.F5.7.3.m3.1.1.3.cmml">𝑆𝐷</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F5.7.3.m3.1c"><apply id="S3.F5.7.3.m3.1.1.cmml" xref="S3.F5.7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F5.7.3.m3.1.1.1.cmml" xref="S3.F5.7.3.m3.1.1">subscript</csymbol><ci id="S3.F5.7.3.m3.1.1.2.cmml" xref="S3.F5.7.3.m3.1.1.2">ℰ</ci><ci id="S3.F5.7.3.m3.1.1.3.cmml" xref="S3.F5.7.3.m3.1.1.3">𝑆𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.7.3.m3.1d">\mathcal{E}_{\mathit{SD}}</annotation><annotation encoding="application/x-llamapun" id="S3.F5.7.3.m3.1e">caligraphic_E start_POSTSUBSCRIPT italic_SD end_POSTSUBSCRIPT</annotation></semantics></math>. The images correspond to the results generated using prompts created by different LLM agents.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p2.1">However, manually crafting these detailed descriptions can be labor-intensive. By using LLM, we can automatically generate these nuanced elements, making the creation of highly detailed scenes more efficient. Consequently, we incorporate another LLM agent that receives prompts produced by <math alttext="\mathcal{E}_{\mathit{SC}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p2.1.m1.1"><semantics id="S3.SS3.SSS0.Px3.p2.1.m1.1a"><msub id="S3.SS3.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS3.SSS0.Px3.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px3.p2.1.m1.1.1.2" xref="S3.SS3.SSS0.Px3.p2.1.m1.1.1.2.cmml">ℰ</mi><mi id="S3.SS3.SSS0.Px3.p2.1.m1.1.1.3" xref="S3.SS3.SSS0.Px3.p2.1.m1.1.1.3.cmml">𝑆𝐶</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p2.1.m1.1b"><apply id="S3.SS3.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p2.1.m1.1.1.2">ℰ</ci><ci id="S3.SS3.SSS0.Px3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p2.1.m1.1.1.3">𝑆𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p2.1.m1.1c">\mathcal{E}_{\mathit{SC}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p2.1.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT</annotation></semantics></math> and uses the provided information to generate various detailed descriptions of scenes. In this way, we can enable a diversity of weather and lighting effects generation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Procedural generation</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px4.p1.6">The procedural prompt generation pipeline is as follows: first, the instance sampler <math alttext="\mathcal{E}_{\mathit{IS}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px4.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.2.cmml">ℰ</mi><mi id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.3.cmml">𝐼𝑆</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.2">ℰ</ci><ci id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.3">𝐼𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.1.m1.1c">\mathcal{E}_{\mathit{IS}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.1.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_IS end_POSTSUBSCRIPT</annotation></semantics></math> queries the sampling probability to select the class to be generated. Next, based on the instance sampler’s output, the scene composer <math alttext="\mathcal{E}_{\mathit{SC}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px4.p1.2.m2.1a"><msub id="S3.SS3.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.2" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.2.cmml">ℰ</mi><mi id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.3" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.3.cmml">𝑆𝐶</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.2.m2.1b"><apply id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.2">ℰ</ci><ci id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.3">𝑆𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.2.m2.1c">\mathcal{E}_{\mathit{SC}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.2.m2.1d">caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT</annotation></semantics></math> composes scenes in the prompt that include typical weather and times of the day. Finally, after receiving the prompt generated by <math alttext="\mathcal{E}_{\mathit{SC}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px4.p1.3.m3.1a"><msub id="S3.SS3.SSS0.Px4.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px4.p1.3.m3.1.1.2" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1.2.cmml">ℰ</mi><mi id="S3.SS3.SSS0.Px4.p1.3.m3.1.1.3" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1.3.cmml">𝑆𝐶</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.3.m3.1b"><apply id="S3.SS3.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1.2">ℰ</ci><ci id="S3.SS3.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1.3">𝑆𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.3.m3.1c">\mathcal{E}_{\mathit{SC}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.3.m3.1d">caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT</annotation></semantics></math>, the scene descriptor <math alttext="\mathcal{E}_{\mathit{SD}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.4.m4.1"><semantics id="S3.SS3.SSS0.Px4.p1.4.m4.1a"><msub id="S3.SS3.SSS0.Px4.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px4.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px4.p1.4.m4.1.1.2" xref="S3.SS3.SSS0.Px4.p1.4.m4.1.1.2.cmml">ℰ</mi><mi id="S3.SS3.SSS0.Px4.p1.4.m4.1.1.3" xref="S3.SS3.SSS0.Px4.p1.4.m4.1.1.3.cmml">𝑆𝐷</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.4.m4.1b"><apply id="S3.SS3.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS0.Px4.p1.4.m4.1.1.2">ℰ</ci><ci id="S3.SS3.SSS0.Px4.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS0.Px4.p1.4.m4.1.1.3">𝑆𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.4.m4.1c">\mathcal{E}_{\mathit{SD}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.4.m4.1d">caligraphic_E start_POSTSUBSCRIPT italic_SD end_POSTSUBSCRIPT</annotation></semantics></math> creates detailed descriptions of the corresponding weather and lighting conditions. Taking the probability of the instance sampling as <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.5.m5.1"><semantics id="S3.SS3.SSS0.Px4.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px4.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px4.p1.5.m5.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.5.m5.1b"><ci id="S3.SS3.SSS0.Px4.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.5.m5.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.5.m5.1c">\mathcal{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.5.m5.1d">caligraphic_P</annotation></semantics></math> , the process of generating text prompt <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.6.m6.1"><semantics id="S3.SS3.SSS0.Px4.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px4.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px4.p1.6.m6.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.6.m6.1b"><ci id="S3.SS3.SSS0.Px4.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.6.m6.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.6.m6.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.6.m6.1d">caligraphic_T</annotation></semantics></math> can be formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{T}=\mathcal{E}_{\mathit{SD}}\left(\mathcal{E}_{\mathit{SC}}\left(%
\mathcal{E}_{\mathit{IS}}(\mathcal{P})\right)\right)." class="ltx_Math" display="block" id="S3.Ex1.m1.2"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.2.1" xref="S3.Ex1.m1.2.2.1.1.cmml"><mrow id="S3.Ex1.m1.2.2.1.1" xref="S3.Ex1.m1.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.2.2.1.1.3" xref="S3.Ex1.m1.2.2.1.1.3.cmml">𝒯</mi><mo id="S3.Ex1.m1.2.2.1.1.2" xref="S3.Ex1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.2.2.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.cmml"><msub id="S3.Ex1.m1.2.2.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.2.2.1.1.1.3.2" xref="S3.Ex1.m1.2.2.1.1.1.3.2.cmml">ℰ</mi><mi id="S3.Ex1.m1.2.2.1.1.1.3.3" xref="S3.Ex1.m1.2.2.1.1.1.3.3.cmml">𝑆𝐷</mi></msub><mo id="S3.Ex1.m1.2.2.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.2.cmml">ℰ</mi><mi id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.3.cmml">𝑆𝐶</mi></msub><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">ℰ</mi><mi id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">𝐼𝑆</mi></msub><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">𝒫</mi><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1.m1.2.2.1.2" lspace="0em" xref="S3.Ex1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.1"><eq id="S3.Ex1.m1.2.2.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.2"></eq><ci id="S3.Ex1.m1.2.2.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.3">𝒯</ci><apply id="S3.Ex1.m1.2.2.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1"><times id="S3.Ex1.m1.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.2"></times><apply id="S3.Ex1.m1.2.2.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.3.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.1.3.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.3.2">ℰ</ci><ci id="S3.Ex1.m1.2.2.1.1.1.3.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.3.3">𝑆𝐷</ci></apply><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1"><times id="S3.Ex1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.2"></times><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.2">ℰ</ci><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.3">𝑆𝐶</ci></apply><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1"><times id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">ℰ</ci><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">𝐼𝑆</ci></apply><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">𝒫</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">\mathcal{T}=\mathcal{E}_{\mathit{SD}}\left(\mathcal{E}_{\mathit{SC}}\left(%
\mathcal{E}_{\mathit{IS}}(\mathcal{P})\right)\right).</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.2d">caligraphic_T = caligraphic_E start_POSTSUBSCRIPT italic_SD end_POSTSUBSCRIPT ( caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT ( caligraphic_E start_POSTSUBSCRIPT italic_IS end_POSTSUBSCRIPT ( caligraphic_P ) ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS0.Px4.p1.7">.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Sample Generation &amp; Model training</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">With the prompt <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">caligraphic_T</annotation></semantics></math>, we send it to the fine-tuned diffusion model to generate image samples. Although the model can produce realistic images featuring challenging weather and lighting conditions, it is still difficult to incorporate pixel-level information because the images lack semantic labels.
To overcome this, we employ UDA methods such as DAFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib1" title="">1</a>]</cite> and HRDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib36" title="">36</a>]</cite>, which facilitate the adaptation of a segmentation model to an unlabeled target dataset. Specifically, we train these methods using Cityscapes<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib37" title="">37</a>]</cite> dataset as the source domain, and our generated dataset as the pseudo-target domain. The model is then evaluated on the target three real-world benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib39" title="">39</a>]</cite>. The proposed framework can be adapted to UDA method easily, transforming it into a domain generalization method.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Dataset</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conduct the experiments with domain generalization settings, using Cityscapes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib37" title="">37</a>]</cite> as source domain dataset, ACDC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib35" title="">35</a>]</cite>, BDD100k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib38" title="">38</a>]</cite> and DarkZurich <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib39" title="">39</a>]</cite> as test target domain dataset. The Cityscapes dataset comprises 2,975 images captured under standard weather conditions and during daytime, each accompanied by corresponding semantic segmentation labels for training. The ACDC dataset features images taken under various typical weather and lighting conditions, including snowy, rainy, foggy, and nighttime settings. BDD100k contains various weathers and geographic locations. DarkZurich consists of images captured at night, providing a challenging environment for nighttime visual perception tasks. For fine-tuning the diffusion model, we use a single image sampled from the Cityscapes dataset. For unsupervised domain adaptation (UDA) training, we utilize the Cityscapes training set along with images generated by our diffusion model. For evaluation, we use 406 images from the ACDC validation set, 1,000 images from the BDD100k validation set, and 50 images from the DarkZurich validation set.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Implementation details</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We utilize the Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib16" title="">16</a>]</cite> as pretrained model and fine-tune it using DreamBooth <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib34" title="">34</a>]</cite> method. During fine-tuning stage, we randomly crop a patch of 512<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mo id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><times id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">×</annotation></semantics></math>512 size from the images selected from Cityscapes dataset. Then we pair it with customized prompt “a photo of V driving scene”. We use mean square error loss to quantify the differences between the original image patch and the generated image during model training. After training, we use text prompts generated by a chain of Llama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib33" title="">33</a>]</cite> models to create images. The generated images are further used in UDA training for generalizing normal weather source domain to adverse conditions domain.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Evaluation and Discussion</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Comparison with State-of-the-art </span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We evaluate WeatherDG against state-of-the-art domain generalization models using ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib40" title="">40</a>]</cite> and MiT-B5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib41" title="">41</a>]</cite> as encoders. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.T1" title="In V-A Comparison with State-of-the-art ‣ V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a>, for models using ResNet-50 as encoder, our model consistently outperforms state-of-the-art methods with the highest average mIoU score. With MiT-B5 as backbone, our model exceeds the second-best model MIC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib10" title="">10</a>]</cite> on the ACDC and DarkZurich datasets by over 10% and on the BDD100K dataset by 4.3% in mIoU performance, achieving the best semantic segmentation performance. In addition, we visualize our model’s semantic segmentation results under challenging conditions and compare them with MIC. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S1.F1" title="In I Introduction ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, our model correctly segments sidewalks and sky in foggy and nighttime scenes. In the rainy scene, reflections on the road that are falsely recognized as vehicles by MIC are alleviated by our model. In the snowy scenario, our model successfully detects pedestrians on the road that MIC fails to recognize. These findings indicate our model’s superior generalization capabilities compared to state-of-the-art methods in real-world challenging weather and lighting conditions.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.2" style="width:433.6pt;height:244.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(9.2pt,-5.2pt) scale(1.04409701857727,1.04409701857727) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T1.2.1">
<tr class="ltx_tr" id="S5.T1.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T1.2.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.2.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.1.2.1">Encoder</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T1.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.1.3.1">Test domains mIoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.2.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.1.4.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.2.1.1">ACDC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.2.2"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.2.2.1">BDD100K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.2.3"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.2.3.1">DarkZurich</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.1.3.1">Source-only</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.3.2" rowspan="6"><span class="ltx_text" id="S5.T1.2.1.3.2.1">ResNet-50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.3.3">35.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.3.4">37.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.3.5">9.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.3.6">27.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.4">
<td class="ltx_td ltx_align_left" id="S5.T1.2.1.4.1">IBN-Net<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib42" title="">42</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.4.2">42.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.4.3">45.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.4.4">17.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.4.5">35.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.5">
<td class="ltx_td ltx_align_left" id="S5.T1.2.1.5.1">RobustNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib7" title="">7</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.5.2">41.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.5.3">43.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.5.4">19.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.5.5">34.8</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.6">
<td class="ltx_td ltx_align_left" id="S5.T1.2.1.6.1">SHADE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib20" title="">20</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.6.2">42.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.6.3"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.6.3.1">49.1</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.6.4">22.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.6.5">37.9</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.7">
<td class="ltx_td ltx_align_left" id="S5.T1.2.1.7.1">DPCL<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib43" title="">43</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.7.2">43.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.7.3">44.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.7.4">23.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.7.5">37.3</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.8">
<td class="ltx_td ltx_align_left" id="S5.T1.2.1.8.1"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.8.1.1">WeatherDG (ours)</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.8.2"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.8.2.1">45.2</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.8.3">45.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.8.4"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.8.4.1">23.5</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.8.5"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.8.5.1">38.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.2.1.9.1">Source-only</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T1.2.1.9.2" rowspan="5"><span class="ltx_text" id="S5.T1.2.1.9.2.1">MiT-B5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.9.3">44.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.9.4">44.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.9.5">18.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.1.9.6">36.0</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.10">
<td class="ltx_td ltx_align_left" id="S5.T1.2.1.10.1">DAFormer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib1" title="">1</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.10.2">47.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.10.3">49.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.10.4">24.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.10.5">40.6</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.11">
<td class="ltx_td ltx_align_left" id="S5.T1.2.1.11.1">HRDA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.11.2">46.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.11.3">49.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.11.4">25.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.11.5">40.6</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.12">
<td class="ltx_td ltx_align_left" id="S5.T1.2.1.12.1">MIC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.12.2">50.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.12.3">53.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.12.4">21.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.1.12.5">41.5</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.1.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.2.1.13.1"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.13.1.1">WeatherDG (ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.2.1.13.2"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.13.2.1">60.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.2.1.13.3"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.13.3.1">57.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.2.1.13.4"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.13.4.1">35.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.2.1.13.5"><span class="ltx_text ltx_font_bold" id="S5.T1.2.1.13.5.1">51.0</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.4.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T1.5.2" style="font-size:90%;">Domain generalization performance (mIoU (%)) of state-of-arts methods using ResNet-50 and MiT-B5 as encoders<span class="ltx_text ltx_font_medium" id="S5.T1.5.2.1">. The compared methods are retrained with Cityscapes dataset. Evaluations are performed on ACDC, BDD100K, and DarkZurich datasets that feature adverse conditions, such as snow, rain, fog, and low-light scenarios.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="140" id="S5.F6.g1" src="extracted/5929420/pics/figure_6.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F6.4.2" style="font-size:90%;">Comparison of images generated by the plain stable diffusion model (top row) and our fine-tuned model (bottom row) using the same prompt template<span class="ltx_text ltx_font_medium" id="S5.F6.4.2.1">. The results illustrate that our fine-tuned model significantly reduces artistic and unrealistic elements, generating images more aligned with real-world autonomous driving scenarios.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Influence of UDA methods for training</span>
</h3>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.2" style="width:433.6pt;height:150.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(9.2pt,-3.2pt) scale(1.04409701857727,1.04409701857727) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.2.1">
<tr class="ltx_tr" id="S5.T2.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.2.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.2.1">Encoder</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T2.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.3.1">Test domains mIoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.1.4.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.2.1.1">ACDC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.2.2"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.2.2.1">BDD100K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.2.3"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.2.3.1">DarkZurich</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.2.1.3.1">DAFormer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib1" title="">1</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.2" rowspan="3"><span class="ltx_text" id="S5.T2.2.1.3.2.1">ResNet-50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.3.3.1">45.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.4"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.3.4.1">45.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.5"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.3.5.1">23.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.3.6"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.3.6.1">38.2</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.4">
<td class="ltx_td ltx_align_left" id="S5.T2.2.1.4.1">MIC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.2">43.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.3">43.7</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.4">22.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.4.5">36.7</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.5">
<td class="ltx_td ltx_align_left" id="S5.T2.2.1.5.1">HRDA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.2">44.5</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.3">44.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.4">22.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.5.5">37.2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.2.1.6.1">DAFormer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib1" title="">1</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.2.1.6.2" rowspan="3"><span class="ltx_text" id="S5.T2.2.1.6.2.1">MiT-B5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.6.3">53.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.6.4">53.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.6.5">24.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.1.6.6">43.8</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.7">
<td class="ltx_td ltx_align_left" id="S5.T2.2.1.7.1">MIC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.2">60.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.3">54.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.4">32.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.2.1.7.5">49.0</td>
</tr>
<tr class="ltx_tr" id="S5.T2.2.1.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.2.1.8.1">HRDA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.1.8.2"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.8.2.1">60.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.1.8.3"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.8.3.1">57.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.1.8.4"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.8.4.1">35.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.1.8.5"><span class="ltx_text ltx_font_bold" id="S5.T2.2.1.8.5.1">51.0</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.4.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T2.5.2" style="font-size:90%;">Comparison of mIoU performance of UDA methods<span class="ltx_text ltx_font_medium" id="S5.T2.5.2.1"> trained using the labeled Cityscapes dataset as the source dataset and our generated unlabeled images as the target dataset.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We investigate the influence of UDA methods by utilizing three different state-of-the-art approaches including DAFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib1" title="">1</a>]</cite>, HRDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib36" title="">36</a>]</cite>, and MIC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib10" title="">10</a>]</cite>, each trained with Cityscapes and our generated dataset. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.T2" title="In V-B Influence of UDA methods for training ‣ V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a>, DAFormer demonstrates superior adaptation to the pseudo-target domain among the ResNet-50 models, while HRDA achieves the best generalization performance across the three domains with the MiT-B5 encoder. To further study the influence of these methods under different conditions, we evaluate them across four typical challenging scenarios in the ACDC dataset. As indicated in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.T3" title="Table III ‣ V-B Influence of UDA methods for training ‣ V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">III</span></a>, all methods perform best in foggy conditions and worst in nighttime conditions. Notably, none of these methods exceed a 40% performance in nighttime scenes, which is significantly lower than in other scenarios. This could be due to the substantial appearance differences between nighttime scenes and the predominantly daytime images in the training source domain, making adaptation challenging. This finding suggests that simply adding a pseudo-target dataset for adaptive training may be inadequate for complete knowledge transfer in the nighttime domain, necessitating more advanced adaptation techniques.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.2" style="width:433.6pt;height:128.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(34.5pt,-10.2pt) scale(1.18917431455255,1.18917431455255) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.2.1">
<tr class="ltx_tr" id="S5.T3.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.2.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S5.T3.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.1.2.1">Test weathers</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.2.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.1.3.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.2.1.1">Snow</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.2.2.1">Foggy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.3"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.2.3.1">Rainy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.2.4"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.2.4.1">Nighttime</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.1.3.1">Source-only</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.3.2">49.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.3.3">61.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.3.4">47.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.3.5">19.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.1.3.6">44.8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.4">
<td class="ltx_td ltx_align_left" id="S5.T3.2.1.4.1">DAFormer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib1" title="">1</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.4.2">54.2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.4.3">66.8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.4.4">54.1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.4.5">27.3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.4.6">50.6</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.5">
<td class="ltx_td ltx_align_left" id="S5.T3.2.1.5.1">MIC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.2">59.4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.3">74.2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.4">62.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.5">36.9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.1.5.6">58.0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.1.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.2.1.6.1">HRDA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib36" title="">36</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.6.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.6.2.1">59.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.6.3"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.6.3.1">75.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.6.4"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.6.4.1">64.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.6.5"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.6.5.1">38.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.1.6.6"><span class="ltx_text ltx_font_bold" id="S5.T3.2.1.6.6.1">59.7</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.3.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S5.T3.4.2" style="font-size:90%;">Comparison of mIoU performance of UDA techniques across typical weather and lighting conditions.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Influence of SD Fine-tuning.</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To demonstrate the influence of scene prior adaptation, we compare images generated by the plain stable diffusion model and our fine-tuned model. We use the same prompt, templated as “A photo of [CLS]” for each model to generate commonly seen objects in the autonomous driving dataset.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T4.2">
<tr class="ltx_tr" id="S5.T4.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.2.1">ACDC</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.3.1">BDD100K</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T4.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.2.1.4.1">DarkZurich</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.2.1">w/o Fine-tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.2.2">49.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.2.2.3">52.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T4.2.2.4">24.7</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.3.1">w Fine-tuning</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.2.3.2"><span class="ltx_text ltx_font_bold" id="S5.T4.2.3.2.1">50.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.2.3.3"><span class="ltx_text ltx_font_bold" id="S5.T4.2.3.3.1">53.2</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T4.2.3.4"><span class="ltx_text ltx_font_bold" id="S5.T4.2.3.4.1">25.6</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.3.1.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="S5.T4.4.2" style="font-size:90%;">Comparison of mIoU performance of models trained on datasets generated with and without fine-tuned model.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">The results in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.F6" title="In V-A Comparison with State-of-the-art ‣ V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> show that the plain stable diffusion tends to generate images with an artistic style or cinematic photography effects, as seen with “truck,” “bicycle,” “motorcycle,” and “bus”. For “car” and “train”, the images present different camera perspectives, such as bird’s-eye view. Additionally, for “traffic light”, “traffic sign”, and “person”, the model exhibits excessive creativity, generating overly stylized traffic lights and rendering “person” as sketches. For the “rider” category, the stable diffusion model imagines a surfer on the sea.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">These samples may acquire incorrect pseudo-labels that negatively impact adaptive training and harm segmentation performance. In contrast, our fine-tuned model ensures that the generated instances are contextually appropriate for the driving scene, resulting in more accurate pseudo-labels and better adaptation to the autonomous driving domain. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.T4" title="In V-C Influence of SD Fine-tuning. ‣ V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">IV</span></a>, the performance of semantic segmentation model trained with images generated by the fine-tuned model is better than plain stable diffusion.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="226" id="S5.F7.g1" src="extracted/5929420/pics/figure_7.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.9.4.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F7.6.3" style="font-size:90%;">Comparison of images generated using prompts created by different LLM agents.<span class="ltx_text ltx_font_medium" id="S5.F7.6.3.3"> Each row represents images generated by a specific LLM agent, while each column showcases images generated by different LLM agents for specific weather or lighting effects during the procedural prompt generation. The results demonstrate that <math alttext="\mathcal{E}_{\mathit{IS}}" class="ltx_Math" display="inline" id="S5.F7.4.1.1.m1.1"><semantics id="S5.F7.4.1.1.m1.1b"><msub id="S5.F7.4.1.1.m1.1.1" xref="S5.F7.4.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.F7.4.1.1.m1.1.1.2" xref="S5.F7.4.1.1.m1.1.1.2.cmml">ℰ</mi><mi id="S5.F7.4.1.1.m1.1.1.3" xref="S5.F7.4.1.1.m1.1.1.3.cmml">𝐼𝑆</mi></msub><annotation-xml encoding="MathML-Content" id="S5.F7.4.1.1.m1.1c"><apply id="S5.F7.4.1.1.m1.1.1.cmml" xref="S5.F7.4.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.F7.4.1.1.m1.1.1.1.cmml" xref="S5.F7.4.1.1.m1.1.1">subscript</csymbol><ci id="S5.F7.4.1.1.m1.1.1.2.cmml" xref="S5.F7.4.1.1.m1.1.1.2">ℰ</ci><ci id="S5.F7.4.1.1.m1.1.1.3.cmml" xref="S5.F7.4.1.1.m1.1.1.3">𝐼𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F7.4.1.1.m1.1d">\mathcal{E}_{\mathit{IS}}</annotation><annotation encoding="application/x-llamapun" id="S5.F7.4.1.1.m1.1e">caligraphic_E start_POSTSUBSCRIPT italic_IS end_POSTSUBSCRIPT</annotation></semantics></math> (top row) enables the model to generate diverse instances, albeit with limited scene detail. While <math alttext="\mathcal{E}_{\mathit{SC}}" class="ltx_Math" display="inline" id="S5.F7.5.2.2.m2.1"><semantics id="S5.F7.5.2.2.m2.1b"><msub id="S5.F7.5.2.2.m2.1.1" xref="S5.F7.5.2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.F7.5.2.2.m2.1.1.2" xref="S5.F7.5.2.2.m2.1.1.2.cmml">ℰ</mi><mi id="S5.F7.5.2.2.m2.1.1.3" xref="S5.F7.5.2.2.m2.1.1.3.cmml">𝑆𝐶</mi></msub><annotation-xml encoding="MathML-Content" id="S5.F7.5.2.2.m2.1c"><apply id="S5.F7.5.2.2.m2.1.1.cmml" xref="S5.F7.5.2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.F7.5.2.2.m2.1.1.1.cmml" xref="S5.F7.5.2.2.m2.1.1">subscript</csymbol><ci id="S5.F7.5.2.2.m2.1.1.2.cmml" xref="S5.F7.5.2.2.m2.1.1.2">ℰ</ci><ci id="S5.F7.5.2.2.m2.1.1.3.cmml" xref="S5.F7.5.2.2.m2.1.1.3">𝑆𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F7.5.2.2.m2.1d">\mathcal{E}_{\mathit{SC}}</annotation><annotation encoding="application/x-llamapun" id="S5.F7.5.2.2.m2.1e">caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT</annotation></semantics></math> (middle row) allows the model to generate weather and lighting effects, the overall impact is rather subtle. With detailed descriptions crafted by <math alttext="\mathcal{E}_{\mathit{SD}}" class="ltx_Math" display="inline" id="S5.F7.6.3.3.m3.1"><semantics id="S5.F7.6.3.3.m3.1b"><msub id="S5.F7.6.3.3.m3.1.1" xref="S5.F7.6.3.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.F7.6.3.3.m3.1.1.2" xref="S5.F7.6.3.3.m3.1.1.2.cmml">ℰ</mi><mi id="S5.F7.6.3.3.m3.1.1.3" xref="S5.F7.6.3.3.m3.1.1.3.cmml">𝑆𝐷</mi></msub><annotation-xml encoding="MathML-Content" id="S5.F7.6.3.3.m3.1c"><apply id="S5.F7.6.3.3.m3.1.1.cmml" xref="S5.F7.6.3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.F7.6.3.3.m3.1.1.1.cmml" xref="S5.F7.6.3.3.m3.1.1">subscript</csymbol><ci id="S5.F7.6.3.3.m3.1.1.2.cmml" xref="S5.F7.6.3.3.m3.1.1.2">ℰ</ci><ci id="S5.F7.6.3.3.m3.1.1.3.cmml" xref="S5.F7.6.3.3.m3.1.1.3">𝑆𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F7.6.3.3.m3.1d">\mathcal{E}_{\mathit{SD}}</annotation><annotation encoding="application/x-llamapun" id="S5.F7.6.3.3.m3.1e">caligraphic_E start_POSTSUBSCRIPT italic_SD end_POSTSUBSCRIPT</annotation></semantics></math>, the model (bottom row) produces intricate scene details and more diverse weather and lighting effects, significantly enhancing the variety and realism of the generated images.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">Influence of Procedural Prompt Generation</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.2">To illustrate the effectiveness of our procedural prompt generation, we compare images generated by text prompts created by different LLM agents in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.F7" title="In V-C Influence of SD Fine-tuning. ‣ V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>. The results show that the instance sampler can generate the desired objects with a basic prompt such as “A photo of [CLS]”. By specifying the general category of weather and time of day, the scene composer only adds basic weather effects to the image, though these effects are subtle. Additionally, the generated samples primarily focus on the subject, often lacking scene details. When the scene descriptor crafts more detailed scene descriptions in the prompt, the model produces images with various realistic weather and lighting effects. As shown in third row, for snowy weather, the model generates a complex scene with heavy accumulated snow on the road and even snowflakes in the air. For rainy weather, the prompt generates reflections, raindrops, and a misty effect, indicating heavy rain. For nighttime, we can see that <math alttext="\mathcal{E}_{\mathit{SC}}" class="ltx_Math" display="inline" id="S5.SS4.p1.1.m1.1"><semantics id="S5.SS4.p1.1.m1.1a"><msub id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">ℰ</mi><mi id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">𝑆𝐶</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2">ℰ</ci><ci id="S5.SS4.p1.1.m1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.3">𝑆𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">\mathcal{E}_{\mathit{SC}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.1.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT</annotation></semantics></math> fails to add sufficient nighttime lighting, but with prompts generated by <math alttext="\mathcal{E}_{\mathit{SD}}" class="ltx_Math" display="inline" id="S5.SS4.p1.2.m2.1"><semantics id="S5.SS4.p1.2.m2.1a"><msub id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS4.p1.2.m2.1.1.2" xref="S5.SS4.p1.2.m2.1.1.2.cmml">ℰ</mi><mi id="S5.SS4.p1.2.m2.1.1.3" xref="S5.SS4.p1.2.m2.1.1.3.cmml">𝑆𝐷</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.2.m2.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS4.p1.2.m2.1.1.2.cmml" xref="S5.SS4.p1.2.m2.1.1.2">ℰ</ci><ci id="S5.SS4.p1.2.m2.1.1.3.cmml" xref="S5.SS4.p1.2.m2.1.1.3">𝑆𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">\mathcal{E}_{\mathit{SD}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p1.2.m2.1d">caligraphic_E start_POSTSUBSCRIPT italic_SD end_POSTSUBSCRIPT</annotation></semantics></math>, the scene in the image exhibit a darker tone and more intricate details, creating a more realistic nighttime environment. In addition, we evaluate the mIoU performance of the DAFormer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#bib.bib1" title="">1</a>]</cite> trained with datasets generated using different LLM agents. In <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.T5" title="In V-D Influence of Procedural Prompt Generation ‣ V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">V</span></a>, the results demonstrate that progressively refining the base prompt “A photo of [CLS]” using these LLM agents results in higher mIoU scores.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.7">
<tr class="ltx_tr" id="S5.T5.7.8">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T5.7.8.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.7.8.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T5.7.8.2"><span class="ltx_text ltx_font_bold" id="S5.T5.7.8.2.1">LLM-Agents</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.7.8.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.7.8.3.1">mIoU</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.1"><math alttext="\mathcal{E}_{\mathit{IS}}" class="ltx_Math" display="inline" id="S5.T5.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.m1.1a"><msub id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.1.1.1.m1.1.1.2" xref="S5.T5.1.1.1.m1.1.1.2.cmml">ℰ</mi><mi id="S5.T5.1.1.1.m1.1.1.3" xref="S5.T5.1.1.1.m1.1.1.3.cmml">𝐼𝑆</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><apply id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.1.1.1.m1.1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T5.1.1.1.m1.1.1.2.cmml" xref="S5.T5.1.1.1.m1.1.1.2">ℰ</ci><ci id="S5.T5.1.1.1.m1.1.1.3.cmml" xref="S5.T5.1.1.1.m1.1.1.3">𝐼𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">\mathcal{E}_{\mathit{IS}}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_IS end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.2.2.2"><math alttext="\mathcal{E}_{\mathit{SC}}" class="ltx_Math" display="inline" id="S5.T5.2.2.2.m1.1"><semantics id="S5.T5.2.2.2.m1.1a"><msub id="S5.T5.2.2.2.m1.1.1" xref="S5.T5.2.2.2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.2.2.2.m1.1.1.2" xref="S5.T5.2.2.2.m1.1.1.2.cmml">ℰ</mi><mi id="S5.T5.2.2.2.m1.1.1.3" xref="S5.T5.2.2.2.m1.1.1.3.cmml">𝑆𝐶</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.m1.1b"><apply id="S5.T5.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.2.2.2.m1.1.1.1.cmml" xref="S5.T5.2.2.2.m1.1.1">subscript</csymbol><ci id="S5.T5.2.2.2.m1.1.1.2.cmml" xref="S5.T5.2.2.2.m1.1.1.2">ℰ</ci><ci id="S5.T5.2.2.2.m1.1.1.3.cmml" xref="S5.T5.2.2.2.m1.1.1.3">𝑆𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.m1.1c">\mathcal{E}_{\mathit{SC}}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.3"><math alttext="\mathcal{E}_{\mathit{SD}}" class="ltx_Math" display="inline" id="S5.T5.3.3.3.m1.1"><semantics id="S5.T5.3.3.3.m1.1a"><msub id="S5.T5.3.3.3.m1.1.1" xref="S5.T5.3.3.3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T5.3.3.3.m1.1.1.2" xref="S5.T5.3.3.3.m1.1.1.2.cmml">ℰ</mi><mi id="S5.T5.3.3.3.m1.1.1.3" xref="S5.T5.3.3.3.m1.1.1.3.cmml">𝑆𝐷</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.m1.1b"><apply id="S5.T5.3.3.3.m1.1.1.cmml" xref="S5.T5.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.3.3.3.m1.1.1.1.cmml" xref="S5.T5.3.3.3.m1.1.1">subscript</csymbol><ci id="S5.T5.3.3.3.m1.1.1.2.cmml" xref="S5.T5.3.3.3.m1.1.1.2">ℰ</ci><ci id="S5.T5.3.3.3.m1.1.1.3.cmml" xref="S5.T5.3.3.3.m1.1.1.3">𝑆𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.m1.1c">\mathcal{E}_{\mathit{SD}}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.3.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_SD end_POSTSUBSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S5.T5.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.4.4.1"><math alttext="M_{base}" class="ltx_Math" display="inline" id="S5.T5.4.4.1.m1.1"><semantics id="S5.T5.4.4.1.m1.1a"><msub id="S5.T5.4.4.1.m1.1.1" xref="S5.T5.4.4.1.m1.1.1.cmml"><mi id="S5.T5.4.4.1.m1.1.1.2" xref="S5.T5.4.4.1.m1.1.1.2.cmml">M</mi><mrow id="S5.T5.4.4.1.m1.1.1.3" xref="S5.T5.4.4.1.m1.1.1.3.cmml"><mi id="S5.T5.4.4.1.m1.1.1.3.2" xref="S5.T5.4.4.1.m1.1.1.3.2.cmml">b</mi><mo id="S5.T5.4.4.1.m1.1.1.3.1" xref="S5.T5.4.4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.T5.4.4.1.m1.1.1.3.3" xref="S5.T5.4.4.1.m1.1.1.3.3.cmml">a</mi><mo id="S5.T5.4.4.1.m1.1.1.3.1a" xref="S5.T5.4.4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.T5.4.4.1.m1.1.1.3.4" xref="S5.T5.4.4.1.m1.1.1.3.4.cmml">s</mi><mo id="S5.T5.4.4.1.m1.1.1.3.1b" xref="S5.T5.4.4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.T5.4.4.1.m1.1.1.3.5" xref="S5.T5.4.4.1.m1.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.1.m1.1b"><apply id="S5.T5.4.4.1.m1.1.1.cmml" xref="S5.T5.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.4.4.1.m1.1.1.1.cmml" xref="S5.T5.4.4.1.m1.1.1">subscript</csymbol><ci id="S5.T5.4.4.1.m1.1.1.2.cmml" xref="S5.T5.4.4.1.m1.1.1.2">𝑀</ci><apply id="S5.T5.4.4.1.m1.1.1.3.cmml" xref="S5.T5.4.4.1.m1.1.1.3"><times id="S5.T5.4.4.1.m1.1.1.3.1.cmml" xref="S5.T5.4.4.1.m1.1.1.3.1"></times><ci id="S5.T5.4.4.1.m1.1.1.3.2.cmml" xref="S5.T5.4.4.1.m1.1.1.3.2">𝑏</ci><ci id="S5.T5.4.4.1.m1.1.1.3.3.cmml" xref="S5.T5.4.4.1.m1.1.1.3.3">𝑎</ci><ci id="S5.T5.4.4.1.m1.1.1.3.4.cmml" xref="S5.T5.4.4.1.m1.1.1.3.4">𝑠</ci><ci id="S5.T5.4.4.1.m1.1.1.3.5.cmml" xref="S5.T5.4.4.1.m1.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.1.m1.1c">M_{base}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.4.1.m1.1d">italic_M start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.4.4.2">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.4.4.3">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.4.4.4">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.4.4.5">50.8</td>
</tr>
<tr class="ltx_tr" id="S5.T5.5.5">
<td class="ltx_td ltx_align_left" id="S5.T5.5.5.1"><math alttext="M_{1}" class="ltx_Math" display="inline" id="S5.T5.5.5.1.m1.1"><semantics id="S5.T5.5.5.1.m1.1a"><msub id="S5.T5.5.5.1.m1.1.1" xref="S5.T5.5.5.1.m1.1.1.cmml"><mi id="S5.T5.5.5.1.m1.1.1.2" xref="S5.T5.5.5.1.m1.1.1.2.cmml">M</mi><mn id="S5.T5.5.5.1.m1.1.1.3" xref="S5.T5.5.5.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.1.m1.1b"><apply id="S5.T5.5.5.1.m1.1.1.cmml" xref="S5.T5.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.5.5.1.m1.1.1.1.cmml" xref="S5.T5.5.5.1.m1.1.1">subscript</csymbol><ci id="S5.T5.5.5.1.m1.1.1.2.cmml" xref="S5.T5.5.5.1.m1.1.1.2">𝑀</ci><cn id="S5.T5.5.5.1.m1.1.1.3.cmml" type="integer" xref="S5.T5.5.5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.1.m1.1c">M_{1}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.5.1.m1.1d">italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.2">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.3">–</td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.4">–</td>
<td class="ltx_td ltx_align_center" id="S5.T5.5.5.5">51.5</td>
</tr>
<tr class="ltx_tr" id="S5.T5.6.6">
<td class="ltx_td ltx_align_left" id="S5.T5.6.6.1"><math alttext="M_{2}" class="ltx_Math" display="inline" id="S5.T5.6.6.1.m1.1"><semantics id="S5.T5.6.6.1.m1.1a"><msub id="S5.T5.6.6.1.m1.1.1" xref="S5.T5.6.6.1.m1.1.1.cmml"><mi id="S5.T5.6.6.1.m1.1.1.2" xref="S5.T5.6.6.1.m1.1.1.2.cmml">M</mi><mn id="S5.T5.6.6.1.m1.1.1.3" xref="S5.T5.6.6.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.1.m1.1b"><apply id="S5.T5.6.6.1.m1.1.1.cmml" xref="S5.T5.6.6.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.6.6.1.m1.1.1.1.cmml" xref="S5.T5.6.6.1.m1.1.1">subscript</csymbol><ci id="S5.T5.6.6.1.m1.1.1.2.cmml" xref="S5.T5.6.6.1.m1.1.1.2">𝑀</ci><cn id="S5.T5.6.6.1.m1.1.1.3.cmml" type="integer" xref="S5.T5.6.6.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.1.m1.1c">M_{2}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.6.1.m1.1d">italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.2">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.3">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.4">–</td>
<td class="ltx_td ltx_align_center" id="S5.T5.6.6.5">52.1</td>
</tr>
<tr class="ltx_tr" id="S5.T5.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.7.7.1"><math alttext="M_{3}" class="ltx_Math" display="inline" id="S5.T5.7.7.1.m1.1"><semantics id="S5.T5.7.7.1.m1.1a"><msub id="S5.T5.7.7.1.m1.1.1" xref="S5.T5.7.7.1.m1.1.1.cmml"><mi id="S5.T5.7.7.1.m1.1.1.2" xref="S5.T5.7.7.1.m1.1.1.2.cmml">M</mi><mn id="S5.T5.7.7.1.m1.1.1.3" xref="S5.T5.7.7.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.1.m1.1b"><apply id="S5.T5.7.7.1.m1.1.1.cmml" xref="S5.T5.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T5.7.7.1.m1.1.1.1.cmml" xref="S5.T5.7.7.1.m1.1.1">subscript</csymbol><ci id="S5.T5.7.7.1.m1.1.1.2.cmml" xref="S5.T5.7.7.1.m1.1.1.2">𝑀</ci><cn id="S5.T5.7.7.1.m1.1.1.3.cmml" type="integer" xref="S5.T5.7.7.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.1.m1.1c">M_{3}</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.7.1.m1.1d">italic_M start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.7.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.7.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.7.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.7.7.5">53.3</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.9.1.1" style="font-size:90%;">TABLE V</span>: </span><span class="ltx_text" id="S5.T5.10.2" style="font-size:90%;">Comparison of mIoU performance for models trained on datasets generated by introducing different LLM agents.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.5.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.6.2">Influence of Numbers of Generated Images</span>
</h3>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="S5.F8.g1" src="extracted/5929420/pics/figure_8.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S5.F8.3.2" style="font-size:90%;">Impact of the number of generated images on training model’s mIoU performance.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">To investigate the impact of the generated dataset size on the performance of the segmentation model, we evaluate the mIoU performance of the DAFormer model trained with the generated dataset on the ACDC, BDD100K, and DarkZurich datasets. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.12075v1#S5.F8" title="In V-E Influence of Numbers of Generated Images ‣ V Evaluation and Discussion ‣ WeatherDG: LLM-assisted Procedural Weather Generation for Domain-Generalized Semantic Segmentation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>, a substantial performance gain are observed as the number of images increases from 10 to 1000 for all three datasets, after which the performance gains level off. Notably, the Dark Zurich dataset shows much smaller improvements with the increased number of images compared to the other two datasets. This can be attributed to the inherent learning difficulty for nighttime images during the model training. These findings indicate that: 1) while increasing the amount of training data generally enhances model performance, the benefits may plateau after some point; and 2) inherent dataset characteristics can impact the extent of the model’s improvement.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we present WeatherDG, a novel approach for domain generalization in semantic segmentation under adverse weather conditions. By combining Stable Diffusion (SD) with a Large Language Model (LLM), our method enables automated generation of realistic images resembling real-world driving scenarios. Fine-tuning SD, along with procedural prompt generation and a balanced strategy, creates diverse weather effects and enhances tailed classes in generated images. These images, combined with source data, improve model generalization. Experiments across challenging datasets show that WeatherDG significantly boosts semantic segmentation performance, setting a new benchmark for robustness in autonomous driving.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:70%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:70%;">
L. Hoyer, D. Dai, and L. Van Gool, “DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib1.3.3" style="font-size:70%;">, 2022, pp. 9924–9935.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:70%;">
A. Seppänen, R. Ojala, and K. Tammi, “4denoisenet: Adverse weather denoising from adjacent point clouds,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2" style="font-size:70%;">IEEE Robotics and Automation Letters</em><span class="ltx_text" id="bib.bib2.3.3" style="font-size:70%;">, vol. 8, no. 1, pp. 456–463, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:70%;">
Y. Lee, Y. Ko, Y. Kim, and M. Jeon, “Perception-friendly video enhancement for autonomous driving under adverse weather conditions,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2" style="font-size:70%;">2022 International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib3.3.3" style="font-size:70%;">, 2022, pp. 7760–7767.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:70%;">
Y. Lee, J. Jeon, Y. Ko, B.-G. Jeon, and M. Jeon, “Task-driven deep image enhancement network for autonomous driving in bad weather,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2" style="font-size:70%;">2021 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib4.3.3" style="font-size:70%;">, pp. 13 746–13 753, 2021. [Online]. Available: </span><a class="ltx_ref ltx_url" href="https://api.semanticscholar.org/CorpusID:238857297" style="font-size:70%;" title="">https://api.semanticscholar.org/CorpusID:238857297</a><span class="ltx_text" id="bib.bib4.4.4" style="font-size:70%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:70%;">
X. Jiang, J. Huang, S. Jin, and S. Lu, “Domain generalization via balancing training difficulty and model capability,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib5.3.3" style="font-size:70%;">, 2023, pp. 18 993–19 003.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:70%;">
D. Peng, Y. Lei, M. Hayat, Y. Guo, and W. Li, “Semantic-aware domain generalized segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib6.3.3" style="font-size:70%;">, 2022, pp. 2594–2605.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:70%;">
S. Choi, S. Jung, H. Yun, J. T. Kim, S. Kim, and J. Choo, “Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib7.3.3" style="font-size:70%;">, 2021, pp. 11 580–11 590.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:70%;">
J. Niemeijer, M. Schwonberg, J.-A. Termöhlen, N. M. Schmidt, and T. Fingscheidt, “Generalization by adaptation: Diffusion-based domain extension for domain-generalized semantic segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em><span class="ltx_text" id="bib.bib8.3.3" style="font-size:70%;">, January 2024, pp. 2830–2840.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:70%;">
Y. Benigmim, S. Roy, S. Essid, V. Kalogeiton, and S. Lathuilière, “Collaborating foundation models for domain generalized semantic segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib9.3.3" style="font-size:70%;">, June 2024, pp. 3108–3119.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:70%;">
L. Hoyer, D. Dai, H. Wang, and L. Van Gool, “MIC: Masked image consistency for context-enhanced domain adaptation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib10.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib10.3.3" style="font-size:70%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:70%;">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA: An open urban driving simulator,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2" style="font-size:70%;">Proceedings of the 1st Annual Conference on Robot Learning</em><span class="ltx_text" id="bib.bib11.3.3" style="font-size:70%;">, 2017, pp. 1–16.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:70%;">
X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to-real transfer of robotic control with dynamics randomization,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib12.2.2" style="font-size:70%;">2018 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib12.3.3" style="font-size:70%;">, pp. 1–8, 2017. [Online]. Available: </span><a class="ltx_ref ltx_url" href="https://api.semanticscholar.org/CorpusID:3707478" style="font-size:70%;" title="">https://api.semanticscholar.org/CorpusID:3707478</a><span class="ltx_text" id="bib.bib12.4.4" style="font-size:70%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:70%;">
R. Zurbrügg, H. Blum, C. Cadena, R. Siegwart, and L. Schmid, “Embodied active domain adaptation for semantic segmentation via informative path planning,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2" style="font-size:70%;">IEEE Robotics and Automation Letters</em><span class="ltx_text" id="bib.bib13.3.3" style="font-size:70%;">, vol. 7, no. 4, pp. 8691–8698, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:70%;">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networkss,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2" style="font-size:70%;">Computer Vision (ICCV), 2017 IEEE International Conference on</em><span class="ltx_text" id="bib.bib14.3.3" style="font-size:70%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:70%;">
X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz, “Multimodal unsupervised image-to-image translation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2" style="font-size:70%;">ECCV</em><span class="ltx_text" id="bib.bib15.3.3" style="font-size:70%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:70%;">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:70%;">
M. Mancini, S. R. Bulo, B. Caputo, and E. Ricci, “Robust place categorization with deep domain generalization,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2" style="font-size:70%;">IEEE Robotics and Automation Letters</em><span class="ltx_text" id="bib.bib17.3.3" style="font-size:70%;">, vol. 3, no. 3, pp. 2093–2100, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:70%;">
J. Weyler, T. Läbe, F. Magistri, J. Behley, and C. Stachniss, “Towards domain generalization in crop and weed segmentation for precision farming robots,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib18.2.2" style="font-size:70%;">IEEE Robotics and Automation Letters</em><span class="ltx_text" id="bib.bib18.3.3" style="font-size:70%;">, vol. 8, no. 6, pp. 3310–3317, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:70%;">
J. Niemeijer and J. P. Schäfer, “Domain adaptation and generalization: A low-complexity approach,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib19.2.2" style="font-size:70%;">Proceedings of The 6th Conference on Robot Learning</em><span class="ltx_text" id="bib.bib19.3.3" style="font-size:70%;">, ser. Proceedings of Machine Learning Research, K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. 205.   PMLR, 14–18 Dec 2023, pp. 1081–1091. [Online]. Available: </span><a class="ltx_ref ltx_url" href="https://proceedings.mlr.press/v205/niemeijer23a.html" style="font-size:70%;" title="">https://proceedings.mlr.press/v205/niemeijer23a.html</a><span class="ltx_text" id="bib.bib19.4.4" style="font-size:70%;">
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:70%;">
Y. Zhao, Z. Zhong, N. Zhao, N. Sebe, and G. H. Lee, “Style-hallucinated dual consistency learning for domain generalized semantic segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2" style="font-size:70%;">Proceedings of the European Conference on Computer Vision (ECCV)</em><span class="ltx_text" id="bib.bib20.3.3" style="font-size:70%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:70%;">
T. Sun, C. Lu, T. Zhang, and H. Ling, “Safe self-refinement for transformer-based domain adaptation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib21.3.3" style="font-size:70%;">, 2022, pp. 7191–7200.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:70%;">
M. Wulfmeier, A. Bewley, and I. Posner, “Incremental adversarial domain adaptation for continually changing environments,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2" style="font-size:70%;">2018 IEEE International Conference on Robotics and Automation (ICRA)</em><span class="ltx_text" id="bib.bib22.3.3" style="font-size:70%;">, 2018, pp. 4489–4495.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:70%;">
M. Diaz-Zapata, Ö. Erkent, and C. Laugier, “Instance segmentation with unsupervised adaptation to different domains for autonomous vehicles,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2" style="font-size:70%;">2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV)</em><span class="ltx_text" id="bib.bib23.3.3" style="font-size:70%;">.   IEEE, 2020, pp. 421–427.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:70%;">
Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker, “Learning to adapt structured output space for semantic segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2" style="font-size:70%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib24.3.3" style="font-size:70%;">, 2018, pp. 7472–7481.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:70%;">
W. Zhang, W. Ouyang, W. Li, and D. Xu, “Collaborative and adversarial network for unsupervised domain adaptation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2" style="font-size:70%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span class="ltx_text" id="bib.bib25.3.3" style="font-size:70%;">, June 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:70%;">
Q. Zhang, J. Zhang, W. Liu, and D. Tao, “Category anchor-guided unsupervised domain adaptation for semantic segmentation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2" style="font-size:70%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib26.3.3" style="font-size:70%;">, 2019, pp. 433–443.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:70%;">
Y. Benigmim, S. Roy, S. Essid, V. Kalogeiton, and S. Lathuilière, “One-shot unsupervised domain adaptation with personalized diffusion models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2" style="font-size:70%;">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em><span class="ltx_text" id="bib.bib27.3.3" style="font-size:70%;">, 2023, pp. 698–708.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:70%;">
M. Fahes, T.-H. Vu, A. Bursuc, P. Pérez, and R. de Charette, “Pøda: Prompt-driven zero-shot domain adaptation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib28.2.2" style="font-size:70%;">ICCV</em><span class="ltx_text" id="bib.bib28.3.3" style="font-size:70%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:70%;">
A. Kerim, F. Chamone, W. L. Ramos, L. S. Marcolino, E. R. Nascimento, and R. Jiang, “Semantic segmentation under adverse conditions: A weather and nighttime-aware synthetic data-based approach,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2" style="font-size:70%;">33nd British Machine Vision Conference 2022, BMVC 2022</em><span class="ltx_text" id="bib.bib29.3.3" style="font-size:70%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:70%;">
L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib30.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib30.3.3" style="font-size:70%;">, 2023, pp. 3836–3847.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:70%;">
S. Pratt, I. Covert, R. Liu, and A. Farhadi, “What does a platypus look like? generating customized prompts for zero-shot image classification,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib31.3.3" style="font-size:70%;">, 2023, pp. 15 691–15 701.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:70%;">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2" style="font-size:70%;">et al.</em><span class="ltx_text" id="bib.bib32.3.3" style="font-size:70%;">, “Language models are few-shot learners,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib32.4.4" style="font-size:70%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib32.5.5" style="font-size:70%;">, vol. 33, pp. 1877–1901, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:70%;">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2" style="font-size:70%;">et al.</em><span class="ltx_text" id="bib.bib33.3.3" style="font-size:70%;">, “Llama: Open and efficient foundation language models,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib33.4.4" style="font-size:70%;">arXiv preprint arXiv:2302.13971</em><span class="ltx_text" id="bib.bib33.5.5" style="font-size:70%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:70%;">
N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, “Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span class="ltx_text" id="bib.bib34.3.3" style="font-size:70%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:70%;">
C. Sakaridis, D. Dai, and L. Van Gool, “Acdc: The adverse conditions dataset with correspondences for semantic driving scene understanding,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span class="ltx_text" id="bib.bib35.3.3" style="font-size:70%;">, 2021, pp. 10 765–10 775.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:70%;">
L. Hoyer, D. Dai, and L. V. Gool, “Domain adaptive and generalizable network architectures and training strategies for semantic image segmentation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2" style="font-size:70%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib36.3.3" style="font-size:70%;">, vol. 46, no. 01, pp. 220–235, jan 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:70%;">
M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib37.2.2" style="font-size:70%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib37.3.3" style="font-size:70%;">, 2016, pp. 3213–3223.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:70%;">
F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell, “Bdd100k: A diverse driving dataset for heterogeneous multitask learning,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib38.2.2" style="font-size:70%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib38.3.3" style="font-size:70%;">, 2020, pp. 2636–2645.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:70%;">
C. Sakaridis, D. Dai, and L. Van Gool, “Map-guided curriculum domain adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib39.2.2" style="font-size:70%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span class="ltx_text" id="bib.bib39.3.3" style="font-size:70%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:70%;">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib40.2.2" style="font-size:70%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span class="ltx_text" id="bib.bib40.3.3" style="font-size:70%;">, 2016, pp. 770–778.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:70%;">
E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib41.2.2" style="font-size:70%;">Neural Information Processing Systems (NeurIPS)</em><span class="ltx_text" id="bib.bib41.3.3" style="font-size:70%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:70%;">
J. S. Xingang Pan, Ping Luo and X. Tang, “Two at once: Enhancing learning and generalization capacities via ibn-net,” in </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.2.2" style="font-size:70%;">ECCV</em><span class="ltx_text" id="bib.bib42.3.3" style="font-size:70%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:70%;">
L. Yang, X. Gu, and J. Sun, “Generalized semantic segmentation by self-supervised source domain projection and multi-level contrastive learning,” </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.2.2" style="font-size:70%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span class="ltx_text" id="bib.bib43.3.3" style="font-size:70%;">, vol. 37, no. 9, pp. 10 789–10 797, Jun. 2023. [Online]. Available: </span><a class="ltx_ref ltx_url" href="https://ojs.aaai.org/index.php/AAAI/article/view/26280" style="font-size:70%;" title="">https://ojs.aaai.org/index.php/AAAI/article/view/26280</a><span class="ltx_text" id="bib.bib43.4.4" style="font-size:70%;">
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct 15 21:28:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
