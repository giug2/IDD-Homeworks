<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2004.14046] Reducing catastrophic forgetting with learning on synthetic data</title><meta property="og:description" content="Catastrophic forgetting is a problem caused by neural networks’ inability to learn data in sequence. After learning two tasks in sequence, performance on the first one drops significantly. This is a serious disadvantag…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reducing catastrophic forgetting with learning on synthetic data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Reducing catastrophic forgetting with learning on synthetic data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2004.14046">

<!--Generated on Sun Mar 17 06:28:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Reducing catastrophic forgetting with learning on synthetic data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wojciech Masarczyk
<br class="ltx_break">Institute of Theoretical and Applied Informatics,
<br class="ltx_break">Polish Academy of Sciences
<br class="ltx_break">Tooploox
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">wojciech.masarczyk@gmail.com</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ivona Tautkute
<br class="ltx_break">Polish-Japanese Academy of Information Technology 
<br class="ltx_break">Tooploox
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ivona.tautkute@tooploox.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Catastrophic forgetting is a problem caused by neural networks’ inability to learn data in sequence. After learning two tasks in sequence, performance on the first one drops significantly. This is a serious disadvantage that prevents many deep learning applications to real-life problems where not all object classes are known beforehand; or change in data requires adjustments to the model. To reduce this problem we investigate the use of synthetic data, namely we answer a question: Is it possible to generate such data synthetically which learned in sequence does not result in catastrophic forgetting? We propose a method to generate such data in two-step optimisation process via meta-gradients. Our experimental results on Split-MNIST dataset show that training a model on such synthetic data in sequence does not result in catastrophic forgetting. We also show that our method of generating data is robust to different learning scenarios.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2004.14046/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synthetic data created from generator is divided into five tasks according to classes and learner (green) learns tasks sequentially. The same procedure is applied to learner with real data (red). The right plot shows that accuracy at the end of each task does not decrease on learned data in contrast to real data where it deteriorates sharply.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep learning methods have succeeded in many different domains such as: scene understanding, image generation, natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. While deep learning methods differ in architecture choice, objective function or optimization strategy, they all assume that the training data is independent and identically distributed (i.i.d).
Methods built on this assumption are effective for fixed environments with stationary data distributions – where tasks to be solved do not change over time or classes present in the dataset are known from the beginning. However, in most real-life scenarios this assumption is violated and there is a need for methods that are able to handle such cases.
Among many examples of such scenarios, a few can be highlighted: new object class is introduced, however the dataset used to train the baseline model is no longer available; the data characteristics seem to change seasonally and model needs to change its predictions accordingly to these trends.
Continual learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is a paradigm where data is presented sequentially to the algorithm without the ability to manipulate this sequence. Additionally, there is no assumption about the structure of the sequence. A successful continual learning algorithm needs to be able to learn a
growing number of tasks, be resistant to catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and be able to adapt do distribution shifts. The memory and computational requirements of such algorithm should scale reasonably with the incoming data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although the problem of continual learning is known for many years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, only recently has the field gained significant traction and many interesting ideas have been proposed.
Most of continual learning contributions can be divided into three categories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>: optimization, architecture and rehersal. Methods based on optimization modifications usually add additional regularization terms to objective function to dampen catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Second category gathers methods that propose various architectural modifications e.g. Progressive Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> where increasing capacity is obtained by initialising new network for each task. The last category – rehersal based methods – consists of methods that assume life-long presence of a subset of historical data that can be re-used to retain knowledge about past tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This work proposes a new data-driven path that is orthogonal to existing approaches. Specifically, we would like to explore the possibility of creating input data artificially in a coordinated manner in such a way that it reduces the catastrophic forgetting phenomena. We achieve this by combining two separate neural networks connected by two-step optimisation.
We use generative model to create synthetic dataset and form a sequence of tasks to evaluate learner model in continual learning scenario. The sequence of synthetic tasks is used to train the learner network. Then, the learner network is evaluated on real data. The loss obtained on real data is used to tune the parameters of the generative network. In the following step, the learning network is replaced with a new one.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Differently from existing approaches, our method is independent of training method and task and it can be easily incorporated to above-mentioned strategies providing additional gains.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">One line of research for continual learning focuses on optimization process. It draws inspiration from the biological phenomena known as synaptic plasticity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. It assumes that weights (connections) that are important for particular task become less plastic in order to retain the desired performance on previous tasks. An example of such approach is Elastic Weight Consolidation (EWC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, where regularisation term based on Fisher Information matrix is used to slow down the change of important weights. However accumulation of these constrains prevents network from learning longer sequences of tasks.
Another optimization based method is Learning without Forgetting (LwF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. It tries to retain the knowledge of previous tasks by optimizing linear combination of current task loss and knowledge distillation loss. LwF is conceptually simple method that benefits from knowledge distillation phenomenon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The downside of such approach is that applying LwF requires additional memory and computation resources for each optimization step.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Methods based on architectural modifications allow to dynamically expand/shrink networks, select sub-networks, freeze weights or create additional networks to preserve knowledge. Authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> propose algorithm that for each new task creates a separate network (a column) that is trained to solve particular task. Additionally, connections between previous columns and the current column are learned to enable forward transfer of knowledge. This algorithm avoids catastrophic forgetting completely and enables effective transfer learning. However the computational cost of this approach is prohibitive for longer sequences of tasks.
Other methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> address the problem of computational cost by expanding single layers/neurons instead of whole networks, however these methods has less capacity to solve upcoming tasks.
Different approaches that modify architectures are based on selecting sub-networks used for solving current task in such a way that only a fraction of network’s parameters relevant to current task is changed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The challenge here is to
balance the number of frozen and active weights in such way that network is still able to learn new tasks and preserve current knowledge.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Rehearsal methods are based on the concept of memory replay. It is assumed that subset of previously processed data is stored in memory bank and interleaved with upcoming data in such a way that neural network learns to solve current task in addition to preserving current knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
A naive rehearsal method would be to save random data samples that were
present during training. However such approach is inefficient, since samples are not equally informative, hence the challenge of rehearsal methods is to choose the most representative samples for a given dataset, such that minimum storage is occupied. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, authors apply method of dataset distillation based on meta-gradient optimization to reduce the size of memory bank. It is possible to represent whole class of examples just by storing one carefully-optimized example. Unfortunately, applying this meta-optimization method is computationally exhaustive. The biggest downside of using rehearsal based methods is the need to store the actual data which in some cases can violate data privacy rules or can be computationally prohibitive. To mitigate this issue solution based on Generative Networks was proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Namely, they use dual model architecture composed of learner network and generative network. Role of the generative network is to model data previously experienced by the learner network. Data sampled from the generator network is used as a rehersal data for learner network to reduce the effect of catastrophic forgetting.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Our method is also dual architecture model based on generative network, however the aim of generative network is radically different. In contrast to authors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> we do not aim to capture the statistics of real data, instead we try to generate entirely synthetic data such that when learner does learn on a sequence of such data it does not suffer from catastrophic forgetting.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The main idea of our approach is to generate data samples such that network trained on them in sequence would not suffer from catastrophic forgetting. One of many ways to generate artificial data is to use meta-optimization strategy introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. It is shown that by applying meta-learning it is possible to use gradient optimization both to hyperparameters and to input data. However, this approach is limited to small problems, since each data point must be optimised separately. To overcome this bottleneck, authors of Generative Teaching Networks (GTNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> use generative network to create artificial data samples instead of directly optimizing the data input.
We adopt similar approach in our method, namely, we use generative network – green rectangle ”Generator” in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> – to produce synthetic data from noise vectors sampled from a random distribution. Next, we split the data into separate tasks according to classes and form a continual learning task for the learner network – blue rectangle in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Learner network after completing whole sequence of tasks in evaluated on
real training data. The loss from real data classification after learning all tasks in sequence is then backpropagated to generator network to tune the parameters as shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Our approach is similar to one proposed in work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Using two step meta-learning optimization they try to learn best representation of input data such that the model learned in with standard optimization does not suffer from catastrophic forgetting.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Differently from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, we do not use curriculum based learning as our goal is to have a realistic continual learning scenario where the order of data sequence is not known beforehand. To ensure that the Generator network does not generate data suitable for particular sequence of tasks at each meta-optimization we shuffle order of tasks. Precisely, at each step we generate <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">p</annotation></semantics></math> samples for each class and then randomly create a sequence of binary classification tasks with particular data.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2004.14046/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="87" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Synthetic data from generator is passed to learner where the inner optimization is performed and meta-loss is backpropagated to <math id="S3.F2.2.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S3.F2.2.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><ci id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">\mathcal{G}</annotation></semantics></math>.</figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.3" class="ltx_p">Precisely, let <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S3.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">\mathcal{G}</annotation></semantics></math> be a generative neural network, <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S3.p4.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><ci id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">\mathcal{S}</annotation></semantics></math> a standard convolutional network for classification, <math id="S3.p4.3.m3.3" class="ltx_Math" alttext="\mathbf{t}=(t_{1},t_{2},\ldots t_{n})" display="inline"><semantics id="S3.p4.3.m3.3a"><mrow id="S3.p4.3.m3.3.3" xref="S3.p4.3.m3.3.3.cmml"><mi id="S3.p4.3.m3.3.3.5" xref="S3.p4.3.m3.3.3.5.cmml">𝐭</mi><mo id="S3.p4.3.m3.3.3.4" xref="S3.p4.3.m3.3.3.4.cmml">=</mo><mrow id="S3.p4.3.m3.3.3.3.3" xref="S3.p4.3.m3.3.3.3.4.cmml"><mo stretchy="false" id="S3.p4.3.m3.3.3.3.3.4" xref="S3.p4.3.m3.3.3.3.4.cmml">(</mo><msub id="S3.p4.3.m3.1.1.1.1.1" xref="S3.p4.3.m3.1.1.1.1.1.cmml"><mi id="S3.p4.3.m3.1.1.1.1.1.2" xref="S3.p4.3.m3.1.1.1.1.1.2.cmml">t</mi><mn id="S3.p4.3.m3.1.1.1.1.1.3" xref="S3.p4.3.m3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.p4.3.m3.3.3.3.3.5" xref="S3.p4.3.m3.3.3.3.4.cmml">,</mo><msub id="S3.p4.3.m3.2.2.2.2.2" xref="S3.p4.3.m3.2.2.2.2.2.cmml"><mi id="S3.p4.3.m3.2.2.2.2.2.2" xref="S3.p4.3.m3.2.2.2.2.2.2.cmml">t</mi><mn id="S3.p4.3.m3.2.2.2.2.2.3" xref="S3.p4.3.m3.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.p4.3.m3.3.3.3.3.6" xref="S3.p4.3.m3.3.3.3.4.cmml">,</mo><mrow id="S3.p4.3.m3.3.3.3.3.3" xref="S3.p4.3.m3.3.3.3.3.3.cmml"><mi mathvariant="normal" id="S3.p4.3.m3.3.3.3.3.3.2" xref="S3.p4.3.m3.3.3.3.3.3.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.p4.3.m3.3.3.3.3.3.1" xref="S3.p4.3.m3.3.3.3.3.3.1.cmml">​</mo><msub id="S3.p4.3.m3.3.3.3.3.3.3" xref="S3.p4.3.m3.3.3.3.3.3.3.cmml"><mi id="S3.p4.3.m3.3.3.3.3.3.3.2" xref="S3.p4.3.m3.3.3.3.3.3.3.2.cmml">t</mi><mi id="S3.p4.3.m3.3.3.3.3.3.3.3" xref="S3.p4.3.m3.3.3.3.3.3.3.3.cmml">n</mi></msub></mrow><mo stretchy="false" id="S3.p4.3.m3.3.3.3.3.7" xref="S3.p4.3.m3.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.3b"><apply id="S3.p4.3.m3.3.3.cmml" xref="S3.p4.3.m3.3.3"><eq id="S3.p4.3.m3.3.3.4.cmml" xref="S3.p4.3.m3.3.3.4"></eq><ci id="S3.p4.3.m3.3.3.5.cmml" xref="S3.p4.3.m3.3.3.5">𝐭</ci><vector id="S3.p4.3.m3.3.3.3.4.cmml" xref="S3.p4.3.m3.3.3.3.3"><apply id="S3.p4.3.m3.1.1.1.1.1.cmml" xref="S3.p4.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p4.3.m3.1.1.1.1.1.1.cmml" xref="S3.p4.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.p4.3.m3.1.1.1.1.1.2.cmml" xref="S3.p4.3.m3.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S3.p4.3.m3.1.1.1.1.1.3.cmml" xref="S3.p4.3.m3.1.1.1.1.1.3">1</cn></apply><apply id="S3.p4.3.m3.2.2.2.2.2.cmml" xref="S3.p4.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p4.3.m3.2.2.2.2.2.1.cmml" xref="S3.p4.3.m3.2.2.2.2.2">subscript</csymbol><ci id="S3.p4.3.m3.2.2.2.2.2.2.cmml" xref="S3.p4.3.m3.2.2.2.2.2.2">𝑡</ci><cn type="integer" id="S3.p4.3.m3.2.2.2.2.2.3.cmml" xref="S3.p4.3.m3.2.2.2.2.2.3">2</cn></apply><apply id="S3.p4.3.m3.3.3.3.3.3.cmml" xref="S3.p4.3.m3.3.3.3.3.3"><times id="S3.p4.3.m3.3.3.3.3.3.1.cmml" xref="S3.p4.3.m3.3.3.3.3.3.1"></times><ci id="S3.p4.3.m3.3.3.3.3.3.2.cmml" xref="S3.p4.3.m3.3.3.3.3.3.2">…</ci><apply id="S3.p4.3.m3.3.3.3.3.3.3.cmml" xref="S3.p4.3.m3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.p4.3.m3.3.3.3.3.3.3.1.cmml" xref="S3.p4.3.m3.3.3.3.3.3.3">subscript</csymbol><ci id="S3.p4.3.m3.3.3.3.3.3.3.2.cmml" xref="S3.p4.3.m3.3.3.3.3.3.3.2">𝑡</ci><ci id="S3.p4.3.m3.3.3.3.3.3.3.3.cmml" xref="S3.p4.3.m3.3.3.3.3.3.3.3">𝑛</ci></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.3c">\mathbf{t}=(t_{1},t_{2},\ldots t_{n})</annotation></semantics></math> a sequence of tasks, where each tasks is binary classification task and classes in each task form mutually disjoint sets.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.9" class="ltx_p">The inner training loop consists of sequence of tasks, where generated samples from previous tasks are not replayed once the task is finished. To achieve this, the sequence of tasks <math id="S3.p5.1.m1.3" class="ltx_Math" alttext="\mathbf{t}=(t_{1},t_{2},\ldots t_{n})" display="inline"><semantics id="S3.p5.1.m1.3a"><mrow id="S3.p5.1.m1.3.3" xref="S3.p5.1.m1.3.3.cmml"><mi id="S3.p5.1.m1.3.3.5" xref="S3.p5.1.m1.3.3.5.cmml">𝐭</mi><mo id="S3.p5.1.m1.3.3.4" xref="S3.p5.1.m1.3.3.4.cmml">=</mo><mrow id="S3.p5.1.m1.3.3.3.3" xref="S3.p5.1.m1.3.3.3.4.cmml"><mo stretchy="false" id="S3.p5.1.m1.3.3.3.3.4" xref="S3.p5.1.m1.3.3.3.4.cmml">(</mo><msub id="S3.p5.1.m1.1.1.1.1.1" xref="S3.p5.1.m1.1.1.1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.1.1.1.2" xref="S3.p5.1.m1.1.1.1.1.1.2.cmml">t</mi><mn id="S3.p5.1.m1.1.1.1.1.1.3" xref="S3.p5.1.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.p5.1.m1.3.3.3.3.5" xref="S3.p5.1.m1.3.3.3.4.cmml">,</mo><msub id="S3.p5.1.m1.2.2.2.2.2" xref="S3.p5.1.m1.2.2.2.2.2.cmml"><mi id="S3.p5.1.m1.2.2.2.2.2.2" xref="S3.p5.1.m1.2.2.2.2.2.2.cmml">t</mi><mn id="S3.p5.1.m1.2.2.2.2.2.3" xref="S3.p5.1.m1.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.p5.1.m1.3.3.3.3.6" xref="S3.p5.1.m1.3.3.3.4.cmml">,</mo><mrow id="S3.p5.1.m1.3.3.3.3.3" xref="S3.p5.1.m1.3.3.3.3.3.cmml"><mi mathvariant="normal" id="S3.p5.1.m1.3.3.3.3.3.2" xref="S3.p5.1.m1.3.3.3.3.3.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="S3.p5.1.m1.3.3.3.3.3.1" xref="S3.p5.1.m1.3.3.3.3.3.1.cmml">​</mo><msub id="S3.p5.1.m1.3.3.3.3.3.3" xref="S3.p5.1.m1.3.3.3.3.3.3.cmml"><mi id="S3.p5.1.m1.3.3.3.3.3.3.2" xref="S3.p5.1.m1.3.3.3.3.3.3.2.cmml">t</mi><mi id="S3.p5.1.m1.3.3.3.3.3.3.3" xref="S3.p5.1.m1.3.3.3.3.3.3.3.cmml">n</mi></msub></mrow><mo stretchy="false" id="S3.p5.1.m1.3.3.3.3.7" xref="S3.p5.1.m1.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.3b"><apply id="S3.p5.1.m1.3.3.cmml" xref="S3.p5.1.m1.3.3"><eq id="S3.p5.1.m1.3.3.4.cmml" xref="S3.p5.1.m1.3.3.4"></eq><ci id="S3.p5.1.m1.3.3.5.cmml" xref="S3.p5.1.m1.3.3.5">𝐭</ci><vector id="S3.p5.1.m1.3.3.3.4.cmml" xref="S3.p5.1.m1.3.3.3.3"><apply id="S3.p5.1.m1.1.1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.1.1.1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S3.p5.1.m1.1.1.1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.1.1.1.3">1</cn></apply><apply id="S3.p5.1.m1.2.2.2.2.2.cmml" xref="S3.p5.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p5.1.m1.2.2.2.2.2.1.cmml" xref="S3.p5.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.p5.1.m1.2.2.2.2.2.2.cmml" xref="S3.p5.1.m1.2.2.2.2.2.2">𝑡</ci><cn type="integer" id="S3.p5.1.m1.2.2.2.2.2.3.cmml" xref="S3.p5.1.m1.2.2.2.2.2.3">2</cn></apply><apply id="S3.p5.1.m1.3.3.3.3.3.cmml" xref="S3.p5.1.m1.3.3.3.3.3"><times id="S3.p5.1.m1.3.3.3.3.3.1.cmml" xref="S3.p5.1.m1.3.3.3.3.3.1"></times><ci id="S3.p5.1.m1.3.3.3.3.3.2.cmml" xref="S3.p5.1.m1.3.3.3.3.3.2">…</ci><apply id="S3.p5.1.m1.3.3.3.3.3.3.cmml" xref="S3.p5.1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.p5.1.m1.3.3.3.3.3.3.1.cmml" xref="S3.p5.1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S3.p5.1.m1.3.3.3.3.3.3.2.cmml" xref="S3.p5.1.m1.3.3.3.3.3.3.2">𝑡</ci><ci id="S3.p5.1.m1.3.3.3.3.3.3.3.cmml" xref="S3.p5.1.m1.3.3.3.3.3.3.3">𝑛</ci></apply></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.3c">\mathbf{t}=(t_{1},t_{2},\ldots t_{n})</annotation></semantics></math> must be defined a priori and samples generated by network <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S3.p5.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><ci id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">\mathcal{G}</annotation></semantics></math> are conditioned on the information of particular task. For each task <math id="S3.p5.3.m3.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="S3.p5.3.m3.1a"><msub id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml"><mi id="S3.p5.3.m3.1.1.2" xref="S3.p5.3.m3.1.1.2.cmml">t</mi><mi id="S3.p5.3.m3.1.1.3" xref="S3.p5.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><apply id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p5.3.m3.1.1.1.cmml" xref="S3.p5.3.m3.1.1">subscript</csymbol><ci id="S3.p5.3.m3.1.1.2.cmml" xref="S3.p5.3.m3.1.1.2">𝑡</ci><ci id="S3.p5.3.m3.1.1.3.cmml" xref="S3.p5.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">t_{i}</annotation></semantics></math> the network <math id="S3.p5.4.m4.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S3.p5.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p5.4.m4.1.1" xref="S3.p5.4.m4.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S3.p5.4.m4.1b"><ci id="S3.p5.4.m4.1.1.cmml" xref="S3.p5.4.m4.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m4.1c">\mathcal{G}</annotation></semantics></math> generates two batches of samples <math id="S3.p5.5.m5.2" class="ltx_Math" alttext="\mathbf{x}=\mathcal{G}(\mathbf{z},\mathbf{y_{i_{j}}})" display="inline"><semantics id="S3.p5.5.m5.2a"><mrow id="S3.p5.5.m5.2.2" xref="S3.p5.5.m5.2.2.cmml"><mi id="S3.p5.5.m5.2.2.3" xref="S3.p5.5.m5.2.2.3.cmml">𝐱</mi><mo id="S3.p5.5.m5.2.2.2" xref="S3.p5.5.m5.2.2.2.cmml">=</mo><mrow id="S3.p5.5.m5.2.2.1" xref="S3.p5.5.m5.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p5.5.m5.2.2.1.3" xref="S3.p5.5.m5.2.2.1.3.cmml">𝒢</mi><mo lspace="0em" rspace="0em" id="S3.p5.5.m5.2.2.1.2" xref="S3.p5.5.m5.2.2.1.2.cmml">​</mo><mrow id="S3.p5.5.m5.2.2.1.1.1" xref="S3.p5.5.m5.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.p5.5.m5.2.2.1.1.1.2" xref="S3.p5.5.m5.2.2.1.1.2.cmml">(</mo><mi id="S3.p5.5.m5.1.1" xref="S3.p5.5.m5.1.1.cmml">𝐳</mi><mo id="S3.p5.5.m5.2.2.1.1.1.3" xref="S3.p5.5.m5.2.2.1.1.2.cmml">,</mo><msub id="S3.p5.5.m5.2.2.1.1.1.1" xref="S3.p5.5.m5.2.2.1.1.1.1.cmml"><mi id="S3.p5.5.m5.2.2.1.1.1.1.2" xref="S3.p5.5.m5.2.2.1.1.1.1.2.cmml">𝐲</mi><msub id="S3.p5.5.m5.2.2.1.1.1.1.3" xref="S3.p5.5.m5.2.2.1.1.1.1.3.cmml"><mi id="S3.p5.5.m5.2.2.1.1.1.1.3.2" xref="S3.p5.5.m5.2.2.1.1.1.1.3.2.cmml">𝐢</mi><mi id="S3.p5.5.m5.2.2.1.1.1.1.3.3" xref="S3.p5.5.m5.2.2.1.1.1.1.3.3.cmml">𝐣</mi></msub></msub><mo stretchy="false" id="S3.p5.5.m5.2.2.1.1.1.4" xref="S3.p5.5.m5.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.5.m5.2b"><apply id="S3.p5.5.m5.2.2.cmml" xref="S3.p5.5.m5.2.2"><eq id="S3.p5.5.m5.2.2.2.cmml" xref="S3.p5.5.m5.2.2.2"></eq><ci id="S3.p5.5.m5.2.2.3.cmml" xref="S3.p5.5.m5.2.2.3">𝐱</ci><apply id="S3.p5.5.m5.2.2.1.cmml" xref="S3.p5.5.m5.2.2.1"><times id="S3.p5.5.m5.2.2.1.2.cmml" xref="S3.p5.5.m5.2.2.1.2"></times><ci id="S3.p5.5.m5.2.2.1.3.cmml" xref="S3.p5.5.m5.2.2.1.3">𝒢</ci><interval closure="open" id="S3.p5.5.m5.2.2.1.1.2.cmml" xref="S3.p5.5.m5.2.2.1.1.1"><ci id="S3.p5.5.m5.1.1.cmml" xref="S3.p5.5.m5.1.1">𝐳</ci><apply id="S3.p5.5.m5.2.2.1.1.1.1.cmml" xref="S3.p5.5.m5.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.p5.5.m5.2.2.1.1.1.1.1.cmml" xref="S3.p5.5.m5.2.2.1.1.1.1">subscript</csymbol><ci id="S3.p5.5.m5.2.2.1.1.1.1.2.cmml" xref="S3.p5.5.m5.2.2.1.1.1.1.2">𝐲</ci><apply id="S3.p5.5.m5.2.2.1.1.1.1.3.cmml" xref="S3.p5.5.m5.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p5.5.m5.2.2.1.1.1.1.3.1.cmml" xref="S3.p5.5.m5.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.p5.5.m5.2.2.1.1.1.1.3.2.cmml" xref="S3.p5.5.m5.2.2.1.1.1.1.3.2">𝐢</ci><ci id="S3.p5.5.m5.2.2.1.1.1.1.3.3.cmml" xref="S3.p5.5.m5.2.2.1.1.1.1.3.3">𝐣</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.5.m5.2c">\mathbf{x}=\mathcal{G}(\mathbf{z},\mathbf{y_{i_{j}}})</annotation></semantics></math> for <math id="S3.p5.6.m6.2" class="ltx_Math" alttext="j=1,2" display="inline"><semantics id="S3.p5.6.m6.2a"><mrow id="S3.p5.6.m6.2.3" xref="S3.p5.6.m6.2.3.cmml"><mi id="S3.p5.6.m6.2.3.2" xref="S3.p5.6.m6.2.3.2.cmml">j</mi><mo id="S3.p5.6.m6.2.3.1" xref="S3.p5.6.m6.2.3.1.cmml">=</mo><mrow id="S3.p5.6.m6.2.3.3.2" xref="S3.p5.6.m6.2.3.3.1.cmml"><mn id="S3.p5.6.m6.1.1" xref="S3.p5.6.m6.1.1.cmml">1</mn><mo id="S3.p5.6.m6.2.3.3.2.1" xref="S3.p5.6.m6.2.3.3.1.cmml">,</mo><mn id="S3.p5.6.m6.2.2" xref="S3.p5.6.m6.2.2.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.6.m6.2b"><apply id="S3.p5.6.m6.2.3.cmml" xref="S3.p5.6.m6.2.3"><eq id="S3.p5.6.m6.2.3.1.cmml" xref="S3.p5.6.m6.2.3.1"></eq><ci id="S3.p5.6.m6.2.3.2.cmml" xref="S3.p5.6.m6.2.3.2">𝑗</ci><list id="S3.p5.6.m6.2.3.3.1.cmml" xref="S3.p5.6.m6.2.3.3.2"><cn type="integer" id="S3.p5.6.m6.1.1.cmml" xref="S3.p5.6.m6.1.1">1</cn><cn type="integer" id="S3.p5.6.m6.2.2.cmml" xref="S3.p5.6.m6.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.6.m6.2c">j=1,2</annotation></semantics></math>, where <math id="S3.p5.7.m7.1" class="ltx_Math" alttext="\mathbf{z}" display="inline"><semantics id="S3.p5.7.m7.1a"><mi id="S3.p5.7.m7.1.1" xref="S3.p5.7.m7.1.1.cmml">𝐳</mi><annotation-xml encoding="MathML-Content" id="S3.p5.7.m7.1b"><ci id="S3.p5.7.m7.1.1.cmml" xref="S3.p5.7.m7.1.1">𝐳</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.7.m7.1c">\mathbf{z}</annotation></semantics></math> is a batch of noise vectors sampled from Normal distribution and <math id="S3.p5.8.m8.1" class="ltx_Math" alttext="\mathbf{y_{i_{j}}}" display="inline"><semantics id="S3.p5.8.m8.1a"><msub id="S3.p5.8.m8.1.1" xref="S3.p5.8.m8.1.1.cmml"><mi id="S3.p5.8.m8.1.1.2" xref="S3.p5.8.m8.1.1.2.cmml">𝐲</mi><msub id="S3.p5.8.m8.1.1.3" xref="S3.p5.8.m8.1.1.3.cmml"><mi id="S3.p5.8.m8.1.1.3.2" xref="S3.p5.8.m8.1.1.3.2.cmml">𝐢</mi><mi id="S3.p5.8.m8.1.1.3.3" xref="S3.p5.8.m8.1.1.3.3.cmml">𝐣</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.p5.8.m8.1b"><apply id="S3.p5.8.m8.1.1.cmml" xref="S3.p5.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p5.8.m8.1.1.1.cmml" xref="S3.p5.8.m8.1.1">subscript</csymbol><ci id="S3.p5.8.m8.1.1.2.cmml" xref="S3.p5.8.m8.1.1.2">𝐲</ci><apply id="S3.p5.8.m8.1.1.3.cmml" xref="S3.p5.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.p5.8.m8.1.1.3.1.cmml" xref="S3.p5.8.m8.1.1.3">subscript</csymbol><ci id="S3.p5.8.m8.1.1.3.2.cmml" xref="S3.p5.8.m8.1.1.3.2">𝐢</ci><ci id="S3.p5.8.m8.1.1.3.3.cmml" xref="S3.p5.8.m8.1.1.3.3">𝐣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.8.m8.1c">\mathbf{y_{i_{j}}}</annotation></semantics></math> is a class indicator for task <math id="S3.p5.9.m9.1" class="ltx_Math" alttext="\mathbf{t_{i}}" display="inline"><semantics id="S3.p5.9.m9.1a"><msub id="S3.p5.9.m9.1.1" xref="S3.p5.9.m9.1.1.cmml"><mi id="S3.p5.9.m9.1.1.2" xref="S3.p5.9.m9.1.1.2.cmml">𝐭</mi><mi id="S3.p5.9.m9.1.1.3" xref="S3.p5.9.m9.1.1.3.cmml">𝐢</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.9.m9.1b"><apply id="S3.p5.9.m9.1.1.cmml" xref="S3.p5.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p5.9.m9.1.1.1.cmml" xref="S3.p5.9.m9.1.1">subscript</csymbol><ci id="S3.p5.9.m9.1.1.2.cmml" xref="S3.p5.9.m9.1.1.2">𝐭</ci><ci id="S3.p5.9.m9.1.1.3.cmml" xref="S3.p5.9.m9.1.1.3">𝐢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.9.m9.1c">\mathbf{t_{i}}</annotation></semantics></math>.
Note that generator networks has access to class indicators since we aim to learn in continual learning scenario only the learner network.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.8" class="ltx_p">Neural network <math id="S3.p6.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S3.p6.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">\mathcal{S}</annotation></semantics></math> learns sequentially on following tasks using standard SGD optimizer with learning rate and momentum optimized through meta-gradients. At the end of the sequence <math id="S3.p6.2.m2.1" class="ltx_Math" alttext="\mathbf{t}" display="inline"><semantics id="S3.p6.2.m2.1a"><mi id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">𝐭</mi><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><ci id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">𝐭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">\mathbf{t}</annotation></semantics></math> network <math id="S3.p6.3.m3.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S3.p6.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><ci id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">\mathcal{S}</annotation></semantics></math> is evaluated on real dataset (<math id="S3.p6.4.m4.2" class="ltx_Math" alttext="\mathbf{x_{r}},\mathbf{y_{r}}" display="inline"><semantics id="S3.p6.4.m4.2a"><mrow id="S3.p6.4.m4.2.2.2" xref="S3.p6.4.m4.2.2.3.cmml"><msub id="S3.p6.4.m4.1.1.1.1" xref="S3.p6.4.m4.1.1.1.1.cmml"><mi id="S3.p6.4.m4.1.1.1.1.2" xref="S3.p6.4.m4.1.1.1.1.2.cmml">𝐱</mi><mi id="S3.p6.4.m4.1.1.1.1.3" xref="S3.p6.4.m4.1.1.1.1.3.cmml">𝐫</mi></msub><mo id="S3.p6.4.m4.2.2.2.3" xref="S3.p6.4.m4.2.2.3.cmml">,</mo><msub id="S3.p6.4.m4.2.2.2.2" xref="S3.p6.4.m4.2.2.2.2.cmml"><mi id="S3.p6.4.m4.2.2.2.2.2" xref="S3.p6.4.m4.2.2.2.2.2.cmml">𝐲</mi><mi id="S3.p6.4.m4.2.2.2.2.3" xref="S3.p6.4.m4.2.2.2.2.3.cmml">𝐫</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.2b"><list id="S3.p6.4.m4.2.2.3.cmml" xref="S3.p6.4.m4.2.2.2"><apply id="S3.p6.4.m4.1.1.1.1.cmml" xref="S3.p6.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.p6.4.m4.1.1.1.1.1.cmml" xref="S3.p6.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.p6.4.m4.1.1.1.1.2.cmml" xref="S3.p6.4.m4.1.1.1.1.2">𝐱</ci><ci id="S3.p6.4.m4.1.1.1.1.3.cmml" xref="S3.p6.4.m4.1.1.1.1.3">𝐫</ci></apply><apply id="S3.p6.4.m4.2.2.2.2.cmml" xref="S3.p6.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.p6.4.m4.2.2.2.2.1.cmml" xref="S3.p6.4.m4.2.2.2.2">subscript</csymbol><ci id="S3.p6.4.m4.2.2.2.2.2.cmml" xref="S3.p6.4.m4.2.2.2.2.2">𝐲</ci><ci id="S3.p6.4.m4.2.2.2.2.3.cmml" xref="S3.p6.4.m4.2.2.2.2.3">𝐫</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.2c">\mathbf{x_{r}},\mathbf{y_{r}}</annotation></semantics></math>) obtaining meta-loss as shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This meta-loss is backpropagated through all training inner-loops of model <math id="S3.p6.5.m5.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S3.p6.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.5.m5.1.1" xref="S3.p6.5.m5.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.p6.5.m5.1b"><ci id="S3.p6.5.m5.1.1.cmml" xref="S3.p6.5.m5.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.5.m5.1c">\mathcal{S}</annotation></semantics></math> to optimize network <math id="S3.p6.6.m6.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S3.p6.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.6.m6.1.1" xref="S3.p6.6.m6.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S3.p6.6.m6.1b"><ci id="S3.p6.6.m6.1.1.cmml" xref="S3.p6.6.m6.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.6.m6.1c">\mathcal{G}</annotation></semantics></math>.
Parameters <math id="S3.p6.7.m7.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.p6.7.m7.1a"><mi id="S3.p6.7.m7.1.1" xref="S3.p6.7.m7.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.p6.7.m7.1b"><ci id="S3.p6.7.m7.1.1.cmml" xref="S3.p6.7.m7.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.7.m7.1c">\theta</annotation></semantics></math> of network <math id="S3.p6.8.m8.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S3.p6.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.8.m8.1.1" xref="S3.p6.8.m8.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S3.p6.8.m8.1b"><ci id="S3.p6.8.m8.1.1.cmml" xref="S3.p6.8.m8.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.8.m8.1c">\mathcal{G}</annotation></semantics></math> are updated according to the equation:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\theta=\theta-\eta\nabla_{\theta}\mathcal{L}(\mathcal{S}(\mathbf{x_{r}};\mathbf{w_{m}}),\mathbf{y_{r}})," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml">θ</mi><mo id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.4" xref="S3.E1.m1.1.1.1.1.2.4.cmml">θ</mi><mo id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.4" xref="S3.E1.m1.1.1.1.1.2.2.4.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.5" xref="S3.E1.m1.1.1.1.1.2.2.5.cmml"><msub id="S3.E1.m1.1.1.1.1.2.2.5.1" xref="S3.E1.m1.1.1.1.1.2.2.5.1.cmml"><mo rspace="0.167em" id="S3.E1.m1.1.1.1.1.2.2.5.1.2" xref="S3.E1.m1.1.1.1.1.2.2.5.1.2.cmml">∇</mo><mi id="S3.E1.m1.1.1.1.1.2.2.5.1.3" xref="S3.E1.m1.1.1.1.1.2.2.5.1.3.cmml">θ</mi></msub><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.2.2.5.2" xref="S3.E1.m1.1.1.1.1.2.2.5.2.cmml">ℒ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.3a" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.4.cmml">𝒮</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐱</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">𝐫</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">;</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">𝐰</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">𝐦</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.5" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.2.2.2.2.4" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.cmml">𝐲</mi><mi id="S3.E1.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml">𝐫</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.2.2.2.5" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"></eq><ci id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4">𝜃</ci><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><minus id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3"></minus><ci id="S3.E1.m1.1.1.1.1.2.4.cmml" xref="S3.E1.m1.1.1.1.1.2.4">𝜃</ci><apply id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2"><times id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3"></times><ci id="S3.E1.m1.1.1.1.1.2.2.4.cmml" xref="S3.E1.m1.1.1.1.1.2.2.4">𝜂</ci><apply id="S3.E1.m1.1.1.1.1.2.2.5.cmml" xref="S3.E1.m1.1.1.1.1.2.2.5"><apply id="S3.E1.m1.1.1.1.1.2.2.5.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.5.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.5.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.5.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.5.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.5.1.2">∇</ci><ci id="S3.E1.m1.1.1.1.1.2.2.5.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.5.1.3">𝜃</ci></apply><ci id="S3.E1.m1.1.1.1.1.2.2.5.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.5.2">ℒ</ci></apply><interval closure="open" id="S3.E1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.4">𝒮</ci><list id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐱</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝐫</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.2">𝐰</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.2.2.3">𝐦</ci></apply></list></apply><apply id="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.2">𝐲</ci><ci id="S3.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.3">𝐫</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\theta=\theta-\eta\nabla_{\theta}\mathcal{L}(\mathcal{S}(\mathbf{x_{r}};\mathbf{w_{m}}),\mathbf{y_{r}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p6.14" class="ltx_p">where <math id="S3.p6.9.m1.1" class="ltx_Math" alttext="\mathbf{w_{m}}" display="inline"><semantics id="S3.p6.9.m1.1a"><msub id="S3.p6.9.m1.1.1" xref="S3.p6.9.m1.1.1.cmml"><mi id="S3.p6.9.m1.1.1.2" xref="S3.p6.9.m1.1.1.2.cmml">𝐰</mi><mi id="S3.p6.9.m1.1.1.3" xref="S3.p6.9.m1.1.1.3.cmml">𝐦</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.9.m1.1b"><apply id="S3.p6.9.m1.1.1.cmml" xref="S3.p6.9.m1.1.1"><csymbol cd="ambiguous" id="S3.p6.9.m1.1.1.1.cmml" xref="S3.p6.9.m1.1.1">subscript</csymbol><ci id="S3.p6.9.m1.1.1.2.cmml" xref="S3.p6.9.m1.1.1.2">𝐰</ci><ci id="S3.p6.9.m1.1.1.3.cmml" xref="S3.p6.9.m1.1.1.3">𝐦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.9.m1.1c">\mathbf{w_{m}}</annotation></semantics></math> are parameters of the network <math id="S3.p6.10.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S3.p6.10.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.10.m2.1.1" xref="S3.p6.10.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.p6.10.m2.1b"><ci id="S3.p6.10.m2.1.1.cmml" xref="S3.p6.10.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.10.m2.1c">\mathcal{S}</annotation></semantics></math> after <math id="S3.p6.11.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.p6.11.m3.1a"><mi id="S3.p6.11.m3.1.1" xref="S3.p6.11.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p6.11.m3.1b"><ci id="S3.p6.11.m3.1.1.cmml" xref="S3.p6.11.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.11.m3.1c">m</annotation></semantics></math> optimization steps, <math id="S3.p6.12.m4.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S3.p6.12.m4.1a"><mi id="S3.p6.12.m4.1.1" xref="S3.p6.12.m4.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S3.p6.12.m4.1b"><ci id="S3.p6.12.m4.1.1.cmml" xref="S3.p6.12.m4.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.12.m4.1c">\eta</annotation></semantics></math> is fixed learning rate, <math id="S3.p6.13.m5.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.p6.13.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p6.13.m5.1.1" xref="S3.p6.13.m5.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.p6.13.m5.1b"><ci id="S3.p6.13.m5.1.1.cmml" xref="S3.p6.13.m5.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.13.m5.1c">\mathcal{L}</annotation></semantics></math> is a cross entropy loss function, <math id="S3.p6.14.m6.2" class="ltx_Math" alttext="\mathbf{x_{r}},\mathbf{y_{r}}" display="inline"><semantics id="S3.p6.14.m6.2a"><mrow id="S3.p6.14.m6.2.2.2" xref="S3.p6.14.m6.2.2.3.cmml"><msub id="S3.p6.14.m6.1.1.1.1" xref="S3.p6.14.m6.1.1.1.1.cmml"><mi id="S3.p6.14.m6.1.1.1.1.2" xref="S3.p6.14.m6.1.1.1.1.2.cmml">𝐱</mi><mi id="S3.p6.14.m6.1.1.1.1.3" xref="S3.p6.14.m6.1.1.1.1.3.cmml">𝐫</mi></msub><mo id="S3.p6.14.m6.2.2.2.3" xref="S3.p6.14.m6.2.2.3.cmml">,</mo><msub id="S3.p6.14.m6.2.2.2.2" xref="S3.p6.14.m6.2.2.2.2.cmml"><mi id="S3.p6.14.m6.2.2.2.2.2" xref="S3.p6.14.m6.2.2.2.2.2.cmml">𝐲</mi><mi id="S3.p6.14.m6.2.2.2.2.3" xref="S3.p6.14.m6.2.2.2.2.3.cmml">𝐫</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.14.m6.2b"><list id="S3.p6.14.m6.2.2.3.cmml" xref="S3.p6.14.m6.2.2.2"><apply id="S3.p6.14.m6.1.1.1.1.cmml" xref="S3.p6.14.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.p6.14.m6.1.1.1.1.1.cmml" xref="S3.p6.14.m6.1.1.1.1">subscript</csymbol><ci id="S3.p6.14.m6.1.1.1.1.2.cmml" xref="S3.p6.14.m6.1.1.1.1.2">𝐱</ci><ci id="S3.p6.14.m6.1.1.1.1.3.cmml" xref="S3.p6.14.m6.1.1.1.1.3">𝐫</ci></apply><apply id="S3.p6.14.m6.2.2.2.2.cmml" xref="S3.p6.14.m6.2.2.2.2"><csymbol cd="ambiguous" id="S3.p6.14.m6.2.2.2.2.1.cmml" xref="S3.p6.14.m6.2.2.2.2">subscript</csymbol><ci id="S3.p6.14.m6.2.2.2.2.2.cmml" xref="S3.p6.14.m6.2.2.2.2.2">𝐲</ci><ci id="S3.p6.14.m6.2.2.2.2.3.cmml" xref="S3.p6.14.m6.2.2.2.2.3">𝐫</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.14.m6.2c">\mathbf{x_{r}},\mathbf{y_{r}}</annotation></semantics></math> are real data samples and labels respectively.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To test our hypothesis we use popular continual learning benchmark Split-MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. In first experiment, we use 5-fold split with two classes for each task to create a moderately difficult sequence of tasks. Network <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\mathcal{G}</annotation></semantics></math> generates 250 samples per each class. During inner optimisation learner network is optimized on batch size formed with 40 generated images (20 samples per class drawn randomly from the pool of 250 samples per class). We train the learner network on each task for 5 inner steps with batch size 40. Once the task is over, samples from this task are not shown to the network to the end of training. At test time, after learning on each task the network is evaluated on part of a test set composed of classes seen in previous taks. Both networks are simple convolutional neural networks with two convolutional layers with addition of one and two fully connected layers for classification and generative network respectively. Each layer is followed by a batch normalisation layer.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">As a baseline to compare with, we use simple fully connected network proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (’MLP’ – red – in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4 Experiments ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). To further investigate the impact of generated data we use the same network architectures and optimizer settings with learning rate and momentum optimized with by a meta learning process as described in Section <a href="#S3" title="3 Method ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> but for optimizing the learner network we use real data (’Real Data’ – yellow – in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4 Experiments ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
We also compare our results with GAN-based data samples. In this scenario we follow the setting of ’Real Data’ scenario except for the source of data. We use Conditional-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to model the original data distribution and then sample 250 samples per each class (’GAN based’ – blue – in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4 Experiments ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We implement experiments in PyTorch library, which is well suited for computing
higher-order gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2004.14046/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="115" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Samples generated by network <math id="S4.F3.2.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.F3.2.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S4.F3.2.m1.1.1" xref="S4.F3.2.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.F3.2.m1.1c"><ci id="S4.F3.2.m1.1.1.cmml" xref="S4.F3.2.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.m1.1d">\mathcal{G}</annotation></semantics></math> at the end of meta-optimisation. Starting from zero (leftmost), each sample to the right represents the following class.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2004.14046/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overall accuracy measured on test data subset. After learning each task, test data subset is made of samples only
from classes seen during recent and previous tasks.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2004.14046/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="231" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overall accuracy measured on test set after learning network
<math id="S4.F5.3.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S4.F5.3.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S4.F5.3.m1.1.1" xref="S4.F5.3.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.F5.3.m1.1c"><ci id="S4.F5.3.m1.1.1.cmml" xref="S4.F5.3.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.3.m1.1d">\mathcal{S}</annotation></semantics></math> with synthetic data for <math id="S4.F5.4.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.F5.4.m2.1b"><mi id="S4.F5.4.m2.1.1" xref="S4.F5.4.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.F5.4.m2.1c"><ci id="S4.F5.4.m2.1.1.cmml" xref="S4.F5.4.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.4.m2.1d">x</annotation></semantics></math> inner steps on each task. </figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Results</span> – obtained results support our hypothesis, that it is possible to generate synthetic data such that, even if networks learns this data in sequence (one time per sample), the learning process does not result in castastrophic forgetting.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4 Experiments ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows how learning on synthetic data in sequence results in less catastrophic forgetting compared to learning on a sequence of real data samples. Note that additional performance could be gained with careful hyperparameter tuning, however we did not want to compete for best performance and rather show the potential of this approach.
Higher accuracy of ’Real data’ scenario over ’MLP’ can be attributed to the effectiveness of optimised learning rate and momentum parameters, however the main advantage comes from using meta learned data samples. Results obtained with data generated with GAN are almost identical to ones obtained with real data. This result is expected as the data modeled by a GAN resembles original data closely.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">An example batch of generated samples is shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Experiments ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The samples are ordered according to classes (starting from 0). In contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> the data samples are abstract blobs, rather than interpretable images. We verify experimentally that the reason for the lack of structure in generated samples is the lack of curriculum learning in our scenario. We skip it intentionally to provide more realistic continual learning scenario for the learner network.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.4" class="ltx_p">Fig. <a href="#S4.F5" title="Figure 5 ‣ 4 Experiments ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the impact of change of learning scenario of network <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S4.p7.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><ci id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">\mathcal{S}</annotation></semantics></math> <span id="S4.p7.4.1" class="ltx_text ltx_font_italic">after</span> network <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p7.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><ci id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">\mathcal{G}</annotation></semantics></math> is trained.
In this experiment data generated by a network <math id="S4.p7.3.m3.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p7.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><ci id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">\mathcal{G}</annotation></semantics></math> in first experiment is used. Here, we investigate how the final accuracy after learning five consecutive tasks changes with the number of inner optimization steps. Note that <math id="S4.p7.4.m4.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p7.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p7.4.m4.1.1" xref="S4.p7.4.m4.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p7.4.m4.1b"><ci id="S4.p7.4.m4.1.1.cmml" xref="S4.p7.4.m4.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m4.1c">\mathcal{G}</annotation></semantics></math> was optimised to create samples that are robust to catastrophic forgetting with inner optimization loop of 5 steps. As we can see, in case of longer learning horizon, network learned on synthetic (green plot Fig. <a href="#S4.F5" title="Figure 5 ‣ 4 Experiments ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) data suffers significantly less than the same network learned on real data (yellow plot Fig. <a href="#S4.F5" title="Figure 5 ‣ 4 Experiments ‣ Reducing catastrophic forgetting with learning on synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). Even though accuracy of the networks drops with increasing number of inner steps, the drop is smoother in case of synthetic data.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The aim of this work was to answer a question, whether it is possible to create data that would dampen the effect of catastrophic forgetting.
Experiments show that this hypothesis is true – it is possible to generate such samples, however usually they do not visually resemble real data. Surprisingly, even applying the method alone can result in high performing network. Additional interesting advantage of this synthetic data is the robustness to changes of inner optimisation parameters – increasing 15-fold size of a batch and length on training still results in compelling performance.
We believe that our experiments open a new and exciting path in continual learning research. As a future work we plan to adjust current method to datasets of higher complexity and test its effectiveness in online learning scenario.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Authors would like to thank Petr Hlubuček and GoodAI for publishing the code at <a target="_blank" href="https://github.com/GoodAI/GTN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/GoodAI/GTN</a>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Joseph Cichon and Wen-Biao Gan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Branch-specific dendritic ca2+ spikes cause persistent synaptic
plasticity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nature</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 520, 03 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">BERT: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 4171–4186,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Siavash Golkar, Michael Kagan, and Kyunghyun Cho.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Continual learning via neural pruning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, abs/1903.04476, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov,
Franziska Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Generalized inner loop meta-learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.01727</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Tyler L. Hayes, Nathan D. Cahill, and Christopher Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Memory efficient experience replay for streaming learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation, ICRA
2019, Montreal, QC, Canada, May 20-24, 2019</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 9769–9776. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Distilling the knowledge in a neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS Deep Learning and Representation Learning Workshop</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">,
2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Khurram Javed and Martha White.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Meta-learning representations for continual learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural
Information Processing Systems 32</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 1820–1830. Curran Associates,
Inc., 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L. Hayes, and Christopher
Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Measuring catastrophic forgetting in neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In Sheila A. McIlraith and Kilian Q. Weinberger, editors, </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, (AAAI-18), the 30th innovative Applications of Artificial
Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances
in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 3390–3398. AAAI Press, 2018.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
Raia Hadsell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Overcoming catastrophic forgetting in neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the National Academy of Sciences</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">,
114(13):3521–3526, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Overcoming catastrophic forgetting by incremental moment matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S.
Vishwanathan, and R. Garnett, editors, </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information
Processing Systems 30</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 4652–4662. Curran Associates, Inc., 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Z. Li and D. Hoiem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Learning without forgetting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">,
40(12):2935–2947, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Vincenzo Lomonaco.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Continual Learning with Deep Architectures</span><span id="bib.bib12.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">PhD thesis, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
David Lopez-Paz and Marc' Aurelio Ranzato.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Gradient episodic memory for continual learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S.
Vishwanathan, and R. Garnett, editors, </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information
Processing Systems 30</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 6467–6476. Curran Associates, Inc., 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Dougal Maclaurin, David Duvenaud, and Ryan P. Adams.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Gradient-based hyperparameter optimization through reversible
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 32nd International Conference on
International Conference on Machine Learning - Volume 37</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, ICML’15, page
2113–2122. JMLR.org, 2015.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Arun Mallya, Dillon Davis, and Svetlana Lazebnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Piggyback: Adapting a single network to multiple tasks by learning to
mask weights.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
Weiss, editors, </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision - ECCV 2018 - 15th European
Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part IV</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">,
volume 11208 of </span><span id="bib.bib15.6.4" class="ltx_text ltx_font_italic" style="font-size:90%;">Lecture Notes in Computer Science</span><span id="bib.bib15.7.5" class="ltx_text" style="font-size:90%;">, pages 72–88.
Springer, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Arun Mallya and Svetlana Lazebnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Packnet: Adding multiple tasks to a single network by iterative
pruning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages
7765–7773. IEEE Computer Society, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Michael Mccloskey and Neil J. Cohen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Catastrophic interference in connectionist networks: The sequential
learning problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The Psychology of Learning and Motivation</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 24:104–169, 1989.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Mehdi Mirza and Simon Osindero.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Conditional generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, abs/1411.1784, 2014.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Witold Oleszkiewicz, Peter Kairouz, Karol Piczak, Ram Rajagopal, and Tomasz
Trzciński.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Siamese generative adversarial privatizer for biometric data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In C.V. Jawahar, Hongdong Li, Greg Mori, and Konrad Schindler,
editors, </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ACCV 2018</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 482–497, Cham, 2019.
Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan
Wermter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Continual lifelong learning with neural networks: A review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Networks</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 113:54 – 71, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Mark Bishop Ring.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Continual Learning in Reinforcement Environments</span><span id="bib.bib21.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">PhD thesis, USA, 1994.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Progressive neural networks, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Continual learning with deep generative replay.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S.
Vishwanathan, and R. Garnett, editors, </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information
Processing Systems 30</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 2990–2999. Curran Associates, Inc., 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Rupesh K Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and
Jürgen Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Compete to compute.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.
Weinberger, editors, </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems
26</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 2310–2318. Curran Associates, Inc., 2013.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Wojciech Stokowiec, Tomasz Trzcinski, Krzysztof Wołk, Krzysztof Marasek, and
Przemyslaw Rokita.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Shallow reading with deep learning: Predicting popularity of online
content using only its title.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">pages 136–145, 07 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth Stanley, and Jeff
Clune.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Generative teaching networks: Accelerating neural architecture search
by learning to generate synthetic training data, 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Ivona Tautkute, Tomasz Trzciński, Aleksander P. Skorupa, Łukasz Brocki, and
Krzysztof Marasek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Deepstyle: Multimodal search engine for fashion and interior design.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Access</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 7:84613–84628, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Dataset distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, abs/1811.10959, 2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Growing a brain: Fine-tuning by increasing model capacity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, abs/1907.07844, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Chenshen Wu, Luis Herranz, Xialei Liu, yaxing wang, Joost van de Weijer, and
Bogdan Raducanu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Memory replay gans: Learning to generate new categories without
forgetting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing
Systems 31</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 5962–5972. Curran Associates, Inc., 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Lifelong learning with dynamically expandable networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2004.14045" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2004.14046" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2004.14046">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2004.14046" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2004.14047" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 06:28:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
