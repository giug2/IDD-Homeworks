<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.16244] On the Equivalency, Substitutability, and Flexibility of Synthetic Data</title><meta property="og:description" content="We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios.
Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its ef…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On the Equivalency, Substitutability, and Flexibility of Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="On the Equivalency, Substitutability, and Flexibility of Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.16244">

<!--Generated on Fri Apr  5 16:32:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">On the Equivalency, Substitutability, and Flexibility of Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Che-Jui Chang
<br class="ltx_break">Rutgers University
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">chejui.chang@rutgers.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Danrui Li
<br class="ltx_break">Rutgers University
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">danrui.li@rutgers.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Seonghyeon Moon
<br class="ltx_break">Rutgers University
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">sm206@cs.rutgers.edu</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mubbasir Kapadia
<br class="ltx_break">Roblox
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">mkapadia@roblox.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">We study, from an empirical standpoint, the efficacy of synthetic data in real-world scenarios.
Leveraging synthetic data for training perception models has become a key strategy embraced by the community due to its efficiency, scalability, perfect annotations, and low costs.
Despite proven advantages, few studies put their stress on how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can reduce the effort for real-world data collection.
To answer the questions, we systematically investigate several interesting properties of synthetic data – the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up domain gaps.
Leveraging the <span id="id1.1.1" class="ltx_text ltx_font_typewriter">M<sup id="id1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> synthetic data generator, we conduct experiments on DanceTrack and MOT17.
Our results suggest that synthetic data not only enhances model performance but also demonstrates substitutability for real data, with 60% to 80% replacement without performance loss.
In addition, our study of the impact of synthetic data distributions on downstream performance reveals the importance of flexible data generators in narrowing domain gaps for improved model adaptability.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2403.16244/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.4.2.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.2.1" class="ltx_text" style="font-size:90%;">The data generation process of <span id="S0.F1.2.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S0.F1.2.1.1.1" class="ltx_sup"><span id="S0.F1.2.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span>. The data generation process is highly parameterized, enabling the adjustment of resulting synthetic data distributions, as illustrated at the right.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">For the past decade, collecting real-world data with human annotations has been driving the momentum of research advancements in the field of computer vision and machine perception.
Despite the tremendous breakthrough in many tasks, obtaining these datasets with high-quality human annotations remains costly and labor-intensive, thus hindering the development for certain research tasks that require fine-grained annotations, including human pose and shape estimation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, multi-object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, and collective activity understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The adoption of synthetic data from game engines has become an emerging alternative to real-world data, due to its efficiency, flexibility, scalability, and perfect annotations.
Previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> also show the capability of synthetic data in providing diverse and photorealistic images, generating customized datasets for edge cases, improving benchmark performances on downstream datasets, and mitigating ethical risks, for a wide range of vision and perception tasks.
Nonetheless, related studies are rarely focused on the similarity between data distributions, the equivalency of synthetic data to real-world data, the substitutability of synthetic data for real data, and the flexibility of synthetic data generators to close up the domain gap.
Therefore, it remains unclear to the community how to efficiently generate synthetic datasets to solve real-world problems and to what extent synthetic data can benefit real-world tasks.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.2" class="ltx_p">In this work, we aim to study the aforementioned properties of synthetic data, by systematically experimenting with different data sources for training and comparing the performance of models trained on synthetic data to those trained on real-world data.
We adopt the multi-person tracking (MPT) task as the focus of our study and leverage <span id="S1.p3.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S1.p3.1.1.1" class="ltx_sup"><span id="S1.p3.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> as the synthetic data generator.
<span id="S1.p3.2.2" class="ltx_text ltx_font_typewriter">M<sup id="S1.p3.2.2.1" class="ltx_sup"><span id="S1.p3.2.2.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> features multiple semantic groups and produces highly diverse and photorealistic videos with a rich set of annotations suitable for human-centered tasks, including multi-person tracking, group activity recognition, and controllable human group activity generation.
Notably, it offers a flexible data generation pipeline to modify the data distribution by editing the highly parameterized and modularized human groups, enabling us to examine the influence of synthetic data distributions on downstream task performances.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The key contribution of this work lies in the proven efficacies and insights gained from our investigations of the equivalency of synthetic data to real-world data and the substitutability of synthetic data for real data.
We show that synthetic data can not only improve model performance on downstream datasets but also effectively replace up to 80% of MOT17 data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> without compromising performances.
Moreover, our investigation of the impact of synthetic data distributions on downstream model performances highlights the importance of a flexible synthetic data generator to shorten domain gaps, thereby enhancing the adaptability of models trained on synthetic data to real-world scenarios</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Synthetic data plays a vital role in contemporary computer vision and machine learning research, particularly because of the increasing demand driven by large data-hungry models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>.
The community widely acknowledges the benefits of employing synthetic data for training machine learning models, due to its efficiency, enhanced performance, customizability, and cost reduction.
A key characteristic of synthetic data is its task-specific nature, where datasets are tailored to specific research objectives or scenarios.
This customization allows researchers to design data generators to produce synthetic data that closely aligns with the requirements of the downstream experiments, enhancing the performance of machine learning models trained on synthetic data.
For example, SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> was designed for human pose estimation to improve the background and data diversity.
BEDLAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> integrates physically animated clothes into its data generation pipeline for improved performance on human pose and shape estimation.
<span id="S2.p1.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S2.p1.1.1.1" class="ltx_sup"><span id="S2.p1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> leverages the rule-based authoring of human group activities for multi-person and multi-group research.
The same study also shows that datasets such as BEDLAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> and GTA-Humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> are inadequate for multi-person tracking.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Despite these advantages, prior studies have primarily focused on the enhanced performances achieved with synthetic data.
While performance enhancement is crucial, seeking optimality may not always be practical in real-world scenarios.
Understanding the tradeoffs involved in collecting more data and the extent to which it leads to performance improvements is equally important.
Previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> have hinted at the potential for synthetic data to replace certain amounts of real data, yet these properties of synthetic data have not been thoroughly explored.
In this study, we delve into several interesting properties – the equivalency, substitutability, and flexibility of synthetic data.
We demonstrate through experiments how these properties can provide valuable insights for using synthetic data in real-world scenarios.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Overview of <span id="S3.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S3.1.1.1" class="ltx_sup"><span id="S3.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We provide an overview of the data generation process, the flexible parameterization, and the properties of <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S3.p1.1.1.1" class="ltx_sup"><span id="S3.p1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Synthetic Data Generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S3.SS1.p1.1.1.1" class="ltx_sup"><span id="S3.SS1.p1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> is a synthetic data generator built with
Unity Engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>.
It features multiple semantic groups and produces highly diverse and photorealistic videos with a rich set of annotations suitable for human-centered tasks.
The data generator contains 25 scenes, 104 HDRIs,
5 lighting volumes, 2200 human models, 384 animations, and 6 group activities.
<span id="S3.SS1.p1.2.2" class="ltx_text ltx_font_typewriter">M<sup id="S3.SS1.p1.2.2.1" class="ltx_sup"><span id="S3.SS1.p1.2.2.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> is tailored to support multi-person and multi-group research, which effectively enhances the model performances on multi-person tracking and group activity recognition, and enables novel research, controllable group activity generation.
Its data generation process, illustrated in Fig. <a href="#S0.F1" title="Figure 1 ‣ On the Equivalency, Substitutability, and Flexibility of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is procedurally driven and contains a high degree of simulation parameters, including scene parameters like lighting, scenes, and cameras, as well as group parameters such as animations, characters, alignments, and group types.
This level of flexibility and customization is rarely seen in previous synthetic data works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>.
It allows for the adjustment of the simulation parameters within the generator, enabling the customization of synthetic data distributions to match specific research requirements.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implications</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">Despite the promising contributions of <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S3.SS2.p1.1.1.1" class="ltx_sup"><span id="S3.SS2.p1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> to performance enhancement, their study has revealed initial findings on several interesting properties of synthetic data.
First, the variability in synthetic data distributions resulting from the flexible parameterization of <span id="S3.SS2.p1.2.2" class="ltx_text ltx_font_typewriter">M<sup id="S3.SS2.p1.2.2.1" class="ltx_sup"><span id="S3.SS2.p1.2.2.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> presents a unique challenge in ensuring the suitability of synthetic data for downstream tasks.
This challenge is particularly pronounced in multi-person scenarios, which are significantly more complex compared to tasks addressed by previous synthetic datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>.
Second, synthetic data generated by <span id="S3.SS2.p1.3.3" class="ltx_text ltx_font_typewriter">M<sup id="S3.SS2.p1.3.3.1" class="ltx_sup"><span id="S3.SS2.p1.3.3.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> exhibits properties of equivalency and substitutability, with results indicating that synthetic data can effectively replace a substantial portion of real data for multi-person tracking.
Our deep investigation of these properties extends beyond the initial findings and offers valuable insights into the understanding of these synthetic data properties in real-world scenarios.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We evaluate the effectiveness of synthetic data by conducting experiments on multi-person tracking (MPT),
the objective of which is to predict the tracklets of all individual persons given an input video stream.
The tracking performances are measured on two distinct real-world datasets: DanceTrack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> (DT) and MOT17 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>.
The former presents a particularly challenging multi-person tracking scenario characterized by dynamic dance movements with individuals of uniform outfits.
It has a total of 100 videos with over 105K frames.
The latter is a widely used dataset for multi-object tracking, with objects including pedestrians, bicycles, and cars, captured in outdoor scenes.
The main challenges involve crowded scenarios and interruptions in tracking due to dynamic camera movements.
The dataset contains 7 long videos with a total of 11K frames.
</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Preliminaries</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our experiments cover the following properties of synthetic data and research questions:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Equivalency</span> and <span id="S4.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">Substitutability</span>. How much synthetic data is equivalent to real data? In other words, how much real data can be substituted by synthetic data, without sacrificing performance? Answers to the question would help the community understand the practical merits of using synthetic data in reducing the cost of data collection and annotation in real-world scenarios.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Distribution</span> and <span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">Flexibility</span>. How does the distribution of the synthetic data affect the performance of target datasets and can we narrow the domain gaps by adjusting the distribution of synthetic data?
This helps us understand the protocol of designing and selecting useful synthetic data for enhancing downstream task performance in the target domain.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.12.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.6.6.6" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Pretrain </span><math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T1.1.1.1.1.2" class="ltx_text" style="font-size:90%;"> Finetune</span>
</th>
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.2.2.2.2.1" class="ltx_text" style="font-size:90%;">HOTA</span><math id="S4.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.3.3.3.3.1" class="ltx_text" style="font-size:90%;">DetA</span><math id="S4.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.4.4.4.4.1" class="ltx_text" style="font-size:90%;">AssA</span><math id="S4.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.4.4.4.4.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.4.4.4.4.m1.1.1" xref="S4.T1.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.5.5.5.5.1" class="ltx_text" style="font-size:90%;">IDF1</span><math id="S4.T1.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.5.5.5.5.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.5.5.5.5.m1.1.1" xref="S4.T1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.6.6.6.6.1" class="ltx_text" style="font-size:90%;">MOTA</span><math id="S4.T1.6.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.6.6.6.6.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.6.6.6.6.m1.1.1" xref="S4.T1.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
<tr id="S4.T1.7.7.7" class="ltx_tr">
<th id="S4.T1.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.7.7.7.1.1" class="ltx_text" style="font-size:90%;">N/A </span><math id="S4.T1.7.7.7.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.7.7.7.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.1b"><ci id="S4.T1.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T1.7.7.7.1.2" class="ltx_text" style="font-size:90%;"> 100% MOT17</span>
</th>
<th id="S4.T1.7.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.7.7.7.2.1" class="ltx_text" style="font-size:90%;">56.3</span></th>
<th id="S4.T1.7.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.7.7.7.3.1" class="ltx_text" style="font-size:90%;">51.9</span></th>
<th id="S4.T1.7.7.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.7.7.7.4.1" class="ltx_text" style="font-size:90%;">62.1</span></th>
<th id="S4.T1.7.7.7.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.7.7.7.5.1" class="ltx_text" style="font-size:90%;">66.6</span></th>
<th id="S4.T1.7.7.7.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.7.7.7.6.1" class="ltx_text" style="font-size:90%;">56.1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.8.8.8" class="ltx_tr">
<th id="S4.T1.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.8.8.8.1.1" class="ltx_text" style="font-size:90%;">Syn </span><math id="S4.T1.8.8.8.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.8.8.8.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.8.8.8.1.m1.1.1" xref="S4.T1.8.8.8.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.1.m1.1b"><ci id="S4.T1.8.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.8.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T1.8.8.8.1.2" class="ltx_text" style="font-size:90%;"> 20% MOT17</span>
</th>
<td id="S4.T1.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.8.8.8.2.1" class="ltx_text" style="font-size:90%;">51.7</span></td>
<td id="S4.T1.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.8.8.8.3.1" class="ltx_text" style="font-size:90%;">48.0</span></td>
<td id="S4.T1.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.8.8.8.4.1" class="ltx_text" style="font-size:90%;">56.3</span></td>
<td id="S4.T1.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.8.8.8.5.1" class="ltx_text" style="font-size:90%;">62.1</span></td>
<td id="S4.T1.8.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.8.8.8.6.1" class="ltx_text" style="font-size:90%;">54.4</span></td>
</tr>
<tr id="S4.T1.9.9.9" class="ltx_tr">
<th id="S4.T1.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.9.9.9.1.1" class="ltx_text" style="font-size:90%;">Syn </span><math id="S4.T1.9.9.9.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.9.9.9.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.9.9.9.1.m1.1.1" xref="S4.T1.9.9.9.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.1.m1.1b"><ci id="S4.T1.9.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.9.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T1.9.9.9.1.2" class="ltx_text" style="font-size:90%;"> 40% MOT17</span>
</th>
<td id="S4.T1.9.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.9.9.9.2.1" class="ltx_text" style="font-size:90%;">57.4</span></td>
<td id="S4.T1.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.9.9.9.3.1" class="ltx_text" style="font-size:90%;">54.3</span></td>
<td id="S4.T1.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.9.9.9.4.1" class="ltx_text" style="font-size:90%;">61.5</span></td>
<td id="S4.T1.9.9.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.9.9.9.5.1" class="ltx_text" style="font-size:90%;">69.3</span></td>
<td id="S4.T1.9.9.9.6" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.9.9.9.6.1" class="ltx_text" style="font-size:90%;">61.3</span></td>
</tr>
<tr id="S4.T1.10.10.10" class="ltx_tr">
<th id="S4.T1.10.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.10.10.10.1.1" class="ltx_text" style="font-size:90%;">Syn </span><math id="S4.T1.10.10.10.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.10.10.10.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.10.10.10.1.m1.1.1" xref="S4.T1.10.10.10.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.1.m1.1b"><ci id="S4.T1.10.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.10.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T1.10.10.10.1.2" class="ltx_text" style="font-size:90%;"> 60% MOT17</span>
</th>
<td id="S4.T1.10.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.10.10.10.2.1" class="ltx_text" style="font-size:90%;">59.7</span></td>
<td id="S4.T1.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.10.10.10.3.1" class="ltx_text" style="font-size:90%;">57.1</span></td>
<td id="S4.T1.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.10.10.10.4.1" class="ltx_text" style="font-size:90%;">63.0</span></td>
<td id="S4.T1.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.10.10.10.5.1" class="ltx_text" style="font-size:90%;">70.0</span></td>
<td id="S4.T1.10.10.10.6" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.10.10.10.6.1" class="ltx_text" style="font-size:90%;">62.9</span></td>
</tr>
<tr id="S4.T1.11.11.11" class="ltx_tr">
<th id="S4.T1.11.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.11.11.11.1.1" class="ltx_text" style="font-size:90%;">Syn </span><math id="S4.T1.11.11.11.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.11.11.11.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.11.11.11.1.m1.1.1" xref="S4.T1.11.11.11.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.1.m1.1b"><ci id="S4.T1.11.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.11.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T1.11.11.11.1.2" class="ltx_text" style="font-size:90%;"> 80% MOT17</span>
</th>
<td id="S4.T1.11.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.11.11.11.2.1" class="ltx_text" style="font-size:90%;">60.6</span></td>
<td id="S4.T1.11.11.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.11.11.11.3.1" class="ltx_text" style="font-size:90%;">58.1</span></td>
<td id="S4.T1.11.11.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.11.11.11.4.1" class="ltx_text" style="font-size:90%;">63.8</span></td>
<td id="S4.T1.11.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.11.11.11.5.1" class="ltx_text" style="font-size:90%;">73.0</span></td>
<td id="S4.T1.11.11.11.6" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.11.11.11.6.1" class="ltx_text" style="font-size:90%;">67.1</span></td>
</tr>
<tr id="S4.T1.12.12.12" class="ltx_tr">
<th id="S4.T1.12.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S4.T1.12.12.12.1.1" class="ltx_text" style="font-size:90%;">Syn </span><math id="S4.T1.12.12.12.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T1.12.12.12.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.12.12.12.1.m1.1.1" xref="S4.T1.12.12.12.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.1.m1.1b"><ci id="S4.T1.12.12.12.1.m1.1.1.cmml" xref="S4.T1.12.12.12.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T1.12.12.12.1.2" class="ltx_text" style="font-size:90%;"> 100% MOT17</span>
</th>
<td id="S4.T1.12.12.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.12.12.12.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">63.7</span></td>
<td id="S4.T1.12.12.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.12.12.12.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">61.6</span></td>
<td id="S4.T1.12.12.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.12.12.12.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.9</span></td>
<td id="S4.T1.12.12.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.12.12.12.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">74.7</span></td>
<td id="S4.T1.12.12.12.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S4.T1.12.12.12.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">68.7</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Tracking results on MOT17 with different amounts of real data.</figcaption>
</figure>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2403.16244/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Plot showing tracking performances with different percentages of MOT17 data for fine-tuning. The dotted line represents the performance when the model is trained solely on 100% real data. </span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To answer the first question, we conduct the experiment on MOT17 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> dataset.
We first train the model with 100% real data as a comparison baseline.
Then we use a constant amount of synthetic data for pretraining and then gradually reduce the amount of real data used for finetuning.
This enables us to not only observe the changes in tracking performances but also identify how much real data can be replaced by synthetic data, by comparing the performances with the baseline.
We follow the protocol of <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S4.SS1.p2.1.1.1" class="ltx_sup"><span id="S4.SS1.p2.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> and use the same portion of synthetic data in this experiment.
For fair comparisons, we pre-trained the model on synthetic data for 5 epochs and then fine-tuned it for 20 epochs for all conditions.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.2" class="ltx_p">For the second research question, we extend the original MPT experiments on DanceTrack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> in <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S4.SS1.p3.1.1.1" class="ltx_sup"><span id="S4.SS1.p3.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> by adjusting the synthetic data distributions.
Specifically, we use the <span id="S4.SS1.p3.2.2" class="ltx_text ltx_font_typewriter">M<sup id="S4.SS1.p3.2.2.1" class="ltx_sup"><span id="S4.SS1.p3.2.2.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> data generator to prepare various datasets with different combinations of human groups.
A total of 5 synthetic datasets are constructed, including 1K video clips of a single “Dance” group (“D”), 1.5K videos with a “walk” group and a “run” group simulated at the same time (denoted as “WalkRun” or “WR”), 2.5K videos of “WR”+“D”, 6K videos of all “Single-group” activities, and 9K videos of all “Multi-group” activities (simulated simultaneously).
We then investigate the influence of these synthetic data distributions on model performance in the downstream target dataset.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Benchmark Method</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> have demonstrated the superior effectiveness of end-to-end methods over traditional tracking-by-detection approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, particularly evident in challenging datasets like DanceTrack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> and MOT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>.
Therefore, our study aims to showcase the effectiveness of synthetic data in improving downstream task performance using end-to-end tracking models, which offer a better assessment of impact of synthetic data on overall tracking performance.
We primarily evaluate the performance using MOTRv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, the state-of-the-art method across various benchmark datasets.
MOTRv2 is an extension of MOTR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> by integrating YOLO-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> for enhanced detection bootstrapping.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Tab. <a href="#S4.T1" title="Table 1 ‣ 4.1 Preliminaries ‣ 4 Experiments ‣ On the Equivalency, Substitutability, and Flexibility of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the tracking results on MOT17 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>.
First, pretraining with our synthetic data leads to significant performance gain on all five metrics, compared to the model without pretraining.
This observation aligns with several previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> that adding synthetic data improves model performances.
Second, the performance improves with the increased amount of real data, as illustrated in Fig. <a href="#S4.F2" title="Figure 2 ‣ 4.1 Preliminaries ‣ 4 Experiments ‣ On the Equivalency, Substitutability, and Flexibility of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The model finetuned on only 20% (or less) real data is inferior to the same model trained on 100% real data, which suggests a domain gap between the synthetic and real data.
Last, the model trained solely on 100% real data achieves comparable performance to the model finetuned on 20% to 40% of real data.
In other words, <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">our synthetic data is equivalent to 60% to 80% of MOT17 training data.</span></p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.5.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.5.5.5" class="ltx_tr">
<th id="S4.T2.5.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.5.6.1" class="ltx_text" style="font-size:90%;">Training Data</span></th>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:-0.25pt 2.1pt;">
<span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">HOTA</span><math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:-0.25pt 2.1pt;">
<span id="S4.T2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">DetA</span><math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:-0.25pt 2.1pt;">
<span id="S4.T2.3.3.3.3.1" class="ltx_text" style="font-size:90%;">AssA</span><math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:-0.25pt 2.1pt;">
<span id="S4.T2.4.4.4.4.1" class="ltx_text" style="font-size:90%;">IDF1</span><math id="S4.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.4.4.4.4.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T2.5.5.5.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:-0.25pt 2.1pt;">
<span id="S4.T2.5.5.5.5.1" class="ltx_text" style="font-size:90%;">MOTA</span><math id="S4.T2.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.5.5.5.5.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T2.5.5.5.5.m1.1.1" xref="S4.T2.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.5.5.6.1" class="ltx_tr">
<th id="S4.T2.5.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.6.1.1.1" class="ltx_text" style="font-size:80%;">DT</span></th>
<td id="S4.T2.5.5.6.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.6.1.2.1" class="ltx_text" style="font-size:90%;">68.8</span></td>
<td id="S4.T2.5.5.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.6.1.3.1" class="ltx_text" style="font-size:90%;">82.5</span></td>
<td id="S4.T2.5.5.6.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.6.1.4.1" class="ltx_text" style="font-size:90%;">57.4</span></td>
<td id="S4.T2.5.5.6.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.6.1.5.1" class="ltx_text" style="font-size:90%;">70.3</span></td>
<td id="S4.T2.5.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.6.1.6.1" class="ltx_text" style="font-size:90%;">90.8</span></td>
</tr>
<tr id="S4.T2.5.5.7.2" class="ltx_tr">
<th id="S4.T2.5.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.7.2.1.1" class="ltx_text" style="font-size:80%;">DT + Syn (D)</span></th>
<td id="S4.T2.5.5.7.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.7.2.2.1" class="ltx_text" style="font-size:90%;">59.0</span></td>
<td id="S4.T2.5.5.7.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.7.2.3.1" class="ltx_text" style="font-size:90%;">75.5</span></td>
<td id="S4.T2.5.5.7.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.7.2.4.1" class="ltx_text" style="font-size:90%;">46.1</span></td>
<td id="S4.T2.5.5.7.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.7.2.5.1" class="ltx_text" style="font-size:90%;">59.0</span></td>
<td id="S4.T2.5.5.7.2.6" class="ltx_td ltx_align_center" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.7.2.6.1" class="ltx_text" style="font-size:90%;">82.6</span></td>
</tr>
<tr id="S4.T2.5.5.8.3" class="ltx_tr">
<th id="S4.T2.5.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.8.3.1.1" class="ltx_text" style="font-size:80%;">DT + Syn (WR)</span></th>
<td id="S4.T2.5.5.8.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.8.3.2.1" class="ltx_text" style="font-size:90%;">70.1</span></td>
<td id="S4.T2.5.5.8.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.8.3.3.1" class="ltx_text" style="font-size:90%;">83.1</span></td>
<td id="S4.T2.5.5.8.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.8.3.4.1" class="ltx_text" style="font-size:90%;">59.4</span></td>
<td id="S4.T2.5.5.8.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.8.3.5.1" class="ltx_text" style="font-size:90%;">72.5</span></td>
<td id="S4.T2.5.5.8.3.6" class="ltx_td ltx_align_center" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.8.3.6.1" class="ltx_text" style="font-size:90%;">92.0</span></td>
</tr>
<tr id="S4.T2.5.5.9.4" class="ltx_tr">
<th id="S4.T2.5.5.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.9.4.1.1" class="ltx_text" style="font-size:80%;">DT + Syn (WR+D)</span></th>
<td id="S4.T2.5.5.9.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.9.4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">71.9</span></td>
<td id="S4.T2.5.5.9.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.9.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">83.6</span></td>
<td id="S4.T2.5.5.9.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.9.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">62.0</span></td>
<td id="S4.T2.5.5.9.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.9.4.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">74.7</span></td>
<td id="S4.T2.5.5.9.4.6" class="ltx_td ltx_align_center" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.9.4.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">92.6</span></td>
</tr>
<tr id="S4.T2.5.5.10.5" class="ltx_tr">
<th id="S4.T2.5.5.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.10.5.1.1" class="ltx_text" style="font-size:80%;">DT + Syn (Single-group)</span></th>
<td id="S4.T2.5.5.10.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.10.5.2.1" class="ltx_text" style="font-size:90%;">65.1</span></td>
<td id="S4.T2.5.5.10.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.10.5.3.1" class="ltx_text" style="font-size:90%;">80.2</span></td>
<td id="S4.T2.5.5.10.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.10.5.4.1" class="ltx_text" style="font-size:90%;">55.8</span></td>
<td id="S4.T2.5.5.10.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.10.5.5.1" class="ltx_text" style="font-size:90%;">66.7</span></td>
<td id="S4.T2.5.5.10.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.10.5.6.1" class="ltx_text" style="font-size:90%;">89.1</span></td>
</tr>
<tr id="S4.T2.5.5.11.6" class="ltx_tr">
<th id="S4.T2.5.5.11.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.11.6.1.1" class="ltx_text" style="font-size:80%;">DT + Syn (Multi-group)</span></th>
<td id="S4.T2.5.5.11.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.11.6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">73.5</span></td>
<td id="S4.T2.5.5.11.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.11.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">83.9</span></td>
<td id="S4.T2.5.5.11.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.11.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.7</span></td>
<td id="S4.T2.5.5.11.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.11.6.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">75.8</span></td>
<td id="S4.T2.5.5.11.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:-0.25pt 2.1pt;"><span id="S4.T2.5.5.11.6.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">93.0</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>MPT results on DanceTrack under different synthetic data distributions (eg. group types). We use all single-group and multi-group data for the last two rows.
</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We present in Tab. <a href="#S4.T2" title="Table 2 ‣ 4.3 Results ‣ 4 Experiments ‣ On the Equivalency, Substitutability, and Flexibility of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> the results on DanceTrack using various group types of synthetic data for training.
These results reveal that training with multi-group synthetic data, such as ”WR” and ”Multi-group,” leads to superior performance.
The design of multi-group synthetic data, with multiple human activities animated in the same scene, provides frequent identity switches and produces high-complexity datasets that are suitable for challenging target datasets with dynamic movements such as DanceTrack.
On the contrary, using synthetic data with each subject animated procedually and independently, such as ”D” and ”Single-group”, yields inferior downstream performance to the baseline that is trained without any synthetic data.
Our findings suggest that <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">synthetic datasets with apparent similarities in data complexity and distribution to target datasets tend to enhance model performance.</span>
It also underscores the importance of having a flexible data generator, such as the modular and highly parameterized groups in <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S4.SS3.p2.1.1.1" class="ltx_sup"><span id="S4.SS3.p2.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span>, which allows for the customization of synthetic data distributions to closely approximate those of target real-world datasets.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Discussions</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We discuss the following properties of synthetic data in our study and how the experimental results help to answer the research questions and provide insights for the potential implications of synthetic data.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Equivalency and Substitutability</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">Synthetic data has shown the potential to replace a substantial portion of real-world data, as evidenced by previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. For instance, results from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> indicate that synthetic data generated by <span id="S4.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_typewriter">M<sup id="S4.SS4.SSS1.p1.1.1.1" class="ltx_sup"><span id="S4.SS4.SSS1.p1.1.1.1.1" class="ltx_text ltx_font_serif ltx_font_italic">3</span></sup>Act</span> can substitute for 62.5% more real data from DanceTrack.
In our study, we found that synthetic data can replace 60% to 80% of real-world data without sacrificing performance on MOT17.
When considering the total number of image frames in both datasets, the equivalency ratio of synthetic to real data is approximately 30 times.
Similarly, when considering the total number of track frames (or annotated bounding boxes), the ratio is roughly 5.6 times.
These equivalency measurements provide valuable insights into the suitability and efficiency of different synthetic datasets for target downstream tasks.
Furthermore, our results highlight that, despite existing domain gaps between synthetic and real datasets, the substitutability of synthetic data for target real-world data can significantly reduce data collection and annotation costs.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Impact of Synthetic Data Distributions</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">Although synthetic data offers practical benefits such as enhanced performance, substitutability, and cost reduction, its effectiveness relies on how closely its distribution matches that of the target real-world data.
Our experiments suggest that not all synthetic datasets contribute equally to enhancing downstream task performances, even when designed for the same task.
In fact, adding some synthetic data to the training dataset could even worsen performance.
While most synthetic data generators <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> offer limited parameterization options
during the generation process, more complex tasks like multi-person tracking or scenarios involving multiple groups necessitate finer adjustments to the configuration parameters such as group types, person alignments, and group size within the data generator.
Therefore, providing flexibility in synthetic data generators and adjusting parameters to generate synthetic data are crucial for narrowing domain gaps and ensuring the efficacy of using synthetic data in real-world applications.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we explore the equivalency, substitutability, and flexibility of synthetic data, offering insights into its practical applications in real-world scenarios.
Despite our efforts and promising results shown in the experiments, concrete evidence of complete substitution of synthetic data for real data in training datasets remains elusive.
One significant challenge arises from the existence of domain gaps between synthetic and real data, which ultimately limit the adaptability of models trained solely on synthetic data.
As we move forward, addressing these limitations will be crucial in maximizing the potential of synthetic data and enhancing its utility in various machine learning tasks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Bazavan et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Eduard Gabriel Bazavan, Andrei Zanfir, Mihai Zanfir, William T Freeman, Rahul Sukthankar, and Cristian Sminchisescu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Hspace: Synthetic parametric humans animated in complex environments.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.12867</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Bewley et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Simple online and realtime tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2016 IEEE international conference on image processing (ICIP)</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, pages 3464–3468. IEEE, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Black et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Michael J Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Bedlam: A synthetic dataset of bodies exhibiting detailed lifelike animated motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 8726–8737, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Borkman et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh, Bowen Li, Steven Leal, Pete Parisi, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Unity perception: Generate synthetic data for computer vision.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.04259</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Brooks et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Video generation models as world simulators.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">2024.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Cai et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, Chen Change Loy, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Playing for 3d human recovery.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2110.07588</em><span id="bib.bib6.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Cao et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Jinkun Cao, Jiangmiao Pang, Xinshuo Weng, Rawal Khirodkar, and Kris Kitani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Observation-centric sort: Rethinking sort for robust multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, pages 9686–9696, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.4.4.1" class="ltx_text" style="font-size:90%;">Chang [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">
Che-Jui Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">Transfer learning from monolingual asr to transcription-free cross-lingual voice conversion.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.14668</em><span id="bib.bib8.9.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.4.4.1" class="ltx_text" style="font-size:90%;">Chang and Jeng [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text" style="font-size:90%;">
Che-Jui Chang and Shyh-Kang Jeng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">Acoustic anomaly detection using multilayer neural networks and semantic pointers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Information Science &amp; Engineering</em><span id="bib.bib9.9.2" class="ltx_text" style="font-size:90%;">, 37(1), 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Chang et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Che-Jui Chang, Sen Zhang, and Mubbasir Kapadia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">The ivi lab entry to the genea challenge 2022–a tacotron2 based method for co-speech gesture generation with locality-constraint attention mechanism.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2022 International Conference on Multimodal Interaction</em><span id="bib.bib10.11.3" class="ltx_text" style="font-size:90%;">, pages 784–789, 2022a.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Chang et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Che-Jui Chang, Long Zhao, Sen Zhang, and Mubbasir Kapadia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Disentangling audio content and emotion with adaptive instance normalization for expressive facial animation synthesis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Animation and Virtual Worlds</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, 33(3-4):e2076, 2022b.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Chang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Che-Jui Chang, Danrui Li, Deep Patel, Parth Goel, Honglu Zhou, Seonghyeon Moon, Samuel S. Sohn, Sejong Yoon, Vladimir Pavlovic, and Mubbasir Kapadia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Learning from synthetic human group activities, 2023a.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Chang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Che-Jui Chang, Samuel S Sohn, Sen Zhang, Rajath Jayashankar, Muhammad Usman, and Mubbasir Kapadia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">The importance of multimodal emotion conditioning and affect consistency for embodied conversational agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 28th International Conference on Intelligent User Interfaces</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, pages 790–801, 2023b.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Dendorfer et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixé.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Mot20: A benchmark for multi object tracking in crowded scenes.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.09003</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Ebadi et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Salehe Erfanian Ebadi, You-Cyuan Jhang, Alex Zook, Saurav Dhakad, Adam Crespi, Pete Parisi, Steven Borkman, Jonathan Hogins, and Sujoy Ganguly.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Peoplesanspeople: a synthetic data generator for human-centric computer vision.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib15.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.09290</em><span id="bib.bib15.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.4.4.1" class="ltx_text" style="font-size:90%;">Gao and Wang [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">
Ruopeng Gao and Limin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">MeMOTR: Long-term memory-augmented transformer for multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em><span id="bib.bib16.10.3" class="ltx_text" style="font-size:90%;">, pages 9901–9910, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Gavrilyuk et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, and Cees GM Snoek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Actor-transformers for group activity recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, pages 839–848, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Ge et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Yolox: Exceeding yolo series in 2021.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.08430</em><span id="bib.bib18.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Milan et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Anton Milan, Laura Leal-Taixé, Ian Reid, Stefan Roth, and Konrad Schindler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Mot16: A benchmark for multi-object tracking.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1603.00831</em><span id="bib.bib19.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Rahman et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Md Ashiqur Rahman, Jasorsi Ghosh, Hrishikesh Viswanath, Kamyar Azizzadenesheli, and Aniket Bera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Pacmo: Partner dependent human motion generation in dyadic human activity using neural operators.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.16210</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Rombach et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, pages 10684–10695, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Sun et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Peize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai, Kris Kitani, and Ping Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Dancetrack: Multi-object tracking in uniform appearance and diverse motion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, pages 20993–21002, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Varol et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J Black, Ivan Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, pages 109–117, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Wojke et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Nicolai Wojke, Alex Bewley, and Dietrich Paulus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Simple online and realtime tracking with a deep association metric.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE international conference on image processing (ICIP)</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, pages 3645–3649. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Yan et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Feng Yan, Weixin Luo, Yujie Zhong, Yiyang Gan, and Lin Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Bridging the gap between end-to-end and non-end-to-end multi-object tracking, 2023.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Synbody: Synthetic dataset with layered human models for 3d human perception and modeling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.17368</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Zeng et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Motr: End-to-end multiple-object tracking with transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, pages 659–675. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Yuang Zhang, Tiancai Wang, and Xiangyu Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Motrv2: Bootstrapping end-to-end multi-object tracking by pretrained object detectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, pages 22056–22065, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Honglu Zhou, Asim Kadav, Aviv Shamsian, Shijie Geng, Farley Lai, Long Zhao, Ting Liu, Mubbasir Kapadia, and Hans Peter Graf.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Composer: Compositional reasoning of group activity in videos with keypoint-only modality.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 17th European Conference on Computer Vision (ECCV 2022)</em><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.16243" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.16244" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.16244">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.16244" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.16245" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 16:32:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
